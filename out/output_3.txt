no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  3
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.34796
[1mStep[0m  [33/339], [94mLoss[0m : 10.25125
[1mStep[0m  [66/339], [94mLoss[0m : 9.71720
[1mStep[0m  [99/339], [94mLoss[0m : 6.73846
[1mStep[0m  [132/339], [94mLoss[0m : 5.90557
[1mStep[0m  [165/339], [94mLoss[0m : 3.93193
[1mStep[0m  [198/339], [94mLoss[0m : 2.14677
[1mStep[0m  [231/339], [94mLoss[0m : 2.71075
[1mStep[0m  [264/339], [94mLoss[0m : 2.62481
[1mStep[0m  [297/339], [94mLoss[0m : 2.51742
[1mStep[0m  [330/339], [94mLoss[0m : 2.09902

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.292, [92mTest[0m: 10.797, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58079
[1mStep[0m  [33/339], [94mLoss[0m : 3.20666
[1mStep[0m  [66/339], [94mLoss[0m : 2.38081
[1mStep[0m  [99/339], [94mLoss[0m : 3.05243
[1mStep[0m  [132/339], [94mLoss[0m : 2.29290
[1mStep[0m  [165/339], [94mLoss[0m : 2.79686
[1mStep[0m  [198/339], [94mLoss[0m : 2.17096
[1mStep[0m  [231/339], [94mLoss[0m : 1.93673
[1mStep[0m  [264/339], [94mLoss[0m : 2.51357
[1mStep[0m  [297/339], [94mLoss[0m : 2.66157
[1mStep[0m  [330/339], [94mLoss[0m : 2.54584

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.517, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48759
[1mStep[0m  [33/339], [94mLoss[0m : 2.39099
[1mStep[0m  [66/339], [94mLoss[0m : 2.54589
[1mStep[0m  [99/339], [94mLoss[0m : 2.51068
[1mStep[0m  [132/339], [94mLoss[0m : 2.40472
[1mStep[0m  [165/339], [94mLoss[0m : 2.95541
[1mStep[0m  [198/339], [94mLoss[0m : 2.72207
[1mStep[0m  [231/339], [94mLoss[0m : 2.34754
[1mStep[0m  [264/339], [94mLoss[0m : 2.13938
[1mStep[0m  [297/339], [94mLoss[0m : 2.68632
[1mStep[0m  [330/339], [94mLoss[0m : 2.36969

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71576
[1mStep[0m  [33/339], [94mLoss[0m : 2.82796
[1mStep[0m  [66/339], [94mLoss[0m : 2.42905
[1mStep[0m  [99/339], [94mLoss[0m : 2.40800
[1mStep[0m  [132/339], [94mLoss[0m : 1.59095
[1mStep[0m  [165/339], [94mLoss[0m : 2.84633
[1mStep[0m  [198/339], [94mLoss[0m : 2.92382
[1mStep[0m  [231/339], [94mLoss[0m : 2.09480
[1mStep[0m  [264/339], [94mLoss[0m : 2.25236
[1mStep[0m  [297/339], [94mLoss[0m : 2.77223
[1mStep[0m  [330/339], [94mLoss[0m : 2.54712

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42764
[1mStep[0m  [33/339], [94mLoss[0m : 2.05974
[1mStep[0m  [66/339], [94mLoss[0m : 3.02169
[1mStep[0m  [99/339], [94mLoss[0m : 1.87076
[1mStep[0m  [132/339], [94mLoss[0m : 2.69707
[1mStep[0m  [165/339], [94mLoss[0m : 2.71964
[1mStep[0m  [198/339], [94mLoss[0m : 2.49174
[1mStep[0m  [231/339], [94mLoss[0m : 2.69445
[1mStep[0m  [264/339], [94mLoss[0m : 2.43986
[1mStep[0m  [297/339], [94mLoss[0m : 2.00203
[1mStep[0m  [330/339], [94mLoss[0m : 2.93997

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16330
[1mStep[0m  [33/339], [94mLoss[0m : 2.59146
[1mStep[0m  [66/339], [94mLoss[0m : 2.17357
[1mStep[0m  [99/339], [94mLoss[0m : 2.07268
[1mStep[0m  [132/339], [94mLoss[0m : 2.86615
[1mStep[0m  [165/339], [94mLoss[0m : 1.81865
[1mStep[0m  [198/339], [94mLoss[0m : 2.79157
[1mStep[0m  [231/339], [94mLoss[0m : 1.89814
[1mStep[0m  [264/339], [94mLoss[0m : 2.64805
[1mStep[0m  [297/339], [94mLoss[0m : 2.68237
[1mStep[0m  [330/339], [94mLoss[0m : 2.54334

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34595
[1mStep[0m  [33/339], [94mLoss[0m : 2.46084
[1mStep[0m  [66/339], [94mLoss[0m : 2.87391
[1mStep[0m  [99/339], [94mLoss[0m : 2.59009
[1mStep[0m  [132/339], [94mLoss[0m : 2.59686
[1mStep[0m  [165/339], [94mLoss[0m : 2.76783
[1mStep[0m  [198/339], [94mLoss[0m : 2.76364
[1mStep[0m  [231/339], [94mLoss[0m : 3.40858
[1mStep[0m  [264/339], [94mLoss[0m : 2.68781
[1mStep[0m  [297/339], [94mLoss[0m : 1.88479
[1mStep[0m  [330/339], [94mLoss[0m : 2.27720

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25320
[1mStep[0m  [33/339], [94mLoss[0m : 3.42249
[1mStep[0m  [66/339], [94mLoss[0m : 2.92395
[1mStep[0m  [99/339], [94mLoss[0m : 2.00488
[1mStep[0m  [132/339], [94mLoss[0m : 2.59070
[1mStep[0m  [165/339], [94mLoss[0m : 2.00927
[1mStep[0m  [198/339], [94mLoss[0m : 2.15077
[1mStep[0m  [231/339], [94mLoss[0m : 3.38611
[1mStep[0m  [264/339], [94mLoss[0m : 2.54404
[1mStep[0m  [297/339], [94mLoss[0m : 2.50393
[1mStep[0m  [330/339], [94mLoss[0m : 2.84947

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.91035
[1mStep[0m  [33/339], [94mLoss[0m : 2.86066
[1mStep[0m  [66/339], [94mLoss[0m : 2.36585
[1mStep[0m  [99/339], [94mLoss[0m : 2.80890
[1mStep[0m  [132/339], [94mLoss[0m : 2.66531
[1mStep[0m  [165/339], [94mLoss[0m : 2.70451
[1mStep[0m  [198/339], [94mLoss[0m : 2.63985
[1mStep[0m  [231/339], [94mLoss[0m : 2.33675
[1mStep[0m  [264/339], [94mLoss[0m : 2.73749
[1mStep[0m  [297/339], [94mLoss[0m : 3.16772
[1mStep[0m  [330/339], [94mLoss[0m : 2.11210

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11948
[1mStep[0m  [33/339], [94mLoss[0m : 1.89432
[1mStep[0m  [66/339], [94mLoss[0m : 2.24049
[1mStep[0m  [99/339], [94mLoss[0m : 2.41059
[1mStep[0m  [132/339], [94mLoss[0m : 2.20041
[1mStep[0m  [165/339], [94mLoss[0m : 2.58232
[1mStep[0m  [198/339], [94mLoss[0m : 2.17785
[1mStep[0m  [231/339], [94mLoss[0m : 3.00606
[1mStep[0m  [264/339], [94mLoss[0m : 1.84545
[1mStep[0m  [297/339], [94mLoss[0m : 2.07705
[1mStep[0m  [330/339], [94mLoss[0m : 2.71158

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04890
[1mStep[0m  [33/339], [94mLoss[0m : 2.51248
[1mStep[0m  [66/339], [94mLoss[0m : 2.41448
[1mStep[0m  [99/339], [94mLoss[0m : 2.73133
[1mStep[0m  [132/339], [94mLoss[0m : 1.96283
[1mStep[0m  [165/339], [94mLoss[0m : 2.51555
[1mStep[0m  [198/339], [94mLoss[0m : 2.48784
[1mStep[0m  [231/339], [94mLoss[0m : 2.05274
[1mStep[0m  [264/339], [94mLoss[0m : 3.20351
[1mStep[0m  [297/339], [94mLoss[0m : 2.45752
[1mStep[0m  [330/339], [94mLoss[0m : 2.86151

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.24537
[1mStep[0m  [33/339], [94mLoss[0m : 2.47424
[1mStep[0m  [66/339], [94mLoss[0m : 2.54141
[1mStep[0m  [99/339], [94mLoss[0m : 1.92848
[1mStep[0m  [132/339], [94mLoss[0m : 2.39713
[1mStep[0m  [165/339], [94mLoss[0m : 2.77673
[1mStep[0m  [198/339], [94mLoss[0m : 3.00612
[1mStep[0m  [231/339], [94mLoss[0m : 2.83616
[1mStep[0m  [264/339], [94mLoss[0m : 2.29278
[1mStep[0m  [297/339], [94mLoss[0m : 2.17286
[1mStep[0m  [330/339], [94mLoss[0m : 2.21668

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94649
[1mStep[0m  [33/339], [94mLoss[0m : 2.53924
[1mStep[0m  [66/339], [94mLoss[0m : 2.93878
[1mStep[0m  [99/339], [94mLoss[0m : 2.14355
[1mStep[0m  [132/339], [94mLoss[0m : 2.45422
[1mStep[0m  [165/339], [94mLoss[0m : 3.01313
[1mStep[0m  [198/339], [94mLoss[0m : 2.55048
[1mStep[0m  [231/339], [94mLoss[0m : 2.80934
[1mStep[0m  [264/339], [94mLoss[0m : 2.20804
[1mStep[0m  [297/339], [94mLoss[0m : 1.99702
[1mStep[0m  [330/339], [94mLoss[0m : 2.18944

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34182
[1mStep[0m  [33/339], [94mLoss[0m : 2.66998
[1mStep[0m  [66/339], [94mLoss[0m : 3.01923
[1mStep[0m  [99/339], [94mLoss[0m : 2.14416
[1mStep[0m  [132/339], [94mLoss[0m : 2.20600
[1mStep[0m  [165/339], [94mLoss[0m : 2.66543
[1mStep[0m  [198/339], [94mLoss[0m : 1.92966
[1mStep[0m  [231/339], [94mLoss[0m : 1.89741
[1mStep[0m  [264/339], [94mLoss[0m : 2.99568
[1mStep[0m  [297/339], [94mLoss[0m : 2.54493
[1mStep[0m  [330/339], [94mLoss[0m : 2.73720

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74840
[1mStep[0m  [33/339], [94mLoss[0m : 2.34249
[1mStep[0m  [66/339], [94mLoss[0m : 2.50375
[1mStep[0m  [99/339], [94mLoss[0m : 2.63289
[1mStep[0m  [132/339], [94mLoss[0m : 1.83544
[1mStep[0m  [165/339], [94mLoss[0m : 2.46751
[1mStep[0m  [198/339], [94mLoss[0m : 2.27852
[1mStep[0m  [231/339], [94mLoss[0m : 2.71712
[1mStep[0m  [264/339], [94mLoss[0m : 2.21993
[1mStep[0m  [297/339], [94mLoss[0m : 2.86359
[1mStep[0m  [330/339], [94mLoss[0m : 2.11760

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82956
[1mStep[0m  [33/339], [94mLoss[0m : 2.39732
[1mStep[0m  [66/339], [94mLoss[0m : 2.13166
[1mStep[0m  [99/339], [94mLoss[0m : 2.24718
[1mStep[0m  [132/339], [94mLoss[0m : 1.98756
[1mStep[0m  [165/339], [94mLoss[0m : 2.42930
[1mStep[0m  [198/339], [94mLoss[0m : 2.13113
[1mStep[0m  [231/339], [94mLoss[0m : 2.52095
[1mStep[0m  [264/339], [94mLoss[0m : 2.49087
[1mStep[0m  [297/339], [94mLoss[0m : 2.46998
[1mStep[0m  [330/339], [94mLoss[0m : 2.65172

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95170
[1mStep[0m  [33/339], [94mLoss[0m : 2.47701
[1mStep[0m  [66/339], [94mLoss[0m : 2.73433
[1mStep[0m  [99/339], [94mLoss[0m : 2.23093
[1mStep[0m  [132/339], [94mLoss[0m : 2.91506
[1mStep[0m  [165/339], [94mLoss[0m : 2.69336
[1mStep[0m  [198/339], [94mLoss[0m : 2.30915
[1mStep[0m  [231/339], [94mLoss[0m : 1.91407
[1mStep[0m  [264/339], [94mLoss[0m : 2.96380
[1mStep[0m  [297/339], [94mLoss[0m : 2.66884
[1mStep[0m  [330/339], [94mLoss[0m : 2.15951

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31904
[1mStep[0m  [33/339], [94mLoss[0m : 1.93858
[1mStep[0m  [66/339], [94mLoss[0m : 2.78930
[1mStep[0m  [99/339], [94mLoss[0m : 2.99759
[1mStep[0m  [132/339], [94mLoss[0m : 2.73326
[1mStep[0m  [165/339], [94mLoss[0m : 2.31401
[1mStep[0m  [198/339], [94mLoss[0m : 2.65503
[1mStep[0m  [231/339], [94mLoss[0m : 2.16279
[1mStep[0m  [264/339], [94mLoss[0m : 3.21788
[1mStep[0m  [297/339], [94mLoss[0m : 2.12682
[1mStep[0m  [330/339], [94mLoss[0m : 1.97596

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77662
[1mStep[0m  [33/339], [94mLoss[0m : 2.57686
[1mStep[0m  [66/339], [94mLoss[0m : 2.25033
[1mStep[0m  [99/339], [94mLoss[0m : 2.45031
[1mStep[0m  [132/339], [94mLoss[0m : 2.55796
[1mStep[0m  [165/339], [94mLoss[0m : 1.97903
[1mStep[0m  [198/339], [94mLoss[0m : 2.33878
[1mStep[0m  [231/339], [94mLoss[0m : 1.87852
[1mStep[0m  [264/339], [94mLoss[0m : 2.30396
[1mStep[0m  [297/339], [94mLoss[0m : 1.96570
[1mStep[0m  [330/339], [94mLoss[0m : 2.69960

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58432
[1mStep[0m  [33/339], [94mLoss[0m : 2.02329
[1mStep[0m  [66/339], [94mLoss[0m : 1.88486
[1mStep[0m  [99/339], [94mLoss[0m : 2.51969
[1mStep[0m  [132/339], [94mLoss[0m : 2.51221
[1mStep[0m  [165/339], [94mLoss[0m : 3.06671
[1mStep[0m  [198/339], [94mLoss[0m : 1.99191
[1mStep[0m  [231/339], [94mLoss[0m : 2.73107
[1mStep[0m  [264/339], [94mLoss[0m : 2.22034
[1mStep[0m  [297/339], [94mLoss[0m : 2.35285
[1mStep[0m  [330/339], [94mLoss[0m : 2.70006

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60004
[1mStep[0m  [33/339], [94mLoss[0m : 2.49279
[1mStep[0m  [66/339], [94mLoss[0m : 2.53199
[1mStep[0m  [99/339], [94mLoss[0m : 2.67290
[1mStep[0m  [132/339], [94mLoss[0m : 2.53087
[1mStep[0m  [165/339], [94mLoss[0m : 2.86336
[1mStep[0m  [198/339], [94mLoss[0m : 2.36107
[1mStep[0m  [231/339], [94mLoss[0m : 1.48277
[1mStep[0m  [264/339], [94mLoss[0m : 2.38119
[1mStep[0m  [297/339], [94mLoss[0m : 2.23427
[1mStep[0m  [330/339], [94mLoss[0m : 1.85024

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30190
[1mStep[0m  [33/339], [94mLoss[0m : 2.08643
[1mStep[0m  [66/339], [94mLoss[0m : 2.22221
[1mStep[0m  [99/339], [94mLoss[0m : 3.10172
[1mStep[0m  [132/339], [94mLoss[0m : 2.67856
[1mStep[0m  [165/339], [94mLoss[0m : 1.93131
[1mStep[0m  [198/339], [94mLoss[0m : 2.40029
[1mStep[0m  [231/339], [94mLoss[0m : 2.17031
[1mStep[0m  [264/339], [94mLoss[0m : 2.28234
[1mStep[0m  [297/339], [94mLoss[0m : 2.16678
[1mStep[0m  [330/339], [94mLoss[0m : 2.16740

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62701
[1mStep[0m  [33/339], [94mLoss[0m : 2.40315
[1mStep[0m  [66/339], [94mLoss[0m : 2.65605
[1mStep[0m  [99/339], [94mLoss[0m : 2.52951
[1mStep[0m  [132/339], [94mLoss[0m : 1.83758
[1mStep[0m  [165/339], [94mLoss[0m : 2.34429
[1mStep[0m  [198/339], [94mLoss[0m : 2.70109
[1mStep[0m  [231/339], [94mLoss[0m : 3.11503
[1mStep[0m  [264/339], [94mLoss[0m : 2.54180
[1mStep[0m  [297/339], [94mLoss[0m : 2.63010
[1mStep[0m  [330/339], [94mLoss[0m : 2.81329

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.14876
[1mStep[0m  [33/339], [94mLoss[0m : 1.82864
[1mStep[0m  [66/339], [94mLoss[0m : 3.43567
[1mStep[0m  [99/339], [94mLoss[0m : 2.09828
[1mStep[0m  [132/339], [94mLoss[0m : 1.62587
[1mStep[0m  [165/339], [94mLoss[0m : 2.28796
[1mStep[0m  [198/339], [94mLoss[0m : 1.95297
[1mStep[0m  [231/339], [94mLoss[0m : 2.92537
[1mStep[0m  [264/339], [94mLoss[0m : 2.96705
[1mStep[0m  [297/339], [94mLoss[0m : 2.78335
[1mStep[0m  [330/339], [94mLoss[0m : 2.46001

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65449
[1mStep[0m  [33/339], [94mLoss[0m : 2.46913
[1mStep[0m  [66/339], [94mLoss[0m : 2.17305
[1mStep[0m  [99/339], [94mLoss[0m : 3.42633
[1mStep[0m  [132/339], [94mLoss[0m : 2.22733
[1mStep[0m  [165/339], [94mLoss[0m : 2.22060
[1mStep[0m  [198/339], [94mLoss[0m : 2.65237
[1mStep[0m  [231/339], [94mLoss[0m : 2.98833
[1mStep[0m  [264/339], [94mLoss[0m : 2.41055
[1mStep[0m  [297/339], [94mLoss[0m : 1.93724
[1mStep[0m  [330/339], [94mLoss[0m : 2.37246

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33872
[1mStep[0m  [33/339], [94mLoss[0m : 2.02611
[1mStep[0m  [66/339], [94mLoss[0m : 1.75411
[1mStep[0m  [99/339], [94mLoss[0m : 2.82075
[1mStep[0m  [132/339], [94mLoss[0m : 2.41960
[1mStep[0m  [165/339], [94mLoss[0m : 2.57663
[1mStep[0m  [198/339], [94mLoss[0m : 2.13971
[1mStep[0m  [231/339], [94mLoss[0m : 3.00121
[1mStep[0m  [264/339], [94mLoss[0m : 2.53318
[1mStep[0m  [297/339], [94mLoss[0m : 1.96617
[1mStep[0m  [330/339], [94mLoss[0m : 2.71199

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35485
[1mStep[0m  [33/339], [94mLoss[0m : 3.25082
[1mStep[0m  [66/339], [94mLoss[0m : 2.62188
[1mStep[0m  [99/339], [94mLoss[0m : 3.75170
[1mStep[0m  [132/339], [94mLoss[0m : 2.41174
[1mStep[0m  [165/339], [94mLoss[0m : 2.79740
[1mStep[0m  [198/339], [94mLoss[0m : 2.47357
[1mStep[0m  [231/339], [94mLoss[0m : 2.23953
[1mStep[0m  [264/339], [94mLoss[0m : 2.42671
[1mStep[0m  [297/339], [94mLoss[0m : 2.59178
[1mStep[0m  [330/339], [94mLoss[0m : 2.96144

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18490
[1mStep[0m  [33/339], [94mLoss[0m : 2.26314
[1mStep[0m  [66/339], [94mLoss[0m : 2.61316
[1mStep[0m  [99/339], [94mLoss[0m : 2.79134
[1mStep[0m  [132/339], [94mLoss[0m : 2.19387
[1mStep[0m  [165/339], [94mLoss[0m : 2.41021
[1mStep[0m  [198/339], [94mLoss[0m : 2.46110
[1mStep[0m  [231/339], [94mLoss[0m : 2.38239
[1mStep[0m  [264/339], [94mLoss[0m : 2.13447
[1mStep[0m  [297/339], [94mLoss[0m : 2.49577
[1mStep[0m  [330/339], [94mLoss[0m : 1.96165

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66531
[1mStep[0m  [33/339], [94mLoss[0m : 2.79016
[1mStep[0m  [66/339], [94mLoss[0m : 2.36669
[1mStep[0m  [99/339], [94mLoss[0m : 2.49578
[1mStep[0m  [132/339], [94mLoss[0m : 2.12921
[1mStep[0m  [165/339], [94mLoss[0m : 2.21768
[1mStep[0m  [198/339], [94mLoss[0m : 2.81520
[1mStep[0m  [231/339], [94mLoss[0m : 1.76887
[1mStep[0m  [264/339], [94mLoss[0m : 2.32933
[1mStep[0m  [297/339], [94mLoss[0m : 2.74502
[1mStep[0m  [330/339], [94mLoss[0m : 2.25196

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01783
[1mStep[0m  [33/339], [94mLoss[0m : 2.51751
[1mStep[0m  [66/339], [94mLoss[0m : 2.36020
[1mStep[0m  [99/339], [94mLoss[0m : 2.11073
[1mStep[0m  [132/339], [94mLoss[0m : 2.66711
[1mStep[0m  [165/339], [94mLoss[0m : 2.34158
[1mStep[0m  [198/339], [94mLoss[0m : 2.57513
[1mStep[0m  [231/339], [94mLoss[0m : 2.57358
[1mStep[0m  [264/339], [94mLoss[0m : 2.38756
[1mStep[0m  [297/339], [94mLoss[0m : 2.77421
[1mStep[0m  [330/339], [94mLoss[0m : 2.25370

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.349
====================================

Phase 1 - Evaluation MAE:  2.3488276722156898
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.87668
[1mStep[0m  [33/339], [94mLoss[0m : 2.15576
[1mStep[0m  [66/339], [94mLoss[0m : 2.84129
[1mStep[0m  [99/339], [94mLoss[0m : 3.06495
[1mStep[0m  [132/339], [94mLoss[0m : 1.76811
[1mStep[0m  [165/339], [94mLoss[0m : 2.35875
[1mStep[0m  [198/339], [94mLoss[0m : 2.39223
[1mStep[0m  [231/339], [94mLoss[0m : 2.83339
[1mStep[0m  [264/339], [94mLoss[0m : 1.88490
[1mStep[0m  [297/339], [94mLoss[0m : 2.76037
[1mStep[0m  [330/339], [94mLoss[0m : 2.34198

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27561
[1mStep[0m  [33/339], [94mLoss[0m : 2.01779
[1mStep[0m  [66/339], [94mLoss[0m : 2.55070
[1mStep[0m  [99/339], [94mLoss[0m : 2.26138
[1mStep[0m  [132/339], [94mLoss[0m : 3.11847
[1mStep[0m  [165/339], [94mLoss[0m : 2.65573
[1mStep[0m  [198/339], [94mLoss[0m : 2.49031
[1mStep[0m  [231/339], [94mLoss[0m : 2.40103
[1mStep[0m  [264/339], [94mLoss[0m : 1.99020
[1mStep[0m  [297/339], [94mLoss[0m : 2.05685
[1mStep[0m  [330/339], [94mLoss[0m : 2.37816

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00944
[1mStep[0m  [33/339], [94mLoss[0m : 2.12061
[1mStep[0m  [66/339], [94mLoss[0m : 2.58353
[1mStep[0m  [99/339], [94mLoss[0m : 2.43182
[1mStep[0m  [132/339], [94mLoss[0m : 2.45381
[1mStep[0m  [165/339], [94mLoss[0m : 2.79418
[1mStep[0m  [198/339], [94mLoss[0m : 2.45282
[1mStep[0m  [231/339], [94mLoss[0m : 2.14376
[1mStep[0m  [264/339], [94mLoss[0m : 2.17769
[1mStep[0m  [297/339], [94mLoss[0m : 2.25509
[1mStep[0m  [330/339], [94mLoss[0m : 2.38072

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78095
[1mStep[0m  [33/339], [94mLoss[0m : 1.75777
[1mStep[0m  [66/339], [94mLoss[0m : 2.52725
[1mStep[0m  [99/339], [94mLoss[0m : 2.15880
[1mStep[0m  [132/339], [94mLoss[0m : 2.53191
[1mStep[0m  [165/339], [94mLoss[0m : 2.75456
[1mStep[0m  [198/339], [94mLoss[0m : 2.03304
[1mStep[0m  [231/339], [94mLoss[0m : 2.26513
[1mStep[0m  [264/339], [94mLoss[0m : 3.05333
[1mStep[0m  [297/339], [94mLoss[0m : 2.76715
[1mStep[0m  [330/339], [94mLoss[0m : 2.85924

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42253
[1mStep[0m  [33/339], [94mLoss[0m : 2.80177
[1mStep[0m  [66/339], [94mLoss[0m : 1.98509
[1mStep[0m  [99/339], [94mLoss[0m : 2.00834
[1mStep[0m  [132/339], [94mLoss[0m : 2.16504
[1mStep[0m  [165/339], [94mLoss[0m : 2.44343
[1mStep[0m  [198/339], [94mLoss[0m : 2.22645
[1mStep[0m  [231/339], [94mLoss[0m : 2.30017
[1mStep[0m  [264/339], [94mLoss[0m : 2.38842
[1mStep[0m  [297/339], [94mLoss[0m : 2.54433
[1mStep[0m  [330/339], [94mLoss[0m : 2.50435

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12707
[1mStep[0m  [33/339], [94mLoss[0m : 2.10788
[1mStep[0m  [66/339], [94mLoss[0m : 1.61165
[1mStep[0m  [99/339], [94mLoss[0m : 1.88136
[1mStep[0m  [132/339], [94mLoss[0m : 3.45279
[1mStep[0m  [165/339], [94mLoss[0m : 2.38460
[1mStep[0m  [198/339], [94mLoss[0m : 2.38462
[1mStep[0m  [231/339], [94mLoss[0m : 2.68814
[1mStep[0m  [264/339], [94mLoss[0m : 2.14331
[1mStep[0m  [297/339], [94mLoss[0m : 1.79725
[1mStep[0m  [330/339], [94mLoss[0m : 2.92668

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04368
[1mStep[0m  [33/339], [94mLoss[0m : 2.23225
[1mStep[0m  [66/339], [94mLoss[0m : 2.54789
[1mStep[0m  [99/339], [94mLoss[0m : 1.74105
[1mStep[0m  [132/339], [94mLoss[0m : 2.38190
[1mStep[0m  [165/339], [94mLoss[0m : 2.43749
[1mStep[0m  [198/339], [94mLoss[0m : 2.34089
[1mStep[0m  [231/339], [94mLoss[0m : 1.83108
[1mStep[0m  [264/339], [94mLoss[0m : 2.46783
[1mStep[0m  [297/339], [94mLoss[0m : 2.19216
[1mStep[0m  [330/339], [94mLoss[0m : 2.83131

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93337
[1mStep[0m  [33/339], [94mLoss[0m : 2.05532
[1mStep[0m  [66/339], [94mLoss[0m : 2.15431
[1mStep[0m  [99/339], [94mLoss[0m : 2.04318
[1mStep[0m  [132/339], [94mLoss[0m : 1.98387
[1mStep[0m  [165/339], [94mLoss[0m : 2.11328
[1mStep[0m  [198/339], [94mLoss[0m : 3.04166
[1mStep[0m  [231/339], [94mLoss[0m : 2.29040
[1mStep[0m  [264/339], [94mLoss[0m : 2.56704
[1mStep[0m  [297/339], [94mLoss[0m : 1.94371
[1mStep[0m  [330/339], [94mLoss[0m : 2.79078

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.221, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80093
[1mStep[0m  [33/339], [94mLoss[0m : 2.18153
[1mStep[0m  [66/339], [94mLoss[0m : 2.29777
[1mStep[0m  [99/339], [94mLoss[0m : 1.58070
[1mStep[0m  [132/339], [94mLoss[0m : 1.87692
[1mStep[0m  [165/339], [94mLoss[0m : 1.97902
[1mStep[0m  [198/339], [94mLoss[0m : 2.40126
[1mStep[0m  [231/339], [94mLoss[0m : 2.26908
[1mStep[0m  [264/339], [94mLoss[0m : 1.61894
[1mStep[0m  [297/339], [94mLoss[0m : 2.33429
[1mStep[0m  [330/339], [94mLoss[0m : 2.19096

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44207
[1mStep[0m  [33/339], [94mLoss[0m : 2.06810
[1mStep[0m  [66/339], [94mLoss[0m : 2.30201
[1mStep[0m  [99/339], [94mLoss[0m : 2.25233
[1mStep[0m  [132/339], [94mLoss[0m : 2.41646
[1mStep[0m  [165/339], [94mLoss[0m : 2.53415
[1mStep[0m  [198/339], [94mLoss[0m : 1.84645
[1mStep[0m  [231/339], [94mLoss[0m : 1.78141
[1mStep[0m  [264/339], [94mLoss[0m : 2.21903
[1mStep[0m  [297/339], [94mLoss[0m : 2.40399
[1mStep[0m  [330/339], [94mLoss[0m : 2.15430

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.32954
[1mStep[0m  [33/339], [94mLoss[0m : 2.29129
[1mStep[0m  [66/339], [94mLoss[0m : 1.71668
[1mStep[0m  [99/339], [94mLoss[0m : 2.13409
[1mStep[0m  [132/339], [94mLoss[0m : 1.67111
[1mStep[0m  [165/339], [94mLoss[0m : 2.59014
[1mStep[0m  [198/339], [94mLoss[0m : 2.48387
[1mStep[0m  [231/339], [94mLoss[0m : 1.49623
[1mStep[0m  [264/339], [94mLoss[0m : 1.91156
[1mStep[0m  [297/339], [94mLoss[0m : 2.20137
[1mStep[0m  [330/339], [94mLoss[0m : 2.64243

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36664
[1mStep[0m  [33/339], [94mLoss[0m : 2.35838
[1mStep[0m  [66/339], [94mLoss[0m : 2.18601
[1mStep[0m  [99/339], [94mLoss[0m : 2.40134
[1mStep[0m  [132/339], [94mLoss[0m : 1.81265
[1mStep[0m  [165/339], [94mLoss[0m : 2.31607
[1mStep[0m  [198/339], [94mLoss[0m : 2.69651
[1mStep[0m  [231/339], [94mLoss[0m : 1.76181
[1mStep[0m  [264/339], [94mLoss[0m : 2.12451
[1mStep[0m  [297/339], [94mLoss[0m : 2.17874
[1mStep[0m  [330/339], [94mLoss[0m : 1.70927

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37285
[1mStep[0m  [33/339], [94mLoss[0m : 1.93953
[1mStep[0m  [66/339], [94mLoss[0m : 2.08376
[1mStep[0m  [99/339], [94mLoss[0m : 2.03674
[1mStep[0m  [132/339], [94mLoss[0m : 1.72197
[1mStep[0m  [165/339], [94mLoss[0m : 2.28452
[1mStep[0m  [198/339], [94mLoss[0m : 2.34599
[1mStep[0m  [231/339], [94mLoss[0m : 2.81326
[1mStep[0m  [264/339], [94mLoss[0m : 2.04160
[1mStep[0m  [297/339], [94mLoss[0m : 2.55343
[1mStep[0m  [330/339], [94mLoss[0m : 1.85662

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72778
[1mStep[0m  [33/339], [94mLoss[0m : 2.25261
[1mStep[0m  [66/339], [94mLoss[0m : 1.83710
[1mStep[0m  [99/339], [94mLoss[0m : 2.00857
[1mStep[0m  [132/339], [94mLoss[0m : 1.50116
[1mStep[0m  [165/339], [94mLoss[0m : 1.83179
[1mStep[0m  [198/339], [94mLoss[0m : 2.62865
[1mStep[0m  [231/339], [94mLoss[0m : 2.50135
[1mStep[0m  [264/339], [94mLoss[0m : 1.70822
[1mStep[0m  [297/339], [94mLoss[0m : 2.46550
[1mStep[0m  [330/339], [94mLoss[0m : 2.02731

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.172, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83810
[1mStep[0m  [33/339], [94mLoss[0m : 3.03593
[1mStep[0m  [66/339], [94mLoss[0m : 1.76447
[1mStep[0m  [99/339], [94mLoss[0m : 1.84728
[1mStep[0m  [132/339], [94mLoss[0m : 2.07328
[1mStep[0m  [165/339], [94mLoss[0m : 2.20048
[1mStep[0m  [198/339], [94mLoss[0m : 2.50065
[1mStep[0m  [231/339], [94mLoss[0m : 1.83715
[1mStep[0m  [264/339], [94mLoss[0m : 2.43923
[1mStep[0m  [297/339], [94mLoss[0m : 2.56295
[1mStep[0m  [330/339], [94mLoss[0m : 2.31082

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.397, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86285
[1mStep[0m  [33/339], [94mLoss[0m : 2.43444
[1mStep[0m  [66/339], [94mLoss[0m : 1.70948
[1mStep[0m  [99/339], [94mLoss[0m : 2.61163
[1mStep[0m  [132/339], [94mLoss[0m : 2.21678
[1mStep[0m  [165/339], [94mLoss[0m : 2.28116
[1mStep[0m  [198/339], [94mLoss[0m : 2.03070
[1mStep[0m  [231/339], [94mLoss[0m : 2.00961
[1mStep[0m  [264/339], [94mLoss[0m : 2.42811
[1mStep[0m  [297/339], [94mLoss[0m : 2.33980
[1mStep[0m  [330/339], [94mLoss[0m : 2.77033

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92645
[1mStep[0m  [33/339], [94mLoss[0m : 2.18306
[1mStep[0m  [66/339], [94mLoss[0m : 2.73156
[1mStep[0m  [99/339], [94mLoss[0m : 2.05580
[1mStep[0m  [132/339], [94mLoss[0m : 1.64401
[1mStep[0m  [165/339], [94mLoss[0m : 2.24055
[1mStep[0m  [198/339], [94mLoss[0m : 2.01232
[1mStep[0m  [231/339], [94mLoss[0m : 2.15276
[1mStep[0m  [264/339], [94mLoss[0m : 1.79912
[1mStep[0m  [297/339], [94mLoss[0m : 2.44400
[1mStep[0m  [330/339], [94mLoss[0m : 2.44535

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78422
[1mStep[0m  [33/339], [94mLoss[0m : 1.67878
[1mStep[0m  [66/339], [94mLoss[0m : 1.76849
[1mStep[0m  [99/339], [94mLoss[0m : 1.86070
[1mStep[0m  [132/339], [94mLoss[0m : 2.90102
[1mStep[0m  [165/339], [94mLoss[0m : 1.99627
[1mStep[0m  [198/339], [94mLoss[0m : 1.71149
[1mStep[0m  [231/339], [94mLoss[0m : 2.06866
[1mStep[0m  [264/339], [94mLoss[0m : 1.93451
[1mStep[0m  [297/339], [94mLoss[0m : 2.86642
[1mStep[0m  [330/339], [94mLoss[0m : 2.49779

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94739
[1mStep[0m  [33/339], [94mLoss[0m : 2.15090
[1mStep[0m  [66/339], [94mLoss[0m : 1.98772
[1mStep[0m  [99/339], [94mLoss[0m : 2.11458
[1mStep[0m  [132/339], [94mLoss[0m : 2.43276
[1mStep[0m  [165/339], [94mLoss[0m : 2.32791
[1mStep[0m  [198/339], [94mLoss[0m : 1.97791
[1mStep[0m  [231/339], [94mLoss[0m : 2.02165
[1mStep[0m  [264/339], [94mLoss[0m : 1.99250
[1mStep[0m  [297/339], [94mLoss[0m : 2.03994
[1mStep[0m  [330/339], [94mLoss[0m : 2.22675

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.115, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59811
[1mStep[0m  [33/339], [94mLoss[0m : 1.84871
[1mStep[0m  [66/339], [94mLoss[0m : 2.25696
[1mStep[0m  [99/339], [94mLoss[0m : 1.91304
[1mStep[0m  [132/339], [94mLoss[0m : 2.17527
[1mStep[0m  [165/339], [94mLoss[0m : 1.82473
[1mStep[0m  [198/339], [94mLoss[0m : 1.63507
[1mStep[0m  [231/339], [94mLoss[0m : 1.93607
[1mStep[0m  [264/339], [94mLoss[0m : 2.13871
[1mStep[0m  [297/339], [94mLoss[0m : 2.30081
[1mStep[0m  [330/339], [94mLoss[0m : 1.45912

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27272
[1mStep[0m  [33/339], [94mLoss[0m : 2.29072
[1mStep[0m  [66/339], [94mLoss[0m : 1.85520
[1mStep[0m  [99/339], [94mLoss[0m : 2.52096
[1mStep[0m  [132/339], [94mLoss[0m : 2.60066
[1mStep[0m  [165/339], [94mLoss[0m : 2.44999
[1mStep[0m  [198/339], [94mLoss[0m : 2.05701
[1mStep[0m  [231/339], [94mLoss[0m : 2.16526
[1mStep[0m  [264/339], [94mLoss[0m : 2.28150
[1mStep[0m  [297/339], [94mLoss[0m : 2.59825
[1mStep[0m  [330/339], [94mLoss[0m : 2.95757

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29519
[1mStep[0m  [33/339], [94mLoss[0m : 2.12906
[1mStep[0m  [66/339], [94mLoss[0m : 1.83686
[1mStep[0m  [99/339], [94mLoss[0m : 2.05955
[1mStep[0m  [132/339], [94mLoss[0m : 2.14175
[1mStep[0m  [165/339], [94mLoss[0m : 2.23801
[1mStep[0m  [198/339], [94mLoss[0m : 2.27570
[1mStep[0m  [231/339], [94mLoss[0m : 2.33139
[1mStep[0m  [264/339], [94mLoss[0m : 2.15475
[1mStep[0m  [297/339], [94mLoss[0m : 1.82514
[1mStep[0m  [330/339], [94mLoss[0m : 1.99740

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.439, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26100
[1mStep[0m  [33/339], [94mLoss[0m : 2.14601
[1mStep[0m  [66/339], [94mLoss[0m : 2.20531
[1mStep[0m  [99/339], [94mLoss[0m : 2.13339
[1mStep[0m  [132/339], [94mLoss[0m : 1.82856
[1mStep[0m  [165/339], [94mLoss[0m : 1.92031
[1mStep[0m  [198/339], [94mLoss[0m : 2.49252
[1mStep[0m  [231/339], [94mLoss[0m : 2.53526
[1mStep[0m  [264/339], [94mLoss[0m : 2.53572
[1mStep[0m  [297/339], [94mLoss[0m : 1.51803
[1mStep[0m  [330/339], [94mLoss[0m : 2.03037

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85539
[1mStep[0m  [33/339], [94mLoss[0m : 2.01631
[1mStep[0m  [66/339], [94mLoss[0m : 1.95564
[1mStep[0m  [99/339], [94mLoss[0m : 1.74285
[1mStep[0m  [132/339], [94mLoss[0m : 1.82579
[1mStep[0m  [165/339], [94mLoss[0m : 1.65173
[1mStep[0m  [198/339], [94mLoss[0m : 2.27521
[1mStep[0m  [231/339], [94mLoss[0m : 1.89388
[1mStep[0m  [264/339], [94mLoss[0m : 1.82581
[1mStep[0m  [297/339], [94mLoss[0m : 2.21364
[1mStep[0m  [330/339], [94mLoss[0m : 1.72239

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71511
[1mStep[0m  [33/339], [94mLoss[0m : 1.95828
[1mStep[0m  [66/339], [94mLoss[0m : 1.60474
[1mStep[0m  [99/339], [94mLoss[0m : 1.73256
[1mStep[0m  [132/339], [94mLoss[0m : 1.67873
[1mStep[0m  [165/339], [94mLoss[0m : 2.34154
[1mStep[0m  [198/339], [94mLoss[0m : 1.75988
[1mStep[0m  [231/339], [94mLoss[0m : 2.34828
[1mStep[0m  [264/339], [94mLoss[0m : 2.35141
[1mStep[0m  [297/339], [94mLoss[0m : 1.62789
[1mStep[0m  [330/339], [94mLoss[0m : 1.72811

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.038, [92mTest[0m: 2.444, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77318
[1mStep[0m  [33/339], [94mLoss[0m : 1.73722
[1mStep[0m  [66/339], [94mLoss[0m : 2.01103
[1mStep[0m  [99/339], [94mLoss[0m : 1.91171
[1mStep[0m  [132/339], [94mLoss[0m : 2.58444
[1mStep[0m  [165/339], [94mLoss[0m : 2.42220
[1mStep[0m  [198/339], [94mLoss[0m : 2.87616
[1mStep[0m  [231/339], [94mLoss[0m : 2.75259
[1mStep[0m  [264/339], [94mLoss[0m : 2.25639
[1mStep[0m  [297/339], [94mLoss[0m : 1.72064
[1mStep[0m  [330/339], [94mLoss[0m : 2.49601

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94919
[1mStep[0m  [33/339], [94mLoss[0m : 1.85677
[1mStep[0m  [66/339], [94mLoss[0m : 2.70451
[1mStep[0m  [99/339], [94mLoss[0m : 1.91972
[1mStep[0m  [132/339], [94mLoss[0m : 1.84376
[1mStep[0m  [165/339], [94mLoss[0m : 1.76829
[1mStep[0m  [198/339], [94mLoss[0m : 2.22731
[1mStep[0m  [231/339], [94mLoss[0m : 2.40204
[1mStep[0m  [264/339], [94mLoss[0m : 2.61963
[1mStep[0m  [297/339], [94mLoss[0m : 1.90516
[1mStep[0m  [330/339], [94mLoss[0m : 2.41743

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.494, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28439
[1mStep[0m  [33/339], [94mLoss[0m : 1.53891
[1mStep[0m  [66/339], [94mLoss[0m : 1.92181
[1mStep[0m  [99/339], [94mLoss[0m : 2.03792
[1mStep[0m  [132/339], [94mLoss[0m : 2.60030
[1mStep[0m  [165/339], [94mLoss[0m : 1.65070
[1mStep[0m  [198/339], [94mLoss[0m : 2.42693
[1mStep[0m  [231/339], [94mLoss[0m : 2.28197
[1mStep[0m  [264/339], [94mLoss[0m : 1.70350
[1mStep[0m  [297/339], [94mLoss[0m : 1.88516
[1mStep[0m  [330/339], [94mLoss[0m : 1.63231

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.459, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90582
[1mStep[0m  [33/339], [94mLoss[0m : 1.99856
[1mStep[0m  [66/339], [94mLoss[0m : 1.99505
[1mStep[0m  [99/339], [94mLoss[0m : 1.64043
[1mStep[0m  [132/339], [94mLoss[0m : 1.71003
[1mStep[0m  [165/339], [94mLoss[0m : 2.10867
[1mStep[0m  [198/339], [94mLoss[0m : 1.43830
[1mStep[0m  [231/339], [94mLoss[0m : 2.04446
[1mStep[0m  [264/339], [94mLoss[0m : 2.22073
[1mStep[0m  [297/339], [94mLoss[0m : 2.43777
[1mStep[0m  [330/339], [94mLoss[0m : 2.29071

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.022, [92mTest[0m: 2.460, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12611
[1mStep[0m  [33/339], [94mLoss[0m : 2.36176
[1mStep[0m  [66/339], [94mLoss[0m : 1.58659
[1mStep[0m  [99/339], [94mLoss[0m : 2.29284
[1mStep[0m  [132/339], [94mLoss[0m : 2.70920
[1mStep[0m  [165/339], [94mLoss[0m : 2.60399
[1mStep[0m  [198/339], [94mLoss[0m : 2.24550
[1mStep[0m  [231/339], [94mLoss[0m : 1.74599
[1mStep[0m  [264/339], [94mLoss[0m : 2.22064
[1mStep[0m  [297/339], [94mLoss[0m : 2.01815
[1mStep[0m  [330/339], [94mLoss[0m : 2.24139

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.467
====================================

Phase 2 - Evaluation MAE:  2.4672764138837833
MAE score P1       2.348828
MAE score P2       2.467276
loss               2.008259
learning_rate      0.002575
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 0, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 32, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.73809
[1mStep[0m  [2/21], [94mLoss[0m : 11.16569
[1mStep[0m  [4/21], [94mLoss[0m : 11.12697
[1mStep[0m  [6/21], [94mLoss[0m : 10.94979
[1mStep[0m  [8/21], [94mLoss[0m : 10.86964
[1mStep[0m  [10/21], [94mLoss[0m : 11.11711
[1mStep[0m  [12/21], [94mLoss[0m : 11.12804
[1mStep[0m  [14/21], [94mLoss[0m : 10.91098
[1mStep[0m  [16/21], [94mLoss[0m : 10.93651
[1mStep[0m  [18/21], [94mLoss[0m : 10.89486
[1mStep[0m  [20/21], [94mLoss[0m : 10.89610

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.979, [92mTest[0m: 10.927, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.95705
[1mStep[0m  [2/21], [94mLoss[0m : 10.77128
[1mStep[0m  [4/21], [94mLoss[0m : 10.92231
[1mStep[0m  [6/21], [94mLoss[0m : 11.11540
[1mStep[0m  [8/21], [94mLoss[0m : 10.92528
[1mStep[0m  [10/21], [94mLoss[0m : 11.01450
[1mStep[0m  [12/21], [94mLoss[0m : 10.91972
[1mStep[0m  [14/21], [94mLoss[0m : 10.94327
[1mStep[0m  [16/21], [94mLoss[0m : 10.79622
[1mStep[0m  [18/21], [94mLoss[0m : 11.09744
[1mStep[0m  [20/21], [94mLoss[0m : 10.95366

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.951, [92mTest[0m: 10.912, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99531
[1mStep[0m  [2/21], [94mLoss[0m : 10.96330
[1mStep[0m  [4/21], [94mLoss[0m : 10.99087
[1mStep[0m  [6/21], [94mLoss[0m : 10.83242
[1mStep[0m  [8/21], [94mLoss[0m : 10.99171
[1mStep[0m  [10/21], [94mLoss[0m : 11.18990
[1mStep[0m  [12/21], [94mLoss[0m : 10.72532
[1mStep[0m  [14/21], [94mLoss[0m : 10.98793
[1mStep[0m  [16/21], [94mLoss[0m : 10.78294
[1mStep[0m  [18/21], [94mLoss[0m : 10.90386
[1mStep[0m  [20/21], [94mLoss[0m : 10.99635

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.928, [92mTest[0m: 10.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.89962
[1mStep[0m  [2/21], [94mLoss[0m : 10.92905
[1mStep[0m  [4/21], [94mLoss[0m : 10.93517
[1mStep[0m  [6/21], [94mLoss[0m : 10.94628
[1mStep[0m  [8/21], [94mLoss[0m : 10.87852
[1mStep[0m  [10/21], [94mLoss[0m : 10.64045
[1mStep[0m  [12/21], [94mLoss[0m : 10.95073
[1mStep[0m  [14/21], [94mLoss[0m : 11.11979
[1mStep[0m  [16/21], [94mLoss[0m : 11.06845
[1mStep[0m  [18/21], [94mLoss[0m : 10.94622
[1mStep[0m  [20/21], [94mLoss[0m : 10.83052

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.908, [92mTest[0m: 10.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.87636
[1mStep[0m  [2/21], [94mLoss[0m : 10.82662
[1mStep[0m  [4/21], [94mLoss[0m : 10.81398
[1mStep[0m  [6/21], [94mLoss[0m : 10.90840
[1mStep[0m  [8/21], [94mLoss[0m : 10.51005
[1mStep[0m  [10/21], [94mLoss[0m : 10.97510
[1mStep[0m  [12/21], [94mLoss[0m : 10.74327
[1mStep[0m  [14/21], [94mLoss[0m : 11.05557
[1mStep[0m  [16/21], [94mLoss[0m : 10.96968
[1mStep[0m  [18/21], [94mLoss[0m : 10.78293
[1mStep[0m  [20/21], [94mLoss[0m : 10.87141

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.882, [92mTest[0m: 10.819, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.18249
[1mStep[0m  [2/21], [94mLoss[0m : 10.87594
[1mStep[0m  [4/21], [94mLoss[0m : 10.79085
[1mStep[0m  [6/21], [94mLoss[0m : 11.43745
[1mStep[0m  [8/21], [94mLoss[0m : 10.78724
[1mStep[0m  [10/21], [94mLoss[0m : 11.02365
[1mStep[0m  [12/21], [94mLoss[0m : 10.85048
[1mStep[0m  [14/21], [94mLoss[0m : 10.54159
[1mStep[0m  [16/21], [94mLoss[0m : 10.68365
[1mStep[0m  [18/21], [94mLoss[0m : 10.74246
[1mStep[0m  [20/21], [94mLoss[0m : 10.85848

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.864, [92mTest[0m: 10.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71642
[1mStep[0m  [2/21], [94mLoss[0m : 10.64587
[1mStep[0m  [4/21], [94mLoss[0m : 10.70637
[1mStep[0m  [6/21], [94mLoss[0m : 10.83742
[1mStep[0m  [8/21], [94mLoss[0m : 10.75712
[1mStep[0m  [10/21], [94mLoss[0m : 10.84995
[1mStep[0m  [12/21], [94mLoss[0m : 10.82848
[1mStep[0m  [14/21], [94mLoss[0m : 10.97509
[1mStep[0m  [16/21], [94mLoss[0m : 10.81532
[1mStep[0m  [18/21], [94mLoss[0m : 10.96327
[1mStep[0m  [20/21], [94mLoss[0m : 10.77166

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.824, [92mTest[0m: 10.752, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47960
[1mStep[0m  [2/21], [94mLoss[0m : 10.92108
[1mStep[0m  [4/21], [94mLoss[0m : 10.98907
[1mStep[0m  [6/21], [94mLoss[0m : 10.70353
[1mStep[0m  [8/21], [94mLoss[0m : 10.97297
[1mStep[0m  [10/21], [94mLoss[0m : 10.81605
[1mStep[0m  [12/21], [94mLoss[0m : 11.02013
[1mStep[0m  [14/21], [94mLoss[0m : 10.74114
[1mStep[0m  [16/21], [94mLoss[0m : 11.02743
[1mStep[0m  [18/21], [94mLoss[0m : 10.68075
[1mStep[0m  [20/21], [94mLoss[0m : 10.85331

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.797, [92mTest[0m: 10.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63410
[1mStep[0m  [2/21], [94mLoss[0m : 10.63072
[1mStep[0m  [4/21], [94mLoss[0m : 11.04697
[1mStep[0m  [6/21], [94mLoss[0m : 10.89854
[1mStep[0m  [8/21], [94mLoss[0m : 10.70589
[1mStep[0m  [10/21], [94mLoss[0m : 10.72032
[1mStep[0m  [12/21], [94mLoss[0m : 10.60577
[1mStep[0m  [14/21], [94mLoss[0m : 10.69879
[1mStep[0m  [16/21], [94mLoss[0m : 10.53848
[1mStep[0m  [18/21], [94mLoss[0m : 10.97349
[1mStep[0m  [20/21], [94mLoss[0m : 10.66905

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.776, [92mTest[0m: 10.699, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90553
[1mStep[0m  [2/21], [94mLoss[0m : 10.49839
[1mStep[0m  [4/21], [94mLoss[0m : 10.79136
[1mStep[0m  [6/21], [94mLoss[0m : 10.67154
[1mStep[0m  [8/21], [94mLoss[0m : 10.74440
[1mStep[0m  [10/21], [94mLoss[0m : 10.46372
[1mStep[0m  [12/21], [94mLoss[0m : 10.45390
[1mStep[0m  [14/21], [94mLoss[0m : 11.05673
[1mStep[0m  [16/21], [94mLoss[0m : 10.78606
[1mStep[0m  [18/21], [94mLoss[0m : 10.73003
[1mStep[0m  [20/21], [94mLoss[0m : 10.99357

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.752, [92mTest[0m: 10.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77445
[1mStep[0m  [2/21], [94mLoss[0m : 10.64025
[1mStep[0m  [4/21], [94mLoss[0m : 10.58778
[1mStep[0m  [6/21], [94mLoss[0m : 10.75426
[1mStep[0m  [8/21], [94mLoss[0m : 10.77282
[1mStep[0m  [10/21], [94mLoss[0m : 10.50342
[1mStep[0m  [12/21], [94mLoss[0m : 10.93373
[1mStep[0m  [14/21], [94mLoss[0m : 10.70967
[1mStep[0m  [16/21], [94mLoss[0m : 10.51671
[1mStep[0m  [18/21], [94mLoss[0m : 10.44056
[1mStep[0m  [20/21], [94mLoss[0m : 10.65091

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79915
[1mStep[0m  [2/21], [94mLoss[0m : 10.71195
[1mStep[0m  [4/21], [94mLoss[0m : 10.49384
[1mStep[0m  [6/21], [94mLoss[0m : 10.76657
[1mStep[0m  [8/21], [94mLoss[0m : 10.85077
[1mStep[0m  [10/21], [94mLoss[0m : 10.33776
[1mStep[0m  [12/21], [94mLoss[0m : 10.93301
[1mStep[0m  [14/21], [94mLoss[0m : 10.70074
[1mStep[0m  [16/21], [94mLoss[0m : 10.66678
[1mStep[0m  [18/21], [94mLoss[0m : 10.67239
[1mStep[0m  [20/21], [94mLoss[0m : 10.57085

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.699, [92mTest[0m: 10.598, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78926
[1mStep[0m  [2/21], [94mLoss[0m : 10.66088
[1mStep[0m  [4/21], [94mLoss[0m : 10.56243
[1mStep[0m  [6/21], [94mLoss[0m : 10.58885
[1mStep[0m  [8/21], [94mLoss[0m : 10.58312
[1mStep[0m  [10/21], [94mLoss[0m : 10.61597
[1mStep[0m  [12/21], [94mLoss[0m : 10.60888
[1mStep[0m  [14/21], [94mLoss[0m : 10.46635
[1mStep[0m  [16/21], [94mLoss[0m : 10.87571
[1mStep[0m  [18/21], [94mLoss[0m : 10.56254
[1mStep[0m  [20/21], [94mLoss[0m : 10.55755

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.681, [92mTest[0m: 10.572, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.64080
[1mStep[0m  [2/21], [94mLoss[0m : 10.65354
[1mStep[0m  [4/21], [94mLoss[0m : 10.79993
[1mStep[0m  [6/21], [94mLoss[0m : 10.83793
[1mStep[0m  [8/21], [94mLoss[0m : 10.72970
[1mStep[0m  [10/21], [94mLoss[0m : 10.69266
[1mStep[0m  [12/21], [94mLoss[0m : 10.76403
[1mStep[0m  [14/21], [94mLoss[0m : 10.83945
[1mStep[0m  [16/21], [94mLoss[0m : 10.36865
[1mStep[0m  [18/21], [94mLoss[0m : 10.79339
[1mStep[0m  [20/21], [94mLoss[0m : 10.77891

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.651, [92mTest[0m: 10.537, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77254
[1mStep[0m  [2/21], [94mLoss[0m : 10.66873
[1mStep[0m  [4/21], [94mLoss[0m : 10.75133
[1mStep[0m  [6/21], [94mLoss[0m : 10.42833
[1mStep[0m  [8/21], [94mLoss[0m : 10.67213
[1mStep[0m  [10/21], [94mLoss[0m : 10.62298
[1mStep[0m  [12/21], [94mLoss[0m : 10.63428
[1mStep[0m  [14/21], [94mLoss[0m : 10.68657
[1mStep[0m  [16/21], [94mLoss[0m : 10.25889
[1mStep[0m  [18/21], [94mLoss[0m : 10.51316
[1mStep[0m  [20/21], [94mLoss[0m : 10.88434

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.624, [92mTest[0m: 10.506, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.35965
[1mStep[0m  [2/21], [94mLoss[0m : 10.46175
[1mStep[0m  [4/21], [94mLoss[0m : 10.51843
[1mStep[0m  [6/21], [94mLoss[0m : 10.52797
[1mStep[0m  [8/21], [94mLoss[0m : 10.66903
[1mStep[0m  [10/21], [94mLoss[0m : 11.01614
[1mStep[0m  [12/21], [94mLoss[0m : 10.53291
[1mStep[0m  [14/21], [94mLoss[0m : 10.57327
[1mStep[0m  [16/21], [94mLoss[0m : 10.61529
[1mStep[0m  [18/21], [94mLoss[0m : 10.40046
[1mStep[0m  [20/21], [94mLoss[0m : 10.75898

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.598, [92mTest[0m: 10.490, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54631
[1mStep[0m  [2/21], [94mLoss[0m : 10.48384
[1mStep[0m  [4/21], [94mLoss[0m : 10.91642
[1mStep[0m  [6/21], [94mLoss[0m : 10.34144
[1mStep[0m  [8/21], [94mLoss[0m : 10.50515
[1mStep[0m  [10/21], [94mLoss[0m : 10.52922
[1mStep[0m  [12/21], [94mLoss[0m : 10.20057
[1mStep[0m  [14/21], [94mLoss[0m : 10.47211
[1mStep[0m  [16/21], [94mLoss[0m : 10.63890
[1mStep[0m  [18/21], [94mLoss[0m : 10.67086
[1mStep[0m  [20/21], [94mLoss[0m : 10.55651

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.575, [92mTest[0m: 10.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49082
[1mStep[0m  [2/21], [94mLoss[0m : 10.49436
[1mStep[0m  [4/21], [94mLoss[0m : 10.43009
[1mStep[0m  [6/21], [94mLoss[0m : 10.51302
[1mStep[0m  [8/21], [94mLoss[0m : 10.53416
[1mStep[0m  [10/21], [94mLoss[0m : 10.61620
[1mStep[0m  [12/21], [94mLoss[0m : 10.59358
[1mStep[0m  [14/21], [94mLoss[0m : 10.69080
[1mStep[0m  [16/21], [94mLoss[0m : 10.49287
[1mStep[0m  [18/21], [94mLoss[0m : 10.64520
[1mStep[0m  [20/21], [94mLoss[0m : 10.61655

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.550, [92mTest[0m: 10.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.34498
[1mStep[0m  [2/21], [94mLoss[0m : 10.47346
[1mStep[0m  [4/21], [94mLoss[0m : 10.89881
[1mStep[0m  [6/21], [94mLoss[0m : 10.50180
[1mStep[0m  [8/21], [94mLoss[0m : 10.64312
[1mStep[0m  [10/21], [94mLoss[0m : 10.31378
[1mStep[0m  [12/21], [94mLoss[0m : 10.53450
[1mStep[0m  [14/21], [94mLoss[0m : 10.68293
[1mStep[0m  [16/21], [94mLoss[0m : 10.79508
[1mStep[0m  [18/21], [94mLoss[0m : 10.49451
[1mStep[0m  [20/21], [94mLoss[0m : 10.76465

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.525, [92mTest[0m: 10.376, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61630
[1mStep[0m  [2/21], [94mLoss[0m : 10.45372
[1mStep[0m  [4/21], [94mLoss[0m : 10.40332
[1mStep[0m  [6/21], [94mLoss[0m : 10.65706
[1mStep[0m  [8/21], [94mLoss[0m : 10.58891
[1mStep[0m  [10/21], [94mLoss[0m : 10.41277
[1mStep[0m  [12/21], [94mLoss[0m : 10.49483
[1mStep[0m  [14/21], [94mLoss[0m : 10.33872
[1mStep[0m  [16/21], [94mLoss[0m : 10.57871
[1mStep[0m  [18/21], [94mLoss[0m : 10.64206
[1mStep[0m  [20/21], [94mLoss[0m : 10.63071

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.488, [92mTest[0m: 10.346, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63748
[1mStep[0m  [2/21], [94mLoss[0m : 10.48489
[1mStep[0m  [4/21], [94mLoss[0m : 10.47413
[1mStep[0m  [6/21], [94mLoss[0m : 10.45083
[1mStep[0m  [8/21], [94mLoss[0m : 10.29756
[1mStep[0m  [10/21], [94mLoss[0m : 10.57158
[1mStep[0m  [12/21], [94mLoss[0m : 10.54760
[1mStep[0m  [14/21], [94mLoss[0m : 10.33484
[1mStep[0m  [16/21], [94mLoss[0m : 10.38833
[1mStep[0m  [18/21], [94mLoss[0m : 10.43699
[1mStep[0m  [20/21], [94mLoss[0m : 10.24735

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.465, [92mTest[0m: 10.323, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.16300
[1mStep[0m  [2/21], [94mLoss[0m : 10.48366
[1mStep[0m  [4/21], [94mLoss[0m : 10.46937
[1mStep[0m  [6/21], [94mLoss[0m : 10.45361
[1mStep[0m  [8/21], [94mLoss[0m : 10.20427
[1mStep[0m  [10/21], [94mLoss[0m : 10.43527
[1mStep[0m  [12/21], [94mLoss[0m : 10.57643
[1mStep[0m  [14/21], [94mLoss[0m : 10.37212
[1mStep[0m  [16/21], [94mLoss[0m : 10.47613
[1mStep[0m  [18/21], [94mLoss[0m : 10.36428
[1mStep[0m  [20/21], [94mLoss[0m : 10.63559

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.451, [92mTest[0m: 10.282, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.41961
[1mStep[0m  [2/21], [94mLoss[0m : 10.25852
[1mStep[0m  [4/21], [94mLoss[0m : 10.43471
[1mStep[0m  [6/21], [94mLoss[0m : 10.46102
[1mStep[0m  [8/21], [94mLoss[0m : 10.56138
[1mStep[0m  [10/21], [94mLoss[0m : 10.42035
[1mStep[0m  [12/21], [94mLoss[0m : 10.36720
[1mStep[0m  [14/21], [94mLoss[0m : 10.35779
[1mStep[0m  [16/21], [94mLoss[0m : 10.64849
[1mStep[0m  [18/21], [94mLoss[0m : 10.34473
[1mStep[0m  [20/21], [94mLoss[0m : 10.62587

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.270, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.19088
[1mStep[0m  [2/21], [94mLoss[0m : 10.12892
[1mStep[0m  [4/21], [94mLoss[0m : 10.44410
[1mStep[0m  [6/21], [94mLoss[0m : 10.46677
[1mStep[0m  [8/21], [94mLoss[0m : 10.33488
[1mStep[0m  [10/21], [94mLoss[0m : 10.44994
[1mStep[0m  [12/21], [94mLoss[0m : 10.53319
[1mStep[0m  [14/21], [94mLoss[0m : 10.38643
[1mStep[0m  [16/21], [94mLoss[0m : 10.29658
[1mStep[0m  [18/21], [94mLoss[0m : 10.47416
[1mStep[0m  [20/21], [94mLoss[0m : 10.62499

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.403, [92mTest[0m: 10.236, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51749
[1mStep[0m  [2/21], [94mLoss[0m : 10.37598
[1mStep[0m  [4/21], [94mLoss[0m : 10.19785
[1mStep[0m  [6/21], [94mLoss[0m : 10.42737
[1mStep[0m  [8/21], [94mLoss[0m : 10.10389
[1mStep[0m  [10/21], [94mLoss[0m : 10.35610
[1mStep[0m  [12/21], [94mLoss[0m : 10.21961
[1mStep[0m  [14/21], [94mLoss[0m : 10.54323
[1mStep[0m  [16/21], [94mLoss[0m : 10.25777
[1mStep[0m  [18/21], [94mLoss[0m : 10.45783
[1mStep[0m  [20/21], [94mLoss[0m : 10.41415

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.381, [92mTest[0m: 10.219, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65718
[1mStep[0m  [2/21], [94mLoss[0m : 10.47289
[1mStep[0m  [4/21], [94mLoss[0m : 10.35036
[1mStep[0m  [6/21], [94mLoss[0m : 10.45192
[1mStep[0m  [8/21], [94mLoss[0m : 10.20612
[1mStep[0m  [10/21], [94mLoss[0m : 10.47388
[1mStep[0m  [12/21], [94mLoss[0m : 10.29888
[1mStep[0m  [14/21], [94mLoss[0m : 10.41045
[1mStep[0m  [16/21], [94mLoss[0m : 10.35627
[1mStep[0m  [18/21], [94mLoss[0m : 10.21278
[1mStep[0m  [20/21], [94mLoss[0m : 10.29583

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.358, [92mTest[0m: 10.180, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.21297
[1mStep[0m  [2/21], [94mLoss[0m : 10.35292
[1mStep[0m  [4/21], [94mLoss[0m : 10.51147
[1mStep[0m  [6/21], [94mLoss[0m : 10.36925
[1mStep[0m  [8/21], [94mLoss[0m : 10.42635
[1mStep[0m  [10/21], [94mLoss[0m : 10.38113
[1mStep[0m  [12/21], [94mLoss[0m : 10.34458
[1mStep[0m  [14/21], [94mLoss[0m : 10.37200
[1mStep[0m  [16/21], [94mLoss[0m : 10.25846
[1mStep[0m  [18/21], [94mLoss[0m : 10.35514
[1mStep[0m  [20/21], [94mLoss[0m : 10.25296

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.334, [92mTest[0m: 10.142, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.13101
[1mStep[0m  [2/21], [94mLoss[0m : 10.26160
[1mStep[0m  [4/21], [94mLoss[0m : 10.35271
[1mStep[0m  [6/21], [94mLoss[0m : 10.20717
[1mStep[0m  [8/21], [94mLoss[0m : 10.41974
[1mStep[0m  [10/21], [94mLoss[0m : 10.29025
[1mStep[0m  [12/21], [94mLoss[0m : 10.39020
[1mStep[0m  [14/21], [94mLoss[0m : 10.37896
[1mStep[0m  [16/21], [94mLoss[0m : 10.33315
[1mStep[0m  [18/21], [94mLoss[0m : 10.29693
[1mStep[0m  [20/21], [94mLoss[0m : 10.43883

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.313, [92mTest[0m: 10.112, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38728
[1mStep[0m  [2/21], [94mLoss[0m : 10.35008
[1mStep[0m  [4/21], [94mLoss[0m : 10.62109
[1mStep[0m  [6/21], [94mLoss[0m : 9.94297
[1mStep[0m  [8/21], [94mLoss[0m : 10.37360
[1mStep[0m  [10/21], [94mLoss[0m : 10.39635
[1mStep[0m  [12/21], [94mLoss[0m : 10.19124
[1mStep[0m  [14/21], [94mLoss[0m : 10.42542
[1mStep[0m  [16/21], [94mLoss[0m : 10.40404
[1mStep[0m  [18/21], [94mLoss[0m : 10.19661
[1mStep[0m  [20/21], [94mLoss[0m : 10.19934

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.285, [92mTest[0m: 10.087, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36247
[1mStep[0m  [2/21], [94mLoss[0m : 10.11775
[1mStep[0m  [4/21], [94mLoss[0m : 10.30406
[1mStep[0m  [6/21], [94mLoss[0m : 10.31350
[1mStep[0m  [8/21], [94mLoss[0m : 10.27611
[1mStep[0m  [10/21], [94mLoss[0m : 10.17485
[1mStep[0m  [12/21], [94mLoss[0m : 10.21553
[1mStep[0m  [14/21], [94mLoss[0m : 10.26326
[1mStep[0m  [16/21], [94mLoss[0m : 10.50488
[1mStep[0m  [18/21], [94mLoss[0m : 9.98130
[1mStep[0m  [20/21], [94mLoss[0m : 10.19580

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.255, [92mTest[0m: 10.058, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.027
====================================

Phase 1 - Evaluation MAE:  10.026971135820661
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.34594
[1mStep[0m  [2/21], [94mLoss[0m : 10.65413
[1mStep[0m  [4/21], [94mLoss[0m : 10.37796
[1mStep[0m  [6/21], [94mLoss[0m : 10.21601
[1mStep[0m  [8/21], [94mLoss[0m : 10.22727
[1mStep[0m  [10/21], [94mLoss[0m : 10.29818
[1mStep[0m  [12/21], [94mLoss[0m : 10.34569
[1mStep[0m  [14/21], [94mLoss[0m : 10.15436
[1mStep[0m  [16/21], [94mLoss[0m : 10.12135
[1mStep[0m  [18/21], [94mLoss[0m : 10.46574
[1mStep[0m  [20/21], [94mLoss[0m : 10.06068

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.237, [92mTest[0m: 10.016, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.93890
[1mStep[0m  [2/21], [94mLoss[0m : 10.04104
[1mStep[0m  [4/21], [94mLoss[0m : 10.21237
[1mStep[0m  [6/21], [94mLoss[0m : 10.06478
[1mStep[0m  [8/21], [94mLoss[0m : 10.24859
[1mStep[0m  [10/21], [94mLoss[0m : 10.29241
[1mStep[0m  [12/21], [94mLoss[0m : 10.25205
[1mStep[0m  [14/21], [94mLoss[0m : 9.90577
[1mStep[0m  [16/21], [94mLoss[0m : 10.32028
[1mStep[0m  [18/21], [94mLoss[0m : 10.37409
[1mStep[0m  [20/21], [94mLoss[0m : 10.29569

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.209, [92mTest[0m: 10.009, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.84704
[1mStep[0m  [2/21], [94mLoss[0m : 10.27358
[1mStep[0m  [4/21], [94mLoss[0m : 10.45489
[1mStep[0m  [6/21], [94mLoss[0m : 9.91815
[1mStep[0m  [8/21], [94mLoss[0m : 10.17372
[1mStep[0m  [10/21], [94mLoss[0m : 10.17382
[1mStep[0m  [12/21], [94mLoss[0m : 10.22929
[1mStep[0m  [14/21], [94mLoss[0m : 10.22409
[1mStep[0m  [16/21], [94mLoss[0m : 10.29317
[1mStep[0m  [18/21], [94mLoss[0m : 10.17905
[1mStep[0m  [20/21], [94mLoss[0m : 10.31961

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.173, [92mTest[0m: 9.994, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12981
[1mStep[0m  [2/21], [94mLoss[0m : 10.21799
[1mStep[0m  [4/21], [94mLoss[0m : 10.31862
[1mStep[0m  [6/21], [94mLoss[0m : 9.97481
[1mStep[0m  [8/21], [94mLoss[0m : 10.05384
[1mStep[0m  [10/21], [94mLoss[0m : 9.88396
[1mStep[0m  [12/21], [94mLoss[0m : 10.14585
[1mStep[0m  [14/21], [94mLoss[0m : 10.36193
[1mStep[0m  [16/21], [94mLoss[0m : 10.33729
[1mStep[0m  [18/21], [94mLoss[0m : 9.92519
[1mStep[0m  [20/21], [94mLoss[0m : 9.94223

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.138, [92mTest[0m: 9.944, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.31680
[1mStep[0m  [2/21], [94mLoss[0m : 10.41110
[1mStep[0m  [4/21], [94mLoss[0m : 9.98157
[1mStep[0m  [6/21], [94mLoss[0m : 10.04810
[1mStep[0m  [8/21], [94mLoss[0m : 10.32666
[1mStep[0m  [10/21], [94mLoss[0m : 9.99764
[1mStep[0m  [12/21], [94mLoss[0m : 10.11853
[1mStep[0m  [14/21], [94mLoss[0m : 9.93356
[1mStep[0m  [16/21], [94mLoss[0m : 10.19014
[1mStep[0m  [18/21], [94mLoss[0m : 9.96705
[1mStep[0m  [20/21], [94mLoss[0m : 10.54128

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.114, [92mTest[0m: 9.923, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.43784
[1mStep[0m  [2/21], [94mLoss[0m : 10.33092
[1mStep[0m  [4/21], [94mLoss[0m : 10.20253
[1mStep[0m  [6/21], [94mLoss[0m : 9.95593
[1mStep[0m  [8/21], [94mLoss[0m : 10.22828
[1mStep[0m  [10/21], [94mLoss[0m : 10.03284
[1mStep[0m  [12/21], [94mLoss[0m : 10.09082
[1mStep[0m  [14/21], [94mLoss[0m : 9.99212
[1mStep[0m  [16/21], [94mLoss[0m : 9.95893
[1mStep[0m  [18/21], [94mLoss[0m : 9.89603
[1mStep[0m  [20/21], [94mLoss[0m : 10.08633

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.088, [92mTest[0m: 9.880, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.15035
[1mStep[0m  [2/21], [94mLoss[0m : 10.22087
[1mStep[0m  [4/21], [94mLoss[0m : 9.92155
[1mStep[0m  [6/21], [94mLoss[0m : 9.99773
[1mStep[0m  [8/21], [94mLoss[0m : 10.11749
[1mStep[0m  [10/21], [94mLoss[0m : 10.18144
[1mStep[0m  [12/21], [94mLoss[0m : 10.23300
[1mStep[0m  [14/21], [94mLoss[0m : 9.92620
[1mStep[0m  [16/21], [94mLoss[0m : 10.16349
[1mStep[0m  [18/21], [94mLoss[0m : 10.29201
[1mStep[0m  [20/21], [94mLoss[0m : 9.74140

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.059, [92mTest[0m: 9.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.76463
[1mStep[0m  [2/21], [94mLoss[0m : 9.86675
[1mStep[0m  [4/21], [94mLoss[0m : 9.85694
[1mStep[0m  [6/21], [94mLoss[0m : 10.03290
[1mStep[0m  [8/21], [94mLoss[0m : 9.90101
[1mStep[0m  [10/21], [94mLoss[0m : 9.93734
[1mStep[0m  [12/21], [94mLoss[0m : 9.91951
[1mStep[0m  [14/21], [94mLoss[0m : 9.84608
[1mStep[0m  [16/21], [94mLoss[0m : 10.23233
[1mStep[0m  [18/21], [94mLoss[0m : 9.99210
[1mStep[0m  [20/21], [94mLoss[0m : 10.05147

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.018, [92mTest[0m: 9.809, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.95615
[1mStep[0m  [2/21], [94mLoss[0m : 10.03121
[1mStep[0m  [4/21], [94mLoss[0m : 10.08567
[1mStep[0m  [6/21], [94mLoss[0m : 9.64615
[1mStep[0m  [8/21], [94mLoss[0m : 9.92818
[1mStep[0m  [10/21], [94mLoss[0m : 10.30458
[1mStep[0m  [12/21], [94mLoss[0m : 10.07770
[1mStep[0m  [14/21], [94mLoss[0m : 9.93144
[1mStep[0m  [16/21], [94mLoss[0m : 9.86588
[1mStep[0m  [18/21], [94mLoss[0m : 10.45944
[1mStep[0m  [20/21], [94mLoss[0m : 10.01618

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.976, [92mTest[0m: 9.793, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.87445
[1mStep[0m  [2/21], [94mLoss[0m : 10.32279
[1mStep[0m  [4/21], [94mLoss[0m : 9.97923
[1mStep[0m  [6/21], [94mLoss[0m : 10.10644
[1mStep[0m  [8/21], [94mLoss[0m : 9.72953
[1mStep[0m  [10/21], [94mLoss[0m : 9.74868
[1mStep[0m  [12/21], [94mLoss[0m : 9.91254
[1mStep[0m  [14/21], [94mLoss[0m : 10.15703
[1mStep[0m  [16/21], [94mLoss[0m : 9.95640
[1mStep[0m  [18/21], [94mLoss[0m : 9.78120
[1mStep[0m  [20/21], [94mLoss[0m : 10.09239

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.955, [92mTest[0m: 9.740, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70421
[1mStep[0m  [2/21], [94mLoss[0m : 10.01625
[1mStep[0m  [4/21], [94mLoss[0m : 9.75478
[1mStep[0m  [6/21], [94mLoss[0m : 9.84990
[1mStep[0m  [8/21], [94mLoss[0m : 9.77105
[1mStep[0m  [10/21], [94mLoss[0m : 9.81270
[1mStep[0m  [12/21], [94mLoss[0m : 10.01875
[1mStep[0m  [14/21], [94mLoss[0m : 10.36073
[1mStep[0m  [16/21], [94mLoss[0m : 9.73704
[1mStep[0m  [18/21], [94mLoss[0m : 10.09391
[1mStep[0m  [20/21], [94mLoss[0m : 9.76961

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.929, [92mTest[0m: 9.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.93806
[1mStep[0m  [2/21], [94mLoss[0m : 9.92030
[1mStep[0m  [4/21], [94mLoss[0m : 9.75115
[1mStep[0m  [6/21], [94mLoss[0m : 9.79168
[1mStep[0m  [8/21], [94mLoss[0m : 9.98405
[1mStep[0m  [10/21], [94mLoss[0m : 9.88561
[1mStep[0m  [12/21], [94mLoss[0m : 9.53936
[1mStep[0m  [14/21], [94mLoss[0m : 9.86180
[1mStep[0m  [16/21], [94mLoss[0m : 10.07456
[1mStep[0m  [18/21], [94mLoss[0m : 9.99718
[1mStep[0m  [20/21], [94mLoss[0m : 9.99758

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.885, [92mTest[0m: 9.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.01023
[1mStep[0m  [2/21], [94mLoss[0m : 9.81907
[1mStep[0m  [4/21], [94mLoss[0m : 9.94011
[1mStep[0m  [6/21], [94mLoss[0m : 9.85114
[1mStep[0m  [8/21], [94mLoss[0m : 9.57611
[1mStep[0m  [10/21], [94mLoss[0m : 9.80865
[1mStep[0m  [12/21], [94mLoss[0m : 9.93159
[1mStep[0m  [14/21], [94mLoss[0m : 9.64086
[1mStep[0m  [16/21], [94mLoss[0m : 9.86850
[1mStep[0m  [18/21], [94mLoss[0m : 9.73592
[1mStep[0m  [20/21], [94mLoss[0m : 9.69764

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.844, [92mTest[0m: 9.674, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.01444
[1mStep[0m  [2/21], [94mLoss[0m : 9.79424
[1mStep[0m  [4/21], [94mLoss[0m : 9.92118
[1mStep[0m  [6/21], [94mLoss[0m : 9.94409
[1mStep[0m  [8/21], [94mLoss[0m : 9.65811
[1mStep[0m  [10/21], [94mLoss[0m : 9.89028
[1mStep[0m  [12/21], [94mLoss[0m : 9.81889
[1mStep[0m  [14/21], [94mLoss[0m : 9.78996
[1mStep[0m  [16/21], [94mLoss[0m : 9.79474
[1mStep[0m  [18/21], [94mLoss[0m : 9.76113
[1mStep[0m  [20/21], [94mLoss[0m : 9.60075

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.816, [92mTest[0m: 9.640, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.79145
[1mStep[0m  [2/21], [94mLoss[0m : 9.77378
[1mStep[0m  [4/21], [94mLoss[0m : 9.73907
[1mStep[0m  [6/21], [94mLoss[0m : 9.72512
[1mStep[0m  [8/21], [94mLoss[0m : 10.14249
[1mStep[0m  [10/21], [94mLoss[0m : 9.80486
[1mStep[0m  [12/21], [94mLoss[0m : 9.57921
[1mStep[0m  [14/21], [94mLoss[0m : 9.66102
[1mStep[0m  [16/21], [94mLoss[0m : 9.71654
[1mStep[0m  [18/21], [94mLoss[0m : 9.85203
[1mStep[0m  [20/21], [94mLoss[0m : 9.70043

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.770, [92mTest[0m: 9.571, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.59631
[1mStep[0m  [2/21], [94mLoss[0m : 9.63932
[1mStep[0m  [4/21], [94mLoss[0m : 9.80396
[1mStep[0m  [6/21], [94mLoss[0m : 9.54867
[1mStep[0m  [8/21], [94mLoss[0m : 9.59356
[1mStep[0m  [10/21], [94mLoss[0m : 9.67724
[1mStep[0m  [12/21], [94mLoss[0m : 9.81824
[1mStep[0m  [14/21], [94mLoss[0m : 9.64312
[1mStep[0m  [16/21], [94mLoss[0m : 9.68134
[1mStep[0m  [18/21], [94mLoss[0m : 9.97012
[1mStep[0m  [20/21], [94mLoss[0m : 9.60851

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.729, [92mTest[0m: 9.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.68638
[1mStep[0m  [2/21], [94mLoss[0m : 9.81695
[1mStep[0m  [4/21], [94mLoss[0m : 9.57921
[1mStep[0m  [6/21], [94mLoss[0m : 9.92768
[1mStep[0m  [8/21], [94mLoss[0m : 9.55781
[1mStep[0m  [10/21], [94mLoss[0m : 9.59998
[1mStep[0m  [12/21], [94mLoss[0m : 9.98080
[1mStep[0m  [14/21], [94mLoss[0m : 9.54082
[1mStep[0m  [16/21], [94mLoss[0m : 9.48623
[1mStep[0m  [18/21], [94mLoss[0m : 9.51539
[1mStep[0m  [20/21], [94mLoss[0m : 9.97068

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.703, [92mTest[0m: 9.514, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.43392
[1mStep[0m  [2/21], [94mLoss[0m : 9.76918
[1mStep[0m  [4/21], [94mLoss[0m : 9.75049
[1mStep[0m  [6/21], [94mLoss[0m : 9.90679
[1mStep[0m  [8/21], [94mLoss[0m : 9.54242
[1mStep[0m  [10/21], [94mLoss[0m : 9.63051
[1mStep[0m  [12/21], [94mLoss[0m : 9.45266
[1mStep[0m  [14/21], [94mLoss[0m : 9.68767
[1mStep[0m  [16/21], [94mLoss[0m : 9.67576
[1mStep[0m  [18/21], [94mLoss[0m : 9.60440
[1mStep[0m  [20/21], [94mLoss[0m : 9.53158

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.667, [92mTest[0m: 9.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.74210
[1mStep[0m  [2/21], [94mLoss[0m : 9.68086
[1mStep[0m  [4/21], [94mLoss[0m : 9.56866
[1mStep[0m  [6/21], [94mLoss[0m : 9.28999
[1mStep[0m  [8/21], [94mLoss[0m : 9.62231
[1mStep[0m  [10/21], [94mLoss[0m : 9.67069
[1mStep[0m  [12/21], [94mLoss[0m : 9.83249
[1mStep[0m  [14/21], [94mLoss[0m : 9.74373
[1mStep[0m  [16/21], [94mLoss[0m : 9.64833
[1mStep[0m  [18/21], [94mLoss[0m : 9.71588
[1mStep[0m  [20/21], [94mLoss[0m : 9.42445

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.630, [92mTest[0m: 9.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.71118
[1mStep[0m  [2/21], [94mLoss[0m : 9.33157
[1mStep[0m  [4/21], [94mLoss[0m : 9.53062
[1mStep[0m  [6/21], [94mLoss[0m : 9.84796
[1mStep[0m  [8/21], [94mLoss[0m : 9.36161
[1mStep[0m  [10/21], [94mLoss[0m : 9.73757
[1mStep[0m  [12/21], [94mLoss[0m : 9.89096
[1mStep[0m  [14/21], [94mLoss[0m : 9.24469
[1mStep[0m  [16/21], [94mLoss[0m : 9.56942
[1mStep[0m  [18/21], [94mLoss[0m : 9.80431
[1mStep[0m  [20/21], [94mLoss[0m : 9.38180

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.588, [92mTest[0m: 9.376, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.61135
[1mStep[0m  [2/21], [94mLoss[0m : 9.56245
[1mStep[0m  [4/21], [94mLoss[0m : 9.50675
[1mStep[0m  [6/21], [94mLoss[0m : 9.62311
[1mStep[0m  [8/21], [94mLoss[0m : 9.57084
[1mStep[0m  [10/21], [94mLoss[0m : 9.43322
[1mStep[0m  [12/21], [94mLoss[0m : 9.71395
[1mStep[0m  [14/21], [94mLoss[0m : 9.84827
[1mStep[0m  [16/21], [94mLoss[0m : 9.70089
[1mStep[0m  [18/21], [94mLoss[0m : 9.40388
[1mStep[0m  [20/21], [94mLoss[0m : 9.57203

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.554, [92mTest[0m: 9.331, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.46473
[1mStep[0m  [2/21], [94mLoss[0m : 9.79820
[1mStep[0m  [4/21], [94mLoss[0m : 9.33522
[1mStep[0m  [6/21], [94mLoss[0m : 9.53953
[1mStep[0m  [8/21], [94mLoss[0m : 9.35964
[1mStep[0m  [10/21], [94mLoss[0m : 9.47620
[1mStep[0m  [12/21], [94mLoss[0m : 9.86319
[1mStep[0m  [14/21], [94mLoss[0m : 9.57247
[1mStep[0m  [16/21], [94mLoss[0m : 9.62193
[1mStep[0m  [18/21], [94mLoss[0m : 9.41267
[1mStep[0m  [20/21], [94mLoss[0m : 9.55274

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.519, [92mTest[0m: 9.289, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.36901
[1mStep[0m  [2/21], [94mLoss[0m : 9.77169
[1mStep[0m  [4/21], [94mLoss[0m : 9.45869
[1mStep[0m  [6/21], [94mLoss[0m : 9.67234
[1mStep[0m  [8/21], [94mLoss[0m : 9.51396
[1mStep[0m  [10/21], [94mLoss[0m : 9.48442
[1mStep[0m  [12/21], [94mLoss[0m : 9.53522
[1mStep[0m  [14/21], [94mLoss[0m : 9.60803
[1mStep[0m  [16/21], [94mLoss[0m : 9.56131
[1mStep[0m  [18/21], [94mLoss[0m : 9.34159
[1mStep[0m  [20/21], [94mLoss[0m : 9.41365

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.485, [92mTest[0m: 9.268, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.38688
[1mStep[0m  [2/21], [94mLoss[0m : 9.32166
[1mStep[0m  [4/21], [94mLoss[0m : 9.49916
[1mStep[0m  [6/21], [94mLoss[0m : 9.46631
[1mStep[0m  [8/21], [94mLoss[0m : 9.45670
[1mStep[0m  [10/21], [94mLoss[0m : 9.41891
[1mStep[0m  [12/21], [94mLoss[0m : 9.35588
[1mStep[0m  [14/21], [94mLoss[0m : 9.31671
[1mStep[0m  [16/21], [94mLoss[0m : 9.32189
[1mStep[0m  [18/21], [94mLoss[0m : 9.40325
[1mStep[0m  [20/21], [94mLoss[0m : 9.52804

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.435, [92mTest[0m: 9.174, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.24504
[1mStep[0m  [2/21], [94mLoss[0m : 9.54158
[1mStep[0m  [4/21], [94mLoss[0m : 9.36063
[1mStep[0m  [6/21], [94mLoss[0m : 9.49904
[1mStep[0m  [8/21], [94mLoss[0m : 9.22654
[1mStep[0m  [10/21], [94mLoss[0m : 9.62389
[1mStep[0m  [12/21], [94mLoss[0m : 9.31062
[1mStep[0m  [14/21], [94mLoss[0m : 9.51671
[1mStep[0m  [16/21], [94mLoss[0m : 9.19911
[1mStep[0m  [18/21], [94mLoss[0m : 9.04606
[1mStep[0m  [20/21], [94mLoss[0m : 9.68119

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.406, [92mTest[0m: 9.155, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.66029
[1mStep[0m  [2/21], [94mLoss[0m : 9.34224
[1mStep[0m  [4/21], [94mLoss[0m : 9.32650
[1mStep[0m  [6/21], [94mLoss[0m : 9.27783
[1mStep[0m  [8/21], [94mLoss[0m : 9.37058
[1mStep[0m  [10/21], [94mLoss[0m : 9.15502
[1mStep[0m  [12/21], [94mLoss[0m : 9.26789
[1mStep[0m  [14/21], [94mLoss[0m : 9.53514
[1mStep[0m  [16/21], [94mLoss[0m : 9.42466
[1mStep[0m  [18/21], [94mLoss[0m : 9.34654
[1mStep[0m  [20/21], [94mLoss[0m : 9.25212

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.369, [92mTest[0m: 9.136, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.29721
[1mStep[0m  [2/21], [94mLoss[0m : 9.55805
[1mStep[0m  [4/21], [94mLoss[0m : 9.49079
[1mStep[0m  [6/21], [94mLoss[0m : 9.12933
[1mStep[0m  [8/21], [94mLoss[0m : 9.24251
[1mStep[0m  [10/21], [94mLoss[0m : 9.43224
[1mStep[0m  [12/21], [94mLoss[0m : 9.27294
[1mStep[0m  [14/21], [94mLoss[0m : 9.37784
[1mStep[0m  [16/21], [94mLoss[0m : 9.30307
[1mStep[0m  [18/21], [94mLoss[0m : 9.33226
[1mStep[0m  [20/21], [94mLoss[0m : 9.32641

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.337, [92mTest[0m: 9.096, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.36202
[1mStep[0m  [2/21], [94mLoss[0m : 9.39656
[1mStep[0m  [4/21], [94mLoss[0m : 9.87615
[1mStep[0m  [6/21], [94mLoss[0m : 9.52914
[1mStep[0m  [8/21], [94mLoss[0m : 8.91313
[1mStep[0m  [10/21], [94mLoss[0m : 9.38044
[1mStep[0m  [12/21], [94mLoss[0m : 9.19110
[1mStep[0m  [14/21], [94mLoss[0m : 9.34761
[1mStep[0m  [16/21], [94mLoss[0m : 9.33335
[1mStep[0m  [18/21], [94mLoss[0m : 9.21606
[1mStep[0m  [20/21], [94mLoss[0m : 9.15434

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.293, [92mTest[0m: 8.991, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.43314
[1mStep[0m  [2/21], [94mLoss[0m : 9.15977
[1mStep[0m  [4/21], [94mLoss[0m : 9.47416
[1mStep[0m  [6/21], [94mLoss[0m : 9.42271
[1mStep[0m  [8/21], [94mLoss[0m : 9.49475
[1mStep[0m  [10/21], [94mLoss[0m : 9.20552
[1mStep[0m  [12/21], [94mLoss[0m : 9.32788
[1mStep[0m  [14/21], [94mLoss[0m : 9.46854
[1mStep[0m  [16/21], [94mLoss[0m : 9.24387
[1mStep[0m  [18/21], [94mLoss[0m : 9.39153
[1mStep[0m  [20/21], [94mLoss[0m : 9.17386

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.250, [92mTest[0m: 8.993, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.79385
[1mStep[0m  [2/21], [94mLoss[0m : 9.24483
[1mStep[0m  [4/21], [94mLoss[0m : 9.33630
[1mStep[0m  [6/21], [94mLoss[0m : 9.22817
[1mStep[0m  [8/21], [94mLoss[0m : 9.02725
[1mStep[0m  [10/21], [94mLoss[0m : 9.08197
[1mStep[0m  [12/21], [94mLoss[0m : 9.43752
[1mStep[0m  [14/21], [94mLoss[0m : 9.20465
[1mStep[0m  [16/21], [94mLoss[0m : 9.05972
[1mStep[0m  [18/21], [94mLoss[0m : 9.34787
[1mStep[0m  [20/21], [94mLoss[0m : 9.42244

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.220, [92mTest[0m: 8.933, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.881
====================================

Phase 2 - Evaluation MAE:  8.881331171308245
MAE score P1      10.026971
MAE score P2       8.881331
loss               9.220375
learning_rate        0.0001
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.9
weight_decay           0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 11.55687
[1mStep[0m  [16/169], [94mLoss[0m : 3.83147
[1mStep[0m  [32/169], [94mLoss[0m : 2.62231
[1mStep[0m  [48/169], [94mLoss[0m : 3.12315
[1mStep[0m  [64/169], [94mLoss[0m : 2.71195
[1mStep[0m  [80/169], [94mLoss[0m : 2.52052
[1mStep[0m  [96/169], [94mLoss[0m : 2.40054
[1mStep[0m  [112/169], [94mLoss[0m : 2.89259
[1mStep[0m  [128/169], [94mLoss[0m : 2.66304
[1mStep[0m  [144/169], [94mLoss[0m : 2.37274
[1mStep[0m  [160/169], [94mLoss[0m : 2.70119

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.995, [92mTest[0m: 11.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48697
[1mStep[0m  [16/169], [94mLoss[0m : 1.87444
[1mStep[0m  [32/169], [94mLoss[0m : 2.20556
[1mStep[0m  [48/169], [94mLoss[0m : 2.32373
[1mStep[0m  [64/169], [94mLoss[0m : 2.28554
[1mStep[0m  [80/169], [94mLoss[0m : 2.47282
[1mStep[0m  [96/169], [94mLoss[0m : 2.62611
[1mStep[0m  [112/169], [94mLoss[0m : 2.54138
[1mStep[0m  [128/169], [94mLoss[0m : 2.10099
[1mStep[0m  [144/169], [94mLoss[0m : 2.54078
[1mStep[0m  [160/169], [94mLoss[0m : 2.33329

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69663
[1mStep[0m  [16/169], [94mLoss[0m : 2.71920
[1mStep[0m  [32/169], [94mLoss[0m : 2.78683
[1mStep[0m  [48/169], [94mLoss[0m : 2.45678
[1mStep[0m  [64/169], [94mLoss[0m : 2.25242
[1mStep[0m  [80/169], [94mLoss[0m : 2.15829
[1mStep[0m  [96/169], [94mLoss[0m : 2.32900
[1mStep[0m  [112/169], [94mLoss[0m : 2.33214
[1mStep[0m  [128/169], [94mLoss[0m : 2.67712
[1mStep[0m  [144/169], [94mLoss[0m : 2.45980
[1mStep[0m  [160/169], [94mLoss[0m : 2.55431

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66404
[1mStep[0m  [16/169], [94mLoss[0m : 1.92133
[1mStep[0m  [32/169], [94mLoss[0m : 2.54236
[1mStep[0m  [48/169], [94mLoss[0m : 2.20782
[1mStep[0m  [64/169], [94mLoss[0m : 2.39915
[1mStep[0m  [80/169], [94mLoss[0m : 2.69948
[1mStep[0m  [96/169], [94mLoss[0m : 2.32490
[1mStep[0m  [112/169], [94mLoss[0m : 2.27606
[1mStep[0m  [128/169], [94mLoss[0m : 2.26555
[1mStep[0m  [144/169], [94mLoss[0m : 2.37604
[1mStep[0m  [160/169], [94mLoss[0m : 2.28058

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41315
[1mStep[0m  [16/169], [94mLoss[0m : 2.39068
[1mStep[0m  [32/169], [94mLoss[0m : 2.40605
[1mStep[0m  [48/169], [94mLoss[0m : 2.35256
[1mStep[0m  [64/169], [94mLoss[0m : 2.43732
[1mStep[0m  [80/169], [94mLoss[0m : 2.75180
[1mStep[0m  [96/169], [94mLoss[0m : 2.11778
[1mStep[0m  [112/169], [94mLoss[0m : 2.69674
[1mStep[0m  [128/169], [94mLoss[0m : 2.19275
[1mStep[0m  [144/169], [94mLoss[0m : 2.43191
[1mStep[0m  [160/169], [94mLoss[0m : 2.17174

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47935
[1mStep[0m  [16/169], [94mLoss[0m : 2.77316
[1mStep[0m  [32/169], [94mLoss[0m : 2.52473
[1mStep[0m  [48/169], [94mLoss[0m : 2.23088
[1mStep[0m  [64/169], [94mLoss[0m : 2.54504
[1mStep[0m  [80/169], [94mLoss[0m : 2.20391
[1mStep[0m  [96/169], [94mLoss[0m : 2.55623
[1mStep[0m  [112/169], [94mLoss[0m : 2.24593
[1mStep[0m  [128/169], [94mLoss[0m : 2.70443
[1mStep[0m  [144/169], [94mLoss[0m : 2.60780
[1mStep[0m  [160/169], [94mLoss[0m : 2.62144

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50600
[1mStep[0m  [16/169], [94mLoss[0m : 2.15941
[1mStep[0m  [32/169], [94mLoss[0m : 2.52747
[1mStep[0m  [48/169], [94mLoss[0m : 2.34464
[1mStep[0m  [64/169], [94mLoss[0m : 1.99893
[1mStep[0m  [80/169], [94mLoss[0m : 2.75007
[1mStep[0m  [96/169], [94mLoss[0m : 3.08630
[1mStep[0m  [112/169], [94mLoss[0m : 2.01475
[1mStep[0m  [128/169], [94mLoss[0m : 2.42274
[1mStep[0m  [144/169], [94mLoss[0m : 2.13140
[1mStep[0m  [160/169], [94mLoss[0m : 2.70231

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.18326
[1mStep[0m  [16/169], [94mLoss[0m : 2.44558
[1mStep[0m  [32/169], [94mLoss[0m : 2.53263
[1mStep[0m  [48/169], [94mLoss[0m : 2.29337
[1mStep[0m  [64/169], [94mLoss[0m : 2.14546
[1mStep[0m  [80/169], [94mLoss[0m : 2.38338
[1mStep[0m  [96/169], [94mLoss[0m : 2.76921
[1mStep[0m  [112/169], [94mLoss[0m : 2.62415
[1mStep[0m  [128/169], [94mLoss[0m : 2.24104
[1mStep[0m  [144/169], [94mLoss[0m : 2.64442
[1mStep[0m  [160/169], [94mLoss[0m : 2.36061

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53484
[1mStep[0m  [16/169], [94mLoss[0m : 2.20857
[1mStep[0m  [32/169], [94mLoss[0m : 2.15839
[1mStep[0m  [48/169], [94mLoss[0m : 2.80886
[1mStep[0m  [64/169], [94mLoss[0m : 2.12620
[1mStep[0m  [80/169], [94mLoss[0m : 2.44540
[1mStep[0m  [96/169], [94mLoss[0m : 2.36665
[1mStep[0m  [112/169], [94mLoss[0m : 2.35776
[1mStep[0m  [128/169], [94mLoss[0m : 2.42313
[1mStep[0m  [144/169], [94mLoss[0m : 2.52742
[1mStep[0m  [160/169], [94mLoss[0m : 2.76773

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.323, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54158
[1mStep[0m  [16/169], [94mLoss[0m : 2.48895
[1mStep[0m  [32/169], [94mLoss[0m : 2.87801
[1mStep[0m  [48/169], [94mLoss[0m : 2.65999
[1mStep[0m  [64/169], [94mLoss[0m : 2.55103
[1mStep[0m  [80/169], [94mLoss[0m : 2.18791
[1mStep[0m  [96/169], [94mLoss[0m : 2.89308
[1mStep[0m  [112/169], [94mLoss[0m : 3.05048
[1mStep[0m  [128/169], [94mLoss[0m : 2.64207
[1mStep[0m  [144/169], [94mLoss[0m : 2.58448
[1mStep[0m  [160/169], [94mLoss[0m : 2.18597

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13218
[1mStep[0m  [16/169], [94mLoss[0m : 2.34258
[1mStep[0m  [32/169], [94mLoss[0m : 2.49102
[1mStep[0m  [48/169], [94mLoss[0m : 2.49513
[1mStep[0m  [64/169], [94mLoss[0m : 2.22287
[1mStep[0m  [80/169], [94mLoss[0m : 2.82702
[1mStep[0m  [96/169], [94mLoss[0m : 2.57299
[1mStep[0m  [112/169], [94mLoss[0m : 2.31779
[1mStep[0m  [128/169], [94mLoss[0m : 2.53944
[1mStep[0m  [144/169], [94mLoss[0m : 2.19316
[1mStep[0m  [160/169], [94mLoss[0m : 2.52604

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07589
[1mStep[0m  [16/169], [94mLoss[0m : 2.57687
[1mStep[0m  [32/169], [94mLoss[0m : 2.55176
[1mStep[0m  [48/169], [94mLoss[0m : 2.34566
[1mStep[0m  [64/169], [94mLoss[0m : 2.45142
[1mStep[0m  [80/169], [94mLoss[0m : 2.58352
[1mStep[0m  [96/169], [94mLoss[0m : 2.51189
[1mStep[0m  [112/169], [94mLoss[0m : 2.24078
[1mStep[0m  [128/169], [94mLoss[0m : 2.64391
[1mStep[0m  [144/169], [94mLoss[0m : 2.37407
[1mStep[0m  [160/169], [94mLoss[0m : 2.49333

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67452
[1mStep[0m  [16/169], [94mLoss[0m : 2.65030
[1mStep[0m  [32/169], [94mLoss[0m : 2.36887
[1mStep[0m  [48/169], [94mLoss[0m : 2.32707
[1mStep[0m  [64/169], [94mLoss[0m : 2.47959
[1mStep[0m  [80/169], [94mLoss[0m : 2.49629
[1mStep[0m  [96/169], [94mLoss[0m : 2.14060
[1mStep[0m  [112/169], [94mLoss[0m : 2.13141
[1mStep[0m  [128/169], [94mLoss[0m : 2.81263
[1mStep[0m  [144/169], [94mLoss[0m : 2.44503
[1mStep[0m  [160/169], [94mLoss[0m : 2.41055

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68907
[1mStep[0m  [16/169], [94mLoss[0m : 1.95758
[1mStep[0m  [32/169], [94mLoss[0m : 2.74917
[1mStep[0m  [48/169], [94mLoss[0m : 2.16968
[1mStep[0m  [64/169], [94mLoss[0m : 2.39518
[1mStep[0m  [80/169], [94mLoss[0m : 2.36026
[1mStep[0m  [96/169], [94mLoss[0m : 2.52987
[1mStep[0m  [112/169], [94mLoss[0m : 2.25407
[1mStep[0m  [128/169], [94mLoss[0m : 2.47349
[1mStep[0m  [144/169], [94mLoss[0m : 2.18912
[1mStep[0m  [160/169], [94mLoss[0m : 2.11784

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63055
[1mStep[0m  [16/169], [94mLoss[0m : 2.11191
[1mStep[0m  [32/169], [94mLoss[0m : 2.34632
[1mStep[0m  [48/169], [94mLoss[0m : 2.40655
[1mStep[0m  [64/169], [94mLoss[0m : 2.80445
[1mStep[0m  [80/169], [94mLoss[0m : 2.38651
[1mStep[0m  [96/169], [94mLoss[0m : 2.37557
[1mStep[0m  [112/169], [94mLoss[0m : 2.47763
[1mStep[0m  [128/169], [94mLoss[0m : 2.97978
[1mStep[0m  [144/169], [94mLoss[0m : 2.14876
[1mStep[0m  [160/169], [94mLoss[0m : 2.93071

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.321, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35585
[1mStep[0m  [16/169], [94mLoss[0m : 2.17891
[1mStep[0m  [32/169], [94mLoss[0m : 2.51922
[1mStep[0m  [48/169], [94mLoss[0m : 2.39069
[1mStep[0m  [64/169], [94mLoss[0m : 2.68121
[1mStep[0m  [80/169], [94mLoss[0m : 2.34777
[1mStep[0m  [96/169], [94mLoss[0m : 2.11860
[1mStep[0m  [112/169], [94mLoss[0m : 2.48972
[1mStep[0m  [128/169], [94mLoss[0m : 2.32141
[1mStep[0m  [144/169], [94mLoss[0m : 2.32805
[1mStep[0m  [160/169], [94mLoss[0m : 2.43383

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38468
[1mStep[0m  [16/169], [94mLoss[0m : 2.31543
[1mStep[0m  [32/169], [94mLoss[0m : 2.27138
[1mStep[0m  [48/169], [94mLoss[0m : 2.78810
[1mStep[0m  [64/169], [94mLoss[0m : 2.20804
[1mStep[0m  [80/169], [94mLoss[0m : 2.29914
[1mStep[0m  [96/169], [94mLoss[0m : 2.17297
[1mStep[0m  [112/169], [94mLoss[0m : 2.67067
[1mStep[0m  [128/169], [94mLoss[0m : 2.11259
[1mStep[0m  [144/169], [94mLoss[0m : 2.40923
[1mStep[0m  [160/169], [94mLoss[0m : 2.69814

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25862
[1mStep[0m  [16/169], [94mLoss[0m : 2.68455
[1mStep[0m  [32/169], [94mLoss[0m : 2.48703
[1mStep[0m  [48/169], [94mLoss[0m : 2.20665
[1mStep[0m  [64/169], [94mLoss[0m : 2.51266
[1mStep[0m  [80/169], [94mLoss[0m : 2.12555
[1mStep[0m  [96/169], [94mLoss[0m : 3.00693
[1mStep[0m  [112/169], [94mLoss[0m : 2.14809
[1mStep[0m  [128/169], [94mLoss[0m : 2.64672
[1mStep[0m  [144/169], [94mLoss[0m : 2.09726
[1mStep[0m  [160/169], [94mLoss[0m : 2.27262

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82977
[1mStep[0m  [16/169], [94mLoss[0m : 2.89814
[1mStep[0m  [32/169], [94mLoss[0m : 2.35033
[1mStep[0m  [48/169], [94mLoss[0m : 2.33644
[1mStep[0m  [64/169], [94mLoss[0m : 2.54121
[1mStep[0m  [80/169], [94mLoss[0m : 2.70778
[1mStep[0m  [96/169], [94mLoss[0m : 2.47558
[1mStep[0m  [112/169], [94mLoss[0m : 2.58413
[1mStep[0m  [128/169], [94mLoss[0m : 2.12062
[1mStep[0m  [144/169], [94mLoss[0m : 2.22060
[1mStep[0m  [160/169], [94mLoss[0m : 2.34142

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86165
[1mStep[0m  [16/169], [94mLoss[0m : 2.56441
[1mStep[0m  [32/169], [94mLoss[0m : 2.13303
[1mStep[0m  [48/169], [94mLoss[0m : 2.72022
[1mStep[0m  [64/169], [94mLoss[0m : 2.25567
[1mStep[0m  [80/169], [94mLoss[0m : 2.15679
[1mStep[0m  [96/169], [94mLoss[0m : 2.59573
[1mStep[0m  [112/169], [94mLoss[0m : 2.22358
[1mStep[0m  [128/169], [94mLoss[0m : 2.32078
[1mStep[0m  [144/169], [94mLoss[0m : 2.39604
[1mStep[0m  [160/169], [94mLoss[0m : 1.99872

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17635
[1mStep[0m  [16/169], [94mLoss[0m : 2.32901
[1mStep[0m  [32/169], [94mLoss[0m : 2.53220
[1mStep[0m  [48/169], [94mLoss[0m : 2.14474
[1mStep[0m  [64/169], [94mLoss[0m : 2.01599
[1mStep[0m  [80/169], [94mLoss[0m : 2.45311
[1mStep[0m  [96/169], [94mLoss[0m : 2.68970
[1mStep[0m  [112/169], [94mLoss[0m : 2.12377
[1mStep[0m  [128/169], [94mLoss[0m : 1.91693
[1mStep[0m  [144/169], [94mLoss[0m : 2.53481
[1mStep[0m  [160/169], [94mLoss[0m : 2.75706

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19221
[1mStep[0m  [16/169], [94mLoss[0m : 2.33366
[1mStep[0m  [32/169], [94mLoss[0m : 2.52426
[1mStep[0m  [48/169], [94mLoss[0m : 2.21048
[1mStep[0m  [64/169], [94mLoss[0m : 2.56214
[1mStep[0m  [80/169], [94mLoss[0m : 2.33868
[1mStep[0m  [96/169], [94mLoss[0m : 2.34944
[1mStep[0m  [112/169], [94mLoss[0m : 2.51989
[1mStep[0m  [128/169], [94mLoss[0m : 2.73672
[1mStep[0m  [144/169], [94mLoss[0m : 2.73219
[1mStep[0m  [160/169], [94mLoss[0m : 2.36602

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79239
[1mStep[0m  [16/169], [94mLoss[0m : 2.53137
[1mStep[0m  [32/169], [94mLoss[0m : 2.62505
[1mStep[0m  [48/169], [94mLoss[0m : 2.69921
[1mStep[0m  [64/169], [94mLoss[0m : 2.24149
[1mStep[0m  [80/169], [94mLoss[0m : 2.12702
[1mStep[0m  [96/169], [94mLoss[0m : 2.42056
[1mStep[0m  [112/169], [94mLoss[0m : 2.55238
[1mStep[0m  [128/169], [94mLoss[0m : 2.55407
[1mStep[0m  [144/169], [94mLoss[0m : 2.17041
[1mStep[0m  [160/169], [94mLoss[0m : 3.03284

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38435
[1mStep[0m  [16/169], [94mLoss[0m : 2.49357
[1mStep[0m  [32/169], [94mLoss[0m : 2.60280
[1mStep[0m  [48/169], [94mLoss[0m : 3.04305
[1mStep[0m  [64/169], [94mLoss[0m : 2.13956
[1mStep[0m  [80/169], [94mLoss[0m : 2.38613
[1mStep[0m  [96/169], [94mLoss[0m : 2.17166
[1mStep[0m  [112/169], [94mLoss[0m : 2.39398
[1mStep[0m  [128/169], [94mLoss[0m : 2.54962
[1mStep[0m  [144/169], [94mLoss[0m : 2.59446
[1mStep[0m  [160/169], [94mLoss[0m : 2.32578

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34293
[1mStep[0m  [16/169], [94mLoss[0m : 2.27850
[1mStep[0m  [32/169], [94mLoss[0m : 2.61517
[1mStep[0m  [48/169], [94mLoss[0m : 2.73867
[1mStep[0m  [64/169], [94mLoss[0m : 2.09821
[1mStep[0m  [80/169], [94mLoss[0m : 2.62120
[1mStep[0m  [96/169], [94mLoss[0m : 2.71159
[1mStep[0m  [112/169], [94mLoss[0m : 2.53291
[1mStep[0m  [128/169], [94mLoss[0m : 2.25812
[1mStep[0m  [144/169], [94mLoss[0m : 2.49502
[1mStep[0m  [160/169], [94mLoss[0m : 2.39407

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49710
[1mStep[0m  [16/169], [94mLoss[0m : 2.63371
[1mStep[0m  [32/169], [94mLoss[0m : 2.44578
[1mStep[0m  [48/169], [94mLoss[0m : 2.48487
[1mStep[0m  [64/169], [94mLoss[0m : 2.50576
[1mStep[0m  [80/169], [94mLoss[0m : 2.48457
[1mStep[0m  [96/169], [94mLoss[0m : 2.14717
[1mStep[0m  [112/169], [94mLoss[0m : 2.02852
[1mStep[0m  [128/169], [94mLoss[0m : 2.58449
[1mStep[0m  [144/169], [94mLoss[0m : 2.60863
[1mStep[0m  [160/169], [94mLoss[0m : 2.41749

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56988
[1mStep[0m  [16/169], [94mLoss[0m : 2.75366
[1mStep[0m  [32/169], [94mLoss[0m : 2.78225
[1mStep[0m  [48/169], [94mLoss[0m : 2.44376
[1mStep[0m  [64/169], [94mLoss[0m : 2.54850
[1mStep[0m  [80/169], [94mLoss[0m : 2.31203
[1mStep[0m  [96/169], [94mLoss[0m : 2.10201
[1mStep[0m  [112/169], [94mLoss[0m : 2.56662
[1mStep[0m  [128/169], [94mLoss[0m : 2.32518
[1mStep[0m  [144/169], [94mLoss[0m : 2.51578
[1mStep[0m  [160/169], [94mLoss[0m : 2.58049

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17171
[1mStep[0m  [16/169], [94mLoss[0m : 2.64276
[1mStep[0m  [32/169], [94mLoss[0m : 2.37993
[1mStep[0m  [48/169], [94mLoss[0m : 2.41418
[1mStep[0m  [64/169], [94mLoss[0m : 2.35981
[1mStep[0m  [80/169], [94mLoss[0m : 2.42482
[1mStep[0m  [96/169], [94mLoss[0m : 2.45832
[1mStep[0m  [112/169], [94mLoss[0m : 2.25477
[1mStep[0m  [128/169], [94mLoss[0m : 2.34612
[1mStep[0m  [144/169], [94mLoss[0m : 2.41957
[1mStep[0m  [160/169], [94mLoss[0m : 2.63566

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80614
[1mStep[0m  [16/169], [94mLoss[0m : 2.10651
[1mStep[0m  [32/169], [94mLoss[0m : 2.54049
[1mStep[0m  [48/169], [94mLoss[0m : 2.29220
[1mStep[0m  [64/169], [94mLoss[0m : 2.33903
[1mStep[0m  [80/169], [94mLoss[0m : 1.78211
[1mStep[0m  [96/169], [94mLoss[0m : 2.38272
[1mStep[0m  [112/169], [94mLoss[0m : 1.96716
[1mStep[0m  [128/169], [94mLoss[0m : 2.55232
[1mStep[0m  [144/169], [94mLoss[0m : 2.35652
[1mStep[0m  [160/169], [94mLoss[0m : 2.57456

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39049
[1mStep[0m  [16/169], [94mLoss[0m : 2.42525
[1mStep[0m  [32/169], [94mLoss[0m : 2.62269
[1mStep[0m  [48/169], [94mLoss[0m : 2.45328
[1mStep[0m  [64/169], [94mLoss[0m : 2.54207
[1mStep[0m  [80/169], [94mLoss[0m : 2.45228
[1mStep[0m  [96/169], [94mLoss[0m : 2.30990
[1mStep[0m  [112/169], [94mLoss[0m : 2.62843
[1mStep[0m  [128/169], [94mLoss[0m : 2.52265
[1mStep[0m  [144/169], [94mLoss[0m : 2.31461
[1mStep[0m  [160/169], [94mLoss[0m : 2.64422

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.390
====================================

Phase 1 - Evaluation MAE:  2.390087344816753
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.29660
[1mStep[0m  [16/169], [94mLoss[0m : 2.18040
[1mStep[0m  [32/169], [94mLoss[0m : 2.27282
[1mStep[0m  [48/169], [94mLoss[0m : 2.67699
[1mStep[0m  [64/169], [94mLoss[0m : 2.74740
[1mStep[0m  [80/169], [94mLoss[0m : 2.27194
[1mStep[0m  [96/169], [94mLoss[0m : 2.46319
[1mStep[0m  [112/169], [94mLoss[0m : 2.40104
[1mStep[0m  [128/169], [94mLoss[0m : 2.79209
[1mStep[0m  [144/169], [94mLoss[0m : 2.50876
[1mStep[0m  [160/169], [94mLoss[0m : 2.58191

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07958
[1mStep[0m  [16/169], [94mLoss[0m : 2.27786
[1mStep[0m  [32/169], [94mLoss[0m : 2.32994
[1mStep[0m  [48/169], [94mLoss[0m : 2.75009
[1mStep[0m  [64/169], [94mLoss[0m : 2.43914
[1mStep[0m  [80/169], [94mLoss[0m : 2.59372
[1mStep[0m  [96/169], [94mLoss[0m : 2.31174
[1mStep[0m  [112/169], [94mLoss[0m : 2.30510
[1mStep[0m  [128/169], [94mLoss[0m : 2.51210
[1mStep[0m  [144/169], [94mLoss[0m : 2.30783
[1mStep[0m  [160/169], [94mLoss[0m : 2.08512

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21388
[1mStep[0m  [16/169], [94mLoss[0m : 1.87361
[1mStep[0m  [32/169], [94mLoss[0m : 2.52173
[1mStep[0m  [48/169], [94mLoss[0m : 2.49545
[1mStep[0m  [64/169], [94mLoss[0m : 2.02818
[1mStep[0m  [80/169], [94mLoss[0m : 1.85003
[1mStep[0m  [96/169], [94mLoss[0m : 2.27728
[1mStep[0m  [112/169], [94mLoss[0m : 2.22864
[1mStep[0m  [128/169], [94mLoss[0m : 2.42801
[1mStep[0m  [144/169], [94mLoss[0m : 2.37378
[1mStep[0m  [160/169], [94mLoss[0m : 2.59661

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57273
[1mStep[0m  [16/169], [94mLoss[0m : 2.15738
[1mStep[0m  [32/169], [94mLoss[0m : 2.35466
[1mStep[0m  [48/169], [94mLoss[0m : 2.16276
[1mStep[0m  [64/169], [94mLoss[0m : 2.16120
[1mStep[0m  [80/169], [94mLoss[0m : 1.96472
[1mStep[0m  [96/169], [94mLoss[0m : 2.34525
[1mStep[0m  [112/169], [94mLoss[0m : 2.19632
[1mStep[0m  [128/169], [94mLoss[0m : 2.41187
[1mStep[0m  [144/169], [94mLoss[0m : 2.66016
[1mStep[0m  [160/169], [94mLoss[0m : 2.32455

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94185
[1mStep[0m  [16/169], [94mLoss[0m : 2.05121
[1mStep[0m  [32/169], [94mLoss[0m : 2.17911
[1mStep[0m  [48/169], [94mLoss[0m : 2.26781
[1mStep[0m  [64/169], [94mLoss[0m : 2.30770
[1mStep[0m  [80/169], [94mLoss[0m : 2.17741
[1mStep[0m  [96/169], [94mLoss[0m : 1.95480
[1mStep[0m  [112/169], [94mLoss[0m : 2.06177
[1mStep[0m  [128/169], [94mLoss[0m : 2.32850
[1mStep[0m  [144/169], [94mLoss[0m : 2.36639
[1mStep[0m  [160/169], [94mLoss[0m : 2.15442

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79186
[1mStep[0m  [16/169], [94mLoss[0m : 1.98444
[1mStep[0m  [32/169], [94mLoss[0m : 2.59111
[1mStep[0m  [48/169], [94mLoss[0m : 2.14246
[1mStep[0m  [64/169], [94mLoss[0m : 2.00803
[1mStep[0m  [80/169], [94mLoss[0m : 2.13937
[1mStep[0m  [96/169], [94mLoss[0m : 2.15661
[1mStep[0m  [112/169], [94mLoss[0m : 1.92721
[1mStep[0m  [128/169], [94mLoss[0m : 2.07893
[1mStep[0m  [144/169], [94mLoss[0m : 2.04255
[1mStep[0m  [160/169], [94mLoss[0m : 2.02932

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86665
[1mStep[0m  [16/169], [94mLoss[0m : 2.06030
[1mStep[0m  [32/169], [94mLoss[0m : 1.91822
[1mStep[0m  [48/169], [94mLoss[0m : 1.90732
[1mStep[0m  [64/169], [94mLoss[0m : 2.11415
[1mStep[0m  [80/169], [94mLoss[0m : 2.05778
[1mStep[0m  [96/169], [94mLoss[0m : 1.85787
[1mStep[0m  [112/169], [94mLoss[0m : 1.93586
[1mStep[0m  [128/169], [94mLoss[0m : 2.04367
[1mStep[0m  [144/169], [94mLoss[0m : 2.22542
[1mStep[0m  [160/169], [94mLoss[0m : 2.35232

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.075, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94431
[1mStep[0m  [16/169], [94mLoss[0m : 2.16075
[1mStep[0m  [32/169], [94mLoss[0m : 2.29349
[1mStep[0m  [48/169], [94mLoss[0m : 2.25381
[1mStep[0m  [64/169], [94mLoss[0m : 2.09124
[1mStep[0m  [80/169], [94mLoss[0m : 2.29453
[1mStep[0m  [96/169], [94mLoss[0m : 1.90122
[1mStep[0m  [112/169], [94mLoss[0m : 1.97291
[1mStep[0m  [128/169], [94mLoss[0m : 2.19900
[1mStep[0m  [144/169], [94mLoss[0m : 2.07410
[1mStep[0m  [160/169], [94mLoss[0m : 2.00024

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86569
[1mStep[0m  [16/169], [94mLoss[0m : 1.77238
[1mStep[0m  [32/169], [94mLoss[0m : 1.71943
[1mStep[0m  [48/169], [94mLoss[0m : 1.82723
[1mStep[0m  [64/169], [94mLoss[0m : 1.70208
[1mStep[0m  [80/169], [94mLoss[0m : 1.92901
[1mStep[0m  [96/169], [94mLoss[0m : 1.62884
[1mStep[0m  [112/169], [94mLoss[0m : 1.78368
[1mStep[0m  [128/169], [94mLoss[0m : 1.97975
[1mStep[0m  [144/169], [94mLoss[0m : 2.12564
[1mStep[0m  [160/169], [94mLoss[0m : 3.09790

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.983, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86668
[1mStep[0m  [16/169], [94mLoss[0m : 1.41623
[1mStep[0m  [32/169], [94mLoss[0m : 1.88915
[1mStep[0m  [48/169], [94mLoss[0m : 2.26419
[1mStep[0m  [64/169], [94mLoss[0m : 1.55750
[1mStep[0m  [80/169], [94mLoss[0m : 1.89035
[1mStep[0m  [96/169], [94mLoss[0m : 2.02610
[1mStep[0m  [112/169], [94mLoss[0m : 2.04493
[1mStep[0m  [128/169], [94mLoss[0m : 1.63723
[1mStep[0m  [144/169], [94mLoss[0m : 2.14632
[1mStep[0m  [160/169], [94mLoss[0m : 2.05086

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87226
[1mStep[0m  [16/169], [94mLoss[0m : 1.92699
[1mStep[0m  [32/169], [94mLoss[0m : 1.72497
[1mStep[0m  [48/169], [94mLoss[0m : 2.08255
[1mStep[0m  [64/169], [94mLoss[0m : 2.16302
[1mStep[0m  [80/169], [94mLoss[0m : 1.57681
[1mStep[0m  [96/169], [94mLoss[0m : 2.22722
[1mStep[0m  [112/169], [94mLoss[0m : 2.00923
[1mStep[0m  [128/169], [94mLoss[0m : 1.40101
[1mStep[0m  [144/169], [94mLoss[0m : 1.68360
[1mStep[0m  [160/169], [94mLoss[0m : 2.06345

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72785
[1mStep[0m  [16/169], [94mLoss[0m : 1.85375
[1mStep[0m  [32/169], [94mLoss[0m : 2.50539
[1mStep[0m  [48/169], [94mLoss[0m : 1.75227
[1mStep[0m  [64/169], [94mLoss[0m : 2.20965
[1mStep[0m  [80/169], [94mLoss[0m : 1.41285
[1mStep[0m  [96/169], [94mLoss[0m : 2.03517
[1mStep[0m  [112/169], [94mLoss[0m : 1.96184
[1mStep[0m  [128/169], [94mLoss[0m : 1.85130
[1mStep[0m  [144/169], [94mLoss[0m : 1.86647
[1mStep[0m  [160/169], [94mLoss[0m : 1.96697

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71287
[1mStep[0m  [16/169], [94mLoss[0m : 1.83725
[1mStep[0m  [32/169], [94mLoss[0m : 1.52854
[1mStep[0m  [48/169], [94mLoss[0m : 2.35297
[1mStep[0m  [64/169], [94mLoss[0m : 1.99768
[1mStep[0m  [80/169], [94mLoss[0m : 1.96035
[1mStep[0m  [96/169], [94mLoss[0m : 2.04443
[1mStep[0m  [112/169], [94mLoss[0m : 1.68668
[1mStep[0m  [128/169], [94mLoss[0m : 2.25537
[1mStep[0m  [144/169], [94mLoss[0m : 2.05152
[1mStep[0m  [160/169], [94mLoss[0m : 1.81886

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92280
[1mStep[0m  [16/169], [94mLoss[0m : 1.66870
[1mStep[0m  [32/169], [94mLoss[0m : 1.80058
[1mStep[0m  [48/169], [94mLoss[0m : 1.79817
[1mStep[0m  [64/169], [94mLoss[0m : 1.91574
[1mStep[0m  [80/169], [94mLoss[0m : 1.81051
[1mStep[0m  [96/169], [94mLoss[0m : 1.90336
[1mStep[0m  [112/169], [94mLoss[0m : 2.11538
[1mStep[0m  [128/169], [94mLoss[0m : 2.11837
[1mStep[0m  [144/169], [94mLoss[0m : 2.03477
[1mStep[0m  [160/169], [94mLoss[0m : 1.89470

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49251
[1mStep[0m  [16/169], [94mLoss[0m : 2.02008
[1mStep[0m  [32/169], [94mLoss[0m : 2.03840
[1mStep[0m  [48/169], [94mLoss[0m : 2.04872
[1mStep[0m  [64/169], [94mLoss[0m : 1.93107
[1mStep[0m  [80/169], [94mLoss[0m : 1.75554
[1mStep[0m  [96/169], [94mLoss[0m : 1.75007
[1mStep[0m  [112/169], [94mLoss[0m : 2.05598
[1mStep[0m  [128/169], [94mLoss[0m : 2.18049
[1mStep[0m  [144/169], [94mLoss[0m : 1.89993
[1mStep[0m  [160/169], [94mLoss[0m : 1.48882

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.875, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84804
[1mStep[0m  [16/169], [94mLoss[0m : 1.72284
[1mStep[0m  [32/169], [94mLoss[0m : 1.80285
[1mStep[0m  [48/169], [94mLoss[0m : 1.78149
[1mStep[0m  [64/169], [94mLoss[0m : 1.70415
[1mStep[0m  [80/169], [94mLoss[0m : 2.37645
[1mStep[0m  [96/169], [94mLoss[0m : 1.76040
[1mStep[0m  [112/169], [94mLoss[0m : 1.85714
[1mStep[0m  [128/169], [94mLoss[0m : 2.17051
[1mStep[0m  [144/169], [94mLoss[0m : 2.02615
[1mStep[0m  [160/169], [94mLoss[0m : 1.56754

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81756
[1mStep[0m  [16/169], [94mLoss[0m : 1.73678
[1mStep[0m  [32/169], [94mLoss[0m : 2.13190
[1mStep[0m  [48/169], [94mLoss[0m : 1.80310
[1mStep[0m  [64/169], [94mLoss[0m : 2.09171
[1mStep[0m  [80/169], [94mLoss[0m : 2.01423
[1mStep[0m  [96/169], [94mLoss[0m : 1.86484
[1mStep[0m  [112/169], [94mLoss[0m : 2.11461
[1mStep[0m  [128/169], [94mLoss[0m : 2.04586
[1mStep[0m  [144/169], [94mLoss[0m : 1.73850
[1mStep[0m  [160/169], [94mLoss[0m : 1.68491

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98215
[1mStep[0m  [16/169], [94mLoss[0m : 1.74237
[1mStep[0m  [32/169], [94mLoss[0m : 1.95546
[1mStep[0m  [48/169], [94mLoss[0m : 1.78422
[1mStep[0m  [64/169], [94mLoss[0m : 1.78557
[1mStep[0m  [80/169], [94mLoss[0m : 2.02771
[1mStep[0m  [96/169], [94mLoss[0m : 1.86993
[1mStep[0m  [112/169], [94mLoss[0m : 1.82562
[1mStep[0m  [128/169], [94mLoss[0m : 1.95483
[1mStep[0m  [144/169], [94mLoss[0m : 1.39403
[1mStep[0m  [160/169], [94mLoss[0m : 1.66689

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06884
[1mStep[0m  [16/169], [94mLoss[0m : 1.99002
[1mStep[0m  [32/169], [94mLoss[0m : 1.56385
[1mStep[0m  [48/169], [94mLoss[0m : 1.92740
[1mStep[0m  [64/169], [94mLoss[0m : 1.78631
[1mStep[0m  [80/169], [94mLoss[0m : 1.92213
[1mStep[0m  [96/169], [94mLoss[0m : 1.72076
[1mStep[0m  [112/169], [94mLoss[0m : 1.64536
[1mStep[0m  [128/169], [94mLoss[0m : 1.99198
[1mStep[0m  [144/169], [94mLoss[0m : 1.78409
[1mStep[0m  [160/169], [94mLoss[0m : 1.71535

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55685
[1mStep[0m  [16/169], [94mLoss[0m : 1.69889
[1mStep[0m  [32/169], [94mLoss[0m : 1.60614
[1mStep[0m  [48/169], [94mLoss[0m : 1.49444
[1mStep[0m  [64/169], [94mLoss[0m : 1.56663
[1mStep[0m  [80/169], [94mLoss[0m : 1.77113
[1mStep[0m  [96/169], [94mLoss[0m : 1.73758
[1mStep[0m  [112/169], [94mLoss[0m : 2.22710
[1mStep[0m  [128/169], [94mLoss[0m : 2.12512
[1mStep[0m  [144/169], [94mLoss[0m : 1.68670
[1mStep[0m  [160/169], [94mLoss[0m : 1.96708

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75473
[1mStep[0m  [16/169], [94mLoss[0m : 1.68326
[1mStep[0m  [32/169], [94mLoss[0m : 1.50480
[1mStep[0m  [48/169], [94mLoss[0m : 1.85884
[1mStep[0m  [64/169], [94mLoss[0m : 1.78437
[1mStep[0m  [80/169], [94mLoss[0m : 1.71422
[1mStep[0m  [96/169], [94mLoss[0m : 1.90009
[1mStep[0m  [112/169], [94mLoss[0m : 2.05523
[1mStep[0m  [128/169], [94mLoss[0m : 1.73217
[1mStep[0m  [144/169], [94mLoss[0m : 1.83758
[1mStep[0m  [160/169], [94mLoss[0m : 1.65423

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.745, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59616
[1mStep[0m  [16/169], [94mLoss[0m : 2.16172
[1mStep[0m  [32/169], [94mLoss[0m : 1.52312
[1mStep[0m  [48/169], [94mLoss[0m : 1.57831
[1mStep[0m  [64/169], [94mLoss[0m : 2.03472
[1mStep[0m  [80/169], [94mLoss[0m : 1.80745
[1mStep[0m  [96/169], [94mLoss[0m : 1.67886
[1mStep[0m  [112/169], [94mLoss[0m : 2.06522
[1mStep[0m  [128/169], [94mLoss[0m : 1.86529
[1mStep[0m  [144/169], [94mLoss[0m : 2.03328
[1mStep[0m  [160/169], [94mLoss[0m : 1.85898

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.736, [92mTest[0m: 2.461, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48266
[1mStep[0m  [16/169], [94mLoss[0m : 1.75745
[1mStep[0m  [32/169], [94mLoss[0m : 2.05440
[1mStep[0m  [48/169], [94mLoss[0m : 2.30016
[1mStep[0m  [64/169], [94mLoss[0m : 1.55508
[1mStep[0m  [80/169], [94mLoss[0m : 1.51868
[1mStep[0m  [96/169], [94mLoss[0m : 1.57007
[1mStep[0m  [112/169], [94mLoss[0m : 2.16671
[1mStep[0m  [128/169], [94mLoss[0m : 1.51150
[1mStep[0m  [144/169], [94mLoss[0m : 1.50510
[1mStep[0m  [160/169], [94mLoss[0m : 1.82112

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.724, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61083
[1mStep[0m  [16/169], [94mLoss[0m : 1.48921
[1mStep[0m  [32/169], [94mLoss[0m : 1.62791
[1mStep[0m  [48/169], [94mLoss[0m : 1.58995
[1mStep[0m  [64/169], [94mLoss[0m : 1.76554
[1mStep[0m  [80/169], [94mLoss[0m : 1.65056
[1mStep[0m  [96/169], [94mLoss[0m : 1.89903
[1mStep[0m  [112/169], [94mLoss[0m : 1.88570
[1mStep[0m  [128/169], [94mLoss[0m : 2.15013
[1mStep[0m  [144/169], [94mLoss[0m : 2.07974
[1mStep[0m  [160/169], [94mLoss[0m : 1.57981

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45101
[1mStep[0m  [16/169], [94mLoss[0m : 1.82267
[1mStep[0m  [32/169], [94mLoss[0m : 1.29574
[1mStep[0m  [48/169], [94mLoss[0m : 1.40554
[1mStep[0m  [64/169], [94mLoss[0m : 1.56645
[1mStep[0m  [80/169], [94mLoss[0m : 1.55814
[1mStep[0m  [96/169], [94mLoss[0m : 1.70604
[1mStep[0m  [112/169], [94mLoss[0m : 1.61678
[1mStep[0m  [128/169], [94mLoss[0m : 2.00571
[1mStep[0m  [144/169], [94mLoss[0m : 1.68468
[1mStep[0m  [160/169], [94mLoss[0m : 1.62188

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66175
[1mStep[0m  [16/169], [94mLoss[0m : 1.43986
[1mStep[0m  [32/169], [94mLoss[0m : 2.02040
[1mStep[0m  [48/169], [94mLoss[0m : 1.50315
[1mStep[0m  [64/169], [94mLoss[0m : 1.90798
[1mStep[0m  [80/169], [94mLoss[0m : 1.78599
[1mStep[0m  [96/169], [94mLoss[0m : 1.50085
[1mStep[0m  [112/169], [94mLoss[0m : 1.89941
[1mStep[0m  [128/169], [94mLoss[0m : 1.67244
[1mStep[0m  [144/169], [94mLoss[0m : 1.76976
[1mStep[0m  [160/169], [94mLoss[0m : 1.59462

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.688, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50568
[1mStep[0m  [16/169], [94mLoss[0m : 1.63151
[1mStep[0m  [32/169], [94mLoss[0m : 1.59499
[1mStep[0m  [48/169], [94mLoss[0m : 1.51430
[1mStep[0m  [64/169], [94mLoss[0m : 1.89562
[1mStep[0m  [80/169], [94mLoss[0m : 1.60909
[1mStep[0m  [96/169], [94mLoss[0m : 1.66072
[1mStep[0m  [112/169], [94mLoss[0m : 1.55056
[1mStep[0m  [128/169], [94mLoss[0m : 2.08863
[1mStep[0m  [144/169], [94mLoss[0m : 2.04820
[1mStep[0m  [160/169], [94mLoss[0m : 1.46532

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.682, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66092
[1mStep[0m  [16/169], [94mLoss[0m : 1.69405
[1mStep[0m  [32/169], [94mLoss[0m : 1.90082
[1mStep[0m  [48/169], [94mLoss[0m : 1.48641
[1mStep[0m  [64/169], [94mLoss[0m : 1.43047
[1mStep[0m  [80/169], [94mLoss[0m : 1.51582
[1mStep[0m  [96/169], [94mLoss[0m : 1.60336
[1mStep[0m  [112/169], [94mLoss[0m : 1.63102
[1mStep[0m  [128/169], [94mLoss[0m : 1.69480
[1mStep[0m  [144/169], [94mLoss[0m : 1.38555
[1mStep[0m  [160/169], [94mLoss[0m : 1.63475

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.510, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72352
[1mStep[0m  [16/169], [94mLoss[0m : 1.77524
[1mStep[0m  [32/169], [94mLoss[0m : 1.92453
[1mStep[0m  [48/169], [94mLoss[0m : 1.64655
[1mStep[0m  [64/169], [94mLoss[0m : 1.73454
[1mStep[0m  [80/169], [94mLoss[0m : 1.54083
[1mStep[0m  [96/169], [94mLoss[0m : 1.95539
[1mStep[0m  [112/169], [94mLoss[0m : 2.13720
[1mStep[0m  [128/169], [94mLoss[0m : 1.94862
[1mStep[0m  [144/169], [94mLoss[0m : 2.08089
[1mStep[0m  [160/169], [94mLoss[0m : 1.66413

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.551, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.37131
[1mStep[0m  [16/169], [94mLoss[0m : 1.49999
[1mStep[0m  [32/169], [94mLoss[0m : 1.60883
[1mStep[0m  [48/169], [94mLoss[0m : 1.55435
[1mStep[0m  [64/169], [94mLoss[0m : 1.54986
[1mStep[0m  [80/169], [94mLoss[0m : 1.80214
[1mStep[0m  [96/169], [94mLoss[0m : 1.45797
[1mStep[0m  [112/169], [94mLoss[0m : 1.75312
[1mStep[0m  [128/169], [94mLoss[0m : 1.74143
[1mStep[0m  [144/169], [94mLoss[0m : 2.00703
[1mStep[0m  [160/169], [94mLoss[0m : 1.45283

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.498
====================================

Phase 2 - Evaluation MAE:  2.4983776658773422
MAE score P1      2.390087
MAE score P2      2.498378
loss              1.637024
learning_rate     0.002575
batch_size              64
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 11.01014
[1mStep[0m  [2/21], [94mLoss[0m : 11.03921
[1mStep[0m  [4/21], [94mLoss[0m : 10.70673
[1mStep[0m  [6/21], [94mLoss[0m : 10.80027
[1mStep[0m  [8/21], [94mLoss[0m : 11.05434
[1mStep[0m  [10/21], [94mLoss[0m : 11.14379
[1mStep[0m  [12/21], [94mLoss[0m : 10.85423
[1mStep[0m  [14/21], [94mLoss[0m : 10.83129
[1mStep[0m  [16/21], [94mLoss[0m : 11.04525
[1mStep[0m  [18/21], [94mLoss[0m : 10.92269
[1mStep[0m  [20/21], [94mLoss[0m : 10.63757

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.887, [92mTest[0m: 10.888, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93291
[1mStep[0m  [2/21], [94mLoss[0m : 10.92818
[1mStep[0m  [4/21], [94mLoss[0m : 10.88508
[1mStep[0m  [6/21], [94mLoss[0m : 11.01786
[1mStep[0m  [8/21], [94mLoss[0m : 10.87900
[1mStep[0m  [10/21], [94mLoss[0m : 10.88346
[1mStep[0m  [12/21], [94mLoss[0m : 10.89096
[1mStep[0m  [14/21], [94mLoss[0m : 10.86367
[1mStep[0m  [16/21], [94mLoss[0m : 10.94373
[1mStep[0m  [18/21], [94mLoss[0m : 10.78161
[1mStep[0m  [20/21], [94mLoss[0m : 11.14765

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.891, [92mTest[0m: 10.893, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62889
[1mStep[0m  [2/21], [94mLoss[0m : 10.80919
[1mStep[0m  [4/21], [94mLoss[0m : 10.91097
[1mStep[0m  [6/21], [94mLoss[0m : 10.95118
[1mStep[0m  [8/21], [94mLoss[0m : 10.97450
[1mStep[0m  [10/21], [94mLoss[0m : 10.83605
[1mStep[0m  [12/21], [94mLoss[0m : 10.85680
[1mStep[0m  [14/21], [94mLoss[0m : 10.95282
[1mStep[0m  [16/21], [94mLoss[0m : 10.77277
[1mStep[0m  [18/21], [94mLoss[0m : 10.82087
[1mStep[0m  [20/21], [94mLoss[0m : 10.89195

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.876, [92mTest[0m: 10.913, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96464
[1mStep[0m  [2/21], [94mLoss[0m : 10.79314
[1mStep[0m  [4/21], [94mLoss[0m : 10.92390
[1mStep[0m  [6/21], [94mLoss[0m : 11.07760
[1mStep[0m  [8/21], [94mLoss[0m : 11.00182
[1mStep[0m  [10/21], [94mLoss[0m : 10.89401
[1mStep[0m  [12/21], [94mLoss[0m : 11.07285
[1mStep[0m  [14/21], [94mLoss[0m : 10.71349
[1mStep[0m  [16/21], [94mLoss[0m : 10.92472
[1mStep[0m  [18/21], [94mLoss[0m : 10.73898
[1mStep[0m  [20/21], [94mLoss[0m : 10.66191

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.879, [92mTest[0m: 10.889, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65961
[1mStep[0m  [2/21], [94mLoss[0m : 10.93636
[1mStep[0m  [4/21], [94mLoss[0m : 11.02108
[1mStep[0m  [6/21], [94mLoss[0m : 10.91023
[1mStep[0m  [8/21], [94mLoss[0m : 10.79649
[1mStep[0m  [10/21], [94mLoss[0m : 10.64070
[1mStep[0m  [12/21], [94mLoss[0m : 10.67337
[1mStep[0m  [14/21], [94mLoss[0m : 10.84482
[1mStep[0m  [16/21], [94mLoss[0m : 10.74113
[1mStep[0m  [18/21], [94mLoss[0m : 11.04597
[1mStep[0m  [20/21], [94mLoss[0m : 11.11632

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.874, [92mTest[0m: 10.904, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73515
[1mStep[0m  [2/21], [94mLoss[0m : 10.87707
[1mStep[0m  [4/21], [94mLoss[0m : 10.92840
[1mStep[0m  [6/21], [94mLoss[0m : 10.96906
[1mStep[0m  [8/21], [94mLoss[0m : 10.81544
[1mStep[0m  [10/21], [94mLoss[0m : 10.84758
[1mStep[0m  [12/21], [94mLoss[0m : 10.84001
[1mStep[0m  [14/21], [94mLoss[0m : 10.81063
[1mStep[0m  [16/21], [94mLoss[0m : 10.97993
[1mStep[0m  [18/21], [94mLoss[0m : 10.72123
[1mStep[0m  [20/21], [94mLoss[0m : 11.15900

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.872, [92mTest[0m: 10.886, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86502
[1mStep[0m  [2/21], [94mLoss[0m : 10.69217
[1mStep[0m  [4/21], [94mLoss[0m : 10.65360
[1mStep[0m  [6/21], [94mLoss[0m : 10.92023
[1mStep[0m  [8/21], [94mLoss[0m : 10.80905
[1mStep[0m  [10/21], [94mLoss[0m : 11.06095
[1mStep[0m  [12/21], [94mLoss[0m : 10.82377
[1mStep[0m  [14/21], [94mLoss[0m : 10.73354
[1mStep[0m  [16/21], [94mLoss[0m : 11.01750
[1mStep[0m  [18/21], [94mLoss[0m : 10.94128
[1mStep[0m  [20/21], [94mLoss[0m : 10.96957

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96964
[1mStep[0m  [2/21], [94mLoss[0m : 10.72809
[1mStep[0m  [4/21], [94mLoss[0m : 10.61071
[1mStep[0m  [6/21], [94mLoss[0m : 10.53713
[1mStep[0m  [8/21], [94mLoss[0m : 10.92976
[1mStep[0m  [10/21], [94mLoss[0m : 10.93878
[1mStep[0m  [12/21], [94mLoss[0m : 11.00268
[1mStep[0m  [14/21], [94mLoss[0m : 11.12214
[1mStep[0m  [16/21], [94mLoss[0m : 11.05173
[1mStep[0m  [18/21], [94mLoss[0m : 11.02224
[1mStep[0m  [20/21], [94mLoss[0m : 10.77026

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.858, [92mTest[0m: 10.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.84696
[1mStep[0m  [2/21], [94mLoss[0m : 11.18997
[1mStep[0m  [4/21], [94mLoss[0m : 10.96828
[1mStep[0m  [6/21], [94mLoss[0m : 10.79295
[1mStep[0m  [8/21], [94mLoss[0m : 10.60468
[1mStep[0m  [10/21], [94mLoss[0m : 11.02478
[1mStep[0m  [12/21], [94mLoss[0m : 10.90975
[1mStep[0m  [14/21], [94mLoss[0m : 10.86941
[1mStep[0m  [16/21], [94mLoss[0m : 10.59189
[1mStep[0m  [18/21], [94mLoss[0m : 10.91402
[1mStep[0m  [20/21], [94mLoss[0m : 10.86461

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.852, [92mTest[0m: 10.866, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90825
[1mStep[0m  [2/21], [94mLoss[0m : 10.83410
[1mStep[0m  [4/21], [94mLoss[0m : 10.77138
[1mStep[0m  [6/21], [94mLoss[0m : 10.95034
[1mStep[0m  [8/21], [94mLoss[0m : 10.72296
[1mStep[0m  [10/21], [94mLoss[0m : 10.99517
[1mStep[0m  [12/21], [94mLoss[0m : 10.79537
[1mStep[0m  [14/21], [94mLoss[0m : 11.00801
[1mStep[0m  [16/21], [94mLoss[0m : 10.55753
[1mStep[0m  [18/21], [94mLoss[0m : 10.70528
[1mStep[0m  [20/21], [94mLoss[0m : 10.86276

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.848, [92mTest[0m: 10.853, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.89929
[1mStep[0m  [2/21], [94mLoss[0m : 10.93431
[1mStep[0m  [4/21], [94mLoss[0m : 10.68303
[1mStep[0m  [6/21], [94mLoss[0m : 10.81396
[1mStep[0m  [8/21], [94mLoss[0m : 11.01776
[1mStep[0m  [10/21], [94mLoss[0m : 10.70717
[1mStep[0m  [12/21], [94mLoss[0m : 10.73732
[1mStep[0m  [14/21], [94mLoss[0m : 10.84325
[1mStep[0m  [16/21], [94mLoss[0m : 11.01666
[1mStep[0m  [18/21], [94mLoss[0m : 10.74689
[1mStep[0m  [20/21], [94mLoss[0m : 10.93568

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.841, [92mTest[0m: 10.860, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72021
[1mStep[0m  [2/21], [94mLoss[0m : 10.58441
[1mStep[0m  [4/21], [94mLoss[0m : 10.60809
[1mStep[0m  [6/21], [94mLoss[0m : 10.72569
[1mStep[0m  [8/21], [94mLoss[0m : 10.64688
[1mStep[0m  [10/21], [94mLoss[0m : 10.85045
[1mStep[0m  [12/21], [94mLoss[0m : 10.81595
[1mStep[0m  [14/21], [94mLoss[0m : 10.92603
[1mStep[0m  [16/21], [94mLoss[0m : 10.75574
[1mStep[0m  [18/21], [94mLoss[0m : 10.79934
[1mStep[0m  [20/21], [94mLoss[0m : 10.89380

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.861, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.91019
[1mStep[0m  [2/21], [94mLoss[0m : 10.89859
[1mStep[0m  [4/21], [94mLoss[0m : 10.86863
[1mStep[0m  [6/21], [94mLoss[0m : 10.83788
[1mStep[0m  [8/21], [94mLoss[0m : 10.44984
[1mStep[0m  [10/21], [94mLoss[0m : 10.81228
[1mStep[0m  [12/21], [94mLoss[0m : 10.73176
[1mStep[0m  [14/21], [94mLoss[0m : 10.88894
[1mStep[0m  [16/21], [94mLoss[0m : 10.78467
[1mStep[0m  [18/21], [94mLoss[0m : 10.76869
[1mStep[0m  [20/21], [94mLoss[0m : 10.53134

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.826, [92mTest[0m: 10.845, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65036
[1mStep[0m  [2/21], [94mLoss[0m : 10.82217
[1mStep[0m  [4/21], [94mLoss[0m : 10.85817
[1mStep[0m  [6/21], [94mLoss[0m : 10.80756
[1mStep[0m  [8/21], [94mLoss[0m : 10.66901
[1mStep[0m  [10/21], [94mLoss[0m : 10.89454
[1mStep[0m  [12/21], [94mLoss[0m : 10.57336
[1mStep[0m  [14/21], [94mLoss[0m : 10.97706
[1mStep[0m  [16/21], [94mLoss[0m : 10.71079
[1mStep[0m  [18/21], [94mLoss[0m : 10.88052
[1mStep[0m  [20/21], [94mLoss[0m : 11.31300

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.839, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.92441
[1mStep[0m  [2/21], [94mLoss[0m : 10.73315
[1mStep[0m  [4/21], [94mLoss[0m : 10.96479
[1mStep[0m  [6/21], [94mLoss[0m : 10.62862
[1mStep[0m  [8/21], [94mLoss[0m : 11.06418
[1mStep[0m  [10/21], [94mLoss[0m : 10.80356
[1mStep[0m  [12/21], [94mLoss[0m : 10.88015
[1mStep[0m  [14/21], [94mLoss[0m : 10.73999
[1mStep[0m  [16/21], [94mLoss[0m : 10.90145
[1mStep[0m  [18/21], [94mLoss[0m : 10.64657
[1mStep[0m  [20/21], [94mLoss[0m : 10.66289

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.825, [92mTest[0m: 10.843, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81771
[1mStep[0m  [2/21], [94mLoss[0m : 10.95982
[1mStep[0m  [4/21], [94mLoss[0m : 10.76529
[1mStep[0m  [6/21], [94mLoss[0m : 10.72423
[1mStep[0m  [8/21], [94mLoss[0m : 10.65353
[1mStep[0m  [10/21], [94mLoss[0m : 10.95587
[1mStep[0m  [12/21], [94mLoss[0m : 10.75140
[1mStep[0m  [14/21], [94mLoss[0m : 10.77141
[1mStep[0m  [16/21], [94mLoss[0m : 11.24417
[1mStep[0m  [18/21], [94mLoss[0m : 10.87411
[1mStep[0m  [20/21], [94mLoss[0m : 10.81929

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.818, [92mTest[0m: 10.832, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96254
[1mStep[0m  [2/21], [94mLoss[0m : 11.09939
[1mStep[0m  [4/21], [94mLoss[0m : 10.51477
[1mStep[0m  [6/21], [94mLoss[0m : 10.84131
[1mStep[0m  [8/21], [94mLoss[0m : 10.82954
[1mStep[0m  [10/21], [94mLoss[0m : 10.79939
[1mStep[0m  [12/21], [94mLoss[0m : 10.44732
[1mStep[0m  [14/21], [94mLoss[0m : 10.85952
[1mStep[0m  [16/21], [94mLoss[0m : 10.94036
[1mStep[0m  [18/21], [94mLoss[0m : 11.09533
[1mStep[0m  [20/21], [94mLoss[0m : 10.65757

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.806, [92mTest[0m: 10.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79298
[1mStep[0m  [2/21], [94mLoss[0m : 10.79669
[1mStep[0m  [4/21], [94mLoss[0m : 10.78083
[1mStep[0m  [6/21], [94mLoss[0m : 10.62092
[1mStep[0m  [8/21], [94mLoss[0m : 11.00159
[1mStep[0m  [10/21], [94mLoss[0m : 10.75292
[1mStep[0m  [12/21], [94mLoss[0m : 10.71176
[1mStep[0m  [14/21], [94mLoss[0m : 10.85686
[1mStep[0m  [16/21], [94mLoss[0m : 10.77321
[1mStep[0m  [18/21], [94mLoss[0m : 10.95675
[1mStep[0m  [20/21], [94mLoss[0m : 10.96855

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.809, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.80314
[1mStep[0m  [2/21], [94mLoss[0m : 10.88936
[1mStep[0m  [4/21], [94mLoss[0m : 10.83385
[1mStep[0m  [6/21], [94mLoss[0m : 11.12340
[1mStep[0m  [8/21], [94mLoss[0m : 10.84181
[1mStep[0m  [10/21], [94mLoss[0m : 10.85250
[1mStep[0m  [12/21], [94mLoss[0m : 10.87478
[1mStep[0m  [14/21], [94mLoss[0m : 10.89581
[1mStep[0m  [16/21], [94mLoss[0m : 10.74513
[1mStep[0m  [18/21], [94mLoss[0m : 10.75266
[1mStep[0m  [20/21], [94mLoss[0m : 10.44466

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.803, [92mTest[0m: 10.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.15121
[1mStep[0m  [2/21], [94mLoss[0m : 10.62270
[1mStep[0m  [4/21], [94mLoss[0m : 10.83269
[1mStep[0m  [6/21], [94mLoss[0m : 10.92142
[1mStep[0m  [8/21], [94mLoss[0m : 10.64561
[1mStep[0m  [10/21], [94mLoss[0m : 10.68598
[1mStep[0m  [12/21], [94mLoss[0m : 10.86971
[1mStep[0m  [14/21], [94mLoss[0m : 10.52845
[1mStep[0m  [16/21], [94mLoss[0m : 11.16111
[1mStep[0m  [18/21], [94mLoss[0m : 10.77599
[1mStep[0m  [20/21], [94mLoss[0m : 11.16396

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.805, [92mTest[0m: 10.807, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88883
[1mStep[0m  [2/21], [94mLoss[0m : 11.01622
[1mStep[0m  [4/21], [94mLoss[0m : 10.49192
[1mStep[0m  [6/21], [94mLoss[0m : 10.82465
[1mStep[0m  [8/21], [94mLoss[0m : 10.90725
[1mStep[0m  [10/21], [94mLoss[0m : 10.88962
[1mStep[0m  [12/21], [94mLoss[0m : 10.61830
[1mStep[0m  [14/21], [94mLoss[0m : 10.81038
[1mStep[0m  [16/21], [94mLoss[0m : 10.55007
[1mStep[0m  [18/21], [94mLoss[0m : 10.54536
[1mStep[0m  [20/21], [94mLoss[0m : 10.86110

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.786, [92mTest[0m: 10.801, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61197
[1mStep[0m  [2/21], [94mLoss[0m : 10.86986
[1mStep[0m  [4/21], [94mLoss[0m : 10.67587
[1mStep[0m  [6/21], [94mLoss[0m : 10.84288
[1mStep[0m  [8/21], [94mLoss[0m : 10.68254
[1mStep[0m  [10/21], [94mLoss[0m : 10.73104
[1mStep[0m  [12/21], [94mLoss[0m : 10.96087
[1mStep[0m  [14/21], [94mLoss[0m : 10.69785
[1mStep[0m  [16/21], [94mLoss[0m : 10.36363
[1mStep[0m  [18/21], [94mLoss[0m : 10.52773
[1mStep[0m  [20/21], [94mLoss[0m : 10.86427

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.788, [92mTest[0m: 10.772, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.83283
[1mStep[0m  [2/21], [94mLoss[0m : 10.65495
[1mStep[0m  [4/21], [94mLoss[0m : 10.52989
[1mStep[0m  [6/21], [94mLoss[0m : 10.75450
[1mStep[0m  [8/21], [94mLoss[0m : 10.80374
[1mStep[0m  [10/21], [94mLoss[0m : 11.00709
[1mStep[0m  [12/21], [94mLoss[0m : 11.10724
[1mStep[0m  [14/21], [94mLoss[0m : 10.69323
[1mStep[0m  [16/21], [94mLoss[0m : 10.31693
[1mStep[0m  [18/21], [94mLoss[0m : 11.06347
[1mStep[0m  [20/21], [94mLoss[0m : 10.81732

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.781, [92mTest[0m: 10.771, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.06348
[1mStep[0m  [2/21], [94mLoss[0m : 10.62198
[1mStep[0m  [4/21], [94mLoss[0m : 10.74692
[1mStep[0m  [6/21], [94mLoss[0m : 10.83697
[1mStep[0m  [8/21], [94mLoss[0m : 10.90003
[1mStep[0m  [10/21], [94mLoss[0m : 11.00169
[1mStep[0m  [12/21], [94mLoss[0m : 10.66144
[1mStep[0m  [14/21], [94mLoss[0m : 10.63045
[1mStep[0m  [16/21], [94mLoss[0m : 10.73999
[1mStep[0m  [18/21], [94mLoss[0m : 10.84614
[1mStep[0m  [20/21], [94mLoss[0m : 10.69144

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.784, [92mTest[0m: 10.771, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.02811
[1mStep[0m  [2/21], [94mLoss[0m : 10.81293
[1mStep[0m  [4/21], [94mLoss[0m : 10.95879
[1mStep[0m  [6/21], [94mLoss[0m : 10.79169
[1mStep[0m  [8/21], [94mLoss[0m : 10.43998
[1mStep[0m  [10/21], [94mLoss[0m : 10.65735
[1mStep[0m  [12/21], [94mLoss[0m : 10.60451
[1mStep[0m  [14/21], [94mLoss[0m : 10.93724
[1mStep[0m  [16/21], [94mLoss[0m : 10.65315
[1mStep[0m  [18/21], [94mLoss[0m : 10.83976
[1mStep[0m  [20/21], [94mLoss[0m : 11.01297

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.777, [92mTest[0m: 10.765, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79297
[1mStep[0m  [2/21], [94mLoss[0m : 10.70543
[1mStep[0m  [4/21], [94mLoss[0m : 10.61619
[1mStep[0m  [6/21], [94mLoss[0m : 10.74433
[1mStep[0m  [8/21], [94mLoss[0m : 10.66550
[1mStep[0m  [10/21], [94mLoss[0m : 10.69195
[1mStep[0m  [12/21], [94mLoss[0m : 10.59487
[1mStep[0m  [14/21], [94mLoss[0m : 10.80593
[1mStep[0m  [16/21], [94mLoss[0m : 10.91680
[1mStep[0m  [18/21], [94mLoss[0m : 10.69608
[1mStep[0m  [20/21], [94mLoss[0m : 10.66229

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.777, [92mTest[0m: 10.771, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78078
[1mStep[0m  [2/21], [94mLoss[0m : 10.73080
[1mStep[0m  [4/21], [94mLoss[0m : 10.83601
[1mStep[0m  [6/21], [94mLoss[0m : 10.91893
[1mStep[0m  [8/21], [94mLoss[0m : 10.86974
[1mStep[0m  [10/21], [94mLoss[0m : 10.87287
[1mStep[0m  [12/21], [94mLoss[0m : 10.48706
[1mStep[0m  [14/21], [94mLoss[0m : 10.80012
[1mStep[0m  [16/21], [94mLoss[0m : 10.71821
[1mStep[0m  [18/21], [94mLoss[0m : 10.90337
[1mStep[0m  [20/21], [94mLoss[0m : 10.67685

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.774, [92mTest[0m: 10.765, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99653
[1mStep[0m  [2/21], [94mLoss[0m : 10.61641
[1mStep[0m  [4/21], [94mLoss[0m : 10.45256
[1mStep[0m  [6/21], [94mLoss[0m : 10.76613
[1mStep[0m  [8/21], [94mLoss[0m : 10.56958
[1mStep[0m  [10/21], [94mLoss[0m : 10.78052
[1mStep[0m  [12/21], [94mLoss[0m : 10.89041
[1mStep[0m  [14/21], [94mLoss[0m : 10.52957
[1mStep[0m  [16/21], [94mLoss[0m : 10.91453
[1mStep[0m  [18/21], [94mLoss[0m : 10.86223
[1mStep[0m  [20/21], [94mLoss[0m : 10.65989

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.768, [92mTest[0m: 10.750, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.76060
[1mStep[0m  [2/21], [94mLoss[0m : 10.88765
[1mStep[0m  [4/21], [94mLoss[0m : 10.50967
[1mStep[0m  [6/21], [94mLoss[0m : 10.72224
[1mStep[0m  [8/21], [94mLoss[0m : 10.81511
[1mStep[0m  [10/21], [94mLoss[0m : 10.52863
[1mStep[0m  [12/21], [94mLoss[0m : 10.81404
[1mStep[0m  [14/21], [94mLoss[0m : 10.87343
[1mStep[0m  [16/21], [94mLoss[0m : 10.75753
[1mStep[0m  [18/21], [94mLoss[0m : 10.39048
[1mStep[0m  [20/21], [94mLoss[0m : 10.96769

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.746, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86035
[1mStep[0m  [2/21], [94mLoss[0m : 10.89603
[1mStep[0m  [4/21], [94mLoss[0m : 10.68077
[1mStep[0m  [6/21], [94mLoss[0m : 10.72846
[1mStep[0m  [8/21], [94mLoss[0m : 10.88007
[1mStep[0m  [10/21], [94mLoss[0m : 10.89380
[1mStep[0m  [12/21], [94mLoss[0m : 10.70859
[1mStep[0m  [14/21], [94mLoss[0m : 10.98739
[1mStep[0m  [16/21], [94mLoss[0m : 10.46890
[1mStep[0m  [18/21], [94mLoss[0m : 10.65065
[1mStep[0m  [20/21], [94mLoss[0m : 10.60412

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.754, [92mTest[0m: 10.738, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.737
====================================

Phase 1 - Evaluation MAE:  10.736502919878278
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.83696
[1mStep[0m  [2/21], [94mLoss[0m : 10.77254
[1mStep[0m  [4/21], [94mLoss[0m : 10.69998
[1mStep[0m  [6/21], [94mLoss[0m : 10.65246
[1mStep[0m  [8/21], [94mLoss[0m : 10.89032
[1mStep[0m  [10/21], [94mLoss[0m : 10.63962
[1mStep[0m  [12/21], [94mLoss[0m : 10.93849
[1mStep[0m  [14/21], [94mLoss[0m : 10.68712
[1mStep[0m  [16/21], [94mLoss[0m : 10.76854
[1mStep[0m  [18/21], [94mLoss[0m : 11.05777
[1mStep[0m  [20/21], [94mLoss[0m : 10.84175

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.725, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71119
[1mStep[0m  [2/21], [94mLoss[0m : 10.64744
[1mStep[0m  [4/21], [94mLoss[0m : 10.67348
[1mStep[0m  [6/21], [94mLoss[0m : 10.64410
[1mStep[0m  [8/21], [94mLoss[0m : 10.90508
[1mStep[0m  [10/21], [94mLoss[0m : 10.77126
[1mStep[0m  [12/21], [94mLoss[0m : 10.90887
[1mStep[0m  [14/21], [94mLoss[0m : 10.63910
[1mStep[0m  [16/21], [94mLoss[0m : 10.54227
[1mStep[0m  [18/21], [94mLoss[0m : 10.65912
[1mStep[0m  [20/21], [94mLoss[0m : 10.75031

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.737, [92mTest[0m: 10.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93633
[1mStep[0m  [2/21], [94mLoss[0m : 10.65952
[1mStep[0m  [4/21], [94mLoss[0m : 10.57119
[1mStep[0m  [6/21], [94mLoss[0m : 10.75455
[1mStep[0m  [8/21], [94mLoss[0m : 10.89105
[1mStep[0m  [10/21], [94mLoss[0m : 10.37194
[1mStep[0m  [12/21], [94mLoss[0m : 10.73411
[1mStep[0m  [14/21], [94mLoss[0m : 10.71147
[1mStep[0m  [16/21], [94mLoss[0m : 10.51940
[1mStep[0m  [18/21], [94mLoss[0m : 10.88797
[1mStep[0m  [20/21], [94mLoss[0m : 10.59186

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.735, [92mTest[0m: 10.720, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62829
[1mStep[0m  [2/21], [94mLoss[0m : 10.94339
[1mStep[0m  [4/21], [94mLoss[0m : 10.70731
[1mStep[0m  [6/21], [94mLoss[0m : 10.51137
[1mStep[0m  [8/21], [94mLoss[0m : 10.59087
[1mStep[0m  [10/21], [94mLoss[0m : 10.58822
[1mStep[0m  [12/21], [94mLoss[0m : 10.83595
[1mStep[0m  [14/21], [94mLoss[0m : 10.59485
[1mStep[0m  [16/21], [94mLoss[0m : 10.71216
[1mStep[0m  [18/21], [94mLoss[0m : 10.76360
[1mStep[0m  [20/21], [94mLoss[0m : 10.78944

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.725, [92mTest[0m: 10.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67286
[1mStep[0m  [2/21], [94mLoss[0m : 10.81899
[1mStep[0m  [4/21], [94mLoss[0m : 10.66421
[1mStep[0m  [6/21], [94mLoss[0m : 10.77695
[1mStep[0m  [8/21], [94mLoss[0m : 10.58400
[1mStep[0m  [10/21], [94mLoss[0m : 10.52364
[1mStep[0m  [12/21], [94mLoss[0m : 10.58900
[1mStep[0m  [14/21], [94mLoss[0m : 10.84037
[1mStep[0m  [16/21], [94mLoss[0m : 10.23929
[1mStep[0m  [18/21], [94mLoss[0m : 10.69543
[1mStep[0m  [20/21], [94mLoss[0m : 10.94029

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.725, [92mTest[0m: 10.703, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70886
[1mStep[0m  [2/21], [94mLoss[0m : 10.84331
[1mStep[0m  [4/21], [94mLoss[0m : 10.57967
[1mStep[0m  [6/21], [94mLoss[0m : 10.72293
[1mStep[0m  [8/21], [94mLoss[0m : 10.69673
[1mStep[0m  [10/21], [94mLoss[0m : 10.57863
[1mStep[0m  [12/21], [94mLoss[0m : 10.72208
[1mStep[0m  [14/21], [94mLoss[0m : 10.88097
[1mStep[0m  [16/21], [94mLoss[0m : 10.86362
[1mStep[0m  [18/21], [94mLoss[0m : 10.73529
[1mStep[0m  [20/21], [94mLoss[0m : 10.38660

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.711, [92mTest[0m: 10.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.01905
[1mStep[0m  [2/21], [94mLoss[0m : 10.91359
[1mStep[0m  [4/21], [94mLoss[0m : 10.93316
[1mStep[0m  [6/21], [94mLoss[0m : 10.82856
[1mStep[0m  [8/21], [94mLoss[0m : 10.58857
[1mStep[0m  [10/21], [94mLoss[0m : 10.97554
[1mStep[0m  [12/21], [94mLoss[0m : 10.84495
[1mStep[0m  [14/21], [94mLoss[0m : 10.45942
[1mStep[0m  [16/21], [94mLoss[0m : 10.88079
[1mStep[0m  [18/21], [94mLoss[0m : 10.64019
[1mStep[0m  [20/21], [94mLoss[0m : 10.63477

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.715, [92mTest[0m: 10.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75319
[1mStep[0m  [2/21], [94mLoss[0m : 10.94793
[1mStep[0m  [4/21], [94mLoss[0m : 10.78057
[1mStep[0m  [6/21], [94mLoss[0m : 10.72784
[1mStep[0m  [8/21], [94mLoss[0m : 10.46863
[1mStep[0m  [10/21], [94mLoss[0m : 10.78507
[1mStep[0m  [12/21], [94mLoss[0m : 10.61573
[1mStep[0m  [14/21], [94mLoss[0m : 11.11810
[1mStep[0m  [16/21], [94mLoss[0m : 10.46890
[1mStep[0m  [18/21], [94mLoss[0m : 10.61627
[1mStep[0m  [20/21], [94mLoss[0m : 10.77625

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.707, [92mTest[0m: 10.684, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75859
[1mStep[0m  [2/21], [94mLoss[0m : 10.72771
[1mStep[0m  [4/21], [94mLoss[0m : 10.63879
[1mStep[0m  [6/21], [94mLoss[0m : 10.97432
[1mStep[0m  [8/21], [94mLoss[0m : 10.89015
[1mStep[0m  [10/21], [94mLoss[0m : 10.81237
[1mStep[0m  [12/21], [94mLoss[0m : 10.85615
[1mStep[0m  [14/21], [94mLoss[0m : 10.76618
[1mStep[0m  [16/21], [94mLoss[0m : 10.63384
[1mStep[0m  [18/21], [94mLoss[0m : 10.72319
[1mStep[0m  [20/21], [94mLoss[0m : 10.56579

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.699, [92mTest[0m: 10.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.57457
[1mStep[0m  [2/21], [94mLoss[0m : 11.08098
[1mStep[0m  [4/21], [94mLoss[0m : 10.54324
[1mStep[0m  [6/21], [94mLoss[0m : 10.68958
[1mStep[0m  [8/21], [94mLoss[0m : 10.88292
[1mStep[0m  [10/21], [94mLoss[0m : 10.67801
[1mStep[0m  [12/21], [94mLoss[0m : 10.62808
[1mStep[0m  [14/21], [94mLoss[0m : 10.50749
[1mStep[0m  [16/21], [94mLoss[0m : 10.56491
[1mStep[0m  [18/21], [94mLoss[0m : 10.80528
[1mStep[0m  [20/21], [94mLoss[0m : 10.75744

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.700, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63420
[1mStep[0m  [2/21], [94mLoss[0m : 10.80319
[1mStep[0m  [4/21], [94mLoss[0m : 10.52837
[1mStep[0m  [6/21], [94mLoss[0m : 10.39447
[1mStep[0m  [8/21], [94mLoss[0m : 10.70302
[1mStep[0m  [10/21], [94mLoss[0m : 10.72907
[1mStep[0m  [12/21], [94mLoss[0m : 10.80678
[1mStep[0m  [14/21], [94mLoss[0m : 10.99298
[1mStep[0m  [16/21], [94mLoss[0m : 10.61804
[1mStep[0m  [18/21], [94mLoss[0m : 11.00119
[1mStep[0m  [20/21], [94mLoss[0m : 10.45181

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.692, [92mTest[0m: 10.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79897
[1mStep[0m  [2/21], [94mLoss[0m : 10.73744
[1mStep[0m  [4/21], [94mLoss[0m : 10.62508
[1mStep[0m  [6/21], [94mLoss[0m : 10.54227
[1mStep[0m  [8/21], [94mLoss[0m : 11.02722
[1mStep[0m  [10/21], [94mLoss[0m : 10.77479
[1mStep[0m  [12/21], [94mLoss[0m : 10.91963
[1mStep[0m  [14/21], [94mLoss[0m : 10.65694
[1mStep[0m  [16/21], [94mLoss[0m : 10.54221
[1mStep[0m  [18/21], [94mLoss[0m : 10.55740
[1mStep[0m  [20/21], [94mLoss[0m : 10.45801

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.688, [92mTest[0m: 10.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77281
[1mStep[0m  [2/21], [94mLoss[0m : 10.92970
[1mStep[0m  [4/21], [94mLoss[0m : 10.59631
[1mStep[0m  [6/21], [94mLoss[0m : 10.38387
[1mStep[0m  [8/21], [94mLoss[0m : 10.77701
[1mStep[0m  [10/21], [94mLoss[0m : 10.71201
[1mStep[0m  [12/21], [94mLoss[0m : 10.83640
[1mStep[0m  [14/21], [94mLoss[0m : 10.61217
[1mStep[0m  [16/21], [94mLoss[0m : 10.69531
[1mStep[0m  [18/21], [94mLoss[0m : 10.64660
[1mStep[0m  [20/21], [94mLoss[0m : 10.86278

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.682, [92mTest[0m: 10.661, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.44405
[1mStep[0m  [2/21], [94mLoss[0m : 10.79471
[1mStep[0m  [4/21], [94mLoss[0m : 10.78905
[1mStep[0m  [6/21], [94mLoss[0m : 10.59693
[1mStep[0m  [8/21], [94mLoss[0m : 10.44933
[1mStep[0m  [10/21], [94mLoss[0m : 10.68940
[1mStep[0m  [12/21], [94mLoss[0m : 10.51052
[1mStep[0m  [14/21], [94mLoss[0m : 10.85226
[1mStep[0m  [16/21], [94mLoss[0m : 10.86388
[1mStep[0m  [18/21], [94mLoss[0m : 10.65712
[1mStep[0m  [20/21], [94mLoss[0m : 10.46069

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.678, [92mTest[0m: 10.644, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67576
[1mStep[0m  [2/21], [94mLoss[0m : 10.42755
[1mStep[0m  [4/21], [94mLoss[0m : 10.66130
[1mStep[0m  [6/21], [94mLoss[0m : 10.72253
[1mStep[0m  [8/21], [94mLoss[0m : 11.01340
[1mStep[0m  [10/21], [94mLoss[0m : 10.98817
[1mStep[0m  [12/21], [94mLoss[0m : 10.49464
[1mStep[0m  [14/21], [94mLoss[0m : 10.47005
[1mStep[0m  [16/21], [94mLoss[0m : 10.68818
[1mStep[0m  [18/21], [94mLoss[0m : 10.83895
[1mStep[0m  [20/21], [94mLoss[0m : 10.77425

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.669, [92mTest[0m: 10.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47423
[1mStep[0m  [2/21], [94mLoss[0m : 10.77120
[1mStep[0m  [4/21], [94mLoss[0m : 10.91823
[1mStep[0m  [6/21], [94mLoss[0m : 10.66832
[1mStep[0m  [8/21], [94mLoss[0m : 11.12097
[1mStep[0m  [10/21], [94mLoss[0m : 10.50443
[1mStep[0m  [12/21], [94mLoss[0m : 10.77315
[1mStep[0m  [14/21], [94mLoss[0m : 10.57699
[1mStep[0m  [16/21], [94mLoss[0m : 10.51734
[1mStep[0m  [18/21], [94mLoss[0m : 10.40638
[1mStep[0m  [20/21], [94mLoss[0m : 10.67759

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.660, [92mTest[0m: 10.621, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.92111
[1mStep[0m  [2/21], [94mLoss[0m : 10.57454
[1mStep[0m  [4/21], [94mLoss[0m : 10.47905
[1mStep[0m  [6/21], [94mLoss[0m : 10.52291
[1mStep[0m  [8/21], [94mLoss[0m : 10.79457
[1mStep[0m  [10/21], [94mLoss[0m : 10.84665
[1mStep[0m  [12/21], [94mLoss[0m : 10.77354
[1mStep[0m  [14/21], [94mLoss[0m : 10.65632
[1mStep[0m  [16/21], [94mLoss[0m : 10.68122
[1mStep[0m  [18/21], [94mLoss[0m : 10.43250
[1mStep[0m  [20/21], [94mLoss[0m : 10.51429

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.651, [92mTest[0m: 10.627, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.87431
[1mStep[0m  [2/21], [94mLoss[0m : 10.48099
[1mStep[0m  [4/21], [94mLoss[0m : 10.71624
[1mStep[0m  [6/21], [94mLoss[0m : 10.90255
[1mStep[0m  [8/21], [94mLoss[0m : 10.67211
[1mStep[0m  [10/21], [94mLoss[0m : 10.60412
[1mStep[0m  [12/21], [94mLoss[0m : 10.67095
[1mStep[0m  [14/21], [94mLoss[0m : 10.61409
[1mStep[0m  [16/21], [94mLoss[0m : 10.45173
[1mStep[0m  [18/21], [94mLoss[0m : 10.78982
[1mStep[0m  [20/21], [94mLoss[0m : 10.66678

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.652, [92mTest[0m: 10.616, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.76341
[1mStep[0m  [2/21], [94mLoss[0m : 10.48271
[1mStep[0m  [4/21], [94mLoss[0m : 10.47416
[1mStep[0m  [6/21], [94mLoss[0m : 10.72239
[1mStep[0m  [8/21], [94mLoss[0m : 10.66567
[1mStep[0m  [10/21], [94mLoss[0m : 11.07723
[1mStep[0m  [12/21], [94mLoss[0m : 10.66164
[1mStep[0m  [14/21], [94mLoss[0m : 10.81778
[1mStep[0m  [16/21], [94mLoss[0m : 10.65709
[1mStep[0m  [18/21], [94mLoss[0m : 10.58432
[1mStep[0m  [20/21], [94mLoss[0m : 10.64752

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.643, [92mTest[0m: 10.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60777
[1mStep[0m  [2/21], [94mLoss[0m : 10.97218
[1mStep[0m  [4/21], [94mLoss[0m : 10.66472
[1mStep[0m  [6/21], [94mLoss[0m : 10.96805
[1mStep[0m  [8/21], [94mLoss[0m : 10.85061
[1mStep[0m  [10/21], [94mLoss[0m : 10.60169
[1mStep[0m  [12/21], [94mLoss[0m : 10.70316
[1mStep[0m  [14/21], [94mLoss[0m : 10.55466
[1mStep[0m  [16/21], [94mLoss[0m : 10.56732
[1mStep[0m  [18/21], [94mLoss[0m : 10.27894
[1mStep[0m  [20/21], [94mLoss[0m : 10.89560

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.631, [92mTest[0m: 10.599, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55134
[1mStep[0m  [2/21], [94mLoss[0m : 10.68730
[1mStep[0m  [4/21], [94mLoss[0m : 10.84011
[1mStep[0m  [6/21], [94mLoss[0m : 10.68658
[1mStep[0m  [8/21], [94mLoss[0m : 10.86823
[1mStep[0m  [10/21], [94mLoss[0m : 10.55962
[1mStep[0m  [12/21], [94mLoss[0m : 10.60703
[1mStep[0m  [14/21], [94mLoss[0m : 10.86702
[1mStep[0m  [16/21], [94mLoss[0m : 10.57916
[1mStep[0m  [18/21], [94mLoss[0m : 10.25533
[1mStep[0m  [20/21], [94mLoss[0m : 10.69658

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.636, [92mTest[0m: 10.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.53235
[1mStep[0m  [2/21], [94mLoss[0m : 10.46979
[1mStep[0m  [4/21], [94mLoss[0m : 11.04133
[1mStep[0m  [6/21], [94mLoss[0m : 10.60296
[1mStep[0m  [8/21], [94mLoss[0m : 10.40446
[1mStep[0m  [10/21], [94mLoss[0m : 10.62331
[1mStep[0m  [12/21], [94mLoss[0m : 10.34920
[1mStep[0m  [14/21], [94mLoss[0m : 10.73487
[1mStep[0m  [16/21], [94mLoss[0m : 10.58844
[1mStep[0m  [18/21], [94mLoss[0m : 10.59321
[1mStep[0m  [20/21], [94mLoss[0m : 10.59807

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.630, [92mTest[0m: 10.602, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.41247
[1mStep[0m  [2/21], [94mLoss[0m : 10.79518
[1mStep[0m  [4/21], [94mLoss[0m : 10.68372
[1mStep[0m  [6/21], [94mLoss[0m : 10.51931
[1mStep[0m  [8/21], [94mLoss[0m : 10.63779
[1mStep[0m  [10/21], [94mLoss[0m : 10.80247
[1mStep[0m  [12/21], [94mLoss[0m : 10.37393
[1mStep[0m  [14/21], [94mLoss[0m : 10.39517
[1mStep[0m  [16/21], [94mLoss[0m : 10.51048
[1mStep[0m  [18/21], [94mLoss[0m : 10.87409
[1mStep[0m  [20/21], [94mLoss[0m : 10.65337

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.627, [92mTest[0m: 10.582, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.03235
[1mStep[0m  [2/21], [94mLoss[0m : 10.54826
[1mStep[0m  [4/21], [94mLoss[0m : 10.69607
[1mStep[0m  [6/21], [94mLoss[0m : 10.56832
[1mStep[0m  [8/21], [94mLoss[0m : 10.26326
[1mStep[0m  [10/21], [94mLoss[0m : 10.33354
[1mStep[0m  [12/21], [94mLoss[0m : 10.66610
[1mStep[0m  [14/21], [94mLoss[0m : 10.26446
[1mStep[0m  [16/21], [94mLoss[0m : 10.71926
[1mStep[0m  [18/21], [94mLoss[0m : 10.75774
[1mStep[0m  [20/21], [94mLoss[0m : 10.88214

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.625, [92mTest[0m: 10.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.37557
[1mStep[0m  [2/21], [94mLoss[0m : 10.41847
[1mStep[0m  [4/21], [94mLoss[0m : 10.59146
[1mStep[0m  [6/21], [94mLoss[0m : 10.61310
[1mStep[0m  [8/21], [94mLoss[0m : 10.53047
[1mStep[0m  [10/21], [94mLoss[0m : 10.56950
[1mStep[0m  [12/21], [94mLoss[0m : 10.82735
[1mStep[0m  [14/21], [94mLoss[0m : 10.87858
[1mStep[0m  [16/21], [94mLoss[0m : 10.58861
[1mStep[0m  [18/21], [94mLoss[0m : 10.15490
[1mStep[0m  [20/21], [94mLoss[0m : 10.60790

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.581, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81615
[1mStep[0m  [2/21], [94mLoss[0m : 10.52902
[1mStep[0m  [4/21], [94mLoss[0m : 10.60279
[1mStep[0m  [6/21], [94mLoss[0m : 10.55530
[1mStep[0m  [8/21], [94mLoss[0m : 10.54806
[1mStep[0m  [10/21], [94mLoss[0m : 10.56282
[1mStep[0m  [12/21], [94mLoss[0m : 10.45961
[1mStep[0m  [14/21], [94mLoss[0m : 10.56062
[1mStep[0m  [16/21], [94mLoss[0m : 10.60723
[1mStep[0m  [18/21], [94mLoss[0m : 10.54876
[1mStep[0m  [20/21], [94mLoss[0m : 10.58436

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.611, [92mTest[0m: 10.564, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49586
[1mStep[0m  [2/21], [94mLoss[0m : 10.89329
[1mStep[0m  [4/21], [94mLoss[0m : 10.39271
[1mStep[0m  [6/21], [94mLoss[0m : 10.49963
[1mStep[0m  [8/21], [94mLoss[0m : 10.70729
[1mStep[0m  [10/21], [94mLoss[0m : 10.78920
[1mStep[0m  [12/21], [94mLoss[0m : 10.32735
[1mStep[0m  [14/21], [94mLoss[0m : 10.71681
[1mStep[0m  [16/21], [94mLoss[0m : 10.53597
[1mStep[0m  [18/21], [94mLoss[0m : 10.42750
[1mStep[0m  [20/21], [94mLoss[0m : 10.48463

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.603, [92mTest[0m: 10.557, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65000
[1mStep[0m  [2/21], [94mLoss[0m : 10.40555
[1mStep[0m  [4/21], [94mLoss[0m : 10.49915
[1mStep[0m  [6/21], [94mLoss[0m : 10.38814
[1mStep[0m  [8/21], [94mLoss[0m : 10.36283
[1mStep[0m  [10/21], [94mLoss[0m : 10.78985
[1mStep[0m  [12/21], [94mLoss[0m : 10.64021
[1mStep[0m  [14/21], [94mLoss[0m : 10.86113
[1mStep[0m  [16/21], [94mLoss[0m : 10.68246
[1mStep[0m  [18/21], [94mLoss[0m : 10.94759
[1mStep[0m  [20/21], [94mLoss[0m : 10.83184

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.606, [92mTest[0m: 10.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.25583
[1mStep[0m  [2/21], [94mLoss[0m : 10.72503
[1mStep[0m  [4/21], [94mLoss[0m : 10.57868
[1mStep[0m  [6/21], [94mLoss[0m : 10.69335
[1mStep[0m  [8/21], [94mLoss[0m : 10.79548
[1mStep[0m  [10/21], [94mLoss[0m : 10.81777
[1mStep[0m  [12/21], [94mLoss[0m : 10.46580
[1mStep[0m  [14/21], [94mLoss[0m : 10.56955
[1mStep[0m  [16/21], [94mLoss[0m : 10.35809
[1mStep[0m  [18/21], [94mLoss[0m : 10.69982
[1mStep[0m  [20/21], [94mLoss[0m : 10.44984

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.598, [92mTest[0m: 10.557, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.83805
[1mStep[0m  [2/21], [94mLoss[0m : 10.45881
[1mStep[0m  [4/21], [94mLoss[0m : 10.66315
[1mStep[0m  [6/21], [94mLoss[0m : 10.48962
[1mStep[0m  [8/21], [94mLoss[0m : 10.73392
[1mStep[0m  [10/21], [94mLoss[0m : 10.42128
[1mStep[0m  [12/21], [94mLoss[0m : 10.49583
[1mStep[0m  [14/21], [94mLoss[0m : 10.63495
[1mStep[0m  [16/21], [94mLoss[0m : 10.74637
[1mStep[0m  [18/21], [94mLoss[0m : 10.59269
[1mStep[0m  [20/21], [94mLoss[0m : 10.48383

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.546, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.517
====================================

Phase 2 - Evaluation MAE:  10.516713959830147
MAE score P1      10.736503
MAE score P2      10.516714
loss              10.588466
learning_rate        0.0001
batch_size              512
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.85759
[1mStep[0m  [33/339], [94mLoss[0m : 8.75995
[1mStep[0m  [66/339], [94mLoss[0m : 9.03983
[1mStep[0m  [99/339], [94mLoss[0m : 8.26379
[1mStep[0m  [132/339], [94mLoss[0m : 7.26711
[1mStep[0m  [165/339], [94mLoss[0m : 6.45966
[1mStep[0m  [198/339], [94mLoss[0m : 5.70289
[1mStep[0m  [231/339], [94mLoss[0m : 5.96416
[1mStep[0m  [264/339], [94mLoss[0m : 3.52480
[1mStep[0m  [297/339], [94mLoss[0m : 3.31936
[1mStep[0m  [330/339], [94mLoss[0m : 3.58339

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.615, [92mTest[0m: 10.915, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73549
[1mStep[0m  [33/339], [94mLoss[0m : 3.20945
[1mStep[0m  [66/339], [94mLoss[0m : 3.02926
[1mStep[0m  [99/339], [94mLoss[0m : 2.36885
[1mStep[0m  [132/339], [94mLoss[0m : 2.49051
[1mStep[0m  [165/339], [94mLoss[0m : 3.72099
[1mStep[0m  [198/339], [94mLoss[0m : 3.18781
[1mStep[0m  [231/339], [94mLoss[0m : 2.72383
[1mStep[0m  [264/339], [94mLoss[0m : 3.51580
[1mStep[0m  [297/339], [94mLoss[0m : 3.05941
[1mStep[0m  [330/339], [94mLoss[0m : 3.29384

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.913, [92mTest[0m: 3.760, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.22267
[1mStep[0m  [33/339], [94mLoss[0m : 2.75133
[1mStep[0m  [66/339], [94mLoss[0m : 2.68487
[1mStep[0m  [99/339], [94mLoss[0m : 2.22474
[1mStep[0m  [132/339], [94mLoss[0m : 2.55606
[1mStep[0m  [165/339], [94mLoss[0m : 2.59141
[1mStep[0m  [198/339], [94mLoss[0m : 3.03170
[1mStep[0m  [231/339], [94mLoss[0m : 3.18159
[1mStep[0m  [264/339], [94mLoss[0m : 2.84335
[1mStep[0m  [297/339], [94mLoss[0m : 3.02647
[1mStep[0m  [330/339], [94mLoss[0m : 2.72936

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.07621
[1mStep[0m  [33/339], [94mLoss[0m : 2.20256
[1mStep[0m  [66/339], [94mLoss[0m : 3.19166
[1mStep[0m  [99/339], [94mLoss[0m : 2.74254
[1mStep[0m  [132/339], [94mLoss[0m : 2.79872
[1mStep[0m  [165/339], [94mLoss[0m : 2.21826
[1mStep[0m  [198/339], [94mLoss[0m : 2.92594
[1mStep[0m  [231/339], [94mLoss[0m : 3.21340
[1mStep[0m  [264/339], [94mLoss[0m : 2.88816
[1mStep[0m  [297/339], [94mLoss[0m : 2.27013
[1mStep[0m  [330/339], [94mLoss[0m : 2.44656

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95148
[1mStep[0m  [33/339], [94mLoss[0m : 2.53314
[1mStep[0m  [66/339], [94mLoss[0m : 2.69990
[1mStep[0m  [99/339], [94mLoss[0m : 3.02242
[1mStep[0m  [132/339], [94mLoss[0m : 2.65160
[1mStep[0m  [165/339], [94mLoss[0m : 2.14754
[1mStep[0m  [198/339], [94mLoss[0m : 2.95412
[1mStep[0m  [231/339], [94mLoss[0m : 2.12587
[1mStep[0m  [264/339], [94mLoss[0m : 3.23545
[1mStep[0m  [297/339], [94mLoss[0m : 2.47744
[1mStep[0m  [330/339], [94mLoss[0m : 2.90916

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.773, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.04830
[1mStep[0m  [33/339], [94mLoss[0m : 2.63015
[1mStep[0m  [66/339], [94mLoss[0m : 2.86252
[1mStep[0m  [99/339], [94mLoss[0m : 2.83596
[1mStep[0m  [132/339], [94mLoss[0m : 2.82671
[1mStep[0m  [165/339], [94mLoss[0m : 2.85768
[1mStep[0m  [198/339], [94mLoss[0m : 2.99847
[1mStep[0m  [231/339], [94mLoss[0m : 2.24814
[1mStep[0m  [264/339], [94mLoss[0m : 2.82081
[1mStep[0m  [297/339], [94mLoss[0m : 2.44628
[1mStep[0m  [330/339], [94mLoss[0m : 2.72982

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.721, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.28102
[1mStep[0m  [33/339], [94mLoss[0m : 2.69340
[1mStep[0m  [66/339], [94mLoss[0m : 2.68476
[1mStep[0m  [99/339], [94mLoss[0m : 3.21119
[1mStep[0m  [132/339], [94mLoss[0m : 2.57881
[1mStep[0m  [165/339], [94mLoss[0m : 2.15901
[1mStep[0m  [198/339], [94mLoss[0m : 2.91096
[1mStep[0m  [231/339], [94mLoss[0m : 2.06725
[1mStep[0m  [264/339], [94mLoss[0m : 2.40266
[1mStep[0m  [297/339], [94mLoss[0m : 2.71898
[1mStep[0m  [330/339], [94mLoss[0m : 2.81980

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.722, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84745
[1mStep[0m  [33/339], [94mLoss[0m : 2.80951
[1mStep[0m  [66/339], [94mLoss[0m : 2.77037
[1mStep[0m  [99/339], [94mLoss[0m : 3.13208
[1mStep[0m  [132/339], [94mLoss[0m : 2.80144
[1mStep[0m  [165/339], [94mLoss[0m : 2.54261
[1mStep[0m  [198/339], [94mLoss[0m : 2.78203
[1mStep[0m  [231/339], [94mLoss[0m : 2.87731
[1mStep[0m  [264/339], [94mLoss[0m : 3.00570
[1mStep[0m  [297/339], [94mLoss[0m : 2.68075
[1mStep[0m  [330/339], [94mLoss[0m : 2.63733

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.33424
[1mStep[0m  [33/339], [94mLoss[0m : 2.92651
[1mStep[0m  [66/339], [94mLoss[0m : 2.46320
[1mStep[0m  [99/339], [94mLoss[0m : 2.30212
[1mStep[0m  [132/339], [94mLoss[0m : 2.30123
[1mStep[0m  [165/339], [94mLoss[0m : 2.64520
[1mStep[0m  [198/339], [94mLoss[0m : 3.22184
[1mStep[0m  [231/339], [94mLoss[0m : 2.99739
[1mStep[0m  [264/339], [94mLoss[0m : 2.57272
[1mStep[0m  [297/339], [94mLoss[0m : 1.99287
[1mStep[0m  [330/339], [94mLoss[0m : 2.56429

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85675
[1mStep[0m  [33/339], [94mLoss[0m : 2.44392
[1mStep[0m  [66/339], [94mLoss[0m : 2.92673
[1mStep[0m  [99/339], [94mLoss[0m : 2.84063
[1mStep[0m  [132/339], [94mLoss[0m : 2.99256
[1mStep[0m  [165/339], [94mLoss[0m : 2.68176
[1mStep[0m  [198/339], [94mLoss[0m : 1.79708
[1mStep[0m  [231/339], [94mLoss[0m : 2.59338
[1mStep[0m  [264/339], [94mLoss[0m : 3.07691
[1mStep[0m  [297/339], [94mLoss[0m : 2.64868
[1mStep[0m  [330/339], [94mLoss[0m : 2.61191

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12322
[1mStep[0m  [33/339], [94mLoss[0m : 3.63255
[1mStep[0m  [66/339], [94mLoss[0m : 2.69630
[1mStep[0m  [99/339], [94mLoss[0m : 2.34572
[1mStep[0m  [132/339], [94mLoss[0m : 2.42381
[1mStep[0m  [165/339], [94mLoss[0m : 2.76146
[1mStep[0m  [198/339], [94mLoss[0m : 3.04394
[1mStep[0m  [231/339], [94mLoss[0m : 2.22342
[1mStep[0m  [264/339], [94mLoss[0m : 2.26131
[1mStep[0m  [297/339], [94mLoss[0m : 2.33594
[1mStep[0m  [330/339], [94mLoss[0m : 3.38637

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50825
[1mStep[0m  [33/339], [94mLoss[0m : 2.40683
[1mStep[0m  [66/339], [94mLoss[0m : 2.81284
[1mStep[0m  [99/339], [94mLoss[0m : 2.60006
[1mStep[0m  [132/339], [94mLoss[0m : 2.48147
[1mStep[0m  [165/339], [94mLoss[0m : 2.68273
[1mStep[0m  [198/339], [94mLoss[0m : 3.15383
[1mStep[0m  [231/339], [94mLoss[0m : 2.27849
[1mStep[0m  [264/339], [94mLoss[0m : 1.96470
[1mStep[0m  [297/339], [94mLoss[0m : 2.71083
[1mStep[0m  [330/339], [94mLoss[0m : 2.36759

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61687
[1mStep[0m  [33/339], [94mLoss[0m : 2.68949
[1mStep[0m  [66/339], [94mLoss[0m : 2.75895
[1mStep[0m  [99/339], [94mLoss[0m : 2.86519
[1mStep[0m  [132/339], [94mLoss[0m : 3.49909
[1mStep[0m  [165/339], [94mLoss[0m : 2.25448
[1mStep[0m  [198/339], [94mLoss[0m : 2.26942
[1mStep[0m  [231/339], [94mLoss[0m : 2.68749
[1mStep[0m  [264/339], [94mLoss[0m : 2.67153
[1mStep[0m  [297/339], [94mLoss[0m : 2.48392
[1mStep[0m  [330/339], [94mLoss[0m : 1.89516

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46521
[1mStep[0m  [33/339], [94mLoss[0m : 3.15034
[1mStep[0m  [66/339], [94mLoss[0m : 2.34694
[1mStep[0m  [99/339], [94mLoss[0m : 3.49636
[1mStep[0m  [132/339], [94mLoss[0m : 2.04223
[1mStep[0m  [165/339], [94mLoss[0m : 3.46250
[1mStep[0m  [198/339], [94mLoss[0m : 2.60009
[1mStep[0m  [231/339], [94mLoss[0m : 1.78998
[1mStep[0m  [264/339], [94mLoss[0m : 3.01408
[1mStep[0m  [297/339], [94mLoss[0m : 2.62562
[1mStep[0m  [330/339], [94mLoss[0m : 2.57163

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44808
[1mStep[0m  [33/339], [94mLoss[0m : 2.49563
[1mStep[0m  [66/339], [94mLoss[0m : 2.96913
[1mStep[0m  [99/339], [94mLoss[0m : 2.47151
[1mStep[0m  [132/339], [94mLoss[0m : 2.65874
[1mStep[0m  [165/339], [94mLoss[0m : 2.66153
[1mStep[0m  [198/339], [94mLoss[0m : 2.37031
[1mStep[0m  [231/339], [94mLoss[0m : 3.33917
[1mStep[0m  [264/339], [94mLoss[0m : 2.28713
[1mStep[0m  [297/339], [94mLoss[0m : 2.40164
[1mStep[0m  [330/339], [94mLoss[0m : 2.29676

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97140
[1mStep[0m  [33/339], [94mLoss[0m : 2.54476
[1mStep[0m  [66/339], [94mLoss[0m : 3.04162
[1mStep[0m  [99/339], [94mLoss[0m : 2.31688
[1mStep[0m  [132/339], [94mLoss[0m : 2.86769
[1mStep[0m  [165/339], [94mLoss[0m : 2.32815
[1mStep[0m  [198/339], [94mLoss[0m : 2.94462
[1mStep[0m  [231/339], [94mLoss[0m : 2.96502
[1mStep[0m  [264/339], [94mLoss[0m : 2.51169
[1mStep[0m  [297/339], [94mLoss[0m : 2.38023
[1mStep[0m  [330/339], [94mLoss[0m : 2.38456

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72852
[1mStep[0m  [33/339], [94mLoss[0m : 2.56370
[1mStep[0m  [66/339], [94mLoss[0m : 2.60298
[1mStep[0m  [99/339], [94mLoss[0m : 2.40559
[1mStep[0m  [132/339], [94mLoss[0m : 2.65435
[1mStep[0m  [165/339], [94mLoss[0m : 2.45605
[1mStep[0m  [198/339], [94mLoss[0m : 1.84586
[1mStep[0m  [231/339], [94mLoss[0m : 2.76644
[1mStep[0m  [264/339], [94mLoss[0m : 2.99379
[1mStep[0m  [297/339], [94mLoss[0m : 2.72114
[1mStep[0m  [330/339], [94mLoss[0m : 3.01392

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.89100
[1mStep[0m  [33/339], [94mLoss[0m : 2.51750
[1mStep[0m  [66/339], [94mLoss[0m : 2.39445
[1mStep[0m  [99/339], [94mLoss[0m : 2.19493
[1mStep[0m  [132/339], [94mLoss[0m : 2.53993
[1mStep[0m  [165/339], [94mLoss[0m : 2.53182
[1mStep[0m  [198/339], [94mLoss[0m : 2.49328
[1mStep[0m  [231/339], [94mLoss[0m : 1.92649
[1mStep[0m  [264/339], [94mLoss[0m : 2.75294
[1mStep[0m  [297/339], [94mLoss[0m : 2.13953
[1mStep[0m  [330/339], [94mLoss[0m : 2.87569

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55642
[1mStep[0m  [33/339], [94mLoss[0m : 2.53647
[1mStep[0m  [66/339], [94mLoss[0m : 2.49907
[1mStep[0m  [99/339], [94mLoss[0m : 3.59883
[1mStep[0m  [132/339], [94mLoss[0m : 2.91129
[1mStep[0m  [165/339], [94mLoss[0m : 2.51187
[1mStep[0m  [198/339], [94mLoss[0m : 2.65822
[1mStep[0m  [231/339], [94mLoss[0m : 2.14351
[1mStep[0m  [264/339], [94mLoss[0m : 2.78484
[1mStep[0m  [297/339], [94mLoss[0m : 3.48013
[1mStep[0m  [330/339], [94mLoss[0m : 2.23098

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32829
[1mStep[0m  [33/339], [94mLoss[0m : 2.18882
[1mStep[0m  [66/339], [94mLoss[0m : 2.40787
[1mStep[0m  [99/339], [94mLoss[0m : 2.87594
[1mStep[0m  [132/339], [94mLoss[0m : 2.69093
[1mStep[0m  [165/339], [94mLoss[0m : 2.87824
[1mStep[0m  [198/339], [94mLoss[0m : 2.72360
[1mStep[0m  [231/339], [94mLoss[0m : 2.05497
[1mStep[0m  [264/339], [94mLoss[0m : 3.57361
[1mStep[0m  [297/339], [94mLoss[0m : 2.29822
[1mStep[0m  [330/339], [94mLoss[0m : 1.90392

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.371, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92362
[1mStep[0m  [33/339], [94mLoss[0m : 2.80752
[1mStep[0m  [66/339], [94mLoss[0m : 2.20228
[1mStep[0m  [99/339], [94mLoss[0m : 2.00605
[1mStep[0m  [132/339], [94mLoss[0m : 2.27419
[1mStep[0m  [165/339], [94mLoss[0m : 2.88365
[1mStep[0m  [198/339], [94mLoss[0m : 2.89625
[1mStep[0m  [231/339], [94mLoss[0m : 2.34708
[1mStep[0m  [264/339], [94mLoss[0m : 1.98422
[1mStep[0m  [297/339], [94mLoss[0m : 2.10435
[1mStep[0m  [330/339], [94mLoss[0m : 3.11310

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.358, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63535
[1mStep[0m  [33/339], [94mLoss[0m : 2.06734
[1mStep[0m  [66/339], [94mLoss[0m : 2.63421
[1mStep[0m  [99/339], [94mLoss[0m : 2.06114
[1mStep[0m  [132/339], [94mLoss[0m : 2.71192
[1mStep[0m  [165/339], [94mLoss[0m : 2.05131
[1mStep[0m  [198/339], [94mLoss[0m : 2.76439
[1mStep[0m  [231/339], [94mLoss[0m : 2.84420
[1mStep[0m  [264/339], [94mLoss[0m : 2.81224
[1mStep[0m  [297/339], [94mLoss[0m : 2.52670
[1mStep[0m  [330/339], [94mLoss[0m : 2.68828

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40719
[1mStep[0m  [33/339], [94mLoss[0m : 2.35262
[1mStep[0m  [66/339], [94mLoss[0m : 2.89963
[1mStep[0m  [99/339], [94mLoss[0m : 2.58592
[1mStep[0m  [132/339], [94mLoss[0m : 2.34095
[1mStep[0m  [165/339], [94mLoss[0m : 2.28670
[1mStep[0m  [198/339], [94mLoss[0m : 2.16803
[1mStep[0m  [231/339], [94mLoss[0m : 2.63209
[1mStep[0m  [264/339], [94mLoss[0m : 2.80199
[1mStep[0m  [297/339], [94mLoss[0m : 2.83994
[1mStep[0m  [330/339], [94mLoss[0m : 1.69770

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71932
[1mStep[0m  [33/339], [94mLoss[0m : 2.12335
[1mStep[0m  [66/339], [94mLoss[0m : 2.29218
[1mStep[0m  [99/339], [94mLoss[0m : 2.77439
[1mStep[0m  [132/339], [94mLoss[0m : 2.80344
[1mStep[0m  [165/339], [94mLoss[0m : 3.02403
[1mStep[0m  [198/339], [94mLoss[0m : 2.46234
[1mStep[0m  [231/339], [94mLoss[0m : 2.79579
[1mStep[0m  [264/339], [94mLoss[0m : 2.96039
[1mStep[0m  [297/339], [94mLoss[0m : 2.82646
[1mStep[0m  [330/339], [94mLoss[0m : 2.65130

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66635
[1mStep[0m  [33/339], [94mLoss[0m : 3.04463
[1mStep[0m  [66/339], [94mLoss[0m : 2.17938
[1mStep[0m  [99/339], [94mLoss[0m : 2.05312
[1mStep[0m  [132/339], [94mLoss[0m : 2.83651
[1mStep[0m  [165/339], [94mLoss[0m : 2.52859
[1mStep[0m  [198/339], [94mLoss[0m : 3.08576
[1mStep[0m  [231/339], [94mLoss[0m : 2.56566
[1mStep[0m  [264/339], [94mLoss[0m : 1.98425
[1mStep[0m  [297/339], [94mLoss[0m : 2.86845
[1mStep[0m  [330/339], [94mLoss[0m : 2.39802

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.373, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00558
[1mStep[0m  [33/339], [94mLoss[0m : 2.63510
[1mStep[0m  [66/339], [94mLoss[0m : 2.98404
[1mStep[0m  [99/339], [94mLoss[0m : 2.47753
[1mStep[0m  [132/339], [94mLoss[0m : 2.67264
[1mStep[0m  [165/339], [94mLoss[0m : 2.86824
[1mStep[0m  [198/339], [94mLoss[0m : 2.67349
[1mStep[0m  [231/339], [94mLoss[0m : 2.24482
[1mStep[0m  [264/339], [94mLoss[0m : 2.64671
[1mStep[0m  [297/339], [94mLoss[0m : 2.01384
[1mStep[0m  [330/339], [94mLoss[0m : 2.90386

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24346
[1mStep[0m  [33/339], [94mLoss[0m : 2.28290
[1mStep[0m  [66/339], [94mLoss[0m : 1.95067
[1mStep[0m  [99/339], [94mLoss[0m : 2.01172
[1mStep[0m  [132/339], [94mLoss[0m : 2.49267
[1mStep[0m  [165/339], [94mLoss[0m : 3.01260
[1mStep[0m  [198/339], [94mLoss[0m : 2.82111
[1mStep[0m  [231/339], [94mLoss[0m : 2.59707
[1mStep[0m  [264/339], [94mLoss[0m : 2.41410
[1mStep[0m  [297/339], [94mLoss[0m : 2.56737
[1mStep[0m  [330/339], [94mLoss[0m : 2.54200

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62814
[1mStep[0m  [33/339], [94mLoss[0m : 2.10500
[1mStep[0m  [66/339], [94mLoss[0m : 2.66644
[1mStep[0m  [99/339], [94mLoss[0m : 2.24864
[1mStep[0m  [132/339], [94mLoss[0m : 2.95770
[1mStep[0m  [165/339], [94mLoss[0m : 2.45069
[1mStep[0m  [198/339], [94mLoss[0m : 2.05806
[1mStep[0m  [231/339], [94mLoss[0m : 2.81996
[1mStep[0m  [264/339], [94mLoss[0m : 3.11502
[1mStep[0m  [297/339], [94mLoss[0m : 3.12636
[1mStep[0m  [330/339], [94mLoss[0m : 2.46215

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.355, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71012
[1mStep[0m  [33/339], [94mLoss[0m : 2.65615
[1mStep[0m  [66/339], [94mLoss[0m : 3.57362
[1mStep[0m  [99/339], [94mLoss[0m : 3.15205
[1mStep[0m  [132/339], [94mLoss[0m : 2.56590
[1mStep[0m  [165/339], [94mLoss[0m : 2.82410
[1mStep[0m  [198/339], [94mLoss[0m : 2.19325
[1mStep[0m  [231/339], [94mLoss[0m : 2.42807
[1mStep[0m  [264/339], [94mLoss[0m : 2.50181
[1mStep[0m  [297/339], [94mLoss[0m : 1.81072
[1mStep[0m  [330/339], [94mLoss[0m : 2.17304

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13641
[1mStep[0m  [33/339], [94mLoss[0m : 2.53384
[1mStep[0m  [66/339], [94mLoss[0m : 2.69423
[1mStep[0m  [99/339], [94mLoss[0m : 1.94624
[1mStep[0m  [132/339], [94mLoss[0m : 2.77879
[1mStep[0m  [165/339], [94mLoss[0m : 2.41505
[1mStep[0m  [198/339], [94mLoss[0m : 2.59414
[1mStep[0m  [231/339], [94mLoss[0m : 2.19411
[1mStep[0m  [264/339], [94mLoss[0m : 2.81802
[1mStep[0m  [297/339], [94mLoss[0m : 2.71040
[1mStep[0m  [330/339], [94mLoss[0m : 2.53483

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.331509713578013
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.19024
[1mStep[0m  [33/339], [94mLoss[0m : 2.53925
[1mStep[0m  [66/339], [94mLoss[0m : 2.58642
[1mStep[0m  [99/339], [94mLoss[0m : 2.10260
[1mStep[0m  [132/339], [94mLoss[0m : 2.65308
[1mStep[0m  [165/339], [94mLoss[0m : 2.98775
[1mStep[0m  [198/339], [94mLoss[0m : 1.65750
[1mStep[0m  [231/339], [94mLoss[0m : 2.61714
[1mStep[0m  [264/339], [94mLoss[0m : 3.33617
[1mStep[0m  [297/339], [94mLoss[0m : 2.33963
[1mStep[0m  [330/339], [94mLoss[0m : 2.51796

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78339
[1mStep[0m  [33/339], [94mLoss[0m : 2.67980
[1mStep[0m  [66/339], [94mLoss[0m : 2.56068
[1mStep[0m  [99/339], [94mLoss[0m : 2.35626
[1mStep[0m  [132/339], [94mLoss[0m : 2.44941
[1mStep[0m  [165/339], [94mLoss[0m : 2.36727
[1mStep[0m  [198/339], [94mLoss[0m : 1.88262
[1mStep[0m  [231/339], [94mLoss[0m : 2.16356
[1mStep[0m  [264/339], [94mLoss[0m : 2.92142
[1mStep[0m  [297/339], [94mLoss[0m : 2.42997
[1mStep[0m  [330/339], [94mLoss[0m : 2.57880

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.664, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67000
[1mStep[0m  [33/339], [94mLoss[0m : 2.50932
[1mStep[0m  [66/339], [94mLoss[0m : 2.69392
[1mStep[0m  [99/339], [94mLoss[0m : 2.70886
[1mStep[0m  [132/339], [94mLoss[0m : 2.64606
[1mStep[0m  [165/339], [94mLoss[0m : 2.92676
[1mStep[0m  [198/339], [94mLoss[0m : 3.12676
[1mStep[0m  [231/339], [94mLoss[0m : 2.88770
[1mStep[0m  [264/339], [94mLoss[0m : 2.77180
[1mStep[0m  [297/339], [94mLoss[0m : 2.59913
[1mStep[0m  [330/339], [94mLoss[0m : 2.42025

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.567, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86502
[1mStep[0m  [33/339], [94mLoss[0m : 2.71971
[1mStep[0m  [66/339], [94mLoss[0m : 2.18704
[1mStep[0m  [99/339], [94mLoss[0m : 2.37933
[1mStep[0m  [132/339], [94mLoss[0m : 3.17531
[1mStep[0m  [165/339], [94mLoss[0m : 2.01767
[1mStep[0m  [198/339], [94mLoss[0m : 2.75212
[1mStep[0m  [231/339], [94mLoss[0m : 2.24178
[1mStep[0m  [264/339], [94mLoss[0m : 2.27515
[1mStep[0m  [297/339], [94mLoss[0m : 2.80500
[1mStep[0m  [330/339], [94mLoss[0m : 2.26791

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.553, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91739
[1mStep[0m  [33/339], [94mLoss[0m : 2.01973
[1mStep[0m  [66/339], [94mLoss[0m : 3.03183
[1mStep[0m  [99/339], [94mLoss[0m : 2.70641
[1mStep[0m  [132/339], [94mLoss[0m : 2.83461
[1mStep[0m  [165/339], [94mLoss[0m : 2.70686
[1mStep[0m  [198/339], [94mLoss[0m : 1.82852
[1mStep[0m  [231/339], [94mLoss[0m : 2.16982
[1mStep[0m  [264/339], [94mLoss[0m : 3.00771
[1mStep[0m  [297/339], [94mLoss[0m : 2.78349
[1mStep[0m  [330/339], [94mLoss[0m : 2.84598

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31667
[1mStep[0m  [33/339], [94mLoss[0m : 1.74573
[1mStep[0m  [66/339], [94mLoss[0m : 2.26683
[1mStep[0m  [99/339], [94mLoss[0m : 2.73890
[1mStep[0m  [132/339], [94mLoss[0m : 2.67605
[1mStep[0m  [165/339], [94mLoss[0m : 2.28735
[1mStep[0m  [198/339], [94mLoss[0m : 3.02150
[1mStep[0m  [231/339], [94mLoss[0m : 2.86295
[1mStep[0m  [264/339], [94mLoss[0m : 2.39780
[1mStep[0m  [297/339], [94mLoss[0m : 2.38818
[1mStep[0m  [330/339], [94mLoss[0m : 2.54538

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11833
[1mStep[0m  [33/339], [94mLoss[0m : 1.60218
[1mStep[0m  [66/339], [94mLoss[0m : 2.48632
[1mStep[0m  [99/339], [94mLoss[0m : 2.53901
[1mStep[0m  [132/339], [94mLoss[0m : 1.93859
[1mStep[0m  [165/339], [94mLoss[0m : 2.79020
[1mStep[0m  [198/339], [94mLoss[0m : 1.78804
[1mStep[0m  [231/339], [94mLoss[0m : 2.13387
[1mStep[0m  [264/339], [94mLoss[0m : 2.33805
[1mStep[0m  [297/339], [94mLoss[0m : 2.54003
[1mStep[0m  [330/339], [94mLoss[0m : 2.05275

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01425
[1mStep[0m  [33/339], [94mLoss[0m : 2.42320
[1mStep[0m  [66/339], [94mLoss[0m : 2.66319
[1mStep[0m  [99/339], [94mLoss[0m : 2.12836
[1mStep[0m  [132/339], [94mLoss[0m : 2.12897
[1mStep[0m  [165/339], [94mLoss[0m : 2.30048
[1mStep[0m  [198/339], [94mLoss[0m : 1.87525
[1mStep[0m  [231/339], [94mLoss[0m : 2.23173
[1mStep[0m  [264/339], [94mLoss[0m : 2.58934
[1mStep[0m  [297/339], [94mLoss[0m : 2.62111
[1mStep[0m  [330/339], [94mLoss[0m : 3.05968

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00330
[1mStep[0m  [33/339], [94mLoss[0m : 2.11334
[1mStep[0m  [66/339], [94mLoss[0m : 1.99369
[1mStep[0m  [99/339], [94mLoss[0m : 2.23841
[1mStep[0m  [132/339], [94mLoss[0m : 1.98628
[1mStep[0m  [165/339], [94mLoss[0m : 2.21349
[1mStep[0m  [198/339], [94mLoss[0m : 2.28281
[1mStep[0m  [231/339], [94mLoss[0m : 2.42452
[1mStep[0m  [264/339], [94mLoss[0m : 1.79194
[1mStep[0m  [297/339], [94mLoss[0m : 2.00179
[1mStep[0m  [330/339], [94mLoss[0m : 2.19075

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32969
[1mStep[0m  [33/339], [94mLoss[0m : 2.15189
[1mStep[0m  [66/339], [94mLoss[0m : 2.68634
[1mStep[0m  [99/339], [94mLoss[0m : 3.28082
[1mStep[0m  [132/339], [94mLoss[0m : 2.31760
[1mStep[0m  [165/339], [94mLoss[0m : 2.07857
[1mStep[0m  [198/339], [94mLoss[0m : 2.25059
[1mStep[0m  [231/339], [94mLoss[0m : 2.72829
[1mStep[0m  [264/339], [94mLoss[0m : 2.78229
[1mStep[0m  [297/339], [94mLoss[0m : 2.47685
[1mStep[0m  [330/339], [94mLoss[0m : 2.65446

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10821
[1mStep[0m  [33/339], [94mLoss[0m : 2.24994
[1mStep[0m  [66/339], [94mLoss[0m : 2.08495
[1mStep[0m  [99/339], [94mLoss[0m : 2.16226
[1mStep[0m  [132/339], [94mLoss[0m : 2.14586
[1mStep[0m  [165/339], [94mLoss[0m : 1.99071
[1mStep[0m  [198/339], [94mLoss[0m : 2.19267
[1mStep[0m  [231/339], [94mLoss[0m : 2.52976
[1mStep[0m  [264/339], [94mLoss[0m : 2.25839
[1mStep[0m  [297/339], [94mLoss[0m : 2.47512
[1mStep[0m  [330/339], [94mLoss[0m : 2.37383

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08774
[1mStep[0m  [33/339], [94mLoss[0m : 2.56196
[1mStep[0m  [66/339], [94mLoss[0m : 2.23373
[1mStep[0m  [99/339], [94mLoss[0m : 2.32221
[1mStep[0m  [132/339], [94mLoss[0m : 2.23755
[1mStep[0m  [165/339], [94mLoss[0m : 2.52817
[1mStep[0m  [198/339], [94mLoss[0m : 2.06167
[1mStep[0m  [231/339], [94mLoss[0m : 2.32424
[1mStep[0m  [264/339], [94mLoss[0m : 2.05209
[1mStep[0m  [297/339], [94mLoss[0m : 1.83853
[1mStep[0m  [330/339], [94mLoss[0m : 2.27196

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12618
[1mStep[0m  [33/339], [94mLoss[0m : 2.00677
[1mStep[0m  [66/339], [94mLoss[0m : 1.96163
[1mStep[0m  [99/339], [94mLoss[0m : 2.73065
[1mStep[0m  [132/339], [94mLoss[0m : 2.34304
[1mStep[0m  [165/339], [94mLoss[0m : 2.65303
[1mStep[0m  [198/339], [94mLoss[0m : 2.53677
[1mStep[0m  [231/339], [94mLoss[0m : 2.10762
[1mStep[0m  [264/339], [94mLoss[0m : 2.12133
[1mStep[0m  [297/339], [94mLoss[0m : 1.97036
[1mStep[0m  [330/339], [94mLoss[0m : 2.54271

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87868
[1mStep[0m  [33/339], [94mLoss[0m : 2.00733
[1mStep[0m  [66/339], [94mLoss[0m : 3.06371
[1mStep[0m  [99/339], [94mLoss[0m : 2.17516
[1mStep[0m  [132/339], [94mLoss[0m : 1.89421
[1mStep[0m  [165/339], [94mLoss[0m : 1.92211
[1mStep[0m  [198/339], [94mLoss[0m : 2.54404
[1mStep[0m  [231/339], [94mLoss[0m : 1.88119
[1mStep[0m  [264/339], [94mLoss[0m : 1.73376
[1mStep[0m  [297/339], [94mLoss[0m : 2.41642
[1mStep[0m  [330/339], [94mLoss[0m : 1.56420

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.521, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75817
[1mStep[0m  [33/339], [94mLoss[0m : 2.72189
[1mStep[0m  [66/339], [94mLoss[0m : 2.06478
[1mStep[0m  [99/339], [94mLoss[0m : 2.39787
[1mStep[0m  [132/339], [94mLoss[0m : 1.79857
[1mStep[0m  [165/339], [94mLoss[0m : 1.94520
[1mStep[0m  [198/339], [94mLoss[0m : 2.80697
[1mStep[0m  [231/339], [94mLoss[0m : 2.06656
[1mStep[0m  [264/339], [94mLoss[0m : 1.80549
[1mStep[0m  [297/339], [94mLoss[0m : 2.05752
[1mStep[0m  [330/339], [94mLoss[0m : 2.13178

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89606
[1mStep[0m  [33/339], [94mLoss[0m : 1.34904
[1mStep[0m  [66/339], [94mLoss[0m : 1.91313
[1mStep[0m  [99/339], [94mLoss[0m : 1.90414
[1mStep[0m  [132/339], [94mLoss[0m : 1.61220
[1mStep[0m  [165/339], [94mLoss[0m : 1.93341
[1mStep[0m  [198/339], [94mLoss[0m : 1.98738
[1mStep[0m  [231/339], [94mLoss[0m : 2.03366
[1mStep[0m  [264/339], [94mLoss[0m : 2.40181
[1mStep[0m  [297/339], [94mLoss[0m : 1.64070
[1mStep[0m  [330/339], [94mLoss[0m : 1.91808

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.101, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25271
[1mStep[0m  [33/339], [94mLoss[0m : 2.15406
[1mStep[0m  [66/339], [94mLoss[0m : 2.18487
[1mStep[0m  [99/339], [94mLoss[0m : 2.09453
[1mStep[0m  [132/339], [94mLoss[0m : 2.61650
[1mStep[0m  [165/339], [94mLoss[0m : 2.53523
[1mStep[0m  [198/339], [94mLoss[0m : 2.11584
[1mStep[0m  [231/339], [94mLoss[0m : 2.14127
[1mStep[0m  [264/339], [94mLoss[0m : 2.03025
[1mStep[0m  [297/339], [94mLoss[0m : 2.20561
[1mStep[0m  [330/339], [94mLoss[0m : 1.93048

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.078, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72678
[1mStep[0m  [33/339], [94mLoss[0m : 2.53500
[1mStep[0m  [66/339], [94mLoss[0m : 1.55287
[1mStep[0m  [99/339], [94mLoss[0m : 2.22997
[1mStep[0m  [132/339], [94mLoss[0m : 1.99316
[1mStep[0m  [165/339], [94mLoss[0m : 2.00685
[1mStep[0m  [198/339], [94mLoss[0m : 1.94036
[1mStep[0m  [231/339], [94mLoss[0m : 2.09750
[1mStep[0m  [264/339], [94mLoss[0m : 1.57589
[1mStep[0m  [297/339], [94mLoss[0m : 2.16463
[1mStep[0m  [330/339], [94mLoss[0m : 2.09170

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66097
[1mStep[0m  [33/339], [94mLoss[0m : 2.33054
[1mStep[0m  [66/339], [94mLoss[0m : 2.08691
[1mStep[0m  [99/339], [94mLoss[0m : 2.18559
[1mStep[0m  [132/339], [94mLoss[0m : 2.32128
[1mStep[0m  [165/339], [94mLoss[0m : 1.96676
[1mStep[0m  [198/339], [94mLoss[0m : 2.95042
[1mStep[0m  [231/339], [94mLoss[0m : 2.03616
[1mStep[0m  [264/339], [94mLoss[0m : 2.39389
[1mStep[0m  [297/339], [94mLoss[0m : 2.05861
[1mStep[0m  [330/339], [94mLoss[0m : 1.96853

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06525
[1mStep[0m  [33/339], [94mLoss[0m : 1.81492
[1mStep[0m  [66/339], [94mLoss[0m : 1.56411
[1mStep[0m  [99/339], [94mLoss[0m : 2.09755
[1mStep[0m  [132/339], [94mLoss[0m : 2.10931
[1mStep[0m  [165/339], [94mLoss[0m : 1.77601
[1mStep[0m  [198/339], [94mLoss[0m : 1.67340
[1mStep[0m  [231/339], [94mLoss[0m : 2.01027
[1mStep[0m  [264/339], [94mLoss[0m : 2.38363
[1mStep[0m  [297/339], [94mLoss[0m : 2.13460
[1mStep[0m  [330/339], [94mLoss[0m : 2.18110

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.513, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56784
[1mStep[0m  [33/339], [94mLoss[0m : 2.38301
[1mStep[0m  [66/339], [94mLoss[0m : 1.81574
[1mStep[0m  [99/339], [94mLoss[0m : 2.42999
[1mStep[0m  [132/339], [94mLoss[0m : 1.61804
[1mStep[0m  [165/339], [94mLoss[0m : 1.99949
[1mStep[0m  [198/339], [94mLoss[0m : 2.29218
[1mStep[0m  [231/339], [94mLoss[0m : 1.91653
[1mStep[0m  [264/339], [94mLoss[0m : 1.95157
[1mStep[0m  [297/339], [94mLoss[0m : 1.72077
[1mStep[0m  [330/339], [94mLoss[0m : 2.11439

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85825
[1mStep[0m  [33/339], [94mLoss[0m : 2.80795
[1mStep[0m  [66/339], [94mLoss[0m : 2.04313
[1mStep[0m  [99/339], [94mLoss[0m : 2.27129
[1mStep[0m  [132/339], [94mLoss[0m : 2.12050
[1mStep[0m  [165/339], [94mLoss[0m : 1.85295
[1mStep[0m  [198/339], [94mLoss[0m : 1.64942
[1mStep[0m  [231/339], [94mLoss[0m : 1.37937
[1mStep[0m  [264/339], [94mLoss[0m : 2.28216
[1mStep[0m  [297/339], [94mLoss[0m : 1.78607
[1mStep[0m  [330/339], [94mLoss[0m : 2.17770

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36118
[1mStep[0m  [33/339], [94mLoss[0m : 1.99627
[1mStep[0m  [66/339], [94mLoss[0m : 1.76050
[1mStep[0m  [99/339], [94mLoss[0m : 1.42432
[1mStep[0m  [132/339], [94mLoss[0m : 1.72953
[1mStep[0m  [165/339], [94mLoss[0m : 1.88904
[1mStep[0m  [198/339], [94mLoss[0m : 1.77244
[1mStep[0m  [231/339], [94mLoss[0m : 1.94192
[1mStep[0m  [264/339], [94mLoss[0m : 2.55265
[1mStep[0m  [297/339], [94mLoss[0m : 2.07494
[1mStep[0m  [330/339], [94mLoss[0m : 1.85683

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.424, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18590
[1mStep[0m  [33/339], [94mLoss[0m : 2.01226
[1mStep[0m  [66/339], [94mLoss[0m : 2.12263
[1mStep[0m  [99/339], [94mLoss[0m : 2.05022
[1mStep[0m  [132/339], [94mLoss[0m : 2.28590
[1mStep[0m  [165/339], [94mLoss[0m : 1.58913
[1mStep[0m  [198/339], [94mLoss[0m : 1.93295
[1mStep[0m  [231/339], [94mLoss[0m : 1.86746
[1mStep[0m  [264/339], [94mLoss[0m : 1.67940
[1mStep[0m  [297/339], [94mLoss[0m : 1.67731
[1mStep[0m  [330/339], [94mLoss[0m : 1.65316

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.495, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.54666
[1mStep[0m  [33/339], [94mLoss[0m : 1.97254
[1mStep[0m  [66/339], [94mLoss[0m : 1.93281
[1mStep[0m  [99/339], [94mLoss[0m : 2.02929
[1mStep[0m  [132/339], [94mLoss[0m : 2.21899
[1mStep[0m  [165/339], [94mLoss[0m : 1.64187
[1mStep[0m  [198/339], [94mLoss[0m : 2.26855
[1mStep[0m  [231/339], [94mLoss[0m : 1.84101
[1mStep[0m  [264/339], [94mLoss[0m : 1.21583
[1mStep[0m  [297/339], [94mLoss[0m : 1.95784
[1mStep[0m  [330/339], [94mLoss[0m : 2.01171

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.522, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79100
[1mStep[0m  [33/339], [94mLoss[0m : 1.79047
[1mStep[0m  [66/339], [94mLoss[0m : 1.85506
[1mStep[0m  [99/339], [94mLoss[0m : 1.73040
[1mStep[0m  [132/339], [94mLoss[0m : 1.89936
[1mStep[0m  [165/339], [94mLoss[0m : 1.53989
[1mStep[0m  [198/339], [94mLoss[0m : 1.70321
[1mStep[0m  [231/339], [94mLoss[0m : 1.92413
[1mStep[0m  [264/339], [94mLoss[0m : 1.83034
[1mStep[0m  [297/339], [94mLoss[0m : 1.98161
[1mStep[0m  [330/339], [94mLoss[0m : 1.87940

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.850, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43960
[1mStep[0m  [33/339], [94mLoss[0m : 2.08744
[1mStep[0m  [66/339], [94mLoss[0m : 1.26759
[1mStep[0m  [99/339], [94mLoss[0m : 1.22454
[1mStep[0m  [132/339], [94mLoss[0m : 1.56630
[1mStep[0m  [165/339], [94mLoss[0m : 1.81463
[1mStep[0m  [198/339], [94mLoss[0m : 1.93147
[1mStep[0m  [231/339], [94mLoss[0m : 1.56488
[1mStep[0m  [264/339], [94mLoss[0m : 1.92230
[1mStep[0m  [297/339], [94mLoss[0m : 1.72661
[1mStep[0m  [330/339], [94mLoss[0m : 2.78441

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.515, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62219
[1mStep[0m  [33/339], [94mLoss[0m : 2.04969
[1mStep[0m  [66/339], [94mLoss[0m : 2.46781
[1mStep[0m  [99/339], [94mLoss[0m : 1.86776
[1mStep[0m  [132/339], [94mLoss[0m : 2.57217
[1mStep[0m  [165/339], [94mLoss[0m : 1.58497
[1mStep[0m  [198/339], [94mLoss[0m : 2.22082
[1mStep[0m  [231/339], [94mLoss[0m : 2.13470
[1mStep[0m  [264/339], [94mLoss[0m : 1.69588
[1mStep[0m  [297/339], [94mLoss[0m : 1.95738
[1mStep[0m  [330/339], [94mLoss[0m : 1.67960

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65229
[1mStep[0m  [33/339], [94mLoss[0m : 1.79360
[1mStep[0m  [66/339], [94mLoss[0m : 1.62834
[1mStep[0m  [99/339], [94mLoss[0m : 1.70435
[1mStep[0m  [132/339], [94mLoss[0m : 1.67320
[1mStep[0m  [165/339], [94mLoss[0m : 1.82431
[1mStep[0m  [198/339], [94mLoss[0m : 1.99376
[1mStep[0m  [231/339], [94mLoss[0m : 1.83346
[1mStep[0m  [264/339], [94mLoss[0m : 1.56261
[1mStep[0m  [297/339], [94mLoss[0m : 2.35298
[1mStep[0m  [330/339], [94mLoss[0m : 1.80930

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.536, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45211
[1mStep[0m  [33/339], [94mLoss[0m : 1.57631
[1mStep[0m  [66/339], [94mLoss[0m : 1.64992
[1mStep[0m  [99/339], [94mLoss[0m : 1.81829
[1mStep[0m  [132/339], [94mLoss[0m : 1.92503
[1mStep[0m  [165/339], [94mLoss[0m : 1.99773
[1mStep[0m  [198/339], [94mLoss[0m : 1.81881
[1mStep[0m  [231/339], [94mLoss[0m : 1.97493
[1mStep[0m  [264/339], [94mLoss[0m : 2.02945
[1mStep[0m  [297/339], [94mLoss[0m : 1.73482
[1mStep[0m  [330/339], [94mLoss[0m : 1.56209

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.513, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.499
====================================

Phase 2 - Evaluation MAE:  2.4985007406336015
MAE score P1        2.33151
MAE score P2       2.498501
loss               1.775659
learning_rate      0.002575
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 11.65996
[1mStep[0m  [33/339], [94mLoss[0m : 10.57125
[1mStep[0m  [66/339], [94mLoss[0m : 11.20946
[1mStep[0m  [99/339], [94mLoss[0m : 10.42609
[1mStep[0m  [132/339], [94mLoss[0m : 9.63072
[1mStep[0m  [165/339], [94mLoss[0m : 9.82349
[1mStep[0m  [198/339], [94mLoss[0m : 9.93800
[1mStep[0m  [231/339], [94mLoss[0m : 10.25978
[1mStep[0m  [264/339], [94mLoss[0m : 10.72737
[1mStep[0m  [297/339], [94mLoss[0m : 8.91696
[1mStep[0m  [330/339], [94mLoss[0m : 9.38062

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.145, [92mTest[0m: 10.922, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 10.16916
[1mStep[0m  [33/339], [94mLoss[0m : 9.92483
[1mStep[0m  [66/339], [94mLoss[0m : 9.04914
[1mStep[0m  [99/339], [94mLoss[0m : 9.41247
[1mStep[0m  [132/339], [94mLoss[0m : 8.16788
[1mStep[0m  [165/339], [94mLoss[0m : 8.85291
[1mStep[0m  [198/339], [94mLoss[0m : 9.31882
[1mStep[0m  [231/339], [94mLoss[0m : 7.59170
[1mStep[0m  [264/339], [94mLoss[0m : 7.86401
[1mStep[0m  [297/339], [94mLoss[0m : 6.52446
[1mStep[0m  [330/339], [94mLoss[0m : 6.87573

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.239, [92mTest[0m: 8.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 7.15712
[1mStep[0m  [33/339], [94mLoss[0m : 7.11669
[1mStep[0m  [66/339], [94mLoss[0m : 7.04262
[1mStep[0m  [99/339], [94mLoss[0m : 7.15489
[1mStep[0m  [132/339], [94mLoss[0m : 5.89366
[1mStep[0m  [165/339], [94mLoss[0m : 5.81799
[1mStep[0m  [198/339], [94mLoss[0m : 5.89777
[1mStep[0m  [231/339], [94mLoss[0m : 5.67519
[1mStep[0m  [264/339], [94mLoss[0m : 5.36016
[1mStep[0m  [297/339], [94mLoss[0m : 6.71326
[1mStep[0m  [330/339], [94mLoss[0m : 4.66643

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.200, [92mTest[0m: 6.323, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 5.32500
[1mStep[0m  [33/339], [94mLoss[0m : 4.56775
[1mStep[0m  [66/339], [94mLoss[0m : 4.22120
[1mStep[0m  [99/339], [94mLoss[0m : 3.52264
[1mStep[0m  [132/339], [94mLoss[0m : 4.90020
[1mStep[0m  [165/339], [94mLoss[0m : 4.65199
[1mStep[0m  [198/339], [94mLoss[0m : 3.26210
[1mStep[0m  [231/339], [94mLoss[0m : 4.35375
[1mStep[0m  [264/339], [94mLoss[0m : 2.66698
[1mStep[0m  [297/339], [94mLoss[0m : 2.91776
[1mStep[0m  [330/339], [94mLoss[0m : 3.26234

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.971, [92mTest[0m: 4.149, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.24495
[1mStep[0m  [33/339], [94mLoss[0m : 2.93978
[1mStep[0m  [66/339], [94mLoss[0m : 2.67409
[1mStep[0m  [99/339], [94mLoss[0m : 3.02649
[1mStep[0m  [132/339], [94mLoss[0m : 2.60793
[1mStep[0m  [165/339], [94mLoss[0m : 3.06137
[1mStep[0m  [198/339], [94mLoss[0m : 2.68784
[1mStep[0m  [231/339], [94mLoss[0m : 2.32726
[1mStep[0m  [264/339], [94mLoss[0m : 2.75014
[1mStep[0m  [297/339], [94mLoss[0m : 3.26055
[1mStep[0m  [330/339], [94mLoss[0m : 2.33157

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.837, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95950
[1mStep[0m  [33/339], [94mLoss[0m : 2.51828
[1mStep[0m  [66/339], [94mLoss[0m : 3.20620
[1mStep[0m  [99/339], [94mLoss[0m : 2.32709
[1mStep[0m  [132/339], [94mLoss[0m : 2.54092
[1mStep[0m  [165/339], [94mLoss[0m : 3.00681
[1mStep[0m  [198/339], [94mLoss[0m : 2.98003
[1mStep[0m  [231/339], [94mLoss[0m : 2.80738
[1mStep[0m  [264/339], [94mLoss[0m : 2.95181
[1mStep[0m  [297/339], [94mLoss[0m : 2.60879
[1mStep[0m  [330/339], [94mLoss[0m : 2.63348

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.733, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.11948
[1mStep[0m  [33/339], [94mLoss[0m : 3.07534
[1mStep[0m  [66/339], [94mLoss[0m : 2.72773
[1mStep[0m  [99/339], [94mLoss[0m : 2.72550
[1mStep[0m  [132/339], [94mLoss[0m : 2.83529
[1mStep[0m  [165/339], [94mLoss[0m : 2.00384
[1mStep[0m  [198/339], [94mLoss[0m : 2.59117
[1mStep[0m  [231/339], [94mLoss[0m : 2.47922
[1mStep[0m  [264/339], [94mLoss[0m : 2.65425
[1mStep[0m  [297/339], [94mLoss[0m : 3.10719
[1mStep[0m  [330/339], [94mLoss[0m : 2.65595

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.706, [92mTest[0m: 2.571, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11769
[1mStep[0m  [33/339], [94mLoss[0m : 3.35248
[1mStep[0m  [66/339], [94mLoss[0m : 2.02222
[1mStep[0m  [99/339], [94mLoss[0m : 2.47683
[1mStep[0m  [132/339], [94mLoss[0m : 2.35494
[1mStep[0m  [165/339], [94mLoss[0m : 2.74795
[1mStep[0m  [198/339], [94mLoss[0m : 3.21371
[1mStep[0m  [231/339], [94mLoss[0m : 2.37970
[1mStep[0m  [264/339], [94mLoss[0m : 2.32351
[1mStep[0m  [297/339], [94mLoss[0m : 2.84873
[1mStep[0m  [330/339], [94mLoss[0m : 2.59885

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79549
[1mStep[0m  [33/339], [94mLoss[0m : 2.47035
[1mStep[0m  [66/339], [94mLoss[0m : 2.52847
[1mStep[0m  [99/339], [94mLoss[0m : 4.15089
[1mStep[0m  [132/339], [94mLoss[0m : 2.61220
[1mStep[0m  [165/339], [94mLoss[0m : 2.56834
[1mStep[0m  [198/339], [94mLoss[0m : 2.46211
[1mStep[0m  [231/339], [94mLoss[0m : 2.45003
[1mStep[0m  [264/339], [94mLoss[0m : 2.76629
[1mStep[0m  [297/339], [94mLoss[0m : 2.65603
[1mStep[0m  [330/339], [94mLoss[0m : 2.24475

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.08411
[1mStep[0m  [33/339], [94mLoss[0m : 2.80444
[1mStep[0m  [66/339], [94mLoss[0m : 2.92415
[1mStep[0m  [99/339], [94mLoss[0m : 2.28476
[1mStep[0m  [132/339], [94mLoss[0m : 2.18350
[1mStep[0m  [165/339], [94mLoss[0m : 2.73870
[1mStep[0m  [198/339], [94mLoss[0m : 2.41344
[1mStep[0m  [231/339], [94mLoss[0m : 3.27558
[1mStep[0m  [264/339], [94mLoss[0m : 1.83254
[1mStep[0m  [297/339], [94mLoss[0m : 2.76980
[1mStep[0m  [330/339], [94mLoss[0m : 3.33148

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.443, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46979
[1mStep[0m  [33/339], [94mLoss[0m : 3.12021
[1mStep[0m  [66/339], [94mLoss[0m : 2.99684
[1mStep[0m  [99/339], [94mLoss[0m : 2.58584
[1mStep[0m  [132/339], [94mLoss[0m : 3.29165
[1mStep[0m  [165/339], [94mLoss[0m : 2.56105
[1mStep[0m  [198/339], [94mLoss[0m : 4.01957
[1mStep[0m  [231/339], [94mLoss[0m : 2.10078
[1mStep[0m  [264/339], [94mLoss[0m : 3.23839
[1mStep[0m  [297/339], [94mLoss[0m : 3.21681
[1mStep[0m  [330/339], [94mLoss[0m : 2.22764

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84144
[1mStep[0m  [33/339], [94mLoss[0m : 2.13795
[1mStep[0m  [66/339], [94mLoss[0m : 2.62118
[1mStep[0m  [99/339], [94mLoss[0m : 3.17546
[1mStep[0m  [132/339], [94mLoss[0m : 2.40830
[1mStep[0m  [165/339], [94mLoss[0m : 2.78597
[1mStep[0m  [198/339], [94mLoss[0m : 2.68505
[1mStep[0m  [231/339], [94mLoss[0m : 2.86340
[1mStep[0m  [264/339], [94mLoss[0m : 2.80772
[1mStep[0m  [297/339], [94mLoss[0m : 2.75145
[1mStep[0m  [330/339], [94mLoss[0m : 3.55766

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14662
[1mStep[0m  [33/339], [94mLoss[0m : 2.50372
[1mStep[0m  [66/339], [94mLoss[0m : 2.77014
[1mStep[0m  [99/339], [94mLoss[0m : 2.13520
[1mStep[0m  [132/339], [94mLoss[0m : 2.30093
[1mStep[0m  [165/339], [94mLoss[0m : 2.42971
[1mStep[0m  [198/339], [94mLoss[0m : 2.35516
[1mStep[0m  [231/339], [94mLoss[0m : 2.50675
[1mStep[0m  [264/339], [94mLoss[0m : 2.66725
[1mStep[0m  [297/339], [94mLoss[0m : 2.35337
[1mStep[0m  [330/339], [94mLoss[0m : 2.74172

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01741
[1mStep[0m  [33/339], [94mLoss[0m : 2.66478
[1mStep[0m  [66/339], [94mLoss[0m : 3.13041
[1mStep[0m  [99/339], [94mLoss[0m : 2.31832
[1mStep[0m  [132/339], [94mLoss[0m : 2.88815
[1mStep[0m  [165/339], [94mLoss[0m : 2.21882
[1mStep[0m  [198/339], [94mLoss[0m : 2.47301
[1mStep[0m  [231/339], [94mLoss[0m : 2.23458
[1mStep[0m  [264/339], [94mLoss[0m : 1.82700
[1mStep[0m  [297/339], [94mLoss[0m : 2.77500
[1mStep[0m  [330/339], [94mLoss[0m : 2.27801

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72294
[1mStep[0m  [33/339], [94mLoss[0m : 2.84835
[1mStep[0m  [66/339], [94mLoss[0m : 2.47571
[1mStep[0m  [99/339], [94mLoss[0m : 2.62870
[1mStep[0m  [132/339], [94mLoss[0m : 2.55674
[1mStep[0m  [165/339], [94mLoss[0m : 2.74746
[1mStep[0m  [198/339], [94mLoss[0m : 2.30875
[1mStep[0m  [231/339], [94mLoss[0m : 2.80579
[1mStep[0m  [264/339], [94mLoss[0m : 2.38044
[1mStep[0m  [297/339], [94mLoss[0m : 3.04396
[1mStep[0m  [330/339], [94mLoss[0m : 1.91037

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56152
[1mStep[0m  [33/339], [94mLoss[0m : 2.57205
[1mStep[0m  [66/339], [94mLoss[0m : 1.75441
[1mStep[0m  [99/339], [94mLoss[0m : 2.43348
[1mStep[0m  [132/339], [94mLoss[0m : 2.60234
[1mStep[0m  [165/339], [94mLoss[0m : 3.48672
[1mStep[0m  [198/339], [94mLoss[0m : 2.29602
[1mStep[0m  [231/339], [94mLoss[0m : 2.14482
[1mStep[0m  [264/339], [94mLoss[0m : 2.19913
[1mStep[0m  [297/339], [94mLoss[0m : 2.76727
[1mStep[0m  [330/339], [94mLoss[0m : 2.18288

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29033
[1mStep[0m  [33/339], [94mLoss[0m : 2.27989
[1mStep[0m  [66/339], [94mLoss[0m : 2.88820
[1mStep[0m  [99/339], [94mLoss[0m : 2.08120
[1mStep[0m  [132/339], [94mLoss[0m : 2.49108
[1mStep[0m  [165/339], [94mLoss[0m : 2.82874
[1mStep[0m  [198/339], [94mLoss[0m : 2.17682
[1mStep[0m  [231/339], [94mLoss[0m : 2.52248
[1mStep[0m  [264/339], [94mLoss[0m : 1.94100
[1mStep[0m  [297/339], [94mLoss[0m : 2.38653
[1mStep[0m  [330/339], [94mLoss[0m : 2.22139

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.89156
[1mStep[0m  [33/339], [94mLoss[0m : 2.55161
[1mStep[0m  [66/339], [94mLoss[0m : 2.25749
[1mStep[0m  [99/339], [94mLoss[0m : 2.22006
[1mStep[0m  [132/339], [94mLoss[0m : 2.27730
[1mStep[0m  [165/339], [94mLoss[0m : 2.71966
[1mStep[0m  [198/339], [94mLoss[0m : 2.38191
[1mStep[0m  [231/339], [94mLoss[0m : 2.91886
[1mStep[0m  [264/339], [94mLoss[0m : 1.93569
[1mStep[0m  [297/339], [94mLoss[0m : 2.89724
[1mStep[0m  [330/339], [94mLoss[0m : 2.66643

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51223
[1mStep[0m  [33/339], [94mLoss[0m : 2.03620
[1mStep[0m  [66/339], [94mLoss[0m : 3.05324
[1mStep[0m  [99/339], [94mLoss[0m : 2.23865
[1mStep[0m  [132/339], [94mLoss[0m : 2.18849
[1mStep[0m  [165/339], [94mLoss[0m : 2.48223
[1mStep[0m  [198/339], [94mLoss[0m : 2.64920
[1mStep[0m  [231/339], [94mLoss[0m : 2.19400
[1mStep[0m  [264/339], [94mLoss[0m : 2.03046
[1mStep[0m  [297/339], [94mLoss[0m : 2.31633
[1mStep[0m  [330/339], [94mLoss[0m : 3.35157

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81195
[1mStep[0m  [33/339], [94mLoss[0m : 1.80520
[1mStep[0m  [66/339], [94mLoss[0m : 2.55679
[1mStep[0m  [99/339], [94mLoss[0m : 2.10628
[1mStep[0m  [132/339], [94mLoss[0m : 2.08836
[1mStep[0m  [165/339], [94mLoss[0m : 1.91498
[1mStep[0m  [198/339], [94mLoss[0m : 3.88835
[1mStep[0m  [231/339], [94mLoss[0m : 2.74572
[1mStep[0m  [264/339], [94mLoss[0m : 1.97548
[1mStep[0m  [297/339], [94mLoss[0m : 2.68671
[1mStep[0m  [330/339], [94mLoss[0m : 3.03995

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.398, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43742
[1mStep[0m  [33/339], [94mLoss[0m : 2.38523
[1mStep[0m  [66/339], [94mLoss[0m : 3.37220
[1mStep[0m  [99/339], [94mLoss[0m : 2.42510
[1mStep[0m  [132/339], [94mLoss[0m : 2.42261
[1mStep[0m  [165/339], [94mLoss[0m : 2.20175
[1mStep[0m  [198/339], [94mLoss[0m : 2.75073
[1mStep[0m  [231/339], [94mLoss[0m : 2.07591
[1mStep[0m  [264/339], [94mLoss[0m : 2.10526
[1mStep[0m  [297/339], [94mLoss[0m : 3.32316
[1mStep[0m  [330/339], [94mLoss[0m : 1.73100

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84831
[1mStep[0m  [33/339], [94mLoss[0m : 2.28341
[1mStep[0m  [66/339], [94mLoss[0m : 2.72220
[1mStep[0m  [99/339], [94mLoss[0m : 2.02116
[1mStep[0m  [132/339], [94mLoss[0m : 2.31355
[1mStep[0m  [165/339], [94mLoss[0m : 2.37894
[1mStep[0m  [198/339], [94mLoss[0m : 2.01547
[1mStep[0m  [231/339], [94mLoss[0m : 3.07319
[1mStep[0m  [264/339], [94mLoss[0m : 2.01773
[1mStep[0m  [297/339], [94mLoss[0m : 2.39808
[1mStep[0m  [330/339], [94mLoss[0m : 2.22360

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30504
[1mStep[0m  [33/339], [94mLoss[0m : 2.05351
[1mStep[0m  [66/339], [94mLoss[0m : 2.44442
[1mStep[0m  [99/339], [94mLoss[0m : 2.60288
[1mStep[0m  [132/339], [94mLoss[0m : 2.29247
[1mStep[0m  [165/339], [94mLoss[0m : 3.55556
[1mStep[0m  [198/339], [94mLoss[0m : 2.16019
[1mStep[0m  [231/339], [94mLoss[0m : 2.55311
[1mStep[0m  [264/339], [94mLoss[0m : 2.96469
[1mStep[0m  [297/339], [94mLoss[0m : 3.17800
[1mStep[0m  [330/339], [94mLoss[0m : 2.14457

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58227
[1mStep[0m  [33/339], [94mLoss[0m : 2.41832
[1mStep[0m  [66/339], [94mLoss[0m : 2.04388
[1mStep[0m  [99/339], [94mLoss[0m : 2.48746
[1mStep[0m  [132/339], [94mLoss[0m : 3.40165
[1mStep[0m  [165/339], [94mLoss[0m : 2.56935
[1mStep[0m  [198/339], [94mLoss[0m : 2.24084
[1mStep[0m  [231/339], [94mLoss[0m : 2.19086
[1mStep[0m  [264/339], [94mLoss[0m : 2.16019
[1mStep[0m  [297/339], [94mLoss[0m : 2.97020
[1mStep[0m  [330/339], [94mLoss[0m : 2.94191

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52840
[1mStep[0m  [33/339], [94mLoss[0m : 2.40483
[1mStep[0m  [66/339], [94mLoss[0m : 2.89441
[1mStep[0m  [99/339], [94mLoss[0m : 2.43362
[1mStep[0m  [132/339], [94mLoss[0m : 2.62636
[1mStep[0m  [165/339], [94mLoss[0m : 1.78763
[1mStep[0m  [198/339], [94mLoss[0m : 2.25841
[1mStep[0m  [231/339], [94mLoss[0m : 1.93234
[1mStep[0m  [264/339], [94mLoss[0m : 2.41198
[1mStep[0m  [297/339], [94mLoss[0m : 2.06741
[1mStep[0m  [330/339], [94mLoss[0m : 2.68624

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.375, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46379
[1mStep[0m  [33/339], [94mLoss[0m : 2.45859
[1mStep[0m  [66/339], [94mLoss[0m : 2.35403
[1mStep[0m  [99/339], [94mLoss[0m : 1.67751
[1mStep[0m  [132/339], [94mLoss[0m : 1.94229
[1mStep[0m  [165/339], [94mLoss[0m : 2.46149
[1mStep[0m  [198/339], [94mLoss[0m : 3.06048
[1mStep[0m  [231/339], [94mLoss[0m : 2.32532
[1mStep[0m  [264/339], [94mLoss[0m : 2.87815
[1mStep[0m  [297/339], [94mLoss[0m : 2.30870
[1mStep[0m  [330/339], [94mLoss[0m : 2.43146

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.367, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29281
[1mStep[0m  [33/339], [94mLoss[0m : 1.81676
[1mStep[0m  [66/339], [94mLoss[0m : 3.06585
[1mStep[0m  [99/339], [94mLoss[0m : 2.83735
[1mStep[0m  [132/339], [94mLoss[0m : 2.84850
[1mStep[0m  [165/339], [94mLoss[0m : 3.32386
[1mStep[0m  [198/339], [94mLoss[0m : 2.60435
[1mStep[0m  [231/339], [94mLoss[0m : 2.44279
[1mStep[0m  [264/339], [94mLoss[0m : 2.45908
[1mStep[0m  [297/339], [94mLoss[0m : 2.20138
[1mStep[0m  [330/339], [94mLoss[0m : 2.46196

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.11960
[1mStep[0m  [33/339], [94mLoss[0m : 2.99017
[1mStep[0m  [66/339], [94mLoss[0m : 2.20619
[1mStep[0m  [99/339], [94mLoss[0m : 2.82801
[1mStep[0m  [132/339], [94mLoss[0m : 2.43826
[1mStep[0m  [165/339], [94mLoss[0m : 2.37497
[1mStep[0m  [198/339], [94mLoss[0m : 2.53483
[1mStep[0m  [231/339], [94mLoss[0m : 2.64502
[1mStep[0m  [264/339], [94mLoss[0m : 2.55863
[1mStep[0m  [297/339], [94mLoss[0m : 2.44359
[1mStep[0m  [330/339], [94mLoss[0m : 2.55603

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41737
[1mStep[0m  [33/339], [94mLoss[0m : 2.51878
[1mStep[0m  [66/339], [94mLoss[0m : 2.09701
[1mStep[0m  [99/339], [94mLoss[0m : 2.82878
[1mStep[0m  [132/339], [94mLoss[0m : 2.16999
[1mStep[0m  [165/339], [94mLoss[0m : 2.74900
[1mStep[0m  [198/339], [94mLoss[0m : 1.63661
[1mStep[0m  [231/339], [94mLoss[0m : 2.51046
[1mStep[0m  [264/339], [94mLoss[0m : 2.72861
[1mStep[0m  [297/339], [94mLoss[0m : 2.29756
[1mStep[0m  [330/339], [94mLoss[0m : 2.93479

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.373, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01817
[1mStep[0m  [33/339], [94mLoss[0m : 3.43770
[1mStep[0m  [66/339], [94mLoss[0m : 2.74196
[1mStep[0m  [99/339], [94mLoss[0m : 2.27108
[1mStep[0m  [132/339], [94mLoss[0m : 2.08750
[1mStep[0m  [165/339], [94mLoss[0m : 2.23967
[1mStep[0m  [198/339], [94mLoss[0m : 2.15206
[1mStep[0m  [231/339], [94mLoss[0m : 2.24169
[1mStep[0m  [264/339], [94mLoss[0m : 2.13251
[1mStep[0m  [297/339], [94mLoss[0m : 2.49548
[1mStep[0m  [330/339], [94mLoss[0m : 2.84707

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.356395129608897
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.78635
[1mStep[0m  [33/339], [94mLoss[0m : 2.64771
[1mStep[0m  [66/339], [94mLoss[0m : 2.88710
[1mStep[0m  [99/339], [94mLoss[0m : 2.82031
[1mStep[0m  [132/339], [94mLoss[0m : 2.41316
[1mStep[0m  [165/339], [94mLoss[0m : 2.46991
[1mStep[0m  [198/339], [94mLoss[0m : 2.23825
[1mStep[0m  [231/339], [94mLoss[0m : 3.32617
[1mStep[0m  [264/339], [94mLoss[0m : 2.38438
[1mStep[0m  [297/339], [94mLoss[0m : 2.46793
[1mStep[0m  [330/339], [94mLoss[0m : 2.48019

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11210
[1mStep[0m  [33/339], [94mLoss[0m : 2.63857
[1mStep[0m  [66/339], [94mLoss[0m : 3.30685
[1mStep[0m  [99/339], [94mLoss[0m : 2.16285
[1mStep[0m  [132/339], [94mLoss[0m : 2.40814
[1mStep[0m  [165/339], [94mLoss[0m : 3.28658
[1mStep[0m  [198/339], [94mLoss[0m : 2.08171
[1mStep[0m  [231/339], [94mLoss[0m : 2.02379
[1mStep[0m  [264/339], [94mLoss[0m : 1.96403
[1mStep[0m  [297/339], [94mLoss[0m : 2.41592
[1mStep[0m  [330/339], [94mLoss[0m : 2.35237

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.572, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25383
[1mStep[0m  [33/339], [94mLoss[0m : 2.22208
[1mStep[0m  [66/339], [94mLoss[0m : 2.27276
[1mStep[0m  [99/339], [94mLoss[0m : 1.73702
[1mStep[0m  [132/339], [94mLoss[0m : 1.84866
[1mStep[0m  [165/339], [94mLoss[0m : 2.43924
[1mStep[0m  [198/339], [94mLoss[0m : 2.43228
[1mStep[0m  [231/339], [94mLoss[0m : 1.85954
[1mStep[0m  [264/339], [94mLoss[0m : 2.37084
[1mStep[0m  [297/339], [94mLoss[0m : 2.73731
[1mStep[0m  [330/339], [94mLoss[0m : 1.84555

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.595, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33194
[1mStep[0m  [33/339], [94mLoss[0m : 1.97851
[1mStep[0m  [66/339], [94mLoss[0m : 2.56907
[1mStep[0m  [99/339], [94mLoss[0m : 2.44123
[1mStep[0m  [132/339], [94mLoss[0m : 1.63585
[1mStep[0m  [165/339], [94mLoss[0m : 2.33549
[1mStep[0m  [198/339], [94mLoss[0m : 2.35051
[1mStep[0m  [231/339], [94mLoss[0m : 2.09836
[1mStep[0m  [264/339], [94mLoss[0m : 2.43579
[1mStep[0m  [297/339], [94mLoss[0m : 1.56258
[1mStep[0m  [330/339], [94mLoss[0m : 2.64347

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.92481
[1mStep[0m  [33/339], [94mLoss[0m : 2.30285
[1mStep[0m  [66/339], [94mLoss[0m : 2.04122
[1mStep[0m  [99/339], [94mLoss[0m : 2.85581
[1mStep[0m  [132/339], [94mLoss[0m : 2.55800
[1mStep[0m  [165/339], [94mLoss[0m : 2.77960
[1mStep[0m  [198/339], [94mLoss[0m : 2.14813
[1mStep[0m  [231/339], [94mLoss[0m : 2.09787
[1mStep[0m  [264/339], [94mLoss[0m : 1.95828
[1mStep[0m  [297/339], [94mLoss[0m : 3.02873
[1mStep[0m  [330/339], [94mLoss[0m : 2.49137

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78985
[1mStep[0m  [33/339], [94mLoss[0m : 2.06489
[1mStep[0m  [66/339], [94mLoss[0m : 1.86218
[1mStep[0m  [99/339], [94mLoss[0m : 2.08391
[1mStep[0m  [132/339], [94mLoss[0m : 1.98929
[1mStep[0m  [165/339], [94mLoss[0m : 2.12194
[1mStep[0m  [198/339], [94mLoss[0m : 1.94093
[1mStep[0m  [231/339], [94mLoss[0m : 1.97738
[1mStep[0m  [264/339], [94mLoss[0m : 2.05312
[1mStep[0m  [297/339], [94mLoss[0m : 2.69528
[1mStep[0m  [330/339], [94mLoss[0m : 2.58473

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10002
[1mStep[0m  [33/339], [94mLoss[0m : 2.21212
[1mStep[0m  [66/339], [94mLoss[0m : 2.59190
[1mStep[0m  [99/339], [94mLoss[0m : 2.45068
[1mStep[0m  [132/339], [94mLoss[0m : 2.26194
[1mStep[0m  [165/339], [94mLoss[0m : 1.88428
[1mStep[0m  [198/339], [94mLoss[0m : 1.76342
[1mStep[0m  [231/339], [94mLoss[0m : 2.74899
[1mStep[0m  [264/339], [94mLoss[0m : 2.31011
[1mStep[0m  [297/339], [94mLoss[0m : 2.25660
[1mStep[0m  [330/339], [94mLoss[0m : 2.62689

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.554, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66289
[1mStep[0m  [33/339], [94mLoss[0m : 2.32813
[1mStep[0m  [66/339], [94mLoss[0m : 1.80826
[1mStep[0m  [99/339], [94mLoss[0m : 2.16853
[1mStep[0m  [132/339], [94mLoss[0m : 2.37635
[1mStep[0m  [165/339], [94mLoss[0m : 2.27590
[1mStep[0m  [198/339], [94mLoss[0m : 2.39250
[1mStep[0m  [231/339], [94mLoss[0m : 2.11387
[1mStep[0m  [264/339], [94mLoss[0m : 1.93157
[1mStep[0m  [297/339], [94mLoss[0m : 2.14258
[1mStep[0m  [330/339], [94mLoss[0m : 2.27244

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.610, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92001
[1mStep[0m  [33/339], [94mLoss[0m : 1.72248
[1mStep[0m  [66/339], [94mLoss[0m : 2.60966
[1mStep[0m  [99/339], [94mLoss[0m : 1.90964
[1mStep[0m  [132/339], [94mLoss[0m : 1.79135
[1mStep[0m  [165/339], [94mLoss[0m : 2.58444
[1mStep[0m  [198/339], [94mLoss[0m : 2.43446
[1mStep[0m  [231/339], [94mLoss[0m : 2.15993
[1mStep[0m  [264/339], [94mLoss[0m : 2.24120
[1mStep[0m  [297/339], [94mLoss[0m : 2.22212
[1mStep[0m  [330/339], [94mLoss[0m : 2.23793

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.653, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59118
[1mStep[0m  [33/339], [94mLoss[0m : 1.62741
[1mStep[0m  [66/339], [94mLoss[0m : 2.77608
[1mStep[0m  [99/339], [94mLoss[0m : 1.96922
[1mStep[0m  [132/339], [94mLoss[0m : 1.94182
[1mStep[0m  [165/339], [94mLoss[0m : 1.86412
[1mStep[0m  [198/339], [94mLoss[0m : 2.33767
[1mStep[0m  [231/339], [94mLoss[0m : 2.23183
[1mStep[0m  [264/339], [94mLoss[0m : 2.35946
[1mStep[0m  [297/339], [94mLoss[0m : 1.95894
[1mStep[0m  [330/339], [94mLoss[0m : 2.40960

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21188
[1mStep[0m  [33/339], [94mLoss[0m : 2.04551
[1mStep[0m  [66/339], [94mLoss[0m : 2.20458
[1mStep[0m  [99/339], [94mLoss[0m : 1.81137
[1mStep[0m  [132/339], [94mLoss[0m : 1.92929
[1mStep[0m  [165/339], [94mLoss[0m : 1.56239
[1mStep[0m  [198/339], [94mLoss[0m : 2.13170
[1mStep[0m  [231/339], [94mLoss[0m : 1.89627
[1mStep[0m  [264/339], [94mLoss[0m : 2.16232
[1mStep[0m  [297/339], [94mLoss[0m : 1.89722
[1mStep[0m  [330/339], [94mLoss[0m : 2.01302

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63486
[1mStep[0m  [33/339], [94mLoss[0m : 2.02736
[1mStep[0m  [66/339], [94mLoss[0m : 2.28646
[1mStep[0m  [99/339], [94mLoss[0m : 1.72326
[1mStep[0m  [132/339], [94mLoss[0m : 2.27031
[1mStep[0m  [165/339], [94mLoss[0m : 2.43457
[1mStep[0m  [198/339], [94mLoss[0m : 1.91005
[1mStep[0m  [231/339], [94mLoss[0m : 2.28649
[1mStep[0m  [264/339], [94mLoss[0m : 1.86538
[1mStep[0m  [297/339], [94mLoss[0m : 2.25828
[1mStep[0m  [330/339], [94mLoss[0m : 2.27113

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65202
[1mStep[0m  [33/339], [94mLoss[0m : 1.72202
[1mStep[0m  [66/339], [94mLoss[0m : 2.20830
[1mStep[0m  [99/339], [94mLoss[0m : 2.06526
[1mStep[0m  [132/339], [94mLoss[0m : 1.59283
[1mStep[0m  [165/339], [94mLoss[0m : 1.91863
[1mStep[0m  [198/339], [94mLoss[0m : 2.09859
[1mStep[0m  [231/339], [94mLoss[0m : 2.38623
[1mStep[0m  [264/339], [94mLoss[0m : 2.12924
[1mStep[0m  [297/339], [94mLoss[0m : 1.85259
[1mStep[0m  [330/339], [94mLoss[0m : 2.09631

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81941
[1mStep[0m  [33/339], [94mLoss[0m : 1.86562
[1mStep[0m  [66/339], [94mLoss[0m : 2.64028
[1mStep[0m  [99/339], [94mLoss[0m : 2.21404
[1mStep[0m  [132/339], [94mLoss[0m : 1.45480
[1mStep[0m  [165/339], [94mLoss[0m : 2.23632
[1mStep[0m  [198/339], [94mLoss[0m : 1.91735
[1mStep[0m  [231/339], [94mLoss[0m : 1.65252
[1mStep[0m  [264/339], [94mLoss[0m : 2.76029
[1mStep[0m  [297/339], [94mLoss[0m : 2.02008
[1mStep[0m  [330/339], [94mLoss[0m : 1.67076

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90345
[1mStep[0m  [33/339], [94mLoss[0m : 2.32965
[1mStep[0m  [66/339], [94mLoss[0m : 1.86314
[1mStep[0m  [99/339], [94mLoss[0m : 1.75484
[1mStep[0m  [132/339], [94mLoss[0m : 1.50732
[1mStep[0m  [165/339], [94mLoss[0m : 1.83608
[1mStep[0m  [198/339], [94mLoss[0m : 1.52010
[1mStep[0m  [231/339], [94mLoss[0m : 1.98658
[1mStep[0m  [264/339], [94mLoss[0m : 1.69965
[1mStep[0m  [297/339], [94mLoss[0m : 2.61877
[1mStep[0m  [330/339], [94mLoss[0m : 2.29451

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.978, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94240
[1mStep[0m  [33/339], [94mLoss[0m : 1.61098
[1mStep[0m  [66/339], [94mLoss[0m : 1.63089
[1mStep[0m  [99/339], [94mLoss[0m : 2.65873
[1mStep[0m  [132/339], [94mLoss[0m : 1.71596
[1mStep[0m  [165/339], [94mLoss[0m : 1.78515
[1mStep[0m  [198/339], [94mLoss[0m : 1.73512
[1mStep[0m  [231/339], [94mLoss[0m : 1.90147
[1mStep[0m  [264/339], [94mLoss[0m : 1.84775
[1mStep[0m  [297/339], [94mLoss[0m : 1.62729
[1mStep[0m  [330/339], [94mLoss[0m : 2.40051

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21276
[1mStep[0m  [33/339], [94mLoss[0m : 1.85920
[1mStep[0m  [66/339], [94mLoss[0m : 2.61714
[1mStep[0m  [99/339], [94mLoss[0m : 2.04197
[1mStep[0m  [132/339], [94mLoss[0m : 1.79166
[1mStep[0m  [165/339], [94mLoss[0m : 1.85107
[1mStep[0m  [198/339], [94mLoss[0m : 1.28898
[1mStep[0m  [231/339], [94mLoss[0m : 1.91323
[1mStep[0m  [264/339], [94mLoss[0m : 1.99741
[1mStep[0m  [297/339], [94mLoss[0m : 2.74909
[1mStep[0m  [330/339], [94mLoss[0m : 2.07417

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92590
[1mStep[0m  [33/339], [94mLoss[0m : 1.61142
[1mStep[0m  [66/339], [94mLoss[0m : 1.91435
[1mStep[0m  [99/339], [94mLoss[0m : 1.68192
[1mStep[0m  [132/339], [94mLoss[0m : 2.40329
[1mStep[0m  [165/339], [94mLoss[0m : 1.61357
[1mStep[0m  [198/339], [94mLoss[0m : 1.92848
[1mStep[0m  [231/339], [94mLoss[0m : 2.28490
[1mStep[0m  [264/339], [94mLoss[0m : 1.94247
[1mStep[0m  [297/339], [94mLoss[0m : 1.80621
[1mStep[0m  [330/339], [94mLoss[0m : 1.97012

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64787
[1mStep[0m  [33/339], [94mLoss[0m : 1.83285
[1mStep[0m  [66/339], [94mLoss[0m : 1.70095
[1mStep[0m  [99/339], [94mLoss[0m : 1.90334
[1mStep[0m  [132/339], [94mLoss[0m : 1.46789
[1mStep[0m  [165/339], [94mLoss[0m : 2.07267
[1mStep[0m  [198/339], [94mLoss[0m : 1.83708
[1mStep[0m  [231/339], [94mLoss[0m : 1.58202
[1mStep[0m  [264/339], [94mLoss[0m : 1.76153
[1mStep[0m  [297/339], [94mLoss[0m : 1.64396
[1mStep[0m  [330/339], [94mLoss[0m : 1.72209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.843, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.32045
[1mStep[0m  [33/339], [94mLoss[0m : 1.36144
[1mStep[0m  [66/339], [94mLoss[0m : 1.84558
[1mStep[0m  [99/339], [94mLoss[0m : 2.14001
[1mStep[0m  [132/339], [94mLoss[0m : 1.50306
[1mStep[0m  [165/339], [94mLoss[0m : 2.04784
[1mStep[0m  [198/339], [94mLoss[0m : 1.63717
[1mStep[0m  [231/339], [94mLoss[0m : 1.65930
[1mStep[0m  [264/339], [94mLoss[0m : 2.44215
[1mStep[0m  [297/339], [94mLoss[0m : 1.74054
[1mStep[0m  [330/339], [94mLoss[0m : 1.95173

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.564, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61053
[1mStep[0m  [33/339], [94mLoss[0m : 1.63069
[1mStep[0m  [66/339], [94mLoss[0m : 1.80739
[1mStep[0m  [99/339], [94mLoss[0m : 1.78866
[1mStep[0m  [132/339], [94mLoss[0m : 1.74496
[1mStep[0m  [165/339], [94mLoss[0m : 1.40732
[1mStep[0m  [198/339], [94mLoss[0m : 2.68919
[1mStep[0m  [231/339], [94mLoss[0m : 1.72864
[1mStep[0m  [264/339], [94mLoss[0m : 2.00308
[1mStep[0m  [297/339], [94mLoss[0m : 1.37928
[1mStep[0m  [330/339], [94mLoss[0m : 2.08471

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56711
[1mStep[0m  [33/339], [94mLoss[0m : 1.95563
[1mStep[0m  [66/339], [94mLoss[0m : 1.78518
[1mStep[0m  [99/339], [94mLoss[0m : 1.98307
[1mStep[0m  [132/339], [94mLoss[0m : 1.78590
[1mStep[0m  [165/339], [94mLoss[0m : 1.79371
[1mStep[0m  [198/339], [94mLoss[0m : 1.88773
[1mStep[0m  [231/339], [94mLoss[0m : 1.60407
[1mStep[0m  [264/339], [94mLoss[0m : 1.97649
[1mStep[0m  [297/339], [94mLoss[0m : 1.79840
[1mStep[0m  [330/339], [94mLoss[0m : 1.29243

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.547, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43271
[1mStep[0m  [33/339], [94mLoss[0m : 1.60014
[1mStep[0m  [66/339], [94mLoss[0m : 1.50720
[1mStep[0m  [99/339], [94mLoss[0m : 1.37291
[1mStep[0m  [132/339], [94mLoss[0m : 1.46368
[1mStep[0m  [165/339], [94mLoss[0m : 1.70213
[1mStep[0m  [198/339], [94mLoss[0m : 1.57858
[1mStep[0m  [231/339], [94mLoss[0m : 1.71108
[1mStep[0m  [264/339], [94mLoss[0m : 1.48316
[1mStep[0m  [297/339], [94mLoss[0m : 1.74921
[1mStep[0m  [330/339], [94mLoss[0m : 2.06419

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37294
[1mStep[0m  [33/339], [94mLoss[0m : 1.75748
[1mStep[0m  [66/339], [94mLoss[0m : 2.28660
[1mStep[0m  [99/339], [94mLoss[0m : 1.57589
[1mStep[0m  [132/339], [94mLoss[0m : 1.72565
[1mStep[0m  [165/339], [94mLoss[0m : 1.80919
[1mStep[0m  [198/339], [94mLoss[0m : 2.03377
[1mStep[0m  [231/339], [94mLoss[0m : 1.73936
[1mStep[0m  [264/339], [94mLoss[0m : 1.50846
[1mStep[0m  [297/339], [94mLoss[0m : 1.62004
[1mStep[0m  [330/339], [94mLoss[0m : 2.15160

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.687, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69024
[1mStep[0m  [33/339], [94mLoss[0m : 1.38728
[1mStep[0m  [66/339], [94mLoss[0m : 1.68900
[1mStep[0m  [99/339], [94mLoss[0m : 1.75967
[1mStep[0m  [132/339], [94mLoss[0m : 1.62955
[1mStep[0m  [165/339], [94mLoss[0m : 1.48549
[1mStep[0m  [198/339], [94mLoss[0m : 1.99598
[1mStep[0m  [231/339], [94mLoss[0m : 1.90493
[1mStep[0m  [264/339], [94mLoss[0m : 1.70324
[1mStep[0m  [297/339], [94mLoss[0m : 1.96665
[1mStep[0m  [330/339], [94mLoss[0m : 1.84853

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.588, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.25337
[1mStep[0m  [33/339], [94mLoss[0m : 1.63603
[1mStep[0m  [66/339], [94mLoss[0m : 1.68386
[1mStep[0m  [99/339], [94mLoss[0m : 1.88610
[1mStep[0m  [132/339], [94mLoss[0m : 1.66393
[1mStep[0m  [165/339], [94mLoss[0m : 1.37262
[1mStep[0m  [198/339], [94mLoss[0m : 1.31336
[1mStep[0m  [231/339], [94mLoss[0m : 1.74986
[1mStep[0m  [264/339], [94mLoss[0m : 1.80993
[1mStep[0m  [297/339], [94mLoss[0m : 2.23547
[1mStep[0m  [330/339], [94mLoss[0m : 2.23965

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.27855
[1mStep[0m  [33/339], [94mLoss[0m : 1.57876
[1mStep[0m  [66/339], [94mLoss[0m : 1.41396
[1mStep[0m  [99/339], [94mLoss[0m : 1.80286
[1mStep[0m  [132/339], [94mLoss[0m : 1.55889
[1mStep[0m  [165/339], [94mLoss[0m : 1.40495
[1mStep[0m  [198/339], [94mLoss[0m : 1.55396
[1mStep[0m  [231/339], [94mLoss[0m : 1.58813
[1mStep[0m  [264/339], [94mLoss[0m : 1.59096
[1mStep[0m  [297/339], [94mLoss[0m : 1.59630
[1mStep[0m  [330/339], [94mLoss[0m : 1.97108

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.555, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65445
[1mStep[0m  [33/339], [94mLoss[0m : 1.17598
[1mStep[0m  [66/339], [94mLoss[0m : 1.24086
[1mStep[0m  [99/339], [94mLoss[0m : 1.29837
[1mStep[0m  [132/339], [94mLoss[0m : 1.40812
[1mStep[0m  [165/339], [94mLoss[0m : 1.84494
[1mStep[0m  [198/339], [94mLoss[0m : 1.11613
[1mStep[0m  [231/339], [94mLoss[0m : 1.77969
[1mStep[0m  [264/339], [94mLoss[0m : 1.46953
[1mStep[0m  [297/339], [94mLoss[0m : 1.45974
[1mStep[0m  [330/339], [94mLoss[0m : 1.62630

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.536, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51935
[1mStep[0m  [33/339], [94mLoss[0m : 1.85640
[1mStep[0m  [66/339], [94mLoss[0m : 1.41128
[1mStep[0m  [99/339], [94mLoss[0m : 1.60012
[1mStep[0m  [132/339], [94mLoss[0m : 1.74899
[1mStep[0m  [165/339], [94mLoss[0m : 1.98051
[1mStep[0m  [198/339], [94mLoss[0m : 1.43977
[1mStep[0m  [231/339], [94mLoss[0m : 1.55547
[1mStep[0m  [264/339], [94mLoss[0m : 1.51041
[1mStep[0m  [297/339], [94mLoss[0m : 1.74667
[1mStep[0m  [330/339], [94mLoss[0m : 1.96867

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.610, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02864
[1mStep[0m  [33/339], [94mLoss[0m : 1.34132
[1mStep[0m  [66/339], [94mLoss[0m : 1.47330
[1mStep[0m  [99/339], [94mLoss[0m : 1.62999
[1mStep[0m  [132/339], [94mLoss[0m : 1.46645
[1mStep[0m  [165/339], [94mLoss[0m : 1.67345
[1mStep[0m  [198/339], [94mLoss[0m : 1.68032
[1mStep[0m  [231/339], [94mLoss[0m : 1.16400
[1mStep[0m  [264/339], [94mLoss[0m : 1.88460
[1mStep[0m  [297/339], [94mLoss[0m : 1.65072
[1mStep[0m  [330/339], [94mLoss[0m : 1.50190

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.610, [92mTest[0m: 2.569, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.544
====================================

Phase 2 - Evaluation MAE:  2.5437115346435952
MAE score P1      2.356395
MAE score P2      2.543712
loss              1.610073
learning_rate     0.002575
batch_size              32
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 5, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.72508
[1mStep[0m  [16/169], [94mLoss[0m : 8.41068
[1mStep[0m  [32/169], [94mLoss[0m : 6.78524
[1mStep[0m  [48/169], [94mLoss[0m : 6.38576
[1mStep[0m  [64/169], [94mLoss[0m : 3.84596
[1mStep[0m  [80/169], [94mLoss[0m : 3.41754
[1mStep[0m  [96/169], [94mLoss[0m : 2.87521
[1mStep[0m  [112/169], [94mLoss[0m : 3.11791
[1mStep[0m  [128/169], [94mLoss[0m : 2.32146
[1mStep[0m  [144/169], [94mLoss[0m : 2.85470
[1mStep[0m  [160/169], [94mLoss[0m : 2.66400

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.632, [92mTest[0m: 11.013, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31837
[1mStep[0m  [16/169], [94mLoss[0m : 2.42719
[1mStep[0m  [32/169], [94mLoss[0m : 2.54731
[1mStep[0m  [48/169], [94mLoss[0m : 2.09557
[1mStep[0m  [64/169], [94mLoss[0m : 2.28734
[1mStep[0m  [80/169], [94mLoss[0m : 2.80060
[1mStep[0m  [96/169], [94mLoss[0m : 2.64297
[1mStep[0m  [112/169], [94mLoss[0m : 2.78248
[1mStep[0m  [128/169], [94mLoss[0m : 2.19461
[1mStep[0m  [144/169], [94mLoss[0m : 3.10252
[1mStep[0m  [160/169], [94mLoss[0m : 2.22209

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.754, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64452
[1mStep[0m  [16/169], [94mLoss[0m : 2.15716
[1mStep[0m  [32/169], [94mLoss[0m : 2.22253
[1mStep[0m  [48/169], [94mLoss[0m : 2.14946
[1mStep[0m  [64/169], [94mLoss[0m : 2.63900
[1mStep[0m  [80/169], [94mLoss[0m : 2.24918
[1mStep[0m  [96/169], [94mLoss[0m : 2.48988
[1mStep[0m  [112/169], [94mLoss[0m : 2.36488
[1mStep[0m  [128/169], [94mLoss[0m : 2.51050
[1mStep[0m  [144/169], [94mLoss[0m : 2.96593
[1mStep[0m  [160/169], [94mLoss[0m : 2.57618

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.623, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76809
[1mStep[0m  [16/169], [94mLoss[0m : 2.33536
[1mStep[0m  [32/169], [94mLoss[0m : 2.58516
[1mStep[0m  [48/169], [94mLoss[0m : 2.84900
[1mStep[0m  [64/169], [94mLoss[0m : 2.39390
[1mStep[0m  [80/169], [94mLoss[0m : 2.92735
[1mStep[0m  [96/169], [94mLoss[0m : 2.45150
[1mStep[0m  [112/169], [94mLoss[0m : 2.95017
[1mStep[0m  [128/169], [94mLoss[0m : 2.49777
[1mStep[0m  [144/169], [94mLoss[0m : 2.57135
[1mStep[0m  [160/169], [94mLoss[0m : 2.53974

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49034
[1mStep[0m  [16/169], [94mLoss[0m : 2.70801
[1mStep[0m  [32/169], [94mLoss[0m : 2.25038
[1mStep[0m  [48/169], [94mLoss[0m : 2.53183
[1mStep[0m  [64/169], [94mLoss[0m : 2.98157
[1mStep[0m  [80/169], [94mLoss[0m : 2.80317
[1mStep[0m  [96/169], [94mLoss[0m : 2.46258
[1mStep[0m  [112/169], [94mLoss[0m : 2.27466
[1mStep[0m  [128/169], [94mLoss[0m : 2.34453
[1mStep[0m  [144/169], [94mLoss[0m : 2.64386
[1mStep[0m  [160/169], [94mLoss[0m : 2.43858

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.551, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60764
[1mStep[0m  [16/169], [94mLoss[0m : 2.73074
[1mStep[0m  [32/169], [94mLoss[0m : 2.18469
[1mStep[0m  [48/169], [94mLoss[0m : 2.87468
[1mStep[0m  [64/169], [94mLoss[0m : 2.36631
[1mStep[0m  [80/169], [94mLoss[0m : 2.16828
[1mStep[0m  [96/169], [94mLoss[0m : 1.89693
[1mStep[0m  [112/169], [94mLoss[0m : 2.08781
[1mStep[0m  [128/169], [94mLoss[0m : 2.52772
[1mStep[0m  [144/169], [94mLoss[0m : 2.44004
[1mStep[0m  [160/169], [94mLoss[0m : 2.17123

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59125
[1mStep[0m  [16/169], [94mLoss[0m : 2.40271
[1mStep[0m  [32/169], [94mLoss[0m : 2.36612
[1mStep[0m  [48/169], [94mLoss[0m : 2.95926
[1mStep[0m  [64/169], [94mLoss[0m : 2.38630
[1mStep[0m  [80/169], [94mLoss[0m : 2.61332
[1mStep[0m  [96/169], [94mLoss[0m : 2.63751
[1mStep[0m  [112/169], [94mLoss[0m : 2.34943
[1mStep[0m  [128/169], [94mLoss[0m : 2.37449
[1mStep[0m  [144/169], [94mLoss[0m : 2.40630
[1mStep[0m  [160/169], [94mLoss[0m : 2.58970

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83190
[1mStep[0m  [16/169], [94mLoss[0m : 2.27462
[1mStep[0m  [32/169], [94mLoss[0m : 2.65687
[1mStep[0m  [48/169], [94mLoss[0m : 2.58868
[1mStep[0m  [64/169], [94mLoss[0m : 2.35802
[1mStep[0m  [80/169], [94mLoss[0m : 2.29866
[1mStep[0m  [96/169], [94mLoss[0m : 2.35390
[1mStep[0m  [112/169], [94mLoss[0m : 2.85223
[1mStep[0m  [128/169], [94mLoss[0m : 2.66174
[1mStep[0m  [144/169], [94mLoss[0m : 2.77320
[1mStep[0m  [160/169], [94mLoss[0m : 2.34861

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71444
[1mStep[0m  [16/169], [94mLoss[0m : 2.43155
[1mStep[0m  [32/169], [94mLoss[0m : 2.41121
[1mStep[0m  [48/169], [94mLoss[0m : 2.36436
[1mStep[0m  [64/169], [94mLoss[0m : 2.13816
[1mStep[0m  [80/169], [94mLoss[0m : 2.46461
[1mStep[0m  [96/169], [94mLoss[0m : 2.49761
[1mStep[0m  [112/169], [94mLoss[0m : 2.52343
[1mStep[0m  [128/169], [94mLoss[0m : 2.66351
[1mStep[0m  [144/169], [94mLoss[0m : 2.20830
[1mStep[0m  [160/169], [94mLoss[0m : 2.61662

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25375
[1mStep[0m  [16/169], [94mLoss[0m : 2.34657
[1mStep[0m  [32/169], [94mLoss[0m : 2.69093
[1mStep[0m  [48/169], [94mLoss[0m : 2.29156
[1mStep[0m  [64/169], [94mLoss[0m : 2.39492
[1mStep[0m  [80/169], [94mLoss[0m : 2.67752
[1mStep[0m  [96/169], [94mLoss[0m : 2.48674
[1mStep[0m  [112/169], [94mLoss[0m : 2.72871
[1mStep[0m  [128/169], [94mLoss[0m : 2.51600
[1mStep[0m  [144/169], [94mLoss[0m : 2.34508
[1mStep[0m  [160/169], [94mLoss[0m : 2.52623

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21144
[1mStep[0m  [16/169], [94mLoss[0m : 2.73463
[1mStep[0m  [32/169], [94mLoss[0m : 2.38201
[1mStep[0m  [48/169], [94mLoss[0m : 2.53972
[1mStep[0m  [64/169], [94mLoss[0m : 2.04168
[1mStep[0m  [80/169], [94mLoss[0m : 2.55488
[1mStep[0m  [96/169], [94mLoss[0m : 2.30530
[1mStep[0m  [112/169], [94mLoss[0m : 2.58766
[1mStep[0m  [128/169], [94mLoss[0m : 2.14417
[1mStep[0m  [144/169], [94mLoss[0m : 2.45125
[1mStep[0m  [160/169], [94mLoss[0m : 1.96997

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.498, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28218
[1mStep[0m  [16/169], [94mLoss[0m : 2.23973
[1mStep[0m  [32/169], [94mLoss[0m : 2.60935
[1mStep[0m  [48/169], [94mLoss[0m : 2.29010
[1mStep[0m  [64/169], [94mLoss[0m : 2.34947
[1mStep[0m  [80/169], [94mLoss[0m : 2.42189
[1mStep[0m  [96/169], [94mLoss[0m : 2.54362
[1mStep[0m  [112/169], [94mLoss[0m : 2.42803
[1mStep[0m  [128/169], [94mLoss[0m : 2.34632
[1mStep[0m  [144/169], [94mLoss[0m : 2.32916
[1mStep[0m  [160/169], [94mLoss[0m : 2.10149

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07317
[1mStep[0m  [16/169], [94mLoss[0m : 2.48083
[1mStep[0m  [32/169], [94mLoss[0m : 2.29162
[1mStep[0m  [48/169], [94mLoss[0m : 2.24366
[1mStep[0m  [64/169], [94mLoss[0m : 2.51197
[1mStep[0m  [80/169], [94mLoss[0m : 2.36094
[1mStep[0m  [96/169], [94mLoss[0m : 2.38840
[1mStep[0m  [112/169], [94mLoss[0m : 2.23961
[1mStep[0m  [128/169], [94mLoss[0m : 2.16421
[1mStep[0m  [144/169], [94mLoss[0m : 2.29007
[1mStep[0m  [160/169], [94mLoss[0m : 2.61911

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.12036
[1mStep[0m  [16/169], [94mLoss[0m : 2.02386
[1mStep[0m  [32/169], [94mLoss[0m : 2.24265
[1mStep[0m  [48/169], [94mLoss[0m : 2.35280
[1mStep[0m  [64/169], [94mLoss[0m : 2.68437
[1mStep[0m  [80/169], [94mLoss[0m : 2.37015
[1mStep[0m  [96/169], [94mLoss[0m : 2.50323
[1mStep[0m  [112/169], [94mLoss[0m : 2.53914
[1mStep[0m  [128/169], [94mLoss[0m : 2.67592
[1mStep[0m  [144/169], [94mLoss[0m : 2.42084
[1mStep[0m  [160/169], [94mLoss[0m : 2.40215

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52694
[1mStep[0m  [16/169], [94mLoss[0m : 2.11309
[1mStep[0m  [32/169], [94mLoss[0m : 2.07558
[1mStep[0m  [48/169], [94mLoss[0m : 2.81873
[1mStep[0m  [64/169], [94mLoss[0m : 2.27496
[1mStep[0m  [80/169], [94mLoss[0m : 2.29712
[1mStep[0m  [96/169], [94mLoss[0m : 2.45238
[1mStep[0m  [112/169], [94mLoss[0m : 2.67052
[1mStep[0m  [128/169], [94mLoss[0m : 2.02584
[1mStep[0m  [144/169], [94mLoss[0m : 2.75583
[1mStep[0m  [160/169], [94mLoss[0m : 2.75651

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.519, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63858
[1mStep[0m  [16/169], [94mLoss[0m : 2.71492
[1mStep[0m  [32/169], [94mLoss[0m : 2.31968
[1mStep[0m  [48/169], [94mLoss[0m : 2.41085
[1mStep[0m  [64/169], [94mLoss[0m : 2.16996
[1mStep[0m  [80/169], [94mLoss[0m : 2.47069
[1mStep[0m  [96/169], [94mLoss[0m : 2.54251
[1mStep[0m  [112/169], [94mLoss[0m : 2.21566
[1mStep[0m  [128/169], [94mLoss[0m : 2.17992
[1mStep[0m  [144/169], [94mLoss[0m : 1.97973
[1mStep[0m  [160/169], [94mLoss[0m : 2.40469

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10707
[1mStep[0m  [16/169], [94mLoss[0m : 2.17531
[1mStep[0m  [32/169], [94mLoss[0m : 2.29243
[1mStep[0m  [48/169], [94mLoss[0m : 2.62732
[1mStep[0m  [64/169], [94mLoss[0m : 2.44982
[1mStep[0m  [80/169], [94mLoss[0m : 2.47188
[1mStep[0m  [96/169], [94mLoss[0m : 2.80075
[1mStep[0m  [112/169], [94mLoss[0m : 2.11579
[1mStep[0m  [128/169], [94mLoss[0m : 2.31051
[1mStep[0m  [144/169], [94mLoss[0m : 2.57912
[1mStep[0m  [160/169], [94mLoss[0m : 2.89685

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.484, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34658
[1mStep[0m  [16/169], [94mLoss[0m : 2.30200
[1mStep[0m  [32/169], [94mLoss[0m : 2.14140
[1mStep[0m  [48/169], [94mLoss[0m : 2.52687
[1mStep[0m  [64/169], [94mLoss[0m : 2.40845
[1mStep[0m  [80/169], [94mLoss[0m : 2.34293
[1mStep[0m  [96/169], [94mLoss[0m : 2.53754
[1mStep[0m  [112/169], [94mLoss[0m : 2.34845
[1mStep[0m  [128/169], [94mLoss[0m : 2.39710
[1mStep[0m  [144/169], [94mLoss[0m : 2.25945
[1mStep[0m  [160/169], [94mLoss[0m : 2.25225

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.533, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.93625
[1mStep[0m  [16/169], [94mLoss[0m : 2.37839
[1mStep[0m  [32/169], [94mLoss[0m : 1.82652
[1mStep[0m  [48/169], [94mLoss[0m : 2.88189
[1mStep[0m  [64/169], [94mLoss[0m : 2.03077
[1mStep[0m  [80/169], [94mLoss[0m : 2.07378
[1mStep[0m  [96/169], [94mLoss[0m : 2.27699
[1mStep[0m  [112/169], [94mLoss[0m : 2.15160
[1mStep[0m  [128/169], [94mLoss[0m : 2.32950
[1mStep[0m  [144/169], [94mLoss[0m : 2.47371
[1mStep[0m  [160/169], [94mLoss[0m : 2.58820

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95074
[1mStep[0m  [16/169], [94mLoss[0m : 2.47276
[1mStep[0m  [32/169], [94mLoss[0m : 2.35237
[1mStep[0m  [48/169], [94mLoss[0m : 2.13625
[1mStep[0m  [64/169], [94mLoss[0m : 2.21732
[1mStep[0m  [80/169], [94mLoss[0m : 2.24708
[1mStep[0m  [96/169], [94mLoss[0m : 2.36710
[1mStep[0m  [112/169], [94mLoss[0m : 2.42741
[1mStep[0m  [128/169], [94mLoss[0m : 2.05064
[1mStep[0m  [144/169], [94mLoss[0m : 2.40029
[1mStep[0m  [160/169], [94mLoss[0m : 3.08879

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22610
[1mStep[0m  [16/169], [94mLoss[0m : 2.46966
[1mStep[0m  [32/169], [94mLoss[0m : 2.56647
[1mStep[0m  [48/169], [94mLoss[0m : 2.19282
[1mStep[0m  [64/169], [94mLoss[0m : 2.20881
[1mStep[0m  [80/169], [94mLoss[0m : 2.30221
[1mStep[0m  [96/169], [94mLoss[0m : 3.04295
[1mStep[0m  [112/169], [94mLoss[0m : 2.61225
[1mStep[0m  [128/169], [94mLoss[0m : 2.28988
[1mStep[0m  [144/169], [94mLoss[0m : 2.53494
[1mStep[0m  [160/169], [94mLoss[0m : 2.24032

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34548
[1mStep[0m  [16/169], [94mLoss[0m : 2.18315
[1mStep[0m  [32/169], [94mLoss[0m : 2.67746
[1mStep[0m  [48/169], [94mLoss[0m : 2.81086
[1mStep[0m  [64/169], [94mLoss[0m : 2.44396
[1mStep[0m  [80/169], [94mLoss[0m : 2.17781
[1mStep[0m  [96/169], [94mLoss[0m : 2.02781
[1mStep[0m  [112/169], [94mLoss[0m : 2.00415
[1mStep[0m  [128/169], [94mLoss[0m : 2.09042
[1mStep[0m  [144/169], [94mLoss[0m : 2.66313
[1mStep[0m  [160/169], [94mLoss[0m : 2.33786

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88768
[1mStep[0m  [16/169], [94mLoss[0m : 2.22922
[1mStep[0m  [32/169], [94mLoss[0m : 1.88448
[1mStep[0m  [48/169], [94mLoss[0m : 2.80135
[1mStep[0m  [64/169], [94mLoss[0m : 2.55528
[1mStep[0m  [80/169], [94mLoss[0m : 1.89489
[1mStep[0m  [96/169], [94mLoss[0m : 2.04823
[1mStep[0m  [112/169], [94mLoss[0m : 2.84357
[1mStep[0m  [128/169], [94mLoss[0m : 2.51700
[1mStep[0m  [144/169], [94mLoss[0m : 2.31411
[1mStep[0m  [160/169], [94mLoss[0m : 2.36186

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.455, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18234
[1mStep[0m  [16/169], [94mLoss[0m : 2.87846
[1mStep[0m  [32/169], [94mLoss[0m : 2.35098
[1mStep[0m  [48/169], [94mLoss[0m : 2.20611
[1mStep[0m  [64/169], [94mLoss[0m : 2.10728
[1mStep[0m  [80/169], [94mLoss[0m : 2.17553
[1mStep[0m  [96/169], [94mLoss[0m : 2.24072
[1mStep[0m  [112/169], [94mLoss[0m : 2.15366
[1mStep[0m  [128/169], [94mLoss[0m : 2.85253
[1mStep[0m  [144/169], [94mLoss[0m : 2.61864
[1mStep[0m  [160/169], [94mLoss[0m : 2.21631

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38562
[1mStep[0m  [16/169], [94mLoss[0m : 2.11791
[1mStep[0m  [32/169], [94mLoss[0m : 2.37667
[1mStep[0m  [48/169], [94mLoss[0m : 2.20408
[1mStep[0m  [64/169], [94mLoss[0m : 2.59735
[1mStep[0m  [80/169], [94mLoss[0m : 2.24211
[1mStep[0m  [96/169], [94mLoss[0m : 2.20482
[1mStep[0m  [112/169], [94mLoss[0m : 2.52170
[1mStep[0m  [128/169], [94mLoss[0m : 2.59520
[1mStep[0m  [144/169], [94mLoss[0m : 2.41786
[1mStep[0m  [160/169], [94mLoss[0m : 2.39421

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73370
[1mStep[0m  [16/169], [94mLoss[0m : 2.23684
[1mStep[0m  [32/169], [94mLoss[0m : 2.43754
[1mStep[0m  [48/169], [94mLoss[0m : 2.77222
[1mStep[0m  [64/169], [94mLoss[0m : 2.38458
[1mStep[0m  [80/169], [94mLoss[0m : 2.22718
[1mStep[0m  [96/169], [94mLoss[0m : 2.41558
[1mStep[0m  [112/169], [94mLoss[0m : 2.65868
[1mStep[0m  [128/169], [94mLoss[0m : 2.75210
[1mStep[0m  [144/169], [94mLoss[0m : 2.52330
[1mStep[0m  [160/169], [94mLoss[0m : 2.40847

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.429, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84914
[1mStep[0m  [16/169], [94mLoss[0m : 2.44686
[1mStep[0m  [32/169], [94mLoss[0m : 2.61865
[1mStep[0m  [48/169], [94mLoss[0m : 2.45465
[1mStep[0m  [64/169], [94mLoss[0m : 2.26528
[1mStep[0m  [80/169], [94mLoss[0m : 2.33799
[1mStep[0m  [96/169], [94mLoss[0m : 2.50484
[1mStep[0m  [112/169], [94mLoss[0m : 2.36701
[1mStep[0m  [128/169], [94mLoss[0m : 2.45790
[1mStep[0m  [144/169], [94mLoss[0m : 2.73369
[1mStep[0m  [160/169], [94mLoss[0m : 2.28609

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31276
[1mStep[0m  [16/169], [94mLoss[0m : 1.93509
[1mStep[0m  [32/169], [94mLoss[0m : 2.48399
[1mStep[0m  [48/169], [94mLoss[0m : 2.17685
[1mStep[0m  [64/169], [94mLoss[0m : 2.01172
[1mStep[0m  [80/169], [94mLoss[0m : 2.61055
[1mStep[0m  [96/169], [94mLoss[0m : 1.94986
[1mStep[0m  [112/169], [94mLoss[0m : 2.50655
[1mStep[0m  [128/169], [94mLoss[0m : 2.80073
[1mStep[0m  [144/169], [94mLoss[0m : 2.34896
[1mStep[0m  [160/169], [94mLoss[0m : 2.77051

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13617
[1mStep[0m  [16/169], [94mLoss[0m : 2.62310
[1mStep[0m  [32/169], [94mLoss[0m : 2.24242
[1mStep[0m  [48/169], [94mLoss[0m : 2.29509
[1mStep[0m  [64/169], [94mLoss[0m : 1.95231
[1mStep[0m  [80/169], [94mLoss[0m : 2.51389
[1mStep[0m  [96/169], [94mLoss[0m : 2.67315
[1mStep[0m  [112/169], [94mLoss[0m : 2.49534
[1mStep[0m  [128/169], [94mLoss[0m : 2.60436
[1mStep[0m  [144/169], [94mLoss[0m : 2.34518
[1mStep[0m  [160/169], [94mLoss[0m : 2.67201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.431, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49954
[1mStep[0m  [16/169], [94mLoss[0m : 2.50694
[1mStep[0m  [32/169], [94mLoss[0m : 2.75076
[1mStep[0m  [48/169], [94mLoss[0m : 2.37015
[1mStep[0m  [64/169], [94mLoss[0m : 2.56982
[1mStep[0m  [80/169], [94mLoss[0m : 2.31123
[1mStep[0m  [96/169], [94mLoss[0m : 2.53706
[1mStep[0m  [112/169], [94mLoss[0m : 2.22717
[1mStep[0m  [128/169], [94mLoss[0m : 2.17821
[1mStep[0m  [144/169], [94mLoss[0m : 1.94017
[1mStep[0m  [160/169], [94mLoss[0m : 1.99596

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.469
====================================

Phase 1 - Evaluation MAE:  2.469454880271639
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.14258
[1mStep[0m  [16/169], [94mLoss[0m : 2.06731
[1mStep[0m  [32/169], [94mLoss[0m : 2.28650
[1mStep[0m  [48/169], [94mLoss[0m : 2.77597
[1mStep[0m  [64/169], [94mLoss[0m : 2.76775
[1mStep[0m  [80/169], [94mLoss[0m : 2.26640
[1mStep[0m  [96/169], [94mLoss[0m : 2.25113
[1mStep[0m  [112/169], [94mLoss[0m : 2.79194
[1mStep[0m  [128/169], [94mLoss[0m : 2.55175
[1mStep[0m  [144/169], [94mLoss[0m : 2.23356
[1mStep[0m  [160/169], [94mLoss[0m : 2.41599

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62955
[1mStep[0m  [16/169], [94mLoss[0m : 2.24717
[1mStep[0m  [32/169], [94mLoss[0m : 2.57663
[1mStep[0m  [48/169], [94mLoss[0m : 2.07686
[1mStep[0m  [64/169], [94mLoss[0m : 2.31623
[1mStep[0m  [80/169], [94mLoss[0m : 2.50674
[1mStep[0m  [96/169], [94mLoss[0m : 2.08536
[1mStep[0m  [112/169], [94mLoss[0m : 2.34178
[1mStep[0m  [128/169], [94mLoss[0m : 2.13674
[1mStep[0m  [144/169], [94mLoss[0m : 2.11881
[1mStep[0m  [160/169], [94mLoss[0m : 2.37056

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.522, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91242
[1mStep[0m  [16/169], [94mLoss[0m : 2.53041
[1mStep[0m  [32/169], [94mLoss[0m : 2.31955
[1mStep[0m  [48/169], [94mLoss[0m : 2.31165
[1mStep[0m  [64/169], [94mLoss[0m : 2.71117
[1mStep[0m  [80/169], [94mLoss[0m : 2.06520
[1mStep[0m  [96/169], [94mLoss[0m : 2.22023
[1mStep[0m  [112/169], [94mLoss[0m : 2.17881
[1mStep[0m  [128/169], [94mLoss[0m : 2.40014
[1mStep[0m  [144/169], [94mLoss[0m : 2.20007
[1mStep[0m  [160/169], [94mLoss[0m : 2.63990

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23847
[1mStep[0m  [16/169], [94mLoss[0m : 2.77915
[1mStep[0m  [32/169], [94mLoss[0m : 2.74107
[1mStep[0m  [48/169], [94mLoss[0m : 1.92722
[1mStep[0m  [64/169], [94mLoss[0m : 2.40857
[1mStep[0m  [80/169], [94mLoss[0m : 1.99759
[1mStep[0m  [96/169], [94mLoss[0m : 2.41307
[1mStep[0m  [112/169], [94mLoss[0m : 2.52360
[1mStep[0m  [128/169], [94mLoss[0m : 2.22770
[1mStep[0m  [144/169], [94mLoss[0m : 2.49426
[1mStep[0m  [160/169], [94mLoss[0m : 2.31269

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32126
[1mStep[0m  [16/169], [94mLoss[0m : 2.44790
[1mStep[0m  [32/169], [94mLoss[0m : 1.97455
[1mStep[0m  [48/169], [94mLoss[0m : 2.53680
[1mStep[0m  [64/169], [94mLoss[0m : 2.38754
[1mStep[0m  [80/169], [94mLoss[0m : 2.69409
[1mStep[0m  [96/169], [94mLoss[0m : 2.32392
[1mStep[0m  [112/169], [94mLoss[0m : 2.27710
[1mStep[0m  [128/169], [94mLoss[0m : 2.27809
[1mStep[0m  [144/169], [94mLoss[0m : 2.57044
[1mStep[0m  [160/169], [94mLoss[0m : 2.29367

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.237, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81965
[1mStep[0m  [16/169], [94mLoss[0m : 1.85999
[1mStep[0m  [32/169], [94mLoss[0m : 2.28488
[1mStep[0m  [48/169], [94mLoss[0m : 2.07562
[1mStep[0m  [64/169], [94mLoss[0m : 2.04134
[1mStep[0m  [80/169], [94mLoss[0m : 2.36932
[1mStep[0m  [96/169], [94mLoss[0m : 2.26362
[1mStep[0m  [112/169], [94mLoss[0m : 2.54343
[1mStep[0m  [128/169], [94mLoss[0m : 2.51543
[1mStep[0m  [144/169], [94mLoss[0m : 2.51197
[1mStep[0m  [160/169], [94mLoss[0m : 2.21311

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13186
[1mStep[0m  [16/169], [94mLoss[0m : 1.95265
[1mStep[0m  [32/169], [94mLoss[0m : 1.65485
[1mStep[0m  [48/169], [94mLoss[0m : 1.80778
[1mStep[0m  [64/169], [94mLoss[0m : 2.29414
[1mStep[0m  [80/169], [94mLoss[0m : 2.10841
[1mStep[0m  [96/169], [94mLoss[0m : 2.14566
[1mStep[0m  [112/169], [94mLoss[0m : 2.01521
[1mStep[0m  [128/169], [94mLoss[0m : 2.05022
[1mStep[0m  [144/169], [94mLoss[0m : 1.97188
[1mStep[0m  [160/169], [94mLoss[0m : 2.24554

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86667
[1mStep[0m  [16/169], [94mLoss[0m : 2.76852
[1mStep[0m  [32/169], [94mLoss[0m : 2.24977
[1mStep[0m  [48/169], [94mLoss[0m : 2.24267
[1mStep[0m  [64/169], [94mLoss[0m : 2.24087
[1mStep[0m  [80/169], [94mLoss[0m : 1.69251
[1mStep[0m  [96/169], [94mLoss[0m : 2.20824
[1mStep[0m  [112/169], [94mLoss[0m : 1.77944
[1mStep[0m  [128/169], [94mLoss[0m : 2.17321
[1mStep[0m  [144/169], [94mLoss[0m : 2.20490
[1mStep[0m  [160/169], [94mLoss[0m : 1.73497

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33084
[1mStep[0m  [16/169], [94mLoss[0m : 1.98136
[1mStep[0m  [32/169], [94mLoss[0m : 2.48769
[1mStep[0m  [48/169], [94mLoss[0m : 1.72521
[1mStep[0m  [64/169], [94mLoss[0m : 1.92623
[1mStep[0m  [80/169], [94mLoss[0m : 2.30528
[1mStep[0m  [96/169], [94mLoss[0m : 1.85493
[1mStep[0m  [112/169], [94mLoss[0m : 1.79352
[1mStep[0m  [128/169], [94mLoss[0m : 1.84618
[1mStep[0m  [144/169], [94mLoss[0m : 1.94847
[1mStep[0m  [160/169], [94mLoss[0m : 1.93366

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24069
[1mStep[0m  [16/169], [94mLoss[0m : 2.12345
[1mStep[0m  [32/169], [94mLoss[0m : 2.09571
[1mStep[0m  [48/169], [94mLoss[0m : 2.08762
[1mStep[0m  [64/169], [94mLoss[0m : 1.97702
[1mStep[0m  [80/169], [94mLoss[0m : 2.11683
[1mStep[0m  [96/169], [94mLoss[0m : 2.59920
[1mStep[0m  [112/169], [94mLoss[0m : 1.59680
[1mStep[0m  [128/169], [94mLoss[0m : 2.06072
[1mStep[0m  [144/169], [94mLoss[0m : 2.07640
[1mStep[0m  [160/169], [94mLoss[0m : 1.63044

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.040, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10702
[1mStep[0m  [16/169], [94mLoss[0m : 1.99132
[1mStep[0m  [32/169], [94mLoss[0m : 2.04890
[1mStep[0m  [48/169], [94mLoss[0m : 1.88812
[1mStep[0m  [64/169], [94mLoss[0m : 1.86479
[1mStep[0m  [80/169], [94mLoss[0m : 1.76675
[1mStep[0m  [96/169], [94mLoss[0m : 2.06273
[1mStep[0m  [112/169], [94mLoss[0m : 2.19218
[1mStep[0m  [128/169], [94mLoss[0m : 2.13665
[1mStep[0m  [144/169], [94mLoss[0m : 2.09834
[1mStep[0m  [160/169], [94mLoss[0m : 1.63498

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27090
[1mStep[0m  [16/169], [94mLoss[0m : 1.72961
[1mStep[0m  [32/169], [94mLoss[0m : 2.09624
[1mStep[0m  [48/169], [94mLoss[0m : 2.12394
[1mStep[0m  [64/169], [94mLoss[0m : 2.17432
[1mStep[0m  [80/169], [94mLoss[0m : 1.69332
[1mStep[0m  [96/169], [94mLoss[0m : 1.81657
[1mStep[0m  [112/169], [94mLoss[0m : 1.90363
[1mStep[0m  [128/169], [94mLoss[0m : 1.80723
[1mStep[0m  [144/169], [94mLoss[0m : 2.02613
[1mStep[0m  [160/169], [94mLoss[0m : 2.02924

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.80446
[1mStep[0m  [16/169], [94mLoss[0m : 1.93094
[1mStep[0m  [32/169], [94mLoss[0m : 1.86864
[1mStep[0m  [48/169], [94mLoss[0m : 2.06233
[1mStep[0m  [64/169], [94mLoss[0m : 2.04783
[1mStep[0m  [80/169], [94mLoss[0m : 2.22758
[1mStep[0m  [96/169], [94mLoss[0m : 1.93874
[1mStep[0m  [112/169], [94mLoss[0m : 1.75747
[1mStep[0m  [128/169], [94mLoss[0m : 1.86089
[1mStep[0m  [144/169], [94mLoss[0m : 1.90993
[1mStep[0m  [160/169], [94mLoss[0m : 1.96047

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69692
[1mStep[0m  [16/169], [94mLoss[0m : 1.69754
[1mStep[0m  [32/169], [94mLoss[0m : 1.75570
[1mStep[0m  [48/169], [94mLoss[0m : 2.04122
[1mStep[0m  [64/169], [94mLoss[0m : 1.77485
[1mStep[0m  [80/169], [94mLoss[0m : 1.81681
[1mStep[0m  [96/169], [94mLoss[0m : 1.98174
[1mStep[0m  [112/169], [94mLoss[0m : 1.74645
[1mStep[0m  [128/169], [94mLoss[0m : 2.03401
[1mStep[0m  [144/169], [94mLoss[0m : 1.94498
[1mStep[0m  [160/169], [94mLoss[0m : 2.11200

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17363
[1mStep[0m  [16/169], [94mLoss[0m : 1.96332
[1mStep[0m  [32/169], [94mLoss[0m : 1.67317
[1mStep[0m  [48/169], [94mLoss[0m : 1.97609
[1mStep[0m  [64/169], [94mLoss[0m : 1.75931
[1mStep[0m  [80/169], [94mLoss[0m : 1.72731
[1mStep[0m  [96/169], [94mLoss[0m : 1.70409
[1mStep[0m  [112/169], [94mLoss[0m : 1.85296
[1mStep[0m  [128/169], [94mLoss[0m : 2.11000
[1mStep[0m  [144/169], [94mLoss[0m : 1.67903
[1mStep[0m  [160/169], [94mLoss[0m : 1.67795

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53289
[1mStep[0m  [16/169], [94mLoss[0m : 1.76596
[1mStep[0m  [32/169], [94mLoss[0m : 2.19435
[1mStep[0m  [48/169], [94mLoss[0m : 1.84349
[1mStep[0m  [64/169], [94mLoss[0m : 1.72119
[1mStep[0m  [80/169], [94mLoss[0m : 2.10987
[1mStep[0m  [96/169], [94mLoss[0m : 1.93618
[1mStep[0m  [112/169], [94mLoss[0m : 2.29776
[1mStep[0m  [128/169], [94mLoss[0m : 1.62749
[1mStep[0m  [144/169], [94mLoss[0m : 1.74396
[1mStep[0m  [160/169], [94mLoss[0m : 2.51070

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.512, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92908
[1mStep[0m  [16/169], [94mLoss[0m : 1.53393
[1mStep[0m  [32/169], [94mLoss[0m : 1.65136
[1mStep[0m  [48/169], [94mLoss[0m : 2.03804
[1mStep[0m  [64/169], [94mLoss[0m : 1.99739
[1mStep[0m  [80/169], [94mLoss[0m : 1.84952
[1mStep[0m  [96/169], [94mLoss[0m : 1.47252
[1mStep[0m  [112/169], [94mLoss[0m : 1.48100
[1mStep[0m  [128/169], [94mLoss[0m : 2.09750
[1mStep[0m  [144/169], [94mLoss[0m : 1.89607
[1mStep[0m  [160/169], [94mLoss[0m : 1.70507

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.546, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86413
[1mStep[0m  [16/169], [94mLoss[0m : 1.74977
[1mStep[0m  [32/169], [94mLoss[0m : 1.38179
[1mStep[0m  [48/169], [94mLoss[0m : 1.81453
[1mStep[0m  [64/169], [94mLoss[0m : 2.01502
[1mStep[0m  [80/169], [94mLoss[0m : 2.01420
[1mStep[0m  [96/169], [94mLoss[0m : 1.72231
[1mStep[0m  [112/169], [94mLoss[0m : 1.76195
[1mStep[0m  [128/169], [94mLoss[0m : 1.54992
[1mStep[0m  [144/169], [94mLoss[0m : 1.72201
[1mStep[0m  [160/169], [94mLoss[0m : 2.10913

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.748, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89254
[1mStep[0m  [16/169], [94mLoss[0m : 1.80638
[1mStep[0m  [32/169], [94mLoss[0m : 1.67304
[1mStep[0m  [48/169], [94mLoss[0m : 1.85431
[1mStep[0m  [64/169], [94mLoss[0m : 1.42023
[1mStep[0m  [80/169], [94mLoss[0m : 1.78861
[1mStep[0m  [96/169], [94mLoss[0m : 1.78565
[1mStep[0m  [112/169], [94mLoss[0m : 1.68503
[1mStep[0m  [128/169], [94mLoss[0m : 1.59454
[1mStep[0m  [144/169], [94mLoss[0m : 1.80723
[1mStep[0m  [160/169], [94mLoss[0m : 1.74094

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32782
[1mStep[0m  [16/169], [94mLoss[0m : 1.45227
[1mStep[0m  [32/169], [94mLoss[0m : 1.71276
[1mStep[0m  [48/169], [94mLoss[0m : 1.64578
[1mStep[0m  [64/169], [94mLoss[0m : 1.76554
[1mStep[0m  [80/169], [94mLoss[0m : 1.51725
[1mStep[0m  [96/169], [94mLoss[0m : 1.92647
[1mStep[0m  [112/169], [94mLoss[0m : 1.76722
[1mStep[0m  [128/169], [94mLoss[0m : 1.92820
[1mStep[0m  [144/169], [94mLoss[0m : 1.93373
[1mStep[0m  [160/169], [94mLoss[0m : 1.87555

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88711
[1mStep[0m  [16/169], [94mLoss[0m : 1.85176
[1mStep[0m  [32/169], [94mLoss[0m : 1.44269
[1mStep[0m  [48/169], [94mLoss[0m : 2.14285
[1mStep[0m  [64/169], [94mLoss[0m : 1.39500
[1mStep[0m  [80/169], [94mLoss[0m : 1.77879
[1mStep[0m  [96/169], [94mLoss[0m : 1.68237
[1mStep[0m  [112/169], [94mLoss[0m : 1.39752
[1mStep[0m  [128/169], [94mLoss[0m : 1.70785
[1mStep[0m  [144/169], [94mLoss[0m : 1.62951
[1mStep[0m  [160/169], [94mLoss[0m : 1.84198

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.652, [92mTest[0m: 2.533, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74076
[1mStep[0m  [16/169], [94mLoss[0m : 1.62715
[1mStep[0m  [32/169], [94mLoss[0m : 1.96835
[1mStep[0m  [48/169], [94mLoss[0m : 1.50517
[1mStep[0m  [64/169], [94mLoss[0m : 1.38952
[1mStep[0m  [80/169], [94mLoss[0m : 1.65703
[1mStep[0m  [96/169], [94mLoss[0m : 1.21245
[1mStep[0m  [112/169], [94mLoss[0m : 1.81394
[1mStep[0m  [128/169], [94mLoss[0m : 1.48645
[1mStep[0m  [144/169], [94mLoss[0m : 1.38974
[1mStep[0m  [160/169], [94mLoss[0m : 1.58481

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.26330
[1mStep[0m  [16/169], [94mLoss[0m : 1.46397
[1mStep[0m  [32/169], [94mLoss[0m : 1.50945
[1mStep[0m  [48/169], [94mLoss[0m : 1.63860
[1mStep[0m  [64/169], [94mLoss[0m : 1.44485
[1mStep[0m  [80/169], [94mLoss[0m : 1.55851
[1mStep[0m  [96/169], [94mLoss[0m : 1.48539
[1mStep[0m  [112/169], [94mLoss[0m : 1.52711
[1mStep[0m  [128/169], [94mLoss[0m : 1.92486
[1mStep[0m  [144/169], [94mLoss[0m : 1.49173
[1mStep[0m  [160/169], [94mLoss[0m : 1.53269

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42788
[1mStep[0m  [16/169], [94mLoss[0m : 1.31879
[1mStep[0m  [32/169], [94mLoss[0m : 1.67399
[1mStep[0m  [48/169], [94mLoss[0m : 1.82328
[1mStep[0m  [64/169], [94mLoss[0m : 1.88465
[1mStep[0m  [80/169], [94mLoss[0m : 1.35806
[1mStep[0m  [96/169], [94mLoss[0m : 1.75917
[1mStep[0m  [112/169], [94mLoss[0m : 1.63563
[1mStep[0m  [128/169], [94mLoss[0m : 1.37610
[1mStep[0m  [144/169], [94mLoss[0m : 1.74929
[1mStep[0m  [160/169], [94mLoss[0m : 1.56412

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.600, [92mTest[0m: 2.515, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53652
[1mStep[0m  [16/169], [94mLoss[0m : 1.45747
[1mStep[0m  [32/169], [94mLoss[0m : 1.44337
[1mStep[0m  [48/169], [94mLoss[0m : 1.90823
[1mStep[0m  [64/169], [94mLoss[0m : 1.75473
[1mStep[0m  [80/169], [94mLoss[0m : 1.94344
[1mStep[0m  [96/169], [94mLoss[0m : 1.54044
[1mStep[0m  [112/169], [94mLoss[0m : 1.71736
[1mStep[0m  [128/169], [94mLoss[0m : 1.56950
[1mStep[0m  [144/169], [94mLoss[0m : 1.60681
[1mStep[0m  [160/169], [94mLoss[0m : 1.78151

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.570, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47720
[1mStep[0m  [16/169], [94mLoss[0m : 1.51605
[1mStep[0m  [32/169], [94mLoss[0m : 1.41740
[1mStep[0m  [48/169], [94mLoss[0m : 1.53286
[1mStep[0m  [64/169], [94mLoss[0m : 1.53562
[1mStep[0m  [80/169], [94mLoss[0m : 2.07439
[1mStep[0m  [96/169], [94mLoss[0m : 1.57925
[1mStep[0m  [112/169], [94mLoss[0m : 1.49533
[1mStep[0m  [128/169], [94mLoss[0m : 1.67116
[1mStep[0m  [144/169], [94mLoss[0m : 1.21354
[1mStep[0m  [160/169], [94mLoss[0m : 1.50854

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.583, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57691
[1mStep[0m  [16/169], [94mLoss[0m : 1.14408
[1mStep[0m  [32/169], [94mLoss[0m : 1.38459
[1mStep[0m  [48/169], [94mLoss[0m : 1.48505
[1mStep[0m  [64/169], [94mLoss[0m : 1.47916
[1mStep[0m  [80/169], [94mLoss[0m : 1.44711
[1mStep[0m  [96/169], [94mLoss[0m : 1.71369
[1mStep[0m  [112/169], [94mLoss[0m : 1.58789
[1mStep[0m  [128/169], [94mLoss[0m : 1.66556
[1mStep[0m  [144/169], [94mLoss[0m : 1.26956
[1mStep[0m  [160/169], [94mLoss[0m : 1.24900

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.549, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74766
[1mStep[0m  [16/169], [94mLoss[0m : 1.65927
[1mStep[0m  [32/169], [94mLoss[0m : 1.50462
[1mStep[0m  [48/169], [94mLoss[0m : 1.61433
[1mStep[0m  [64/169], [94mLoss[0m : 1.56391
[1mStep[0m  [80/169], [94mLoss[0m : 1.61495
[1mStep[0m  [96/169], [94mLoss[0m : 1.35570
[1mStep[0m  [112/169], [94mLoss[0m : 1.51135
[1mStep[0m  [128/169], [94mLoss[0m : 1.56942
[1mStep[0m  [144/169], [94mLoss[0m : 1.23154
[1mStep[0m  [160/169], [94mLoss[0m : 1.77441

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.616, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.43454
[1mStep[0m  [16/169], [94mLoss[0m : 1.68201
[1mStep[0m  [32/169], [94mLoss[0m : 1.27534
[1mStep[0m  [48/169], [94mLoss[0m : 1.54230
[1mStep[0m  [64/169], [94mLoss[0m : 1.60709
[1mStep[0m  [80/169], [94mLoss[0m : 1.29873
[1mStep[0m  [96/169], [94mLoss[0m : 1.60344
[1mStep[0m  [112/169], [94mLoss[0m : 1.34426
[1mStep[0m  [128/169], [94mLoss[0m : 1.48352
[1mStep[0m  [144/169], [94mLoss[0m : 1.28168
[1mStep[0m  [160/169], [94mLoss[0m : 1.51695

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.494, [92mTest[0m: 2.584, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.610
====================================

Phase 2 - Evaluation MAE:  2.609643122979573
MAE score P1      2.469455
MAE score P2      2.609643
loss              1.493995
learning_rate     0.002575
batch_size              64
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.50402
[1mStep[0m  [33/339], [94mLoss[0m : 11.09031
[1mStep[0m  [66/339], [94mLoss[0m : 8.97875
[1mStep[0m  [99/339], [94mLoss[0m : 8.06022
[1mStep[0m  [132/339], [94mLoss[0m : 5.31352
[1mStep[0m  [165/339], [94mLoss[0m : 5.45092
[1mStep[0m  [198/339], [94mLoss[0m : 3.93474
[1mStep[0m  [231/339], [94mLoss[0m : 3.95005
[1mStep[0m  [264/339], [94mLoss[0m : 2.67421
[1mStep[0m  [297/339], [94mLoss[0m : 2.80680
[1mStep[0m  [330/339], [94mLoss[0m : 2.38936

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.524, [92mTest[0m: 10.949, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76101
[1mStep[0m  [33/339], [94mLoss[0m : 2.85280
[1mStep[0m  [66/339], [94mLoss[0m : 2.52472
[1mStep[0m  [99/339], [94mLoss[0m : 2.41324
[1mStep[0m  [132/339], [94mLoss[0m : 2.50354
[1mStep[0m  [165/339], [94mLoss[0m : 2.32359
[1mStep[0m  [198/339], [94mLoss[0m : 2.42371
[1mStep[0m  [231/339], [94mLoss[0m : 2.41939
[1mStep[0m  [264/339], [94mLoss[0m : 2.88758
[1mStep[0m  [297/339], [94mLoss[0m : 2.71630
[1mStep[0m  [330/339], [94mLoss[0m : 2.21674

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55645
[1mStep[0m  [33/339], [94mLoss[0m : 2.45351
[1mStep[0m  [66/339], [94mLoss[0m : 2.79350
[1mStep[0m  [99/339], [94mLoss[0m : 2.65234
[1mStep[0m  [132/339], [94mLoss[0m : 2.38071
[1mStep[0m  [165/339], [94mLoss[0m : 2.64680
[1mStep[0m  [198/339], [94mLoss[0m : 2.38337
[1mStep[0m  [231/339], [94mLoss[0m : 2.23864
[1mStep[0m  [264/339], [94mLoss[0m : 2.53845
[1mStep[0m  [297/339], [94mLoss[0m : 2.77655
[1mStep[0m  [330/339], [94mLoss[0m : 2.06194

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17910
[1mStep[0m  [33/339], [94mLoss[0m : 2.49131
[1mStep[0m  [66/339], [94mLoss[0m : 2.07844
[1mStep[0m  [99/339], [94mLoss[0m : 2.60723
[1mStep[0m  [132/339], [94mLoss[0m : 2.38916
[1mStep[0m  [165/339], [94mLoss[0m : 2.59216
[1mStep[0m  [198/339], [94mLoss[0m : 2.61292
[1mStep[0m  [231/339], [94mLoss[0m : 2.96984
[1mStep[0m  [264/339], [94mLoss[0m : 2.13313
[1mStep[0m  [297/339], [94mLoss[0m : 1.83153
[1mStep[0m  [330/339], [94mLoss[0m : 2.34885

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67781
[1mStep[0m  [33/339], [94mLoss[0m : 3.07129
[1mStep[0m  [66/339], [94mLoss[0m : 3.02152
[1mStep[0m  [99/339], [94mLoss[0m : 2.13306
[1mStep[0m  [132/339], [94mLoss[0m : 2.31836
[1mStep[0m  [165/339], [94mLoss[0m : 2.71260
[1mStep[0m  [198/339], [94mLoss[0m : 3.11622
[1mStep[0m  [231/339], [94mLoss[0m : 2.61157
[1mStep[0m  [264/339], [94mLoss[0m : 2.96765
[1mStep[0m  [297/339], [94mLoss[0m : 2.97151
[1mStep[0m  [330/339], [94mLoss[0m : 2.82363

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95658
[1mStep[0m  [33/339], [94mLoss[0m : 1.94087
[1mStep[0m  [66/339], [94mLoss[0m : 3.59027
[1mStep[0m  [99/339], [94mLoss[0m : 2.40585
[1mStep[0m  [132/339], [94mLoss[0m : 2.10665
[1mStep[0m  [165/339], [94mLoss[0m : 3.33586
[1mStep[0m  [198/339], [94mLoss[0m : 2.60564
[1mStep[0m  [231/339], [94mLoss[0m : 2.82947
[1mStep[0m  [264/339], [94mLoss[0m : 2.11255
[1mStep[0m  [297/339], [94mLoss[0m : 2.81346
[1mStep[0m  [330/339], [94mLoss[0m : 2.59511

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55537
[1mStep[0m  [33/339], [94mLoss[0m : 3.03554
[1mStep[0m  [66/339], [94mLoss[0m : 2.44861
[1mStep[0m  [99/339], [94mLoss[0m : 2.65431
[1mStep[0m  [132/339], [94mLoss[0m : 2.27193
[1mStep[0m  [165/339], [94mLoss[0m : 1.80634
[1mStep[0m  [198/339], [94mLoss[0m : 2.29069
[1mStep[0m  [231/339], [94mLoss[0m : 2.62653
[1mStep[0m  [264/339], [94mLoss[0m : 2.13334
[1mStep[0m  [297/339], [94mLoss[0m : 3.27301
[1mStep[0m  [330/339], [94mLoss[0m : 2.67277

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08763
[1mStep[0m  [33/339], [94mLoss[0m : 2.99794
[1mStep[0m  [66/339], [94mLoss[0m : 2.40239
[1mStep[0m  [99/339], [94mLoss[0m : 1.99481
[1mStep[0m  [132/339], [94mLoss[0m : 2.50746
[1mStep[0m  [165/339], [94mLoss[0m : 1.66176
[1mStep[0m  [198/339], [94mLoss[0m : 2.77539
[1mStep[0m  [231/339], [94mLoss[0m : 2.87745
[1mStep[0m  [264/339], [94mLoss[0m : 2.47847
[1mStep[0m  [297/339], [94mLoss[0m : 2.35068
[1mStep[0m  [330/339], [94mLoss[0m : 2.10588

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41406
[1mStep[0m  [33/339], [94mLoss[0m : 2.78410
[1mStep[0m  [66/339], [94mLoss[0m : 2.04234
[1mStep[0m  [99/339], [94mLoss[0m : 2.78413
[1mStep[0m  [132/339], [94mLoss[0m : 2.45376
[1mStep[0m  [165/339], [94mLoss[0m : 2.09592
[1mStep[0m  [198/339], [94mLoss[0m : 2.19947
[1mStep[0m  [231/339], [94mLoss[0m : 2.86165
[1mStep[0m  [264/339], [94mLoss[0m : 2.89185
[1mStep[0m  [297/339], [94mLoss[0m : 2.51013
[1mStep[0m  [330/339], [94mLoss[0m : 2.66835

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50072
[1mStep[0m  [33/339], [94mLoss[0m : 2.51773
[1mStep[0m  [66/339], [94mLoss[0m : 2.23582
[1mStep[0m  [99/339], [94mLoss[0m : 2.72747
[1mStep[0m  [132/339], [94mLoss[0m : 2.42564
[1mStep[0m  [165/339], [94mLoss[0m : 2.06568
[1mStep[0m  [198/339], [94mLoss[0m : 2.39920
[1mStep[0m  [231/339], [94mLoss[0m : 2.33096
[1mStep[0m  [264/339], [94mLoss[0m : 2.36025
[1mStep[0m  [297/339], [94mLoss[0m : 2.32835
[1mStep[0m  [330/339], [94mLoss[0m : 2.45050

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.04884
[1mStep[0m  [33/339], [94mLoss[0m : 2.10065
[1mStep[0m  [66/339], [94mLoss[0m : 2.38612
[1mStep[0m  [99/339], [94mLoss[0m : 2.74214
[1mStep[0m  [132/339], [94mLoss[0m : 2.18665
[1mStep[0m  [165/339], [94mLoss[0m : 2.35199
[1mStep[0m  [198/339], [94mLoss[0m : 2.60959
[1mStep[0m  [231/339], [94mLoss[0m : 2.85104
[1mStep[0m  [264/339], [94mLoss[0m : 2.24263
[1mStep[0m  [297/339], [94mLoss[0m : 2.07345
[1mStep[0m  [330/339], [94mLoss[0m : 2.10975

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29158
[1mStep[0m  [33/339], [94mLoss[0m : 1.99424
[1mStep[0m  [66/339], [94mLoss[0m : 2.37868
[1mStep[0m  [99/339], [94mLoss[0m : 2.10107
[1mStep[0m  [132/339], [94mLoss[0m : 2.19308
[1mStep[0m  [165/339], [94mLoss[0m : 2.10973
[1mStep[0m  [198/339], [94mLoss[0m : 2.85079
[1mStep[0m  [231/339], [94mLoss[0m : 2.36852
[1mStep[0m  [264/339], [94mLoss[0m : 2.32701
[1mStep[0m  [297/339], [94mLoss[0m : 3.20081
[1mStep[0m  [330/339], [94mLoss[0m : 2.57927

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63603
[1mStep[0m  [33/339], [94mLoss[0m : 1.96783
[1mStep[0m  [66/339], [94mLoss[0m : 2.33779
[1mStep[0m  [99/339], [94mLoss[0m : 3.08168
[1mStep[0m  [132/339], [94mLoss[0m : 2.18457
[1mStep[0m  [165/339], [94mLoss[0m : 2.10305
[1mStep[0m  [198/339], [94mLoss[0m : 2.47337
[1mStep[0m  [231/339], [94mLoss[0m : 2.31544
[1mStep[0m  [264/339], [94mLoss[0m : 2.69140
[1mStep[0m  [297/339], [94mLoss[0m : 2.63198
[1mStep[0m  [330/339], [94mLoss[0m : 2.23415

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22647
[1mStep[0m  [33/339], [94mLoss[0m : 2.53232
[1mStep[0m  [66/339], [94mLoss[0m : 2.87432
[1mStep[0m  [99/339], [94mLoss[0m : 2.55359
[1mStep[0m  [132/339], [94mLoss[0m : 2.36422
[1mStep[0m  [165/339], [94mLoss[0m : 2.24245
[1mStep[0m  [198/339], [94mLoss[0m : 2.62506
[1mStep[0m  [231/339], [94mLoss[0m : 2.26343
[1mStep[0m  [264/339], [94mLoss[0m : 2.40128
[1mStep[0m  [297/339], [94mLoss[0m : 2.92955
[1mStep[0m  [330/339], [94mLoss[0m : 2.63827

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25433
[1mStep[0m  [33/339], [94mLoss[0m : 2.27283
[1mStep[0m  [66/339], [94mLoss[0m : 2.26336
[1mStep[0m  [99/339], [94mLoss[0m : 2.76712
[1mStep[0m  [132/339], [94mLoss[0m : 1.91501
[1mStep[0m  [165/339], [94mLoss[0m : 2.81352
[1mStep[0m  [198/339], [94mLoss[0m : 2.54664
[1mStep[0m  [231/339], [94mLoss[0m : 2.82594
[1mStep[0m  [264/339], [94mLoss[0m : 2.07382
[1mStep[0m  [297/339], [94mLoss[0m : 1.92428
[1mStep[0m  [330/339], [94mLoss[0m : 2.20875

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27680
[1mStep[0m  [33/339], [94mLoss[0m : 2.45481
[1mStep[0m  [66/339], [94mLoss[0m : 2.62208
[1mStep[0m  [99/339], [94mLoss[0m : 3.58623
[1mStep[0m  [132/339], [94mLoss[0m : 2.59081
[1mStep[0m  [165/339], [94mLoss[0m : 2.82447
[1mStep[0m  [198/339], [94mLoss[0m : 2.00212
[1mStep[0m  [231/339], [94mLoss[0m : 2.84336
[1mStep[0m  [264/339], [94mLoss[0m : 2.32540
[1mStep[0m  [297/339], [94mLoss[0m : 2.26343
[1mStep[0m  [330/339], [94mLoss[0m : 2.83761

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58691
[1mStep[0m  [33/339], [94mLoss[0m : 3.03827
[1mStep[0m  [66/339], [94mLoss[0m : 2.31780
[1mStep[0m  [99/339], [94mLoss[0m : 2.59070
[1mStep[0m  [132/339], [94mLoss[0m : 1.88743
[1mStep[0m  [165/339], [94mLoss[0m : 2.45974
[1mStep[0m  [198/339], [94mLoss[0m : 2.34893
[1mStep[0m  [231/339], [94mLoss[0m : 2.41974
[1mStep[0m  [264/339], [94mLoss[0m : 3.02960
[1mStep[0m  [297/339], [94mLoss[0m : 2.54045
[1mStep[0m  [330/339], [94mLoss[0m : 2.17693

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.10517
[1mStep[0m  [33/339], [94mLoss[0m : 2.78291
[1mStep[0m  [66/339], [94mLoss[0m : 2.51669
[1mStep[0m  [99/339], [94mLoss[0m : 2.28038
[1mStep[0m  [132/339], [94mLoss[0m : 2.38169
[1mStep[0m  [165/339], [94mLoss[0m : 2.82380
[1mStep[0m  [198/339], [94mLoss[0m : 2.81978
[1mStep[0m  [231/339], [94mLoss[0m : 3.00654
[1mStep[0m  [264/339], [94mLoss[0m : 2.39593
[1mStep[0m  [297/339], [94mLoss[0m : 2.16207
[1mStep[0m  [330/339], [94mLoss[0m : 2.63585

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.12073
[1mStep[0m  [33/339], [94mLoss[0m : 2.46987
[1mStep[0m  [66/339], [94mLoss[0m : 2.52730
[1mStep[0m  [99/339], [94mLoss[0m : 2.50304
[1mStep[0m  [132/339], [94mLoss[0m : 2.41366
[1mStep[0m  [165/339], [94mLoss[0m : 2.75739
[1mStep[0m  [198/339], [94mLoss[0m : 2.52845
[1mStep[0m  [231/339], [94mLoss[0m : 2.10440
[1mStep[0m  [264/339], [94mLoss[0m : 2.09810
[1mStep[0m  [297/339], [94mLoss[0m : 2.88932
[1mStep[0m  [330/339], [94mLoss[0m : 2.30521

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71180
[1mStep[0m  [33/339], [94mLoss[0m : 3.42479
[1mStep[0m  [66/339], [94mLoss[0m : 1.87211
[1mStep[0m  [99/339], [94mLoss[0m : 2.37493
[1mStep[0m  [132/339], [94mLoss[0m : 1.71682
[1mStep[0m  [165/339], [94mLoss[0m : 2.64742
[1mStep[0m  [198/339], [94mLoss[0m : 2.52011
[1mStep[0m  [231/339], [94mLoss[0m : 2.15253
[1mStep[0m  [264/339], [94mLoss[0m : 2.61904
[1mStep[0m  [297/339], [94mLoss[0m : 2.39568
[1mStep[0m  [330/339], [94mLoss[0m : 2.71712

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62761
[1mStep[0m  [33/339], [94mLoss[0m : 2.13071
[1mStep[0m  [66/339], [94mLoss[0m : 2.05142
[1mStep[0m  [99/339], [94mLoss[0m : 2.63166
[1mStep[0m  [132/339], [94mLoss[0m : 2.84214
[1mStep[0m  [165/339], [94mLoss[0m : 2.34639
[1mStep[0m  [198/339], [94mLoss[0m : 2.43003
[1mStep[0m  [231/339], [94mLoss[0m : 2.23801
[1mStep[0m  [264/339], [94mLoss[0m : 1.64717
[1mStep[0m  [297/339], [94mLoss[0m : 2.13754
[1mStep[0m  [330/339], [94mLoss[0m : 2.74513

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97797
[1mStep[0m  [33/339], [94mLoss[0m : 2.40389
[1mStep[0m  [66/339], [94mLoss[0m : 3.02438
[1mStep[0m  [99/339], [94mLoss[0m : 2.34732
[1mStep[0m  [132/339], [94mLoss[0m : 2.38443
[1mStep[0m  [165/339], [94mLoss[0m : 2.80565
[1mStep[0m  [198/339], [94mLoss[0m : 2.92222
[1mStep[0m  [231/339], [94mLoss[0m : 2.71102
[1mStep[0m  [264/339], [94mLoss[0m : 2.37129
[1mStep[0m  [297/339], [94mLoss[0m : 2.12302
[1mStep[0m  [330/339], [94mLoss[0m : 2.01704

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.89107
[1mStep[0m  [33/339], [94mLoss[0m : 2.40960
[1mStep[0m  [66/339], [94mLoss[0m : 2.16954
[1mStep[0m  [99/339], [94mLoss[0m : 2.42204
[1mStep[0m  [132/339], [94mLoss[0m : 2.42336
[1mStep[0m  [165/339], [94mLoss[0m : 2.71128
[1mStep[0m  [198/339], [94mLoss[0m : 2.69373
[1mStep[0m  [231/339], [94mLoss[0m : 2.10986
[1mStep[0m  [264/339], [94mLoss[0m : 2.26293
[1mStep[0m  [297/339], [94mLoss[0m : 2.54570
[1mStep[0m  [330/339], [94mLoss[0m : 2.62765

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29047
[1mStep[0m  [33/339], [94mLoss[0m : 2.48573
[1mStep[0m  [66/339], [94mLoss[0m : 2.93469
[1mStep[0m  [99/339], [94mLoss[0m : 2.30656
[1mStep[0m  [132/339], [94mLoss[0m : 2.02764
[1mStep[0m  [165/339], [94mLoss[0m : 2.97959
[1mStep[0m  [198/339], [94mLoss[0m : 2.66994
[1mStep[0m  [231/339], [94mLoss[0m : 2.52781
[1mStep[0m  [264/339], [94mLoss[0m : 2.47266
[1mStep[0m  [297/339], [94mLoss[0m : 2.17455
[1mStep[0m  [330/339], [94mLoss[0m : 2.23489

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54954
[1mStep[0m  [33/339], [94mLoss[0m : 2.35954
[1mStep[0m  [66/339], [94mLoss[0m : 1.93639
[1mStep[0m  [99/339], [94mLoss[0m : 3.01905
[1mStep[0m  [132/339], [94mLoss[0m : 1.93284
[1mStep[0m  [165/339], [94mLoss[0m : 2.70950
[1mStep[0m  [198/339], [94mLoss[0m : 2.75090
[1mStep[0m  [231/339], [94mLoss[0m : 2.97139
[1mStep[0m  [264/339], [94mLoss[0m : 2.59728
[1mStep[0m  [297/339], [94mLoss[0m : 2.50402
[1mStep[0m  [330/339], [94mLoss[0m : 3.04698

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47633
[1mStep[0m  [33/339], [94mLoss[0m : 2.45679
[1mStep[0m  [66/339], [94mLoss[0m : 2.22823
[1mStep[0m  [99/339], [94mLoss[0m : 2.85850
[1mStep[0m  [132/339], [94mLoss[0m : 2.36834
[1mStep[0m  [165/339], [94mLoss[0m : 1.81376
[1mStep[0m  [198/339], [94mLoss[0m : 2.42160
[1mStep[0m  [231/339], [94mLoss[0m : 2.71820
[1mStep[0m  [264/339], [94mLoss[0m : 2.35019
[1mStep[0m  [297/339], [94mLoss[0m : 2.61227
[1mStep[0m  [330/339], [94mLoss[0m : 2.10691

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.91635
[1mStep[0m  [33/339], [94mLoss[0m : 2.53217
[1mStep[0m  [66/339], [94mLoss[0m : 2.30614
[1mStep[0m  [99/339], [94mLoss[0m : 2.56945
[1mStep[0m  [132/339], [94mLoss[0m : 2.79819
[1mStep[0m  [165/339], [94mLoss[0m : 2.50431
[1mStep[0m  [198/339], [94mLoss[0m : 2.91951
[1mStep[0m  [231/339], [94mLoss[0m : 2.33042
[1mStep[0m  [264/339], [94mLoss[0m : 2.40269
[1mStep[0m  [297/339], [94mLoss[0m : 2.73366
[1mStep[0m  [330/339], [94mLoss[0m : 2.67656

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60371
[1mStep[0m  [33/339], [94mLoss[0m : 2.45010
[1mStep[0m  [66/339], [94mLoss[0m : 2.73182
[1mStep[0m  [99/339], [94mLoss[0m : 2.56472
[1mStep[0m  [132/339], [94mLoss[0m : 2.01287
[1mStep[0m  [165/339], [94mLoss[0m : 1.84349
[1mStep[0m  [198/339], [94mLoss[0m : 2.71068
[1mStep[0m  [231/339], [94mLoss[0m : 2.34383
[1mStep[0m  [264/339], [94mLoss[0m : 2.65890
[1mStep[0m  [297/339], [94mLoss[0m : 2.82599
[1mStep[0m  [330/339], [94mLoss[0m : 2.12435

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60907
[1mStep[0m  [33/339], [94mLoss[0m : 2.08269
[1mStep[0m  [66/339], [94mLoss[0m : 2.35764
[1mStep[0m  [99/339], [94mLoss[0m : 2.36092
[1mStep[0m  [132/339], [94mLoss[0m : 2.28040
[1mStep[0m  [165/339], [94mLoss[0m : 2.47354
[1mStep[0m  [198/339], [94mLoss[0m : 2.38162
[1mStep[0m  [231/339], [94mLoss[0m : 2.04132
[1mStep[0m  [264/339], [94mLoss[0m : 2.66425
[1mStep[0m  [297/339], [94mLoss[0m : 2.67924
[1mStep[0m  [330/339], [94mLoss[0m : 2.90358

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94103
[1mStep[0m  [33/339], [94mLoss[0m : 2.05385
[1mStep[0m  [66/339], [94mLoss[0m : 2.24141
[1mStep[0m  [99/339], [94mLoss[0m : 2.62627
[1mStep[0m  [132/339], [94mLoss[0m : 2.38824
[1mStep[0m  [165/339], [94mLoss[0m : 2.34638
[1mStep[0m  [198/339], [94mLoss[0m : 2.68822
[1mStep[0m  [231/339], [94mLoss[0m : 2.51253
[1mStep[0m  [264/339], [94mLoss[0m : 3.36540
[1mStep[0m  [297/339], [94mLoss[0m : 2.62879
[1mStep[0m  [330/339], [94mLoss[0m : 2.12229

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.345
====================================

Phase 1 - Evaluation MAE:  2.3449014836708
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.63180
[1mStep[0m  [33/339], [94mLoss[0m : 2.14643
[1mStep[0m  [66/339], [94mLoss[0m : 2.45920
[1mStep[0m  [99/339], [94mLoss[0m : 2.38971
[1mStep[0m  [132/339], [94mLoss[0m : 2.71886
[1mStep[0m  [165/339], [94mLoss[0m : 2.12821
[1mStep[0m  [198/339], [94mLoss[0m : 2.70947
[1mStep[0m  [231/339], [94mLoss[0m : 2.27700
[1mStep[0m  [264/339], [94mLoss[0m : 2.10401
[1mStep[0m  [297/339], [94mLoss[0m : 2.12874
[1mStep[0m  [330/339], [94mLoss[0m : 2.82109

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06477
[1mStep[0m  [33/339], [94mLoss[0m : 2.84001
[1mStep[0m  [66/339], [94mLoss[0m : 2.60813
[1mStep[0m  [99/339], [94mLoss[0m : 2.30899
[1mStep[0m  [132/339], [94mLoss[0m : 2.30066
[1mStep[0m  [165/339], [94mLoss[0m : 2.35977
[1mStep[0m  [198/339], [94mLoss[0m : 2.42377
[1mStep[0m  [231/339], [94mLoss[0m : 2.85583
[1mStep[0m  [264/339], [94mLoss[0m : 2.64867
[1mStep[0m  [297/339], [94mLoss[0m : 2.42233
[1mStep[0m  [330/339], [94mLoss[0m : 2.45107

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.546, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48208
[1mStep[0m  [33/339], [94mLoss[0m : 2.89885
[1mStep[0m  [66/339], [94mLoss[0m : 2.61323
[1mStep[0m  [99/339], [94mLoss[0m : 2.70232
[1mStep[0m  [132/339], [94mLoss[0m : 2.92563
[1mStep[0m  [165/339], [94mLoss[0m : 2.19732
[1mStep[0m  [198/339], [94mLoss[0m : 1.89307
[1mStep[0m  [231/339], [94mLoss[0m : 3.00284
[1mStep[0m  [264/339], [94mLoss[0m : 2.12217
[1mStep[0m  [297/339], [94mLoss[0m : 2.44467
[1mStep[0m  [330/339], [94mLoss[0m : 2.55490

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.512, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35749
[1mStep[0m  [33/339], [94mLoss[0m : 2.49590
[1mStep[0m  [66/339], [94mLoss[0m : 2.37618
[1mStep[0m  [99/339], [94mLoss[0m : 2.63749
[1mStep[0m  [132/339], [94mLoss[0m : 2.85170
[1mStep[0m  [165/339], [94mLoss[0m : 1.99138
[1mStep[0m  [198/339], [94mLoss[0m : 2.31045
[1mStep[0m  [231/339], [94mLoss[0m : 2.40036
[1mStep[0m  [264/339], [94mLoss[0m : 2.17643
[1mStep[0m  [297/339], [94mLoss[0m : 2.83109
[1mStep[0m  [330/339], [94mLoss[0m : 2.81661

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63410
[1mStep[0m  [33/339], [94mLoss[0m : 2.14929
[1mStep[0m  [66/339], [94mLoss[0m : 2.44628
[1mStep[0m  [99/339], [94mLoss[0m : 2.61035
[1mStep[0m  [132/339], [94mLoss[0m : 2.58550
[1mStep[0m  [165/339], [94mLoss[0m : 2.50687
[1mStep[0m  [198/339], [94mLoss[0m : 2.58731
[1mStep[0m  [231/339], [94mLoss[0m : 2.57810
[1mStep[0m  [264/339], [94mLoss[0m : 2.20714
[1mStep[0m  [297/339], [94mLoss[0m : 1.88947
[1mStep[0m  [330/339], [94mLoss[0m : 2.55177

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46503
[1mStep[0m  [33/339], [94mLoss[0m : 2.89797
[1mStep[0m  [66/339], [94mLoss[0m : 2.23478
[1mStep[0m  [99/339], [94mLoss[0m : 2.42238
[1mStep[0m  [132/339], [94mLoss[0m : 2.50586
[1mStep[0m  [165/339], [94mLoss[0m : 2.66567
[1mStep[0m  [198/339], [94mLoss[0m : 1.68497
[1mStep[0m  [231/339], [94mLoss[0m : 2.28861
[1mStep[0m  [264/339], [94mLoss[0m : 2.20350
[1mStep[0m  [297/339], [94mLoss[0m : 2.26793
[1mStep[0m  [330/339], [94mLoss[0m : 2.20832

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73091
[1mStep[0m  [33/339], [94mLoss[0m : 1.67856
[1mStep[0m  [66/339], [94mLoss[0m : 2.34480
[1mStep[0m  [99/339], [94mLoss[0m : 2.52898
[1mStep[0m  [132/339], [94mLoss[0m : 2.64337
[1mStep[0m  [165/339], [94mLoss[0m : 2.17972
[1mStep[0m  [198/339], [94mLoss[0m : 2.53250
[1mStep[0m  [231/339], [94mLoss[0m : 2.00784
[1mStep[0m  [264/339], [94mLoss[0m : 2.53824
[1mStep[0m  [297/339], [94mLoss[0m : 2.08421
[1mStep[0m  [330/339], [94mLoss[0m : 1.61783

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90542
[1mStep[0m  [33/339], [94mLoss[0m : 1.68221
[1mStep[0m  [66/339], [94mLoss[0m : 1.90479
[1mStep[0m  [99/339], [94mLoss[0m : 2.00554
[1mStep[0m  [132/339], [94mLoss[0m : 2.26280
[1mStep[0m  [165/339], [94mLoss[0m : 1.92651
[1mStep[0m  [198/339], [94mLoss[0m : 1.96389
[1mStep[0m  [231/339], [94mLoss[0m : 1.76870
[1mStep[0m  [264/339], [94mLoss[0m : 2.36484
[1mStep[0m  [297/339], [94mLoss[0m : 2.71715
[1mStep[0m  [330/339], [94mLoss[0m : 2.80456

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37839
[1mStep[0m  [33/339], [94mLoss[0m : 1.98232
[1mStep[0m  [66/339], [94mLoss[0m : 2.37675
[1mStep[0m  [99/339], [94mLoss[0m : 1.81566
[1mStep[0m  [132/339], [94mLoss[0m : 2.41831
[1mStep[0m  [165/339], [94mLoss[0m : 2.29287
[1mStep[0m  [198/339], [94mLoss[0m : 2.19147
[1mStep[0m  [231/339], [94mLoss[0m : 1.97043
[1mStep[0m  [264/339], [94mLoss[0m : 2.22726
[1mStep[0m  [297/339], [94mLoss[0m : 2.15386
[1mStep[0m  [330/339], [94mLoss[0m : 2.11036

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02905
[1mStep[0m  [33/339], [94mLoss[0m : 2.46732
[1mStep[0m  [66/339], [94mLoss[0m : 2.04044
[1mStep[0m  [99/339], [94mLoss[0m : 2.06582
[1mStep[0m  [132/339], [94mLoss[0m : 2.19426
[1mStep[0m  [165/339], [94mLoss[0m : 2.13385
[1mStep[0m  [198/339], [94mLoss[0m : 1.85651
[1mStep[0m  [231/339], [94mLoss[0m : 1.79621
[1mStep[0m  [264/339], [94mLoss[0m : 2.97519
[1mStep[0m  [297/339], [94mLoss[0m : 2.00823
[1mStep[0m  [330/339], [94mLoss[0m : 1.83172

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14089
[1mStep[0m  [33/339], [94mLoss[0m : 2.47143
[1mStep[0m  [66/339], [94mLoss[0m : 1.61968
[1mStep[0m  [99/339], [94mLoss[0m : 1.86697
[1mStep[0m  [132/339], [94mLoss[0m : 1.72623
[1mStep[0m  [165/339], [94mLoss[0m : 1.47563
[1mStep[0m  [198/339], [94mLoss[0m : 2.40819
[1mStep[0m  [231/339], [94mLoss[0m : 1.99820
[1mStep[0m  [264/339], [94mLoss[0m : 1.92460
[1mStep[0m  [297/339], [94mLoss[0m : 2.01225
[1mStep[0m  [330/339], [94mLoss[0m : 1.87517

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69224
[1mStep[0m  [33/339], [94mLoss[0m : 2.83094
[1mStep[0m  [66/339], [94mLoss[0m : 2.36186
[1mStep[0m  [99/339], [94mLoss[0m : 1.74446
[1mStep[0m  [132/339], [94mLoss[0m : 2.76609
[1mStep[0m  [165/339], [94mLoss[0m : 1.71804
[1mStep[0m  [198/339], [94mLoss[0m : 2.42307
[1mStep[0m  [231/339], [94mLoss[0m : 2.43041
[1mStep[0m  [264/339], [94mLoss[0m : 2.10013
[1mStep[0m  [297/339], [94mLoss[0m : 2.54953
[1mStep[0m  [330/339], [94mLoss[0m : 1.86481

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.087, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60501
[1mStep[0m  [33/339], [94mLoss[0m : 1.95446
[1mStep[0m  [66/339], [94mLoss[0m : 1.47067
[1mStep[0m  [99/339], [94mLoss[0m : 2.04375
[1mStep[0m  [132/339], [94mLoss[0m : 2.05457
[1mStep[0m  [165/339], [94mLoss[0m : 2.68114
[1mStep[0m  [198/339], [94mLoss[0m : 2.49999
[1mStep[0m  [231/339], [94mLoss[0m : 2.68649
[1mStep[0m  [264/339], [94mLoss[0m : 2.63637
[1mStep[0m  [297/339], [94mLoss[0m : 2.03641
[1mStep[0m  [330/339], [94mLoss[0m : 1.58158

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75613
[1mStep[0m  [33/339], [94mLoss[0m : 1.89517
[1mStep[0m  [66/339], [94mLoss[0m : 2.17268
[1mStep[0m  [99/339], [94mLoss[0m : 2.38733
[1mStep[0m  [132/339], [94mLoss[0m : 2.13657
[1mStep[0m  [165/339], [94mLoss[0m : 2.33713
[1mStep[0m  [198/339], [94mLoss[0m : 2.02099
[1mStep[0m  [231/339], [94mLoss[0m : 2.04218
[1mStep[0m  [264/339], [94mLoss[0m : 2.19059
[1mStep[0m  [297/339], [94mLoss[0m : 2.15952
[1mStep[0m  [330/339], [94mLoss[0m : 2.16333

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15339
[1mStep[0m  [33/339], [94mLoss[0m : 2.07285
[1mStep[0m  [66/339], [94mLoss[0m : 1.73474
[1mStep[0m  [99/339], [94mLoss[0m : 1.78847
[1mStep[0m  [132/339], [94mLoss[0m : 2.08144
[1mStep[0m  [165/339], [94mLoss[0m : 2.37895
[1mStep[0m  [198/339], [94mLoss[0m : 2.28253
[1mStep[0m  [231/339], [94mLoss[0m : 1.68394
[1mStep[0m  [264/339], [94mLoss[0m : 1.90320
[1mStep[0m  [297/339], [94mLoss[0m : 1.99598
[1mStep[0m  [330/339], [94mLoss[0m : 2.23112

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06403
[1mStep[0m  [33/339], [94mLoss[0m : 1.87108
[1mStep[0m  [66/339], [94mLoss[0m : 1.77512
[1mStep[0m  [99/339], [94mLoss[0m : 1.43533
[1mStep[0m  [132/339], [94mLoss[0m : 1.84915
[1mStep[0m  [165/339], [94mLoss[0m : 1.68121
[1mStep[0m  [198/339], [94mLoss[0m : 1.86443
[1mStep[0m  [231/339], [94mLoss[0m : 1.86395
[1mStep[0m  [264/339], [94mLoss[0m : 1.91555
[1mStep[0m  [297/339], [94mLoss[0m : 1.75740
[1mStep[0m  [330/339], [94mLoss[0m : 1.76490

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85609
[1mStep[0m  [33/339], [94mLoss[0m : 1.79742
[1mStep[0m  [66/339], [94mLoss[0m : 2.11198
[1mStep[0m  [99/339], [94mLoss[0m : 2.13617
[1mStep[0m  [132/339], [94mLoss[0m : 1.85228
[1mStep[0m  [165/339], [94mLoss[0m : 1.95165
[1mStep[0m  [198/339], [94mLoss[0m : 1.64700
[1mStep[0m  [231/339], [94mLoss[0m : 2.03265
[1mStep[0m  [264/339], [94mLoss[0m : 1.81298
[1mStep[0m  [297/339], [94mLoss[0m : 1.96581
[1mStep[0m  [330/339], [94mLoss[0m : 2.30469

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.943, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81136
[1mStep[0m  [33/339], [94mLoss[0m : 1.92823
[1mStep[0m  [66/339], [94mLoss[0m : 1.43903
[1mStep[0m  [99/339], [94mLoss[0m : 2.07471
[1mStep[0m  [132/339], [94mLoss[0m : 2.74350
[1mStep[0m  [165/339], [94mLoss[0m : 1.97186
[1mStep[0m  [198/339], [94mLoss[0m : 1.67257
[1mStep[0m  [231/339], [94mLoss[0m : 1.40996
[1mStep[0m  [264/339], [94mLoss[0m : 2.21661
[1mStep[0m  [297/339], [94mLoss[0m : 2.09889
[1mStep[0m  [330/339], [94mLoss[0m : 1.98974

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.915, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78055
[1mStep[0m  [33/339], [94mLoss[0m : 1.69169
[1mStep[0m  [66/339], [94mLoss[0m : 2.06646
[1mStep[0m  [99/339], [94mLoss[0m : 1.43756
[1mStep[0m  [132/339], [94mLoss[0m : 2.14458
[1mStep[0m  [165/339], [94mLoss[0m : 1.93815
[1mStep[0m  [198/339], [94mLoss[0m : 1.99843
[1mStep[0m  [231/339], [94mLoss[0m : 1.72200
[1mStep[0m  [264/339], [94mLoss[0m : 1.27974
[1mStep[0m  [297/339], [94mLoss[0m : 2.37460
[1mStep[0m  [330/339], [94mLoss[0m : 1.68307

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.867, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30546
[1mStep[0m  [33/339], [94mLoss[0m : 1.62128
[1mStep[0m  [66/339], [94mLoss[0m : 1.58191
[1mStep[0m  [99/339], [94mLoss[0m : 1.70983
[1mStep[0m  [132/339], [94mLoss[0m : 2.30635
[1mStep[0m  [165/339], [94mLoss[0m : 1.63928
[1mStep[0m  [198/339], [94mLoss[0m : 1.76076
[1mStep[0m  [231/339], [94mLoss[0m : 2.15862
[1mStep[0m  [264/339], [94mLoss[0m : 2.05743
[1mStep[0m  [297/339], [94mLoss[0m : 1.88665
[1mStep[0m  [330/339], [94mLoss[0m : 2.04337

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72924
[1mStep[0m  [33/339], [94mLoss[0m : 1.56982
[1mStep[0m  [66/339], [94mLoss[0m : 1.70832
[1mStep[0m  [99/339], [94mLoss[0m : 2.10284
[1mStep[0m  [132/339], [94mLoss[0m : 1.69365
[1mStep[0m  [165/339], [94mLoss[0m : 1.56515
[1mStep[0m  [198/339], [94mLoss[0m : 2.04015
[1mStep[0m  [231/339], [94mLoss[0m : 2.21171
[1mStep[0m  [264/339], [94mLoss[0m : 1.82430
[1mStep[0m  [297/339], [94mLoss[0m : 1.52296
[1mStep[0m  [330/339], [94mLoss[0m : 1.57722

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.502, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.17673
[1mStep[0m  [33/339], [94mLoss[0m : 1.66021
[1mStep[0m  [66/339], [94mLoss[0m : 1.78048
[1mStep[0m  [99/339], [94mLoss[0m : 1.84633
[1mStep[0m  [132/339], [94mLoss[0m : 2.25423
[1mStep[0m  [165/339], [94mLoss[0m : 1.91620
[1mStep[0m  [198/339], [94mLoss[0m : 1.90556
[1mStep[0m  [231/339], [94mLoss[0m : 2.12548
[1mStep[0m  [264/339], [94mLoss[0m : 1.56521
[1mStep[0m  [297/339], [94mLoss[0m : 1.52772
[1mStep[0m  [330/339], [94mLoss[0m : 1.92619

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.434, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38837
[1mStep[0m  [33/339], [94mLoss[0m : 1.82596
[1mStep[0m  [66/339], [94mLoss[0m : 1.78527
[1mStep[0m  [99/339], [94mLoss[0m : 2.12159
[1mStep[0m  [132/339], [94mLoss[0m : 1.69402
[1mStep[0m  [165/339], [94mLoss[0m : 2.10993
[1mStep[0m  [198/339], [94mLoss[0m : 1.83901
[1mStep[0m  [231/339], [94mLoss[0m : 2.19493
[1mStep[0m  [264/339], [94mLoss[0m : 1.74955
[1mStep[0m  [297/339], [94mLoss[0m : 1.63924
[1mStep[0m  [330/339], [94mLoss[0m : 1.50974

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88693
[1mStep[0m  [33/339], [94mLoss[0m : 2.23267
[1mStep[0m  [66/339], [94mLoss[0m : 2.15460
[1mStep[0m  [99/339], [94mLoss[0m : 1.97760
[1mStep[0m  [132/339], [94mLoss[0m : 1.62700
[1mStep[0m  [165/339], [94mLoss[0m : 1.70829
[1mStep[0m  [198/339], [94mLoss[0m : 1.48386
[1mStep[0m  [231/339], [94mLoss[0m : 1.82833
[1mStep[0m  [264/339], [94mLoss[0m : 1.77561
[1mStep[0m  [297/339], [94mLoss[0m : 1.67159
[1mStep[0m  [330/339], [94mLoss[0m : 1.78227

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.531, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.30941
[1mStep[0m  [33/339], [94mLoss[0m : 1.49313
[1mStep[0m  [66/339], [94mLoss[0m : 1.86707
[1mStep[0m  [99/339], [94mLoss[0m : 1.70722
[1mStep[0m  [132/339], [94mLoss[0m : 1.53200
[1mStep[0m  [165/339], [94mLoss[0m : 2.01068
[1mStep[0m  [198/339], [94mLoss[0m : 2.34607
[1mStep[0m  [231/339], [94mLoss[0m : 2.00365
[1mStep[0m  [264/339], [94mLoss[0m : 1.86443
[1mStep[0m  [297/339], [94mLoss[0m : 2.24129
[1mStep[0m  [330/339], [94mLoss[0m : 1.53744

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42781
[1mStep[0m  [33/339], [94mLoss[0m : 1.62644
[1mStep[0m  [66/339], [94mLoss[0m : 1.61551
[1mStep[0m  [99/339], [94mLoss[0m : 1.71518
[1mStep[0m  [132/339], [94mLoss[0m : 1.35770
[1mStep[0m  [165/339], [94mLoss[0m : 1.54884
[1mStep[0m  [198/339], [94mLoss[0m : 2.01243
[1mStep[0m  [231/339], [94mLoss[0m : 1.99848
[1mStep[0m  [264/339], [94mLoss[0m : 1.87484
[1mStep[0m  [297/339], [94mLoss[0m : 1.84229
[1mStep[0m  [330/339], [94mLoss[0m : 1.63064

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52298
[1mStep[0m  [33/339], [94mLoss[0m : 1.37438
[1mStep[0m  [66/339], [94mLoss[0m : 2.03237
[1mStep[0m  [99/339], [94mLoss[0m : 1.99417
[1mStep[0m  [132/339], [94mLoss[0m : 1.85751
[1mStep[0m  [165/339], [94mLoss[0m : 2.18713
[1mStep[0m  [198/339], [94mLoss[0m : 1.75570
[1mStep[0m  [231/339], [94mLoss[0m : 1.50584
[1mStep[0m  [264/339], [94mLoss[0m : 1.49638
[1mStep[0m  [297/339], [94mLoss[0m : 1.86748
[1mStep[0m  [330/339], [94mLoss[0m : 1.37493

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.458, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35165
[1mStep[0m  [33/339], [94mLoss[0m : 1.23354
[1mStep[0m  [66/339], [94mLoss[0m : 1.48314
[1mStep[0m  [99/339], [94mLoss[0m : 1.31260
[1mStep[0m  [132/339], [94mLoss[0m : 1.85064
[1mStep[0m  [165/339], [94mLoss[0m : 1.64692
[1mStep[0m  [198/339], [94mLoss[0m : 1.64978
[1mStep[0m  [231/339], [94mLoss[0m : 1.72505
[1mStep[0m  [264/339], [94mLoss[0m : 1.90841
[1mStep[0m  [297/339], [94mLoss[0m : 2.06380
[1mStep[0m  [330/339], [94mLoss[0m : 2.09422

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.16141
[1mStep[0m  [33/339], [94mLoss[0m : 1.51876
[1mStep[0m  [66/339], [94mLoss[0m : 1.43328
[1mStep[0m  [99/339], [94mLoss[0m : 1.59891
[1mStep[0m  [132/339], [94mLoss[0m : 1.77542
[1mStep[0m  [165/339], [94mLoss[0m : 1.79289
[1mStep[0m  [198/339], [94mLoss[0m : 2.03134
[1mStep[0m  [231/339], [94mLoss[0m : 2.08445
[1mStep[0m  [264/339], [94mLoss[0m : 1.69557
[1mStep[0m  [297/339], [94mLoss[0m : 1.62808
[1mStep[0m  [330/339], [94mLoss[0m : 1.86779

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.708, [92mTest[0m: 2.514, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76512
[1mStep[0m  [33/339], [94mLoss[0m : 1.34825
[1mStep[0m  [66/339], [94mLoss[0m : 1.39918
[1mStep[0m  [99/339], [94mLoss[0m : 1.45991
[1mStep[0m  [132/339], [94mLoss[0m : 1.78620
[1mStep[0m  [165/339], [94mLoss[0m : 1.67878
[1mStep[0m  [198/339], [94mLoss[0m : 1.40806
[1mStep[0m  [231/339], [94mLoss[0m : 1.82524
[1mStep[0m  [264/339], [94mLoss[0m : 1.37872
[1mStep[0m  [297/339], [94mLoss[0m : 1.65896
[1mStep[0m  [330/339], [94mLoss[0m : 1.41398

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.466, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.451
====================================

Phase 2 - Evaluation MAE:  2.450748353932811
MAE score P1       2.344901
MAE score P2       2.450748
loss               1.689345
learning_rate      0.002575
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.06405
[1mStep[0m  [33/339], [94mLoss[0m : 2.20515
[1mStep[0m  [66/339], [94mLoss[0m : 2.72556
[1mStep[0m  [99/339], [94mLoss[0m : 2.47655
[1mStep[0m  [132/339], [94mLoss[0m : 2.29166
[1mStep[0m  [165/339], [94mLoss[0m : 2.69804
[1mStep[0m  [198/339], [94mLoss[0m : 2.38062
[1mStep[0m  [231/339], [94mLoss[0m : 2.31773
[1mStep[0m  [264/339], [94mLoss[0m : 2.48018
[1mStep[0m  [297/339], [94mLoss[0m : 3.49989
[1mStep[0m  [330/339], [94mLoss[0m : 1.82292

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.721, [92mTest[0m: 10.927, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54323
[1mStep[0m  [33/339], [94mLoss[0m : 2.81983
[1mStep[0m  [66/339], [94mLoss[0m : 2.24561
[1mStep[0m  [99/339], [94mLoss[0m : 2.31065
[1mStep[0m  [132/339], [94mLoss[0m : 2.46967
[1mStep[0m  [165/339], [94mLoss[0m : 2.22421
[1mStep[0m  [198/339], [94mLoss[0m : 2.81949
[1mStep[0m  [231/339], [94mLoss[0m : 3.04988
[1mStep[0m  [264/339], [94mLoss[0m : 2.80372
[1mStep[0m  [297/339], [94mLoss[0m : 2.58314
[1mStep[0m  [330/339], [94mLoss[0m : 2.38883

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13783
[1mStep[0m  [33/339], [94mLoss[0m : 2.44091
[1mStep[0m  [66/339], [94mLoss[0m : 2.16997
[1mStep[0m  [99/339], [94mLoss[0m : 2.72657
[1mStep[0m  [132/339], [94mLoss[0m : 2.24639
[1mStep[0m  [165/339], [94mLoss[0m : 2.95572
[1mStep[0m  [198/339], [94mLoss[0m : 2.54097
[1mStep[0m  [231/339], [94mLoss[0m : 1.91646
[1mStep[0m  [264/339], [94mLoss[0m : 2.42758
[1mStep[0m  [297/339], [94mLoss[0m : 2.24348
[1mStep[0m  [330/339], [94mLoss[0m : 1.84709

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32081
[1mStep[0m  [33/339], [94mLoss[0m : 2.45570
[1mStep[0m  [66/339], [94mLoss[0m : 2.44538
[1mStep[0m  [99/339], [94mLoss[0m : 2.48026
[1mStep[0m  [132/339], [94mLoss[0m : 2.57485
[1mStep[0m  [165/339], [94mLoss[0m : 2.82490
[1mStep[0m  [198/339], [94mLoss[0m : 2.45679
[1mStep[0m  [231/339], [94mLoss[0m : 2.02864
[1mStep[0m  [264/339], [94mLoss[0m : 2.76400
[1mStep[0m  [297/339], [94mLoss[0m : 2.03044
[1mStep[0m  [330/339], [94mLoss[0m : 2.01436

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80758
[1mStep[0m  [33/339], [94mLoss[0m : 2.50886
[1mStep[0m  [66/339], [94mLoss[0m : 2.25675
[1mStep[0m  [99/339], [94mLoss[0m : 2.19092
[1mStep[0m  [132/339], [94mLoss[0m : 2.31177
[1mStep[0m  [165/339], [94mLoss[0m : 2.25535
[1mStep[0m  [198/339], [94mLoss[0m : 2.12327
[1mStep[0m  [231/339], [94mLoss[0m : 2.61470
[1mStep[0m  [264/339], [94mLoss[0m : 2.37436
[1mStep[0m  [297/339], [94mLoss[0m : 2.63739
[1mStep[0m  [330/339], [94mLoss[0m : 2.80995

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47308
[1mStep[0m  [33/339], [94mLoss[0m : 2.30780
[1mStep[0m  [66/339], [94mLoss[0m : 2.10402
[1mStep[0m  [99/339], [94mLoss[0m : 2.12177
[1mStep[0m  [132/339], [94mLoss[0m : 2.80607
[1mStep[0m  [165/339], [94mLoss[0m : 2.25352
[1mStep[0m  [198/339], [94mLoss[0m : 2.13461
[1mStep[0m  [231/339], [94mLoss[0m : 2.46817
[1mStep[0m  [264/339], [94mLoss[0m : 2.37044
[1mStep[0m  [297/339], [94mLoss[0m : 2.57206
[1mStep[0m  [330/339], [94mLoss[0m : 2.31484

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63886
[1mStep[0m  [33/339], [94mLoss[0m : 2.04371
[1mStep[0m  [66/339], [94mLoss[0m : 2.03055
[1mStep[0m  [99/339], [94mLoss[0m : 2.59606
[1mStep[0m  [132/339], [94mLoss[0m : 2.92750
[1mStep[0m  [165/339], [94mLoss[0m : 2.65534
[1mStep[0m  [198/339], [94mLoss[0m : 1.89363
[1mStep[0m  [231/339], [94mLoss[0m : 2.38982
[1mStep[0m  [264/339], [94mLoss[0m : 2.50427
[1mStep[0m  [297/339], [94mLoss[0m : 2.44572
[1mStep[0m  [330/339], [94mLoss[0m : 2.27855

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39273
[1mStep[0m  [33/339], [94mLoss[0m : 2.25472
[1mStep[0m  [66/339], [94mLoss[0m : 2.33389
[1mStep[0m  [99/339], [94mLoss[0m : 2.99393
[1mStep[0m  [132/339], [94mLoss[0m : 2.17544
[1mStep[0m  [165/339], [94mLoss[0m : 2.26855
[1mStep[0m  [198/339], [94mLoss[0m : 2.24362
[1mStep[0m  [231/339], [94mLoss[0m : 2.32658
[1mStep[0m  [264/339], [94mLoss[0m : 2.50766
[1mStep[0m  [297/339], [94mLoss[0m : 2.43091
[1mStep[0m  [330/339], [94mLoss[0m : 2.58734

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85745
[1mStep[0m  [33/339], [94mLoss[0m : 2.27089
[1mStep[0m  [66/339], [94mLoss[0m : 2.64803
[1mStep[0m  [99/339], [94mLoss[0m : 2.70059
[1mStep[0m  [132/339], [94mLoss[0m : 2.49039
[1mStep[0m  [165/339], [94mLoss[0m : 2.91294
[1mStep[0m  [198/339], [94mLoss[0m : 2.41401
[1mStep[0m  [231/339], [94mLoss[0m : 2.35662
[1mStep[0m  [264/339], [94mLoss[0m : 1.94805
[1mStep[0m  [297/339], [94mLoss[0m : 2.74244
[1mStep[0m  [330/339], [94mLoss[0m : 2.11634

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64824
[1mStep[0m  [33/339], [94mLoss[0m : 1.93481
[1mStep[0m  [66/339], [94mLoss[0m : 2.59889
[1mStep[0m  [99/339], [94mLoss[0m : 2.34995
[1mStep[0m  [132/339], [94mLoss[0m : 2.38920
[1mStep[0m  [165/339], [94mLoss[0m : 2.15343
[1mStep[0m  [198/339], [94mLoss[0m : 3.07930
[1mStep[0m  [231/339], [94mLoss[0m : 2.25390
[1mStep[0m  [264/339], [94mLoss[0m : 2.36167
[1mStep[0m  [297/339], [94mLoss[0m : 2.45987
[1mStep[0m  [330/339], [94mLoss[0m : 2.50559

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37625
[1mStep[0m  [33/339], [94mLoss[0m : 2.33092
[1mStep[0m  [66/339], [94mLoss[0m : 2.18276
[1mStep[0m  [99/339], [94mLoss[0m : 2.29455
[1mStep[0m  [132/339], [94mLoss[0m : 2.24876
[1mStep[0m  [165/339], [94mLoss[0m : 2.14801
[1mStep[0m  [198/339], [94mLoss[0m : 1.66988
[1mStep[0m  [231/339], [94mLoss[0m : 2.12873
[1mStep[0m  [264/339], [94mLoss[0m : 2.08963
[1mStep[0m  [297/339], [94mLoss[0m : 1.75728
[1mStep[0m  [330/339], [94mLoss[0m : 3.14762

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68176
[1mStep[0m  [33/339], [94mLoss[0m : 2.31767
[1mStep[0m  [66/339], [94mLoss[0m : 2.22326
[1mStep[0m  [99/339], [94mLoss[0m : 2.32479
[1mStep[0m  [132/339], [94mLoss[0m : 2.79454
[1mStep[0m  [165/339], [94mLoss[0m : 2.59699
[1mStep[0m  [198/339], [94mLoss[0m : 2.81705
[1mStep[0m  [231/339], [94mLoss[0m : 1.56576
[1mStep[0m  [264/339], [94mLoss[0m : 1.79839
[1mStep[0m  [297/339], [94mLoss[0m : 2.57854
[1mStep[0m  [330/339], [94mLoss[0m : 2.84800

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81882
[1mStep[0m  [33/339], [94mLoss[0m : 2.84846
[1mStep[0m  [66/339], [94mLoss[0m : 2.99612
[1mStep[0m  [99/339], [94mLoss[0m : 2.45302
[1mStep[0m  [132/339], [94mLoss[0m : 2.48014
[1mStep[0m  [165/339], [94mLoss[0m : 2.51776
[1mStep[0m  [198/339], [94mLoss[0m : 2.30684
[1mStep[0m  [231/339], [94mLoss[0m : 2.38081
[1mStep[0m  [264/339], [94mLoss[0m : 2.12673
[1mStep[0m  [297/339], [94mLoss[0m : 2.78351
[1mStep[0m  [330/339], [94mLoss[0m : 2.34163

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78422
[1mStep[0m  [33/339], [94mLoss[0m : 2.10382
[1mStep[0m  [66/339], [94mLoss[0m : 2.09493
[1mStep[0m  [99/339], [94mLoss[0m : 2.34105
[1mStep[0m  [132/339], [94mLoss[0m : 2.98428
[1mStep[0m  [165/339], [94mLoss[0m : 2.32983
[1mStep[0m  [198/339], [94mLoss[0m : 1.88514
[1mStep[0m  [231/339], [94mLoss[0m : 2.35986
[1mStep[0m  [264/339], [94mLoss[0m : 2.18724
[1mStep[0m  [297/339], [94mLoss[0m : 2.06500
[1mStep[0m  [330/339], [94mLoss[0m : 1.80791

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70987
[1mStep[0m  [33/339], [94mLoss[0m : 1.96911
[1mStep[0m  [66/339], [94mLoss[0m : 2.17337
[1mStep[0m  [99/339], [94mLoss[0m : 2.80507
[1mStep[0m  [132/339], [94mLoss[0m : 2.84052
[1mStep[0m  [165/339], [94mLoss[0m : 2.11903
[1mStep[0m  [198/339], [94mLoss[0m : 2.47475
[1mStep[0m  [231/339], [94mLoss[0m : 2.01401
[1mStep[0m  [264/339], [94mLoss[0m : 2.12652
[1mStep[0m  [297/339], [94mLoss[0m : 2.81138
[1mStep[0m  [330/339], [94mLoss[0m : 2.52160

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31292
[1mStep[0m  [33/339], [94mLoss[0m : 2.36825
[1mStep[0m  [66/339], [94mLoss[0m : 2.99600
[1mStep[0m  [99/339], [94mLoss[0m : 2.27292
[1mStep[0m  [132/339], [94mLoss[0m : 2.56447
[1mStep[0m  [165/339], [94mLoss[0m : 2.05760
[1mStep[0m  [198/339], [94mLoss[0m : 2.94024
[1mStep[0m  [231/339], [94mLoss[0m : 2.53450
[1mStep[0m  [264/339], [94mLoss[0m : 2.42434
[1mStep[0m  [297/339], [94mLoss[0m : 2.32825
[1mStep[0m  [330/339], [94mLoss[0m : 2.57562

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14009
[1mStep[0m  [33/339], [94mLoss[0m : 2.83735
[1mStep[0m  [66/339], [94mLoss[0m : 2.65143
[1mStep[0m  [99/339], [94mLoss[0m : 2.35248
[1mStep[0m  [132/339], [94mLoss[0m : 2.72250
[1mStep[0m  [165/339], [94mLoss[0m : 2.40343
[1mStep[0m  [198/339], [94mLoss[0m : 2.53753
[1mStep[0m  [231/339], [94mLoss[0m : 2.53303
[1mStep[0m  [264/339], [94mLoss[0m : 2.42036
[1mStep[0m  [297/339], [94mLoss[0m : 2.35731
[1mStep[0m  [330/339], [94mLoss[0m : 2.80932

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67130
[1mStep[0m  [33/339], [94mLoss[0m : 2.59812
[1mStep[0m  [66/339], [94mLoss[0m : 2.87915
[1mStep[0m  [99/339], [94mLoss[0m : 2.58970
[1mStep[0m  [132/339], [94mLoss[0m : 2.77807
[1mStep[0m  [165/339], [94mLoss[0m : 2.72556
[1mStep[0m  [198/339], [94mLoss[0m : 2.02147
[1mStep[0m  [231/339], [94mLoss[0m : 2.66496
[1mStep[0m  [264/339], [94mLoss[0m : 2.25179
[1mStep[0m  [297/339], [94mLoss[0m : 2.40434
[1mStep[0m  [330/339], [94mLoss[0m : 2.61493

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71773
[1mStep[0m  [33/339], [94mLoss[0m : 1.92474
[1mStep[0m  [66/339], [94mLoss[0m : 2.26460
[1mStep[0m  [99/339], [94mLoss[0m : 2.85148
[1mStep[0m  [132/339], [94mLoss[0m : 2.27931
[1mStep[0m  [165/339], [94mLoss[0m : 2.89773
[1mStep[0m  [198/339], [94mLoss[0m : 3.50325
[1mStep[0m  [231/339], [94mLoss[0m : 1.95929
[1mStep[0m  [264/339], [94mLoss[0m : 2.02695
[1mStep[0m  [297/339], [94mLoss[0m : 1.93570
[1mStep[0m  [330/339], [94mLoss[0m : 2.55484

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71990
[1mStep[0m  [33/339], [94mLoss[0m : 2.27106
[1mStep[0m  [66/339], [94mLoss[0m : 2.30801
[1mStep[0m  [99/339], [94mLoss[0m : 2.57809
[1mStep[0m  [132/339], [94mLoss[0m : 2.46599
[1mStep[0m  [165/339], [94mLoss[0m : 2.95588
[1mStep[0m  [198/339], [94mLoss[0m : 2.75611
[1mStep[0m  [231/339], [94mLoss[0m : 2.73879
[1mStep[0m  [264/339], [94mLoss[0m : 3.19103
[1mStep[0m  [297/339], [94mLoss[0m : 2.25986
[1mStep[0m  [330/339], [94mLoss[0m : 2.38866

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.353, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57016
[1mStep[0m  [33/339], [94mLoss[0m : 2.78701
[1mStep[0m  [66/339], [94mLoss[0m : 2.58257
[1mStep[0m  [99/339], [94mLoss[0m : 2.67843
[1mStep[0m  [132/339], [94mLoss[0m : 2.08680
[1mStep[0m  [165/339], [94mLoss[0m : 2.29141
[1mStep[0m  [198/339], [94mLoss[0m : 2.36270
[1mStep[0m  [231/339], [94mLoss[0m : 2.83020
[1mStep[0m  [264/339], [94mLoss[0m : 1.96250
[1mStep[0m  [297/339], [94mLoss[0m : 2.19302
[1mStep[0m  [330/339], [94mLoss[0m : 2.88596

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.17235
[1mStep[0m  [33/339], [94mLoss[0m : 2.35842
[1mStep[0m  [66/339], [94mLoss[0m : 2.73838
[1mStep[0m  [99/339], [94mLoss[0m : 2.82117
[1mStep[0m  [132/339], [94mLoss[0m : 2.81390
[1mStep[0m  [165/339], [94mLoss[0m : 2.70917
[1mStep[0m  [198/339], [94mLoss[0m : 2.42039
[1mStep[0m  [231/339], [94mLoss[0m : 2.56535
[1mStep[0m  [264/339], [94mLoss[0m : 3.09348
[1mStep[0m  [297/339], [94mLoss[0m : 2.09390
[1mStep[0m  [330/339], [94mLoss[0m : 2.17440

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29541
[1mStep[0m  [33/339], [94mLoss[0m : 2.49881
[1mStep[0m  [66/339], [94mLoss[0m : 2.35990
[1mStep[0m  [99/339], [94mLoss[0m : 2.19656
[1mStep[0m  [132/339], [94mLoss[0m : 2.66851
[1mStep[0m  [165/339], [94mLoss[0m : 2.82428
[1mStep[0m  [198/339], [94mLoss[0m : 2.17516
[1mStep[0m  [231/339], [94mLoss[0m : 2.88479
[1mStep[0m  [264/339], [94mLoss[0m : 2.08103
[1mStep[0m  [297/339], [94mLoss[0m : 2.48681
[1mStep[0m  [330/339], [94mLoss[0m : 2.24241

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.368, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36005
[1mStep[0m  [33/339], [94mLoss[0m : 3.07351
[1mStep[0m  [66/339], [94mLoss[0m : 2.58064
[1mStep[0m  [99/339], [94mLoss[0m : 2.40387
[1mStep[0m  [132/339], [94mLoss[0m : 2.64396
[1mStep[0m  [165/339], [94mLoss[0m : 2.20280
[1mStep[0m  [198/339], [94mLoss[0m : 2.80873
[1mStep[0m  [231/339], [94mLoss[0m : 2.01748
[1mStep[0m  [264/339], [94mLoss[0m : 2.34929
[1mStep[0m  [297/339], [94mLoss[0m : 2.24657
[1mStep[0m  [330/339], [94mLoss[0m : 2.70221

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.362, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42879
[1mStep[0m  [33/339], [94mLoss[0m : 2.42049
[1mStep[0m  [66/339], [94mLoss[0m : 2.46786
[1mStep[0m  [99/339], [94mLoss[0m : 2.40485
[1mStep[0m  [132/339], [94mLoss[0m : 2.46096
[1mStep[0m  [165/339], [94mLoss[0m : 2.67063
[1mStep[0m  [198/339], [94mLoss[0m : 2.35019
[1mStep[0m  [231/339], [94mLoss[0m : 2.65193
[1mStep[0m  [264/339], [94mLoss[0m : 2.30249
[1mStep[0m  [297/339], [94mLoss[0m : 3.22947
[1mStep[0m  [330/339], [94mLoss[0m : 2.32385

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.367, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20451
[1mStep[0m  [33/339], [94mLoss[0m : 2.43824
[1mStep[0m  [66/339], [94mLoss[0m : 3.30056
[1mStep[0m  [99/339], [94mLoss[0m : 2.79553
[1mStep[0m  [132/339], [94mLoss[0m : 2.15232
[1mStep[0m  [165/339], [94mLoss[0m : 2.52692
[1mStep[0m  [198/339], [94mLoss[0m : 2.59581
[1mStep[0m  [231/339], [94mLoss[0m : 2.80345
[1mStep[0m  [264/339], [94mLoss[0m : 3.74182
[1mStep[0m  [297/339], [94mLoss[0m : 2.78844
[1mStep[0m  [330/339], [94mLoss[0m : 2.52324

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76173
[1mStep[0m  [33/339], [94mLoss[0m : 3.32495
[1mStep[0m  [66/339], [94mLoss[0m : 1.94649
[1mStep[0m  [99/339], [94mLoss[0m : 2.52024
[1mStep[0m  [132/339], [94mLoss[0m : 2.06392
[1mStep[0m  [165/339], [94mLoss[0m : 2.76853
[1mStep[0m  [198/339], [94mLoss[0m : 2.08115
[1mStep[0m  [231/339], [94mLoss[0m : 2.24048
[1mStep[0m  [264/339], [94mLoss[0m : 3.22442
[1mStep[0m  [297/339], [94mLoss[0m : 2.59006
[1mStep[0m  [330/339], [94mLoss[0m : 2.57589

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67684
[1mStep[0m  [33/339], [94mLoss[0m : 2.32909
[1mStep[0m  [66/339], [94mLoss[0m : 2.34482
[1mStep[0m  [99/339], [94mLoss[0m : 2.77162
[1mStep[0m  [132/339], [94mLoss[0m : 2.94063
[1mStep[0m  [165/339], [94mLoss[0m : 2.40356
[1mStep[0m  [198/339], [94mLoss[0m : 2.79210
[1mStep[0m  [231/339], [94mLoss[0m : 3.11469
[1mStep[0m  [264/339], [94mLoss[0m : 2.51719
[1mStep[0m  [297/339], [94mLoss[0m : 2.80221
[1mStep[0m  [330/339], [94mLoss[0m : 2.56106

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.422, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72834
[1mStep[0m  [33/339], [94mLoss[0m : 3.19185
[1mStep[0m  [66/339], [94mLoss[0m : 2.48820
[1mStep[0m  [99/339], [94mLoss[0m : 1.94048
[1mStep[0m  [132/339], [94mLoss[0m : 2.44965
[1mStep[0m  [165/339], [94mLoss[0m : 2.62925
[1mStep[0m  [198/339], [94mLoss[0m : 2.39468
[1mStep[0m  [231/339], [94mLoss[0m : 2.36226
[1mStep[0m  [264/339], [94mLoss[0m : 2.38885
[1mStep[0m  [297/339], [94mLoss[0m : 2.46547
[1mStep[0m  [330/339], [94mLoss[0m : 2.43554

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54405
[1mStep[0m  [33/339], [94mLoss[0m : 2.27093
[1mStep[0m  [66/339], [94mLoss[0m : 3.39232
[1mStep[0m  [99/339], [94mLoss[0m : 1.98964
[1mStep[0m  [132/339], [94mLoss[0m : 2.56436
[1mStep[0m  [165/339], [94mLoss[0m : 2.51568
[1mStep[0m  [198/339], [94mLoss[0m : 2.03454
[1mStep[0m  [231/339], [94mLoss[0m : 2.65052
[1mStep[0m  [264/339], [94mLoss[0m : 2.50535
[1mStep[0m  [297/339], [94mLoss[0m : 2.41061
[1mStep[0m  [330/339], [94mLoss[0m : 2.79741

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.3564302710305274
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.10701
[1mStep[0m  [33/339], [94mLoss[0m : 2.69658
[1mStep[0m  [66/339], [94mLoss[0m : 2.16481
[1mStep[0m  [99/339], [94mLoss[0m : 2.42188
[1mStep[0m  [132/339], [94mLoss[0m : 2.18879
[1mStep[0m  [165/339], [94mLoss[0m : 2.36026
[1mStep[0m  [198/339], [94mLoss[0m : 1.99246
[1mStep[0m  [231/339], [94mLoss[0m : 2.68223
[1mStep[0m  [264/339], [94mLoss[0m : 2.97081
[1mStep[0m  [297/339], [94mLoss[0m : 2.40894
[1mStep[0m  [330/339], [94mLoss[0m : 2.43110

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08249
[1mStep[0m  [33/339], [94mLoss[0m : 2.76240
[1mStep[0m  [66/339], [94mLoss[0m : 2.20576
[1mStep[0m  [99/339], [94mLoss[0m : 2.45621
[1mStep[0m  [132/339], [94mLoss[0m : 2.84829
[1mStep[0m  [165/339], [94mLoss[0m : 2.71251
[1mStep[0m  [198/339], [94mLoss[0m : 2.69065
[1mStep[0m  [231/339], [94mLoss[0m : 2.66544
[1mStep[0m  [264/339], [94mLoss[0m : 2.71122
[1mStep[0m  [297/339], [94mLoss[0m : 2.08319
[1mStep[0m  [330/339], [94mLoss[0m : 2.26329

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76497
[1mStep[0m  [33/339], [94mLoss[0m : 2.60179
[1mStep[0m  [66/339], [94mLoss[0m : 2.23270
[1mStep[0m  [99/339], [94mLoss[0m : 2.91568
[1mStep[0m  [132/339], [94mLoss[0m : 2.43088
[1mStep[0m  [165/339], [94mLoss[0m : 2.99912
[1mStep[0m  [198/339], [94mLoss[0m : 3.13074
[1mStep[0m  [231/339], [94mLoss[0m : 1.85935
[1mStep[0m  [264/339], [94mLoss[0m : 2.28792
[1mStep[0m  [297/339], [94mLoss[0m : 2.01116
[1mStep[0m  [330/339], [94mLoss[0m : 2.72541

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65966
[1mStep[0m  [33/339], [94mLoss[0m : 2.27587
[1mStep[0m  [66/339], [94mLoss[0m : 2.36793
[1mStep[0m  [99/339], [94mLoss[0m : 1.85905
[1mStep[0m  [132/339], [94mLoss[0m : 2.00809
[1mStep[0m  [165/339], [94mLoss[0m : 2.76251
[1mStep[0m  [198/339], [94mLoss[0m : 1.52565
[1mStep[0m  [231/339], [94mLoss[0m : 3.12546
[1mStep[0m  [264/339], [94mLoss[0m : 2.42093
[1mStep[0m  [297/339], [94mLoss[0m : 2.35797
[1mStep[0m  [330/339], [94mLoss[0m : 2.69331

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25270
[1mStep[0m  [33/339], [94mLoss[0m : 2.01509
[1mStep[0m  [66/339], [94mLoss[0m : 2.86214
[1mStep[0m  [99/339], [94mLoss[0m : 1.99135
[1mStep[0m  [132/339], [94mLoss[0m : 2.49858
[1mStep[0m  [165/339], [94mLoss[0m : 2.66572
[1mStep[0m  [198/339], [94mLoss[0m : 2.19157
[1mStep[0m  [231/339], [94mLoss[0m : 2.61978
[1mStep[0m  [264/339], [94mLoss[0m : 2.30260
[1mStep[0m  [297/339], [94mLoss[0m : 2.06628
[1mStep[0m  [330/339], [94mLoss[0m : 2.04327

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31722
[1mStep[0m  [33/339], [94mLoss[0m : 2.70910
[1mStep[0m  [66/339], [94mLoss[0m : 2.04009
[1mStep[0m  [99/339], [94mLoss[0m : 2.27982
[1mStep[0m  [132/339], [94mLoss[0m : 2.35034
[1mStep[0m  [165/339], [94mLoss[0m : 2.53400
[1mStep[0m  [198/339], [94mLoss[0m : 2.33051
[1mStep[0m  [231/339], [94mLoss[0m : 2.43545
[1mStep[0m  [264/339], [94mLoss[0m : 1.98679
[1mStep[0m  [297/339], [94mLoss[0m : 2.42972
[1mStep[0m  [330/339], [94mLoss[0m : 2.34833

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12600
[1mStep[0m  [33/339], [94mLoss[0m : 2.35899
[1mStep[0m  [66/339], [94mLoss[0m : 1.86470
[1mStep[0m  [99/339], [94mLoss[0m : 2.37887
[1mStep[0m  [132/339], [94mLoss[0m : 1.94792
[1mStep[0m  [165/339], [94mLoss[0m : 2.94196
[1mStep[0m  [198/339], [94mLoss[0m : 2.66271
[1mStep[0m  [231/339], [94mLoss[0m : 2.37248
[1mStep[0m  [264/339], [94mLoss[0m : 2.50870
[1mStep[0m  [297/339], [94mLoss[0m : 2.20501
[1mStep[0m  [330/339], [94mLoss[0m : 2.20317

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.239, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14572
[1mStep[0m  [33/339], [94mLoss[0m : 2.61811
[1mStep[0m  [66/339], [94mLoss[0m : 2.33190
[1mStep[0m  [99/339], [94mLoss[0m : 1.64824
[1mStep[0m  [132/339], [94mLoss[0m : 2.14226
[1mStep[0m  [165/339], [94mLoss[0m : 1.53613
[1mStep[0m  [198/339], [94mLoss[0m : 2.02318
[1mStep[0m  [231/339], [94mLoss[0m : 2.01570
[1mStep[0m  [264/339], [94mLoss[0m : 2.41239
[1mStep[0m  [297/339], [94mLoss[0m : 2.48671
[1mStep[0m  [330/339], [94mLoss[0m : 2.38568

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.239, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50681
[1mStep[0m  [33/339], [94mLoss[0m : 2.26166
[1mStep[0m  [66/339], [94mLoss[0m : 1.76896
[1mStep[0m  [99/339], [94mLoss[0m : 2.17697
[1mStep[0m  [132/339], [94mLoss[0m : 1.56093
[1mStep[0m  [165/339], [94mLoss[0m : 2.30626
[1mStep[0m  [198/339], [94mLoss[0m : 2.22570
[1mStep[0m  [231/339], [94mLoss[0m : 3.06304
[1mStep[0m  [264/339], [94mLoss[0m : 2.87297
[1mStep[0m  [297/339], [94mLoss[0m : 1.82750
[1mStep[0m  [330/339], [94mLoss[0m : 2.28821

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.204, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83516
[1mStep[0m  [33/339], [94mLoss[0m : 2.08313
[1mStep[0m  [66/339], [94mLoss[0m : 1.95043
[1mStep[0m  [99/339], [94mLoss[0m : 2.50116
[1mStep[0m  [132/339], [94mLoss[0m : 1.77382
[1mStep[0m  [165/339], [94mLoss[0m : 1.87716
[1mStep[0m  [198/339], [94mLoss[0m : 2.62136
[1mStep[0m  [231/339], [94mLoss[0m : 2.23410
[1mStep[0m  [264/339], [94mLoss[0m : 2.18034
[1mStep[0m  [297/339], [94mLoss[0m : 2.02838
[1mStep[0m  [330/339], [94mLoss[0m : 2.44039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.204, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43847
[1mStep[0m  [33/339], [94mLoss[0m : 2.75154
[1mStep[0m  [66/339], [94mLoss[0m : 2.02514
[1mStep[0m  [99/339], [94mLoss[0m : 1.80277
[1mStep[0m  [132/339], [94mLoss[0m : 1.52852
[1mStep[0m  [165/339], [94mLoss[0m : 2.07336
[1mStep[0m  [198/339], [94mLoss[0m : 1.58607
[1mStep[0m  [231/339], [94mLoss[0m : 1.83342
[1mStep[0m  [264/339], [94mLoss[0m : 1.89406
[1mStep[0m  [297/339], [94mLoss[0m : 2.39665
[1mStep[0m  [330/339], [94mLoss[0m : 2.32189

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.173, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70227
[1mStep[0m  [33/339], [94mLoss[0m : 2.50272
[1mStep[0m  [66/339], [94mLoss[0m : 2.41210
[1mStep[0m  [99/339], [94mLoss[0m : 2.27956
[1mStep[0m  [132/339], [94mLoss[0m : 1.84299
[1mStep[0m  [165/339], [94mLoss[0m : 2.22103
[1mStep[0m  [198/339], [94mLoss[0m : 2.11395
[1mStep[0m  [231/339], [94mLoss[0m : 1.64529
[1mStep[0m  [264/339], [94mLoss[0m : 1.56109
[1mStep[0m  [297/339], [94mLoss[0m : 2.05935
[1mStep[0m  [330/339], [94mLoss[0m : 2.26322

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04933
[1mStep[0m  [33/339], [94mLoss[0m : 2.03766
[1mStep[0m  [66/339], [94mLoss[0m : 2.34905
[1mStep[0m  [99/339], [94mLoss[0m : 1.89296
[1mStep[0m  [132/339], [94mLoss[0m : 1.66712
[1mStep[0m  [165/339], [94mLoss[0m : 1.81793
[1mStep[0m  [198/339], [94mLoss[0m : 2.96148
[1mStep[0m  [231/339], [94mLoss[0m : 2.41198
[1mStep[0m  [264/339], [94mLoss[0m : 1.80557
[1mStep[0m  [297/339], [94mLoss[0m : 2.18115
[1mStep[0m  [330/339], [94mLoss[0m : 3.29661

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85334
[1mStep[0m  [33/339], [94mLoss[0m : 1.65876
[1mStep[0m  [66/339], [94mLoss[0m : 1.69351
[1mStep[0m  [99/339], [94mLoss[0m : 1.79902
[1mStep[0m  [132/339], [94mLoss[0m : 2.14626
[1mStep[0m  [165/339], [94mLoss[0m : 2.34104
[1mStep[0m  [198/339], [94mLoss[0m : 1.80878
[1mStep[0m  [231/339], [94mLoss[0m : 1.83312
[1mStep[0m  [264/339], [94mLoss[0m : 2.09776
[1mStep[0m  [297/339], [94mLoss[0m : 2.07158
[1mStep[0m  [330/339], [94mLoss[0m : 2.31344

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71558
[1mStep[0m  [33/339], [94mLoss[0m : 2.08770
[1mStep[0m  [66/339], [94mLoss[0m : 2.54391
[1mStep[0m  [99/339], [94mLoss[0m : 2.35728
[1mStep[0m  [132/339], [94mLoss[0m : 1.88133
[1mStep[0m  [165/339], [94mLoss[0m : 1.82864
[1mStep[0m  [198/339], [94mLoss[0m : 1.81638
[1mStep[0m  [231/339], [94mLoss[0m : 2.00388
[1mStep[0m  [264/339], [94mLoss[0m : 1.86581
[1mStep[0m  [297/339], [94mLoss[0m : 1.52656
[1mStep[0m  [330/339], [94mLoss[0m : 1.84916

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53886
[1mStep[0m  [33/339], [94mLoss[0m : 2.10138
[1mStep[0m  [66/339], [94mLoss[0m : 2.08412
[1mStep[0m  [99/339], [94mLoss[0m : 1.53408
[1mStep[0m  [132/339], [94mLoss[0m : 2.43463
[1mStep[0m  [165/339], [94mLoss[0m : 2.38337
[1mStep[0m  [198/339], [94mLoss[0m : 2.40100
[1mStep[0m  [231/339], [94mLoss[0m : 2.11276
[1mStep[0m  [264/339], [94mLoss[0m : 1.80081
[1mStep[0m  [297/339], [94mLoss[0m : 1.69174
[1mStep[0m  [330/339], [94mLoss[0m : 1.86497

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26617
[1mStep[0m  [33/339], [94mLoss[0m : 1.71572
[1mStep[0m  [66/339], [94mLoss[0m : 2.00194
[1mStep[0m  [99/339], [94mLoss[0m : 2.32931
[1mStep[0m  [132/339], [94mLoss[0m : 2.06913
[1mStep[0m  [165/339], [94mLoss[0m : 1.77954
[1mStep[0m  [198/339], [94mLoss[0m : 1.96548
[1mStep[0m  [231/339], [94mLoss[0m : 1.69859
[1mStep[0m  [264/339], [94mLoss[0m : 2.05417
[1mStep[0m  [297/339], [94mLoss[0m : 2.18995
[1mStep[0m  [330/339], [94mLoss[0m : 1.94027

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20263
[1mStep[0m  [33/339], [94mLoss[0m : 2.39070
[1mStep[0m  [66/339], [94mLoss[0m : 2.18165
[1mStep[0m  [99/339], [94mLoss[0m : 2.04687
[1mStep[0m  [132/339], [94mLoss[0m : 2.00766
[1mStep[0m  [165/339], [94mLoss[0m : 2.18600
[1mStep[0m  [198/339], [94mLoss[0m : 1.85313
[1mStep[0m  [231/339], [94mLoss[0m : 2.00352
[1mStep[0m  [264/339], [94mLoss[0m : 2.96747
[1mStep[0m  [297/339], [94mLoss[0m : 2.09770
[1mStep[0m  [330/339], [94mLoss[0m : 3.01704

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69589
[1mStep[0m  [33/339], [94mLoss[0m : 2.43299
[1mStep[0m  [66/339], [94mLoss[0m : 2.54852
[1mStep[0m  [99/339], [94mLoss[0m : 2.26216
[1mStep[0m  [132/339], [94mLoss[0m : 2.07038
[1mStep[0m  [165/339], [94mLoss[0m : 2.15509
[1mStep[0m  [198/339], [94mLoss[0m : 2.22660
[1mStep[0m  [231/339], [94mLoss[0m : 2.16460
[1mStep[0m  [264/339], [94mLoss[0m : 2.18869
[1mStep[0m  [297/339], [94mLoss[0m : 2.15007
[1mStep[0m  [330/339], [94mLoss[0m : 2.45570

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99049
[1mStep[0m  [33/339], [94mLoss[0m : 2.01199
[1mStep[0m  [66/339], [94mLoss[0m : 2.22063
[1mStep[0m  [99/339], [94mLoss[0m : 1.53786
[1mStep[0m  [132/339], [94mLoss[0m : 2.58885
[1mStep[0m  [165/339], [94mLoss[0m : 2.30850
[1mStep[0m  [198/339], [94mLoss[0m : 3.25745
[1mStep[0m  [231/339], [94mLoss[0m : 1.99378
[1mStep[0m  [264/339], [94mLoss[0m : 1.98532
[1mStep[0m  [297/339], [94mLoss[0m : 2.33621
[1mStep[0m  [330/339], [94mLoss[0m : 2.55686

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.395, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68064
[1mStep[0m  [33/339], [94mLoss[0m : 2.38390
[1mStep[0m  [66/339], [94mLoss[0m : 2.27505
[1mStep[0m  [99/339], [94mLoss[0m : 1.80038
[1mStep[0m  [132/339], [94mLoss[0m : 2.23439
[1mStep[0m  [165/339], [94mLoss[0m : 1.68779
[1mStep[0m  [198/339], [94mLoss[0m : 2.19359
[1mStep[0m  [231/339], [94mLoss[0m : 1.83500
[1mStep[0m  [264/339], [94mLoss[0m : 1.86835
[1mStep[0m  [297/339], [94mLoss[0m : 2.19641
[1mStep[0m  [330/339], [94mLoss[0m : 1.98706

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.523, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77627
[1mStep[0m  [33/339], [94mLoss[0m : 1.94698
[1mStep[0m  [66/339], [94mLoss[0m : 1.96859
[1mStep[0m  [99/339], [94mLoss[0m : 1.79452
[1mStep[0m  [132/339], [94mLoss[0m : 1.78667
[1mStep[0m  [165/339], [94mLoss[0m : 1.95377
[1mStep[0m  [198/339], [94mLoss[0m : 1.89726
[1mStep[0m  [231/339], [94mLoss[0m : 2.77544
[1mStep[0m  [264/339], [94mLoss[0m : 2.26184
[1mStep[0m  [297/339], [94mLoss[0m : 2.11914
[1mStep[0m  [330/339], [94mLoss[0m : 1.99367

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.452, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31879
[1mStep[0m  [33/339], [94mLoss[0m : 2.16313
[1mStep[0m  [66/339], [94mLoss[0m : 1.90766
[1mStep[0m  [99/339], [94mLoss[0m : 1.69484
[1mStep[0m  [132/339], [94mLoss[0m : 2.36358
[1mStep[0m  [165/339], [94mLoss[0m : 1.66278
[1mStep[0m  [198/339], [94mLoss[0m : 1.76647
[1mStep[0m  [231/339], [94mLoss[0m : 1.79472
[1mStep[0m  [264/339], [94mLoss[0m : 2.83670
[1mStep[0m  [297/339], [94mLoss[0m : 1.64671
[1mStep[0m  [330/339], [94mLoss[0m : 2.06972

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62518
[1mStep[0m  [33/339], [94mLoss[0m : 1.94378
[1mStep[0m  [66/339], [94mLoss[0m : 2.46000
[1mStep[0m  [99/339], [94mLoss[0m : 1.72179
[1mStep[0m  [132/339], [94mLoss[0m : 2.07894
[1mStep[0m  [165/339], [94mLoss[0m : 1.90285
[1mStep[0m  [198/339], [94mLoss[0m : 2.41257
[1mStep[0m  [231/339], [94mLoss[0m : 1.86969
[1mStep[0m  [264/339], [94mLoss[0m : 3.12709
[1mStep[0m  [297/339], [94mLoss[0m : 2.03238
[1mStep[0m  [330/339], [94mLoss[0m : 1.79963

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83605
[1mStep[0m  [33/339], [94mLoss[0m : 2.29922
[1mStep[0m  [66/339], [94mLoss[0m : 1.71972
[1mStep[0m  [99/339], [94mLoss[0m : 1.73773
[1mStep[0m  [132/339], [94mLoss[0m : 1.79355
[1mStep[0m  [165/339], [94mLoss[0m : 2.20781
[1mStep[0m  [198/339], [94mLoss[0m : 2.09516
[1mStep[0m  [231/339], [94mLoss[0m : 1.75542
[1mStep[0m  [264/339], [94mLoss[0m : 1.92235
[1mStep[0m  [297/339], [94mLoss[0m : 2.01865
[1mStep[0m  [330/339], [94mLoss[0m : 1.52018

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96387
[1mStep[0m  [33/339], [94mLoss[0m : 2.40656
[1mStep[0m  [66/339], [94mLoss[0m : 1.55484
[1mStep[0m  [99/339], [94mLoss[0m : 2.12906
[1mStep[0m  [132/339], [94mLoss[0m : 2.17807
[1mStep[0m  [165/339], [94mLoss[0m : 2.01933
[1mStep[0m  [198/339], [94mLoss[0m : 1.77925
[1mStep[0m  [231/339], [94mLoss[0m : 1.35672
[1mStep[0m  [264/339], [94mLoss[0m : 2.33947
[1mStep[0m  [297/339], [94mLoss[0m : 2.14687
[1mStep[0m  [330/339], [94mLoss[0m : 2.12672

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59086
[1mStep[0m  [33/339], [94mLoss[0m : 1.75927
[1mStep[0m  [66/339], [94mLoss[0m : 1.88902
[1mStep[0m  [99/339], [94mLoss[0m : 1.95177
[1mStep[0m  [132/339], [94mLoss[0m : 2.22254
[1mStep[0m  [165/339], [94mLoss[0m : 1.99912
[1mStep[0m  [198/339], [94mLoss[0m : 1.78821
[1mStep[0m  [231/339], [94mLoss[0m : 2.29393
[1mStep[0m  [264/339], [94mLoss[0m : 2.10574
[1mStep[0m  [297/339], [94mLoss[0m : 1.97318
[1mStep[0m  [330/339], [94mLoss[0m : 1.74765

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.030, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92286
[1mStep[0m  [33/339], [94mLoss[0m : 2.19411
[1mStep[0m  [66/339], [94mLoss[0m : 2.48065
[1mStep[0m  [99/339], [94mLoss[0m : 1.47463
[1mStep[0m  [132/339], [94mLoss[0m : 2.44639
[1mStep[0m  [165/339], [94mLoss[0m : 2.33323
[1mStep[0m  [198/339], [94mLoss[0m : 1.88461
[1mStep[0m  [231/339], [94mLoss[0m : 1.30693
[1mStep[0m  [264/339], [94mLoss[0m : 1.68698
[1mStep[0m  [297/339], [94mLoss[0m : 1.76170
[1mStep[0m  [330/339], [94mLoss[0m : 2.08064

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.020, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84728
[1mStep[0m  [33/339], [94mLoss[0m : 1.76265
[1mStep[0m  [66/339], [94mLoss[0m : 2.24670
[1mStep[0m  [99/339], [94mLoss[0m : 2.02646
[1mStep[0m  [132/339], [94mLoss[0m : 1.84873
[1mStep[0m  [165/339], [94mLoss[0m : 1.87135
[1mStep[0m  [198/339], [94mLoss[0m : 2.04269
[1mStep[0m  [231/339], [94mLoss[0m : 1.87307
[1mStep[0m  [264/339], [94mLoss[0m : 2.24098
[1mStep[0m  [297/339], [94mLoss[0m : 2.09946
[1mStep[0m  [330/339], [94mLoss[0m : 2.08228

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.468, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26614
[1mStep[0m  [33/339], [94mLoss[0m : 1.85689
[1mStep[0m  [66/339], [94mLoss[0m : 1.59343
[1mStep[0m  [99/339], [94mLoss[0m : 1.71866
[1mStep[0m  [132/339], [94mLoss[0m : 2.04088
[1mStep[0m  [165/339], [94mLoss[0m : 1.96119
[1mStep[0m  [198/339], [94mLoss[0m : 2.31776
[1mStep[0m  [231/339], [94mLoss[0m : 1.88845
[1mStep[0m  [264/339], [94mLoss[0m : 1.72355
[1mStep[0m  [297/339], [94mLoss[0m : 2.46758
[1mStep[0m  [330/339], [94mLoss[0m : 1.88622

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.430
====================================

Phase 2 - Evaluation MAE:  2.4303002568472802
MAE score P1       2.35643
MAE score P2        2.4303
loss              2.017932
learning_rate     0.002575
batch_size              32
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.31420
[1mStep[0m  [33/339], [94mLoss[0m : 5.50501
[1mStep[0m  [66/339], [94mLoss[0m : 4.13122
[1mStep[0m  [99/339], [94mLoss[0m : 3.64750
[1mStep[0m  [132/339], [94mLoss[0m : 2.28227
[1mStep[0m  [165/339], [94mLoss[0m : 2.44959
[1mStep[0m  [198/339], [94mLoss[0m : 3.14942
[1mStep[0m  [231/339], [94mLoss[0m : 2.48489
[1mStep[0m  [264/339], [94mLoss[0m : 2.01004
[1mStep[0m  [297/339], [94mLoss[0m : 2.99586
[1mStep[0m  [330/339], [94mLoss[0m : 2.04844

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.630, [92mTest[0m: 10.808, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32516
[1mStep[0m  [33/339], [94mLoss[0m : 2.50984
[1mStep[0m  [66/339], [94mLoss[0m : 1.94412
[1mStep[0m  [99/339], [94mLoss[0m : 2.82228
[1mStep[0m  [132/339], [94mLoss[0m : 2.47193
[1mStep[0m  [165/339], [94mLoss[0m : 2.50779
[1mStep[0m  [198/339], [94mLoss[0m : 2.34251
[1mStep[0m  [231/339], [94mLoss[0m : 2.06648
[1mStep[0m  [264/339], [94mLoss[0m : 2.13871
[1mStep[0m  [297/339], [94mLoss[0m : 3.16133
[1mStep[0m  [330/339], [94mLoss[0m : 2.12195

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.658, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30705
[1mStep[0m  [33/339], [94mLoss[0m : 2.64495
[1mStep[0m  [66/339], [94mLoss[0m : 1.95250
[1mStep[0m  [99/339], [94mLoss[0m : 2.42345
[1mStep[0m  [132/339], [94mLoss[0m : 2.57482
[1mStep[0m  [165/339], [94mLoss[0m : 2.56134
[1mStep[0m  [198/339], [94mLoss[0m : 2.48097
[1mStep[0m  [231/339], [94mLoss[0m : 2.82653
[1mStep[0m  [264/339], [94mLoss[0m : 3.00110
[1mStep[0m  [297/339], [94mLoss[0m : 1.65341
[1mStep[0m  [330/339], [94mLoss[0m : 2.09551

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24447
[1mStep[0m  [33/339], [94mLoss[0m : 2.64311
[1mStep[0m  [66/339], [94mLoss[0m : 2.03370
[1mStep[0m  [99/339], [94mLoss[0m : 2.87188
[1mStep[0m  [132/339], [94mLoss[0m : 1.84823
[1mStep[0m  [165/339], [94mLoss[0m : 2.74145
[1mStep[0m  [198/339], [94mLoss[0m : 2.32477
[1mStep[0m  [231/339], [94mLoss[0m : 1.97822
[1mStep[0m  [264/339], [94mLoss[0m : 4.03872
[1mStep[0m  [297/339], [94mLoss[0m : 2.39122
[1mStep[0m  [330/339], [94mLoss[0m : 3.25251

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.98832
[1mStep[0m  [33/339], [94mLoss[0m : 1.77049
[1mStep[0m  [66/339], [94mLoss[0m : 2.54344
[1mStep[0m  [99/339], [94mLoss[0m : 2.59542
[1mStep[0m  [132/339], [94mLoss[0m : 1.90074
[1mStep[0m  [165/339], [94mLoss[0m : 2.53475
[1mStep[0m  [198/339], [94mLoss[0m : 2.88508
[1mStep[0m  [231/339], [94mLoss[0m : 2.30533
[1mStep[0m  [264/339], [94mLoss[0m : 2.48371
[1mStep[0m  [297/339], [94mLoss[0m : 2.82117
[1mStep[0m  [330/339], [94mLoss[0m : 2.20306

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16244
[1mStep[0m  [33/339], [94mLoss[0m : 2.30444
[1mStep[0m  [66/339], [94mLoss[0m : 2.75287
[1mStep[0m  [99/339], [94mLoss[0m : 2.43233
[1mStep[0m  [132/339], [94mLoss[0m : 2.14406
[1mStep[0m  [165/339], [94mLoss[0m : 2.26800
[1mStep[0m  [198/339], [94mLoss[0m : 2.29383
[1mStep[0m  [231/339], [94mLoss[0m : 2.17632
[1mStep[0m  [264/339], [94mLoss[0m : 2.41276
[1mStep[0m  [297/339], [94mLoss[0m : 2.97823
[1mStep[0m  [330/339], [94mLoss[0m : 2.56595

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.11379
[1mStep[0m  [33/339], [94mLoss[0m : 3.02922
[1mStep[0m  [66/339], [94mLoss[0m : 2.54584
[1mStep[0m  [99/339], [94mLoss[0m : 2.04181
[1mStep[0m  [132/339], [94mLoss[0m : 2.12271
[1mStep[0m  [165/339], [94mLoss[0m : 2.64851
[1mStep[0m  [198/339], [94mLoss[0m : 2.63426
[1mStep[0m  [231/339], [94mLoss[0m : 1.92670
[1mStep[0m  [264/339], [94mLoss[0m : 2.48265
[1mStep[0m  [297/339], [94mLoss[0m : 3.00882
[1mStep[0m  [330/339], [94mLoss[0m : 2.43597

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02810
[1mStep[0m  [33/339], [94mLoss[0m : 2.54664
[1mStep[0m  [66/339], [94mLoss[0m : 2.00984
[1mStep[0m  [99/339], [94mLoss[0m : 2.57099
[1mStep[0m  [132/339], [94mLoss[0m : 2.55062
[1mStep[0m  [165/339], [94mLoss[0m : 2.35139
[1mStep[0m  [198/339], [94mLoss[0m : 2.70385
[1mStep[0m  [231/339], [94mLoss[0m : 2.24149
[1mStep[0m  [264/339], [94mLoss[0m : 2.26782
[1mStep[0m  [297/339], [94mLoss[0m : 2.82084
[1mStep[0m  [330/339], [94mLoss[0m : 2.84924

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70547
[1mStep[0m  [33/339], [94mLoss[0m : 1.70313
[1mStep[0m  [66/339], [94mLoss[0m : 2.24098
[1mStep[0m  [99/339], [94mLoss[0m : 2.93646
[1mStep[0m  [132/339], [94mLoss[0m : 3.15655
[1mStep[0m  [165/339], [94mLoss[0m : 2.99570
[1mStep[0m  [198/339], [94mLoss[0m : 2.30072
[1mStep[0m  [231/339], [94mLoss[0m : 3.30546
[1mStep[0m  [264/339], [94mLoss[0m : 2.21494
[1mStep[0m  [297/339], [94mLoss[0m : 2.56383
[1mStep[0m  [330/339], [94mLoss[0m : 2.43934

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53067
[1mStep[0m  [33/339], [94mLoss[0m : 2.65609
[1mStep[0m  [66/339], [94mLoss[0m : 2.37663
[1mStep[0m  [99/339], [94mLoss[0m : 1.85489
[1mStep[0m  [132/339], [94mLoss[0m : 2.37932
[1mStep[0m  [165/339], [94mLoss[0m : 2.33842
[1mStep[0m  [198/339], [94mLoss[0m : 2.09291
[1mStep[0m  [231/339], [94mLoss[0m : 2.68774
[1mStep[0m  [264/339], [94mLoss[0m : 3.16484
[1mStep[0m  [297/339], [94mLoss[0m : 2.32503
[1mStep[0m  [330/339], [94mLoss[0m : 2.85940

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67677
[1mStep[0m  [33/339], [94mLoss[0m : 2.74218
[1mStep[0m  [66/339], [94mLoss[0m : 2.80135
[1mStep[0m  [99/339], [94mLoss[0m : 2.75273
[1mStep[0m  [132/339], [94mLoss[0m : 3.40692
[1mStep[0m  [165/339], [94mLoss[0m : 2.82459
[1mStep[0m  [198/339], [94mLoss[0m : 2.18691
[1mStep[0m  [231/339], [94mLoss[0m : 2.28684
[1mStep[0m  [264/339], [94mLoss[0m : 2.06516
[1mStep[0m  [297/339], [94mLoss[0m : 2.34191
[1mStep[0m  [330/339], [94mLoss[0m : 1.84369

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05538
[1mStep[0m  [33/339], [94mLoss[0m : 2.26489
[1mStep[0m  [66/339], [94mLoss[0m : 2.71858
[1mStep[0m  [99/339], [94mLoss[0m : 2.72360
[1mStep[0m  [132/339], [94mLoss[0m : 2.81847
[1mStep[0m  [165/339], [94mLoss[0m : 2.42739
[1mStep[0m  [198/339], [94mLoss[0m : 2.07929
[1mStep[0m  [231/339], [94mLoss[0m : 2.95805
[1mStep[0m  [264/339], [94mLoss[0m : 2.21351
[1mStep[0m  [297/339], [94mLoss[0m : 2.21362
[1mStep[0m  [330/339], [94mLoss[0m : 2.36821

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15670
[1mStep[0m  [33/339], [94mLoss[0m : 2.50643
[1mStep[0m  [66/339], [94mLoss[0m : 2.51408
[1mStep[0m  [99/339], [94mLoss[0m : 2.92865
[1mStep[0m  [132/339], [94mLoss[0m : 2.25525
[1mStep[0m  [165/339], [94mLoss[0m : 2.34341
[1mStep[0m  [198/339], [94mLoss[0m : 2.83151
[1mStep[0m  [231/339], [94mLoss[0m : 2.63220
[1mStep[0m  [264/339], [94mLoss[0m : 2.78925
[1mStep[0m  [297/339], [94mLoss[0m : 2.31577
[1mStep[0m  [330/339], [94mLoss[0m : 2.08648

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50267
[1mStep[0m  [33/339], [94mLoss[0m : 2.28339
[1mStep[0m  [66/339], [94mLoss[0m : 1.98449
[1mStep[0m  [99/339], [94mLoss[0m : 2.08283
[1mStep[0m  [132/339], [94mLoss[0m : 2.65534
[1mStep[0m  [165/339], [94mLoss[0m : 2.21987
[1mStep[0m  [198/339], [94mLoss[0m : 2.04939
[1mStep[0m  [231/339], [94mLoss[0m : 2.09978
[1mStep[0m  [264/339], [94mLoss[0m : 2.65974
[1mStep[0m  [297/339], [94mLoss[0m : 2.42595
[1mStep[0m  [330/339], [94mLoss[0m : 2.31990

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65839
[1mStep[0m  [33/339], [94mLoss[0m : 2.12889
[1mStep[0m  [66/339], [94mLoss[0m : 2.24435
[1mStep[0m  [99/339], [94mLoss[0m : 2.60288
[1mStep[0m  [132/339], [94mLoss[0m : 2.35267
[1mStep[0m  [165/339], [94mLoss[0m : 2.14732
[1mStep[0m  [198/339], [94mLoss[0m : 1.67082
[1mStep[0m  [231/339], [94mLoss[0m : 2.70194
[1mStep[0m  [264/339], [94mLoss[0m : 2.39513
[1mStep[0m  [297/339], [94mLoss[0m : 2.25091
[1mStep[0m  [330/339], [94mLoss[0m : 2.59812

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80635
[1mStep[0m  [33/339], [94mLoss[0m : 2.47492
[1mStep[0m  [66/339], [94mLoss[0m : 1.77769
[1mStep[0m  [99/339], [94mLoss[0m : 1.83239
[1mStep[0m  [132/339], [94mLoss[0m : 2.39546
[1mStep[0m  [165/339], [94mLoss[0m : 2.70801
[1mStep[0m  [198/339], [94mLoss[0m : 2.11447
[1mStep[0m  [231/339], [94mLoss[0m : 2.56529
[1mStep[0m  [264/339], [94mLoss[0m : 3.27076
[1mStep[0m  [297/339], [94mLoss[0m : 2.89467
[1mStep[0m  [330/339], [94mLoss[0m : 2.23015

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56068
[1mStep[0m  [33/339], [94mLoss[0m : 3.07563
[1mStep[0m  [66/339], [94mLoss[0m : 2.83665
[1mStep[0m  [99/339], [94mLoss[0m : 2.19802
[1mStep[0m  [132/339], [94mLoss[0m : 2.61515
[1mStep[0m  [165/339], [94mLoss[0m : 2.48326
[1mStep[0m  [198/339], [94mLoss[0m : 2.27592
[1mStep[0m  [231/339], [94mLoss[0m : 2.53220
[1mStep[0m  [264/339], [94mLoss[0m : 2.67518
[1mStep[0m  [297/339], [94mLoss[0m : 1.92139
[1mStep[0m  [330/339], [94mLoss[0m : 2.48523

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41066
[1mStep[0m  [33/339], [94mLoss[0m : 2.16482
[1mStep[0m  [66/339], [94mLoss[0m : 2.01634
[1mStep[0m  [99/339], [94mLoss[0m : 2.63428
[1mStep[0m  [132/339], [94mLoss[0m : 2.72666
[1mStep[0m  [165/339], [94mLoss[0m : 2.39985
[1mStep[0m  [198/339], [94mLoss[0m : 2.54418
[1mStep[0m  [231/339], [94mLoss[0m : 2.74069
[1mStep[0m  [264/339], [94mLoss[0m : 2.45457
[1mStep[0m  [297/339], [94mLoss[0m : 2.99172
[1mStep[0m  [330/339], [94mLoss[0m : 2.60227

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83258
[1mStep[0m  [33/339], [94mLoss[0m : 2.12491
[1mStep[0m  [66/339], [94mLoss[0m : 2.66017
[1mStep[0m  [99/339], [94mLoss[0m : 3.10411
[1mStep[0m  [132/339], [94mLoss[0m : 2.02425
[1mStep[0m  [165/339], [94mLoss[0m : 2.09385
[1mStep[0m  [198/339], [94mLoss[0m : 2.57420
[1mStep[0m  [231/339], [94mLoss[0m : 2.39269
[1mStep[0m  [264/339], [94mLoss[0m : 2.45859
[1mStep[0m  [297/339], [94mLoss[0m : 2.43037
[1mStep[0m  [330/339], [94mLoss[0m : 2.17435

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.08834
[1mStep[0m  [33/339], [94mLoss[0m : 2.70786
[1mStep[0m  [66/339], [94mLoss[0m : 2.73133
[1mStep[0m  [99/339], [94mLoss[0m : 2.32877
[1mStep[0m  [132/339], [94mLoss[0m : 2.11308
[1mStep[0m  [165/339], [94mLoss[0m : 3.09845
[1mStep[0m  [198/339], [94mLoss[0m : 2.62790
[1mStep[0m  [231/339], [94mLoss[0m : 2.34261
[1mStep[0m  [264/339], [94mLoss[0m : 2.25913
[1mStep[0m  [297/339], [94mLoss[0m : 1.89619
[1mStep[0m  [330/339], [94mLoss[0m : 2.47631

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56950
[1mStep[0m  [33/339], [94mLoss[0m : 2.62016
[1mStep[0m  [66/339], [94mLoss[0m : 1.99814
[1mStep[0m  [99/339], [94mLoss[0m : 1.89343
[1mStep[0m  [132/339], [94mLoss[0m : 2.17500
[1mStep[0m  [165/339], [94mLoss[0m : 2.16395
[1mStep[0m  [198/339], [94mLoss[0m : 2.04266
[1mStep[0m  [231/339], [94mLoss[0m : 2.44577
[1mStep[0m  [264/339], [94mLoss[0m : 2.68954
[1mStep[0m  [297/339], [94mLoss[0m : 2.33630
[1mStep[0m  [330/339], [94mLoss[0m : 2.87762

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24372
[1mStep[0m  [33/339], [94mLoss[0m : 2.71379
[1mStep[0m  [66/339], [94mLoss[0m : 1.92199
[1mStep[0m  [99/339], [94mLoss[0m : 2.07836
[1mStep[0m  [132/339], [94mLoss[0m : 2.22834
[1mStep[0m  [165/339], [94mLoss[0m : 2.47240
[1mStep[0m  [198/339], [94mLoss[0m : 2.25309
[1mStep[0m  [231/339], [94mLoss[0m : 2.21387
[1mStep[0m  [264/339], [94mLoss[0m : 2.38372
[1mStep[0m  [297/339], [94mLoss[0m : 2.51364
[1mStep[0m  [330/339], [94mLoss[0m : 2.67003

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.394, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33761
[1mStep[0m  [33/339], [94mLoss[0m : 2.53782
[1mStep[0m  [66/339], [94mLoss[0m : 2.82388
[1mStep[0m  [99/339], [94mLoss[0m : 2.67223
[1mStep[0m  [132/339], [94mLoss[0m : 2.79919
[1mStep[0m  [165/339], [94mLoss[0m : 2.59390
[1mStep[0m  [198/339], [94mLoss[0m : 3.10107
[1mStep[0m  [231/339], [94mLoss[0m : 3.10925
[1mStep[0m  [264/339], [94mLoss[0m : 2.41942
[1mStep[0m  [297/339], [94mLoss[0m : 1.97052
[1mStep[0m  [330/339], [94mLoss[0m : 2.68709

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76959
[1mStep[0m  [33/339], [94mLoss[0m : 2.54027
[1mStep[0m  [66/339], [94mLoss[0m : 2.33797
[1mStep[0m  [99/339], [94mLoss[0m : 2.60534
[1mStep[0m  [132/339], [94mLoss[0m : 2.34475
[1mStep[0m  [165/339], [94mLoss[0m : 2.18260
[1mStep[0m  [198/339], [94mLoss[0m : 2.93236
[1mStep[0m  [231/339], [94mLoss[0m : 2.30191
[1mStep[0m  [264/339], [94mLoss[0m : 1.99968
[1mStep[0m  [297/339], [94mLoss[0m : 3.06960
[1mStep[0m  [330/339], [94mLoss[0m : 2.56314

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83533
[1mStep[0m  [33/339], [94mLoss[0m : 3.06814
[1mStep[0m  [66/339], [94mLoss[0m : 3.21272
[1mStep[0m  [99/339], [94mLoss[0m : 1.98821
[1mStep[0m  [132/339], [94mLoss[0m : 2.11064
[1mStep[0m  [165/339], [94mLoss[0m : 2.19627
[1mStep[0m  [198/339], [94mLoss[0m : 2.51814
[1mStep[0m  [231/339], [94mLoss[0m : 1.51800
[1mStep[0m  [264/339], [94mLoss[0m : 2.49213
[1mStep[0m  [297/339], [94mLoss[0m : 1.91413
[1mStep[0m  [330/339], [94mLoss[0m : 1.66302

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97777
[1mStep[0m  [33/339], [94mLoss[0m : 2.09845
[1mStep[0m  [66/339], [94mLoss[0m : 2.13156
[1mStep[0m  [99/339], [94mLoss[0m : 2.55561
[1mStep[0m  [132/339], [94mLoss[0m : 2.03364
[1mStep[0m  [165/339], [94mLoss[0m : 2.10979
[1mStep[0m  [198/339], [94mLoss[0m : 1.92396
[1mStep[0m  [231/339], [94mLoss[0m : 2.54917
[1mStep[0m  [264/339], [94mLoss[0m : 2.43031
[1mStep[0m  [297/339], [94mLoss[0m : 2.22509
[1mStep[0m  [330/339], [94mLoss[0m : 1.97075

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.368, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58633
[1mStep[0m  [33/339], [94mLoss[0m : 2.29242
[1mStep[0m  [66/339], [94mLoss[0m : 2.74061
[1mStep[0m  [99/339], [94mLoss[0m : 2.16540
[1mStep[0m  [132/339], [94mLoss[0m : 2.75239
[1mStep[0m  [165/339], [94mLoss[0m : 3.04757
[1mStep[0m  [198/339], [94mLoss[0m : 2.34244
[1mStep[0m  [231/339], [94mLoss[0m : 2.60124
[1mStep[0m  [264/339], [94mLoss[0m : 2.60765
[1mStep[0m  [297/339], [94mLoss[0m : 2.46119
[1mStep[0m  [330/339], [94mLoss[0m : 1.83233

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78093
[1mStep[0m  [33/339], [94mLoss[0m : 2.96678
[1mStep[0m  [66/339], [94mLoss[0m : 1.81819
[1mStep[0m  [99/339], [94mLoss[0m : 2.40605
[1mStep[0m  [132/339], [94mLoss[0m : 2.92444
[1mStep[0m  [165/339], [94mLoss[0m : 2.33837
[1mStep[0m  [198/339], [94mLoss[0m : 2.23529
[1mStep[0m  [231/339], [94mLoss[0m : 2.15806
[1mStep[0m  [264/339], [94mLoss[0m : 1.87045
[1mStep[0m  [297/339], [94mLoss[0m : 2.18335
[1mStep[0m  [330/339], [94mLoss[0m : 2.04800

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77671
[1mStep[0m  [33/339], [94mLoss[0m : 1.98481
[1mStep[0m  [66/339], [94mLoss[0m : 1.98259
[1mStep[0m  [99/339], [94mLoss[0m : 2.20018
[1mStep[0m  [132/339], [94mLoss[0m : 2.16733
[1mStep[0m  [165/339], [94mLoss[0m : 2.87330
[1mStep[0m  [198/339], [94mLoss[0m : 2.50871
[1mStep[0m  [231/339], [94mLoss[0m : 2.31354
[1mStep[0m  [264/339], [94mLoss[0m : 2.12701
[1mStep[0m  [297/339], [94mLoss[0m : 1.94573
[1mStep[0m  [330/339], [94mLoss[0m : 2.00529

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59157
[1mStep[0m  [33/339], [94mLoss[0m : 2.74151
[1mStep[0m  [66/339], [94mLoss[0m : 2.12337
[1mStep[0m  [99/339], [94mLoss[0m : 2.38792
[1mStep[0m  [132/339], [94mLoss[0m : 2.40162
[1mStep[0m  [165/339], [94mLoss[0m : 2.12946
[1mStep[0m  [198/339], [94mLoss[0m : 1.94879
[1mStep[0m  [231/339], [94mLoss[0m : 2.33078
[1mStep[0m  [264/339], [94mLoss[0m : 2.58524
[1mStep[0m  [297/339], [94mLoss[0m : 2.19535
[1mStep[0m  [330/339], [94mLoss[0m : 2.15792

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.385, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.361
====================================

Phase 1 - Evaluation MAE:  2.360803297135682
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 1.59609
[1mStep[0m  [33/339], [94mLoss[0m : 1.88926
[1mStep[0m  [66/339], [94mLoss[0m : 2.10179
[1mStep[0m  [99/339], [94mLoss[0m : 1.90261
[1mStep[0m  [132/339], [94mLoss[0m : 2.50524
[1mStep[0m  [165/339], [94mLoss[0m : 2.51464
[1mStep[0m  [198/339], [94mLoss[0m : 2.71573
[1mStep[0m  [231/339], [94mLoss[0m : 2.13675
[1mStep[0m  [264/339], [94mLoss[0m : 2.41276
[1mStep[0m  [297/339], [94mLoss[0m : 2.56780
[1mStep[0m  [330/339], [94mLoss[0m : 2.21009

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31250
[1mStep[0m  [33/339], [94mLoss[0m : 1.86908
[1mStep[0m  [66/339], [94mLoss[0m : 1.71764
[1mStep[0m  [99/339], [94mLoss[0m : 2.53719
[1mStep[0m  [132/339], [94mLoss[0m : 1.95790
[1mStep[0m  [165/339], [94mLoss[0m : 2.86260
[1mStep[0m  [198/339], [94mLoss[0m : 2.36036
[1mStep[0m  [231/339], [94mLoss[0m : 2.23641
[1mStep[0m  [264/339], [94mLoss[0m : 2.06903
[1mStep[0m  [297/339], [94mLoss[0m : 2.63590
[1mStep[0m  [330/339], [94mLoss[0m : 2.52433

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91268
[1mStep[0m  [33/339], [94mLoss[0m : 3.50775
[1mStep[0m  [66/339], [94mLoss[0m : 2.12862
[1mStep[0m  [99/339], [94mLoss[0m : 2.19318
[1mStep[0m  [132/339], [94mLoss[0m : 1.75593
[1mStep[0m  [165/339], [94mLoss[0m : 1.95358
[1mStep[0m  [198/339], [94mLoss[0m : 2.02824
[1mStep[0m  [231/339], [94mLoss[0m : 2.21347
[1mStep[0m  [264/339], [94mLoss[0m : 3.17181
[1mStep[0m  [297/339], [94mLoss[0m : 2.24650
[1mStep[0m  [330/339], [94mLoss[0m : 2.61337

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36316
[1mStep[0m  [33/339], [94mLoss[0m : 1.85149
[1mStep[0m  [66/339], [94mLoss[0m : 1.84482
[1mStep[0m  [99/339], [94mLoss[0m : 1.85784
[1mStep[0m  [132/339], [94mLoss[0m : 2.53260
[1mStep[0m  [165/339], [94mLoss[0m : 2.28961
[1mStep[0m  [198/339], [94mLoss[0m : 1.90328
[1mStep[0m  [231/339], [94mLoss[0m : 1.92364
[1mStep[0m  [264/339], [94mLoss[0m : 1.87143
[1mStep[0m  [297/339], [94mLoss[0m : 1.82830
[1mStep[0m  [330/339], [94mLoss[0m : 2.28923

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78065
[1mStep[0m  [33/339], [94mLoss[0m : 2.29430
[1mStep[0m  [66/339], [94mLoss[0m : 2.76297
[1mStep[0m  [99/339], [94mLoss[0m : 2.49143
[1mStep[0m  [132/339], [94mLoss[0m : 1.80045
[1mStep[0m  [165/339], [94mLoss[0m : 2.96921
[1mStep[0m  [198/339], [94mLoss[0m : 2.16844
[1mStep[0m  [231/339], [94mLoss[0m : 1.85759
[1mStep[0m  [264/339], [94mLoss[0m : 2.42749
[1mStep[0m  [297/339], [94mLoss[0m : 2.89008
[1mStep[0m  [330/339], [94mLoss[0m : 2.22198

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.234, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04191
[1mStep[0m  [33/339], [94mLoss[0m : 2.04911
[1mStep[0m  [66/339], [94mLoss[0m : 1.78341
[1mStep[0m  [99/339], [94mLoss[0m : 2.01081
[1mStep[0m  [132/339], [94mLoss[0m : 2.12584
[1mStep[0m  [165/339], [94mLoss[0m : 2.51884
[1mStep[0m  [198/339], [94mLoss[0m : 1.78792
[1mStep[0m  [231/339], [94mLoss[0m : 2.23601
[1mStep[0m  [264/339], [94mLoss[0m : 1.62254
[1mStep[0m  [297/339], [94mLoss[0m : 2.00943
[1mStep[0m  [330/339], [94mLoss[0m : 1.84352

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28759
[1mStep[0m  [33/339], [94mLoss[0m : 2.28959
[1mStep[0m  [66/339], [94mLoss[0m : 2.13106
[1mStep[0m  [99/339], [94mLoss[0m : 2.46383
[1mStep[0m  [132/339], [94mLoss[0m : 2.67193
[1mStep[0m  [165/339], [94mLoss[0m : 1.85276
[1mStep[0m  [198/339], [94mLoss[0m : 2.14679
[1mStep[0m  [231/339], [94mLoss[0m : 2.32747
[1mStep[0m  [264/339], [94mLoss[0m : 1.81519
[1mStep[0m  [297/339], [94mLoss[0m : 1.73872
[1mStep[0m  [330/339], [94mLoss[0m : 1.94069

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.165, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21276
[1mStep[0m  [33/339], [94mLoss[0m : 2.04153
[1mStep[0m  [66/339], [94mLoss[0m : 2.32651
[1mStep[0m  [99/339], [94mLoss[0m : 1.71791
[1mStep[0m  [132/339], [94mLoss[0m : 1.79525
[1mStep[0m  [165/339], [94mLoss[0m : 2.85364
[1mStep[0m  [198/339], [94mLoss[0m : 2.52731
[1mStep[0m  [231/339], [94mLoss[0m : 2.62142
[1mStep[0m  [264/339], [94mLoss[0m : 2.09551
[1mStep[0m  [297/339], [94mLoss[0m : 2.24333
[1mStep[0m  [330/339], [94mLoss[0m : 1.79500

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34188
[1mStep[0m  [33/339], [94mLoss[0m : 1.98234
[1mStep[0m  [66/339], [94mLoss[0m : 2.13883
[1mStep[0m  [99/339], [94mLoss[0m : 1.65103
[1mStep[0m  [132/339], [94mLoss[0m : 1.73749
[1mStep[0m  [165/339], [94mLoss[0m : 1.91525
[1mStep[0m  [198/339], [94mLoss[0m : 2.09314
[1mStep[0m  [231/339], [94mLoss[0m : 2.56283
[1mStep[0m  [264/339], [94mLoss[0m : 1.68057
[1mStep[0m  [297/339], [94mLoss[0m : 2.35271
[1mStep[0m  [330/339], [94mLoss[0m : 2.89476

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.071, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.54887
[1mStep[0m  [33/339], [94mLoss[0m : 1.71210
[1mStep[0m  [66/339], [94mLoss[0m : 1.67879
[1mStep[0m  [99/339], [94mLoss[0m : 2.59728
[1mStep[0m  [132/339], [94mLoss[0m : 2.09773
[1mStep[0m  [165/339], [94mLoss[0m : 2.32590
[1mStep[0m  [198/339], [94mLoss[0m : 2.56932
[1mStep[0m  [231/339], [94mLoss[0m : 1.75370
[1mStep[0m  [264/339], [94mLoss[0m : 1.98879
[1mStep[0m  [297/339], [94mLoss[0m : 2.13765
[1mStep[0m  [330/339], [94mLoss[0m : 2.28765

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62040
[1mStep[0m  [33/339], [94mLoss[0m : 1.71581
[1mStep[0m  [66/339], [94mLoss[0m : 1.65360
[1mStep[0m  [99/339], [94mLoss[0m : 1.50851
[1mStep[0m  [132/339], [94mLoss[0m : 2.62130
[1mStep[0m  [165/339], [94mLoss[0m : 2.13650
[1mStep[0m  [198/339], [94mLoss[0m : 1.88573
[1mStep[0m  [231/339], [94mLoss[0m : 2.14057
[1mStep[0m  [264/339], [94mLoss[0m : 1.78747
[1mStep[0m  [297/339], [94mLoss[0m : 1.91696
[1mStep[0m  [330/339], [94mLoss[0m : 2.67528

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24239
[1mStep[0m  [33/339], [94mLoss[0m : 1.95594
[1mStep[0m  [66/339], [94mLoss[0m : 2.20672
[1mStep[0m  [99/339], [94mLoss[0m : 1.70948
[1mStep[0m  [132/339], [94mLoss[0m : 1.80964
[1mStep[0m  [165/339], [94mLoss[0m : 1.76298
[1mStep[0m  [198/339], [94mLoss[0m : 1.34457
[1mStep[0m  [231/339], [94mLoss[0m : 1.85435
[1mStep[0m  [264/339], [94mLoss[0m : 2.18956
[1mStep[0m  [297/339], [94mLoss[0m : 2.03937
[1mStep[0m  [330/339], [94mLoss[0m : 2.27169

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07061
[1mStep[0m  [33/339], [94mLoss[0m : 2.06227
[1mStep[0m  [66/339], [94mLoss[0m : 1.81303
[1mStep[0m  [99/339], [94mLoss[0m : 2.40291
[1mStep[0m  [132/339], [94mLoss[0m : 2.04996
[1mStep[0m  [165/339], [94mLoss[0m : 1.77951
[1mStep[0m  [198/339], [94mLoss[0m : 1.94123
[1mStep[0m  [231/339], [94mLoss[0m : 2.36254
[1mStep[0m  [264/339], [94mLoss[0m : 1.47125
[1mStep[0m  [297/339], [94mLoss[0m : 1.93732
[1mStep[0m  [330/339], [94mLoss[0m : 1.58250

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.939, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23139
[1mStep[0m  [33/339], [94mLoss[0m : 1.84798
[1mStep[0m  [66/339], [94mLoss[0m : 2.33795
[1mStep[0m  [99/339], [94mLoss[0m : 1.98238
[1mStep[0m  [132/339], [94mLoss[0m : 2.11153
[1mStep[0m  [165/339], [94mLoss[0m : 1.92630
[1mStep[0m  [198/339], [94mLoss[0m : 1.68796
[1mStep[0m  [231/339], [94mLoss[0m : 2.19426
[1mStep[0m  [264/339], [94mLoss[0m : 2.01072
[1mStep[0m  [297/339], [94mLoss[0m : 1.93903
[1mStep[0m  [330/339], [94mLoss[0m : 2.28827

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.908, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99708
[1mStep[0m  [33/339], [94mLoss[0m : 1.55076
[1mStep[0m  [66/339], [94mLoss[0m : 1.89477
[1mStep[0m  [99/339], [94mLoss[0m : 1.75686
[1mStep[0m  [132/339], [94mLoss[0m : 1.65405
[1mStep[0m  [165/339], [94mLoss[0m : 2.23781
[1mStep[0m  [198/339], [94mLoss[0m : 1.59083
[1mStep[0m  [231/339], [94mLoss[0m : 1.57767
[1mStep[0m  [264/339], [94mLoss[0m : 1.75602
[1mStep[0m  [297/339], [94mLoss[0m : 1.80563
[1mStep[0m  [330/339], [94mLoss[0m : 2.39846

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.864, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72410
[1mStep[0m  [33/339], [94mLoss[0m : 1.64417
[1mStep[0m  [66/339], [94mLoss[0m : 2.12388
[1mStep[0m  [99/339], [94mLoss[0m : 2.45311
[1mStep[0m  [132/339], [94mLoss[0m : 1.94808
[1mStep[0m  [165/339], [94mLoss[0m : 2.23263
[1mStep[0m  [198/339], [94mLoss[0m : 2.17326
[1mStep[0m  [231/339], [94mLoss[0m : 1.89212
[1mStep[0m  [264/339], [94mLoss[0m : 1.63311
[1mStep[0m  [297/339], [94mLoss[0m : 1.90649
[1mStep[0m  [330/339], [94mLoss[0m : 1.92533

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12327
[1mStep[0m  [33/339], [94mLoss[0m : 2.01498
[1mStep[0m  [66/339], [94mLoss[0m : 1.87093
[1mStep[0m  [99/339], [94mLoss[0m : 1.57834
[1mStep[0m  [132/339], [94mLoss[0m : 1.68112
[1mStep[0m  [165/339], [94mLoss[0m : 1.52483
[1mStep[0m  [198/339], [94mLoss[0m : 0.91332
[1mStep[0m  [231/339], [94mLoss[0m : 2.18838
[1mStep[0m  [264/339], [94mLoss[0m : 1.70028
[1mStep[0m  [297/339], [94mLoss[0m : 1.47180
[1mStep[0m  [330/339], [94mLoss[0m : 2.30411

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91151
[1mStep[0m  [33/339], [94mLoss[0m : 1.74838
[1mStep[0m  [66/339], [94mLoss[0m : 2.07429
[1mStep[0m  [99/339], [94mLoss[0m : 1.60762
[1mStep[0m  [132/339], [94mLoss[0m : 1.54522
[1mStep[0m  [165/339], [94mLoss[0m : 1.67555
[1mStep[0m  [198/339], [94mLoss[0m : 1.82713
[1mStep[0m  [231/339], [94mLoss[0m : 2.01123
[1mStep[0m  [264/339], [94mLoss[0m : 1.69458
[1mStep[0m  [297/339], [94mLoss[0m : 1.73054
[1mStep[0m  [330/339], [94mLoss[0m : 1.71388

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46001
[1mStep[0m  [33/339], [94mLoss[0m : 1.52754
[1mStep[0m  [66/339], [94mLoss[0m : 1.69649
[1mStep[0m  [99/339], [94mLoss[0m : 1.35232
[1mStep[0m  [132/339], [94mLoss[0m : 1.60794
[1mStep[0m  [165/339], [94mLoss[0m : 2.34842
[1mStep[0m  [198/339], [94mLoss[0m : 1.73493
[1mStep[0m  [231/339], [94mLoss[0m : 1.61851
[1mStep[0m  [264/339], [94mLoss[0m : 1.75091
[1mStep[0m  [297/339], [94mLoss[0m : 1.67627
[1mStep[0m  [330/339], [94mLoss[0m : 1.79983

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68051
[1mStep[0m  [33/339], [94mLoss[0m : 2.74658
[1mStep[0m  [66/339], [94mLoss[0m : 2.13148
[1mStep[0m  [99/339], [94mLoss[0m : 1.50377
[1mStep[0m  [132/339], [94mLoss[0m : 1.28544
[1mStep[0m  [165/339], [94mLoss[0m : 1.61719
[1mStep[0m  [198/339], [94mLoss[0m : 1.84885
[1mStep[0m  [231/339], [94mLoss[0m : 2.35027
[1mStep[0m  [264/339], [94mLoss[0m : 2.25139
[1mStep[0m  [297/339], [94mLoss[0m : 1.94165
[1mStep[0m  [330/339], [94mLoss[0m : 2.09451

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.594, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02748
[1mStep[0m  [33/339], [94mLoss[0m : 2.27416
[1mStep[0m  [66/339], [94mLoss[0m : 1.57797
[1mStep[0m  [99/339], [94mLoss[0m : 1.99460
[1mStep[0m  [132/339], [94mLoss[0m : 1.90564
[1mStep[0m  [165/339], [94mLoss[0m : 1.79364
[1mStep[0m  [198/339], [94mLoss[0m : 1.50646
[1mStep[0m  [231/339], [94mLoss[0m : 2.65399
[1mStep[0m  [264/339], [94mLoss[0m : 1.78275
[1mStep[0m  [297/339], [94mLoss[0m : 1.46399
[1mStep[0m  [330/339], [94mLoss[0m : 1.57372

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.562, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.20233
[1mStep[0m  [33/339], [94mLoss[0m : 1.65000
[1mStep[0m  [66/339], [94mLoss[0m : 2.15934
[1mStep[0m  [99/339], [94mLoss[0m : 1.93243
[1mStep[0m  [132/339], [94mLoss[0m : 1.44871
[1mStep[0m  [165/339], [94mLoss[0m : 1.80395
[1mStep[0m  [198/339], [94mLoss[0m : 1.49128
[1mStep[0m  [231/339], [94mLoss[0m : 1.52552
[1mStep[0m  [264/339], [94mLoss[0m : 1.91718
[1mStep[0m  [297/339], [94mLoss[0m : 1.67892
[1mStep[0m  [330/339], [94mLoss[0m : 1.87244

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.689, [92mTest[0m: 2.577, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77842
[1mStep[0m  [33/339], [94mLoss[0m : 1.57352
[1mStep[0m  [66/339], [94mLoss[0m : 2.03487
[1mStep[0m  [99/339], [94mLoss[0m : 2.00237
[1mStep[0m  [132/339], [94mLoss[0m : 1.51976
[1mStep[0m  [165/339], [94mLoss[0m : 1.74704
[1mStep[0m  [198/339], [94mLoss[0m : 1.40829
[1mStep[0m  [231/339], [94mLoss[0m : 1.59651
[1mStep[0m  [264/339], [94mLoss[0m : 1.44228
[1mStep[0m  [297/339], [94mLoss[0m : 1.96840
[1mStep[0m  [330/339], [94mLoss[0m : 1.90643

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.33368
[1mStep[0m  [33/339], [94mLoss[0m : 1.82336
[1mStep[0m  [66/339], [94mLoss[0m : 1.20575
[1mStep[0m  [99/339], [94mLoss[0m : 1.63014
[1mStep[0m  [132/339], [94mLoss[0m : 1.79359
[1mStep[0m  [165/339], [94mLoss[0m : 1.48004
[1mStep[0m  [198/339], [94mLoss[0m : 1.64621
[1mStep[0m  [231/339], [94mLoss[0m : 1.54835
[1mStep[0m  [264/339], [94mLoss[0m : 1.42767
[1mStep[0m  [297/339], [94mLoss[0m : 1.45182
[1mStep[0m  [330/339], [94mLoss[0m : 1.98811

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.583, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41331
[1mStep[0m  [33/339], [94mLoss[0m : 1.99463
[1mStep[0m  [66/339], [94mLoss[0m : 1.24807
[1mStep[0m  [99/339], [94mLoss[0m : 1.59088
[1mStep[0m  [132/339], [94mLoss[0m : 1.71909
[1mStep[0m  [165/339], [94mLoss[0m : 1.65243
[1mStep[0m  [198/339], [94mLoss[0m : 1.63469
[1mStep[0m  [231/339], [94mLoss[0m : 1.55786
[1mStep[0m  [264/339], [94mLoss[0m : 1.27720
[1mStep[0m  [297/339], [94mLoss[0m : 1.54117
[1mStep[0m  [330/339], [94mLoss[0m : 2.10186

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.629, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52218
[1mStep[0m  [33/339], [94mLoss[0m : 1.62384
[1mStep[0m  [66/339], [94mLoss[0m : 1.57931
[1mStep[0m  [99/339], [94mLoss[0m : 1.32050
[1mStep[0m  [132/339], [94mLoss[0m : 1.63581
[1mStep[0m  [165/339], [94mLoss[0m : 1.88111
[1mStep[0m  [198/339], [94mLoss[0m : 1.87995
[1mStep[0m  [231/339], [94mLoss[0m : 1.33314
[1mStep[0m  [264/339], [94mLoss[0m : 1.33228
[1mStep[0m  [297/339], [94mLoss[0m : 1.16696
[1mStep[0m  [330/339], [94mLoss[0m : 1.90976

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.541, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59666
[1mStep[0m  [33/339], [94mLoss[0m : 1.44501
[1mStep[0m  [66/339], [94mLoss[0m : 1.49498
[1mStep[0m  [99/339], [94mLoss[0m : 1.47690
[1mStep[0m  [132/339], [94mLoss[0m : 1.35800
[1mStep[0m  [165/339], [94mLoss[0m : 1.64912
[1mStep[0m  [198/339], [94mLoss[0m : 1.18610
[1mStep[0m  [231/339], [94mLoss[0m : 1.31980
[1mStep[0m  [264/339], [94mLoss[0m : 2.37836
[1mStep[0m  [297/339], [94mLoss[0m : 1.99740
[1mStep[0m  [330/339], [94mLoss[0m : 2.18283

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.621, [92mTest[0m: 2.552, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66064
[1mStep[0m  [33/339], [94mLoss[0m : 1.82752
[1mStep[0m  [66/339], [94mLoss[0m : 1.96766
[1mStep[0m  [99/339], [94mLoss[0m : 1.64792
[1mStep[0m  [132/339], [94mLoss[0m : 1.54581
[1mStep[0m  [165/339], [94mLoss[0m : 1.46662
[1mStep[0m  [198/339], [94mLoss[0m : 1.78399
[1mStep[0m  [231/339], [94mLoss[0m : 1.36347
[1mStep[0m  [264/339], [94mLoss[0m : 1.62091
[1mStep[0m  [297/339], [94mLoss[0m : 1.50887
[1mStep[0m  [330/339], [94mLoss[0m : 1.26405

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.590, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.29010
[1mStep[0m  [33/339], [94mLoss[0m : 1.51531
[1mStep[0m  [66/339], [94mLoss[0m : 2.16420
[1mStep[0m  [99/339], [94mLoss[0m : 1.73185
[1mStep[0m  [132/339], [94mLoss[0m : 1.29764
[1mStep[0m  [165/339], [94mLoss[0m : 2.02457
[1mStep[0m  [198/339], [94mLoss[0m : 1.59390
[1mStep[0m  [231/339], [94mLoss[0m : 2.18088
[1mStep[0m  [264/339], [94mLoss[0m : 1.50460
[1mStep[0m  [297/339], [94mLoss[0m : 1.89995
[1mStep[0m  [330/339], [94mLoss[0m : 1.83932

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25829
[1mStep[0m  [33/339], [94mLoss[0m : 1.26794
[1mStep[0m  [66/339], [94mLoss[0m : 1.58567
[1mStep[0m  [99/339], [94mLoss[0m : 1.44784
[1mStep[0m  [132/339], [94mLoss[0m : 1.69396
[1mStep[0m  [165/339], [94mLoss[0m : 1.47064
[1mStep[0m  [198/339], [94mLoss[0m : 1.97878
[1mStep[0m  [231/339], [94mLoss[0m : 1.52540
[1mStep[0m  [264/339], [94mLoss[0m : 1.87425
[1mStep[0m  [297/339], [94mLoss[0m : 1.00649
[1mStep[0m  [330/339], [94mLoss[0m : 1.98254

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.491, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.494133885982817
MAE score P1      2.360803
MAE score P2      2.494134
loss              1.546793
learning_rate     0.002575
batch_size              32
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 9.75115
[1mStep[0m  [33/339], [94mLoss[0m : 10.57326
[1mStep[0m  [66/339], [94mLoss[0m : 7.42734
[1mStep[0m  [99/339], [94mLoss[0m : 7.53676
[1mStep[0m  [132/339], [94mLoss[0m : 5.04988
[1mStep[0m  [165/339], [94mLoss[0m : 4.93771
[1mStep[0m  [198/339], [94mLoss[0m : 2.82395
[1mStep[0m  [231/339], [94mLoss[0m : 2.75310
[1mStep[0m  [264/339], [94mLoss[0m : 2.71866
[1mStep[0m  [297/339], [94mLoss[0m : 3.00253
[1mStep[0m  [330/339], [94mLoss[0m : 2.59334

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.276, [92mTest[0m: 10.988, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.16718
[1mStep[0m  [33/339], [94mLoss[0m : 2.93886
[1mStep[0m  [66/339], [94mLoss[0m : 3.24440
[1mStep[0m  [99/339], [94mLoss[0m : 2.61146
[1mStep[0m  [132/339], [94mLoss[0m : 2.60806
[1mStep[0m  [165/339], [94mLoss[0m : 2.83993
[1mStep[0m  [198/339], [94mLoss[0m : 2.84088
[1mStep[0m  [231/339], [94mLoss[0m : 2.85123
[1mStep[0m  [264/339], [94mLoss[0m : 2.57267
[1mStep[0m  [297/339], [94mLoss[0m : 2.67244
[1mStep[0m  [330/339], [94mLoss[0m : 2.90723

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.738, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04647
[1mStep[0m  [33/339], [94mLoss[0m : 2.81947
[1mStep[0m  [66/339], [94mLoss[0m : 3.36509
[1mStep[0m  [99/339], [94mLoss[0m : 2.45663
[1mStep[0m  [132/339], [94mLoss[0m : 2.71783
[1mStep[0m  [165/339], [94mLoss[0m : 2.36090
[1mStep[0m  [198/339], [94mLoss[0m : 2.03178
[1mStep[0m  [231/339], [94mLoss[0m : 2.44899
[1mStep[0m  [264/339], [94mLoss[0m : 2.38447
[1mStep[0m  [297/339], [94mLoss[0m : 2.91732
[1mStep[0m  [330/339], [94mLoss[0m : 2.72862

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55452
[1mStep[0m  [33/339], [94mLoss[0m : 2.51381
[1mStep[0m  [66/339], [94mLoss[0m : 2.67876
[1mStep[0m  [99/339], [94mLoss[0m : 2.43966
[1mStep[0m  [132/339], [94mLoss[0m : 2.11063
[1mStep[0m  [165/339], [94mLoss[0m : 2.04073
[1mStep[0m  [198/339], [94mLoss[0m : 2.35442
[1mStep[0m  [231/339], [94mLoss[0m : 2.48598
[1mStep[0m  [264/339], [94mLoss[0m : 2.57223
[1mStep[0m  [297/339], [94mLoss[0m : 2.51485
[1mStep[0m  [330/339], [94mLoss[0m : 2.84909

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83259
[1mStep[0m  [33/339], [94mLoss[0m : 2.66917
[1mStep[0m  [66/339], [94mLoss[0m : 2.65379
[1mStep[0m  [99/339], [94mLoss[0m : 2.11849
[1mStep[0m  [132/339], [94mLoss[0m : 2.70510
[1mStep[0m  [165/339], [94mLoss[0m : 2.51461
[1mStep[0m  [198/339], [94mLoss[0m : 2.80814
[1mStep[0m  [231/339], [94mLoss[0m : 2.08057
[1mStep[0m  [264/339], [94mLoss[0m : 2.87751
[1mStep[0m  [297/339], [94mLoss[0m : 2.04441
[1mStep[0m  [330/339], [94mLoss[0m : 2.01663

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67278
[1mStep[0m  [33/339], [94mLoss[0m : 2.64928
[1mStep[0m  [66/339], [94mLoss[0m : 2.39368
[1mStep[0m  [99/339], [94mLoss[0m : 3.08562
[1mStep[0m  [132/339], [94mLoss[0m : 2.29148
[1mStep[0m  [165/339], [94mLoss[0m : 2.76417
[1mStep[0m  [198/339], [94mLoss[0m : 2.64773
[1mStep[0m  [231/339], [94mLoss[0m : 2.53314
[1mStep[0m  [264/339], [94mLoss[0m : 2.37830
[1mStep[0m  [297/339], [94mLoss[0m : 2.61380
[1mStep[0m  [330/339], [94mLoss[0m : 2.22352

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67911
[1mStep[0m  [33/339], [94mLoss[0m : 2.32175
[1mStep[0m  [66/339], [94mLoss[0m : 2.49309
[1mStep[0m  [99/339], [94mLoss[0m : 2.30373
[1mStep[0m  [132/339], [94mLoss[0m : 3.11403
[1mStep[0m  [165/339], [94mLoss[0m : 1.99798
[1mStep[0m  [198/339], [94mLoss[0m : 2.08259
[1mStep[0m  [231/339], [94mLoss[0m : 2.41129
[1mStep[0m  [264/339], [94mLoss[0m : 2.15785
[1mStep[0m  [297/339], [94mLoss[0m : 2.35103
[1mStep[0m  [330/339], [94mLoss[0m : 2.49549

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75703
[1mStep[0m  [33/339], [94mLoss[0m : 2.94091
[1mStep[0m  [66/339], [94mLoss[0m : 2.51385
[1mStep[0m  [99/339], [94mLoss[0m : 3.12359
[1mStep[0m  [132/339], [94mLoss[0m : 2.65366
[1mStep[0m  [165/339], [94mLoss[0m : 2.38423
[1mStep[0m  [198/339], [94mLoss[0m : 2.26022
[1mStep[0m  [231/339], [94mLoss[0m : 2.26154
[1mStep[0m  [264/339], [94mLoss[0m : 2.34471
[1mStep[0m  [297/339], [94mLoss[0m : 2.40895
[1mStep[0m  [330/339], [94mLoss[0m : 2.37689

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16917
[1mStep[0m  [33/339], [94mLoss[0m : 2.12476
[1mStep[0m  [66/339], [94mLoss[0m : 2.71249
[1mStep[0m  [99/339], [94mLoss[0m : 2.64062
[1mStep[0m  [132/339], [94mLoss[0m : 1.91840
[1mStep[0m  [165/339], [94mLoss[0m : 2.35878
[1mStep[0m  [198/339], [94mLoss[0m : 2.24923
[1mStep[0m  [231/339], [94mLoss[0m : 2.27130
[1mStep[0m  [264/339], [94mLoss[0m : 2.49522
[1mStep[0m  [297/339], [94mLoss[0m : 2.86416
[1mStep[0m  [330/339], [94mLoss[0m : 2.75346

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05831
[1mStep[0m  [33/339], [94mLoss[0m : 1.66638
[1mStep[0m  [66/339], [94mLoss[0m : 2.53923
[1mStep[0m  [99/339], [94mLoss[0m : 2.12499
[1mStep[0m  [132/339], [94mLoss[0m : 2.09262
[1mStep[0m  [165/339], [94mLoss[0m : 3.26096
[1mStep[0m  [198/339], [94mLoss[0m : 2.84669
[1mStep[0m  [231/339], [94mLoss[0m : 1.92200
[1mStep[0m  [264/339], [94mLoss[0m : 1.75753
[1mStep[0m  [297/339], [94mLoss[0m : 2.78211
[1mStep[0m  [330/339], [94mLoss[0m : 2.38582

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.320, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40487
[1mStep[0m  [33/339], [94mLoss[0m : 2.44697
[1mStep[0m  [66/339], [94mLoss[0m : 3.04536
[1mStep[0m  [99/339], [94mLoss[0m : 2.07026
[1mStep[0m  [132/339], [94mLoss[0m : 3.26589
[1mStep[0m  [165/339], [94mLoss[0m : 2.81887
[1mStep[0m  [198/339], [94mLoss[0m : 2.61153
[1mStep[0m  [231/339], [94mLoss[0m : 3.28175
[1mStep[0m  [264/339], [94mLoss[0m : 2.24317
[1mStep[0m  [297/339], [94mLoss[0m : 2.21869
[1mStep[0m  [330/339], [94mLoss[0m : 2.51870

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16453
[1mStep[0m  [33/339], [94mLoss[0m : 2.31494
[1mStep[0m  [66/339], [94mLoss[0m : 2.74040
[1mStep[0m  [99/339], [94mLoss[0m : 2.35808
[1mStep[0m  [132/339], [94mLoss[0m : 2.72201
[1mStep[0m  [165/339], [94mLoss[0m : 2.67061
[1mStep[0m  [198/339], [94mLoss[0m : 2.35183
[1mStep[0m  [231/339], [94mLoss[0m : 2.93789
[1mStep[0m  [264/339], [94mLoss[0m : 2.83249
[1mStep[0m  [297/339], [94mLoss[0m : 1.93273
[1mStep[0m  [330/339], [94mLoss[0m : 2.04939

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14906
[1mStep[0m  [33/339], [94mLoss[0m : 2.14438
[1mStep[0m  [66/339], [94mLoss[0m : 2.88064
[1mStep[0m  [99/339], [94mLoss[0m : 2.49691
[1mStep[0m  [132/339], [94mLoss[0m : 2.94036
[1mStep[0m  [165/339], [94mLoss[0m : 2.32170
[1mStep[0m  [198/339], [94mLoss[0m : 1.77230
[1mStep[0m  [231/339], [94mLoss[0m : 2.72651
[1mStep[0m  [264/339], [94mLoss[0m : 2.12704
[1mStep[0m  [297/339], [94mLoss[0m : 2.50701
[1mStep[0m  [330/339], [94mLoss[0m : 2.39882

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09782
[1mStep[0m  [33/339], [94mLoss[0m : 2.60046
[1mStep[0m  [66/339], [94mLoss[0m : 1.84945
[1mStep[0m  [99/339], [94mLoss[0m : 2.30158
[1mStep[0m  [132/339], [94mLoss[0m : 1.88311
[1mStep[0m  [165/339], [94mLoss[0m : 2.42537
[1mStep[0m  [198/339], [94mLoss[0m : 2.20112
[1mStep[0m  [231/339], [94mLoss[0m : 2.46378
[1mStep[0m  [264/339], [94mLoss[0m : 2.55406
[1mStep[0m  [297/339], [94mLoss[0m : 2.42984
[1mStep[0m  [330/339], [94mLoss[0m : 2.47632

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87545
[1mStep[0m  [33/339], [94mLoss[0m : 2.67214
[1mStep[0m  [66/339], [94mLoss[0m : 2.39389
[1mStep[0m  [99/339], [94mLoss[0m : 2.13860
[1mStep[0m  [132/339], [94mLoss[0m : 3.07822
[1mStep[0m  [165/339], [94mLoss[0m : 2.26506
[1mStep[0m  [198/339], [94mLoss[0m : 2.42960
[1mStep[0m  [231/339], [94mLoss[0m : 2.92462
[1mStep[0m  [264/339], [94mLoss[0m : 2.32534
[1mStep[0m  [297/339], [94mLoss[0m : 2.36160
[1mStep[0m  [330/339], [94mLoss[0m : 2.25862

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45125
[1mStep[0m  [33/339], [94mLoss[0m : 2.46184
[1mStep[0m  [66/339], [94mLoss[0m : 2.10955
[1mStep[0m  [99/339], [94mLoss[0m : 2.67044
[1mStep[0m  [132/339], [94mLoss[0m : 2.81489
[1mStep[0m  [165/339], [94mLoss[0m : 2.17577
[1mStep[0m  [198/339], [94mLoss[0m : 2.53329
[1mStep[0m  [231/339], [94mLoss[0m : 2.61326
[1mStep[0m  [264/339], [94mLoss[0m : 2.60509
[1mStep[0m  [297/339], [94mLoss[0m : 2.81846
[1mStep[0m  [330/339], [94mLoss[0m : 2.77019

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28773
[1mStep[0m  [33/339], [94mLoss[0m : 1.79852
[1mStep[0m  [66/339], [94mLoss[0m : 2.26954
[1mStep[0m  [99/339], [94mLoss[0m : 2.68867
[1mStep[0m  [132/339], [94mLoss[0m : 2.60336
[1mStep[0m  [165/339], [94mLoss[0m : 2.52136
[1mStep[0m  [198/339], [94mLoss[0m : 2.47104
[1mStep[0m  [231/339], [94mLoss[0m : 2.67629
[1mStep[0m  [264/339], [94mLoss[0m : 2.63361
[1mStep[0m  [297/339], [94mLoss[0m : 2.18703
[1mStep[0m  [330/339], [94mLoss[0m : 2.03712

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00840
[1mStep[0m  [33/339], [94mLoss[0m : 2.21716
[1mStep[0m  [66/339], [94mLoss[0m : 2.64939
[1mStep[0m  [99/339], [94mLoss[0m : 2.27302
[1mStep[0m  [132/339], [94mLoss[0m : 2.62742
[1mStep[0m  [165/339], [94mLoss[0m : 2.14573
[1mStep[0m  [198/339], [94mLoss[0m : 1.80328
[1mStep[0m  [231/339], [94mLoss[0m : 1.79527
[1mStep[0m  [264/339], [94mLoss[0m : 2.14621
[1mStep[0m  [297/339], [94mLoss[0m : 2.55542
[1mStep[0m  [330/339], [94mLoss[0m : 2.40987

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88439
[1mStep[0m  [33/339], [94mLoss[0m : 2.33388
[1mStep[0m  [66/339], [94mLoss[0m : 2.99056
[1mStep[0m  [99/339], [94mLoss[0m : 2.39418
[1mStep[0m  [132/339], [94mLoss[0m : 2.29639
[1mStep[0m  [165/339], [94mLoss[0m : 2.50316
[1mStep[0m  [198/339], [94mLoss[0m : 1.79387
[1mStep[0m  [231/339], [94mLoss[0m : 2.56386
[1mStep[0m  [264/339], [94mLoss[0m : 2.66122
[1mStep[0m  [297/339], [94mLoss[0m : 2.14069
[1mStep[0m  [330/339], [94mLoss[0m : 2.43385

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70430
[1mStep[0m  [33/339], [94mLoss[0m : 2.53920
[1mStep[0m  [66/339], [94mLoss[0m : 2.35615
[1mStep[0m  [99/339], [94mLoss[0m : 3.26629
[1mStep[0m  [132/339], [94mLoss[0m : 2.20391
[1mStep[0m  [165/339], [94mLoss[0m : 1.80713
[1mStep[0m  [198/339], [94mLoss[0m : 2.30293
[1mStep[0m  [231/339], [94mLoss[0m : 1.70043
[1mStep[0m  [264/339], [94mLoss[0m : 2.14997
[1mStep[0m  [297/339], [94mLoss[0m : 1.77350
[1mStep[0m  [330/339], [94mLoss[0m : 3.01970

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88110
[1mStep[0m  [33/339], [94mLoss[0m : 2.36495
[1mStep[0m  [66/339], [94mLoss[0m : 2.45886
[1mStep[0m  [99/339], [94mLoss[0m : 2.26927
[1mStep[0m  [132/339], [94mLoss[0m : 2.28446
[1mStep[0m  [165/339], [94mLoss[0m : 2.50988
[1mStep[0m  [198/339], [94mLoss[0m : 2.97053
[1mStep[0m  [231/339], [94mLoss[0m : 2.24123
[1mStep[0m  [264/339], [94mLoss[0m : 2.44805
[1mStep[0m  [297/339], [94mLoss[0m : 2.56575
[1mStep[0m  [330/339], [94mLoss[0m : 2.03672

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20605
[1mStep[0m  [33/339], [94mLoss[0m : 2.38014
[1mStep[0m  [66/339], [94mLoss[0m : 2.22640
[1mStep[0m  [99/339], [94mLoss[0m : 2.32430
[1mStep[0m  [132/339], [94mLoss[0m : 2.46775
[1mStep[0m  [165/339], [94mLoss[0m : 2.25356
[1mStep[0m  [198/339], [94mLoss[0m : 2.84914
[1mStep[0m  [231/339], [94mLoss[0m : 2.56757
[1mStep[0m  [264/339], [94mLoss[0m : 2.67130
[1mStep[0m  [297/339], [94mLoss[0m : 2.90881
[1mStep[0m  [330/339], [94mLoss[0m : 2.58550

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99444
[1mStep[0m  [33/339], [94mLoss[0m : 2.59826
[1mStep[0m  [66/339], [94mLoss[0m : 2.08297
[1mStep[0m  [99/339], [94mLoss[0m : 2.42244
[1mStep[0m  [132/339], [94mLoss[0m : 2.57771
[1mStep[0m  [165/339], [94mLoss[0m : 2.42959
[1mStep[0m  [198/339], [94mLoss[0m : 2.38678
[1mStep[0m  [231/339], [94mLoss[0m : 2.15482
[1mStep[0m  [264/339], [94mLoss[0m : 2.44716
[1mStep[0m  [297/339], [94mLoss[0m : 1.85072
[1mStep[0m  [330/339], [94mLoss[0m : 2.36716

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60598
[1mStep[0m  [33/339], [94mLoss[0m : 1.97412
[1mStep[0m  [66/339], [94mLoss[0m : 3.22026
[1mStep[0m  [99/339], [94mLoss[0m : 2.62806
[1mStep[0m  [132/339], [94mLoss[0m : 2.64611
[1mStep[0m  [165/339], [94mLoss[0m : 2.16438
[1mStep[0m  [198/339], [94mLoss[0m : 2.35279
[1mStep[0m  [231/339], [94mLoss[0m : 3.11081
[1mStep[0m  [264/339], [94mLoss[0m : 1.93885
[1mStep[0m  [297/339], [94mLoss[0m : 2.49143
[1mStep[0m  [330/339], [94mLoss[0m : 1.97761

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23029
[1mStep[0m  [33/339], [94mLoss[0m : 2.62121
[1mStep[0m  [66/339], [94mLoss[0m : 2.27966
[1mStep[0m  [99/339], [94mLoss[0m : 2.62048
[1mStep[0m  [132/339], [94mLoss[0m : 2.49672
[1mStep[0m  [165/339], [94mLoss[0m : 2.11850
[1mStep[0m  [198/339], [94mLoss[0m : 2.48600
[1mStep[0m  [231/339], [94mLoss[0m : 2.75946
[1mStep[0m  [264/339], [94mLoss[0m : 2.05122
[1mStep[0m  [297/339], [94mLoss[0m : 2.78057
[1mStep[0m  [330/339], [94mLoss[0m : 2.25078

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01689
[1mStep[0m  [33/339], [94mLoss[0m : 2.26342
[1mStep[0m  [66/339], [94mLoss[0m : 3.05251
[1mStep[0m  [99/339], [94mLoss[0m : 2.16199
[1mStep[0m  [132/339], [94mLoss[0m : 2.41779
[1mStep[0m  [165/339], [94mLoss[0m : 2.31360
[1mStep[0m  [198/339], [94mLoss[0m : 2.23635
[1mStep[0m  [231/339], [94mLoss[0m : 2.66677
[1mStep[0m  [264/339], [94mLoss[0m : 2.36056
[1mStep[0m  [297/339], [94mLoss[0m : 2.51743
[1mStep[0m  [330/339], [94mLoss[0m : 2.67679

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41558
[1mStep[0m  [33/339], [94mLoss[0m : 2.41727
[1mStep[0m  [66/339], [94mLoss[0m : 2.41857
[1mStep[0m  [99/339], [94mLoss[0m : 2.80329
[1mStep[0m  [132/339], [94mLoss[0m : 2.63090
[1mStep[0m  [165/339], [94mLoss[0m : 2.61432
[1mStep[0m  [198/339], [94mLoss[0m : 2.51564
[1mStep[0m  [231/339], [94mLoss[0m : 2.34060
[1mStep[0m  [264/339], [94mLoss[0m : 2.42263
[1mStep[0m  [297/339], [94mLoss[0m : 2.15685
[1mStep[0m  [330/339], [94mLoss[0m : 2.72207

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.317, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.92308
[1mStep[0m  [33/339], [94mLoss[0m : 1.99551
[1mStep[0m  [66/339], [94mLoss[0m : 2.36354
[1mStep[0m  [99/339], [94mLoss[0m : 2.08198
[1mStep[0m  [132/339], [94mLoss[0m : 2.03955
[1mStep[0m  [165/339], [94mLoss[0m : 2.31352
[1mStep[0m  [198/339], [94mLoss[0m : 1.97434
[1mStep[0m  [231/339], [94mLoss[0m : 2.56550
[1mStep[0m  [264/339], [94mLoss[0m : 1.82251
[1mStep[0m  [297/339], [94mLoss[0m : 2.25381
[1mStep[0m  [330/339], [94mLoss[0m : 2.23005

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36441
[1mStep[0m  [33/339], [94mLoss[0m : 2.20099
[1mStep[0m  [66/339], [94mLoss[0m : 2.27345
[1mStep[0m  [99/339], [94mLoss[0m : 2.18058
[1mStep[0m  [132/339], [94mLoss[0m : 2.34603
[1mStep[0m  [165/339], [94mLoss[0m : 2.47971
[1mStep[0m  [198/339], [94mLoss[0m : 2.14016
[1mStep[0m  [231/339], [94mLoss[0m : 2.75325
[1mStep[0m  [264/339], [94mLoss[0m : 2.89398
[1mStep[0m  [297/339], [94mLoss[0m : 2.16035
[1mStep[0m  [330/339], [94mLoss[0m : 2.79485

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29548
[1mStep[0m  [33/339], [94mLoss[0m : 2.31695
[1mStep[0m  [66/339], [94mLoss[0m : 2.07695
[1mStep[0m  [99/339], [94mLoss[0m : 2.04116
[1mStep[0m  [132/339], [94mLoss[0m : 2.49785
[1mStep[0m  [165/339], [94mLoss[0m : 2.26568
[1mStep[0m  [198/339], [94mLoss[0m : 2.15023
[1mStep[0m  [231/339], [94mLoss[0m : 2.43093
[1mStep[0m  [264/339], [94mLoss[0m : 2.95790
[1mStep[0m  [297/339], [94mLoss[0m : 1.92114
[1mStep[0m  [330/339], [94mLoss[0m : 2.68506

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.355, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.332643585922444
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.16649
[1mStep[0m  [33/339], [94mLoss[0m : 3.16482
[1mStep[0m  [66/339], [94mLoss[0m : 2.60096
[1mStep[0m  [99/339], [94mLoss[0m : 2.22287
[1mStep[0m  [132/339], [94mLoss[0m : 2.68760
[1mStep[0m  [165/339], [94mLoss[0m : 2.31667
[1mStep[0m  [198/339], [94mLoss[0m : 2.62728
[1mStep[0m  [231/339], [94mLoss[0m : 2.31403
[1mStep[0m  [264/339], [94mLoss[0m : 2.09001
[1mStep[0m  [297/339], [94mLoss[0m : 2.94891
[1mStep[0m  [330/339], [94mLoss[0m : 2.76473

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27256
[1mStep[0m  [33/339], [94mLoss[0m : 2.25560
[1mStep[0m  [66/339], [94mLoss[0m : 2.57107
[1mStep[0m  [99/339], [94mLoss[0m : 2.36953
[1mStep[0m  [132/339], [94mLoss[0m : 1.90086
[1mStep[0m  [165/339], [94mLoss[0m : 2.92170
[1mStep[0m  [198/339], [94mLoss[0m : 2.42795
[1mStep[0m  [231/339], [94mLoss[0m : 2.46691
[1mStep[0m  [264/339], [94mLoss[0m : 2.25909
[1mStep[0m  [297/339], [94mLoss[0m : 2.01586
[1mStep[0m  [330/339], [94mLoss[0m : 2.10844

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80389
[1mStep[0m  [33/339], [94mLoss[0m : 2.52909
[1mStep[0m  [66/339], [94mLoss[0m : 2.21734
[1mStep[0m  [99/339], [94mLoss[0m : 2.33226
[1mStep[0m  [132/339], [94mLoss[0m : 2.52028
[1mStep[0m  [165/339], [94mLoss[0m : 2.42298
[1mStep[0m  [198/339], [94mLoss[0m : 1.81826
[1mStep[0m  [231/339], [94mLoss[0m : 1.99817
[1mStep[0m  [264/339], [94mLoss[0m : 3.05053
[1mStep[0m  [297/339], [94mLoss[0m : 1.66385
[1mStep[0m  [330/339], [94mLoss[0m : 2.32209

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30583
[1mStep[0m  [33/339], [94mLoss[0m : 2.37837
[1mStep[0m  [66/339], [94mLoss[0m : 2.18039
[1mStep[0m  [99/339], [94mLoss[0m : 2.11559
[1mStep[0m  [132/339], [94mLoss[0m : 1.77232
[1mStep[0m  [165/339], [94mLoss[0m : 2.04949
[1mStep[0m  [198/339], [94mLoss[0m : 1.85843
[1mStep[0m  [231/339], [94mLoss[0m : 2.33865
[1mStep[0m  [264/339], [94mLoss[0m : 2.51138
[1mStep[0m  [297/339], [94mLoss[0m : 2.34305
[1mStep[0m  [330/339], [94mLoss[0m : 1.92640

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23488
[1mStep[0m  [33/339], [94mLoss[0m : 2.29096
[1mStep[0m  [66/339], [94mLoss[0m : 2.19560
[1mStep[0m  [99/339], [94mLoss[0m : 2.04734
[1mStep[0m  [132/339], [94mLoss[0m : 2.00623
[1mStep[0m  [165/339], [94mLoss[0m : 1.86345
[1mStep[0m  [198/339], [94mLoss[0m : 1.38773
[1mStep[0m  [231/339], [94mLoss[0m : 1.93424
[1mStep[0m  [264/339], [94mLoss[0m : 2.01739
[1mStep[0m  [297/339], [94mLoss[0m : 2.02280
[1mStep[0m  [330/339], [94mLoss[0m : 2.43260

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87202
[1mStep[0m  [33/339], [94mLoss[0m : 2.28512
[1mStep[0m  [66/339], [94mLoss[0m : 1.94541
[1mStep[0m  [99/339], [94mLoss[0m : 1.82520
[1mStep[0m  [132/339], [94mLoss[0m : 2.10842
[1mStep[0m  [165/339], [94mLoss[0m : 2.06001
[1mStep[0m  [198/339], [94mLoss[0m : 2.68524
[1mStep[0m  [231/339], [94mLoss[0m : 2.46581
[1mStep[0m  [264/339], [94mLoss[0m : 1.97815
[1mStep[0m  [297/339], [94mLoss[0m : 2.20046
[1mStep[0m  [330/339], [94mLoss[0m : 1.98450

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09025
[1mStep[0m  [33/339], [94mLoss[0m : 1.92928
[1mStep[0m  [66/339], [94mLoss[0m : 2.05371
[1mStep[0m  [99/339], [94mLoss[0m : 1.78434
[1mStep[0m  [132/339], [94mLoss[0m : 2.06186
[1mStep[0m  [165/339], [94mLoss[0m : 2.15298
[1mStep[0m  [198/339], [94mLoss[0m : 2.59304
[1mStep[0m  [231/339], [94mLoss[0m : 2.45944
[1mStep[0m  [264/339], [94mLoss[0m : 2.08515
[1mStep[0m  [297/339], [94mLoss[0m : 2.20866
[1mStep[0m  [330/339], [94mLoss[0m : 1.81971

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40591
[1mStep[0m  [33/339], [94mLoss[0m : 2.69649
[1mStep[0m  [66/339], [94mLoss[0m : 2.12403
[1mStep[0m  [99/339], [94mLoss[0m : 2.12274
[1mStep[0m  [132/339], [94mLoss[0m : 1.61177
[1mStep[0m  [165/339], [94mLoss[0m : 2.15666
[1mStep[0m  [198/339], [94mLoss[0m : 1.76191
[1mStep[0m  [231/339], [94mLoss[0m : 2.48654
[1mStep[0m  [264/339], [94mLoss[0m : 2.01145
[1mStep[0m  [297/339], [94mLoss[0m : 1.64977
[1mStep[0m  [330/339], [94mLoss[0m : 2.05075

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98851
[1mStep[0m  [33/339], [94mLoss[0m : 1.76355
[1mStep[0m  [66/339], [94mLoss[0m : 1.56264
[1mStep[0m  [99/339], [94mLoss[0m : 2.43884
[1mStep[0m  [132/339], [94mLoss[0m : 2.06786
[1mStep[0m  [165/339], [94mLoss[0m : 1.78184
[1mStep[0m  [198/339], [94mLoss[0m : 1.92632
[1mStep[0m  [231/339], [94mLoss[0m : 1.98883
[1mStep[0m  [264/339], [94mLoss[0m : 1.95273
[1mStep[0m  [297/339], [94mLoss[0m : 1.72544
[1mStep[0m  [330/339], [94mLoss[0m : 2.37567

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10351
[1mStep[0m  [33/339], [94mLoss[0m : 1.65711
[1mStep[0m  [66/339], [94mLoss[0m : 2.06196
[1mStep[0m  [99/339], [94mLoss[0m : 1.93399
[1mStep[0m  [132/339], [94mLoss[0m : 2.11098
[1mStep[0m  [165/339], [94mLoss[0m : 2.01870
[1mStep[0m  [198/339], [94mLoss[0m : 1.93045
[1mStep[0m  [231/339], [94mLoss[0m : 2.54910
[1mStep[0m  [264/339], [94mLoss[0m : 1.74090
[1mStep[0m  [297/339], [94mLoss[0m : 1.76967
[1mStep[0m  [330/339], [94mLoss[0m : 2.22050

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60983
[1mStep[0m  [33/339], [94mLoss[0m : 1.58532
[1mStep[0m  [66/339], [94mLoss[0m : 1.90555
[1mStep[0m  [99/339], [94mLoss[0m : 1.97013
[1mStep[0m  [132/339], [94mLoss[0m : 2.25048
[1mStep[0m  [165/339], [94mLoss[0m : 1.77356
[1mStep[0m  [198/339], [94mLoss[0m : 1.74696
[1mStep[0m  [231/339], [94mLoss[0m : 1.36059
[1mStep[0m  [264/339], [94mLoss[0m : 1.51851
[1mStep[0m  [297/339], [94mLoss[0m : 2.00411
[1mStep[0m  [330/339], [94mLoss[0m : 1.84219

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83116
[1mStep[0m  [33/339], [94mLoss[0m : 2.35422
[1mStep[0m  [66/339], [94mLoss[0m : 2.55808
[1mStep[0m  [99/339], [94mLoss[0m : 1.84372
[1mStep[0m  [132/339], [94mLoss[0m : 1.93502
[1mStep[0m  [165/339], [94mLoss[0m : 1.71464
[1mStep[0m  [198/339], [94mLoss[0m : 2.00148
[1mStep[0m  [231/339], [94mLoss[0m : 2.00042
[1mStep[0m  [264/339], [94mLoss[0m : 1.46031
[1mStep[0m  [297/339], [94mLoss[0m : 1.99405
[1mStep[0m  [330/339], [94mLoss[0m : 2.19878

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91204
[1mStep[0m  [33/339], [94mLoss[0m : 2.19313
[1mStep[0m  [66/339], [94mLoss[0m : 1.16799
[1mStep[0m  [99/339], [94mLoss[0m : 1.54902
[1mStep[0m  [132/339], [94mLoss[0m : 1.94519
[1mStep[0m  [165/339], [94mLoss[0m : 1.44686
[1mStep[0m  [198/339], [94mLoss[0m : 1.72884
[1mStep[0m  [231/339], [94mLoss[0m : 2.11385
[1mStep[0m  [264/339], [94mLoss[0m : 1.46757
[1mStep[0m  [297/339], [94mLoss[0m : 1.57209
[1mStep[0m  [330/339], [94mLoss[0m : 1.91218

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.875, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.45971
[1mStep[0m  [33/339], [94mLoss[0m : 1.64969
[1mStep[0m  [66/339], [94mLoss[0m : 1.92391
[1mStep[0m  [99/339], [94mLoss[0m : 2.29694
[1mStep[0m  [132/339], [94mLoss[0m : 1.75831
[1mStep[0m  [165/339], [94mLoss[0m : 2.14907
[1mStep[0m  [198/339], [94mLoss[0m : 2.11797
[1mStep[0m  [231/339], [94mLoss[0m : 1.99950
[1mStep[0m  [264/339], [94mLoss[0m : 2.03373
[1mStep[0m  [297/339], [94mLoss[0m : 2.34794
[1mStep[0m  [330/339], [94mLoss[0m : 1.89855

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56261
[1mStep[0m  [33/339], [94mLoss[0m : 1.68697
[1mStep[0m  [66/339], [94mLoss[0m : 1.83389
[1mStep[0m  [99/339], [94mLoss[0m : 1.81640
[1mStep[0m  [132/339], [94mLoss[0m : 1.64985
[1mStep[0m  [165/339], [94mLoss[0m : 1.75164
[1mStep[0m  [198/339], [94mLoss[0m : 2.18989
[1mStep[0m  [231/339], [94mLoss[0m : 2.22144
[1mStep[0m  [264/339], [94mLoss[0m : 1.94395
[1mStep[0m  [297/339], [94mLoss[0m : 1.97338
[1mStep[0m  [330/339], [94mLoss[0m : 1.69901

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76468
[1mStep[0m  [33/339], [94mLoss[0m : 1.88083
[1mStep[0m  [66/339], [94mLoss[0m : 1.89823
[1mStep[0m  [99/339], [94mLoss[0m : 1.72981
[1mStep[0m  [132/339], [94mLoss[0m : 1.87492
[1mStep[0m  [165/339], [94mLoss[0m : 1.98503
[1mStep[0m  [198/339], [94mLoss[0m : 1.69652
[1mStep[0m  [231/339], [94mLoss[0m : 1.80659
[1mStep[0m  [264/339], [94mLoss[0m : 1.87388
[1mStep[0m  [297/339], [94mLoss[0m : 1.83407
[1mStep[0m  [330/339], [94mLoss[0m : 2.09004

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.796, [92mTest[0m: 2.531, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.19669
[1mStep[0m  [33/339], [94mLoss[0m : 1.71400
[1mStep[0m  [66/339], [94mLoss[0m : 1.70084
[1mStep[0m  [99/339], [94mLoss[0m : 1.46637
[1mStep[0m  [132/339], [94mLoss[0m : 2.18721
[1mStep[0m  [165/339], [94mLoss[0m : 1.96772
[1mStep[0m  [198/339], [94mLoss[0m : 1.81465
[1mStep[0m  [231/339], [94mLoss[0m : 2.12235
[1mStep[0m  [264/339], [94mLoss[0m : 1.24837
[1mStep[0m  [297/339], [94mLoss[0m : 1.67985
[1mStep[0m  [330/339], [94mLoss[0m : 1.69395

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99637
[1mStep[0m  [33/339], [94mLoss[0m : 1.30317
[1mStep[0m  [66/339], [94mLoss[0m : 2.03940
[1mStep[0m  [99/339], [94mLoss[0m : 1.93793
[1mStep[0m  [132/339], [94mLoss[0m : 1.86392
[1mStep[0m  [165/339], [94mLoss[0m : 2.14630
[1mStep[0m  [198/339], [94mLoss[0m : 1.90603
[1mStep[0m  [231/339], [94mLoss[0m : 1.89717
[1mStep[0m  [264/339], [94mLoss[0m : 1.72964
[1mStep[0m  [297/339], [94mLoss[0m : 1.82405
[1mStep[0m  [330/339], [94mLoss[0m : 1.60969

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.732, [92mTest[0m: 2.535, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.23841
[1mStep[0m  [33/339], [94mLoss[0m : 1.15862
[1mStep[0m  [66/339], [94mLoss[0m : 2.11002
[1mStep[0m  [99/339], [94mLoss[0m : 1.53915
[1mStep[0m  [132/339], [94mLoss[0m : 1.62122
[1mStep[0m  [165/339], [94mLoss[0m : 1.35781
[1mStep[0m  [198/339], [94mLoss[0m : 1.77985
[1mStep[0m  [231/339], [94mLoss[0m : 2.14149
[1mStep[0m  [264/339], [94mLoss[0m : 1.47794
[1mStep[0m  [297/339], [94mLoss[0m : 1.94778
[1mStep[0m  [330/339], [94mLoss[0m : 1.47810

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28596
[1mStep[0m  [33/339], [94mLoss[0m : 1.77042
[1mStep[0m  [66/339], [94mLoss[0m : 1.61318
[1mStep[0m  [99/339], [94mLoss[0m : 2.00338
[1mStep[0m  [132/339], [94mLoss[0m : 1.98590
[1mStep[0m  [165/339], [94mLoss[0m : 1.77769
[1mStep[0m  [198/339], [94mLoss[0m : 1.47090
[1mStep[0m  [231/339], [94mLoss[0m : 1.42554
[1mStep[0m  [264/339], [94mLoss[0m : 1.95450
[1mStep[0m  [297/339], [94mLoss[0m : 1.77503
[1mStep[0m  [330/339], [94mLoss[0m : 1.31150

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.514, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46694
[1mStep[0m  [33/339], [94mLoss[0m : 1.23979
[1mStep[0m  [66/339], [94mLoss[0m : 1.59907
[1mStep[0m  [99/339], [94mLoss[0m : 2.03233
[1mStep[0m  [132/339], [94mLoss[0m : 1.45386
[1mStep[0m  [165/339], [94mLoss[0m : 1.27544
[1mStep[0m  [198/339], [94mLoss[0m : 1.45808
[1mStep[0m  [231/339], [94mLoss[0m : 1.30724
[1mStep[0m  [264/339], [94mLoss[0m : 1.41951
[1mStep[0m  [297/339], [94mLoss[0m : 1.18468
[1mStep[0m  [330/339], [94mLoss[0m : 1.58774

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.32772
[1mStep[0m  [33/339], [94mLoss[0m : 1.22628
[1mStep[0m  [66/339], [94mLoss[0m : 1.51705
[1mStep[0m  [99/339], [94mLoss[0m : 1.47207
[1mStep[0m  [132/339], [94mLoss[0m : 1.97460
[1mStep[0m  [165/339], [94mLoss[0m : 1.47778
[1mStep[0m  [198/339], [94mLoss[0m : 1.36988
[1mStep[0m  [231/339], [94mLoss[0m : 1.42215
[1mStep[0m  [264/339], [94mLoss[0m : 2.11072
[1mStep[0m  [297/339], [94mLoss[0m : 1.79798
[1mStep[0m  [330/339], [94mLoss[0m : 1.71915

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.468, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43146
[1mStep[0m  [33/339], [94mLoss[0m : 1.41410
[1mStep[0m  [66/339], [94mLoss[0m : 1.43882
[1mStep[0m  [99/339], [94mLoss[0m : 1.55170
[1mStep[0m  [132/339], [94mLoss[0m : 1.76514
[1mStep[0m  [165/339], [94mLoss[0m : 1.34919
[1mStep[0m  [198/339], [94mLoss[0m : 1.34596
[1mStep[0m  [231/339], [94mLoss[0m : 1.67444
[1mStep[0m  [264/339], [94mLoss[0m : 1.30412
[1mStep[0m  [297/339], [94mLoss[0m : 1.32166
[1mStep[0m  [330/339], [94mLoss[0m : 1.53241

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.609, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48630
[1mStep[0m  [33/339], [94mLoss[0m : 1.21783
[1mStep[0m  [66/339], [94mLoss[0m : 1.37760
[1mStep[0m  [99/339], [94mLoss[0m : 1.19427
[1mStep[0m  [132/339], [94mLoss[0m : 1.66113
[1mStep[0m  [165/339], [94mLoss[0m : 1.67775
[1mStep[0m  [198/339], [94mLoss[0m : 1.83309
[1mStep[0m  [231/339], [94mLoss[0m : 1.62536
[1mStep[0m  [264/339], [94mLoss[0m : 1.98231
[1mStep[0m  [297/339], [94mLoss[0m : 1.55472
[1mStep[0m  [330/339], [94mLoss[0m : 2.43890

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38874
[1mStep[0m  [33/339], [94mLoss[0m : 1.54734
[1mStep[0m  [66/339], [94mLoss[0m : 1.27776
[1mStep[0m  [99/339], [94mLoss[0m : 1.47143
[1mStep[0m  [132/339], [94mLoss[0m : 1.44936
[1mStep[0m  [165/339], [94mLoss[0m : 1.33786
[1mStep[0m  [198/339], [94mLoss[0m : 1.20058
[1mStep[0m  [231/339], [94mLoss[0m : 1.81192
[1mStep[0m  [264/339], [94mLoss[0m : 1.46476
[1mStep[0m  [297/339], [94mLoss[0m : 1.76277
[1mStep[0m  [330/339], [94mLoss[0m : 1.82650

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.32067
[1mStep[0m  [33/339], [94mLoss[0m : 1.81167
[1mStep[0m  [66/339], [94mLoss[0m : 1.95292
[1mStep[0m  [99/339], [94mLoss[0m : 2.06468
[1mStep[0m  [132/339], [94mLoss[0m : 1.33815
[1mStep[0m  [165/339], [94mLoss[0m : 1.34296
[1mStep[0m  [198/339], [94mLoss[0m : 1.38779
[1mStep[0m  [231/339], [94mLoss[0m : 1.68336
[1mStep[0m  [264/339], [94mLoss[0m : 1.49754
[1mStep[0m  [297/339], [94mLoss[0m : 1.70121
[1mStep[0m  [330/339], [94mLoss[0m : 1.32106

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67194
[1mStep[0m  [33/339], [94mLoss[0m : 1.73567
[1mStep[0m  [66/339], [94mLoss[0m : 1.31828
[1mStep[0m  [99/339], [94mLoss[0m : 1.28092
[1mStep[0m  [132/339], [94mLoss[0m : 1.37213
[1mStep[0m  [165/339], [94mLoss[0m : 1.52684
[1mStep[0m  [198/339], [94mLoss[0m : 1.45086
[1mStep[0m  [231/339], [94mLoss[0m : 1.50206
[1mStep[0m  [264/339], [94mLoss[0m : 1.48754
[1mStep[0m  [297/339], [94mLoss[0m : 1.21831
[1mStep[0m  [330/339], [94mLoss[0m : 2.23470

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.556, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53979
[1mStep[0m  [33/339], [94mLoss[0m : 1.64083
[1mStep[0m  [66/339], [94mLoss[0m : 1.57495
[1mStep[0m  [99/339], [94mLoss[0m : 1.28223
[1mStep[0m  [132/339], [94mLoss[0m : 1.62480
[1mStep[0m  [165/339], [94mLoss[0m : 1.89170
[1mStep[0m  [198/339], [94mLoss[0m : 1.75529
[1mStep[0m  [231/339], [94mLoss[0m : 1.44934
[1mStep[0m  [264/339], [94mLoss[0m : 2.17960
[1mStep[0m  [297/339], [94mLoss[0m : 1.38897
[1mStep[0m  [330/339], [94mLoss[0m : 2.23244

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.524, [92mTest[0m: 2.621, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.15616
[1mStep[0m  [33/339], [94mLoss[0m : 1.43527
[1mStep[0m  [66/339], [94mLoss[0m : 1.78918
[1mStep[0m  [99/339], [94mLoss[0m : 1.02511
[1mStep[0m  [132/339], [94mLoss[0m : 1.58571
[1mStep[0m  [165/339], [94mLoss[0m : 1.25385
[1mStep[0m  [198/339], [94mLoss[0m : 1.59195
[1mStep[0m  [231/339], [94mLoss[0m : 1.40534
[1mStep[0m  [264/339], [94mLoss[0m : 1.27965
[1mStep[0m  [297/339], [94mLoss[0m : 1.12770
[1mStep[0m  [330/339], [94mLoss[0m : 1.40822

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.525, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.16179
[1mStep[0m  [33/339], [94mLoss[0m : 1.37848
[1mStep[0m  [66/339], [94mLoss[0m : 1.45855
[1mStep[0m  [99/339], [94mLoss[0m : 1.45674
[1mStep[0m  [132/339], [94mLoss[0m : 1.14444
[1mStep[0m  [165/339], [94mLoss[0m : 1.30803
[1mStep[0m  [198/339], [94mLoss[0m : 1.50757
[1mStep[0m  [231/339], [94mLoss[0m : 1.27887
[1mStep[0m  [264/339], [94mLoss[0m : 1.85321
[1mStep[0m  [297/339], [94mLoss[0m : 1.37493
[1mStep[0m  [330/339], [94mLoss[0m : 1.28067

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.542
====================================

Phase 2 - Evaluation MAE:  2.542433184860027
MAE score P1      2.332644
MAE score P2      2.542433
loss              1.489082
learning_rate     0.002575
batch_size              32
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.56493
[1mStep[0m  [33/339], [94mLoss[0m : 10.11374
[1mStep[0m  [66/339], [94mLoss[0m : 9.08097
[1mStep[0m  [99/339], [94mLoss[0m : 7.52935
[1mStep[0m  [132/339], [94mLoss[0m : 6.32998
[1mStep[0m  [165/339], [94mLoss[0m : 4.22290
[1mStep[0m  [198/339], [94mLoss[0m : 1.88239
[1mStep[0m  [231/339], [94mLoss[0m : 3.17026
[1mStep[0m  [264/339], [94mLoss[0m : 3.30629
[1mStep[0m  [297/339], [94mLoss[0m : 1.98787
[1mStep[0m  [330/339], [94mLoss[0m : 2.03700

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.281, [92mTest[0m: 11.014, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56676
[1mStep[0m  [33/339], [94mLoss[0m : 2.70941
[1mStep[0m  [66/339], [94mLoss[0m : 2.70121
[1mStep[0m  [99/339], [94mLoss[0m : 2.19334
[1mStep[0m  [132/339], [94mLoss[0m : 2.41845
[1mStep[0m  [165/339], [94mLoss[0m : 2.84610
[1mStep[0m  [198/339], [94mLoss[0m : 2.47104
[1mStep[0m  [231/339], [94mLoss[0m : 2.44313
[1mStep[0m  [264/339], [94mLoss[0m : 2.61474
[1mStep[0m  [297/339], [94mLoss[0m : 2.13663
[1mStep[0m  [330/339], [94mLoss[0m : 1.85522

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75580
[1mStep[0m  [33/339], [94mLoss[0m : 2.11905
[1mStep[0m  [66/339], [94mLoss[0m : 3.02206
[1mStep[0m  [99/339], [94mLoss[0m : 2.36451
[1mStep[0m  [132/339], [94mLoss[0m : 2.59651
[1mStep[0m  [165/339], [94mLoss[0m : 2.67064
[1mStep[0m  [198/339], [94mLoss[0m : 2.84259
[1mStep[0m  [231/339], [94mLoss[0m : 2.80383
[1mStep[0m  [264/339], [94mLoss[0m : 2.35531
[1mStep[0m  [297/339], [94mLoss[0m : 2.82159
[1mStep[0m  [330/339], [94mLoss[0m : 2.92158

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64730
[1mStep[0m  [33/339], [94mLoss[0m : 2.48554
[1mStep[0m  [66/339], [94mLoss[0m : 2.81551
[1mStep[0m  [99/339], [94mLoss[0m : 2.93913
[1mStep[0m  [132/339], [94mLoss[0m : 2.73725
[1mStep[0m  [165/339], [94mLoss[0m : 2.46959
[1mStep[0m  [198/339], [94mLoss[0m : 2.63155
[1mStep[0m  [231/339], [94mLoss[0m : 2.48325
[1mStep[0m  [264/339], [94mLoss[0m : 2.80331
[1mStep[0m  [297/339], [94mLoss[0m : 3.06611
[1mStep[0m  [330/339], [94mLoss[0m : 2.47675

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41665
[1mStep[0m  [33/339], [94mLoss[0m : 2.51554
[1mStep[0m  [66/339], [94mLoss[0m : 2.80404
[1mStep[0m  [99/339], [94mLoss[0m : 2.20582
[1mStep[0m  [132/339], [94mLoss[0m : 2.83243
[1mStep[0m  [165/339], [94mLoss[0m : 2.91965
[1mStep[0m  [198/339], [94mLoss[0m : 2.50807
[1mStep[0m  [231/339], [94mLoss[0m : 2.15487
[1mStep[0m  [264/339], [94mLoss[0m : 2.65680
[1mStep[0m  [297/339], [94mLoss[0m : 2.71087
[1mStep[0m  [330/339], [94mLoss[0m : 2.95745

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94626
[1mStep[0m  [33/339], [94mLoss[0m : 2.30119
[1mStep[0m  [66/339], [94mLoss[0m : 2.10556
[1mStep[0m  [99/339], [94mLoss[0m : 3.58949
[1mStep[0m  [132/339], [94mLoss[0m : 2.35290
[1mStep[0m  [165/339], [94mLoss[0m : 2.61166
[1mStep[0m  [198/339], [94mLoss[0m : 2.39582
[1mStep[0m  [231/339], [94mLoss[0m : 2.63478
[1mStep[0m  [264/339], [94mLoss[0m : 2.55171
[1mStep[0m  [297/339], [94mLoss[0m : 2.45729
[1mStep[0m  [330/339], [94mLoss[0m : 2.78570

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34947
[1mStep[0m  [33/339], [94mLoss[0m : 1.90839
[1mStep[0m  [66/339], [94mLoss[0m : 2.45978
[1mStep[0m  [99/339], [94mLoss[0m : 2.14312
[1mStep[0m  [132/339], [94mLoss[0m : 2.49842
[1mStep[0m  [165/339], [94mLoss[0m : 2.31496
[1mStep[0m  [198/339], [94mLoss[0m : 1.79043
[1mStep[0m  [231/339], [94mLoss[0m : 2.34788
[1mStep[0m  [264/339], [94mLoss[0m : 3.28969
[1mStep[0m  [297/339], [94mLoss[0m : 3.18976
[1mStep[0m  [330/339], [94mLoss[0m : 2.23305

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00108
[1mStep[0m  [33/339], [94mLoss[0m : 2.95363
[1mStep[0m  [66/339], [94mLoss[0m : 3.00330
[1mStep[0m  [99/339], [94mLoss[0m : 2.49185
[1mStep[0m  [132/339], [94mLoss[0m : 2.17958
[1mStep[0m  [165/339], [94mLoss[0m : 3.13511
[1mStep[0m  [198/339], [94mLoss[0m : 2.26907
[1mStep[0m  [231/339], [94mLoss[0m : 1.86326
[1mStep[0m  [264/339], [94mLoss[0m : 2.86317
[1mStep[0m  [297/339], [94mLoss[0m : 2.33026
[1mStep[0m  [330/339], [94mLoss[0m : 2.58401

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53282
[1mStep[0m  [33/339], [94mLoss[0m : 2.95683
[1mStep[0m  [66/339], [94mLoss[0m : 2.35155
[1mStep[0m  [99/339], [94mLoss[0m : 2.15843
[1mStep[0m  [132/339], [94mLoss[0m : 2.89219
[1mStep[0m  [165/339], [94mLoss[0m : 2.83663
[1mStep[0m  [198/339], [94mLoss[0m : 2.40855
[1mStep[0m  [231/339], [94mLoss[0m : 2.39824
[1mStep[0m  [264/339], [94mLoss[0m : 2.88849
[1mStep[0m  [297/339], [94mLoss[0m : 2.28250
[1mStep[0m  [330/339], [94mLoss[0m : 2.42509

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48628
[1mStep[0m  [33/339], [94mLoss[0m : 1.83059
[1mStep[0m  [66/339], [94mLoss[0m : 2.38348
[1mStep[0m  [99/339], [94mLoss[0m : 2.00081
[1mStep[0m  [132/339], [94mLoss[0m : 2.21299
[1mStep[0m  [165/339], [94mLoss[0m : 2.28083
[1mStep[0m  [198/339], [94mLoss[0m : 2.04067
[1mStep[0m  [231/339], [94mLoss[0m : 2.47412
[1mStep[0m  [264/339], [94mLoss[0m : 2.02715
[1mStep[0m  [297/339], [94mLoss[0m : 2.44700
[1mStep[0m  [330/339], [94mLoss[0m : 2.93015

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83243
[1mStep[0m  [33/339], [94mLoss[0m : 2.56382
[1mStep[0m  [66/339], [94mLoss[0m : 2.54823
[1mStep[0m  [99/339], [94mLoss[0m : 2.17689
[1mStep[0m  [132/339], [94mLoss[0m : 2.35924
[1mStep[0m  [165/339], [94mLoss[0m : 2.05677
[1mStep[0m  [198/339], [94mLoss[0m : 2.40199
[1mStep[0m  [231/339], [94mLoss[0m : 1.99654
[1mStep[0m  [264/339], [94mLoss[0m : 1.95625
[1mStep[0m  [297/339], [94mLoss[0m : 2.60902
[1mStep[0m  [330/339], [94mLoss[0m : 2.03719

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92486
[1mStep[0m  [33/339], [94mLoss[0m : 2.25924
[1mStep[0m  [66/339], [94mLoss[0m : 2.32439
[1mStep[0m  [99/339], [94mLoss[0m : 3.03433
[1mStep[0m  [132/339], [94mLoss[0m : 1.91840
[1mStep[0m  [165/339], [94mLoss[0m : 3.49916
[1mStep[0m  [198/339], [94mLoss[0m : 2.46189
[1mStep[0m  [231/339], [94mLoss[0m : 2.70949
[1mStep[0m  [264/339], [94mLoss[0m : 2.26109
[1mStep[0m  [297/339], [94mLoss[0m : 2.31158
[1mStep[0m  [330/339], [94mLoss[0m : 2.21713

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24581
[1mStep[0m  [33/339], [94mLoss[0m : 2.39649
[1mStep[0m  [66/339], [94mLoss[0m : 2.76109
[1mStep[0m  [99/339], [94mLoss[0m : 1.96267
[1mStep[0m  [132/339], [94mLoss[0m : 2.24442
[1mStep[0m  [165/339], [94mLoss[0m : 2.63065
[1mStep[0m  [198/339], [94mLoss[0m : 2.60943
[1mStep[0m  [231/339], [94mLoss[0m : 2.35392
[1mStep[0m  [264/339], [94mLoss[0m : 2.88297
[1mStep[0m  [297/339], [94mLoss[0m : 2.59214
[1mStep[0m  [330/339], [94mLoss[0m : 2.64739

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01097
[1mStep[0m  [33/339], [94mLoss[0m : 2.17031
[1mStep[0m  [66/339], [94mLoss[0m : 2.19553
[1mStep[0m  [99/339], [94mLoss[0m : 2.66329
[1mStep[0m  [132/339], [94mLoss[0m : 2.77690
[1mStep[0m  [165/339], [94mLoss[0m : 2.36321
[1mStep[0m  [198/339], [94mLoss[0m : 2.60026
[1mStep[0m  [231/339], [94mLoss[0m : 2.24896
[1mStep[0m  [264/339], [94mLoss[0m : 2.13922
[1mStep[0m  [297/339], [94mLoss[0m : 2.32269
[1mStep[0m  [330/339], [94mLoss[0m : 2.75098

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61813
[1mStep[0m  [33/339], [94mLoss[0m : 2.24840
[1mStep[0m  [66/339], [94mLoss[0m : 2.93687
[1mStep[0m  [99/339], [94mLoss[0m : 2.35221
[1mStep[0m  [132/339], [94mLoss[0m : 3.07017
[1mStep[0m  [165/339], [94mLoss[0m : 2.61589
[1mStep[0m  [198/339], [94mLoss[0m : 2.60938
[1mStep[0m  [231/339], [94mLoss[0m : 2.38588
[1mStep[0m  [264/339], [94mLoss[0m : 2.23970
[1mStep[0m  [297/339], [94mLoss[0m : 2.39731
[1mStep[0m  [330/339], [94mLoss[0m : 2.38802

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44770
[1mStep[0m  [33/339], [94mLoss[0m : 1.92907
[1mStep[0m  [66/339], [94mLoss[0m : 2.51444
[1mStep[0m  [99/339], [94mLoss[0m : 2.21821
[1mStep[0m  [132/339], [94mLoss[0m : 2.03329
[1mStep[0m  [165/339], [94mLoss[0m : 2.54282
[1mStep[0m  [198/339], [94mLoss[0m : 1.61525
[1mStep[0m  [231/339], [94mLoss[0m : 2.35829
[1mStep[0m  [264/339], [94mLoss[0m : 2.30089
[1mStep[0m  [297/339], [94mLoss[0m : 1.92935
[1mStep[0m  [330/339], [94mLoss[0m : 2.05410

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13925
[1mStep[0m  [33/339], [94mLoss[0m : 3.05185
[1mStep[0m  [66/339], [94mLoss[0m : 2.09802
[1mStep[0m  [99/339], [94mLoss[0m : 2.41187
[1mStep[0m  [132/339], [94mLoss[0m : 2.56845
[1mStep[0m  [165/339], [94mLoss[0m : 3.08826
[1mStep[0m  [198/339], [94mLoss[0m : 2.21863
[1mStep[0m  [231/339], [94mLoss[0m : 2.85048
[1mStep[0m  [264/339], [94mLoss[0m : 1.99545
[1mStep[0m  [297/339], [94mLoss[0m : 2.19781
[1mStep[0m  [330/339], [94mLoss[0m : 2.11568

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32145
[1mStep[0m  [33/339], [94mLoss[0m : 2.75679
[1mStep[0m  [66/339], [94mLoss[0m : 2.44117
[1mStep[0m  [99/339], [94mLoss[0m : 2.67074
[1mStep[0m  [132/339], [94mLoss[0m : 1.99142
[1mStep[0m  [165/339], [94mLoss[0m : 3.16755
[1mStep[0m  [198/339], [94mLoss[0m : 2.20100
[1mStep[0m  [231/339], [94mLoss[0m : 2.46258
[1mStep[0m  [264/339], [94mLoss[0m : 2.12570
[1mStep[0m  [297/339], [94mLoss[0m : 2.24701
[1mStep[0m  [330/339], [94mLoss[0m : 2.88636

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.311, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53203
[1mStep[0m  [33/339], [94mLoss[0m : 2.55685
[1mStep[0m  [66/339], [94mLoss[0m : 2.01612
[1mStep[0m  [99/339], [94mLoss[0m : 2.34309
[1mStep[0m  [132/339], [94mLoss[0m : 3.01829
[1mStep[0m  [165/339], [94mLoss[0m : 2.49692
[1mStep[0m  [198/339], [94mLoss[0m : 2.13888
[1mStep[0m  [231/339], [94mLoss[0m : 2.60383
[1mStep[0m  [264/339], [94mLoss[0m : 2.81292
[1mStep[0m  [297/339], [94mLoss[0m : 2.15361
[1mStep[0m  [330/339], [94mLoss[0m : 2.73372

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82348
[1mStep[0m  [33/339], [94mLoss[0m : 2.55185
[1mStep[0m  [66/339], [94mLoss[0m : 2.37141
[1mStep[0m  [99/339], [94mLoss[0m : 2.12009
[1mStep[0m  [132/339], [94mLoss[0m : 2.75055
[1mStep[0m  [165/339], [94mLoss[0m : 1.94548
[1mStep[0m  [198/339], [94mLoss[0m : 3.47235
[1mStep[0m  [231/339], [94mLoss[0m : 2.03125
[1mStep[0m  [264/339], [94mLoss[0m : 2.57068
[1mStep[0m  [297/339], [94mLoss[0m : 2.36776
[1mStep[0m  [330/339], [94mLoss[0m : 2.76930

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43135
[1mStep[0m  [33/339], [94mLoss[0m : 2.89550
[1mStep[0m  [66/339], [94mLoss[0m : 2.30244
[1mStep[0m  [99/339], [94mLoss[0m : 2.27323
[1mStep[0m  [132/339], [94mLoss[0m : 2.64167
[1mStep[0m  [165/339], [94mLoss[0m : 2.46097
[1mStep[0m  [198/339], [94mLoss[0m : 2.69951
[1mStep[0m  [231/339], [94mLoss[0m : 2.25566
[1mStep[0m  [264/339], [94mLoss[0m : 1.96850
[1mStep[0m  [297/339], [94mLoss[0m : 2.67690
[1mStep[0m  [330/339], [94mLoss[0m : 2.49262

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82310
[1mStep[0m  [33/339], [94mLoss[0m : 2.71849
[1mStep[0m  [66/339], [94mLoss[0m : 2.15848
[1mStep[0m  [99/339], [94mLoss[0m : 2.34221
[1mStep[0m  [132/339], [94mLoss[0m : 2.07161
[1mStep[0m  [165/339], [94mLoss[0m : 2.27301
[1mStep[0m  [198/339], [94mLoss[0m : 2.09056
[1mStep[0m  [231/339], [94mLoss[0m : 2.05015
[1mStep[0m  [264/339], [94mLoss[0m : 1.63957
[1mStep[0m  [297/339], [94mLoss[0m : 2.42036
[1mStep[0m  [330/339], [94mLoss[0m : 2.53332

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.317, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23601
[1mStep[0m  [33/339], [94mLoss[0m : 2.43990
[1mStep[0m  [66/339], [94mLoss[0m : 1.86834
[1mStep[0m  [99/339], [94mLoss[0m : 2.60707
[1mStep[0m  [132/339], [94mLoss[0m : 2.40679
[1mStep[0m  [165/339], [94mLoss[0m : 1.99293
[1mStep[0m  [198/339], [94mLoss[0m : 2.95052
[1mStep[0m  [231/339], [94mLoss[0m : 2.33420
[1mStep[0m  [264/339], [94mLoss[0m : 2.91220
[1mStep[0m  [297/339], [94mLoss[0m : 1.92412
[1mStep[0m  [330/339], [94mLoss[0m : 2.73191

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14298
[1mStep[0m  [33/339], [94mLoss[0m : 2.76924
[1mStep[0m  [66/339], [94mLoss[0m : 2.11140
[1mStep[0m  [99/339], [94mLoss[0m : 2.60169
[1mStep[0m  [132/339], [94mLoss[0m : 2.36658
[1mStep[0m  [165/339], [94mLoss[0m : 2.15352
[1mStep[0m  [198/339], [94mLoss[0m : 1.79769
[1mStep[0m  [231/339], [94mLoss[0m : 2.48273
[1mStep[0m  [264/339], [94mLoss[0m : 2.59815
[1mStep[0m  [297/339], [94mLoss[0m : 2.19393
[1mStep[0m  [330/339], [94mLoss[0m : 2.61268

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31624
[1mStep[0m  [33/339], [94mLoss[0m : 1.78033
[1mStep[0m  [66/339], [94mLoss[0m : 2.63644
[1mStep[0m  [99/339], [94mLoss[0m : 2.81460
[1mStep[0m  [132/339], [94mLoss[0m : 2.18752
[1mStep[0m  [165/339], [94mLoss[0m : 2.86629
[1mStep[0m  [198/339], [94mLoss[0m : 3.39926
[1mStep[0m  [231/339], [94mLoss[0m : 2.92491
[1mStep[0m  [264/339], [94mLoss[0m : 2.39604
[1mStep[0m  [297/339], [94mLoss[0m : 2.19874
[1mStep[0m  [330/339], [94mLoss[0m : 2.35718

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93829
[1mStep[0m  [33/339], [94mLoss[0m : 2.18921
[1mStep[0m  [66/339], [94mLoss[0m : 2.44767
[1mStep[0m  [99/339], [94mLoss[0m : 2.49966
[1mStep[0m  [132/339], [94mLoss[0m : 2.64939
[1mStep[0m  [165/339], [94mLoss[0m : 2.39985
[1mStep[0m  [198/339], [94mLoss[0m : 2.80894
[1mStep[0m  [231/339], [94mLoss[0m : 2.43593
[1mStep[0m  [264/339], [94mLoss[0m : 1.95681
[1mStep[0m  [297/339], [94mLoss[0m : 2.44722
[1mStep[0m  [330/339], [94mLoss[0m : 2.52058

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39884
[1mStep[0m  [33/339], [94mLoss[0m : 2.86624
[1mStep[0m  [66/339], [94mLoss[0m : 2.77590
[1mStep[0m  [99/339], [94mLoss[0m : 3.38372
[1mStep[0m  [132/339], [94mLoss[0m : 2.20131
[1mStep[0m  [165/339], [94mLoss[0m : 2.52863
[1mStep[0m  [198/339], [94mLoss[0m : 1.83574
[1mStep[0m  [231/339], [94mLoss[0m : 2.10354
[1mStep[0m  [264/339], [94mLoss[0m : 2.40451
[1mStep[0m  [297/339], [94mLoss[0m : 1.99298
[1mStep[0m  [330/339], [94mLoss[0m : 2.70322

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31468
[1mStep[0m  [33/339], [94mLoss[0m : 3.17402
[1mStep[0m  [66/339], [94mLoss[0m : 2.48290
[1mStep[0m  [99/339], [94mLoss[0m : 2.84189
[1mStep[0m  [132/339], [94mLoss[0m : 1.97796
[1mStep[0m  [165/339], [94mLoss[0m : 2.88900
[1mStep[0m  [198/339], [94mLoss[0m : 1.75131
[1mStep[0m  [231/339], [94mLoss[0m : 2.62894
[1mStep[0m  [264/339], [94mLoss[0m : 2.15997
[1mStep[0m  [297/339], [94mLoss[0m : 2.42847
[1mStep[0m  [330/339], [94mLoss[0m : 2.10944

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.378, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01866
[1mStep[0m  [33/339], [94mLoss[0m : 3.38940
[1mStep[0m  [66/339], [94mLoss[0m : 2.06548
[1mStep[0m  [99/339], [94mLoss[0m : 1.93781
[1mStep[0m  [132/339], [94mLoss[0m : 2.68747
[1mStep[0m  [165/339], [94mLoss[0m : 2.57575
[1mStep[0m  [198/339], [94mLoss[0m : 2.09364
[1mStep[0m  [231/339], [94mLoss[0m : 2.53741
[1mStep[0m  [264/339], [94mLoss[0m : 3.10288
[1mStep[0m  [297/339], [94mLoss[0m : 2.74582
[1mStep[0m  [330/339], [94mLoss[0m : 2.80370

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97500
[1mStep[0m  [33/339], [94mLoss[0m : 2.91736
[1mStep[0m  [66/339], [94mLoss[0m : 1.92738
[1mStep[0m  [99/339], [94mLoss[0m : 1.96492
[1mStep[0m  [132/339], [94mLoss[0m : 2.45778
[1mStep[0m  [165/339], [94mLoss[0m : 2.87392
[1mStep[0m  [198/339], [94mLoss[0m : 2.55249
[1mStep[0m  [231/339], [94mLoss[0m : 2.47280
[1mStep[0m  [264/339], [94mLoss[0m : 2.59261
[1mStep[0m  [297/339], [94mLoss[0m : 2.44988
[1mStep[0m  [330/339], [94mLoss[0m : 2.51036

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.322
====================================

Phase 1 - Evaluation MAE:  2.322434714410157
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.44671
[1mStep[0m  [33/339], [94mLoss[0m : 2.35026
[1mStep[0m  [66/339], [94mLoss[0m : 1.88329
[1mStep[0m  [99/339], [94mLoss[0m : 2.12905
[1mStep[0m  [132/339], [94mLoss[0m : 1.95246
[1mStep[0m  [165/339], [94mLoss[0m : 2.67496
[1mStep[0m  [198/339], [94mLoss[0m : 2.22583
[1mStep[0m  [231/339], [94mLoss[0m : 2.54125
[1mStep[0m  [264/339], [94mLoss[0m : 2.80549
[1mStep[0m  [297/339], [94mLoss[0m : 2.98338
[1mStep[0m  [330/339], [94mLoss[0m : 2.14630

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13669
[1mStep[0m  [33/339], [94mLoss[0m : 2.45970
[1mStep[0m  [66/339], [94mLoss[0m : 2.18415
[1mStep[0m  [99/339], [94mLoss[0m : 2.21921
[1mStep[0m  [132/339], [94mLoss[0m : 2.81613
[1mStep[0m  [165/339], [94mLoss[0m : 2.76840
[1mStep[0m  [198/339], [94mLoss[0m : 2.49201
[1mStep[0m  [231/339], [94mLoss[0m : 2.37706
[1mStep[0m  [264/339], [94mLoss[0m : 2.30088
[1mStep[0m  [297/339], [94mLoss[0m : 2.33475
[1mStep[0m  [330/339], [94mLoss[0m : 2.06313

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78243
[1mStep[0m  [33/339], [94mLoss[0m : 1.85817
[1mStep[0m  [66/339], [94mLoss[0m : 2.13184
[1mStep[0m  [99/339], [94mLoss[0m : 2.75100
[1mStep[0m  [132/339], [94mLoss[0m : 2.81757
[1mStep[0m  [165/339], [94mLoss[0m : 2.41276
[1mStep[0m  [198/339], [94mLoss[0m : 2.40396
[1mStep[0m  [231/339], [94mLoss[0m : 1.84297
[1mStep[0m  [264/339], [94mLoss[0m : 1.83042
[1mStep[0m  [297/339], [94mLoss[0m : 2.53382
[1mStep[0m  [330/339], [94mLoss[0m : 2.76133

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20199
[1mStep[0m  [33/339], [94mLoss[0m : 2.58160
[1mStep[0m  [66/339], [94mLoss[0m : 1.77653
[1mStep[0m  [99/339], [94mLoss[0m : 2.55472
[1mStep[0m  [132/339], [94mLoss[0m : 2.39841
[1mStep[0m  [165/339], [94mLoss[0m : 2.13762
[1mStep[0m  [198/339], [94mLoss[0m : 3.07592
[1mStep[0m  [231/339], [94mLoss[0m : 2.06246
[1mStep[0m  [264/339], [94mLoss[0m : 2.03438
[1mStep[0m  [297/339], [94mLoss[0m : 2.34774
[1mStep[0m  [330/339], [94mLoss[0m : 2.44664

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18606
[1mStep[0m  [33/339], [94mLoss[0m : 2.91393
[1mStep[0m  [66/339], [94mLoss[0m : 3.10432
[1mStep[0m  [99/339], [94mLoss[0m : 2.18560
[1mStep[0m  [132/339], [94mLoss[0m : 2.13718
[1mStep[0m  [165/339], [94mLoss[0m : 2.00681
[1mStep[0m  [198/339], [94mLoss[0m : 2.00341
[1mStep[0m  [231/339], [94mLoss[0m : 2.46721
[1mStep[0m  [264/339], [94mLoss[0m : 2.11523
[1mStep[0m  [297/339], [94mLoss[0m : 2.07736
[1mStep[0m  [330/339], [94mLoss[0m : 2.62564

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18484
[1mStep[0m  [33/339], [94mLoss[0m : 1.99066
[1mStep[0m  [66/339], [94mLoss[0m : 2.42984
[1mStep[0m  [99/339], [94mLoss[0m : 1.92494
[1mStep[0m  [132/339], [94mLoss[0m : 1.70725
[1mStep[0m  [165/339], [94mLoss[0m : 2.12082
[1mStep[0m  [198/339], [94mLoss[0m : 2.58139
[1mStep[0m  [231/339], [94mLoss[0m : 2.33691
[1mStep[0m  [264/339], [94mLoss[0m : 2.22278
[1mStep[0m  [297/339], [94mLoss[0m : 2.14614
[1mStep[0m  [330/339], [94mLoss[0m : 2.36101

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97650
[1mStep[0m  [33/339], [94mLoss[0m : 1.88668
[1mStep[0m  [66/339], [94mLoss[0m : 2.31481
[1mStep[0m  [99/339], [94mLoss[0m : 2.54604
[1mStep[0m  [132/339], [94mLoss[0m : 2.39745
[1mStep[0m  [165/339], [94mLoss[0m : 2.05101
[1mStep[0m  [198/339], [94mLoss[0m : 1.97469
[1mStep[0m  [231/339], [94mLoss[0m : 2.60941
[1mStep[0m  [264/339], [94mLoss[0m : 2.01738
[1mStep[0m  [297/339], [94mLoss[0m : 2.17241
[1mStep[0m  [330/339], [94mLoss[0m : 2.38352

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.081, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12793
[1mStep[0m  [33/339], [94mLoss[0m : 2.75581
[1mStep[0m  [66/339], [94mLoss[0m : 2.12338
[1mStep[0m  [99/339], [94mLoss[0m : 2.47637
[1mStep[0m  [132/339], [94mLoss[0m : 1.67557
[1mStep[0m  [165/339], [94mLoss[0m : 1.73660
[1mStep[0m  [198/339], [94mLoss[0m : 1.88601
[1mStep[0m  [231/339], [94mLoss[0m : 2.18886
[1mStep[0m  [264/339], [94mLoss[0m : 2.26173
[1mStep[0m  [297/339], [94mLoss[0m : 1.59175
[1mStep[0m  [330/339], [94mLoss[0m : 1.68245

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63431
[1mStep[0m  [33/339], [94mLoss[0m : 1.69388
[1mStep[0m  [66/339], [94mLoss[0m : 2.15190
[1mStep[0m  [99/339], [94mLoss[0m : 1.75323
[1mStep[0m  [132/339], [94mLoss[0m : 2.17647
[1mStep[0m  [165/339], [94mLoss[0m : 1.81268
[1mStep[0m  [198/339], [94mLoss[0m : 1.58539
[1mStep[0m  [231/339], [94mLoss[0m : 1.83893
[1mStep[0m  [264/339], [94mLoss[0m : 2.33613
[1mStep[0m  [297/339], [94mLoss[0m : 1.92821
[1mStep[0m  [330/339], [94mLoss[0m : 2.01217

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80229
[1mStep[0m  [33/339], [94mLoss[0m : 1.85997
[1mStep[0m  [66/339], [94mLoss[0m : 1.41633
[1mStep[0m  [99/339], [94mLoss[0m : 2.32832
[1mStep[0m  [132/339], [94mLoss[0m : 2.06315
[1mStep[0m  [165/339], [94mLoss[0m : 1.59872
[1mStep[0m  [198/339], [94mLoss[0m : 1.99338
[1mStep[0m  [231/339], [94mLoss[0m : 1.70844
[1mStep[0m  [264/339], [94mLoss[0m : 2.04110
[1mStep[0m  [297/339], [94mLoss[0m : 2.14723
[1mStep[0m  [330/339], [94mLoss[0m : 2.17181

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61079
[1mStep[0m  [33/339], [94mLoss[0m : 2.24773
[1mStep[0m  [66/339], [94mLoss[0m : 1.68758
[1mStep[0m  [99/339], [94mLoss[0m : 1.74481
[1mStep[0m  [132/339], [94mLoss[0m : 1.83586
[1mStep[0m  [165/339], [94mLoss[0m : 1.58559
[1mStep[0m  [198/339], [94mLoss[0m : 2.08568
[1mStep[0m  [231/339], [94mLoss[0m : 2.53354
[1mStep[0m  [264/339], [94mLoss[0m : 1.65302
[1mStep[0m  [297/339], [94mLoss[0m : 1.77841
[1mStep[0m  [330/339], [94mLoss[0m : 1.61821

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70966
[1mStep[0m  [33/339], [94mLoss[0m : 2.23823
[1mStep[0m  [66/339], [94mLoss[0m : 1.66215
[1mStep[0m  [99/339], [94mLoss[0m : 2.09524
[1mStep[0m  [132/339], [94mLoss[0m : 1.78511
[1mStep[0m  [165/339], [94mLoss[0m : 1.56667
[1mStep[0m  [198/339], [94mLoss[0m : 1.80369
[1mStep[0m  [231/339], [94mLoss[0m : 1.98398
[1mStep[0m  [264/339], [94mLoss[0m : 2.54341
[1mStep[0m  [297/339], [94mLoss[0m : 2.22503
[1mStep[0m  [330/339], [94mLoss[0m : 1.98442

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.882, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58594
[1mStep[0m  [33/339], [94mLoss[0m : 2.18676
[1mStep[0m  [66/339], [94mLoss[0m : 2.01340
[1mStep[0m  [99/339], [94mLoss[0m : 1.35643
[1mStep[0m  [132/339], [94mLoss[0m : 1.42506
[1mStep[0m  [165/339], [94mLoss[0m : 1.32473
[1mStep[0m  [198/339], [94mLoss[0m : 1.45228
[1mStep[0m  [231/339], [94mLoss[0m : 1.73050
[1mStep[0m  [264/339], [94mLoss[0m : 1.78711
[1mStep[0m  [297/339], [94mLoss[0m : 1.47764
[1mStep[0m  [330/339], [94mLoss[0m : 1.52792

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87809
[1mStep[0m  [33/339], [94mLoss[0m : 1.62308
[1mStep[0m  [66/339], [94mLoss[0m : 1.94723
[1mStep[0m  [99/339], [94mLoss[0m : 2.19919
[1mStep[0m  [132/339], [94mLoss[0m : 1.71233
[1mStep[0m  [165/339], [94mLoss[0m : 1.76638
[1mStep[0m  [198/339], [94mLoss[0m : 2.00465
[1mStep[0m  [231/339], [94mLoss[0m : 2.03115
[1mStep[0m  [264/339], [94mLoss[0m : 1.38291
[1mStep[0m  [297/339], [94mLoss[0m : 1.77691
[1mStep[0m  [330/339], [94mLoss[0m : 1.73104

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.821, [92mTest[0m: 2.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78697
[1mStep[0m  [33/339], [94mLoss[0m : 1.69370
[1mStep[0m  [66/339], [94mLoss[0m : 1.73476
[1mStep[0m  [99/339], [94mLoss[0m : 1.46865
[1mStep[0m  [132/339], [94mLoss[0m : 2.09179
[1mStep[0m  [165/339], [94mLoss[0m : 1.55062
[1mStep[0m  [198/339], [94mLoss[0m : 1.97908
[1mStep[0m  [231/339], [94mLoss[0m : 2.13778
[1mStep[0m  [264/339], [94mLoss[0m : 1.75556
[1mStep[0m  [297/339], [94mLoss[0m : 1.80173
[1mStep[0m  [330/339], [94mLoss[0m : 2.28656

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78393
[1mStep[0m  [33/339], [94mLoss[0m : 2.12258
[1mStep[0m  [66/339], [94mLoss[0m : 1.86383
[1mStep[0m  [99/339], [94mLoss[0m : 1.75641
[1mStep[0m  [132/339], [94mLoss[0m : 1.41499
[1mStep[0m  [165/339], [94mLoss[0m : 1.81967
[1mStep[0m  [198/339], [94mLoss[0m : 1.68839
[1mStep[0m  [231/339], [94mLoss[0m : 1.77946
[1mStep[0m  [264/339], [94mLoss[0m : 2.11743
[1mStep[0m  [297/339], [94mLoss[0m : 1.75236
[1mStep[0m  [330/339], [94mLoss[0m : 1.89187

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37294
[1mStep[0m  [33/339], [94mLoss[0m : 1.45189
[1mStep[0m  [66/339], [94mLoss[0m : 2.86824
[1mStep[0m  [99/339], [94mLoss[0m : 1.98916
[1mStep[0m  [132/339], [94mLoss[0m : 2.30525
[1mStep[0m  [165/339], [94mLoss[0m : 1.79442
[1mStep[0m  [198/339], [94mLoss[0m : 1.79838
[1mStep[0m  [231/339], [94mLoss[0m : 1.83712
[1mStep[0m  [264/339], [94mLoss[0m : 1.87340
[1mStep[0m  [297/339], [94mLoss[0m : 2.53362
[1mStep[0m  [330/339], [94mLoss[0m : 1.73550

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.580, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52267
[1mStep[0m  [33/339], [94mLoss[0m : 1.62251
[1mStep[0m  [66/339], [94mLoss[0m : 1.19470
[1mStep[0m  [99/339], [94mLoss[0m : 1.92120
[1mStep[0m  [132/339], [94mLoss[0m : 1.49253
[1mStep[0m  [165/339], [94mLoss[0m : 1.74098
[1mStep[0m  [198/339], [94mLoss[0m : 2.09991
[1mStep[0m  [231/339], [94mLoss[0m : 1.75183
[1mStep[0m  [264/339], [94mLoss[0m : 1.53380
[1mStep[0m  [297/339], [94mLoss[0m : 2.14719
[1mStep[0m  [330/339], [94mLoss[0m : 1.47781

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.498, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61516
[1mStep[0m  [33/339], [94mLoss[0m : 1.62839
[1mStep[0m  [66/339], [94mLoss[0m : 1.74966
[1mStep[0m  [99/339], [94mLoss[0m : 2.03091
[1mStep[0m  [132/339], [94mLoss[0m : 1.56468
[1mStep[0m  [165/339], [94mLoss[0m : 1.76208
[1mStep[0m  [198/339], [94mLoss[0m : 1.22541
[1mStep[0m  [231/339], [94mLoss[0m : 1.71825
[1mStep[0m  [264/339], [94mLoss[0m : 1.77070
[1mStep[0m  [297/339], [94mLoss[0m : 1.78609
[1mStep[0m  [330/339], [94mLoss[0m : 1.74675

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42972
[1mStep[0m  [33/339], [94mLoss[0m : 1.71327
[1mStep[0m  [66/339], [94mLoss[0m : 1.51827
[1mStep[0m  [99/339], [94mLoss[0m : 1.76045
[1mStep[0m  [132/339], [94mLoss[0m : 1.49211
[1mStep[0m  [165/339], [94mLoss[0m : 1.48310
[1mStep[0m  [198/339], [94mLoss[0m : 1.36703
[1mStep[0m  [231/339], [94mLoss[0m : 1.41952
[1mStep[0m  [264/339], [94mLoss[0m : 1.93899
[1mStep[0m  [297/339], [94mLoss[0m : 1.65054
[1mStep[0m  [330/339], [94mLoss[0m : 1.93525

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.672, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.50018
[1mStep[0m  [33/339], [94mLoss[0m : 1.65870
[1mStep[0m  [66/339], [94mLoss[0m : 1.29218
[1mStep[0m  [99/339], [94mLoss[0m : 1.69394
[1mStep[0m  [132/339], [94mLoss[0m : 1.57159
[1mStep[0m  [165/339], [94mLoss[0m : 1.80985
[1mStep[0m  [198/339], [94mLoss[0m : 1.48019
[1mStep[0m  [231/339], [94mLoss[0m : 2.01247
[1mStep[0m  [264/339], [94mLoss[0m : 1.49817
[1mStep[0m  [297/339], [94mLoss[0m : 1.47124
[1mStep[0m  [330/339], [94mLoss[0m : 1.74933

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.625, [92mTest[0m: 2.525, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63520
[1mStep[0m  [33/339], [94mLoss[0m : 1.40825
[1mStep[0m  [66/339], [94mLoss[0m : 1.66091
[1mStep[0m  [99/339], [94mLoss[0m : 1.96915
[1mStep[0m  [132/339], [94mLoss[0m : 1.57589
[1mStep[0m  [165/339], [94mLoss[0m : 1.48448
[1mStep[0m  [198/339], [94mLoss[0m : 1.55115
[1mStep[0m  [231/339], [94mLoss[0m : 1.68264
[1mStep[0m  [264/339], [94mLoss[0m : 0.95569
[1mStep[0m  [297/339], [94mLoss[0m : 2.12791
[1mStep[0m  [330/339], [94mLoss[0m : 1.65378

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.502, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62475
[1mStep[0m  [33/339], [94mLoss[0m : 1.76877
[1mStep[0m  [66/339], [94mLoss[0m : 1.21415
[1mStep[0m  [99/339], [94mLoss[0m : 1.45865
[1mStep[0m  [132/339], [94mLoss[0m : 1.43211
[1mStep[0m  [165/339], [94mLoss[0m : 1.18444
[1mStep[0m  [198/339], [94mLoss[0m : 1.74993
[1mStep[0m  [231/339], [94mLoss[0m : 1.46070
[1mStep[0m  [264/339], [94mLoss[0m : 1.39986
[1mStep[0m  [297/339], [94mLoss[0m : 1.34617
[1mStep[0m  [330/339], [94mLoss[0m : 1.19569

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.559, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42147
[1mStep[0m  [33/339], [94mLoss[0m : 1.42357
[1mStep[0m  [66/339], [94mLoss[0m : 1.45678
[1mStep[0m  [99/339], [94mLoss[0m : 1.56245
[1mStep[0m  [132/339], [94mLoss[0m : 1.57460
[1mStep[0m  [165/339], [94mLoss[0m : 2.08941
[1mStep[0m  [198/339], [94mLoss[0m : 1.63816
[1mStep[0m  [231/339], [94mLoss[0m : 2.00759
[1mStep[0m  [264/339], [94mLoss[0m : 2.07266
[1mStep[0m  [297/339], [94mLoss[0m : 1.65452
[1mStep[0m  [330/339], [94mLoss[0m : 1.75212

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.50090
[1mStep[0m  [33/339], [94mLoss[0m : 1.57810
[1mStep[0m  [66/339], [94mLoss[0m : 1.37402
[1mStep[0m  [99/339], [94mLoss[0m : 1.36796
[1mStep[0m  [132/339], [94mLoss[0m : 1.46508
[1mStep[0m  [165/339], [94mLoss[0m : 1.16831
[1mStep[0m  [198/339], [94mLoss[0m : 1.28981
[1mStep[0m  [231/339], [94mLoss[0m : 1.72542
[1mStep[0m  [264/339], [94mLoss[0m : 1.87970
[1mStep[0m  [297/339], [94mLoss[0m : 1.81239
[1mStep[0m  [330/339], [94mLoss[0m : 1.16256

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.36986
[1mStep[0m  [33/339], [94mLoss[0m : 1.62907
[1mStep[0m  [66/339], [94mLoss[0m : 1.72169
[1mStep[0m  [99/339], [94mLoss[0m : 1.45372
[1mStep[0m  [132/339], [94mLoss[0m : 1.40087
[1mStep[0m  [165/339], [94mLoss[0m : 1.14159
[1mStep[0m  [198/339], [94mLoss[0m : 1.61449
[1mStep[0m  [231/339], [94mLoss[0m : 1.69685
[1mStep[0m  [264/339], [94mLoss[0m : 1.57963
[1mStep[0m  [297/339], [94mLoss[0m : 1.78305
[1mStep[0m  [330/339], [94mLoss[0m : 1.20720

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.548, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69116
[1mStep[0m  [33/339], [94mLoss[0m : 1.19980
[1mStep[0m  [66/339], [94mLoss[0m : 1.57443
[1mStep[0m  [99/339], [94mLoss[0m : 1.43181
[1mStep[0m  [132/339], [94mLoss[0m : 2.05848
[1mStep[0m  [165/339], [94mLoss[0m : 1.72366
[1mStep[0m  [198/339], [94mLoss[0m : 1.52735
[1mStep[0m  [231/339], [94mLoss[0m : 2.13759
[1mStep[0m  [264/339], [94mLoss[0m : 1.31959
[1mStep[0m  [297/339], [94mLoss[0m : 1.60172
[1mStep[0m  [330/339], [94mLoss[0m : 1.30647

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35452
[1mStep[0m  [33/339], [94mLoss[0m : 1.56295
[1mStep[0m  [66/339], [94mLoss[0m : 1.64242
[1mStep[0m  [99/339], [94mLoss[0m : 1.73728
[1mStep[0m  [132/339], [94mLoss[0m : 1.27198
[1mStep[0m  [165/339], [94mLoss[0m : 1.29191
[1mStep[0m  [198/339], [94mLoss[0m : 1.88069
[1mStep[0m  [231/339], [94mLoss[0m : 1.45232
[1mStep[0m  [264/339], [94mLoss[0m : 1.13844
[1mStep[0m  [297/339], [94mLoss[0m : 2.00988
[1mStep[0m  [330/339], [94mLoss[0m : 1.36763

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.512, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42766
[1mStep[0m  [33/339], [94mLoss[0m : 1.70774
[1mStep[0m  [66/339], [94mLoss[0m : 1.23045
[1mStep[0m  [99/339], [94mLoss[0m : 1.87687
[1mStep[0m  [132/339], [94mLoss[0m : 1.48410
[1mStep[0m  [165/339], [94mLoss[0m : 2.17863
[1mStep[0m  [198/339], [94mLoss[0m : 1.60913
[1mStep[0m  [231/339], [94mLoss[0m : 1.17996
[1mStep[0m  [264/339], [94mLoss[0m : 1.69270
[1mStep[0m  [297/339], [94mLoss[0m : 1.80610
[1mStep[0m  [330/339], [94mLoss[0m : 1.53503

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24197
[1mStep[0m  [33/339], [94mLoss[0m : 1.24059
[1mStep[0m  [66/339], [94mLoss[0m : 1.67219
[1mStep[0m  [99/339], [94mLoss[0m : 1.53933
[1mStep[0m  [132/339], [94mLoss[0m : 1.46435
[1mStep[0m  [165/339], [94mLoss[0m : 1.24230
[1mStep[0m  [198/339], [94mLoss[0m : 1.21293
[1mStep[0m  [231/339], [94mLoss[0m : 1.41897
[1mStep[0m  [264/339], [94mLoss[0m : 1.45774
[1mStep[0m  [297/339], [94mLoss[0m : 1.42129
[1mStep[0m  [330/339], [94mLoss[0m : 1.65057

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.569, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.478
====================================

Phase 2 - Evaluation MAE:  2.47780515240357
MAE score P1        2.322435
MAE score P2        2.477805
loss                1.470526
learning_rate       0.002575
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay           0.001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.53028
[1mStep[0m  [33/339], [94mLoss[0m : 1.93789
[1mStep[0m  [66/339], [94mLoss[0m : 2.60192
[1mStep[0m  [99/339], [94mLoss[0m : 2.22712
[1mStep[0m  [132/339], [94mLoss[0m : 2.12593
[1mStep[0m  [165/339], [94mLoss[0m : 1.68426
[1mStep[0m  [198/339], [94mLoss[0m : 2.33272
[1mStep[0m  [231/339], [94mLoss[0m : 2.44470
[1mStep[0m  [264/339], [94mLoss[0m : 2.70013
[1mStep[0m  [297/339], [94mLoss[0m : 2.77744
[1mStep[0m  [330/339], [94mLoss[0m : 3.02281

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.735, [92mTest[0m: 10.963, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66897
[1mStep[0m  [33/339], [94mLoss[0m : 2.89025
[1mStep[0m  [66/339], [94mLoss[0m : 2.61870
[1mStep[0m  [99/339], [94mLoss[0m : 2.34881
[1mStep[0m  [132/339], [94mLoss[0m : 2.65305
[1mStep[0m  [165/339], [94mLoss[0m : 2.12920
[1mStep[0m  [198/339], [94mLoss[0m : 1.92072
[1mStep[0m  [231/339], [94mLoss[0m : 2.50967
[1mStep[0m  [264/339], [94mLoss[0m : 2.37068
[1mStep[0m  [297/339], [94mLoss[0m : 2.73304
[1mStep[0m  [330/339], [94mLoss[0m : 2.87634

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41260
[1mStep[0m  [33/339], [94mLoss[0m : 1.96797
[1mStep[0m  [66/339], [94mLoss[0m : 2.60276
[1mStep[0m  [99/339], [94mLoss[0m : 2.01630
[1mStep[0m  [132/339], [94mLoss[0m : 2.36747
[1mStep[0m  [165/339], [94mLoss[0m : 1.96207
[1mStep[0m  [198/339], [94mLoss[0m : 2.72423
[1mStep[0m  [231/339], [94mLoss[0m : 2.76886
[1mStep[0m  [264/339], [94mLoss[0m : 2.74194
[1mStep[0m  [297/339], [94mLoss[0m : 2.07047
[1mStep[0m  [330/339], [94mLoss[0m : 2.76170

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20708
[1mStep[0m  [33/339], [94mLoss[0m : 2.09129
[1mStep[0m  [66/339], [94mLoss[0m : 1.91020
[1mStep[0m  [99/339], [94mLoss[0m : 2.86948
[1mStep[0m  [132/339], [94mLoss[0m : 2.97640
[1mStep[0m  [165/339], [94mLoss[0m : 2.46805
[1mStep[0m  [198/339], [94mLoss[0m : 2.29540
[1mStep[0m  [231/339], [94mLoss[0m : 2.78421
[1mStep[0m  [264/339], [94mLoss[0m : 2.54996
[1mStep[0m  [297/339], [94mLoss[0m : 2.95346
[1mStep[0m  [330/339], [94mLoss[0m : 2.62102

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.34022
[1mStep[0m  [33/339], [94mLoss[0m : 1.79669
[1mStep[0m  [66/339], [94mLoss[0m : 2.20065
[1mStep[0m  [99/339], [94mLoss[0m : 2.43117
[1mStep[0m  [132/339], [94mLoss[0m : 3.16272
[1mStep[0m  [165/339], [94mLoss[0m : 2.43076
[1mStep[0m  [198/339], [94mLoss[0m : 2.75966
[1mStep[0m  [231/339], [94mLoss[0m : 1.96406
[1mStep[0m  [264/339], [94mLoss[0m : 2.96321
[1mStep[0m  [297/339], [94mLoss[0m : 2.81882
[1mStep[0m  [330/339], [94mLoss[0m : 2.14541

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67170
[1mStep[0m  [33/339], [94mLoss[0m : 2.15734
[1mStep[0m  [66/339], [94mLoss[0m : 1.66924
[1mStep[0m  [99/339], [94mLoss[0m : 2.68100
[1mStep[0m  [132/339], [94mLoss[0m : 2.22293
[1mStep[0m  [165/339], [94mLoss[0m : 2.38061
[1mStep[0m  [198/339], [94mLoss[0m : 2.45030
[1mStep[0m  [231/339], [94mLoss[0m : 2.61978
[1mStep[0m  [264/339], [94mLoss[0m : 1.78256
[1mStep[0m  [297/339], [94mLoss[0m : 2.44760
[1mStep[0m  [330/339], [94mLoss[0m : 2.44402

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.333, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07667
[1mStep[0m  [33/339], [94mLoss[0m : 2.18562
[1mStep[0m  [66/339], [94mLoss[0m : 2.77248
[1mStep[0m  [99/339], [94mLoss[0m : 2.32347
[1mStep[0m  [132/339], [94mLoss[0m : 2.10250
[1mStep[0m  [165/339], [94mLoss[0m : 2.14872
[1mStep[0m  [198/339], [94mLoss[0m : 3.05508
[1mStep[0m  [231/339], [94mLoss[0m : 3.03251
[1mStep[0m  [264/339], [94mLoss[0m : 2.33489
[1mStep[0m  [297/339], [94mLoss[0m : 2.45027
[1mStep[0m  [330/339], [94mLoss[0m : 2.13062

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.13570
[1mStep[0m  [33/339], [94mLoss[0m : 2.29707
[1mStep[0m  [66/339], [94mLoss[0m : 2.80634
[1mStep[0m  [99/339], [94mLoss[0m : 1.90598
[1mStep[0m  [132/339], [94mLoss[0m : 2.76928
[1mStep[0m  [165/339], [94mLoss[0m : 3.03101
[1mStep[0m  [198/339], [94mLoss[0m : 2.91141
[1mStep[0m  [231/339], [94mLoss[0m : 2.48471
[1mStep[0m  [264/339], [94mLoss[0m : 2.58691
[1mStep[0m  [297/339], [94mLoss[0m : 2.45271
[1mStep[0m  [330/339], [94mLoss[0m : 2.20563

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87810
[1mStep[0m  [33/339], [94mLoss[0m : 2.15085
[1mStep[0m  [66/339], [94mLoss[0m : 2.11780
[1mStep[0m  [99/339], [94mLoss[0m : 2.87520
[1mStep[0m  [132/339], [94mLoss[0m : 2.69696
[1mStep[0m  [165/339], [94mLoss[0m : 2.65957
[1mStep[0m  [198/339], [94mLoss[0m : 2.42650
[1mStep[0m  [231/339], [94mLoss[0m : 2.40155
[1mStep[0m  [264/339], [94mLoss[0m : 2.44925
[1mStep[0m  [297/339], [94mLoss[0m : 3.03300
[1mStep[0m  [330/339], [94mLoss[0m : 2.41698

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53855
[1mStep[0m  [33/339], [94mLoss[0m : 2.07337
[1mStep[0m  [66/339], [94mLoss[0m : 3.75070
[1mStep[0m  [99/339], [94mLoss[0m : 2.84113
[1mStep[0m  [132/339], [94mLoss[0m : 2.86300
[1mStep[0m  [165/339], [94mLoss[0m : 2.70808
[1mStep[0m  [198/339], [94mLoss[0m : 2.33269
[1mStep[0m  [231/339], [94mLoss[0m : 2.15916
[1mStep[0m  [264/339], [94mLoss[0m : 1.74599
[1mStep[0m  [297/339], [94mLoss[0m : 2.11108
[1mStep[0m  [330/339], [94mLoss[0m : 2.45540

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44949
[1mStep[0m  [33/339], [94mLoss[0m : 2.31333
[1mStep[0m  [66/339], [94mLoss[0m : 2.85447
[1mStep[0m  [99/339], [94mLoss[0m : 2.43781
[1mStep[0m  [132/339], [94mLoss[0m : 2.63803
[1mStep[0m  [165/339], [94mLoss[0m : 2.54450
[1mStep[0m  [198/339], [94mLoss[0m : 2.37575
[1mStep[0m  [231/339], [94mLoss[0m : 2.63220
[1mStep[0m  [264/339], [94mLoss[0m : 2.34155
[1mStep[0m  [297/339], [94mLoss[0m : 2.63768
[1mStep[0m  [330/339], [94mLoss[0m : 2.91567

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20655
[1mStep[0m  [33/339], [94mLoss[0m : 2.91248
[1mStep[0m  [66/339], [94mLoss[0m : 2.55235
[1mStep[0m  [99/339], [94mLoss[0m : 2.57542
[1mStep[0m  [132/339], [94mLoss[0m : 2.22127
[1mStep[0m  [165/339], [94mLoss[0m : 2.97978
[1mStep[0m  [198/339], [94mLoss[0m : 2.41091
[1mStep[0m  [231/339], [94mLoss[0m : 2.09896
[1mStep[0m  [264/339], [94mLoss[0m : 1.97510
[1mStep[0m  [297/339], [94mLoss[0m : 2.64319
[1mStep[0m  [330/339], [94mLoss[0m : 2.20478

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88986
[1mStep[0m  [33/339], [94mLoss[0m : 2.22337
[1mStep[0m  [66/339], [94mLoss[0m : 1.93027
[1mStep[0m  [99/339], [94mLoss[0m : 2.25019
[1mStep[0m  [132/339], [94mLoss[0m : 2.87579
[1mStep[0m  [165/339], [94mLoss[0m : 2.69019
[1mStep[0m  [198/339], [94mLoss[0m : 2.26714
[1mStep[0m  [231/339], [94mLoss[0m : 2.31887
[1mStep[0m  [264/339], [94mLoss[0m : 2.14898
[1mStep[0m  [297/339], [94mLoss[0m : 2.60851
[1mStep[0m  [330/339], [94mLoss[0m : 1.95623

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38416
[1mStep[0m  [33/339], [94mLoss[0m : 2.16198
[1mStep[0m  [66/339], [94mLoss[0m : 1.76962
[1mStep[0m  [99/339], [94mLoss[0m : 2.37471
[1mStep[0m  [132/339], [94mLoss[0m : 2.01585
[1mStep[0m  [165/339], [94mLoss[0m : 1.73062
[1mStep[0m  [198/339], [94mLoss[0m : 2.61729
[1mStep[0m  [231/339], [94mLoss[0m : 2.89017
[1mStep[0m  [264/339], [94mLoss[0m : 2.52737
[1mStep[0m  [297/339], [94mLoss[0m : 3.21315
[1mStep[0m  [330/339], [94mLoss[0m : 2.29216

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.16661
[1mStep[0m  [33/339], [94mLoss[0m : 2.09982
[1mStep[0m  [66/339], [94mLoss[0m : 2.42113
[1mStep[0m  [99/339], [94mLoss[0m : 2.67844
[1mStep[0m  [132/339], [94mLoss[0m : 2.17791
[1mStep[0m  [165/339], [94mLoss[0m : 2.61021
[1mStep[0m  [198/339], [94mLoss[0m : 2.11952
[1mStep[0m  [231/339], [94mLoss[0m : 2.21418
[1mStep[0m  [264/339], [94mLoss[0m : 2.75969
[1mStep[0m  [297/339], [94mLoss[0m : 1.96145
[1mStep[0m  [330/339], [94mLoss[0m : 2.98143

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13922
[1mStep[0m  [33/339], [94mLoss[0m : 2.55452
[1mStep[0m  [66/339], [94mLoss[0m : 2.93277
[1mStep[0m  [99/339], [94mLoss[0m : 2.38634
[1mStep[0m  [132/339], [94mLoss[0m : 2.61301
[1mStep[0m  [165/339], [94mLoss[0m : 1.87122
[1mStep[0m  [198/339], [94mLoss[0m : 2.28350
[1mStep[0m  [231/339], [94mLoss[0m : 2.21653
[1mStep[0m  [264/339], [94mLoss[0m : 1.97717
[1mStep[0m  [297/339], [94mLoss[0m : 2.13179
[1mStep[0m  [330/339], [94mLoss[0m : 1.82571

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47569
[1mStep[0m  [33/339], [94mLoss[0m : 3.11263
[1mStep[0m  [66/339], [94mLoss[0m : 2.61138
[1mStep[0m  [99/339], [94mLoss[0m : 2.58489
[1mStep[0m  [132/339], [94mLoss[0m : 2.03148
[1mStep[0m  [165/339], [94mLoss[0m : 1.85711
[1mStep[0m  [198/339], [94mLoss[0m : 2.76802
[1mStep[0m  [231/339], [94mLoss[0m : 2.46177
[1mStep[0m  [264/339], [94mLoss[0m : 1.71930
[1mStep[0m  [297/339], [94mLoss[0m : 2.42186
[1mStep[0m  [330/339], [94mLoss[0m : 1.92466

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36233
[1mStep[0m  [33/339], [94mLoss[0m : 2.38813
[1mStep[0m  [66/339], [94mLoss[0m : 2.52605
[1mStep[0m  [99/339], [94mLoss[0m : 2.17940
[1mStep[0m  [132/339], [94mLoss[0m : 2.23827
[1mStep[0m  [165/339], [94mLoss[0m : 2.27754
[1mStep[0m  [198/339], [94mLoss[0m : 3.08086
[1mStep[0m  [231/339], [94mLoss[0m : 2.82724
[1mStep[0m  [264/339], [94mLoss[0m : 2.46376
[1mStep[0m  [297/339], [94mLoss[0m : 1.99702
[1mStep[0m  [330/339], [94mLoss[0m : 3.76399

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19906
[1mStep[0m  [33/339], [94mLoss[0m : 2.80718
[1mStep[0m  [66/339], [94mLoss[0m : 2.19427
[1mStep[0m  [99/339], [94mLoss[0m : 2.46677
[1mStep[0m  [132/339], [94mLoss[0m : 1.95370
[1mStep[0m  [165/339], [94mLoss[0m : 2.30541
[1mStep[0m  [198/339], [94mLoss[0m : 2.12413
[1mStep[0m  [231/339], [94mLoss[0m : 3.08034
[1mStep[0m  [264/339], [94mLoss[0m : 2.40152
[1mStep[0m  [297/339], [94mLoss[0m : 2.57838
[1mStep[0m  [330/339], [94mLoss[0m : 2.46624

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.308, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32313
[1mStep[0m  [33/339], [94mLoss[0m : 2.41609
[1mStep[0m  [66/339], [94mLoss[0m : 2.57958
[1mStep[0m  [99/339], [94mLoss[0m : 2.41902
[1mStep[0m  [132/339], [94mLoss[0m : 2.67292
[1mStep[0m  [165/339], [94mLoss[0m : 2.75223
[1mStep[0m  [198/339], [94mLoss[0m : 2.55277
[1mStep[0m  [231/339], [94mLoss[0m : 2.15336
[1mStep[0m  [264/339], [94mLoss[0m : 2.69360
[1mStep[0m  [297/339], [94mLoss[0m : 1.92915
[1mStep[0m  [330/339], [94mLoss[0m : 2.20221

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.386, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05614
[1mStep[0m  [33/339], [94mLoss[0m : 3.39591
[1mStep[0m  [66/339], [94mLoss[0m : 2.50235
[1mStep[0m  [99/339], [94mLoss[0m : 1.65611
[1mStep[0m  [132/339], [94mLoss[0m : 2.63381
[1mStep[0m  [165/339], [94mLoss[0m : 1.85109
[1mStep[0m  [198/339], [94mLoss[0m : 2.50305
[1mStep[0m  [231/339], [94mLoss[0m : 2.47424
[1mStep[0m  [264/339], [94mLoss[0m : 2.61653
[1mStep[0m  [297/339], [94mLoss[0m : 2.14617
[1mStep[0m  [330/339], [94mLoss[0m : 2.74872

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96527
[1mStep[0m  [33/339], [94mLoss[0m : 1.88091
[1mStep[0m  [66/339], [94mLoss[0m : 2.92002
[1mStep[0m  [99/339], [94mLoss[0m : 2.64251
[1mStep[0m  [132/339], [94mLoss[0m : 2.54262
[1mStep[0m  [165/339], [94mLoss[0m : 3.11992
[1mStep[0m  [198/339], [94mLoss[0m : 2.77504
[1mStep[0m  [231/339], [94mLoss[0m : 2.30471
[1mStep[0m  [264/339], [94mLoss[0m : 2.51016
[1mStep[0m  [297/339], [94mLoss[0m : 1.93849
[1mStep[0m  [330/339], [94mLoss[0m : 2.62396

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.317, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25506
[1mStep[0m  [33/339], [94mLoss[0m : 2.30454
[1mStep[0m  [66/339], [94mLoss[0m : 3.41689
[1mStep[0m  [99/339], [94mLoss[0m : 2.55958
[1mStep[0m  [132/339], [94mLoss[0m : 2.53586
[1mStep[0m  [165/339], [94mLoss[0m : 2.69499
[1mStep[0m  [198/339], [94mLoss[0m : 2.37823
[1mStep[0m  [231/339], [94mLoss[0m : 2.22831
[1mStep[0m  [264/339], [94mLoss[0m : 2.41635
[1mStep[0m  [297/339], [94mLoss[0m : 2.21321
[1mStep[0m  [330/339], [94mLoss[0m : 1.64818

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29802
[1mStep[0m  [33/339], [94mLoss[0m : 3.23421
[1mStep[0m  [66/339], [94mLoss[0m : 2.57188
[1mStep[0m  [99/339], [94mLoss[0m : 2.18997
[1mStep[0m  [132/339], [94mLoss[0m : 2.55596
[1mStep[0m  [165/339], [94mLoss[0m : 2.47540
[1mStep[0m  [198/339], [94mLoss[0m : 3.54353
[1mStep[0m  [231/339], [94mLoss[0m : 2.71579
[1mStep[0m  [264/339], [94mLoss[0m : 1.93762
[1mStep[0m  [297/339], [94mLoss[0m : 2.20944
[1mStep[0m  [330/339], [94mLoss[0m : 2.47673

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23480
[1mStep[0m  [33/339], [94mLoss[0m : 2.46601
[1mStep[0m  [66/339], [94mLoss[0m : 1.80720
[1mStep[0m  [99/339], [94mLoss[0m : 2.61805
[1mStep[0m  [132/339], [94mLoss[0m : 2.51049
[1mStep[0m  [165/339], [94mLoss[0m : 2.74155
[1mStep[0m  [198/339], [94mLoss[0m : 3.51076
[1mStep[0m  [231/339], [94mLoss[0m : 2.26119
[1mStep[0m  [264/339], [94mLoss[0m : 3.12678
[1mStep[0m  [297/339], [94mLoss[0m : 3.11411
[1mStep[0m  [330/339], [94mLoss[0m : 2.51165

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.02403
[1mStep[0m  [33/339], [94mLoss[0m : 2.13350
[1mStep[0m  [66/339], [94mLoss[0m : 2.48295
[1mStep[0m  [99/339], [94mLoss[0m : 2.58575
[1mStep[0m  [132/339], [94mLoss[0m : 1.79105
[1mStep[0m  [165/339], [94mLoss[0m : 2.25533
[1mStep[0m  [198/339], [94mLoss[0m : 2.02541
[1mStep[0m  [231/339], [94mLoss[0m : 3.64450
[1mStep[0m  [264/339], [94mLoss[0m : 2.37965
[1mStep[0m  [297/339], [94mLoss[0m : 2.49775
[1mStep[0m  [330/339], [94mLoss[0m : 2.63009

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25042
[1mStep[0m  [33/339], [94mLoss[0m : 2.93731
[1mStep[0m  [66/339], [94mLoss[0m : 1.99948
[1mStep[0m  [99/339], [94mLoss[0m : 2.31395
[1mStep[0m  [132/339], [94mLoss[0m : 2.81741
[1mStep[0m  [165/339], [94mLoss[0m : 2.53536
[1mStep[0m  [198/339], [94mLoss[0m : 2.54273
[1mStep[0m  [231/339], [94mLoss[0m : 2.38470
[1mStep[0m  [264/339], [94mLoss[0m : 2.61251
[1mStep[0m  [297/339], [94mLoss[0m : 1.96133
[1mStep[0m  [330/339], [94mLoss[0m : 1.99799

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03574
[1mStep[0m  [33/339], [94mLoss[0m : 2.95558
[1mStep[0m  [66/339], [94mLoss[0m : 2.36279
[1mStep[0m  [99/339], [94mLoss[0m : 2.28794
[1mStep[0m  [132/339], [94mLoss[0m : 2.46227
[1mStep[0m  [165/339], [94mLoss[0m : 2.45645
[1mStep[0m  [198/339], [94mLoss[0m : 2.08121
[1mStep[0m  [231/339], [94mLoss[0m : 1.94184
[1mStep[0m  [264/339], [94mLoss[0m : 2.35740
[1mStep[0m  [297/339], [94mLoss[0m : 2.46154
[1mStep[0m  [330/339], [94mLoss[0m : 3.07418

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87048
[1mStep[0m  [33/339], [94mLoss[0m : 2.60022
[1mStep[0m  [66/339], [94mLoss[0m : 3.19195
[1mStep[0m  [99/339], [94mLoss[0m : 2.81483
[1mStep[0m  [132/339], [94mLoss[0m : 1.95212
[1mStep[0m  [165/339], [94mLoss[0m : 1.96711
[1mStep[0m  [198/339], [94mLoss[0m : 2.67636
[1mStep[0m  [231/339], [94mLoss[0m : 2.66575
[1mStep[0m  [264/339], [94mLoss[0m : 2.50096
[1mStep[0m  [297/339], [94mLoss[0m : 2.84434
[1mStep[0m  [330/339], [94mLoss[0m : 2.83976

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81415
[1mStep[0m  [33/339], [94mLoss[0m : 2.50343
[1mStep[0m  [66/339], [94mLoss[0m : 2.91083
[1mStep[0m  [99/339], [94mLoss[0m : 2.51322
[1mStep[0m  [132/339], [94mLoss[0m : 2.34261
[1mStep[0m  [165/339], [94mLoss[0m : 1.90476
[1mStep[0m  [198/339], [94mLoss[0m : 2.72297
[1mStep[0m  [231/339], [94mLoss[0m : 1.60092
[1mStep[0m  [264/339], [94mLoss[0m : 2.26063
[1mStep[0m  [297/339], [94mLoss[0m : 2.10073
[1mStep[0m  [330/339], [94mLoss[0m : 1.98254

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.393061568251753
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.43071
[1mStep[0m  [33/339], [94mLoss[0m : 2.06586
[1mStep[0m  [66/339], [94mLoss[0m : 2.23743
[1mStep[0m  [99/339], [94mLoss[0m : 2.52023
[1mStep[0m  [132/339], [94mLoss[0m : 2.55342
[1mStep[0m  [165/339], [94mLoss[0m : 2.69923
[1mStep[0m  [198/339], [94mLoss[0m : 2.77073
[1mStep[0m  [231/339], [94mLoss[0m : 2.32384
[1mStep[0m  [264/339], [94mLoss[0m : 2.66287
[1mStep[0m  [297/339], [94mLoss[0m : 2.38222
[1mStep[0m  [330/339], [94mLoss[0m : 2.60737

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47714
[1mStep[0m  [33/339], [94mLoss[0m : 2.29422
[1mStep[0m  [66/339], [94mLoss[0m : 2.80818
[1mStep[0m  [99/339], [94mLoss[0m : 2.60640
[1mStep[0m  [132/339], [94mLoss[0m : 2.33791
[1mStep[0m  [165/339], [94mLoss[0m : 2.52751
[1mStep[0m  [198/339], [94mLoss[0m : 2.16891
[1mStep[0m  [231/339], [94mLoss[0m : 2.55197
[1mStep[0m  [264/339], [94mLoss[0m : 1.74561
[1mStep[0m  [297/339], [94mLoss[0m : 2.72987
[1mStep[0m  [330/339], [94mLoss[0m : 2.31714

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55954
[1mStep[0m  [33/339], [94mLoss[0m : 1.68580
[1mStep[0m  [66/339], [94mLoss[0m : 2.06109
[1mStep[0m  [99/339], [94mLoss[0m : 1.91873
[1mStep[0m  [132/339], [94mLoss[0m : 2.05781
[1mStep[0m  [165/339], [94mLoss[0m : 2.49254
[1mStep[0m  [198/339], [94mLoss[0m : 1.75920
[1mStep[0m  [231/339], [94mLoss[0m : 1.93044
[1mStep[0m  [264/339], [94mLoss[0m : 2.17598
[1mStep[0m  [297/339], [94mLoss[0m : 2.81103
[1mStep[0m  [330/339], [94mLoss[0m : 2.60084

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42930
[1mStep[0m  [33/339], [94mLoss[0m : 2.48674
[1mStep[0m  [66/339], [94mLoss[0m : 1.83223
[1mStep[0m  [99/339], [94mLoss[0m : 2.03144
[1mStep[0m  [132/339], [94mLoss[0m : 1.72683
[1mStep[0m  [165/339], [94mLoss[0m : 2.56917
[1mStep[0m  [198/339], [94mLoss[0m : 2.44522
[1mStep[0m  [231/339], [94mLoss[0m : 3.18303
[1mStep[0m  [264/339], [94mLoss[0m : 1.90671
[1mStep[0m  [297/339], [94mLoss[0m : 2.19700
[1mStep[0m  [330/339], [94mLoss[0m : 2.15524

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.557, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60796
[1mStep[0m  [33/339], [94mLoss[0m : 2.29113
[1mStep[0m  [66/339], [94mLoss[0m : 1.95554
[1mStep[0m  [99/339], [94mLoss[0m : 1.72263
[1mStep[0m  [132/339], [94mLoss[0m : 3.02917
[1mStep[0m  [165/339], [94mLoss[0m : 1.78472
[1mStep[0m  [198/339], [94mLoss[0m : 2.06505
[1mStep[0m  [231/339], [94mLoss[0m : 3.71693
[1mStep[0m  [264/339], [94mLoss[0m : 2.30511
[1mStep[0m  [297/339], [94mLoss[0m : 1.86476
[1mStep[0m  [330/339], [94mLoss[0m : 1.78386

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.154, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01933
[1mStep[0m  [33/339], [94mLoss[0m : 1.79504
[1mStep[0m  [66/339], [94mLoss[0m : 1.91974
[1mStep[0m  [99/339], [94mLoss[0m : 2.40606
[1mStep[0m  [132/339], [94mLoss[0m : 2.63238
[1mStep[0m  [165/339], [94mLoss[0m : 1.29932
[1mStep[0m  [198/339], [94mLoss[0m : 2.23541
[1mStep[0m  [231/339], [94mLoss[0m : 1.78262
[1mStep[0m  [264/339], [94mLoss[0m : 2.13145
[1mStep[0m  [297/339], [94mLoss[0m : 1.94682
[1mStep[0m  [330/339], [94mLoss[0m : 2.54645

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.122, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01671
[1mStep[0m  [33/339], [94mLoss[0m : 2.56761
[1mStep[0m  [66/339], [94mLoss[0m : 1.72101
[1mStep[0m  [99/339], [94mLoss[0m : 2.00396
[1mStep[0m  [132/339], [94mLoss[0m : 1.99863
[1mStep[0m  [165/339], [94mLoss[0m : 1.78627
[1mStep[0m  [198/339], [94mLoss[0m : 2.37691
[1mStep[0m  [231/339], [94mLoss[0m : 1.82718
[1mStep[0m  [264/339], [94mLoss[0m : 2.10505
[1mStep[0m  [297/339], [94mLoss[0m : 2.07816
[1mStep[0m  [330/339], [94mLoss[0m : 2.04094

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81244
[1mStep[0m  [33/339], [94mLoss[0m : 2.05408
[1mStep[0m  [66/339], [94mLoss[0m : 1.77117
[1mStep[0m  [99/339], [94mLoss[0m : 2.26455
[1mStep[0m  [132/339], [94mLoss[0m : 2.06713
[1mStep[0m  [165/339], [94mLoss[0m : 1.91894
[1mStep[0m  [198/339], [94mLoss[0m : 1.71448
[1mStep[0m  [231/339], [94mLoss[0m : 1.66254
[1mStep[0m  [264/339], [94mLoss[0m : 2.38229
[1mStep[0m  [297/339], [94mLoss[0m : 1.69426
[1mStep[0m  [330/339], [94mLoss[0m : 1.49657

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99594
[1mStep[0m  [33/339], [94mLoss[0m : 1.71436
[1mStep[0m  [66/339], [94mLoss[0m : 1.71505
[1mStep[0m  [99/339], [94mLoss[0m : 2.01546
[1mStep[0m  [132/339], [94mLoss[0m : 1.87546
[1mStep[0m  [165/339], [94mLoss[0m : 1.66048
[1mStep[0m  [198/339], [94mLoss[0m : 2.05815
[1mStep[0m  [231/339], [94mLoss[0m : 1.77633
[1mStep[0m  [264/339], [94mLoss[0m : 1.82633
[1mStep[0m  [297/339], [94mLoss[0m : 2.03395
[1mStep[0m  [330/339], [94mLoss[0m : 1.80203

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27009
[1mStep[0m  [33/339], [94mLoss[0m : 1.96254
[1mStep[0m  [66/339], [94mLoss[0m : 1.69385
[1mStep[0m  [99/339], [94mLoss[0m : 1.94960
[1mStep[0m  [132/339], [94mLoss[0m : 1.99603
[1mStep[0m  [165/339], [94mLoss[0m : 2.09415
[1mStep[0m  [198/339], [94mLoss[0m : 1.81550
[1mStep[0m  [231/339], [94mLoss[0m : 1.56266
[1mStep[0m  [264/339], [94mLoss[0m : 1.63236
[1mStep[0m  [297/339], [94mLoss[0m : 1.84730
[1mStep[0m  [330/339], [94mLoss[0m : 2.29552

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58893
[1mStep[0m  [33/339], [94mLoss[0m : 1.89124
[1mStep[0m  [66/339], [94mLoss[0m : 1.54895
[1mStep[0m  [99/339], [94mLoss[0m : 1.37207
[1mStep[0m  [132/339], [94mLoss[0m : 2.26363
[1mStep[0m  [165/339], [94mLoss[0m : 1.61962
[1mStep[0m  [198/339], [94mLoss[0m : 2.00815
[1mStep[0m  [231/339], [94mLoss[0m : 1.82494
[1mStep[0m  [264/339], [94mLoss[0m : 1.90818
[1mStep[0m  [297/339], [94mLoss[0m : 2.04320
[1mStep[0m  [330/339], [94mLoss[0m : 2.15447

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.15450
[1mStep[0m  [33/339], [94mLoss[0m : 1.56386
[1mStep[0m  [66/339], [94mLoss[0m : 2.17970
[1mStep[0m  [99/339], [94mLoss[0m : 1.80509
[1mStep[0m  [132/339], [94mLoss[0m : 1.86094
[1mStep[0m  [165/339], [94mLoss[0m : 2.00651
[1mStep[0m  [198/339], [94mLoss[0m : 1.80473
[1mStep[0m  [231/339], [94mLoss[0m : 2.10678
[1mStep[0m  [264/339], [94mLoss[0m : 1.68945
[1mStep[0m  [297/339], [94mLoss[0m : 1.76127
[1mStep[0m  [330/339], [94mLoss[0m : 1.57329

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08909
[1mStep[0m  [33/339], [94mLoss[0m : 1.80279
[1mStep[0m  [66/339], [94mLoss[0m : 1.33958
[1mStep[0m  [99/339], [94mLoss[0m : 1.67381
[1mStep[0m  [132/339], [94mLoss[0m : 1.63643
[1mStep[0m  [165/339], [94mLoss[0m : 2.30551
[1mStep[0m  [198/339], [94mLoss[0m : 2.09166
[1mStep[0m  [231/339], [94mLoss[0m : 2.22743
[1mStep[0m  [264/339], [94mLoss[0m : 2.05294
[1mStep[0m  [297/339], [94mLoss[0m : 1.77646
[1mStep[0m  [330/339], [94mLoss[0m : 1.38779

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08161
[1mStep[0m  [33/339], [94mLoss[0m : 1.99515
[1mStep[0m  [66/339], [94mLoss[0m : 1.87241
[1mStep[0m  [99/339], [94mLoss[0m : 2.36229
[1mStep[0m  [132/339], [94mLoss[0m : 1.76593
[1mStep[0m  [165/339], [94mLoss[0m : 2.02473
[1mStep[0m  [198/339], [94mLoss[0m : 1.46515
[1mStep[0m  [231/339], [94mLoss[0m : 1.45362
[1mStep[0m  [264/339], [94mLoss[0m : 1.80547
[1mStep[0m  [297/339], [94mLoss[0m : 1.99422
[1mStep[0m  [330/339], [94mLoss[0m : 1.80525

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.588, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.76551
[1mStep[0m  [33/339], [94mLoss[0m : 1.59277
[1mStep[0m  [66/339], [94mLoss[0m : 1.96087
[1mStep[0m  [99/339], [94mLoss[0m : 1.60437
[1mStep[0m  [132/339], [94mLoss[0m : 1.76348
[1mStep[0m  [165/339], [94mLoss[0m : 1.90170
[1mStep[0m  [198/339], [94mLoss[0m : 1.77985
[1mStep[0m  [231/339], [94mLoss[0m : 1.91453
[1mStep[0m  [264/339], [94mLoss[0m : 1.23750
[1mStep[0m  [297/339], [94mLoss[0m : 2.03770
[1mStep[0m  [330/339], [94mLoss[0m : 1.91371

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.770, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85021
[1mStep[0m  [33/339], [94mLoss[0m : 1.85028
[1mStep[0m  [66/339], [94mLoss[0m : 1.46485
[1mStep[0m  [99/339], [94mLoss[0m : 2.35005
[1mStep[0m  [132/339], [94mLoss[0m : 2.77099
[1mStep[0m  [165/339], [94mLoss[0m : 1.72300
[1mStep[0m  [198/339], [94mLoss[0m : 1.65324
[1mStep[0m  [231/339], [94mLoss[0m : 1.88377
[1mStep[0m  [264/339], [94mLoss[0m : 1.22228
[1mStep[0m  [297/339], [94mLoss[0m : 1.75376
[1mStep[0m  [330/339], [94mLoss[0m : 1.80238

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80571
[1mStep[0m  [33/339], [94mLoss[0m : 1.41782
[1mStep[0m  [66/339], [94mLoss[0m : 1.45794
[1mStep[0m  [99/339], [94mLoss[0m : 1.66563
[1mStep[0m  [132/339], [94mLoss[0m : 1.59337
[1mStep[0m  [165/339], [94mLoss[0m : 1.63635
[1mStep[0m  [198/339], [94mLoss[0m : 1.35828
[1mStep[0m  [231/339], [94mLoss[0m : 1.65627
[1mStep[0m  [264/339], [94mLoss[0m : 1.50863
[1mStep[0m  [297/339], [94mLoss[0m : 1.36904
[1mStep[0m  [330/339], [94mLoss[0m : 1.74097

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60827
[1mStep[0m  [33/339], [94mLoss[0m : 1.55490
[1mStep[0m  [66/339], [94mLoss[0m : 1.74437
[1mStep[0m  [99/339], [94mLoss[0m : 1.60773
[1mStep[0m  [132/339], [94mLoss[0m : 1.23540
[1mStep[0m  [165/339], [94mLoss[0m : 2.32064
[1mStep[0m  [198/339], [94mLoss[0m : 1.36511
[1mStep[0m  [231/339], [94mLoss[0m : 1.91491
[1mStep[0m  [264/339], [94mLoss[0m : 1.38280
[1mStep[0m  [297/339], [94mLoss[0m : 2.29842
[1mStep[0m  [330/339], [94mLoss[0m : 1.83763

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51737
[1mStep[0m  [33/339], [94mLoss[0m : 1.29159
[1mStep[0m  [66/339], [94mLoss[0m : 1.92321
[1mStep[0m  [99/339], [94mLoss[0m : 1.43315
[1mStep[0m  [132/339], [94mLoss[0m : 1.62474
[1mStep[0m  [165/339], [94mLoss[0m : 1.64247
[1mStep[0m  [198/339], [94mLoss[0m : 1.60480
[1mStep[0m  [231/339], [94mLoss[0m : 1.41565
[1mStep[0m  [264/339], [94mLoss[0m : 1.44429
[1mStep[0m  [297/339], [94mLoss[0m : 1.31424
[1mStep[0m  [330/339], [94mLoss[0m : 1.67458

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43701
[1mStep[0m  [33/339], [94mLoss[0m : 1.47726
[1mStep[0m  [66/339], [94mLoss[0m : 1.09412
[1mStep[0m  [99/339], [94mLoss[0m : 1.62128
[1mStep[0m  [132/339], [94mLoss[0m : 1.64157
[1mStep[0m  [165/339], [94mLoss[0m : 1.67713
[1mStep[0m  [198/339], [94mLoss[0m : 1.74529
[1mStep[0m  [231/339], [94mLoss[0m : 1.31885
[1mStep[0m  [264/339], [94mLoss[0m : 1.78960
[1mStep[0m  [297/339], [94mLoss[0m : 1.68281
[1mStep[0m  [330/339], [94mLoss[0m : 1.86097

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.30957
[1mStep[0m  [33/339], [94mLoss[0m : 1.65002
[1mStep[0m  [66/339], [94mLoss[0m : 1.38098
[1mStep[0m  [99/339], [94mLoss[0m : 1.75245
[1mStep[0m  [132/339], [94mLoss[0m : 1.29680
[1mStep[0m  [165/339], [94mLoss[0m : 1.70158
[1mStep[0m  [198/339], [94mLoss[0m : 1.69794
[1mStep[0m  [231/339], [94mLoss[0m : 1.82411
[1mStep[0m  [264/339], [94mLoss[0m : 1.82717
[1mStep[0m  [297/339], [94mLoss[0m : 2.10591
[1mStep[0m  [330/339], [94mLoss[0m : 1.75504

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.561, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66564
[1mStep[0m  [33/339], [94mLoss[0m : 2.11940
[1mStep[0m  [66/339], [94mLoss[0m : 1.66461
[1mStep[0m  [99/339], [94mLoss[0m : 1.79273
[1mStep[0m  [132/339], [94mLoss[0m : 1.38258
[1mStep[0m  [165/339], [94mLoss[0m : 1.53081
[1mStep[0m  [198/339], [94mLoss[0m : 1.65213
[1mStep[0m  [231/339], [94mLoss[0m : 1.61239
[1mStep[0m  [264/339], [94mLoss[0m : 1.99472
[1mStep[0m  [297/339], [94mLoss[0m : 2.21178
[1mStep[0m  [330/339], [94mLoss[0m : 1.34123

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.452, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.20182
[1mStep[0m  [33/339], [94mLoss[0m : 1.34878
[1mStep[0m  [66/339], [94mLoss[0m : 1.37157
[1mStep[0m  [99/339], [94mLoss[0m : 1.46714
[1mStep[0m  [132/339], [94mLoss[0m : 1.63212
[1mStep[0m  [165/339], [94mLoss[0m : 1.32188
[1mStep[0m  [198/339], [94mLoss[0m : 1.64025
[1mStep[0m  [231/339], [94mLoss[0m : 1.54369
[1mStep[0m  [264/339], [94mLoss[0m : 1.43259
[1mStep[0m  [297/339], [94mLoss[0m : 1.17893
[1mStep[0m  [330/339], [94mLoss[0m : 1.86292

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37593
[1mStep[0m  [33/339], [94mLoss[0m : 1.55209
[1mStep[0m  [66/339], [94mLoss[0m : 1.17386
[1mStep[0m  [99/339], [94mLoss[0m : 1.39107
[1mStep[0m  [132/339], [94mLoss[0m : 0.96611
[1mStep[0m  [165/339], [94mLoss[0m : 1.49202
[1mStep[0m  [198/339], [94mLoss[0m : 1.45834
[1mStep[0m  [231/339], [94mLoss[0m : 1.69875
[1mStep[0m  [264/339], [94mLoss[0m : 1.43321
[1mStep[0m  [297/339], [94mLoss[0m : 1.75820
[1mStep[0m  [330/339], [94mLoss[0m : 1.17746

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.533, [92mTest[0m: 2.620, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77173
[1mStep[0m  [33/339], [94mLoss[0m : 1.31314
[1mStep[0m  [66/339], [94mLoss[0m : 1.24782
[1mStep[0m  [99/339], [94mLoss[0m : 1.50373
[1mStep[0m  [132/339], [94mLoss[0m : 1.48134
[1mStep[0m  [165/339], [94mLoss[0m : 1.40903
[1mStep[0m  [198/339], [94mLoss[0m : 1.42276
[1mStep[0m  [231/339], [94mLoss[0m : 1.48651
[1mStep[0m  [264/339], [94mLoss[0m : 1.43166
[1mStep[0m  [297/339], [94mLoss[0m : 0.91375
[1mStep[0m  [330/339], [94mLoss[0m : 1.41352

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68128
[1mStep[0m  [33/339], [94mLoss[0m : 1.58532
[1mStep[0m  [66/339], [94mLoss[0m : 1.97274
[1mStep[0m  [99/339], [94mLoss[0m : 1.92639
[1mStep[0m  [132/339], [94mLoss[0m : 1.59592
[1mStep[0m  [165/339], [94mLoss[0m : 1.31441
[1mStep[0m  [198/339], [94mLoss[0m : 1.01760
[1mStep[0m  [231/339], [94mLoss[0m : 2.45473
[1mStep[0m  [264/339], [94mLoss[0m : 1.17543
[1mStep[0m  [297/339], [94mLoss[0m : 1.63463
[1mStep[0m  [330/339], [94mLoss[0m : 1.18280

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.514, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35110
[1mStep[0m  [33/339], [94mLoss[0m : 1.24515
[1mStep[0m  [66/339], [94mLoss[0m : 1.43330
[1mStep[0m  [99/339], [94mLoss[0m : 1.40828
[1mStep[0m  [132/339], [94mLoss[0m : 1.17982
[1mStep[0m  [165/339], [94mLoss[0m : 1.48329
[1mStep[0m  [198/339], [94mLoss[0m : 1.25391
[1mStep[0m  [231/339], [94mLoss[0m : 1.94934
[1mStep[0m  [264/339], [94mLoss[0m : 1.51977
[1mStep[0m  [297/339], [94mLoss[0m : 1.61432
[1mStep[0m  [330/339], [94mLoss[0m : 1.40362

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.480, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.07060
[1mStep[0m  [33/339], [94mLoss[0m : 1.37927
[1mStep[0m  [66/339], [94mLoss[0m : 1.16973
[1mStep[0m  [99/339], [94mLoss[0m : 1.40251
[1mStep[0m  [132/339], [94mLoss[0m : 2.14146
[1mStep[0m  [165/339], [94mLoss[0m : 1.45172
[1mStep[0m  [198/339], [94mLoss[0m : 2.11116
[1mStep[0m  [231/339], [94mLoss[0m : 1.72232
[1mStep[0m  [264/339], [94mLoss[0m : 1.54261
[1mStep[0m  [297/339], [94mLoss[0m : 1.27451
[1mStep[0m  [330/339], [94mLoss[0m : 1.36591

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.453, [92mTest[0m: 2.457, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.13652
[1mStep[0m  [33/339], [94mLoss[0m : 1.46377
[1mStep[0m  [66/339], [94mLoss[0m : 1.19311
[1mStep[0m  [99/339], [94mLoss[0m : 1.10282
[1mStep[0m  [132/339], [94mLoss[0m : 1.82586
[1mStep[0m  [165/339], [94mLoss[0m : 1.19455
[1mStep[0m  [198/339], [94mLoss[0m : 2.26153
[1mStep[0m  [231/339], [94mLoss[0m : 1.81641
[1mStep[0m  [264/339], [94mLoss[0m : 1.50358
[1mStep[0m  [297/339], [94mLoss[0m : 1.91872
[1mStep[0m  [330/339], [94mLoss[0m : 1.45167

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.505
====================================

Phase 2 - Evaluation MAE:  2.5047815415711527
MAE score P1      2.393062
MAE score P2      2.504782
loss               1.45274
learning_rate     0.002575
batch_size              32
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay        0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 11.28092
[1mStep[0m  [33/339], [94mLoss[0m : 2.56614
[1mStep[0m  [66/339], [94mLoss[0m : 2.64384
[1mStep[0m  [99/339], [94mLoss[0m : 2.80294
[1mStep[0m  [132/339], [94mLoss[0m : 2.75976
[1mStep[0m  [165/339], [94mLoss[0m : 2.77070
[1mStep[0m  [198/339], [94mLoss[0m : 3.16933
[1mStep[0m  [231/339], [94mLoss[0m : 2.46303
[1mStep[0m  [264/339], [94mLoss[0m : 2.15138
[1mStep[0m  [297/339], [94mLoss[0m : 2.89382
[1mStep[0m  [330/339], [94mLoss[0m : 2.66796

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.192, [92mTest[0m: 11.020, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69495
[1mStep[0m  [33/339], [94mLoss[0m : 2.44348
[1mStep[0m  [66/339], [94mLoss[0m : 2.61217
[1mStep[0m  [99/339], [94mLoss[0m : 2.42802
[1mStep[0m  [132/339], [94mLoss[0m : 2.00474
[1mStep[0m  [165/339], [94mLoss[0m : 2.88251
[1mStep[0m  [198/339], [94mLoss[0m : 2.24042
[1mStep[0m  [231/339], [94mLoss[0m : 2.47917
[1mStep[0m  [264/339], [94mLoss[0m : 2.57893
[1mStep[0m  [297/339], [94mLoss[0m : 2.27659
[1mStep[0m  [330/339], [94mLoss[0m : 2.17616

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82420
[1mStep[0m  [33/339], [94mLoss[0m : 2.23325
[1mStep[0m  [66/339], [94mLoss[0m : 2.42100
[1mStep[0m  [99/339], [94mLoss[0m : 1.95157
[1mStep[0m  [132/339], [94mLoss[0m : 2.01372
[1mStep[0m  [165/339], [94mLoss[0m : 2.05728
[1mStep[0m  [198/339], [94mLoss[0m : 2.38514
[1mStep[0m  [231/339], [94mLoss[0m : 2.51018
[1mStep[0m  [264/339], [94mLoss[0m : 2.78757
[1mStep[0m  [297/339], [94mLoss[0m : 2.92261
[1mStep[0m  [330/339], [94mLoss[0m : 2.32130

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.539, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99508
[1mStep[0m  [33/339], [94mLoss[0m : 2.36837
[1mStep[0m  [66/339], [94mLoss[0m : 2.52404
[1mStep[0m  [99/339], [94mLoss[0m : 2.87397
[1mStep[0m  [132/339], [94mLoss[0m : 2.94901
[1mStep[0m  [165/339], [94mLoss[0m : 2.03433
[1mStep[0m  [198/339], [94mLoss[0m : 2.32797
[1mStep[0m  [231/339], [94mLoss[0m : 2.05671
[1mStep[0m  [264/339], [94mLoss[0m : 2.63894
[1mStep[0m  [297/339], [94mLoss[0m : 2.85999
[1mStep[0m  [330/339], [94mLoss[0m : 1.88168

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39821
[1mStep[0m  [33/339], [94mLoss[0m : 2.60940
[1mStep[0m  [66/339], [94mLoss[0m : 2.11273
[1mStep[0m  [99/339], [94mLoss[0m : 2.54244
[1mStep[0m  [132/339], [94mLoss[0m : 2.32586
[1mStep[0m  [165/339], [94mLoss[0m : 2.52655
[1mStep[0m  [198/339], [94mLoss[0m : 2.02196
[1mStep[0m  [231/339], [94mLoss[0m : 2.27218
[1mStep[0m  [264/339], [94mLoss[0m : 2.77484
[1mStep[0m  [297/339], [94mLoss[0m : 2.22024
[1mStep[0m  [330/339], [94mLoss[0m : 2.18369

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62457
[1mStep[0m  [33/339], [94mLoss[0m : 2.29382
[1mStep[0m  [66/339], [94mLoss[0m : 2.53724
[1mStep[0m  [99/339], [94mLoss[0m : 2.15076
[1mStep[0m  [132/339], [94mLoss[0m : 1.86615
[1mStep[0m  [165/339], [94mLoss[0m : 2.54423
[1mStep[0m  [198/339], [94mLoss[0m : 2.27956
[1mStep[0m  [231/339], [94mLoss[0m : 2.90790
[1mStep[0m  [264/339], [94mLoss[0m : 2.45066
[1mStep[0m  [297/339], [94mLoss[0m : 2.05505
[1mStep[0m  [330/339], [94mLoss[0m : 2.96866

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.32669
[1mStep[0m  [33/339], [94mLoss[0m : 1.90008
[1mStep[0m  [66/339], [94mLoss[0m : 1.81802
[1mStep[0m  [99/339], [94mLoss[0m : 2.35563
[1mStep[0m  [132/339], [94mLoss[0m : 2.75008
[1mStep[0m  [165/339], [94mLoss[0m : 1.97132
[1mStep[0m  [198/339], [94mLoss[0m : 2.41160
[1mStep[0m  [231/339], [94mLoss[0m : 2.52086
[1mStep[0m  [264/339], [94mLoss[0m : 2.89356
[1mStep[0m  [297/339], [94mLoss[0m : 2.17563
[1mStep[0m  [330/339], [94mLoss[0m : 2.68458

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.99329
[1mStep[0m  [33/339], [94mLoss[0m : 2.94791
[1mStep[0m  [66/339], [94mLoss[0m : 1.69068
[1mStep[0m  [99/339], [94mLoss[0m : 2.03815
[1mStep[0m  [132/339], [94mLoss[0m : 2.07312
[1mStep[0m  [165/339], [94mLoss[0m : 2.53974
[1mStep[0m  [198/339], [94mLoss[0m : 2.81903
[1mStep[0m  [231/339], [94mLoss[0m : 3.09624
[1mStep[0m  [264/339], [94mLoss[0m : 2.42972
[1mStep[0m  [297/339], [94mLoss[0m : 2.74378
[1mStep[0m  [330/339], [94mLoss[0m : 2.05617

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15699
[1mStep[0m  [33/339], [94mLoss[0m : 2.05449
[1mStep[0m  [66/339], [94mLoss[0m : 1.79160
[1mStep[0m  [99/339], [94mLoss[0m : 1.67138
[1mStep[0m  [132/339], [94mLoss[0m : 3.01973
[1mStep[0m  [165/339], [94mLoss[0m : 2.69667
[1mStep[0m  [198/339], [94mLoss[0m : 2.39262
[1mStep[0m  [231/339], [94mLoss[0m : 2.90319
[1mStep[0m  [264/339], [94mLoss[0m : 2.38547
[1mStep[0m  [297/339], [94mLoss[0m : 2.61921
[1mStep[0m  [330/339], [94mLoss[0m : 1.99811

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29402
[1mStep[0m  [33/339], [94mLoss[0m : 2.61198
[1mStep[0m  [66/339], [94mLoss[0m : 2.36876
[1mStep[0m  [99/339], [94mLoss[0m : 2.48803
[1mStep[0m  [132/339], [94mLoss[0m : 2.51873
[1mStep[0m  [165/339], [94mLoss[0m : 2.72063
[1mStep[0m  [198/339], [94mLoss[0m : 1.67988
[1mStep[0m  [231/339], [94mLoss[0m : 2.65091
[1mStep[0m  [264/339], [94mLoss[0m : 2.90966
[1mStep[0m  [297/339], [94mLoss[0m : 2.59808
[1mStep[0m  [330/339], [94mLoss[0m : 2.69941

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29536
[1mStep[0m  [33/339], [94mLoss[0m : 2.44289
[1mStep[0m  [66/339], [94mLoss[0m : 2.13734
[1mStep[0m  [99/339], [94mLoss[0m : 2.22289
[1mStep[0m  [132/339], [94mLoss[0m : 2.49504
[1mStep[0m  [165/339], [94mLoss[0m : 2.62969
[1mStep[0m  [198/339], [94mLoss[0m : 2.85319
[1mStep[0m  [231/339], [94mLoss[0m : 2.47961
[1mStep[0m  [264/339], [94mLoss[0m : 1.98298
[1mStep[0m  [297/339], [94mLoss[0m : 2.06658
[1mStep[0m  [330/339], [94mLoss[0m : 1.90841

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95403
[1mStep[0m  [33/339], [94mLoss[0m : 2.02164
[1mStep[0m  [66/339], [94mLoss[0m : 2.18559
[1mStep[0m  [99/339], [94mLoss[0m : 2.19183
[1mStep[0m  [132/339], [94mLoss[0m : 2.41583
[1mStep[0m  [165/339], [94mLoss[0m : 2.42146
[1mStep[0m  [198/339], [94mLoss[0m : 2.22132
[1mStep[0m  [231/339], [94mLoss[0m : 2.54132
[1mStep[0m  [264/339], [94mLoss[0m : 2.65259
[1mStep[0m  [297/339], [94mLoss[0m : 2.98304
[1mStep[0m  [330/339], [94mLoss[0m : 2.69369

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.319, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08281
[1mStep[0m  [33/339], [94mLoss[0m : 2.38237
[1mStep[0m  [66/339], [94mLoss[0m : 2.59121
[1mStep[0m  [99/339], [94mLoss[0m : 2.42417
[1mStep[0m  [132/339], [94mLoss[0m : 2.56236
[1mStep[0m  [165/339], [94mLoss[0m : 2.43101
[1mStep[0m  [198/339], [94mLoss[0m : 2.69123
[1mStep[0m  [231/339], [94mLoss[0m : 2.28346
[1mStep[0m  [264/339], [94mLoss[0m : 2.69620
[1mStep[0m  [297/339], [94mLoss[0m : 1.84346
[1mStep[0m  [330/339], [94mLoss[0m : 2.27592

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46172
[1mStep[0m  [33/339], [94mLoss[0m : 2.40850
[1mStep[0m  [66/339], [94mLoss[0m : 2.17859
[1mStep[0m  [99/339], [94mLoss[0m : 2.53391
[1mStep[0m  [132/339], [94mLoss[0m : 2.21166
[1mStep[0m  [165/339], [94mLoss[0m : 1.80991
[1mStep[0m  [198/339], [94mLoss[0m : 2.41338
[1mStep[0m  [231/339], [94mLoss[0m : 2.87560
[1mStep[0m  [264/339], [94mLoss[0m : 2.08467
[1mStep[0m  [297/339], [94mLoss[0m : 2.53609
[1mStep[0m  [330/339], [94mLoss[0m : 2.10950

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.293, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82210
[1mStep[0m  [33/339], [94mLoss[0m : 2.54205
[1mStep[0m  [66/339], [94mLoss[0m : 1.88022
[1mStep[0m  [99/339], [94mLoss[0m : 2.64235
[1mStep[0m  [132/339], [94mLoss[0m : 2.49004
[1mStep[0m  [165/339], [94mLoss[0m : 2.93985
[1mStep[0m  [198/339], [94mLoss[0m : 2.41131
[1mStep[0m  [231/339], [94mLoss[0m : 2.31438
[1mStep[0m  [264/339], [94mLoss[0m : 2.35224
[1mStep[0m  [297/339], [94mLoss[0m : 2.63108
[1mStep[0m  [330/339], [94mLoss[0m : 2.72468

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.312, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87314
[1mStep[0m  [33/339], [94mLoss[0m : 2.31740
[1mStep[0m  [66/339], [94mLoss[0m : 2.10700
[1mStep[0m  [99/339], [94mLoss[0m : 2.19919
[1mStep[0m  [132/339], [94mLoss[0m : 2.53205
[1mStep[0m  [165/339], [94mLoss[0m : 2.32814
[1mStep[0m  [198/339], [94mLoss[0m : 2.55918
[1mStep[0m  [231/339], [94mLoss[0m : 2.83228
[1mStep[0m  [264/339], [94mLoss[0m : 2.38715
[1mStep[0m  [297/339], [94mLoss[0m : 2.58413
[1mStep[0m  [330/339], [94mLoss[0m : 1.88348

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22051
[1mStep[0m  [33/339], [94mLoss[0m : 2.70792
[1mStep[0m  [66/339], [94mLoss[0m : 2.21135
[1mStep[0m  [99/339], [94mLoss[0m : 2.14356
[1mStep[0m  [132/339], [94mLoss[0m : 2.67782
[1mStep[0m  [165/339], [94mLoss[0m : 2.28126
[1mStep[0m  [198/339], [94mLoss[0m : 2.17698
[1mStep[0m  [231/339], [94mLoss[0m : 1.93535
[1mStep[0m  [264/339], [94mLoss[0m : 2.44879
[1mStep[0m  [297/339], [94mLoss[0m : 2.11734
[1mStep[0m  [330/339], [94mLoss[0m : 2.77322

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60662
[1mStep[0m  [33/339], [94mLoss[0m : 2.30260
[1mStep[0m  [66/339], [94mLoss[0m : 2.58091
[1mStep[0m  [99/339], [94mLoss[0m : 2.43712
[1mStep[0m  [132/339], [94mLoss[0m : 3.04985
[1mStep[0m  [165/339], [94mLoss[0m : 1.98704
[1mStep[0m  [198/339], [94mLoss[0m : 1.86643
[1mStep[0m  [231/339], [94mLoss[0m : 1.65113
[1mStep[0m  [264/339], [94mLoss[0m : 1.91109
[1mStep[0m  [297/339], [94mLoss[0m : 2.22206
[1mStep[0m  [330/339], [94mLoss[0m : 2.27048

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.323, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29622
[1mStep[0m  [33/339], [94mLoss[0m : 3.01359
[1mStep[0m  [66/339], [94mLoss[0m : 3.24059
[1mStep[0m  [99/339], [94mLoss[0m : 2.25310
[1mStep[0m  [132/339], [94mLoss[0m : 2.83239
[1mStep[0m  [165/339], [94mLoss[0m : 2.96474
[1mStep[0m  [198/339], [94mLoss[0m : 2.74076
[1mStep[0m  [231/339], [94mLoss[0m : 2.25513
[1mStep[0m  [264/339], [94mLoss[0m : 2.64303
[1mStep[0m  [297/339], [94mLoss[0m : 2.51978
[1mStep[0m  [330/339], [94mLoss[0m : 2.69397

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89964
[1mStep[0m  [33/339], [94mLoss[0m : 2.36820
[1mStep[0m  [66/339], [94mLoss[0m : 1.95818
[1mStep[0m  [99/339], [94mLoss[0m : 3.00078
[1mStep[0m  [132/339], [94mLoss[0m : 2.42416
[1mStep[0m  [165/339], [94mLoss[0m : 2.45085
[1mStep[0m  [198/339], [94mLoss[0m : 1.77945
[1mStep[0m  [231/339], [94mLoss[0m : 1.47788
[1mStep[0m  [264/339], [94mLoss[0m : 2.06124
[1mStep[0m  [297/339], [94mLoss[0m : 2.23556
[1mStep[0m  [330/339], [94mLoss[0m : 2.30392

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17668
[1mStep[0m  [33/339], [94mLoss[0m : 3.68622
[1mStep[0m  [66/339], [94mLoss[0m : 2.25450
[1mStep[0m  [99/339], [94mLoss[0m : 1.97084
[1mStep[0m  [132/339], [94mLoss[0m : 2.52341
[1mStep[0m  [165/339], [94mLoss[0m : 2.45937
[1mStep[0m  [198/339], [94mLoss[0m : 2.31996
[1mStep[0m  [231/339], [94mLoss[0m : 2.75395
[1mStep[0m  [264/339], [94mLoss[0m : 2.12818
[1mStep[0m  [297/339], [94mLoss[0m : 2.44870
[1mStep[0m  [330/339], [94mLoss[0m : 2.78996

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40574
[1mStep[0m  [33/339], [94mLoss[0m : 2.40487
[1mStep[0m  [66/339], [94mLoss[0m : 2.39866
[1mStep[0m  [99/339], [94mLoss[0m : 2.28138
[1mStep[0m  [132/339], [94mLoss[0m : 2.26316
[1mStep[0m  [165/339], [94mLoss[0m : 2.71651
[1mStep[0m  [198/339], [94mLoss[0m : 2.81430
[1mStep[0m  [231/339], [94mLoss[0m : 2.74853
[1mStep[0m  [264/339], [94mLoss[0m : 3.14365
[1mStep[0m  [297/339], [94mLoss[0m : 2.60528
[1mStep[0m  [330/339], [94mLoss[0m : 2.77093

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.74002
[1mStep[0m  [33/339], [94mLoss[0m : 1.73562
[1mStep[0m  [66/339], [94mLoss[0m : 2.73587
[1mStep[0m  [99/339], [94mLoss[0m : 2.41837
[1mStep[0m  [132/339], [94mLoss[0m : 2.42221
[1mStep[0m  [165/339], [94mLoss[0m : 1.94755
[1mStep[0m  [198/339], [94mLoss[0m : 2.27946
[1mStep[0m  [231/339], [94mLoss[0m : 2.50500
[1mStep[0m  [264/339], [94mLoss[0m : 2.31181
[1mStep[0m  [297/339], [94mLoss[0m : 2.54359
[1mStep[0m  [330/339], [94mLoss[0m : 2.64295

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.298, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50279
[1mStep[0m  [33/339], [94mLoss[0m : 2.89382
[1mStep[0m  [66/339], [94mLoss[0m : 1.74508
[1mStep[0m  [99/339], [94mLoss[0m : 2.17996
[1mStep[0m  [132/339], [94mLoss[0m : 2.11360
[1mStep[0m  [165/339], [94mLoss[0m : 2.34707
[1mStep[0m  [198/339], [94mLoss[0m : 2.70408
[1mStep[0m  [231/339], [94mLoss[0m : 2.10540
[1mStep[0m  [264/339], [94mLoss[0m : 2.86571
[1mStep[0m  [297/339], [94mLoss[0m : 2.89644
[1mStep[0m  [330/339], [94mLoss[0m : 2.07451

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.74998
[1mStep[0m  [33/339], [94mLoss[0m : 2.44750
[1mStep[0m  [66/339], [94mLoss[0m : 2.83042
[1mStep[0m  [99/339], [94mLoss[0m : 2.08738
[1mStep[0m  [132/339], [94mLoss[0m : 1.59310
[1mStep[0m  [165/339], [94mLoss[0m : 2.08724
[1mStep[0m  [198/339], [94mLoss[0m : 2.61080
[1mStep[0m  [231/339], [94mLoss[0m : 2.43029
[1mStep[0m  [264/339], [94mLoss[0m : 2.32064
[1mStep[0m  [297/339], [94mLoss[0m : 2.08438
[1mStep[0m  [330/339], [94mLoss[0m : 2.15575

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.358, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.90430
[1mStep[0m  [33/339], [94mLoss[0m : 2.01093
[1mStep[0m  [66/339], [94mLoss[0m : 3.37152
[1mStep[0m  [99/339], [94mLoss[0m : 1.87552
[1mStep[0m  [132/339], [94mLoss[0m : 3.15250
[1mStep[0m  [165/339], [94mLoss[0m : 2.09293
[1mStep[0m  [198/339], [94mLoss[0m : 2.37338
[1mStep[0m  [231/339], [94mLoss[0m : 2.57002
[1mStep[0m  [264/339], [94mLoss[0m : 2.14889
[1mStep[0m  [297/339], [94mLoss[0m : 2.22685
[1mStep[0m  [330/339], [94mLoss[0m : 2.47004

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.302, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.94050
[1mStep[0m  [33/339], [94mLoss[0m : 2.31533
[1mStep[0m  [66/339], [94mLoss[0m : 2.69080
[1mStep[0m  [99/339], [94mLoss[0m : 2.06151
[1mStep[0m  [132/339], [94mLoss[0m : 2.25054
[1mStep[0m  [165/339], [94mLoss[0m : 2.49103
[1mStep[0m  [198/339], [94mLoss[0m : 2.75886
[1mStep[0m  [231/339], [94mLoss[0m : 3.13060
[1mStep[0m  [264/339], [94mLoss[0m : 2.63005
[1mStep[0m  [297/339], [94mLoss[0m : 2.32016
[1mStep[0m  [330/339], [94mLoss[0m : 2.46768

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24331
[1mStep[0m  [33/339], [94mLoss[0m : 1.68186
[1mStep[0m  [66/339], [94mLoss[0m : 2.57904
[1mStep[0m  [99/339], [94mLoss[0m : 2.10643
[1mStep[0m  [132/339], [94mLoss[0m : 2.07896
[1mStep[0m  [165/339], [94mLoss[0m : 2.01557
[1mStep[0m  [198/339], [94mLoss[0m : 2.35257
[1mStep[0m  [231/339], [94mLoss[0m : 2.19175
[1mStep[0m  [264/339], [94mLoss[0m : 2.37102
[1mStep[0m  [297/339], [94mLoss[0m : 2.41714
[1mStep[0m  [330/339], [94mLoss[0m : 2.71583

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28975
[1mStep[0m  [33/339], [94mLoss[0m : 2.46881
[1mStep[0m  [66/339], [94mLoss[0m : 2.49514
[1mStep[0m  [99/339], [94mLoss[0m : 2.22754
[1mStep[0m  [132/339], [94mLoss[0m : 2.40799
[1mStep[0m  [165/339], [94mLoss[0m : 2.32709
[1mStep[0m  [198/339], [94mLoss[0m : 2.57249
[1mStep[0m  [231/339], [94mLoss[0m : 2.48856
[1mStep[0m  [264/339], [94mLoss[0m : 2.51329
[1mStep[0m  [297/339], [94mLoss[0m : 2.26207
[1mStep[0m  [330/339], [94mLoss[0m : 2.30676

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33517
[1mStep[0m  [33/339], [94mLoss[0m : 2.35780
[1mStep[0m  [66/339], [94mLoss[0m : 2.39735
[1mStep[0m  [99/339], [94mLoss[0m : 2.61277
[1mStep[0m  [132/339], [94mLoss[0m : 2.15390
[1mStep[0m  [165/339], [94mLoss[0m : 2.85272
[1mStep[0m  [198/339], [94mLoss[0m : 2.03419
[1mStep[0m  [231/339], [94mLoss[0m : 2.03908
[1mStep[0m  [264/339], [94mLoss[0m : 2.34286
[1mStep[0m  [297/339], [94mLoss[0m : 2.60437
[1mStep[0m  [330/339], [94mLoss[0m : 2.12551

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.308
====================================

Phase 1 - Evaluation MAE:  2.308047590002549
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.04249
[1mStep[0m  [33/339], [94mLoss[0m : 1.99953
[1mStep[0m  [66/339], [94mLoss[0m : 2.65442
[1mStep[0m  [99/339], [94mLoss[0m : 3.59911
[1mStep[0m  [132/339], [94mLoss[0m : 2.22917
[1mStep[0m  [165/339], [94mLoss[0m : 2.75915
[1mStep[0m  [198/339], [94mLoss[0m : 2.92989
[1mStep[0m  [231/339], [94mLoss[0m : 2.72791
[1mStep[0m  [264/339], [94mLoss[0m : 2.42024
[1mStep[0m  [297/339], [94mLoss[0m : 2.30037
[1mStep[0m  [330/339], [94mLoss[0m : 2.76546

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.310, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00880
[1mStep[0m  [33/339], [94mLoss[0m : 2.49031
[1mStep[0m  [66/339], [94mLoss[0m : 2.63037
[1mStep[0m  [99/339], [94mLoss[0m : 3.12066
[1mStep[0m  [132/339], [94mLoss[0m : 2.54626
[1mStep[0m  [165/339], [94mLoss[0m : 2.21920
[1mStep[0m  [198/339], [94mLoss[0m : 2.18811
[1mStep[0m  [231/339], [94mLoss[0m : 1.72880
[1mStep[0m  [264/339], [94mLoss[0m : 2.89687
[1mStep[0m  [297/339], [94mLoss[0m : 2.62770
[1mStep[0m  [330/339], [94mLoss[0m : 2.85869

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45893
[1mStep[0m  [33/339], [94mLoss[0m : 2.45622
[1mStep[0m  [66/339], [94mLoss[0m : 2.31078
[1mStep[0m  [99/339], [94mLoss[0m : 2.47603
[1mStep[0m  [132/339], [94mLoss[0m : 2.40145
[1mStep[0m  [165/339], [94mLoss[0m : 1.74314
[1mStep[0m  [198/339], [94mLoss[0m : 2.03268
[1mStep[0m  [231/339], [94mLoss[0m : 2.17669
[1mStep[0m  [264/339], [94mLoss[0m : 2.39840
[1mStep[0m  [297/339], [94mLoss[0m : 1.99023
[1mStep[0m  [330/339], [94mLoss[0m : 2.31341

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27655
[1mStep[0m  [33/339], [94mLoss[0m : 2.33762
[1mStep[0m  [66/339], [94mLoss[0m : 2.57635
[1mStep[0m  [99/339], [94mLoss[0m : 2.08134
[1mStep[0m  [132/339], [94mLoss[0m : 1.87803
[1mStep[0m  [165/339], [94mLoss[0m : 2.97493
[1mStep[0m  [198/339], [94mLoss[0m : 2.93904
[1mStep[0m  [231/339], [94mLoss[0m : 2.18254
[1mStep[0m  [264/339], [94mLoss[0m : 1.78819
[1mStep[0m  [297/339], [94mLoss[0m : 2.61055
[1mStep[0m  [330/339], [94mLoss[0m : 2.91543

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37184
[1mStep[0m  [33/339], [94mLoss[0m : 2.25095
[1mStep[0m  [66/339], [94mLoss[0m : 2.06146
[1mStep[0m  [99/339], [94mLoss[0m : 2.03116
[1mStep[0m  [132/339], [94mLoss[0m : 1.89561
[1mStep[0m  [165/339], [94mLoss[0m : 2.27975
[1mStep[0m  [198/339], [94mLoss[0m : 2.45239
[1mStep[0m  [231/339], [94mLoss[0m : 1.74408
[1mStep[0m  [264/339], [94mLoss[0m : 2.44852
[1mStep[0m  [297/339], [94mLoss[0m : 1.94837
[1mStep[0m  [330/339], [94mLoss[0m : 2.28605

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73555
[1mStep[0m  [33/339], [94mLoss[0m : 2.21741
[1mStep[0m  [66/339], [94mLoss[0m : 1.98282
[1mStep[0m  [99/339], [94mLoss[0m : 1.37570
[1mStep[0m  [132/339], [94mLoss[0m : 1.87676
[1mStep[0m  [165/339], [94mLoss[0m : 1.49011
[1mStep[0m  [198/339], [94mLoss[0m : 1.80872
[1mStep[0m  [231/339], [94mLoss[0m : 2.35290
[1mStep[0m  [264/339], [94mLoss[0m : 2.55078
[1mStep[0m  [297/339], [94mLoss[0m : 2.30955
[1mStep[0m  [330/339], [94mLoss[0m : 2.49899

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62729
[1mStep[0m  [33/339], [94mLoss[0m : 1.48814
[1mStep[0m  [66/339], [94mLoss[0m : 2.45006
[1mStep[0m  [99/339], [94mLoss[0m : 2.32509
[1mStep[0m  [132/339], [94mLoss[0m : 2.49168
[1mStep[0m  [165/339], [94mLoss[0m : 1.73582
[1mStep[0m  [198/339], [94mLoss[0m : 3.03544
[1mStep[0m  [231/339], [94mLoss[0m : 2.08202
[1mStep[0m  [264/339], [94mLoss[0m : 2.70326
[1mStep[0m  [297/339], [94mLoss[0m : 2.02155
[1mStep[0m  [330/339], [94mLoss[0m : 2.08390

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26815
[1mStep[0m  [33/339], [94mLoss[0m : 2.04610
[1mStep[0m  [66/339], [94mLoss[0m : 1.89249
[1mStep[0m  [99/339], [94mLoss[0m : 2.70248
[1mStep[0m  [132/339], [94mLoss[0m : 2.10993
[1mStep[0m  [165/339], [94mLoss[0m : 1.71381
[1mStep[0m  [198/339], [94mLoss[0m : 1.99441
[1mStep[0m  [231/339], [94mLoss[0m : 2.44579
[1mStep[0m  [264/339], [94mLoss[0m : 1.36967
[1mStep[0m  [297/339], [94mLoss[0m : 1.95880
[1mStep[0m  [330/339], [94mLoss[0m : 2.75805

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83767
[1mStep[0m  [33/339], [94mLoss[0m : 1.28451
[1mStep[0m  [66/339], [94mLoss[0m : 2.14913
[1mStep[0m  [99/339], [94mLoss[0m : 2.49291
[1mStep[0m  [132/339], [94mLoss[0m : 2.80790
[1mStep[0m  [165/339], [94mLoss[0m : 2.23896
[1mStep[0m  [198/339], [94mLoss[0m : 2.36062
[1mStep[0m  [231/339], [94mLoss[0m : 2.78862
[1mStep[0m  [264/339], [94mLoss[0m : 2.73035
[1mStep[0m  [297/339], [94mLoss[0m : 1.90554
[1mStep[0m  [330/339], [94mLoss[0m : 1.76046

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62369
[1mStep[0m  [33/339], [94mLoss[0m : 1.71967
[1mStep[0m  [66/339], [94mLoss[0m : 2.25868
[1mStep[0m  [99/339], [94mLoss[0m : 1.66104
[1mStep[0m  [132/339], [94mLoss[0m : 2.09204
[1mStep[0m  [165/339], [94mLoss[0m : 2.02687
[1mStep[0m  [198/339], [94mLoss[0m : 2.53562
[1mStep[0m  [231/339], [94mLoss[0m : 2.21665
[1mStep[0m  [264/339], [94mLoss[0m : 1.98060
[1mStep[0m  [297/339], [94mLoss[0m : 2.02233
[1mStep[0m  [330/339], [94mLoss[0m : 2.13176

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92932
[1mStep[0m  [33/339], [94mLoss[0m : 2.15877
[1mStep[0m  [66/339], [94mLoss[0m : 1.92917
[1mStep[0m  [99/339], [94mLoss[0m : 1.42480
[1mStep[0m  [132/339], [94mLoss[0m : 2.50714
[1mStep[0m  [165/339], [94mLoss[0m : 2.00819
[1mStep[0m  [198/339], [94mLoss[0m : 2.38731
[1mStep[0m  [231/339], [94mLoss[0m : 1.83639
[1mStep[0m  [264/339], [94mLoss[0m : 2.55467
[1mStep[0m  [297/339], [94mLoss[0m : 2.19331
[1mStep[0m  [330/339], [94mLoss[0m : 1.86251

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62263
[1mStep[0m  [33/339], [94mLoss[0m : 2.30354
[1mStep[0m  [66/339], [94mLoss[0m : 1.98871
[1mStep[0m  [99/339], [94mLoss[0m : 2.54005
[1mStep[0m  [132/339], [94mLoss[0m : 2.08547
[1mStep[0m  [165/339], [94mLoss[0m : 2.22400
[1mStep[0m  [198/339], [94mLoss[0m : 2.08143
[1mStep[0m  [231/339], [94mLoss[0m : 1.69600
[1mStep[0m  [264/339], [94mLoss[0m : 2.18807
[1mStep[0m  [297/339], [94mLoss[0m : 2.20907
[1mStep[0m  [330/339], [94mLoss[0m : 2.93837

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85757
[1mStep[0m  [33/339], [94mLoss[0m : 2.34988
[1mStep[0m  [66/339], [94mLoss[0m : 1.88130
[1mStep[0m  [99/339], [94mLoss[0m : 2.52075
[1mStep[0m  [132/339], [94mLoss[0m : 1.90298
[1mStep[0m  [165/339], [94mLoss[0m : 1.54455
[1mStep[0m  [198/339], [94mLoss[0m : 1.99480
[1mStep[0m  [231/339], [94mLoss[0m : 2.36178
[1mStep[0m  [264/339], [94mLoss[0m : 2.47528
[1mStep[0m  [297/339], [94mLoss[0m : 2.40441
[1mStep[0m  [330/339], [94mLoss[0m : 2.46965

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18435
[1mStep[0m  [33/339], [94mLoss[0m : 2.08945
[1mStep[0m  [66/339], [94mLoss[0m : 2.10868
[1mStep[0m  [99/339], [94mLoss[0m : 2.60901
[1mStep[0m  [132/339], [94mLoss[0m : 1.64784
[1mStep[0m  [165/339], [94mLoss[0m : 1.54848
[1mStep[0m  [198/339], [94mLoss[0m : 1.92974
[1mStep[0m  [231/339], [94mLoss[0m : 2.69891
[1mStep[0m  [264/339], [94mLoss[0m : 1.91641
[1mStep[0m  [297/339], [94mLoss[0m : 2.21028
[1mStep[0m  [330/339], [94mLoss[0m : 2.20015

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67560
[1mStep[0m  [33/339], [94mLoss[0m : 1.87890
[1mStep[0m  [66/339], [94mLoss[0m : 2.49228
[1mStep[0m  [99/339], [94mLoss[0m : 2.67083
[1mStep[0m  [132/339], [94mLoss[0m : 2.99344
[1mStep[0m  [165/339], [94mLoss[0m : 1.71630
[1mStep[0m  [198/339], [94mLoss[0m : 2.44026
[1mStep[0m  [231/339], [94mLoss[0m : 1.84910
[1mStep[0m  [264/339], [94mLoss[0m : 2.02861
[1mStep[0m  [297/339], [94mLoss[0m : 2.13480
[1mStep[0m  [330/339], [94mLoss[0m : 2.29447

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.082, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79330
[1mStep[0m  [33/339], [94mLoss[0m : 2.01942
[1mStep[0m  [66/339], [94mLoss[0m : 2.90240
[1mStep[0m  [99/339], [94mLoss[0m : 1.97316
[1mStep[0m  [132/339], [94mLoss[0m : 1.85504
[1mStep[0m  [165/339], [94mLoss[0m : 2.36257
[1mStep[0m  [198/339], [94mLoss[0m : 2.13114
[1mStep[0m  [231/339], [94mLoss[0m : 2.47471
[1mStep[0m  [264/339], [94mLoss[0m : 1.85588
[1mStep[0m  [297/339], [94mLoss[0m : 2.37274
[1mStep[0m  [330/339], [94mLoss[0m : 2.31474

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40201
[1mStep[0m  [33/339], [94mLoss[0m : 2.20370
[1mStep[0m  [66/339], [94mLoss[0m : 2.12050
[1mStep[0m  [99/339], [94mLoss[0m : 1.81751
[1mStep[0m  [132/339], [94mLoss[0m : 2.15915
[1mStep[0m  [165/339], [94mLoss[0m : 2.26570
[1mStep[0m  [198/339], [94mLoss[0m : 1.86793
[1mStep[0m  [231/339], [94mLoss[0m : 2.42780
[1mStep[0m  [264/339], [94mLoss[0m : 2.12932
[1mStep[0m  [297/339], [94mLoss[0m : 2.30961
[1mStep[0m  [330/339], [94mLoss[0m : 2.05064

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04329
[1mStep[0m  [33/339], [94mLoss[0m : 2.12009
[1mStep[0m  [66/339], [94mLoss[0m : 2.88394
[1mStep[0m  [99/339], [94mLoss[0m : 1.81355
[1mStep[0m  [132/339], [94mLoss[0m : 1.69794
[1mStep[0m  [165/339], [94mLoss[0m : 1.91525
[1mStep[0m  [198/339], [94mLoss[0m : 1.83773
[1mStep[0m  [231/339], [94mLoss[0m : 2.06252
[1mStep[0m  [264/339], [94mLoss[0m : 2.13849
[1mStep[0m  [297/339], [94mLoss[0m : 2.02390
[1mStep[0m  [330/339], [94mLoss[0m : 2.15252

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75085
[1mStep[0m  [33/339], [94mLoss[0m : 1.68618
[1mStep[0m  [66/339], [94mLoss[0m : 2.30324
[1mStep[0m  [99/339], [94mLoss[0m : 1.85757
[1mStep[0m  [132/339], [94mLoss[0m : 2.30676
[1mStep[0m  [165/339], [94mLoss[0m : 2.09792
[1mStep[0m  [198/339], [94mLoss[0m : 1.57431
[1mStep[0m  [231/339], [94mLoss[0m : 2.53102
[1mStep[0m  [264/339], [94mLoss[0m : 2.23743
[1mStep[0m  [297/339], [94mLoss[0m : 2.00294
[1mStep[0m  [330/339], [94mLoss[0m : 1.97355

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55656
[1mStep[0m  [33/339], [94mLoss[0m : 1.77885
[1mStep[0m  [66/339], [94mLoss[0m : 1.74330
[1mStep[0m  [99/339], [94mLoss[0m : 2.17840
[1mStep[0m  [132/339], [94mLoss[0m : 2.20378
[1mStep[0m  [165/339], [94mLoss[0m : 1.39721
[1mStep[0m  [198/339], [94mLoss[0m : 2.05046
[1mStep[0m  [231/339], [94mLoss[0m : 1.97218
[1mStep[0m  [264/339], [94mLoss[0m : 1.91649
[1mStep[0m  [297/339], [94mLoss[0m : 3.42552
[1mStep[0m  [330/339], [94mLoss[0m : 2.13070

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87297
[1mStep[0m  [33/339], [94mLoss[0m : 1.84346
[1mStep[0m  [66/339], [94mLoss[0m : 2.08036
[1mStep[0m  [99/339], [94mLoss[0m : 2.17850
[1mStep[0m  [132/339], [94mLoss[0m : 3.07111
[1mStep[0m  [165/339], [94mLoss[0m : 2.13476
[1mStep[0m  [198/339], [94mLoss[0m : 2.10009
[1mStep[0m  [231/339], [94mLoss[0m : 2.26947
[1mStep[0m  [264/339], [94mLoss[0m : 2.18331
[1mStep[0m  [297/339], [94mLoss[0m : 1.92033
[1mStep[0m  [330/339], [94mLoss[0m : 1.80142

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94897
[1mStep[0m  [33/339], [94mLoss[0m : 2.84843
[1mStep[0m  [66/339], [94mLoss[0m : 2.23771
[1mStep[0m  [99/339], [94mLoss[0m : 1.67529
[1mStep[0m  [132/339], [94mLoss[0m : 1.69092
[1mStep[0m  [165/339], [94mLoss[0m : 1.78126
[1mStep[0m  [198/339], [94mLoss[0m : 2.10919
[1mStep[0m  [231/339], [94mLoss[0m : 2.21828
[1mStep[0m  [264/339], [94mLoss[0m : 2.38244
[1mStep[0m  [297/339], [94mLoss[0m : 1.84337
[1mStep[0m  [330/339], [94mLoss[0m : 2.02748

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.434, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16305
[1mStep[0m  [33/339], [94mLoss[0m : 2.51932
[1mStep[0m  [66/339], [94mLoss[0m : 1.95228
[1mStep[0m  [99/339], [94mLoss[0m : 1.77453
[1mStep[0m  [132/339], [94mLoss[0m : 1.82322
[1mStep[0m  [165/339], [94mLoss[0m : 1.83413
[1mStep[0m  [198/339], [94mLoss[0m : 1.96929
[1mStep[0m  [231/339], [94mLoss[0m : 1.56418
[1mStep[0m  [264/339], [94mLoss[0m : 2.44827
[1mStep[0m  [297/339], [94mLoss[0m : 1.79544
[1mStep[0m  [330/339], [94mLoss[0m : 2.24897

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78939
[1mStep[0m  [33/339], [94mLoss[0m : 1.89333
[1mStep[0m  [66/339], [94mLoss[0m : 1.75051
[1mStep[0m  [99/339], [94mLoss[0m : 2.26753
[1mStep[0m  [132/339], [94mLoss[0m : 1.61635
[1mStep[0m  [165/339], [94mLoss[0m : 1.66232
[1mStep[0m  [198/339], [94mLoss[0m : 1.82309
[1mStep[0m  [231/339], [94mLoss[0m : 2.25348
[1mStep[0m  [264/339], [94mLoss[0m : 2.42075
[1mStep[0m  [297/339], [94mLoss[0m : 2.04572
[1mStep[0m  [330/339], [94mLoss[0m : 1.85341

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28318
[1mStep[0m  [33/339], [94mLoss[0m : 2.08657
[1mStep[0m  [66/339], [94mLoss[0m : 2.43593
[1mStep[0m  [99/339], [94mLoss[0m : 2.04806
[1mStep[0m  [132/339], [94mLoss[0m : 1.98970
[1mStep[0m  [165/339], [94mLoss[0m : 1.97306
[1mStep[0m  [198/339], [94mLoss[0m : 2.10914
[1mStep[0m  [231/339], [94mLoss[0m : 2.50888
[1mStep[0m  [264/339], [94mLoss[0m : 2.59370
[1mStep[0m  [297/339], [94mLoss[0m : 1.84316
[1mStep[0m  [330/339], [94mLoss[0m : 3.46729

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.466, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66319
[1mStep[0m  [33/339], [94mLoss[0m : 1.44672
[1mStep[0m  [66/339], [94mLoss[0m : 1.75417
[1mStep[0m  [99/339], [94mLoss[0m : 1.72704
[1mStep[0m  [132/339], [94mLoss[0m : 1.83178
[1mStep[0m  [165/339], [94mLoss[0m : 1.56048
[1mStep[0m  [198/339], [94mLoss[0m : 2.05037
[1mStep[0m  [231/339], [94mLoss[0m : 1.98024
[1mStep[0m  [264/339], [94mLoss[0m : 1.90492
[1mStep[0m  [297/339], [94mLoss[0m : 1.75881
[1mStep[0m  [330/339], [94mLoss[0m : 1.94172

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16148
[1mStep[0m  [33/339], [94mLoss[0m : 2.03167
[1mStep[0m  [66/339], [94mLoss[0m : 1.89118
[1mStep[0m  [99/339], [94mLoss[0m : 1.97042
[1mStep[0m  [132/339], [94mLoss[0m : 2.30557
[1mStep[0m  [165/339], [94mLoss[0m : 1.84654
[1mStep[0m  [198/339], [94mLoss[0m : 2.44384
[1mStep[0m  [231/339], [94mLoss[0m : 1.66912
[1mStep[0m  [264/339], [94mLoss[0m : 1.55718
[1mStep[0m  [297/339], [94mLoss[0m : 1.97445
[1mStep[0m  [330/339], [94mLoss[0m : 2.02083

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.513, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08705
[1mStep[0m  [33/339], [94mLoss[0m : 2.03499
[1mStep[0m  [66/339], [94mLoss[0m : 2.43258
[1mStep[0m  [99/339], [94mLoss[0m : 1.97547
[1mStep[0m  [132/339], [94mLoss[0m : 1.81904
[1mStep[0m  [165/339], [94mLoss[0m : 1.99612
[1mStep[0m  [198/339], [94mLoss[0m : 2.34359
[1mStep[0m  [231/339], [94mLoss[0m : 2.03244
[1mStep[0m  [264/339], [94mLoss[0m : 2.19057
[1mStep[0m  [297/339], [94mLoss[0m : 1.70287
[1mStep[0m  [330/339], [94mLoss[0m : 2.08195

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.46913
[1mStep[0m  [33/339], [94mLoss[0m : 2.01210
[1mStep[0m  [66/339], [94mLoss[0m : 1.78070
[1mStep[0m  [99/339], [94mLoss[0m : 1.46615
[1mStep[0m  [132/339], [94mLoss[0m : 2.40815
[1mStep[0m  [165/339], [94mLoss[0m : 2.17136
[1mStep[0m  [198/339], [94mLoss[0m : 2.07942
[1mStep[0m  [231/339], [94mLoss[0m : 1.99661
[1mStep[0m  [264/339], [94mLoss[0m : 1.38204
[1mStep[0m  [297/339], [94mLoss[0m : 2.19809
[1mStep[0m  [330/339], [94mLoss[0m : 2.10034

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07584
[1mStep[0m  [33/339], [94mLoss[0m : 1.58980
[1mStep[0m  [66/339], [94mLoss[0m : 1.83702
[1mStep[0m  [99/339], [94mLoss[0m : 1.96207
[1mStep[0m  [132/339], [94mLoss[0m : 2.18581
[1mStep[0m  [165/339], [94mLoss[0m : 1.49864
[1mStep[0m  [198/339], [94mLoss[0m : 1.63864
[1mStep[0m  [231/339], [94mLoss[0m : 2.29455
[1mStep[0m  [264/339], [94mLoss[0m : 1.81068
[1mStep[0m  [297/339], [94mLoss[0m : 2.00296
[1mStep[0m  [330/339], [94mLoss[0m : 1.85954

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.965, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.464
====================================

Phase 2 - Evaluation MAE:  2.4640291275176325
MAE score P1      2.308048
MAE score P2      2.464029
loss              1.953662
learning_rate     0.002575
batch_size              32
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 32, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 32, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.17653
[1mStep[0m  [16/169], [94mLoss[0m : 11.29849
[1mStep[0m  [32/169], [94mLoss[0m : 11.87415
[1mStep[0m  [48/169], [94mLoss[0m : 10.70168
[1mStep[0m  [64/169], [94mLoss[0m : 10.45509
[1mStep[0m  [80/169], [94mLoss[0m : 11.39393
[1mStep[0m  [96/169], [94mLoss[0m : 10.18695
[1mStep[0m  [112/169], [94mLoss[0m : 10.18301
[1mStep[0m  [128/169], [94mLoss[0m : 9.74490
[1mStep[0m  [144/169], [94mLoss[0m : 10.09053
[1mStep[0m  [160/169], [94mLoss[0m : 10.12788

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.590, [92mTest[0m: 10.880, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.59781
[1mStep[0m  [16/169], [94mLoss[0m : 9.85656
[1mStep[0m  [32/169], [94mLoss[0m : 10.38105
[1mStep[0m  [48/169], [94mLoss[0m : 9.41089
[1mStep[0m  [64/169], [94mLoss[0m : 9.66442
[1mStep[0m  [80/169], [94mLoss[0m : 9.50379
[1mStep[0m  [96/169], [94mLoss[0m : 9.40899
[1mStep[0m  [112/169], [94mLoss[0m : 9.30062
[1mStep[0m  [128/169], [94mLoss[0m : 9.60694
[1mStep[0m  [144/169], [94mLoss[0m : 8.87718
[1mStep[0m  [160/169], [94mLoss[0m : 9.56543

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.727, [92mTest[0m: 9.931, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.59349
[1mStep[0m  [16/169], [94mLoss[0m : 9.63631
[1mStep[0m  [32/169], [94mLoss[0m : 9.07656
[1mStep[0m  [48/169], [94mLoss[0m : 8.47993
[1mStep[0m  [64/169], [94mLoss[0m : 8.90160
[1mStep[0m  [80/169], [94mLoss[0m : 8.25649
[1mStep[0m  [96/169], [94mLoss[0m : 8.85677
[1mStep[0m  [112/169], [94mLoss[0m : 8.74737
[1mStep[0m  [128/169], [94mLoss[0m : 8.20221
[1mStep[0m  [144/169], [94mLoss[0m : 7.76709
[1mStep[0m  [160/169], [94mLoss[0m : 8.22669

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.629, [92mTest[0m: 8.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.41948
[1mStep[0m  [16/169], [94mLoss[0m : 7.69317
[1mStep[0m  [32/169], [94mLoss[0m : 8.84081
[1mStep[0m  [48/169], [94mLoss[0m : 7.45431
[1mStep[0m  [64/169], [94mLoss[0m : 7.82398
[1mStep[0m  [80/169], [94mLoss[0m : 7.60282
[1mStep[0m  [96/169], [94mLoss[0m : 7.22911
[1mStep[0m  [112/169], [94mLoss[0m : 7.59620
[1mStep[0m  [128/169], [94mLoss[0m : 6.40348
[1mStep[0m  [144/169], [94mLoss[0m : 6.83042
[1mStep[0m  [160/169], [94mLoss[0m : 7.18756

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.504, [92mTest[0m: 6.925, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.69868
[1mStep[0m  [16/169], [94mLoss[0m : 6.78616
[1mStep[0m  [32/169], [94mLoss[0m : 7.09228
[1mStep[0m  [48/169], [94mLoss[0m : 6.86080
[1mStep[0m  [64/169], [94mLoss[0m : 6.01459
[1mStep[0m  [80/169], [94mLoss[0m : 6.36220
[1mStep[0m  [96/169], [94mLoss[0m : 6.10653
[1mStep[0m  [112/169], [94mLoss[0m : 6.42666
[1mStep[0m  [128/169], [94mLoss[0m : 5.84543
[1mStep[0m  [144/169], [94mLoss[0m : 5.70619
[1mStep[0m  [160/169], [94mLoss[0m : 6.16037

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.557, [92mTest[0m: 5.667, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.71035
[1mStep[0m  [16/169], [94mLoss[0m : 5.97584
[1mStep[0m  [32/169], [94mLoss[0m : 5.86075
[1mStep[0m  [48/169], [94mLoss[0m : 5.31017
[1mStep[0m  [64/169], [94mLoss[0m : 6.01490
[1mStep[0m  [80/169], [94mLoss[0m : 4.96859
[1mStep[0m  [96/169], [94mLoss[0m : 5.49575
[1mStep[0m  [112/169], [94mLoss[0m : 5.23398
[1mStep[0m  [128/169], [94mLoss[0m : 5.56384
[1mStep[0m  [144/169], [94mLoss[0m : 5.23491
[1mStep[0m  [160/169], [94mLoss[0m : 5.56865

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.642, [92mTest[0m: 5.044, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.38105
[1mStep[0m  [16/169], [94mLoss[0m : 5.28056
[1mStep[0m  [32/169], [94mLoss[0m : 4.24150
[1mStep[0m  [48/169], [94mLoss[0m : 4.77715
[1mStep[0m  [64/169], [94mLoss[0m : 4.17961
[1mStep[0m  [80/169], [94mLoss[0m : 4.10173
[1mStep[0m  [96/169], [94mLoss[0m : 4.54415
[1mStep[0m  [112/169], [94mLoss[0m : 4.69753
[1mStep[0m  [128/169], [94mLoss[0m : 4.70505
[1mStep[0m  [144/169], [94mLoss[0m : 4.98175
[1mStep[0m  [160/169], [94mLoss[0m : 3.99973

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.548, [92mTest[0m: 4.254, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.42625
[1mStep[0m  [16/169], [94mLoss[0m : 3.96262
[1mStep[0m  [32/169], [94mLoss[0m : 3.69583
[1mStep[0m  [48/169], [94mLoss[0m : 3.96244
[1mStep[0m  [64/169], [94mLoss[0m : 3.54229
[1mStep[0m  [80/169], [94mLoss[0m : 3.61407
[1mStep[0m  [96/169], [94mLoss[0m : 3.87114
[1mStep[0m  [112/169], [94mLoss[0m : 3.34573
[1mStep[0m  [128/169], [94mLoss[0m : 2.65166
[1mStep[0m  [144/169], [94mLoss[0m : 3.48516
[1mStep[0m  [160/169], [94mLoss[0m : 3.12431

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.459, [92mTest[0m: 3.255, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41289
[1mStep[0m  [16/169], [94mLoss[0m : 3.41622
[1mStep[0m  [32/169], [94mLoss[0m : 3.19715
[1mStep[0m  [48/169], [94mLoss[0m : 2.56725
[1mStep[0m  [64/169], [94mLoss[0m : 3.13223
[1mStep[0m  [80/169], [94mLoss[0m : 3.25828
[1mStep[0m  [96/169], [94mLoss[0m : 2.83386
[1mStep[0m  [112/169], [94mLoss[0m : 2.62507
[1mStep[0m  [128/169], [94mLoss[0m : 3.01698
[1mStep[0m  [144/169], [94mLoss[0m : 2.93441
[1mStep[0m  [160/169], [94mLoss[0m : 2.53475

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.865, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.02787
[1mStep[0m  [16/169], [94mLoss[0m : 2.88504
[1mStep[0m  [32/169], [94mLoss[0m : 2.66578
[1mStep[0m  [48/169], [94mLoss[0m : 2.62449
[1mStep[0m  [64/169], [94mLoss[0m : 3.32245
[1mStep[0m  [80/169], [94mLoss[0m : 3.05892
[1mStep[0m  [96/169], [94mLoss[0m : 2.70061
[1mStep[0m  [112/169], [94mLoss[0m : 2.37659
[1mStep[0m  [128/169], [94mLoss[0m : 3.26523
[1mStep[0m  [144/169], [94mLoss[0m : 2.77688
[1mStep[0m  [160/169], [94mLoss[0m : 2.74147

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.12961
[1mStep[0m  [16/169], [94mLoss[0m : 2.39951
[1mStep[0m  [32/169], [94mLoss[0m : 3.09056
[1mStep[0m  [48/169], [94mLoss[0m : 2.57579
[1mStep[0m  [64/169], [94mLoss[0m : 2.30787
[1mStep[0m  [80/169], [94mLoss[0m : 2.64626
[1mStep[0m  [96/169], [94mLoss[0m : 2.40965
[1mStep[0m  [112/169], [94mLoss[0m : 2.20421
[1mStep[0m  [128/169], [94mLoss[0m : 2.80255
[1mStep[0m  [144/169], [94mLoss[0m : 2.72931
[1mStep[0m  [160/169], [94mLoss[0m : 2.66291

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53758
[1mStep[0m  [16/169], [94mLoss[0m : 2.47721
[1mStep[0m  [32/169], [94mLoss[0m : 2.52757
[1mStep[0m  [48/169], [94mLoss[0m : 2.01440
[1mStep[0m  [64/169], [94mLoss[0m : 2.83165
[1mStep[0m  [80/169], [94mLoss[0m : 2.41602
[1mStep[0m  [96/169], [94mLoss[0m : 2.52642
[1mStep[0m  [112/169], [94mLoss[0m : 3.06009
[1mStep[0m  [128/169], [94mLoss[0m : 2.57510
[1mStep[0m  [144/169], [94mLoss[0m : 2.37953
[1mStep[0m  [160/169], [94mLoss[0m : 2.53538

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29607
[1mStep[0m  [16/169], [94mLoss[0m : 2.81029
[1mStep[0m  [32/169], [94mLoss[0m : 3.07653
[1mStep[0m  [48/169], [94mLoss[0m : 2.46127
[1mStep[0m  [64/169], [94mLoss[0m : 2.55755
[1mStep[0m  [80/169], [94mLoss[0m : 3.19283
[1mStep[0m  [96/169], [94mLoss[0m : 2.21628
[1mStep[0m  [112/169], [94mLoss[0m : 2.77314
[1mStep[0m  [128/169], [94mLoss[0m : 2.77974
[1mStep[0m  [144/169], [94mLoss[0m : 2.52575
[1mStep[0m  [160/169], [94mLoss[0m : 2.63297

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82403
[1mStep[0m  [16/169], [94mLoss[0m : 3.04299
[1mStep[0m  [32/169], [94mLoss[0m : 2.63399
[1mStep[0m  [48/169], [94mLoss[0m : 2.47739
[1mStep[0m  [64/169], [94mLoss[0m : 3.12953
[1mStep[0m  [80/169], [94mLoss[0m : 2.46799
[1mStep[0m  [96/169], [94mLoss[0m : 2.77006
[1mStep[0m  [112/169], [94mLoss[0m : 2.57375
[1mStep[0m  [128/169], [94mLoss[0m : 3.13031
[1mStep[0m  [144/169], [94mLoss[0m : 2.79532
[1mStep[0m  [160/169], [94mLoss[0m : 2.75926

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73896
[1mStep[0m  [16/169], [94mLoss[0m : 2.67024
[1mStep[0m  [32/169], [94mLoss[0m : 2.80352
[1mStep[0m  [48/169], [94mLoss[0m : 2.48153
[1mStep[0m  [64/169], [94mLoss[0m : 2.84348
[1mStep[0m  [80/169], [94mLoss[0m : 2.25618
[1mStep[0m  [96/169], [94mLoss[0m : 2.97297
[1mStep[0m  [112/169], [94mLoss[0m : 3.05343
[1mStep[0m  [128/169], [94mLoss[0m : 2.77525
[1mStep[0m  [144/169], [94mLoss[0m : 2.42098
[1mStep[0m  [160/169], [94mLoss[0m : 3.07936

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41810
[1mStep[0m  [16/169], [94mLoss[0m : 2.41426
[1mStep[0m  [32/169], [94mLoss[0m : 2.45682
[1mStep[0m  [48/169], [94mLoss[0m : 2.22651
[1mStep[0m  [64/169], [94mLoss[0m : 2.57576
[1mStep[0m  [80/169], [94mLoss[0m : 2.56240
[1mStep[0m  [96/169], [94mLoss[0m : 2.88998
[1mStep[0m  [112/169], [94mLoss[0m : 2.59191
[1mStep[0m  [128/169], [94mLoss[0m : 2.67111
[1mStep[0m  [144/169], [94mLoss[0m : 2.58712
[1mStep[0m  [160/169], [94mLoss[0m : 2.04657

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94300
[1mStep[0m  [16/169], [94mLoss[0m : 2.34192
[1mStep[0m  [32/169], [94mLoss[0m : 2.20434
[1mStep[0m  [48/169], [94mLoss[0m : 2.56369
[1mStep[0m  [64/169], [94mLoss[0m : 2.66989
[1mStep[0m  [80/169], [94mLoss[0m : 2.39001
[1mStep[0m  [96/169], [94mLoss[0m : 2.63690
[1mStep[0m  [112/169], [94mLoss[0m : 2.44105
[1mStep[0m  [128/169], [94mLoss[0m : 2.96843
[1mStep[0m  [144/169], [94mLoss[0m : 2.66180
[1mStep[0m  [160/169], [94mLoss[0m : 2.71487

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61590
[1mStep[0m  [16/169], [94mLoss[0m : 2.96918
[1mStep[0m  [32/169], [94mLoss[0m : 2.56782
[1mStep[0m  [48/169], [94mLoss[0m : 2.86767
[1mStep[0m  [64/169], [94mLoss[0m : 2.58589
[1mStep[0m  [80/169], [94mLoss[0m : 2.66111
[1mStep[0m  [96/169], [94mLoss[0m : 2.47913
[1mStep[0m  [112/169], [94mLoss[0m : 2.66411
[1mStep[0m  [128/169], [94mLoss[0m : 2.73572
[1mStep[0m  [144/169], [94mLoss[0m : 2.54848
[1mStep[0m  [160/169], [94mLoss[0m : 2.52411

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77800
[1mStep[0m  [16/169], [94mLoss[0m : 2.21158
[1mStep[0m  [32/169], [94mLoss[0m : 2.61425
[1mStep[0m  [48/169], [94mLoss[0m : 2.82831
[1mStep[0m  [64/169], [94mLoss[0m : 2.61671
[1mStep[0m  [80/169], [94mLoss[0m : 2.49304
[1mStep[0m  [96/169], [94mLoss[0m : 2.69286
[1mStep[0m  [112/169], [94mLoss[0m : 2.54820
[1mStep[0m  [128/169], [94mLoss[0m : 2.48389
[1mStep[0m  [144/169], [94mLoss[0m : 2.04233
[1mStep[0m  [160/169], [94mLoss[0m : 2.73261

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94163
[1mStep[0m  [16/169], [94mLoss[0m : 2.32138
[1mStep[0m  [32/169], [94mLoss[0m : 2.10571
[1mStep[0m  [48/169], [94mLoss[0m : 2.77717
[1mStep[0m  [64/169], [94mLoss[0m : 2.79671
[1mStep[0m  [80/169], [94mLoss[0m : 2.92953
[1mStep[0m  [96/169], [94mLoss[0m : 1.97287
[1mStep[0m  [112/169], [94mLoss[0m : 2.99552
[1mStep[0m  [128/169], [94mLoss[0m : 2.73540
[1mStep[0m  [144/169], [94mLoss[0m : 2.82974
[1mStep[0m  [160/169], [94mLoss[0m : 2.24692

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.403, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49231
[1mStep[0m  [16/169], [94mLoss[0m : 2.88832
[1mStep[0m  [32/169], [94mLoss[0m : 2.13163
[1mStep[0m  [48/169], [94mLoss[0m : 2.75415
[1mStep[0m  [64/169], [94mLoss[0m : 2.57700
[1mStep[0m  [80/169], [94mLoss[0m : 2.22983
[1mStep[0m  [96/169], [94mLoss[0m : 2.57463
[1mStep[0m  [112/169], [94mLoss[0m : 2.58284
[1mStep[0m  [128/169], [94mLoss[0m : 2.79760
[1mStep[0m  [144/169], [94mLoss[0m : 2.52836
[1mStep[0m  [160/169], [94mLoss[0m : 2.58719

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.389, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96520
[1mStep[0m  [16/169], [94mLoss[0m : 2.41634
[1mStep[0m  [32/169], [94mLoss[0m : 2.37417
[1mStep[0m  [48/169], [94mLoss[0m : 2.60849
[1mStep[0m  [64/169], [94mLoss[0m : 2.29575
[1mStep[0m  [80/169], [94mLoss[0m : 2.44606
[1mStep[0m  [96/169], [94mLoss[0m : 2.54364
[1mStep[0m  [112/169], [94mLoss[0m : 2.42327
[1mStep[0m  [128/169], [94mLoss[0m : 2.32095
[1mStep[0m  [144/169], [94mLoss[0m : 2.73746
[1mStep[0m  [160/169], [94mLoss[0m : 2.67560

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24236
[1mStep[0m  [16/169], [94mLoss[0m : 2.34836
[1mStep[0m  [32/169], [94mLoss[0m : 2.68135
[1mStep[0m  [48/169], [94mLoss[0m : 2.64630
[1mStep[0m  [64/169], [94mLoss[0m : 2.25577
[1mStep[0m  [80/169], [94mLoss[0m : 2.65326
[1mStep[0m  [96/169], [94mLoss[0m : 2.92718
[1mStep[0m  [112/169], [94mLoss[0m : 2.81301
[1mStep[0m  [128/169], [94mLoss[0m : 2.45719
[1mStep[0m  [144/169], [94mLoss[0m : 2.53958
[1mStep[0m  [160/169], [94mLoss[0m : 2.42939

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48894
[1mStep[0m  [16/169], [94mLoss[0m : 2.45169
[1mStep[0m  [32/169], [94mLoss[0m : 2.36241
[1mStep[0m  [48/169], [94mLoss[0m : 2.03527
[1mStep[0m  [64/169], [94mLoss[0m : 2.06912
[1mStep[0m  [80/169], [94mLoss[0m : 2.56217
[1mStep[0m  [96/169], [94mLoss[0m : 2.69334
[1mStep[0m  [112/169], [94mLoss[0m : 2.67737
[1mStep[0m  [128/169], [94mLoss[0m : 2.74961
[1mStep[0m  [144/169], [94mLoss[0m : 2.79923
[1mStep[0m  [160/169], [94mLoss[0m : 2.60104

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16507
[1mStep[0m  [16/169], [94mLoss[0m : 2.46886
[1mStep[0m  [32/169], [94mLoss[0m : 2.37787
[1mStep[0m  [48/169], [94mLoss[0m : 3.02809
[1mStep[0m  [64/169], [94mLoss[0m : 2.80020
[1mStep[0m  [80/169], [94mLoss[0m : 2.45351
[1mStep[0m  [96/169], [94mLoss[0m : 2.11598
[1mStep[0m  [112/169], [94mLoss[0m : 2.73323
[1mStep[0m  [128/169], [94mLoss[0m : 2.36540
[1mStep[0m  [144/169], [94mLoss[0m : 2.63383
[1mStep[0m  [160/169], [94mLoss[0m : 2.41832

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.362, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59379
[1mStep[0m  [16/169], [94mLoss[0m : 2.76871
[1mStep[0m  [32/169], [94mLoss[0m : 2.41648
[1mStep[0m  [48/169], [94mLoss[0m : 2.21125
[1mStep[0m  [64/169], [94mLoss[0m : 2.71012
[1mStep[0m  [80/169], [94mLoss[0m : 2.62782
[1mStep[0m  [96/169], [94mLoss[0m : 2.81531
[1mStep[0m  [112/169], [94mLoss[0m : 2.96322
[1mStep[0m  [128/169], [94mLoss[0m : 2.39102
[1mStep[0m  [144/169], [94mLoss[0m : 2.51143
[1mStep[0m  [160/169], [94mLoss[0m : 2.51351

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79510
[1mStep[0m  [16/169], [94mLoss[0m : 2.76893
[1mStep[0m  [32/169], [94mLoss[0m : 2.56739
[1mStep[0m  [48/169], [94mLoss[0m : 2.32926
[1mStep[0m  [64/169], [94mLoss[0m : 2.59172
[1mStep[0m  [80/169], [94mLoss[0m : 2.44088
[1mStep[0m  [96/169], [94mLoss[0m : 2.04422
[1mStep[0m  [112/169], [94mLoss[0m : 2.39117
[1mStep[0m  [128/169], [94mLoss[0m : 2.80883
[1mStep[0m  [144/169], [94mLoss[0m : 2.42871
[1mStep[0m  [160/169], [94mLoss[0m : 2.87360

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.368, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43280
[1mStep[0m  [16/169], [94mLoss[0m : 2.61255
[1mStep[0m  [32/169], [94mLoss[0m : 2.95867
[1mStep[0m  [48/169], [94mLoss[0m : 2.39256
[1mStep[0m  [64/169], [94mLoss[0m : 2.53854
[1mStep[0m  [80/169], [94mLoss[0m : 2.56585
[1mStep[0m  [96/169], [94mLoss[0m : 2.70931
[1mStep[0m  [112/169], [94mLoss[0m : 2.48766
[1mStep[0m  [128/169], [94mLoss[0m : 2.91963
[1mStep[0m  [144/169], [94mLoss[0m : 2.56641
[1mStep[0m  [160/169], [94mLoss[0m : 2.93348

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63425
[1mStep[0m  [16/169], [94mLoss[0m : 2.50556
[1mStep[0m  [32/169], [94mLoss[0m : 2.29622
[1mStep[0m  [48/169], [94mLoss[0m : 2.61574
[1mStep[0m  [64/169], [94mLoss[0m : 2.44478
[1mStep[0m  [80/169], [94mLoss[0m : 2.59151
[1mStep[0m  [96/169], [94mLoss[0m : 2.31126
[1mStep[0m  [112/169], [94mLoss[0m : 2.63424
[1mStep[0m  [128/169], [94mLoss[0m : 2.50850
[1mStep[0m  [144/169], [94mLoss[0m : 2.58420
[1mStep[0m  [160/169], [94mLoss[0m : 2.58182

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.367, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60404
[1mStep[0m  [16/169], [94mLoss[0m : 2.52182
[1mStep[0m  [32/169], [94mLoss[0m : 2.52517
[1mStep[0m  [48/169], [94mLoss[0m : 2.20780
[1mStep[0m  [64/169], [94mLoss[0m : 2.22802
[1mStep[0m  [80/169], [94mLoss[0m : 2.21738
[1mStep[0m  [96/169], [94mLoss[0m : 2.26150
[1mStep[0m  [112/169], [94mLoss[0m : 2.27010
[1mStep[0m  [128/169], [94mLoss[0m : 2.88491
[1mStep[0m  [144/169], [94mLoss[0m : 2.23123
[1mStep[0m  [160/169], [94mLoss[0m : 2.44880

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.353
====================================

Phase 1 - Evaluation MAE:  2.3526943645306995
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.47163
[1mStep[0m  [16/169], [94mLoss[0m : 2.88176
[1mStep[0m  [32/169], [94mLoss[0m : 2.74388
[1mStep[0m  [48/169], [94mLoss[0m : 2.40867
[1mStep[0m  [64/169], [94mLoss[0m : 2.57202
[1mStep[0m  [80/169], [94mLoss[0m : 2.67405
[1mStep[0m  [96/169], [94mLoss[0m : 2.97454
[1mStep[0m  [112/169], [94mLoss[0m : 2.86781
[1mStep[0m  [128/169], [94mLoss[0m : 2.36352
[1mStep[0m  [144/169], [94mLoss[0m : 2.80249
[1mStep[0m  [160/169], [94mLoss[0m : 2.53617

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39025
[1mStep[0m  [16/169], [94mLoss[0m : 2.45066
[1mStep[0m  [32/169], [94mLoss[0m : 3.03628
[1mStep[0m  [48/169], [94mLoss[0m : 2.49178
[1mStep[0m  [64/169], [94mLoss[0m : 2.47250
[1mStep[0m  [80/169], [94mLoss[0m : 2.49287
[1mStep[0m  [96/169], [94mLoss[0m : 2.51840
[1mStep[0m  [112/169], [94mLoss[0m : 2.17444
[1mStep[0m  [128/169], [94mLoss[0m : 2.35123
[1mStep[0m  [144/169], [94mLoss[0m : 2.23038
[1mStep[0m  [160/169], [94mLoss[0m : 2.78575

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.651, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58602
[1mStep[0m  [16/169], [94mLoss[0m : 2.23034
[1mStep[0m  [32/169], [94mLoss[0m : 2.59884
[1mStep[0m  [48/169], [94mLoss[0m : 2.82195
[1mStep[0m  [64/169], [94mLoss[0m : 2.76225
[1mStep[0m  [80/169], [94mLoss[0m : 2.60315
[1mStep[0m  [96/169], [94mLoss[0m : 2.54427
[1mStep[0m  [112/169], [94mLoss[0m : 2.57347
[1mStep[0m  [128/169], [94mLoss[0m : 2.34763
[1mStep[0m  [144/169], [94mLoss[0m : 2.82612
[1mStep[0m  [160/169], [94mLoss[0m : 2.93146

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.596, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63820
[1mStep[0m  [16/169], [94mLoss[0m : 2.39544
[1mStep[0m  [32/169], [94mLoss[0m : 2.17225
[1mStep[0m  [48/169], [94mLoss[0m : 2.38235
[1mStep[0m  [64/169], [94mLoss[0m : 2.21779
[1mStep[0m  [80/169], [94mLoss[0m : 2.94900
[1mStep[0m  [96/169], [94mLoss[0m : 2.43102
[1mStep[0m  [112/169], [94mLoss[0m : 2.25405
[1mStep[0m  [128/169], [94mLoss[0m : 2.90199
[1mStep[0m  [144/169], [94mLoss[0m : 2.03211
[1mStep[0m  [160/169], [94mLoss[0m : 2.33260

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.594, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42155
[1mStep[0m  [16/169], [94mLoss[0m : 2.73922
[1mStep[0m  [32/169], [94mLoss[0m : 2.00779
[1mStep[0m  [48/169], [94mLoss[0m : 2.57853
[1mStep[0m  [64/169], [94mLoss[0m : 2.08328
[1mStep[0m  [80/169], [94mLoss[0m : 2.28466
[1mStep[0m  [96/169], [94mLoss[0m : 2.40810
[1mStep[0m  [112/169], [94mLoss[0m : 2.36173
[1mStep[0m  [128/169], [94mLoss[0m : 2.34949
[1mStep[0m  [144/169], [94mLoss[0m : 2.26769
[1mStep[0m  [160/169], [94mLoss[0m : 2.72576

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24023
[1mStep[0m  [16/169], [94mLoss[0m : 1.96152
[1mStep[0m  [32/169], [94mLoss[0m : 1.97261
[1mStep[0m  [48/169], [94mLoss[0m : 2.52334
[1mStep[0m  [64/169], [94mLoss[0m : 2.65860
[1mStep[0m  [80/169], [94mLoss[0m : 2.42312
[1mStep[0m  [96/169], [94mLoss[0m : 2.18777
[1mStep[0m  [112/169], [94mLoss[0m : 2.21185
[1mStep[0m  [128/169], [94mLoss[0m : 2.45734
[1mStep[0m  [144/169], [94mLoss[0m : 2.45287
[1mStep[0m  [160/169], [94mLoss[0m : 2.26014

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.714, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22415
[1mStep[0m  [16/169], [94mLoss[0m : 2.95027
[1mStep[0m  [32/169], [94mLoss[0m : 2.01841
[1mStep[0m  [48/169], [94mLoss[0m : 3.15282
[1mStep[0m  [64/169], [94mLoss[0m : 2.03350
[1mStep[0m  [80/169], [94mLoss[0m : 2.52822
[1mStep[0m  [96/169], [94mLoss[0m : 2.14308
[1mStep[0m  [112/169], [94mLoss[0m : 2.01773
[1mStep[0m  [128/169], [94mLoss[0m : 2.48600
[1mStep[0m  [144/169], [94mLoss[0m : 2.05350
[1mStep[0m  [160/169], [94mLoss[0m : 2.08721

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94512
[1mStep[0m  [16/169], [94mLoss[0m : 2.15992
[1mStep[0m  [32/169], [94mLoss[0m : 2.36522
[1mStep[0m  [48/169], [94mLoss[0m : 2.38008
[1mStep[0m  [64/169], [94mLoss[0m : 2.21000
[1mStep[0m  [80/169], [94mLoss[0m : 2.13735
[1mStep[0m  [96/169], [94mLoss[0m : 2.23864
[1mStep[0m  [112/169], [94mLoss[0m : 2.60490
[1mStep[0m  [128/169], [94mLoss[0m : 2.32696
[1mStep[0m  [144/169], [94mLoss[0m : 2.79633
[1mStep[0m  [160/169], [94mLoss[0m : 2.42214

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.673, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97310
[1mStep[0m  [16/169], [94mLoss[0m : 2.04716
[1mStep[0m  [32/169], [94mLoss[0m : 2.37644
[1mStep[0m  [48/169], [94mLoss[0m : 1.99169
[1mStep[0m  [64/169], [94mLoss[0m : 2.15172
[1mStep[0m  [80/169], [94mLoss[0m : 2.25418
[1mStep[0m  [96/169], [94mLoss[0m : 2.15753
[1mStep[0m  [112/169], [94mLoss[0m : 2.29644
[1mStep[0m  [128/169], [94mLoss[0m : 1.82053
[1mStep[0m  [144/169], [94mLoss[0m : 2.53001
[1mStep[0m  [160/169], [94mLoss[0m : 2.83971

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.594, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99519
[1mStep[0m  [16/169], [94mLoss[0m : 2.35614
[1mStep[0m  [32/169], [94mLoss[0m : 2.47513
[1mStep[0m  [48/169], [94mLoss[0m : 1.94398
[1mStep[0m  [64/169], [94mLoss[0m : 2.54284
[1mStep[0m  [80/169], [94mLoss[0m : 1.93169
[1mStep[0m  [96/169], [94mLoss[0m : 2.35282
[1mStep[0m  [112/169], [94mLoss[0m : 2.30101
[1mStep[0m  [128/169], [94mLoss[0m : 2.53933
[1mStep[0m  [144/169], [94mLoss[0m : 2.16767
[1mStep[0m  [160/169], [94mLoss[0m : 2.37359

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.580, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59284
[1mStep[0m  [16/169], [94mLoss[0m : 2.49664
[1mStep[0m  [32/169], [94mLoss[0m : 2.19149
[1mStep[0m  [48/169], [94mLoss[0m : 2.16486
[1mStep[0m  [64/169], [94mLoss[0m : 2.10024
[1mStep[0m  [80/169], [94mLoss[0m : 2.36416
[1mStep[0m  [96/169], [94mLoss[0m : 2.08977
[1mStep[0m  [112/169], [94mLoss[0m : 1.87246
[1mStep[0m  [128/169], [94mLoss[0m : 2.06315
[1mStep[0m  [144/169], [94mLoss[0m : 2.31335
[1mStep[0m  [160/169], [94mLoss[0m : 2.55742

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.486, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18659
[1mStep[0m  [16/169], [94mLoss[0m : 2.63120
[1mStep[0m  [32/169], [94mLoss[0m : 1.99194
[1mStep[0m  [48/169], [94mLoss[0m : 1.95031
[1mStep[0m  [64/169], [94mLoss[0m : 2.21470
[1mStep[0m  [80/169], [94mLoss[0m : 2.11153
[1mStep[0m  [96/169], [94mLoss[0m : 2.10802
[1mStep[0m  [112/169], [94mLoss[0m : 2.29927
[1mStep[0m  [128/169], [94mLoss[0m : 2.15821
[1mStep[0m  [144/169], [94mLoss[0m : 2.07338
[1mStep[0m  [160/169], [94mLoss[0m : 2.38111

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.179, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89166
[1mStep[0m  [16/169], [94mLoss[0m : 2.42029
[1mStep[0m  [32/169], [94mLoss[0m : 2.25677
[1mStep[0m  [48/169], [94mLoss[0m : 2.06922
[1mStep[0m  [64/169], [94mLoss[0m : 2.03218
[1mStep[0m  [80/169], [94mLoss[0m : 1.74780
[1mStep[0m  [96/169], [94mLoss[0m : 2.56868
[1mStep[0m  [112/169], [94mLoss[0m : 2.22962
[1mStep[0m  [128/169], [94mLoss[0m : 1.86629
[1mStep[0m  [144/169], [94mLoss[0m : 1.91981
[1mStep[0m  [160/169], [94mLoss[0m : 2.30463

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.131, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03234
[1mStep[0m  [16/169], [94mLoss[0m : 2.03843
[1mStep[0m  [32/169], [94mLoss[0m : 2.30385
[1mStep[0m  [48/169], [94mLoss[0m : 2.03152
[1mStep[0m  [64/169], [94mLoss[0m : 1.84674
[1mStep[0m  [80/169], [94mLoss[0m : 2.11857
[1mStep[0m  [96/169], [94mLoss[0m : 1.98458
[1mStep[0m  [112/169], [94mLoss[0m : 2.10590
[1mStep[0m  [128/169], [94mLoss[0m : 1.91889
[1mStep[0m  [144/169], [94mLoss[0m : 1.85236
[1mStep[0m  [160/169], [94mLoss[0m : 2.48380

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.129, [92mTest[0m: 2.509, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92398
[1mStep[0m  [16/169], [94mLoss[0m : 2.01059
[1mStep[0m  [32/169], [94mLoss[0m : 2.22312
[1mStep[0m  [48/169], [94mLoss[0m : 2.12600
[1mStep[0m  [64/169], [94mLoss[0m : 2.14658
[1mStep[0m  [80/169], [94mLoss[0m : 2.20117
[1mStep[0m  [96/169], [94mLoss[0m : 1.86423
[1mStep[0m  [112/169], [94mLoss[0m : 2.10732
[1mStep[0m  [128/169], [94mLoss[0m : 1.70538
[1mStep[0m  [144/169], [94mLoss[0m : 2.15025
[1mStep[0m  [160/169], [94mLoss[0m : 1.97118

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.073, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78001
[1mStep[0m  [16/169], [94mLoss[0m : 2.20259
[1mStep[0m  [32/169], [94mLoss[0m : 1.83173
[1mStep[0m  [48/169], [94mLoss[0m : 2.12059
[1mStep[0m  [64/169], [94mLoss[0m : 1.97671
[1mStep[0m  [80/169], [94mLoss[0m : 1.99568
[1mStep[0m  [96/169], [94mLoss[0m : 1.67482
[1mStep[0m  [112/169], [94mLoss[0m : 2.23888
[1mStep[0m  [128/169], [94mLoss[0m : 1.89319
[1mStep[0m  [144/169], [94mLoss[0m : 2.16489
[1mStep[0m  [160/169], [94mLoss[0m : 2.08895

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23891
[1mStep[0m  [16/169], [94mLoss[0m : 1.74641
[1mStep[0m  [32/169], [94mLoss[0m : 1.95027
[1mStep[0m  [48/169], [94mLoss[0m : 1.96798
[1mStep[0m  [64/169], [94mLoss[0m : 1.98944
[1mStep[0m  [80/169], [94mLoss[0m : 2.06462
[1mStep[0m  [96/169], [94mLoss[0m : 2.19022
[1mStep[0m  [112/169], [94mLoss[0m : 2.09242
[1mStep[0m  [128/169], [94mLoss[0m : 2.23099
[1mStep[0m  [144/169], [94mLoss[0m : 1.84167
[1mStep[0m  [160/169], [94mLoss[0m : 2.11361

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65257
[1mStep[0m  [16/169], [94mLoss[0m : 2.41215
[1mStep[0m  [32/169], [94mLoss[0m : 1.64909
[1mStep[0m  [48/169], [94mLoss[0m : 1.85239
[1mStep[0m  [64/169], [94mLoss[0m : 1.96496
[1mStep[0m  [80/169], [94mLoss[0m : 1.82647
[1mStep[0m  [96/169], [94mLoss[0m : 2.12685
[1mStep[0m  [112/169], [94mLoss[0m : 2.24326
[1mStep[0m  [128/169], [94mLoss[0m : 1.66279
[1mStep[0m  [144/169], [94mLoss[0m : 1.79861
[1mStep[0m  [160/169], [94mLoss[0m : 2.03339

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04320
[1mStep[0m  [16/169], [94mLoss[0m : 1.91030
[1mStep[0m  [32/169], [94mLoss[0m : 2.09818
[1mStep[0m  [48/169], [94mLoss[0m : 2.15755
[1mStep[0m  [64/169], [94mLoss[0m : 1.35958
[1mStep[0m  [80/169], [94mLoss[0m : 2.17615
[1mStep[0m  [96/169], [94mLoss[0m : 1.75576
[1mStep[0m  [112/169], [94mLoss[0m : 1.52981
[1mStep[0m  [128/169], [94mLoss[0m : 2.03522
[1mStep[0m  [144/169], [94mLoss[0m : 1.93599
[1mStep[0m  [160/169], [94mLoss[0m : 2.17461

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57843
[1mStep[0m  [16/169], [94mLoss[0m : 1.88612
[1mStep[0m  [32/169], [94mLoss[0m : 1.91507
[1mStep[0m  [48/169], [94mLoss[0m : 1.92014
[1mStep[0m  [64/169], [94mLoss[0m : 1.79839
[1mStep[0m  [80/169], [94mLoss[0m : 1.77746
[1mStep[0m  [96/169], [94mLoss[0m : 1.90554
[1mStep[0m  [112/169], [94mLoss[0m : 1.83888
[1mStep[0m  [128/169], [94mLoss[0m : 2.30503
[1mStep[0m  [144/169], [94mLoss[0m : 1.90898
[1mStep[0m  [160/169], [94mLoss[0m : 1.67531

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64855
[1mStep[0m  [16/169], [94mLoss[0m : 2.00468
[1mStep[0m  [32/169], [94mLoss[0m : 1.86079
[1mStep[0m  [48/169], [94mLoss[0m : 1.82203
[1mStep[0m  [64/169], [94mLoss[0m : 1.79996
[1mStep[0m  [80/169], [94mLoss[0m : 1.72564
[1mStep[0m  [96/169], [94mLoss[0m : 1.72815
[1mStep[0m  [112/169], [94mLoss[0m : 1.90487
[1mStep[0m  [128/169], [94mLoss[0m : 1.93969
[1mStep[0m  [144/169], [94mLoss[0m : 2.10834
[1mStep[0m  [160/169], [94mLoss[0m : 1.83222

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.491, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83991
[1mStep[0m  [16/169], [94mLoss[0m : 1.81606
[1mStep[0m  [32/169], [94mLoss[0m : 2.25646
[1mStep[0m  [48/169], [94mLoss[0m : 1.76604
[1mStep[0m  [64/169], [94mLoss[0m : 1.80106
[1mStep[0m  [80/169], [94mLoss[0m : 2.22133
[1mStep[0m  [96/169], [94mLoss[0m : 1.92047
[1mStep[0m  [112/169], [94mLoss[0m : 2.08440
[1mStep[0m  [128/169], [94mLoss[0m : 1.46791
[1mStep[0m  [144/169], [94mLoss[0m : 1.85559
[1mStep[0m  [160/169], [94mLoss[0m : 1.51861

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54008
[1mStep[0m  [16/169], [94mLoss[0m : 1.89048
[1mStep[0m  [32/169], [94mLoss[0m : 2.02178
[1mStep[0m  [48/169], [94mLoss[0m : 2.20070
[1mStep[0m  [64/169], [94mLoss[0m : 1.81734
[1mStep[0m  [80/169], [94mLoss[0m : 1.88193
[1mStep[0m  [96/169], [94mLoss[0m : 1.98201
[1mStep[0m  [112/169], [94mLoss[0m : 1.73371
[1mStep[0m  [128/169], [94mLoss[0m : 1.48020
[1mStep[0m  [144/169], [94mLoss[0m : 1.87564
[1mStep[0m  [160/169], [94mLoss[0m : 1.96202

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83407
[1mStep[0m  [16/169], [94mLoss[0m : 1.54925
[1mStep[0m  [32/169], [94mLoss[0m : 1.63040
[1mStep[0m  [48/169], [94mLoss[0m : 1.65999
[1mStep[0m  [64/169], [94mLoss[0m : 1.80260
[1mStep[0m  [80/169], [94mLoss[0m : 1.99933
[1mStep[0m  [96/169], [94mLoss[0m : 2.01257
[1mStep[0m  [112/169], [94mLoss[0m : 1.77674
[1mStep[0m  [128/169], [94mLoss[0m : 1.71012
[1mStep[0m  [144/169], [94mLoss[0m : 1.84282
[1mStep[0m  [160/169], [94mLoss[0m : 1.58077

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.582, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69611
[1mStep[0m  [16/169], [94mLoss[0m : 1.74503
[1mStep[0m  [32/169], [94mLoss[0m : 1.86759
[1mStep[0m  [48/169], [94mLoss[0m : 1.84951
[1mStep[0m  [64/169], [94mLoss[0m : 1.79682
[1mStep[0m  [80/169], [94mLoss[0m : 1.94942
[1mStep[0m  [96/169], [94mLoss[0m : 2.05699
[1mStep[0m  [112/169], [94mLoss[0m : 1.79806
[1mStep[0m  [128/169], [94mLoss[0m : 1.57165
[1mStep[0m  [144/169], [94mLoss[0m : 1.78923
[1mStep[0m  [160/169], [94mLoss[0m : 1.75825

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.438, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59373
[1mStep[0m  [16/169], [94mLoss[0m : 2.00132
[1mStep[0m  [32/169], [94mLoss[0m : 1.88520
[1mStep[0m  [48/169], [94mLoss[0m : 1.47958
[1mStep[0m  [64/169], [94mLoss[0m : 1.92136
[1mStep[0m  [80/169], [94mLoss[0m : 1.98209
[1mStep[0m  [96/169], [94mLoss[0m : 1.75837
[1mStep[0m  [112/169], [94mLoss[0m : 1.39217
[1mStep[0m  [128/169], [94mLoss[0m : 1.58643
[1mStep[0m  [144/169], [94mLoss[0m : 1.91971
[1mStep[0m  [160/169], [94mLoss[0m : 2.06093

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63663
[1mStep[0m  [16/169], [94mLoss[0m : 2.03148
[1mStep[0m  [32/169], [94mLoss[0m : 1.77310
[1mStep[0m  [48/169], [94mLoss[0m : 1.40582
[1mStep[0m  [64/169], [94mLoss[0m : 1.51686
[1mStep[0m  [80/169], [94mLoss[0m : 1.76151
[1mStep[0m  [96/169], [94mLoss[0m : 1.85971
[1mStep[0m  [112/169], [94mLoss[0m : 1.92552
[1mStep[0m  [128/169], [94mLoss[0m : 1.57490
[1mStep[0m  [144/169], [94mLoss[0m : 1.58009
[1mStep[0m  [160/169], [94mLoss[0m : 1.68362

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82359
[1mStep[0m  [16/169], [94mLoss[0m : 1.70079
[1mStep[0m  [32/169], [94mLoss[0m : 1.61086
[1mStep[0m  [48/169], [94mLoss[0m : 1.97892
[1mStep[0m  [64/169], [94mLoss[0m : 1.50850
[1mStep[0m  [80/169], [94mLoss[0m : 1.77091
[1mStep[0m  [96/169], [94mLoss[0m : 1.65055
[1mStep[0m  [112/169], [94mLoss[0m : 1.71328
[1mStep[0m  [128/169], [94mLoss[0m : 1.55936
[1mStep[0m  [144/169], [94mLoss[0m : 1.72353
[1mStep[0m  [160/169], [94mLoss[0m : 1.58011

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.721, [92mTest[0m: 2.522, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64853
[1mStep[0m  [16/169], [94mLoss[0m : 1.94796
[1mStep[0m  [32/169], [94mLoss[0m : 1.75409
[1mStep[0m  [48/169], [94mLoss[0m : 2.05711
[1mStep[0m  [64/169], [94mLoss[0m : 1.93210
[1mStep[0m  [80/169], [94mLoss[0m : 1.65599
[1mStep[0m  [96/169], [94mLoss[0m : 2.05237
[1mStep[0m  [112/169], [94mLoss[0m : 1.71014
[1mStep[0m  [128/169], [94mLoss[0m : 1.74109
[1mStep[0m  [144/169], [94mLoss[0m : 1.44014
[1mStep[0m  [160/169], [94mLoss[0m : 1.90159

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.494, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45818
[1mStep[0m  [16/169], [94mLoss[0m : 1.50425
[1mStep[0m  [32/169], [94mLoss[0m : 1.38547
[1mStep[0m  [48/169], [94mLoss[0m : 2.03803
[1mStep[0m  [64/169], [94mLoss[0m : 1.44246
[1mStep[0m  [80/169], [94mLoss[0m : 1.49979
[1mStep[0m  [96/169], [94mLoss[0m : 1.73751
[1mStep[0m  [112/169], [94mLoss[0m : 1.47437
[1mStep[0m  [128/169], [94mLoss[0m : 1.55344
[1mStep[0m  [144/169], [94mLoss[0m : 1.65577
[1mStep[0m  [160/169], [94mLoss[0m : 1.87427

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.609
====================================

Phase 2 - Evaluation MAE:  2.6091769444090978
MAE score P1      2.352694
MAE score P2      2.609177
loss              1.691002
learning_rate     0.002575
batch_size              64
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 32, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 11.05300
[1mStep[0m  [33/339], [94mLoss[0m : 6.35755
[1mStep[0m  [66/339], [94mLoss[0m : 3.30232
[1mStep[0m  [99/339], [94mLoss[0m : 2.43059
[1mStep[0m  [132/339], [94mLoss[0m : 2.46535
[1mStep[0m  [165/339], [94mLoss[0m : 3.48762
[1mStep[0m  [198/339], [94mLoss[0m : 2.98274
[1mStep[0m  [231/339], [94mLoss[0m : 2.56684
[1mStep[0m  [264/339], [94mLoss[0m : 2.37728
[1mStep[0m  [297/339], [94mLoss[0m : 2.12990
[1mStep[0m  [330/339], [94mLoss[0m : 3.11437

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.453, [92mTest[0m: 10.889, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68588
[1mStep[0m  [33/339], [94mLoss[0m : 2.83733
[1mStep[0m  [66/339], [94mLoss[0m : 2.31073
[1mStep[0m  [99/339], [94mLoss[0m : 2.89790
[1mStep[0m  [132/339], [94mLoss[0m : 2.63999
[1mStep[0m  [165/339], [94mLoss[0m : 3.08550
[1mStep[0m  [198/339], [94mLoss[0m : 2.44289
[1mStep[0m  [231/339], [94mLoss[0m : 2.68168
[1mStep[0m  [264/339], [94mLoss[0m : 2.20866
[1mStep[0m  [297/339], [94mLoss[0m : 2.58368
[1mStep[0m  [330/339], [94mLoss[0m : 2.66761

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.91900
[1mStep[0m  [33/339], [94mLoss[0m : 2.46534
[1mStep[0m  [66/339], [94mLoss[0m : 2.45339
[1mStep[0m  [99/339], [94mLoss[0m : 2.34002
[1mStep[0m  [132/339], [94mLoss[0m : 2.65332
[1mStep[0m  [165/339], [94mLoss[0m : 2.51704
[1mStep[0m  [198/339], [94mLoss[0m : 2.89299
[1mStep[0m  [231/339], [94mLoss[0m : 2.67622
[1mStep[0m  [264/339], [94mLoss[0m : 2.67939
[1mStep[0m  [297/339], [94mLoss[0m : 2.72415
[1mStep[0m  [330/339], [94mLoss[0m : 2.88925

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97064
[1mStep[0m  [33/339], [94mLoss[0m : 2.14384
[1mStep[0m  [66/339], [94mLoss[0m : 2.47920
[1mStep[0m  [99/339], [94mLoss[0m : 2.71865
[1mStep[0m  [132/339], [94mLoss[0m : 2.37161
[1mStep[0m  [165/339], [94mLoss[0m : 2.66467
[1mStep[0m  [198/339], [94mLoss[0m : 2.60812
[1mStep[0m  [231/339], [94mLoss[0m : 1.96598
[1mStep[0m  [264/339], [94mLoss[0m : 2.85446
[1mStep[0m  [297/339], [94mLoss[0m : 2.06925
[1mStep[0m  [330/339], [94mLoss[0m : 2.31862

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50892
[1mStep[0m  [33/339], [94mLoss[0m : 2.41640
[1mStep[0m  [66/339], [94mLoss[0m : 2.58599
[1mStep[0m  [99/339], [94mLoss[0m : 2.66164
[1mStep[0m  [132/339], [94mLoss[0m : 2.73031
[1mStep[0m  [165/339], [94mLoss[0m : 2.36698
[1mStep[0m  [198/339], [94mLoss[0m : 2.77065
[1mStep[0m  [231/339], [94mLoss[0m : 2.53980
[1mStep[0m  [264/339], [94mLoss[0m : 2.29868
[1mStep[0m  [297/339], [94mLoss[0m : 2.51439
[1mStep[0m  [330/339], [94mLoss[0m : 2.53837

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35843
[1mStep[0m  [33/339], [94mLoss[0m : 2.16608
[1mStep[0m  [66/339], [94mLoss[0m : 2.11895
[1mStep[0m  [99/339], [94mLoss[0m : 2.22985
[1mStep[0m  [132/339], [94mLoss[0m : 2.37058
[1mStep[0m  [165/339], [94mLoss[0m : 2.48363
[1mStep[0m  [198/339], [94mLoss[0m : 2.74947
[1mStep[0m  [231/339], [94mLoss[0m : 3.22982
[1mStep[0m  [264/339], [94mLoss[0m : 2.57439
[1mStep[0m  [297/339], [94mLoss[0m : 1.95883
[1mStep[0m  [330/339], [94mLoss[0m : 2.51064

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15291
[1mStep[0m  [33/339], [94mLoss[0m : 2.35486
[1mStep[0m  [66/339], [94mLoss[0m : 2.39644
[1mStep[0m  [99/339], [94mLoss[0m : 1.87319
[1mStep[0m  [132/339], [94mLoss[0m : 2.22603
[1mStep[0m  [165/339], [94mLoss[0m : 2.56054
[1mStep[0m  [198/339], [94mLoss[0m : 1.53819
[1mStep[0m  [231/339], [94mLoss[0m : 2.44986
[1mStep[0m  [264/339], [94mLoss[0m : 2.38911
[1mStep[0m  [297/339], [94mLoss[0m : 2.30180
[1mStep[0m  [330/339], [94mLoss[0m : 2.24877

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17278
[1mStep[0m  [33/339], [94mLoss[0m : 2.48853
[1mStep[0m  [66/339], [94mLoss[0m : 2.34315
[1mStep[0m  [99/339], [94mLoss[0m : 2.18718
[1mStep[0m  [132/339], [94mLoss[0m : 2.47961
[1mStep[0m  [165/339], [94mLoss[0m : 2.14907
[1mStep[0m  [198/339], [94mLoss[0m : 2.50738
[1mStep[0m  [231/339], [94mLoss[0m : 2.83952
[1mStep[0m  [264/339], [94mLoss[0m : 2.74081
[1mStep[0m  [297/339], [94mLoss[0m : 2.04850
[1mStep[0m  [330/339], [94mLoss[0m : 2.73179

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69219
[1mStep[0m  [33/339], [94mLoss[0m : 2.47466
[1mStep[0m  [66/339], [94mLoss[0m : 2.13881
[1mStep[0m  [99/339], [94mLoss[0m : 2.47367
[1mStep[0m  [132/339], [94mLoss[0m : 2.93032
[1mStep[0m  [165/339], [94mLoss[0m : 2.54068
[1mStep[0m  [198/339], [94mLoss[0m : 1.82227
[1mStep[0m  [231/339], [94mLoss[0m : 2.57706
[1mStep[0m  [264/339], [94mLoss[0m : 3.16619
[1mStep[0m  [297/339], [94mLoss[0m : 2.26310
[1mStep[0m  [330/339], [94mLoss[0m : 2.48045

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44847
[1mStep[0m  [33/339], [94mLoss[0m : 2.25608
[1mStep[0m  [66/339], [94mLoss[0m : 2.60113
[1mStep[0m  [99/339], [94mLoss[0m : 2.75291
[1mStep[0m  [132/339], [94mLoss[0m : 2.41268
[1mStep[0m  [165/339], [94mLoss[0m : 2.76144
[1mStep[0m  [198/339], [94mLoss[0m : 2.11712
[1mStep[0m  [231/339], [94mLoss[0m : 3.10685
[1mStep[0m  [264/339], [94mLoss[0m : 2.32105
[1mStep[0m  [297/339], [94mLoss[0m : 2.34127
[1mStep[0m  [330/339], [94mLoss[0m : 2.74603

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.323, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65823
[1mStep[0m  [33/339], [94mLoss[0m : 2.68345
[1mStep[0m  [66/339], [94mLoss[0m : 3.15676
[1mStep[0m  [99/339], [94mLoss[0m : 2.41625
[1mStep[0m  [132/339], [94mLoss[0m : 2.07123
[1mStep[0m  [165/339], [94mLoss[0m : 3.50901
[1mStep[0m  [198/339], [94mLoss[0m : 2.42894
[1mStep[0m  [231/339], [94mLoss[0m : 2.17168
[1mStep[0m  [264/339], [94mLoss[0m : 2.86221
[1mStep[0m  [297/339], [94mLoss[0m : 2.22859
[1mStep[0m  [330/339], [94mLoss[0m : 3.03753

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89443
[1mStep[0m  [33/339], [94mLoss[0m : 2.83593
[1mStep[0m  [66/339], [94mLoss[0m : 3.06696
[1mStep[0m  [99/339], [94mLoss[0m : 2.56043
[1mStep[0m  [132/339], [94mLoss[0m : 2.51661
[1mStep[0m  [165/339], [94mLoss[0m : 2.07839
[1mStep[0m  [198/339], [94mLoss[0m : 2.31376
[1mStep[0m  [231/339], [94mLoss[0m : 2.37516
[1mStep[0m  [264/339], [94mLoss[0m : 2.81945
[1mStep[0m  [297/339], [94mLoss[0m : 2.27956
[1mStep[0m  [330/339], [94mLoss[0m : 2.76574

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90661
[1mStep[0m  [33/339], [94mLoss[0m : 2.20162
[1mStep[0m  [66/339], [94mLoss[0m : 1.97902
[1mStep[0m  [99/339], [94mLoss[0m : 2.23932
[1mStep[0m  [132/339], [94mLoss[0m : 2.34004
[1mStep[0m  [165/339], [94mLoss[0m : 2.72899
[1mStep[0m  [198/339], [94mLoss[0m : 2.35135
[1mStep[0m  [231/339], [94mLoss[0m : 2.73483
[1mStep[0m  [264/339], [94mLoss[0m : 2.27439
[1mStep[0m  [297/339], [94mLoss[0m : 1.92863
[1mStep[0m  [330/339], [94mLoss[0m : 2.38648

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00624
[1mStep[0m  [33/339], [94mLoss[0m : 2.14080
[1mStep[0m  [66/339], [94mLoss[0m : 3.02916
[1mStep[0m  [99/339], [94mLoss[0m : 2.12211
[1mStep[0m  [132/339], [94mLoss[0m : 2.23266
[1mStep[0m  [165/339], [94mLoss[0m : 1.93216
[1mStep[0m  [198/339], [94mLoss[0m : 1.86397
[1mStep[0m  [231/339], [94mLoss[0m : 2.84966
[1mStep[0m  [264/339], [94mLoss[0m : 2.29488
[1mStep[0m  [297/339], [94mLoss[0m : 2.39968
[1mStep[0m  [330/339], [94mLoss[0m : 2.04096

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60828
[1mStep[0m  [33/339], [94mLoss[0m : 2.50292
[1mStep[0m  [66/339], [94mLoss[0m : 1.93259
[1mStep[0m  [99/339], [94mLoss[0m : 2.39931
[1mStep[0m  [132/339], [94mLoss[0m : 2.76662
[1mStep[0m  [165/339], [94mLoss[0m : 2.55607
[1mStep[0m  [198/339], [94mLoss[0m : 2.54850
[1mStep[0m  [231/339], [94mLoss[0m : 2.63287
[1mStep[0m  [264/339], [94mLoss[0m : 2.00809
[1mStep[0m  [297/339], [94mLoss[0m : 1.87757
[1mStep[0m  [330/339], [94mLoss[0m : 2.40415

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66132
[1mStep[0m  [33/339], [94mLoss[0m : 2.19752
[1mStep[0m  [66/339], [94mLoss[0m : 2.44645
[1mStep[0m  [99/339], [94mLoss[0m : 2.13556
[1mStep[0m  [132/339], [94mLoss[0m : 2.03408
[1mStep[0m  [165/339], [94mLoss[0m : 1.68433
[1mStep[0m  [198/339], [94mLoss[0m : 2.62215
[1mStep[0m  [231/339], [94mLoss[0m : 2.29078
[1mStep[0m  [264/339], [94mLoss[0m : 2.66922
[1mStep[0m  [297/339], [94mLoss[0m : 2.22980
[1mStep[0m  [330/339], [94mLoss[0m : 2.95920

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.320, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59546
[1mStep[0m  [33/339], [94mLoss[0m : 2.37172
[1mStep[0m  [66/339], [94mLoss[0m : 2.67401
[1mStep[0m  [99/339], [94mLoss[0m : 2.90331
[1mStep[0m  [132/339], [94mLoss[0m : 2.82895
[1mStep[0m  [165/339], [94mLoss[0m : 2.56761
[1mStep[0m  [198/339], [94mLoss[0m : 2.85743
[1mStep[0m  [231/339], [94mLoss[0m : 2.46926
[1mStep[0m  [264/339], [94mLoss[0m : 2.82019
[1mStep[0m  [297/339], [94mLoss[0m : 2.64880
[1mStep[0m  [330/339], [94mLoss[0m : 2.58937

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.03861
[1mStep[0m  [33/339], [94mLoss[0m : 2.37673
[1mStep[0m  [66/339], [94mLoss[0m : 1.98088
[1mStep[0m  [99/339], [94mLoss[0m : 2.05175
[1mStep[0m  [132/339], [94mLoss[0m : 1.86156
[1mStep[0m  [165/339], [94mLoss[0m : 1.97632
[1mStep[0m  [198/339], [94mLoss[0m : 2.07032
[1mStep[0m  [231/339], [94mLoss[0m : 2.77444
[1mStep[0m  [264/339], [94mLoss[0m : 3.17244
[1mStep[0m  [297/339], [94mLoss[0m : 2.23283
[1mStep[0m  [330/339], [94mLoss[0m : 2.36926

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24571
[1mStep[0m  [33/339], [94mLoss[0m : 2.31452
[1mStep[0m  [66/339], [94mLoss[0m : 2.19759
[1mStep[0m  [99/339], [94mLoss[0m : 2.77875
[1mStep[0m  [132/339], [94mLoss[0m : 2.38834
[1mStep[0m  [165/339], [94mLoss[0m : 2.67502
[1mStep[0m  [198/339], [94mLoss[0m : 2.87481
[1mStep[0m  [231/339], [94mLoss[0m : 2.27320
[1mStep[0m  [264/339], [94mLoss[0m : 2.78421
[1mStep[0m  [297/339], [94mLoss[0m : 2.23584
[1mStep[0m  [330/339], [94mLoss[0m : 2.63840

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42392
[1mStep[0m  [33/339], [94mLoss[0m : 3.29500
[1mStep[0m  [66/339], [94mLoss[0m : 2.52469
[1mStep[0m  [99/339], [94mLoss[0m : 2.15541
[1mStep[0m  [132/339], [94mLoss[0m : 2.06902
[1mStep[0m  [165/339], [94mLoss[0m : 2.49096
[1mStep[0m  [198/339], [94mLoss[0m : 2.57526
[1mStep[0m  [231/339], [94mLoss[0m : 2.41328
[1mStep[0m  [264/339], [94mLoss[0m : 2.40128
[1mStep[0m  [297/339], [94mLoss[0m : 2.40456
[1mStep[0m  [330/339], [94mLoss[0m : 2.12437

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31144
[1mStep[0m  [33/339], [94mLoss[0m : 2.21163
[1mStep[0m  [66/339], [94mLoss[0m : 2.16221
[1mStep[0m  [99/339], [94mLoss[0m : 2.57596
[1mStep[0m  [132/339], [94mLoss[0m : 2.90967
[1mStep[0m  [165/339], [94mLoss[0m : 2.37417
[1mStep[0m  [198/339], [94mLoss[0m : 2.61041
[1mStep[0m  [231/339], [94mLoss[0m : 2.22097
[1mStep[0m  [264/339], [94mLoss[0m : 2.30232
[1mStep[0m  [297/339], [94mLoss[0m : 2.41818
[1mStep[0m  [330/339], [94mLoss[0m : 2.28364

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42080
[1mStep[0m  [33/339], [94mLoss[0m : 2.02747
[1mStep[0m  [66/339], [94mLoss[0m : 1.86274
[1mStep[0m  [99/339], [94mLoss[0m : 2.49855
[1mStep[0m  [132/339], [94mLoss[0m : 2.27695
[1mStep[0m  [165/339], [94mLoss[0m : 2.35007
[1mStep[0m  [198/339], [94mLoss[0m : 2.42575
[1mStep[0m  [231/339], [94mLoss[0m : 2.58359
[1mStep[0m  [264/339], [94mLoss[0m : 2.79984
[1mStep[0m  [297/339], [94mLoss[0m : 2.74124
[1mStep[0m  [330/339], [94mLoss[0m : 2.57298

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58822
[1mStep[0m  [33/339], [94mLoss[0m : 1.79971
[1mStep[0m  [66/339], [94mLoss[0m : 2.66252
[1mStep[0m  [99/339], [94mLoss[0m : 2.29140
[1mStep[0m  [132/339], [94mLoss[0m : 1.96723
[1mStep[0m  [165/339], [94mLoss[0m : 3.10269
[1mStep[0m  [198/339], [94mLoss[0m : 2.48112
[1mStep[0m  [231/339], [94mLoss[0m : 2.05717
[1mStep[0m  [264/339], [94mLoss[0m : 2.30157
[1mStep[0m  [297/339], [94mLoss[0m : 3.01052
[1mStep[0m  [330/339], [94mLoss[0m : 2.22543

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50978
[1mStep[0m  [33/339], [94mLoss[0m : 2.11234
[1mStep[0m  [66/339], [94mLoss[0m : 2.30827
[1mStep[0m  [99/339], [94mLoss[0m : 3.31114
[1mStep[0m  [132/339], [94mLoss[0m : 2.71463
[1mStep[0m  [165/339], [94mLoss[0m : 2.32627
[1mStep[0m  [198/339], [94mLoss[0m : 2.45320
[1mStep[0m  [231/339], [94mLoss[0m : 2.29002
[1mStep[0m  [264/339], [94mLoss[0m : 2.33022
[1mStep[0m  [297/339], [94mLoss[0m : 3.05460
[1mStep[0m  [330/339], [94mLoss[0m : 2.44613

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21504
[1mStep[0m  [33/339], [94mLoss[0m : 2.39946
[1mStep[0m  [66/339], [94mLoss[0m : 2.06103
[1mStep[0m  [99/339], [94mLoss[0m : 2.30700
[1mStep[0m  [132/339], [94mLoss[0m : 2.34622
[1mStep[0m  [165/339], [94mLoss[0m : 2.36688
[1mStep[0m  [198/339], [94mLoss[0m : 2.62554
[1mStep[0m  [231/339], [94mLoss[0m : 2.19189
[1mStep[0m  [264/339], [94mLoss[0m : 2.15853
[1mStep[0m  [297/339], [94mLoss[0m : 2.97280
[1mStep[0m  [330/339], [94mLoss[0m : 2.13886

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73318
[1mStep[0m  [33/339], [94mLoss[0m : 2.26371
[1mStep[0m  [66/339], [94mLoss[0m : 1.88155
[1mStep[0m  [99/339], [94mLoss[0m : 2.87437
[1mStep[0m  [132/339], [94mLoss[0m : 2.46474
[1mStep[0m  [165/339], [94mLoss[0m : 2.36735
[1mStep[0m  [198/339], [94mLoss[0m : 2.20536
[1mStep[0m  [231/339], [94mLoss[0m : 2.69457
[1mStep[0m  [264/339], [94mLoss[0m : 2.13947
[1mStep[0m  [297/339], [94mLoss[0m : 2.62877
[1mStep[0m  [330/339], [94mLoss[0m : 2.18528

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01610
[1mStep[0m  [33/339], [94mLoss[0m : 1.59139
[1mStep[0m  [66/339], [94mLoss[0m : 2.23072
[1mStep[0m  [99/339], [94mLoss[0m : 3.01697
[1mStep[0m  [132/339], [94mLoss[0m : 2.91491
[1mStep[0m  [165/339], [94mLoss[0m : 2.28856
[1mStep[0m  [198/339], [94mLoss[0m : 1.99473
[1mStep[0m  [231/339], [94mLoss[0m : 2.75662
[1mStep[0m  [264/339], [94mLoss[0m : 2.39426
[1mStep[0m  [297/339], [94mLoss[0m : 2.67884
[1mStep[0m  [330/339], [94mLoss[0m : 2.31738

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35097
[1mStep[0m  [33/339], [94mLoss[0m : 2.95736
[1mStep[0m  [66/339], [94mLoss[0m : 2.36469
[1mStep[0m  [99/339], [94mLoss[0m : 1.88003
[1mStep[0m  [132/339], [94mLoss[0m : 2.13944
[1mStep[0m  [165/339], [94mLoss[0m : 2.04300
[1mStep[0m  [198/339], [94mLoss[0m : 1.77713
[1mStep[0m  [231/339], [94mLoss[0m : 2.36639
[1mStep[0m  [264/339], [94mLoss[0m : 2.48667
[1mStep[0m  [297/339], [94mLoss[0m : 2.28697
[1mStep[0m  [330/339], [94mLoss[0m : 1.95740

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.318, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35119
[1mStep[0m  [33/339], [94mLoss[0m : 2.67090
[1mStep[0m  [66/339], [94mLoss[0m : 2.62387
[1mStep[0m  [99/339], [94mLoss[0m : 2.40098
[1mStep[0m  [132/339], [94mLoss[0m : 2.33536
[1mStep[0m  [165/339], [94mLoss[0m : 2.31342
[1mStep[0m  [198/339], [94mLoss[0m : 2.43920
[1mStep[0m  [231/339], [94mLoss[0m : 2.15654
[1mStep[0m  [264/339], [94mLoss[0m : 2.84805
[1mStep[0m  [297/339], [94mLoss[0m : 2.27249
[1mStep[0m  [330/339], [94mLoss[0m : 2.70182

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60233
[1mStep[0m  [33/339], [94mLoss[0m : 1.99621
[1mStep[0m  [66/339], [94mLoss[0m : 2.87194
[1mStep[0m  [99/339], [94mLoss[0m : 2.23022
[1mStep[0m  [132/339], [94mLoss[0m : 2.77611
[1mStep[0m  [165/339], [94mLoss[0m : 1.98060
[1mStep[0m  [198/339], [94mLoss[0m : 2.36907
[1mStep[0m  [231/339], [94mLoss[0m : 2.22536
[1mStep[0m  [264/339], [94mLoss[0m : 2.21902
[1mStep[0m  [297/339], [94mLoss[0m : 3.15384
[1mStep[0m  [330/339], [94mLoss[0m : 2.45355

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3313844995161075
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.14731
[1mStep[0m  [33/339], [94mLoss[0m : 3.43258
[1mStep[0m  [66/339], [94mLoss[0m : 2.50280
[1mStep[0m  [99/339], [94mLoss[0m : 2.34326
[1mStep[0m  [132/339], [94mLoss[0m : 2.32232
[1mStep[0m  [165/339], [94mLoss[0m : 2.46060
[1mStep[0m  [198/339], [94mLoss[0m : 2.21421
[1mStep[0m  [231/339], [94mLoss[0m : 2.33551
[1mStep[0m  [264/339], [94mLoss[0m : 2.46067
[1mStep[0m  [297/339], [94mLoss[0m : 2.90580
[1mStep[0m  [330/339], [94mLoss[0m : 2.46017

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27307
[1mStep[0m  [33/339], [94mLoss[0m : 2.28885
[1mStep[0m  [66/339], [94mLoss[0m : 2.43044
[1mStep[0m  [99/339], [94mLoss[0m : 1.96321
[1mStep[0m  [132/339], [94mLoss[0m : 1.89453
[1mStep[0m  [165/339], [94mLoss[0m : 1.98340
[1mStep[0m  [198/339], [94mLoss[0m : 2.69585
[1mStep[0m  [231/339], [94mLoss[0m : 2.33021
[1mStep[0m  [264/339], [94mLoss[0m : 2.41778
[1mStep[0m  [297/339], [94mLoss[0m : 2.43472
[1mStep[0m  [330/339], [94mLoss[0m : 2.97119

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.524, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36689
[1mStep[0m  [33/339], [94mLoss[0m : 2.81243
[1mStep[0m  [66/339], [94mLoss[0m : 2.05269
[1mStep[0m  [99/339], [94mLoss[0m : 2.84266
[1mStep[0m  [132/339], [94mLoss[0m : 2.05663
[1mStep[0m  [165/339], [94mLoss[0m : 2.59798
[1mStep[0m  [198/339], [94mLoss[0m : 2.47336
[1mStep[0m  [231/339], [94mLoss[0m : 3.01014
[1mStep[0m  [264/339], [94mLoss[0m : 2.31274
[1mStep[0m  [297/339], [94mLoss[0m : 2.34198
[1mStep[0m  [330/339], [94mLoss[0m : 2.17788

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

