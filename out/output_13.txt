no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  13
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.69740
[1mStep[0m  [16/169], [94mLoss[0m : 6.94103
[1mStep[0m  [32/169], [94mLoss[0m : 4.12466
[1mStep[0m  [48/169], [94mLoss[0m : 2.98847
[1mStep[0m  [64/169], [94mLoss[0m : 2.63692
[1mStep[0m  [80/169], [94mLoss[0m : 2.54446
[1mStep[0m  [96/169], [94mLoss[0m : 2.83941
[1mStep[0m  [112/169], [94mLoss[0m : 2.52703
[1mStep[0m  [128/169], [94mLoss[0m : 2.35528
[1mStep[0m  [144/169], [94mLoss[0m : 2.20807
[1mStep[0m  [160/169], [94mLoss[0m : 2.21087

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.363, [92mTest[0m: 10.980, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61168
[1mStep[0m  [16/169], [94mLoss[0m : 2.27943
[1mStep[0m  [32/169], [94mLoss[0m : 2.61926
[1mStep[0m  [48/169], [94mLoss[0m : 2.01246
[1mStep[0m  [64/169], [94mLoss[0m : 2.72068
[1mStep[0m  [80/169], [94mLoss[0m : 2.89996
[1mStep[0m  [96/169], [94mLoss[0m : 2.91214
[1mStep[0m  [112/169], [94mLoss[0m : 2.40568
[1mStep[0m  [128/169], [94mLoss[0m : 2.43070
[1mStep[0m  [144/169], [94mLoss[0m : 2.62246
[1mStep[0m  [160/169], [94mLoss[0m : 2.30894

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.379, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75120
[1mStep[0m  [16/169], [94mLoss[0m : 2.60715
[1mStep[0m  [32/169], [94mLoss[0m : 2.52413
[1mStep[0m  [48/169], [94mLoss[0m : 2.72042
[1mStep[0m  [64/169], [94mLoss[0m : 2.38881
[1mStep[0m  [80/169], [94mLoss[0m : 2.57114
[1mStep[0m  [96/169], [94mLoss[0m : 2.85371
[1mStep[0m  [112/169], [94mLoss[0m : 2.49058
[1mStep[0m  [128/169], [94mLoss[0m : 2.75560
[1mStep[0m  [144/169], [94mLoss[0m : 2.37203
[1mStep[0m  [160/169], [94mLoss[0m : 2.03540

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40702
[1mStep[0m  [16/169], [94mLoss[0m : 2.37791
[1mStep[0m  [32/169], [94mLoss[0m : 2.41228
[1mStep[0m  [48/169], [94mLoss[0m : 2.67276
[1mStep[0m  [64/169], [94mLoss[0m : 2.66308
[1mStep[0m  [80/169], [94mLoss[0m : 2.26725
[1mStep[0m  [96/169], [94mLoss[0m : 2.10779
[1mStep[0m  [112/169], [94mLoss[0m : 3.07149
[1mStep[0m  [128/169], [94mLoss[0m : 2.74189
[1mStep[0m  [144/169], [94mLoss[0m : 2.39699
[1mStep[0m  [160/169], [94mLoss[0m : 2.59655

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35473
[1mStep[0m  [16/169], [94mLoss[0m : 2.17806
[1mStep[0m  [32/169], [94mLoss[0m : 2.23654
[1mStep[0m  [48/169], [94mLoss[0m : 2.32588
[1mStep[0m  [64/169], [94mLoss[0m : 2.40743
[1mStep[0m  [80/169], [94mLoss[0m : 1.93896
[1mStep[0m  [96/169], [94mLoss[0m : 2.10786
[1mStep[0m  [112/169], [94mLoss[0m : 2.96342
[1mStep[0m  [128/169], [94mLoss[0m : 2.50052
[1mStep[0m  [144/169], [94mLoss[0m : 2.54266
[1mStep[0m  [160/169], [94mLoss[0m : 2.61597

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64983
[1mStep[0m  [16/169], [94mLoss[0m : 2.45906
[1mStep[0m  [32/169], [94mLoss[0m : 2.19804
[1mStep[0m  [48/169], [94mLoss[0m : 2.72065
[1mStep[0m  [64/169], [94mLoss[0m : 2.46569
[1mStep[0m  [80/169], [94mLoss[0m : 2.55968
[1mStep[0m  [96/169], [94mLoss[0m : 2.27355
[1mStep[0m  [112/169], [94mLoss[0m : 2.61872
[1mStep[0m  [128/169], [94mLoss[0m : 2.48664
[1mStep[0m  [144/169], [94mLoss[0m : 2.72426
[1mStep[0m  [160/169], [94mLoss[0m : 2.28865

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63545
[1mStep[0m  [16/169], [94mLoss[0m : 2.41336
[1mStep[0m  [32/169], [94mLoss[0m : 2.54452
[1mStep[0m  [48/169], [94mLoss[0m : 2.34373
[1mStep[0m  [64/169], [94mLoss[0m : 2.85580
[1mStep[0m  [80/169], [94mLoss[0m : 2.55559
[1mStep[0m  [96/169], [94mLoss[0m : 2.45785
[1mStep[0m  [112/169], [94mLoss[0m : 2.10911
[1mStep[0m  [128/169], [94mLoss[0m : 2.26681
[1mStep[0m  [144/169], [94mLoss[0m : 2.57784
[1mStep[0m  [160/169], [94mLoss[0m : 2.45886

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62178
[1mStep[0m  [16/169], [94mLoss[0m : 2.38221
[1mStep[0m  [32/169], [94mLoss[0m : 2.02874
[1mStep[0m  [48/169], [94mLoss[0m : 2.63201
[1mStep[0m  [64/169], [94mLoss[0m : 2.46840
[1mStep[0m  [80/169], [94mLoss[0m : 2.49803
[1mStep[0m  [96/169], [94mLoss[0m : 2.60762
[1mStep[0m  [112/169], [94mLoss[0m : 2.27912
[1mStep[0m  [128/169], [94mLoss[0m : 2.32985
[1mStep[0m  [144/169], [94mLoss[0m : 2.60780
[1mStep[0m  [160/169], [94mLoss[0m : 2.12058

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14498
[1mStep[0m  [16/169], [94mLoss[0m : 2.71981
[1mStep[0m  [32/169], [94mLoss[0m : 2.25874
[1mStep[0m  [48/169], [94mLoss[0m : 2.91670
[1mStep[0m  [64/169], [94mLoss[0m : 2.29246
[1mStep[0m  [80/169], [94mLoss[0m : 2.38796
[1mStep[0m  [96/169], [94mLoss[0m : 2.45596
[1mStep[0m  [112/169], [94mLoss[0m : 2.26517
[1mStep[0m  [128/169], [94mLoss[0m : 1.90038
[1mStep[0m  [144/169], [94mLoss[0m : 2.62540
[1mStep[0m  [160/169], [94mLoss[0m : 2.32477

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70334
[1mStep[0m  [16/169], [94mLoss[0m : 2.69466
[1mStep[0m  [32/169], [94mLoss[0m : 2.66141
[1mStep[0m  [48/169], [94mLoss[0m : 2.61458
[1mStep[0m  [64/169], [94mLoss[0m : 2.48291
[1mStep[0m  [80/169], [94mLoss[0m : 2.63583
[1mStep[0m  [96/169], [94mLoss[0m : 2.43435
[1mStep[0m  [112/169], [94mLoss[0m : 2.66056
[1mStep[0m  [128/169], [94mLoss[0m : 2.97441
[1mStep[0m  [144/169], [94mLoss[0m : 2.40217
[1mStep[0m  [160/169], [94mLoss[0m : 2.21406

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81308
[1mStep[0m  [16/169], [94mLoss[0m : 2.21192
[1mStep[0m  [32/169], [94mLoss[0m : 2.62088
[1mStep[0m  [48/169], [94mLoss[0m : 2.48229
[1mStep[0m  [64/169], [94mLoss[0m : 2.62333
[1mStep[0m  [80/169], [94mLoss[0m : 2.34957
[1mStep[0m  [96/169], [94mLoss[0m : 2.52726
[1mStep[0m  [112/169], [94mLoss[0m : 2.07163
[1mStep[0m  [128/169], [94mLoss[0m : 2.24227
[1mStep[0m  [144/169], [94mLoss[0m : 2.58225
[1mStep[0m  [160/169], [94mLoss[0m : 2.94420

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80553
[1mStep[0m  [16/169], [94mLoss[0m : 2.17372
[1mStep[0m  [32/169], [94mLoss[0m : 2.65498
[1mStep[0m  [48/169], [94mLoss[0m : 2.78844
[1mStep[0m  [64/169], [94mLoss[0m : 2.30576
[1mStep[0m  [80/169], [94mLoss[0m : 2.57324
[1mStep[0m  [96/169], [94mLoss[0m : 2.70516
[1mStep[0m  [112/169], [94mLoss[0m : 2.57698
[1mStep[0m  [128/169], [94mLoss[0m : 2.61253
[1mStep[0m  [144/169], [94mLoss[0m : 2.48325
[1mStep[0m  [160/169], [94mLoss[0m : 2.44816

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66380
[1mStep[0m  [16/169], [94mLoss[0m : 2.17913
[1mStep[0m  [32/169], [94mLoss[0m : 2.38110
[1mStep[0m  [48/169], [94mLoss[0m : 2.30788
[1mStep[0m  [64/169], [94mLoss[0m : 2.23048
[1mStep[0m  [80/169], [94mLoss[0m : 2.77907
[1mStep[0m  [96/169], [94mLoss[0m : 2.39540
[1mStep[0m  [112/169], [94mLoss[0m : 2.26370
[1mStep[0m  [128/169], [94mLoss[0m : 2.55514
[1mStep[0m  [144/169], [94mLoss[0m : 2.52483
[1mStep[0m  [160/169], [94mLoss[0m : 2.10882

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.317, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18015
[1mStep[0m  [16/169], [94mLoss[0m : 2.18275
[1mStep[0m  [32/169], [94mLoss[0m : 2.41204
[1mStep[0m  [48/169], [94mLoss[0m : 2.54591
[1mStep[0m  [64/169], [94mLoss[0m : 2.47463
[1mStep[0m  [80/169], [94mLoss[0m : 2.10656
[1mStep[0m  [96/169], [94mLoss[0m : 2.79060
[1mStep[0m  [112/169], [94mLoss[0m : 3.00961
[1mStep[0m  [128/169], [94mLoss[0m : 2.45177
[1mStep[0m  [144/169], [94mLoss[0m : 2.18448
[1mStep[0m  [160/169], [94mLoss[0m : 2.20522

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40619
[1mStep[0m  [16/169], [94mLoss[0m : 2.14189
[1mStep[0m  [32/169], [94mLoss[0m : 2.52042
[1mStep[0m  [48/169], [94mLoss[0m : 2.70804
[1mStep[0m  [64/169], [94mLoss[0m : 2.74335
[1mStep[0m  [80/169], [94mLoss[0m : 2.22903
[1mStep[0m  [96/169], [94mLoss[0m : 2.21241
[1mStep[0m  [112/169], [94mLoss[0m : 2.25457
[1mStep[0m  [128/169], [94mLoss[0m : 2.72955
[1mStep[0m  [144/169], [94mLoss[0m : 2.38318
[1mStep[0m  [160/169], [94mLoss[0m : 2.19703

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76651
[1mStep[0m  [16/169], [94mLoss[0m : 2.37327
[1mStep[0m  [32/169], [94mLoss[0m : 2.49132
[1mStep[0m  [48/169], [94mLoss[0m : 2.34720
[1mStep[0m  [64/169], [94mLoss[0m : 2.26399
[1mStep[0m  [80/169], [94mLoss[0m : 2.87019
[1mStep[0m  [96/169], [94mLoss[0m : 2.68060
[1mStep[0m  [112/169], [94mLoss[0m : 2.46586
[1mStep[0m  [128/169], [94mLoss[0m : 2.49429
[1mStep[0m  [144/169], [94mLoss[0m : 2.33654
[1mStep[0m  [160/169], [94mLoss[0m : 2.17136

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.311, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55862
[1mStep[0m  [16/169], [94mLoss[0m : 2.66916
[1mStep[0m  [32/169], [94mLoss[0m : 2.46425
[1mStep[0m  [48/169], [94mLoss[0m : 2.62650
[1mStep[0m  [64/169], [94mLoss[0m : 3.21998
[1mStep[0m  [80/169], [94mLoss[0m : 2.42525
[1mStep[0m  [96/169], [94mLoss[0m : 2.51834
[1mStep[0m  [112/169], [94mLoss[0m : 2.28868
[1mStep[0m  [128/169], [94mLoss[0m : 2.18032
[1mStep[0m  [144/169], [94mLoss[0m : 2.36313
[1mStep[0m  [160/169], [94mLoss[0m : 2.08313

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56765
[1mStep[0m  [16/169], [94mLoss[0m : 2.52218
[1mStep[0m  [32/169], [94mLoss[0m : 2.40764
[1mStep[0m  [48/169], [94mLoss[0m : 2.12874
[1mStep[0m  [64/169], [94mLoss[0m : 2.36626
[1mStep[0m  [80/169], [94mLoss[0m : 2.07423
[1mStep[0m  [96/169], [94mLoss[0m : 2.31366
[1mStep[0m  [112/169], [94mLoss[0m : 2.41710
[1mStep[0m  [128/169], [94mLoss[0m : 2.31735
[1mStep[0m  [144/169], [94mLoss[0m : 2.51857
[1mStep[0m  [160/169], [94mLoss[0m : 2.33363

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.315, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51086
[1mStep[0m  [16/169], [94mLoss[0m : 2.26188
[1mStep[0m  [32/169], [94mLoss[0m : 2.85355
[1mStep[0m  [48/169], [94mLoss[0m : 2.36325
[1mStep[0m  [64/169], [94mLoss[0m : 2.23091
[1mStep[0m  [80/169], [94mLoss[0m : 2.40790
[1mStep[0m  [96/169], [94mLoss[0m : 2.83872
[1mStep[0m  [112/169], [94mLoss[0m : 2.56474
[1mStep[0m  [128/169], [94mLoss[0m : 2.38816
[1mStep[0m  [144/169], [94mLoss[0m : 2.60776
[1mStep[0m  [160/169], [94mLoss[0m : 2.27178

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.312, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10394
[1mStep[0m  [16/169], [94mLoss[0m : 2.49534
[1mStep[0m  [32/169], [94mLoss[0m : 2.26002
[1mStep[0m  [48/169], [94mLoss[0m : 2.65052
[1mStep[0m  [64/169], [94mLoss[0m : 2.39551
[1mStep[0m  [80/169], [94mLoss[0m : 2.36175
[1mStep[0m  [96/169], [94mLoss[0m : 2.33139
[1mStep[0m  [112/169], [94mLoss[0m : 2.37625
[1mStep[0m  [128/169], [94mLoss[0m : 2.21796
[1mStep[0m  [144/169], [94mLoss[0m : 2.37795
[1mStep[0m  [160/169], [94mLoss[0m : 2.21369

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68003
[1mStep[0m  [16/169], [94mLoss[0m : 2.28679
[1mStep[0m  [32/169], [94mLoss[0m : 2.54605
[1mStep[0m  [48/169], [94mLoss[0m : 2.81300
[1mStep[0m  [64/169], [94mLoss[0m : 2.70409
[1mStep[0m  [80/169], [94mLoss[0m : 2.52408
[1mStep[0m  [96/169], [94mLoss[0m : 2.47672
[1mStep[0m  [112/169], [94mLoss[0m : 2.26910
[1mStep[0m  [128/169], [94mLoss[0m : 1.84981
[1mStep[0m  [144/169], [94mLoss[0m : 2.57368
[1mStep[0m  [160/169], [94mLoss[0m : 2.53657

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40729
[1mStep[0m  [16/169], [94mLoss[0m : 2.59621
[1mStep[0m  [32/169], [94mLoss[0m : 2.54722
[1mStep[0m  [48/169], [94mLoss[0m : 2.68270
[1mStep[0m  [64/169], [94mLoss[0m : 2.49857
[1mStep[0m  [80/169], [94mLoss[0m : 2.45066
[1mStep[0m  [96/169], [94mLoss[0m : 2.30526
[1mStep[0m  [112/169], [94mLoss[0m : 2.84897
[1mStep[0m  [128/169], [94mLoss[0m : 2.55150
[1mStep[0m  [144/169], [94mLoss[0m : 2.30393
[1mStep[0m  [160/169], [94mLoss[0m : 2.40829

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11302
[1mStep[0m  [16/169], [94mLoss[0m : 2.84988
[1mStep[0m  [32/169], [94mLoss[0m : 2.85559
[1mStep[0m  [48/169], [94mLoss[0m : 2.83844
[1mStep[0m  [64/169], [94mLoss[0m : 2.88472
[1mStep[0m  [80/169], [94mLoss[0m : 2.32274
[1mStep[0m  [96/169], [94mLoss[0m : 2.68919
[1mStep[0m  [112/169], [94mLoss[0m : 2.41886
[1mStep[0m  [128/169], [94mLoss[0m : 2.96533
[1mStep[0m  [144/169], [94mLoss[0m : 2.74219
[1mStep[0m  [160/169], [94mLoss[0m : 2.65817

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50439
[1mStep[0m  [16/169], [94mLoss[0m : 2.59195
[1mStep[0m  [32/169], [94mLoss[0m : 2.48298
[1mStep[0m  [48/169], [94mLoss[0m : 2.24616
[1mStep[0m  [64/169], [94mLoss[0m : 2.16870
[1mStep[0m  [80/169], [94mLoss[0m : 2.15910
[1mStep[0m  [96/169], [94mLoss[0m : 2.47180
[1mStep[0m  [112/169], [94mLoss[0m : 1.96028
[1mStep[0m  [128/169], [94mLoss[0m : 2.38059
[1mStep[0m  [144/169], [94mLoss[0m : 2.42537
[1mStep[0m  [160/169], [94mLoss[0m : 2.55355

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28074
[1mStep[0m  [16/169], [94mLoss[0m : 2.44923
[1mStep[0m  [32/169], [94mLoss[0m : 2.43366
[1mStep[0m  [48/169], [94mLoss[0m : 2.37600
[1mStep[0m  [64/169], [94mLoss[0m : 2.54391
[1mStep[0m  [80/169], [94mLoss[0m : 2.27197
[1mStep[0m  [96/169], [94mLoss[0m : 2.34531
[1mStep[0m  [112/169], [94mLoss[0m : 2.14193
[1mStep[0m  [128/169], [94mLoss[0m : 2.16687
[1mStep[0m  [144/169], [94mLoss[0m : 2.65762
[1mStep[0m  [160/169], [94mLoss[0m : 2.66625

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.326, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51012
[1mStep[0m  [16/169], [94mLoss[0m : 2.42288
[1mStep[0m  [32/169], [94mLoss[0m : 2.27510
[1mStep[0m  [48/169], [94mLoss[0m : 2.35203
[1mStep[0m  [64/169], [94mLoss[0m : 2.36104
[1mStep[0m  [80/169], [94mLoss[0m : 2.47171
[1mStep[0m  [96/169], [94mLoss[0m : 2.49405
[1mStep[0m  [112/169], [94mLoss[0m : 2.77344
[1mStep[0m  [128/169], [94mLoss[0m : 2.65657
[1mStep[0m  [144/169], [94mLoss[0m : 2.40536
[1mStep[0m  [160/169], [94mLoss[0m : 2.33371

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29280
[1mStep[0m  [16/169], [94mLoss[0m : 2.49399
[1mStep[0m  [32/169], [94mLoss[0m : 2.64357
[1mStep[0m  [48/169], [94mLoss[0m : 1.81595
[1mStep[0m  [64/169], [94mLoss[0m : 2.34032
[1mStep[0m  [80/169], [94mLoss[0m : 2.79044
[1mStep[0m  [96/169], [94mLoss[0m : 1.99689
[1mStep[0m  [112/169], [94mLoss[0m : 2.19868
[1mStep[0m  [128/169], [94mLoss[0m : 2.25740
[1mStep[0m  [144/169], [94mLoss[0m : 3.26082
[1mStep[0m  [160/169], [94mLoss[0m : 2.36982

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70891
[1mStep[0m  [16/169], [94mLoss[0m : 2.59966
[1mStep[0m  [32/169], [94mLoss[0m : 2.75018
[1mStep[0m  [48/169], [94mLoss[0m : 2.62580
[1mStep[0m  [64/169], [94mLoss[0m : 2.79055
[1mStep[0m  [80/169], [94mLoss[0m : 2.60273
[1mStep[0m  [96/169], [94mLoss[0m : 3.14694
[1mStep[0m  [112/169], [94mLoss[0m : 2.39996
[1mStep[0m  [128/169], [94mLoss[0m : 2.48769
[1mStep[0m  [144/169], [94mLoss[0m : 2.84805
[1mStep[0m  [160/169], [94mLoss[0m : 2.25894

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03776
[1mStep[0m  [16/169], [94mLoss[0m : 3.01635
[1mStep[0m  [32/169], [94mLoss[0m : 2.02587
[1mStep[0m  [48/169], [94mLoss[0m : 2.31363
[1mStep[0m  [64/169], [94mLoss[0m : 2.12589
[1mStep[0m  [80/169], [94mLoss[0m : 2.49431
[1mStep[0m  [96/169], [94mLoss[0m : 2.62582
[1mStep[0m  [112/169], [94mLoss[0m : 2.50439
[1mStep[0m  [128/169], [94mLoss[0m : 2.02119
[1mStep[0m  [144/169], [94mLoss[0m : 2.42956
[1mStep[0m  [160/169], [94mLoss[0m : 2.12494

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55931
[1mStep[0m  [16/169], [94mLoss[0m : 2.55632
[1mStep[0m  [32/169], [94mLoss[0m : 2.26664
[1mStep[0m  [48/169], [94mLoss[0m : 2.21661
[1mStep[0m  [64/169], [94mLoss[0m : 2.07676
[1mStep[0m  [80/169], [94mLoss[0m : 2.34774
[1mStep[0m  [96/169], [94mLoss[0m : 2.52964
[1mStep[0m  [112/169], [94mLoss[0m : 2.17275
[1mStep[0m  [128/169], [94mLoss[0m : 2.79846
[1mStep[0m  [144/169], [94mLoss[0m : 2.30701
[1mStep[0m  [160/169], [94mLoss[0m : 2.48493

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.319
====================================

Phase 1 - Evaluation MAE:  2.318949797323772
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 1.98488
[1mStep[0m  [16/169], [94mLoss[0m : 2.68716
[1mStep[0m  [32/169], [94mLoss[0m : 2.31221
[1mStep[0m  [48/169], [94mLoss[0m : 2.99335
[1mStep[0m  [64/169], [94mLoss[0m : 2.29273
[1mStep[0m  [80/169], [94mLoss[0m : 2.22386
[1mStep[0m  [96/169], [94mLoss[0m : 2.52026
[1mStep[0m  [112/169], [94mLoss[0m : 2.65319
[1mStep[0m  [128/169], [94mLoss[0m : 2.49669
[1mStep[0m  [144/169], [94mLoss[0m : 2.09404
[1mStep[0m  [160/169], [94mLoss[0m : 2.86492

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26114
[1mStep[0m  [16/169], [94mLoss[0m : 2.13422
[1mStep[0m  [32/169], [94mLoss[0m : 2.02260
[1mStep[0m  [48/169], [94mLoss[0m : 2.14459
[1mStep[0m  [64/169], [94mLoss[0m : 2.32272
[1mStep[0m  [80/169], [94mLoss[0m : 2.91337
[1mStep[0m  [96/169], [94mLoss[0m : 2.41391
[1mStep[0m  [112/169], [94mLoss[0m : 2.32100
[1mStep[0m  [128/169], [94mLoss[0m : 2.16914
[1mStep[0m  [144/169], [94mLoss[0m : 2.22464
[1mStep[0m  [160/169], [94mLoss[0m : 2.34279

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18523
[1mStep[0m  [16/169], [94mLoss[0m : 2.11911
[1mStep[0m  [32/169], [94mLoss[0m : 2.23544
[1mStep[0m  [48/169], [94mLoss[0m : 2.54715
[1mStep[0m  [64/169], [94mLoss[0m : 2.11531
[1mStep[0m  [80/169], [94mLoss[0m : 2.42558
[1mStep[0m  [96/169], [94mLoss[0m : 2.16654
[1mStep[0m  [112/169], [94mLoss[0m : 2.32909
[1mStep[0m  [128/169], [94mLoss[0m : 1.97222
[1mStep[0m  [144/169], [94mLoss[0m : 2.58463
[1mStep[0m  [160/169], [94mLoss[0m : 2.51648

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98832
[1mStep[0m  [16/169], [94mLoss[0m : 2.37190
[1mStep[0m  [32/169], [94mLoss[0m : 2.15589
[1mStep[0m  [48/169], [94mLoss[0m : 2.80348
[1mStep[0m  [64/169], [94mLoss[0m : 2.67802
[1mStep[0m  [80/169], [94mLoss[0m : 2.21100
[1mStep[0m  [96/169], [94mLoss[0m : 2.04806
[1mStep[0m  [112/169], [94mLoss[0m : 1.95635
[1mStep[0m  [128/169], [94mLoss[0m : 1.88777
[1mStep[0m  [144/169], [94mLoss[0m : 2.10782
[1mStep[0m  [160/169], [94mLoss[0m : 2.52715

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.246, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22347
[1mStep[0m  [16/169], [94mLoss[0m : 2.22755
[1mStep[0m  [32/169], [94mLoss[0m : 2.28083
[1mStep[0m  [48/169], [94mLoss[0m : 1.89396
[1mStep[0m  [64/169], [94mLoss[0m : 2.09221
[1mStep[0m  [80/169], [94mLoss[0m : 2.16503
[1mStep[0m  [96/169], [94mLoss[0m : 1.88554
[1mStep[0m  [112/169], [94mLoss[0m : 2.16316
[1mStep[0m  [128/169], [94mLoss[0m : 2.32134
[1mStep[0m  [144/169], [94mLoss[0m : 2.04432
[1mStep[0m  [160/169], [94mLoss[0m : 2.46109

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60112
[1mStep[0m  [16/169], [94mLoss[0m : 2.70592
[1mStep[0m  [32/169], [94mLoss[0m : 1.85246
[1mStep[0m  [48/169], [94mLoss[0m : 1.96561
[1mStep[0m  [64/169], [94mLoss[0m : 2.40008
[1mStep[0m  [80/169], [94mLoss[0m : 2.32040
[1mStep[0m  [96/169], [94mLoss[0m : 2.24636
[1mStep[0m  [112/169], [94mLoss[0m : 2.43631
[1mStep[0m  [128/169], [94mLoss[0m : 2.45982
[1mStep[0m  [144/169], [94mLoss[0m : 1.92305
[1mStep[0m  [160/169], [94mLoss[0m : 1.98846

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.116, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90787
[1mStep[0m  [16/169], [94mLoss[0m : 2.36593
[1mStep[0m  [32/169], [94mLoss[0m : 1.98951
[1mStep[0m  [48/169], [94mLoss[0m : 1.80365
[1mStep[0m  [64/169], [94mLoss[0m : 2.16199
[1mStep[0m  [80/169], [94mLoss[0m : 1.77466
[1mStep[0m  [96/169], [94mLoss[0m : 2.21896
[1mStep[0m  [112/169], [94mLoss[0m : 2.16238
[1mStep[0m  [128/169], [94mLoss[0m : 1.97923
[1mStep[0m  [144/169], [94mLoss[0m : 2.45856
[1mStep[0m  [160/169], [94mLoss[0m : 1.75888

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.052, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09172
[1mStep[0m  [16/169], [94mLoss[0m : 1.96205
[1mStep[0m  [32/169], [94mLoss[0m : 2.02685
[1mStep[0m  [48/169], [94mLoss[0m : 2.03624
[1mStep[0m  [64/169], [94mLoss[0m : 1.88305
[1mStep[0m  [80/169], [94mLoss[0m : 1.93921
[1mStep[0m  [96/169], [94mLoss[0m : 2.56350
[1mStep[0m  [112/169], [94mLoss[0m : 1.95342
[1mStep[0m  [128/169], [94mLoss[0m : 2.03982
[1mStep[0m  [144/169], [94mLoss[0m : 2.13665
[1mStep[0m  [160/169], [94mLoss[0m : 1.83430

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.392, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93930
[1mStep[0m  [16/169], [94mLoss[0m : 2.09772
[1mStep[0m  [32/169], [94mLoss[0m : 2.29749
[1mStep[0m  [48/169], [94mLoss[0m : 1.72072
[1mStep[0m  [64/169], [94mLoss[0m : 1.80305
[1mStep[0m  [80/169], [94mLoss[0m : 1.92267
[1mStep[0m  [96/169], [94mLoss[0m : 1.96434
[1mStep[0m  [112/169], [94mLoss[0m : 2.14530
[1mStep[0m  [128/169], [94mLoss[0m : 2.20876
[1mStep[0m  [144/169], [94mLoss[0m : 1.90485
[1mStep[0m  [160/169], [94mLoss[0m : 1.80830

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26459
[1mStep[0m  [16/169], [94mLoss[0m : 1.93551
[1mStep[0m  [32/169], [94mLoss[0m : 1.87625
[1mStep[0m  [48/169], [94mLoss[0m : 1.77419
[1mStep[0m  [64/169], [94mLoss[0m : 1.82979
[1mStep[0m  [80/169], [94mLoss[0m : 1.95050
[1mStep[0m  [96/169], [94mLoss[0m : 1.83183
[1mStep[0m  [112/169], [94mLoss[0m : 1.91252
[1mStep[0m  [128/169], [94mLoss[0m : 2.03068
[1mStep[0m  [144/169], [94mLoss[0m : 1.59370
[1mStep[0m  [160/169], [94mLoss[0m : 2.02027

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82838
[1mStep[0m  [16/169], [94mLoss[0m : 1.89046
[1mStep[0m  [32/169], [94mLoss[0m : 1.77604
[1mStep[0m  [48/169], [94mLoss[0m : 1.66417
[1mStep[0m  [64/169], [94mLoss[0m : 1.68761
[1mStep[0m  [80/169], [94mLoss[0m : 2.01655
[1mStep[0m  [96/169], [94mLoss[0m : 1.82417
[1mStep[0m  [112/169], [94mLoss[0m : 1.78534
[1mStep[0m  [128/169], [94mLoss[0m : 1.98601
[1mStep[0m  [144/169], [94mLoss[0m : 1.63395
[1mStep[0m  [160/169], [94mLoss[0m : 2.18788

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.863, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69447
[1mStep[0m  [16/169], [94mLoss[0m : 1.83013
[1mStep[0m  [32/169], [94mLoss[0m : 1.75985
[1mStep[0m  [48/169], [94mLoss[0m : 1.58969
[1mStep[0m  [64/169], [94mLoss[0m : 1.62667
[1mStep[0m  [80/169], [94mLoss[0m : 2.13813
[1mStep[0m  [96/169], [94mLoss[0m : 1.75733
[1mStep[0m  [112/169], [94mLoss[0m : 1.80699
[1mStep[0m  [128/169], [94mLoss[0m : 1.95340
[1mStep[0m  [144/169], [94mLoss[0m : 1.69038
[1mStep[0m  [160/169], [94mLoss[0m : 1.85495

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61011
[1mStep[0m  [16/169], [94mLoss[0m : 1.74385
[1mStep[0m  [32/169], [94mLoss[0m : 1.66226
[1mStep[0m  [48/169], [94mLoss[0m : 1.73748
[1mStep[0m  [64/169], [94mLoss[0m : 2.28602
[1mStep[0m  [80/169], [94mLoss[0m : 1.55826
[1mStep[0m  [96/169], [94mLoss[0m : 2.24550
[1mStep[0m  [112/169], [94mLoss[0m : 1.56785
[1mStep[0m  [128/169], [94mLoss[0m : 2.05381
[1mStep[0m  [144/169], [94mLoss[0m : 2.08156
[1mStep[0m  [160/169], [94mLoss[0m : 1.81680

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.785, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.80734
[1mStep[0m  [16/169], [94mLoss[0m : 1.61882
[1mStep[0m  [32/169], [94mLoss[0m : 1.82878
[1mStep[0m  [48/169], [94mLoss[0m : 2.11002
[1mStep[0m  [64/169], [94mLoss[0m : 1.78751
[1mStep[0m  [80/169], [94mLoss[0m : 1.70944
[1mStep[0m  [96/169], [94mLoss[0m : 1.76532
[1mStep[0m  [112/169], [94mLoss[0m : 1.86040
[1mStep[0m  [128/169], [94mLoss[0m : 1.52838
[1mStep[0m  [144/169], [94mLoss[0m : 1.61821
[1mStep[0m  [160/169], [94mLoss[0m : 1.80826

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76176
[1mStep[0m  [16/169], [94mLoss[0m : 1.71874
[1mStep[0m  [32/169], [94mLoss[0m : 1.61376
[1mStep[0m  [48/169], [94mLoss[0m : 2.00115
[1mStep[0m  [64/169], [94mLoss[0m : 1.52151
[1mStep[0m  [80/169], [94mLoss[0m : 1.22015
[1mStep[0m  [96/169], [94mLoss[0m : 1.46948
[1mStep[0m  [112/169], [94mLoss[0m : 1.89176
[1mStep[0m  [128/169], [94mLoss[0m : 1.68231
[1mStep[0m  [144/169], [94mLoss[0m : 1.93878
[1mStep[0m  [160/169], [94mLoss[0m : 1.67910

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65957
[1mStep[0m  [16/169], [94mLoss[0m : 1.58737
[1mStep[0m  [32/169], [94mLoss[0m : 1.57081
[1mStep[0m  [48/169], [94mLoss[0m : 1.67899
[1mStep[0m  [64/169], [94mLoss[0m : 1.60541
[1mStep[0m  [80/169], [94mLoss[0m : 1.77012
[1mStep[0m  [96/169], [94mLoss[0m : 1.45030
[1mStep[0m  [112/169], [94mLoss[0m : 1.73286
[1mStep[0m  [128/169], [94mLoss[0m : 1.75904
[1mStep[0m  [144/169], [94mLoss[0m : 1.86630
[1mStep[0m  [160/169], [94mLoss[0m : 1.81793

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.44710
[1mStep[0m  [16/169], [94mLoss[0m : 1.47587
[1mStep[0m  [32/169], [94mLoss[0m : 2.02725
[1mStep[0m  [48/169], [94mLoss[0m : 1.99321
[1mStep[0m  [64/169], [94mLoss[0m : 1.66493
[1mStep[0m  [80/169], [94mLoss[0m : 1.24742
[1mStep[0m  [96/169], [94mLoss[0m : 1.53055
[1mStep[0m  [112/169], [94mLoss[0m : 1.93299
[1mStep[0m  [128/169], [94mLoss[0m : 1.65682
[1mStep[0m  [144/169], [94mLoss[0m : 1.55444
[1mStep[0m  [160/169], [94mLoss[0m : 1.35768

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.646, [92mTest[0m: 2.497, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82209
[1mStep[0m  [16/169], [94mLoss[0m : 1.56761
[1mStep[0m  [32/169], [94mLoss[0m : 1.73598
[1mStep[0m  [48/169], [94mLoss[0m : 1.70361
[1mStep[0m  [64/169], [94mLoss[0m : 2.01803
[1mStep[0m  [80/169], [94mLoss[0m : 1.63998
[1mStep[0m  [96/169], [94mLoss[0m : 1.62132
[1mStep[0m  [112/169], [94mLoss[0m : 1.58590
[1mStep[0m  [128/169], [94mLoss[0m : 1.89244
[1mStep[0m  [144/169], [94mLoss[0m : 2.05519
[1mStep[0m  [160/169], [94mLoss[0m : 1.41161

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58754
[1mStep[0m  [16/169], [94mLoss[0m : 1.26107
[1mStep[0m  [32/169], [94mLoss[0m : 1.63280
[1mStep[0m  [48/169], [94mLoss[0m : 1.52161
[1mStep[0m  [64/169], [94mLoss[0m : 1.60827
[1mStep[0m  [80/169], [94mLoss[0m : 1.60788
[1mStep[0m  [96/169], [94mLoss[0m : 1.72937
[1mStep[0m  [112/169], [94mLoss[0m : 1.58215
[1mStep[0m  [128/169], [94mLoss[0m : 1.70963
[1mStep[0m  [144/169], [94mLoss[0m : 1.72926
[1mStep[0m  [160/169], [94mLoss[0m : 1.42861

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.589, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55319
[1mStep[0m  [16/169], [94mLoss[0m : 1.52724
[1mStep[0m  [32/169], [94mLoss[0m : 1.35759
[1mStep[0m  [48/169], [94mLoss[0m : 1.96447
[1mStep[0m  [64/169], [94mLoss[0m : 1.46386
[1mStep[0m  [80/169], [94mLoss[0m : 2.24718
[1mStep[0m  [96/169], [94mLoss[0m : 1.64701
[1mStep[0m  [112/169], [94mLoss[0m : 1.39415
[1mStep[0m  [128/169], [94mLoss[0m : 1.68969
[1mStep[0m  [144/169], [94mLoss[0m : 1.59490
[1mStep[0m  [160/169], [94mLoss[0m : 1.46784

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.558, [92mTest[0m: 2.483, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71106
[1mStep[0m  [16/169], [94mLoss[0m : 1.17210
[1mStep[0m  [32/169], [94mLoss[0m : 1.31570
[1mStep[0m  [48/169], [94mLoss[0m : 1.56843
[1mStep[0m  [64/169], [94mLoss[0m : 2.02247
[1mStep[0m  [80/169], [94mLoss[0m : 1.52505
[1mStep[0m  [96/169], [94mLoss[0m : 1.49681
[1mStep[0m  [112/169], [94mLoss[0m : 1.81614
[1mStep[0m  [128/169], [94mLoss[0m : 1.65678
[1mStep[0m  [144/169], [94mLoss[0m : 1.52210
[1mStep[0m  [160/169], [94mLoss[0m : 1.79987

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.520, [92mTest[0m: 2.509, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50565
[1mStep[0m  [16/169], [94mLoss[0m : 1.37168
[1mStep[0m  [32/169], [94mLoss[0m : 1.39762
[1mStep[0m  [48/169], [94mLoss[0m : 1.50219
[1mStep[0m  [64/169], [94mLoss[0m : 1.45768
[1mStep[0m  [80/169], [94mLoss[0m : 1.44464
[1mStep[0m  [96/169], [94mLoss[0m : 1.24782
[1mStep[0m  [112/169], [94mLoss[0m : 1.19058
[1mStep[0m  [128/169], [94mLoss[0m : 1.75921
[1mStep[0m  [144/169], [94mLoss[0m : 1.25031
[1mStep[0m  [160/169], [94mLoss[0m : 1.48699

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59273
[1mStep[0m  [16/169], [94mLoss[0m : 1.38359
[1mStep[0m  [32/169], [94mLoss[0m : 1.36225
[1mStep[0m  [48/169], [94mLoss[0m : 1.52724
[1mStep[0m  [64/169], [94mLoss[0m : 1.44682
[1mStep[0m  [80/169], [94mLoss[0m : 1.46760
[1mStep[0m  [96/169], [94mLoss[0m : 1.45713
[1mStep[0m  [112/169], [94mLoss[0m : 1.51960
[1mStep[0m  [128/169], [94mLoss[0m : 1.25053
[1mStep[0m  [144/169], [94mLoss[0m : 1.42289
[1mStep[0m  [160/169], [94mLoss[0m : 1.49040

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60950
[1mStep[0m  [16/169], [94mLoss[0m : 1.28984
[1mStep[0m  [32/169], [94mLoss[0m : 1.54917
[1mStep[0m  [48/169], [94mLoss[0m : 1.47121
[1mStep[0m  [64/169], [94mLoss[0m : 1.44721
[1mStep[0m  [80/169], [94mLoss[0m : 1.63063
[1mStep[0m  [96/169], [94mLoss[0m : 1.56063
[1mStep[0m  [112/169], [94mLoss[0m : 1.52610
[1mStep[0m  [128/169], [94mLoss[0m : 1.18700
[1mStep[0m  [144/169], [94mLoss[0m : 1.53565
[1mStep[0m  [160/169], [94mLoss[0m : 1.39274

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.550, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47983
[1mStep[0m  [16/169], [94mLoss[0m : 1.58202
[1mStep[0m  [32/169], [94mLoss[0m : 1.56153
[1mStep[0m  [48/169], [94mLoss[0m : 1.21425
[1mStep[0m  [64/169], [94mLoss[0m : 1.14982
[1mStep[0m  [80/169], [94mLoss[0m : 1.59568
[1mStep[0m  [96/169], [94mLoss[0m : 1.53828
[1mStep[0m  [112/169], [94mLoss[0m : 1.75210
[1mStep[0m  [128/169], [94mLoss[0m : 1.50349
[1mStep[0m  [144/169], [94mLoss[0m : 1.36162
[1mStep[0m  [160/169], [94mLoss[0m : 1.64319

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.580, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52758
[1mStep[0m  [16/169], [94mLoss[0m : 1.37502
[1mStep[0m  [32/169], [94mLoss[0m : 1.40558
[1mStep[0m  [48/169], [94mLoss[0m : 1.29988
[1mStep[0m  [64/169], [94mLoss[0m : 1.16205
[1mStep[0m  [80/169], [94mLoss[0m : 1.46528
[1mStep[0m  [96/169], [94mLoss[0m : 1.36091
[1mStep[0m  [112/169], [94mLoss[0m : 1.44171
[1mStep[0m  [128/169], [94mLoss[0m : 1.64921
[1mStep[0m  [144/169], [94mLoss[0m : 1.70515
[1mStep[0m  [160/169], [94mLoss[0m : 1.42380

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.549, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45591
[1mStep[0m  [16/169], [94mLoss[0m : 1.46935
[1mStep[0m  [32/169], [94mLoss[0m : 1.38970
[1mStep[0m  [48/169], [94mLoss[0m : 1.19190
[1mStep[0m  [64/169], [94mLoss[0m : 1.51276
[1mStep[0m  [80/169], [94mLoss[0m : 1.32379
[1mStep[0m  [96/169], [94mLoss[0m : 1.65690
[1mStep[0m  [112/169], [94mLoss[0m : 1.46447
[1mStep[0m  [128/169], [94mLoss[0m : 1.27673
[1mStep[0m  [144/169], [94mLoss[0m : 1.32126
[1mStep[0m  [160/169], [94mLoss[0m : 1.40786

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.410, [92mTest[0m: 2.454, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.502
====================================

Phase 2 - Evaluation MAE:  2.501616824950491
MAE score P1       2.31895
MAE score P2      2.501617
loss              1.410406
learning_rate         0.01
batch_size              64
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 11.78486
[1mStep[0m  [33/339], [94mLoss[0m : 2.45302
[1mStep[0m  [66/339], [94mLoss[0m : 2.76223
[1mStep[0m  [99/339], [94mLoss[0m : 2.41975
[1mStep[0m  [132/339], [94mLoss[0m : 2.81998
[1mStep[0m  [165/339], [94mLoss[0m : 2.57250
[1mStep[0m  [198/339], [94mLoss[0m : 2.34530
[1mStep[0m  [231/339], [94mLoss[0m : 2.01864
[1mStep[0m  [264/339], [94mLoss[0m : 2.42445
[1mStep[0m  [297/339], [94mLoss[0m : 2.76097
[1mStep[0m  [330/339], [94mLoss[0m : 2.17435

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.832, [92mTest[0m: 11.102, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41706
[1mStep[0m  [33/339], [94mLoss[0m : 2.41017
[1mStep[0m  [66/339], [94mLoss[0m : 2.56448
[1mStep[0m  [99/339], [94mLoss[0m : 2.75528
[1mStep[0m  [132/339], [94mLoss[0m : 2.45401
[1mStep[0m  [165/339], [94mLoss[0m : 3.39569
[1mStep[0m  [198/339], [94mLoss[0m : 2.20315
[1mStep[0m  [231/339], [94mLoss[0m : 3.90794
[1mStep[0m  [264/339], [94mLoss[0m : 2.98000
[1mStep[0m  [297/339], [94mLoss[0m : 2.99893
[1mStep[0m  [330/339], [94mLoss[0m : 3.12269

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.08507
[1mStep[0m  [33/339], [94mLoss[0m : 2.83061
[1mStep[0m  [66/339], [94mLoss[0m : 2.36993
[1mStep[0m  [99/339], [94mLoss[0m : 2.28073
[1mStep[0m  [132/339], [94mLoss[0m : 2.88782
[1mStep[0m  [165/339], [94mLoss[0m : 2.37518
[1mStep[0m  [198/339], [94mLoss[0m : 2.34624
[1mStep[0m  [231/339], [94mLoss[0m : 3.51352
[1mStep[0m  [264/339], [94mLoss[0m : 2.76951
[1mStep[0m  [297/339], [94mLoss[0m : 2.86823
[1mStep[0m  [330/339], [94mLoss[0m : 2.47429

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41912
[1mStep[0m  [33/339], [94mLoss[0m : 2.01187
[1mStep[0m  [66/339], [94mLoss[0m : 2.51681
[1mStep[0m  [99/339], [94mLoss[0m : 1.99995
[1mStep[0m  [132/339], [94mLoss[0m : 2.74447
[1mStep[0m  [165/339], [94mLoss[0m : 1.94732
[1mStep[0m  [198/339], [94mLoss[0m : 2.56693
[1mStep[0m  [231/339], [94mLoss[0m : 2.23433
[1mStep[0m  [264/339], [94mLoss[0m : 3.57786
[1mStep[0m  [297/339], [94mLoss[0m : 2.81732
[1mStep[0m  [330/339], [94mLoss[0m : 2.67360

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.90874
[1mStep[0m  [33/339], [94mLoss[0m : 2.31664
[1mStep[0m  [66/339], [94mLoss[0m : 2.72378
[1mStep[0m  [99/339], [94mLoss[0m : 2.55725
[1mStep[0m  [132/339], [94mLoss[0m : 2.59315
[1mStep[0m  [165/339], [94mLoss[0m : 2.99104
[1mStep[0m  [198/339], [94mLoss[0m : 2.54042
[1mStep[0m  [231/339], [94mLoss[0m : 2.35497
[1mStep[0m  [264/339], [94mLoss[0m : 2.19921
[1mStep[0m  [297/339], [94mLoss[0m : 2.42104
[1mStep[0m  [330/339], [94mLoss[0m : 2.31480

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32569
[1mStep[0m  [33/339], [94mLoss[0m : 2.49138
[1mStep[0m  [66/339], [94mLoss[0m : 2.40086
[1mStep[0m  [99/339], [94mLoss[0m : 3.13178
[1mStep[0m  [132/339], [94mLoss[0m : 2.30048
[1mStep[0m  [165/339], [94mLoss[0m : 2.04556
[1mStep[0m  [198/339], [94mLoss[0m : 2.61353
[1mStep[0m  [231/339], [94mLoss[0m : 3.28610
[1mStep[0m  [264/339], [94mLoss[0m : 2.10460
[1mStep[0m  [297/339], [94mLoss[0m : 2.64681
[1mStep[0m  [330/339], [94mLoss[0m : 2.36397

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45676
[1mStep[0m  [33/339], [94mLoss[0m : 3.03910
[1mStep[0m  [66/339], [94mLoss[0m : 3.02995
[1mStep[0m  [99/339], [94mLoss[0m : 2.21055
[1mStep[0m  [132/339], [94mLoss[0m : 2.40333
[1mStep[0m  [165/339], [94mLoss[0m : 2.32718
[1mStep[0m  [198/339], [94mLoss[0m : 2.77087
[1mStep[0m  [231/339], [94mLoss[0m : 2.82630
[1mStep[0m  [264/339], [94mLoss[0m : 1.83206
[1mStep[0m  [297/339], [94mLoss[0m : 2.22227
[1mStep[0m  [330/339], [94mLoss[0m : 2.31878

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83414
[1mStep[0m  [33/339], [94mLoss[0m : 2.72639
[1mStep[0m  [66/339], [94mLoss[0m : 1.57247
[1mStep[0m  [99/339], [94mLoss[0m : 2.61720
[1mStep[0m  [132/339], [94mLoss[0m : 2.46462
[1mStep[0m  [165/339], [94mLoss[0m : 2.00170
[1mStep[0m  [198/339], [94mLoss[0m : 2.44918
[1mStep[0m  [231/339], [94mLoss[0m : 2.32962
[1mStep[0m  [264/339], [94mLoss[0m : 2.23333
[1mStep[0m  [297/339], [94mLoss[0m : 2.42295
[1mStep[0m  [330/339], [94mLoss[0m : 2.74509

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40599
[1mStep[0m  [33/339], [94mLoss[0m : 2.66807
[1mStep[0m  [66/339], [94mLoss[0m : 3.01444
[1mStep[0m  [99/339], [94mLoss[0m : 2.55809
[1mStep[0m  [132/339], [94mLoss[0m : 2.13889
[1mStep[0m  [165/339], [94mLoss[0m : 2.43040
[1mStep[0m  [198/339], [94mLoss[0m : 2.83729
[1mStep[0m  [231/339], [94mLoss[0m : 2.39983
[1mStep[0m  [264/339], [94mLoss[0m : 2.49182
[1mStep[0m  [297/339], [94mLoss[0m : 2.26280
[1mStep[0m  [330/339], [94mLoss[0m : 2.78567

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97915
[1mStep[0m  [33/339], [94mLoss[0m : 2.76541
[1mStep[0m  [66/339], [94mLoss[0m : 2.16641
[1mStep[0m  [99/339], [94mLoss[0m : 2.32172
[1mStep[0m  [132/339], [94mLoss[0m : 2.14294
[1mStep[0m  [165/339], [94mLoss[0m : 2.84825
[1mStep[0m  [198/339], [94mLoss[0m : 2.37837
[1mStep[0m  [231/339], [94mLoss[0m : 2.58701
[1mStep[0m  [264/339], [94mLoss[0m : 2.01529
[1mStep[0m  [297/339], [94mLoss[0m : 2.63898
[1mStep[0m  [330/339], [94mLoss[0m : 2.42161

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67194
[1mStep[0m  [33/339], [94mLoss[0m : 2.52268
[1mStep[0m  [66/339], [94mLoss[0m : 1.76386
[1mStep[0m  [99/339], [94mLoss[0m : 2.21368
[1mStep[0m  [132/339], [94mLoss[0m : 2.38641
[1mStep[0m  [165/339], [94mLoss[0m : 2.18715
[1mStep[0m  [198/339], [94mLoss[0m : 2.00405
[1mStep[0m  [231/339], [94mLoss[0m : 2.98580
[1mStep[0m  [264/339], [94mLoss[0m : 2.95239
[1mStep[0m  [297/339], [94mLoss[0m : 2.34538
[1mStep[0m  [330/339], [94mLoss[0m : 2.48829

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39892
[1mStep[0m  [33/339], [94mLoss[0m : 2.36502
[1mStep[0m  [66/339], [94mLoss[0m : 2.62947
[1mStep[0m  [99/339], [94mLoss[0m : 2.44504
[1mStep[0m  [132/339], [94mLoss[0m : 1.64575
[1mStep[0m  [165/339], [94mLoss[0m : 2.35243
[1mStep[0m  [198/339], [94mLoss[0m : 2.11315
[1mStep[0m  [231/339], [94mLoss[0m : 2.03599
[1mStep[0m  [264/339], [94mLoss[0m : 2.30270
[1mStep[0m  [297/339], [94mLoss[0m : 2.25329
[1mStep[0m  [330/339], [94mLoss[0m : 2.04078

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49959
[1mStep[0m  [33/339], [94mLoss[0m : 2.22276
[1mStep[0m  [66/339], [94mLoss[0m : 2.25469
[1mStep[0m  [99/339], [94mLoss[0m : 2.50177
[1mStep[0m  [132/339], [94mLoss[0m : 1.86365
[1mStep[0m  [165/339], [94mLoss[0m : 2.67083
[1mStep[0m  [198/339], [94mLoss[0m : 2.24622
[1mStep[0m  [231/339], [94mLoss[0m : 2.18561
[1mStep[0m  [264/339], [94mLoss[0m : 2.77753
[1mStep[0m  [297/339], [94mLoss[0m : 2.42497
[1mStep[0m  [330/339], [94mLoss[0m : 2.82472

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61344
[1mStep[0m  [33/339], [94mLoss[0m : 2.37142
[1mStep[0m  [66/339], [94mLoss[0m : 2.33148
[1mStep[0m  [99/339], [94mLoss[0m : 2.50277
[1mStep[0m  [132/339], [94mLoss[0m : 3.13831
[1mStep[0m  [165/339], [94mLoss[0m : 1.63625
[1mStep[0m  [198/339], [94mLoss[0m : 2.39773
[1mStep[0m  [231/339], [94mLoss[0m : 2.91283
[1mStep[0m  [264/339], [94mLoss[0m : 1.96332
[1mStep[0m  [297/339], [94mLoss[0m : 2.22427
[1mStep[0m  [330/339], [94mLoss[0m : 1.97364

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85911
[1mStep[0m  [33/339], [94mLoss[0m : 2.54187
[1mStep[0m  [66/339], [94mLoss[0m : 2.54946
[1mStep[0m  [99/339], [94mLoss[0m : 1.91504
[1mStep[0m  [132/339], [94mLoss[0m : 2.04915
[1mStep[0m  [165/339], [94mLoss[0m : 2.50329
[1mStep[0m  [198/339], [94mLoss[0m : 1.94862
[1mStep[0m  [231/339], [94mLoss[0m : 2.44888
[1mStep[0m  [264/339], [94mLoss[0m : 2.09405
[1mStep[0m  [297/339], [94mLoss[0m : 2.18181
[1mStep[0m  [330/339], [94mLoss[0m : 1.60379

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27566
[1mStep[0m  [33/339], [94mLoss[0m : 2.52734
[1mStep[0m  [66/339], [94mLoss[0m : 2.41987
[1mStep[0m  [99/339], [94mLoss[0m : 2.26245
[1mStep[0m  [132/339], [94mLoss[0m : 2.63445
[1mStep[0m  [165/339], [94mLoss[0m : 3.05588
[1mStep[0m  [198/339], [94mLoss[0m : 2.63859
[1mStep[0m  [231/339], [94mLoss[0m : 2.90423
[1mStep[0m  [264/339], [94mLoss[0m : 2.82335
[1mStep[0m  [297/339], [94mLoss[0m : 2.24347
[1mStep[0m  [330/339], [94mLoss[0m : 2.82365

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56905
[1mStep[0m  [33/339], [94mLoss[0m : 2.41026
[1mStep[0m  [66/339], [94mLoss[0m : 2.40903
[1mStep[0m  [99/339], [94mLoss[0m : 2.69867
[1mStep[0m  [132/339], [94mLoss[0m : 1.69384
[1mStep[0m  [165/339], [94mLoss[0m : 1.92764
[1mStep[0m  [198/339], [94mLoss[0m : 2.16649
[1mStep[0m  [231/339], [94mLoss[0m : 2.35957
[1mStep[0m  [264/339], [94mLoss[0m : 2.15978
[1mStep[0m  [297/339], [94mLoss[0m : 2.95285
[1mStep[0m  [330/339], [94mLoss[0m : 2.70726

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02994
[1mStep[0m  [33/339], [94mLoss[0m : 2.81827
[1mStep[0m  [66/339], [94mLoss[0m : 1.87231
[1mStep[0m  [99/339], [94mLoss[0m : 2.95253
[1mStep[0m  [132/339], [94mLoss[0m : 2.39204
[1mStep[0m  [165/339], [94mLoss[0m : 2.77376
[1mStep[0m  [198/339], [94mLoss[0m : 1.73039
[1mStep[0m  [231/339], [94mLoss[0m : 2.24297
[1mStep[0m  [264/339], [94mLoss[0m : 2.11439
[1mStep[0m  [297/339], [94mLoss[0m : 2.90849
[1mStep[0m  [330/339], [94mLoss[0m : 2.67820

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72625
[1mStep[0m  [33/339], [94mLoss[0m : 2.70060
[1mStep[0m  [66/339], [94mLoss[0m : 2.37676
[1mStep[0m  [99/339], [94mLoss[0m : 3.12200
[1mStep[0m  [132/339], [94mLoss[0m : 2.19630
[1mStep[0m  [165/339], [94mLoss[0m : 2.77826
[1mStep[0m  [198/339], [94mLoss[0m : 2.12751
[1mStep[0m  [231/339], [94mLoss[0m : 2.72302
[1mStep[0m  [264/339], [94mLoss[0m : 1.97926
[1mStep[0m  [297/339], [94mLoss[0m : 2.41926
[1mStep[0m  [330/339], [94mLoss[0m : 1.96108

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81083
[1mStep[0m  [33/339], [94mLoss[0m : 2.12069
[1mStep[0m  [66/339], [94mLoss[0m : 2.60175
[1mStep[0m  [99/339], [94mLoss[0m : 2.01808
[1mStep[0m  [132/339], [94mLoss[0m : 2.45459
[1mStep[0m  [165/339], [94mLoss[0m : 2.99342
[1mStep[0m  [198/339], [94mLoss[0m : 2.55886
[1mStep[0m  [231/339], [94mLoss[0m : 3.10797
[1mStep[0m  [264/339], [94mLoss[0m : 2.05888
[1mStep[0m  [297/339], [94mLoss[0m : 2.17171
[1mStep[0m  [330/339], [94mLoss[0m : 3.15221

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.17813
[1mStep[0m  [33/339], [94mLoss[0m : 2.14316
[1mStep[0m  [66/339], [94mLoss[0m : 2.71455
[1mStep[0m  [99/339], [94mLoss[0m : 2.50927
[1mStep[0m  [132/339], [94mLoss[0m : 2.44350
[1mStep[0m  [165/339], [94mLoss[0m : 3.16158
[1mStep[0m  [198/339], [94mLoss[0m : 2.14548
[1mStep[0m  [231/339], [94mLoss[0m : 3.50274
[1mStep[0m  [264/339], [94mLoss[0m : 2.28729
[1mStep[0m  [297/339], [94mLoss[0m : 2.51911
[1mStep[0m  [330/339], [94mLoss[0m : 1.77363

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60074
[1mStep[0m  [33/339], [94mLoss[0m : 1.84125
[1mStep[0m  [66/339], [94mLoss[0m : 2.64799
[1mStep[0m  [99/339], [94mLoss[0m : 2.11707
[1mStep[0m  [132/339], [94mLoss[0m : 2.60379
[1mStep[0m  [165/339], [94mLoss[0m : 2.09444
[1mStep[0m  [198/339], [94mLoss[0m : 2.23708
[1mStep[0m  [231/339], [94mLoss[0m : 2.20612
[1mStep[0m  [264/339], [94mLoss[0m : 2.72135
[1mStep[0m  [297/339], [94mLoss[0m : 3.18043
[1mStep[0m  [330/339], [94mLoss[0m : 2.07664

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59586
[1mStep[0m  [33/339], [94mLoss[0m : 3.02157
[1mStep[0m  [66/339], [94mLoss[0m : 2.48792
[1mStep[0m  [99/339], [94mLoss[0m : 2.87764
[1mStep[0m  [132/339], [94mLoss[0m : 2.66444
[1mStep[0m  [165/339], [94mLoss[0m : 2.85737
[1mStep[0m  [198/339], [94mLoss[0m : 1.91335
[1mStep[0m  [231/339], [94mLoss[0m : 3.00066
[1mStep[0m  [264/339], [94mLoss[0m : 2.41836
[1mStep[0m  [297/339], [94mLoss[0m : 2.25276
[1mStep[0m  [330/339], [94mLoss[0m : 2.62152

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46931
[1mStep[0m  [33/339], [94mLoss[0m : 2.19022
[1mStep[0m  [66/339], [94mLoss[0m : 2.46825
[1mStep[0m  [99/339], [94mLoss[0m : 2.17412
[1mStep[0m  [132/339], [94mLoss[0m : 2.58628
[1mStep[0m  [165/339], [94mLoss[0m : 2.22584
[1mStep[0m  [198/339], [94mLoss[0m : 2.77446
[1mStep[0m  [231/339], [94mLoss[0m : 2.20943
[1mStep[0m  [264/339], [94mLoss[0m : 2.13456
[1mStep[0m  [297/339], [94mLoss[0m : 2.63631
[1mStep[0m  [330/339], [94mLoss[0m : 2.17624

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33130
[1mStep[0m  [33/339], [94mLoss[0m : 1.91267
[1mStep[0m  [66/339], [94mLoss[0m : 2.01494
[1mStep[0m  [99/339], [94mLoss[0m : 2.18778
[1mStep[0m  [132/339], [94mLoss[0m : 2.78553
[1mStep[0m  [165/339], [94mLoss[0m : 2.19587
[1mStep[0m  [198/339], [94mLoss[0m : 2.42510
[1mStep[0m  [231/339], [94mLoss[0m : 2.36648
[1mStep[0m  [264/339], [94mLoss[0m : 2.60164
[1mStep[0m  [297/339], [94mLoss[0m : 2.51586
[1mStep[0m  [330/339], [94mLoss[0m : 2.37041

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36311
[1mStep[0m  [33/339], [94mLoss[0m : 2.91440
[1mStep[0m  [66/339], [94mLoss[0m : 2.18082
[1mStep[0m  [99/339], [94mLoss[0m : 2.50916
[1mStep[0m  [132/339], [94mLoss[0m : 2.14359
[1mStep[0m  [165/339], [94mLoss[0m : 1.79428
[1mStep[0m  [198/339], [94mLoss[0m : 2.37507
[1mStep[0m  [231/339], [94mLoss[0m : 2.45781
[1mStep[0m  [264/339], [94mLoss[0m : 2.64831
[1mStep[0m  [297/339], [94mLoss[0m : 2.35042
[1mStep[0m  [330/339], [94mLoss[0m : 2.43064

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44778
[1mStep[0m  [33/339], [94mLoss[0m : 2.74962
[1mStep[0m  [66/339], [94mLoss[0m : 2.55653
[1mStep[0m  [99/339], [94mLoss[0m : 2.32268
[1mStep[0m  [132/339], [94mLoss[0m : 1.86350
[1mStep[0m  [165/339], [94mLoss[0m : 1.81038
[1mStep[0m  [198/339], [94mLoss[0m : 2.77360
[1mStep[0m  [231/339], [94mLoss[0m : 2.31462
[1mStep[0m  [264/339], [94mLoss[0m : 3.56273
[1mStep[0m  [297/339], [94mLoss[0m : 2.48887
[1mStep[0m  [330/339], [94mLoss[0m : 2.42978

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81434
[1mStep[0m  [33/339], [94mLoss[0m : 2.15306
[1mStep[0m  [66/339], [94mLoss[0m : 2.08133
[1mStep[0m  [99/339], [94mLoss[0m : 2.35297
[1mStep[0m  [132/339], [94mLoss[0m : 2.74743
[1mStep[0m  [165/339], [94mLoss[0m : 1.94441
[1mStep[0m  [198/339], [94mLoss[0m : 2.40633
[1mStep[0m  [231/339], [94mLoss[0m : 2.44418
[1mStep[0m  [264/339], [94mLoss[0m : 2.14550
[1mStep[0m  [297/339], [94mLoss[0m : 2.37575
[1mStep[0m  [330/339], [94mLoss[0m : 2.97406

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.27463
[1mStep[0m  [33/339], [94mLoss[0m : 1.91770
[1mStep[0m  [66/339], [94mLoss[0m : 1.89047
[1mStep[0m  [99/339], [94mLoss[0m : 2.55689
[1mStep[0m  [132/339], [94mLoss[0m : 2.11569
[1mStep[0m  [165/339], [94mLoss[0m : 2.31872
[1mStep[0m  [198/339], [94mLoss[0m : 2.80376
[1mStep[0m  [231/339], [94mLoss[0m : 3.18822
[1mStep[0m  [264/339], [94mLoss[0m : 2.83379
[1mStep[0m  [297/339], [94mLoss[0m : 2.23724
[1mStep[0m  [330/339], [94mLoss[0m : 2.80060

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05897
[1mStep[0m  [33/339], [94mLoss[0m : 3.10615
[1mStep[0m  [66/339], [94mLoss[0m : 2.47887
[1mStep[0m  [99/339], [94mLoss[0m : 1.81681
[1mStep[0m  [132/339], [94mLoss[0m : 2.37954
[1mStep[0m  [165/339], [94mLoss[0m : 2.74331
[1mStep[0m  [198/339], [94mLoss[0m : 2.72665
[1mStep[0m  [231/339], [94mLoss[0m : 2.37062
[1mStep[0m  [264/339], [94mLoss[0m : 2.29731
[1mStep[0m  [297/339], [94mLoss[0m : 2.44685
[1mStep[0m  [330/339], [94mLoss[0m : 2.29822

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.355
====================================

Phase 1 - Evaluation MAE:  2.3551030950208682
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.94662
[1mStep[0m  [33/339], [94mLoss[0m : 2.22932
[1mStep[0m  [66/339], [94mLoss[0m : 2.86913
[1mStep[0m  [99/339], [94mLoss[0m : 2.58524
[1mStep[0m  [132/339], [94mLoss[0m : 2.74731
[1mStep[0m  [165/339], [94mLoss[0m : 2.35430
[1mStep[0m  [198/339], [94mLoss[0m : 2.64781
[1mStep[0m  [231/339], [94mLoss[0m : 2.58587
[1mStep[0m  [264/339], [94mLoss[0m : 1.82995
[1mStep[0m  [297/339], [94mLoss[0m : 3.13000
[1mStep[0m  [330/339], [94mLoss[0m : 2.54676

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51413
[1mStep[0m  [33/339], [94mLoss[0m : 2.53813
[1mStep[0m  [66/339], [94mLoss[0m : 2.42913
[1mStep[0m  [99/339], [94mLoss[0m : 2.38401
[1mStep[0m  [132/339], [94mLoss[0m : 2.41126
[1mStep[0m  [165/339], [94mLoss[0m : 2.26735
[1mStep[0m  [198/339], [94mLoss[0m : 1.99117
[1mStep[0m  [231/339], [94mLoss[0m : 2.35409
[1mStep[0m  [264/339], [94mLoss[0m : 1.66952
[1mStep[0m  [297/339], [94mLoss[0m : 2.88476
[1mStep[0m  [330/339], [94mLoss[0m : 2.28432

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27316
[1mStep[0m  [33/339], [94mLoss[0m : 2.31083
[1mStep[0m  [66/339], [94mLoss[0m : 2.69878
[1mStep[0m  [99/339], [94mLoss[0m : 2.42034
[1mStep[0m  [132/339], [94mLoss[0m : 1.92430
[1mStep[0m  [165/339], [94mLoss[0m : 2.18477
[1mStep[0m  [198/339], [94mLoss[0m : 2.39651
[1mStep[0m  [231/339], [94mLoss[0m : 2.43905
[1mStep[0m  [264/339], [94mLoss[0m : 2.44318
[1mStep[0m  [297/339], [94mLoss[0m : 2.42356
[1mStep[0m  [330/339], [94mLoss[0m : 2.81743

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05766
[1mStep[0m  [33/339], [94mLoss[0m : 1.74240
[1mStep[0m  [66/339], [94mLoss[0m : 3.02876
[1mStep[0m  [99/339], [94mLoss[0m : 2.11038
[1mStep[0m  [132/339], [94mLoss[0m : 1.76036
[1mStep[0m  [165/339], [94mLoss[0m : 2.00653
[1mStep[0m  [198/339], [94mLoss[0m : 3.32622
[1mStep[0m  [231/339], [94mLoss[0m : 2.55852
[1mStep[0m  [264/339], [94mLoss[0m : 2.01870
[1mStep[0m  [297/339], [94mLoss[0m : 2.06908
[1mStep[0m  [330/339], [94mLoss[0m : 2.57238

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50544
[1mStep[0m  [33/339], [94mLoss[0m : 2.40325
[1mStep[0m  [66/339], [94mLoss[0m : 2.06134
[1mStep[0m  [99/339], [94mLoss[0m : 3.04540
[1mStep[0m  [132/339], [94mLoss[0m : 1.81804
[1mStep[0m  [165/339], [94mLoss[0m : 1.69449
[1mStep[0m  [198/339], [94mLoss[0m : 2.65624
[1mStep[0m  [231/339], [94mLoss[0m : 2.37326
[1mStep[0m  [264/339], [94mLoss[0m : 1.97617
[1mStep[0m  [297/339], [94mLoss[0m : 2.13042
[1mStep[0m  [330/339], [94mLoss[0m : 2.07565

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60278
[1mStep[0m  [33/339], [94mLoss[0m : 2.28792
[1mStep[0m  [66/339], [94mLoss[0m : 2.19104
[1mStep[0m  [99/339], [94mLoss[0m : 1.93155
[1mStep[0m  [132/339], [94mLoss[0m : 2.42368
[1mStep[0m  [165/339], [94mLoss[0m : 1.77119
[1mStep[0m  [198/339], [94mLoss[0m : 3.06387
[1mStep[0m  [231/339], [94mLoss[0m : 2.73191
[1mStep[0m  [264/339], [94mLoss[0m : 1.76842
[1mStep[0m  [297/339], [94mLoss[0m : 2.73559
[1mStep[0m  [330/339], [94mLoss[0m : 2.22838

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48507
[1mStep[0m  [33/339], [94mLoss[0m : 2.55727
[1mStep[0m  [66/339], [94mLoss[0m : 1.92536
[1mStep[0m  [99/339], [94mLoss[0m : 1.94778
[1mStep[0m  [132/339], [94mLoss[0m : 2.61009
[1mStep[0m  [165/339], [94mLoss[0m : 2.33241
[1mStep[0m  [198/339], [94mLoss[0m : 1.91579
[1mStep[0m  [231/339], [94mLoss[0m : 1.83775
[1mStep[0m  [264/339], [94mLoss[0m : 2.43764
[1mStep[0m  [297/339], [94mLoss[0m : 2.26232
[1mStep[0m  [330/339], [94mLoss[0m : 1.32469

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.435, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16730
[1mStep[0m  [33/339], [94mLoss[0m : 1.93709
[1mStep[0m  [66/339], [94mLoss[0m : 2.58856
[1mStep[0m  [99/339], [94mLoss[0m : 1.91363
[1mStep[0m  [132/339], [94mLoss[0m : 1.73510
[1mStep[0m  [165/339], [94mLoss[0m : 1.94045
[1mStep[0m  [198/339], [94mLoss[0m : 1.90035
[1mStep[0m  [231/339], [94mLoss[0m : 3.17776
[1mStep[0m  [264/339], [94mLoss[0m : 2.79246
[1mStep[0m  [297/339], [94mLoss[0m : 1.62606
[1mStep[0m  [330/339], [94mLoss[0m : 2.19488

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33397
[1mStep[0m  [33/339], [94mLoss[0m : 1.94367
[1mStep[0m  [66/339], [94mLoss[0m : 2.20093
[1mStep[0m  [99/339], [94mLoss[0m : 1.72158
[1mStep[0m  [132/339], [94mLoss[0m : 1.97529
[1mStep[0m  [165/339], [94mLoss[0m : 2.61780
[1mStep[0m  [198/339], [94mLoss[0m : 2.69238
[1mStep[0m  [231/339], [94mLoss[0m : 2.43589
[1mStep[0m  [264/339], [94mLoss[0m : 1.85239
[1mStep[0m  [297/339], [94mLoss[0m : 2.48653
[1mStep[0m  [330/339], [94mLoss[0m : 2.44488

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.095, [92mTest[0m: 2.394, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28023
[1mStep[0m  [33/339], [94mLoss[0m : 1.86865
[1mStep[0m  [66/339], [94mLoss[0m : 1.80803
[1mStep[0m  [99/339], [94mLoss[0m : 2.10476
[1mStep[0m  [132/339], [94mLoss[0m : 1.96734
[1mStep[0m  [165/339], [94mLoss[0m : 1.98472
[1mStep[0m  [198/339], [94mLoss[0m : 2.15543
[1mStep[0m  [231/339], [94mLoss[0m : 2.34996
[1mStep[0m  [264/339], [94mLoss[0m : 2.32547
[1mStep[0m  [297/339], [94mLoss[0m : 2.15026
[1mStep[0m  [330/339], [94mLoss[0m : 1.84542

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30136
[1mStep[0m  [33/339], [94mLoss[0m : 2.02373
[1mStep[0m  [66/339], [94mLoss[0m : 1.97321
[1mStep[0m  [99/339], [94mLoss[0m : 1.70182
[1mStep[0m  [132/339], [94mLoss[0m : 2.23228
[1mStep[0m  [165/339], [94mLoss[0m : 2.11931
[1mStep[0m  [198/339], [94mLoss[0m : 2.05374
[1mStep[0m  [231/339], [94mLoss[0m : 2.23535
[1mStep[0m  [264/339], [94mLoss[0m : 1.98922
[1mStep[0m  [297/339], [94mLoss[0m : 1.65761
[1mStep[0m  [330/339], [94mLoss[0m : 2.00546

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.010, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02684
[1mStep[0m  [33/339], [94mLoss[0m : 2.07561
[1mStep[0m  [66/339], [94mLoss[0m : 1.90969
[1mStep[0m  [99/339], [94mLoss[0m : 2.23104
[1mStep[0m  [132/339], [94mLoss[0m : 1.79151
[1mStep[0m  [165/339], [94mLoss[0m : 1.89662
[1mStep[0m  [198/339], [94mLoss[0m : 1.66438
[1mStep[0m  [231/339], [94mLoss[0m : 1.84321
[1mStep[0m  [264/339], [94mLoss[0m : 1.64992
[1mStep[0m  [297/339], [94mLoss[0m : 1.79671
[1mStep[0m  [330/339], [94mLoss[0m : 2.24777

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82942
[1mStep[0m  [33/339], [94mLoss[0m : 1.65882
[1mStep[0m  [66/339], [94mLoss[0m : 1.50146
[1mStep[0m  [99/339], [94mLoss[0m : 2.40031
[1mStep[0m  [132/339], [94mLoss[0m : 2.15886
[1mStep[0m  [165/339], [94mLoss[0m : 2.35246
[1mStep[0m  [198/339], [94mLoss[0m : 2.04330
[1mStep[0m  [231/339], [94mLoss[0m : 2.15702
[1mStep[0m  [264/339], [94mLoss[0m : 1.74345
[1mStep[0m  [297/339], [94mLoss[0m : 2.07513
[1mStep[0m  [330/339], [94mLoss[0m : 2.01928

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88413
[1mStep[0m  [33/339], [94mLoss[0m : 1.39949
[1mStep[0m  [66/339], [94mLoss[0m : 1.85378
[1mStep[0m  [99/339], [94mLoss[0m : 1.81242
[1mStep[0m  [132/339], [94mLoss[0m : 2.92327
[1mStep[0m  [165/339], [94mLoss[0m : 2.04303
[1mStep[0m  [198/339], [94mLoss[0m : 2.52696
[1mStep[0m  [231/339], [94mLoss[0m : 1.48199
[1mStep[0m  [264/339], [94mLoss[0m : 2.24465
[1mStep[0m  [297/339], [94mLoss[0m : 2.04204
[1mStep[0m  [330/339], [94mLoss[0m : 2.03971

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.07704
[1mStep[0m  [33/339], [94mLoss[0m : 1.67764
[1mStep[0m  [66/339], [94mLoss[0m : 1.70667
[1mStep[0m  [99/339], [94mLoss[0m : 2.40879
[1mStep[0m  [132/339], [94mLoss[0m : 1.48609
[1mStep[0m  [165/339], [94mLoss[0m : 2.19684
[1mStep[0m  [198/339], [94mLoss[0m : 2.22883
[1mStep[0m  [231/339], [94mLoss[0m : 1.79122
[1mStep[0m  [264/339], [94mLoss[0m : 1.80242
[1mStep[0m  [297/339], [94mLoss[0m : 2.39527
[1mStep[0m  [330/339], [94mLoss[0m : 1.59383

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.419, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14674
[1mStep[0m  [33/339], [94mLoss[0m : 2.16732
[1mStep[0m  [66/339], [94mLoss[0m : 1.41111
[1mStep[0m  [99/339], [94mLoss[0m : 2.14921
[1mStep[0m  [132/339], [94mLoss[0m : 1.53469
[1mStep[0m  [165/339], [94mLoss[0m : 1.85498
[1mStep[0m  [198/339], [94mLoss[0m : 1.38738
[1mStep[0m  [231/339], [94mLoss[0m : 1.84852
[1mStep[0m  [264/339], [94mLoss[0m : 1.92911
[1mStep[0m  [297/339], [94mLoss[0m : 1.81365
[1mStep[0m  [330/339], [94mLoss[0m : 2.21084

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63002
[1mStep[0m  [33/339], [94mLoss[0m : 1.76284
[1mStep[0m  [66/339], [94mLoss[0m : 1.73628
[1mStep[0m  [99/339], [94mLoss[0m : 1.68516
[1mStep[0m  [132/339], [94mLoss[0m : 2.22564
[1mStep[0m  [165/339], [94mLoss[0m : 2.04137
[1mStep[0m  [198/339], [94mLoss[0m : 1.72416
[1mStep[0m  [231/339], [94mLoss[0m : 1.76586
[1mStep[0m  [264/339], [94mLoss[0m : 1.80242
[1mStep[0m  [297/339], [94mLoss[0m : 1.81193
[1mStep[0m  [330/339], [94mLoss[0m : 1.90616

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70185
[1mStep[0m  [33/339], [94mLoss[0m : 1.38309
[1mStep[0m  [66/339], [94mLoss[0m : 1.72571
[1mStep[0m  [99/339], [94mLoss[0m : 2.03621
[1mStep[0m  [132/339], [94mLoss[0m : 1.54952
[1mStep[0m  [165/339], [94mLoss[0m : 1.74547
[1mStep[0m  [198/339], [94mLoss[0m : 1.59383
[1mStep[0m  [231/339], [94mLoss[0m : 1.91154
[1mStep[0m  [264/339], [94mLoss[0m : 1.76649
[1mStep[0m  [297/339], [94mLoss[0m : 1.57117
[1mStep[0m  [330/339], [94mLoss[0m : 1.94004

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.820, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79509
[1mStep[0m  [33/339], [94mLoss[0m : 1.71139
[1mStep[0m  [66/339], [94mLoss[0m : 1.46403
[1mStep[0m  [99/339], [94mLoss[0m : 1.82062
[1mStep[0m  [132/339], [94mLoss[0m : 1.72185
[1mStep[0m  [165/339], [94mLoss[0m : 1.28476
[1mStep[0m  [198/339], [94mLoss[0m : 1.95037
[1mStep[0m  [231/339], [94mLoss[0m : 1.64509
[1mStep[0m  [264/339], [94mLoss[0m : 1.82160
[1mStep[0m  [297/339], [94mLoss[0m : 1.83504
[1mStep[0m  [330/339], [94mLoss[0m : 2.09517

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.781, [92mTest[0m: 2.536, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.33770
[1mStep[0m  [33/339], [94mLoss[0m : 1.33260
[1mStep[0m  [66/339], [94mLoss[0m : 1.53884
[1mStep[0m  [99/339], [94mLoss[0m : 1.95269
[1mStep[0m  [132/339], [94mLoss[0m : 1.55134
[1mStep[0m  [165/339], [94mLoss[0m : 1.73194
[1mStep[0m  [198/339], [94mLoss[0m : 1.51948
[1mStep[0m  [231/339], [94mLoss[0m : 1.84458
[1mStep[0m  [264/339], [94mLoss[0m : 2.12986
[1mStep[0m  [297/339], [94mLoss[0m : 1.37702
[1mStep[0m  [330/339], [94mLoss[0m : 1.87036

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.525, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.53375
[1mStep[0m  [33/339], [94mLoss[0m : 2.10283
[1mStep[0m  [66/339], [94mLoss[0m : 2.18080
[1mStep[0m  [99/339], [94mLoss[0m : 1.48012
[1mStep[0m  [132/339], [94mLoss[0m : 2.10025
[1mStep[0m  [165/339], [94mLoss[0m : 1.65380
[1mStep[0m  [198/339], [94mLoss[0m : 1.49587
[1mStep[0m  [231/339], [94mLoss[0m : 1.51204
[1mStep[0m  [264/339], [94mLoss[0m : 1.68480
[1mStep[0m  [297/339], [94mLoss[0m : 1.91746
[1mStep[0m  [330/339], [94mLoss[0m : 2.56340

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.522, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47996
[1mStep[0m  [33/339], [94mLoss[0m : 1.50214
[1mStep[0m  [66/339], [94mLoss[0m : 2.28464
[1mStep[0m  [99/339], [94mLoss[0m : 1.09705
[1mStep[0m  [132/339], [94mLoss[0m : 1.65403
[1mStep[0m  [165/339], [94mLoss[0m : 1.35123
[1mStep[0m  [198/339], [94mLoss[0m : 2.09036
[1mStep[0m  [231/339], [94mLoss[0m : 1.32304
[1mStep[0m  [264/339], [94mLoss[0m : 2.32016
[1mStep[0m  [297/339], [94mLoss[0m : 1.88877
[1mStep[0m  [330/339], [94mLoss[0m : 1.97094

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.700, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48676
[1mStep[0m  [33/339], [94mLoss[0m : 1.49987
[1mStep[0m  [66/339], [94mLoss[0m : 1.78640
[1mStep[0m  [99/339], [94mLoss[0m : 1.56300
[1mStep[0m  [132/339], [94mLoss[0m : 1.84163
[1mStep[0m  [165/339], [94mLoss[0m : 2.20282
[1mStep[0m  [198/339], [94mLoss[0m : 2.09840
[1mStep[0m  [231/339], [94mLoss[0m : 1.36355
[1mStep[0m  [264/339], [94mLoss[0m : 1.23079
[1mStep[0m  [297/339], [94mLoss[0m : 1.64529
[1mStep[0m  [330/339], [94mLoss[0m : 1.67565

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.42241
[1mStep[0m  [33/339], [94mLoss[0m : 1.49130
[1mStep[0m  [66/339], [94mLoss[0m : 1.53487
[1mStep[0m  [99/339], [94mLoss[0m : 1.59097
[1mStep[0m  [132/339], [94mLoss[0m : 1.50086
[1mStep[0m  [165/339], [94mLoss[0m : 1.70878
[1mStep[0m  [198/339], [94mLoss[0m : 1.37770
[1mStep[0m  [231/339], [94mLoss[0m : 1.60055
[1mStep[0m  [264/339], [94mLoss[0m : 1.37596
[1mStep[0m  [297/339], [94mLoss[0m : 1.65617
[1mStep[0m  [330/339], [94mLoss[0m : 1.52406

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.655, [92mTest[0m: 2.557, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41699
[1mStep[0m  [33/339], [94mLoss[0m : 1.71576
[1mStep[0m  [66/339], [94mLoss[0m : 1.30533
[1mStep[0m  [99/339], [94mLoss[0m : 1.79129
[1mStep[0m  [132/339], [94mLoss[0m : 1.79007
[1mStep[0m  [165/339], [94mLoss[0m : 1.40833
[1mStep[0m  [198/339], [94mLoss[0m : 1.64931
[1mStep[0m  [231/339], [94mLoss[0m : 1.91854
[1mStep[0m  [264/339], [94mLoss[0m : 1.39011
[1mStep[0m  [297/339], [94mLoss[0m : 1.51937
[1mStep[0m  [330/339], [94mLoss[0m : 2.03432

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.528, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.87524
[1mStep[0m  [33/339], [94mLoss[0m : 1.11744
[1mStep[0m  [66/339], [94mLoss[0m : 1.08123
[1mStep[0m  [99/339], [94mLoss[0m : 1.38503
[1mStep[0m  [132/339], [94mLoss[0m : 1.59188
[1mStep[0m  [165/339], [94mLoss[0m : 1.74014
[1mStep[0m  [198/339], [94mLoss[0m : 1.51660
[1mStep[0m  [231/339], [94mLoss[0m : 1.61845
[1mStep[0m  [264/339], [94mLoss[0m : 1.56302
[1mStep[0m  [297/339], [94mLoss[0m : 1.41798
[1mStep[0m  [330/339], [94mLoss[0m : 1.60194

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.576, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56601
[1mStep[0m  [33/339], [94mLoss[0m : 1.67132
[1mStep[0m  [66/339], [94mLoss[0m : 1.12648
[1mStep[0m  [99/339], [94mLoss[0m : 1.66242
[1mStep[0m  [132/339], [94mLoss[0m : 1.37645
[1mStep[0m  [165/339], [94mLoss[0m : 1.58880
[1mStep[0m  [198/339], [94mLoss[0m : 1.52380
[1mStep[0m  [231/339], [94mLoss[0m : 1.37109
[1mStep[0m  [264/339], [94mLoss[0m : 1.93722
[1mStep[0m  [297/339], [94mLoss[0m : 1.24741
[1mStep[0m  [330/339], [94mLoss[0m : 1.61118

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24466
[1mStep[0m  [33/339], [94mLoss[0m : 1.53234
[1mStep[0m  [66/339], [94mLoss[0m : 1.54691
[1mStep[0m  [99/339], [94mLoss[0m : 1.69895
[1mStep[0m  [132/339], [94mLoss[0m : 1.23159
[1mStep[0m  [165/339], [94mLoss[0m : 1.28662
[1mStep[0m  [198/339], [94mLoss[0m : 1.32944
[1mStep[0m  [231/339], [94mLoss[0m : 1.46379
[1mStep[0m  [264/339], [94mLoss[0m : 1.46076
[1mStep[0m  [297/339], [94mLoss[0m : 1.62039
[1mStep[0m  [330/339], [94mLoss[0m : 2.24920

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59097
[1mStep[0m  [33/339], [94mLoss[0m : 1.24089
[1mStep[0m  [66/339], [94mLoss[0m : 1.66053
[1mStep[0m  [99/339], [94mLoss[0m : 1.21066
[1mStep[0m  [132/339], [94mLoss[0m : 1.74789
[1mStep[0m  [165/339], [94mLoss[0m : 1.65172
[1mStep[0m  [198/339], [94mLoss[0m : 2.23506
[1mStep[0m  [231/339], [94mLoss[0m : 1.51199
[1mStep[0m  [264/339], [94mLoss[0m : 1.30753
[1mStep[0m  [297/339], [94mLoss[0m : 1.92333
[1mStep[0m  [330/339], [94mLoss[0m : 1.26202

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.557, [92mTest[0m: 2.555, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.19926
[1mStep[0m  [33/339], [94mLoss[0m : 1.64391
[1mStep[0m  [66/339], [94mLoss[0m : 1.64932
[1mStep[0m  [99/339], [94mLoss[0m : 1.24507
[1mStep[0m  [132/339], [94mLoss[0m : 1.32104
[1mStep[0m  [165/339], [94mLoss[0m : 2.01983
[1mStep[0m  [198/339], [94mLoss[0m : 1.40391
[1mStep[0m  [231/339], [94mLoss[0m : 1.55989
[1mStep[0m  [264/339], [94mLoss[0m : 1.97032
[1mStep[0m  [297/339], [94mLoss[0m : 1.50090
[1mStep[0m  [330/339], [94mLoss[0m : 1.37983

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.473, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.653
====================================

Phase 2 - Evaluation MAE:  2.6534669578602883
MAE score P1       2.355103
MAE score P2       2.653467
loss               1.544429
learning_rate          0.01
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 11.45725
[1mStep[0m  [33/339], [94mLoss[0m : 2.59367
[1mStep[0m  [66/339], [94mLoss[0m : 2.07234
[1mStep[0m  [99/339], [94mLoss[0m : 2.33238
[1mStep[0m  [132/339], [94mLoss[0m : 2.38181
[1mStep[0m  [165/339], [94mLoss[0m : 2.48032
[1mStep[0m  [198/339], [94mLoss[0m : 2.40368
[1mStep[0m  [231/339], [94mLoss[0m : 2.82574
[1mStep[0m  [264/339], [94mLoss[0m : 2.66225
[1mStep[0m  [297/339], [94mLoss[0m : 3.59443
[1mStep[0m  [330/339], [94mLoss[0m : 2.86866

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.167, [92mTest[0m: 10.826, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56223
[1mStep[0m  [33/339], [94mLoss[0m : 3.77958
[1mStep[0m  [66/339], [94mLoss[0m : 1.94988
[1mStep[0m  [99/339], [94mLoss[0m : 2.79311
[1mStep[0m  [132/339], [94mLoss[0m : 3.22397
[1mStep[0m  [165/339], [94mLoss[0m : 2.43959
[1mStep[0m  [198/339], [94mLoss[0m : 2.05206
[1mStep[0m  [231/339], [94mLoss[0m : 2.94396
[1mStep[0m  [264/339], [94mLoss[0m : 2.83469
[1mStep[0m  [297/339], [94mLoss[0m : 2.52697
[1mStep[0m  [330/339], [94mLoss[0m : 2.29443

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60776
[1mStep[0m  [33/339], [94mLoss[0m : 2.41036
[1mStep[0m  [66/339], [94mLoss[0m : 1.89801
[1mStep[0m  [99/339], [94mLoss[0m : 2.96473
[1mStep[0m  [132/339], [94mLoss[0m : 2.59950
[1mStep[0m  [165/339], [94mLoss[0m : 2.65480
[1mStep[0m  [198/339], [94mLoss[0m : 2.42216
[1mStep[0m  [231/339], [94mLoss[0m : 2.70049
[1mStep[0m  [264/339], [94mLoss[0m : 2.61790
[1mStep[0m  [297/339], [94mLoss[0m : 2.70636
[1mStep[0m  [330/339], [94mLoss[0m : 2.48978

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13901
[1mStep[0m  [33/339], [94mLoss[0m : 2.56091
[1mStep[0m  [66/339], [94mLoss[0m : 3.11247
[1mStep[0m  [99/339], [94mLoss[0m : 2.72283
[1mStep[0m  [132/339], [94mLoss[0m : 2.70541
[1mStep[0m  [165/339], [94mLoss[0m : 2.93534
[1mStep[0m  [198/339], [94mLoss[0m : 2.49589
[1mStep[0m  [231/339], [94mLoss[0m : 2.46403
[1mStep[0m  [264/339], [94mLoss[0m : 2.44380
[1mStep[0m  [297/339], [94mLoss[0m : 2.34596
[1mStep[0m  [330/339], [94mLoss[0m : 2.30296

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.20051
[1mStep[0m  [33/339], [94mLoss[0m : 2.11329
[1mStep[0m  [66/339], [94mLoss[0m : 2.24980
[1mStep[0m  [99/339], [94mLoss[0m : 2.62291
[1mStep[0m  [132/339], [94mLoss[0m : 2.05340
[1mStep[0m  [165/339], [94mLoss[0m : 2.07530
[1mStep[0m  [198/339], [94mLoss[0m : 2.24734
[1mStep[0m  [231/339], [94mLoss[0m : 2.51752
[1mStep[0m  [264/339], [94mLoss[0m : 2.69431
[1mStep[0m  [297/339], [94mLoss[0m : 2.61113
[1mStep[0m  [330/339], [94mLoss[0m : 2.55479

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82706
[1mStep[0m  [33/339], [94mLoss[0m : 2.60020
[1mStep[0m  [66/339], [94mLoss[0m : 2.34819
[1mStep[0m  [99/339], [94mLoss[0m : 2.46378
[1mStep[0m  [132/339], [94mLoss[0m : 2.77662
[1mStep[0m  [165/339], [94mLoss[0m : 2.54355
[1mStep[0m  [198/339], [94mLoss[0m : 2.22828
[1mStep[0m  [231/339], [94mLoss[0m : 2.82571
[1mStep[0m  [264/339], [94mLoss[0m : 2.27671
[1mStep[0m  [297/339], [94mLoss[0m : 1.74222
[1mStep[0m  [330/339], [94mLoss[0m : 2.92128

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.356, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88828
[1mStep[0m  [33/339], [94mLoss[0m : 1.83446
[1mStep[0m  [66/339], [94mLoss[0m : 2.34352
[1mStep[0m  [99/339], [94mLoss[0m : 3.58886
[1mStep[0m  [132/339], [94mLoss[0m : 2.34736
[1mStep[0m  [165/339], [94mLoss[0m : 2.82643
[1mStep[0m  [198/339], [94mLoss[0m : 2.88690
[1mStep[0m  [231/339], [94mLoss[0m : 2.01260
[1mStep[0m  [264/339], [94mLoss[0m : 2.53324
[1mStep[0m  [297/339], [94mLoss[0m : 2.91142
[1mStep[0m  [330/339], [94mLoss[0m : 2.75853

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91254
[1mStep[0m  [33/339], [94mLoss[0m : 2.60953
[1mStep[0m  [66/339], [94mLoss[0m : 2.06827
[1mStep[0m  [99/339], [94mLoss[0m : 2.46773
[1mStep[0m  [132/339], [94mLoss[0m : 2.15817
[1mStep[0m  [165/339], [94mLoss[0m : 2.87919
[1mStep[0m  [198/339], [94mLoss[0m : 2.43879
[1mStep[0m  [231/339], [94mLoss[0m : 2.60396
[1mStep[0m  [264/339], [94mLoss[0m : 2.65763
[1mStep[0m  [297/339], [94mLoss[0m : 1.99197
[1mStep[0m  [330/339], [94mLoss[0m : 2.70946

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57762
[1mStep[0m  [33/339], [94mLoss[0m : 2.62062
[1mStep[0m  [66/339], [94mLoss[0m : 2.75721
[1mStep[0m  [99/339], [94mLoss[0m : 2.21402
[1mStep[0m  [132/339], [94mLoss[0m : 2.33006
[1mStep[0m  [165/339], [94mLoss[0m : 2.90527
[1mStep[0m  [198/339], [94mLoss[0m : 2.43351
[1mStep[0m  [231/339], [94mLoss[0m : 2.32685
[1mStep[0m  [264/339], [94mLoss[0m : 2.70664
[1mStep[0m  [297/339], [94mLoss[0m : 2.59011
[1mStep[0m  [330/339], [94mLoss[0m : 2.77354

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77929
[1mStep[0m  [33/339], [94mLoss[0m : 2.74865
[1mStep[0m  [66/339], [94mLoss[0m : 2.17042
[1mStep[0m  [99/339], [94mLoss[0m : 2.33087
[1mStep[0m  [132/339], [94mLoss[0m : 2.35789
[1mStep[0m  [165/339], [94mLoss[0m : 2.51133
[1mStep[0m  [198/339], [94mLoss[0m : 2.49449
[1mStep[0m  [231/339], [94mLoss[0m : 2.44133
[1mStep[0m  [264/339], [94mLoss[0m : 2.26438
[1mStep[0m  [297/339], [94mLoss[0m : 2.53520
[1mStep[0m  [330/339], [94mLoss[0m : 2.58151

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40985
[1mStep[0m  [33/339], [94mLoss[0m : 2.86056
[1mStep[0m  [66/339], [94mLoss[0m : 2.26700
[1mStep[0m  [99/339], [94mLoss[0m : 2.54530
[1mStep[0m  [132/339], [94mLoss[0m : 2.37759
[1mStep[0m  [165/339], [94mLoss[0m : 2.37435
[1mStep[0m  [198/339], [94mLoss[0m : 2.76285
[1mStep[0m  [231/339], [94mLoss[0m : 2.17733
[1mStep[0m  [264/339], [94mLoss[0m : 2.35986
[1mStep[0m  [297/339], [94mLoss[0m : 2.90904
[1mStep[0m  [330/339], [94mLoss[0m : 2.14643

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61963
[1mStep[0m  [33/339], [94mLoss[0m : 2.63026
[1mStep[0m  [66/339], [94mLoss[0m : 2.50290
[1mStep[0m  [99/339], [94mLoss[0m : 2.35550
[1mStep[0m  [132/339], [94mLoss[0m : 2.28122
[1mStep[0m  [165/339], [94mLoss[0m : 2.22450
[1mStep[0m  [198/339], [94mLoss[0m : 1.80869
[1mStep[0m  [231/339], [94mLoss[0m : 2.97580
[1mStep[0m  [264/339], [94mLoss[0m : 3.45514
[1mStep[0m  [297/339], [94mLoss[0m : 3.31807
[1mStep[0m  [330/339], [94mLoss[0m : 2.16529

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33552
[1mStep[0m  [33/339], [94mLoss[0m : 2.20285
[1mStep[0m  [66/339], [94mLoss[0m : 2.23940
[1mStep[0m  [99/339], [94mLoss[0m : 2.75867
[1mStep[0m  [132/339], [94mLoss[0m : 2.28663
[1mStep[0m  [165/339], [94mLoss[0m : 2.99659
[1mStep[0m  [198/339], [94mLoss[0m : 2.62555
[1mStep[0m  [231/339], [94mLoss[0m : 2.71949
[1mStep[0m  [264/339], [94mLoss[0m : 2.53005
[1mStep[0m  [297/339], [94mLoss[0m : 2.72793
[1mStep[0m  [330/339], [94mLoss[0m : 2.79179

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32649
[1mStep[0m  [33/339], [94mLoss[0m : 2.98109
[1mStep[0m  [66/339], [94mLoss[0m : 2.77427
[1mStep[0m  [99/339], [94mLoss[0m : 2.15337
[1mStep[0m  [132/339], [94mLoss[0m : 2.28194
[1mStep[0m  [165/339], [94mLoss[0m : 2.65910
[1mStep[0m  [198/339], [94mLoss[0m : 3.03246
[1mStep[0m  [231/339], [94mLoss[0m : 3.16563
[1mStep[0m  [264/339], [94mLoss[0m : 2.45704
[1mStep[0m  [297/339], [94mLoss[0m : 2.97736
[1mStep[0m  [330/339], [94mLoss[0m : 2.01766

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.05592
[1mStep[0m  [33/339], [94mLoss[0m : 2.78694
[1mStep[0m  [66/339], [94mLoss[0m : 2.45762
[1mStep[0m  [99/339], [94mLoss[0m : 2.44418
[1mStep[0m  [132/339], [94mLoss[0m : 2.42449
[1mStep[0m  [165/339], [94mLoss[0m : 2.81592
[1mStep[0m  [198/339], [94mLoss[0m : 2.60286
[1mStep[0m  [231/339], [94mLoss[0m : 2.37569
[1mStep[0m  [264/339], [94mLoss[0m : 3.27372
[1mStep[0m  [297/339], [94mLoss[0m : 2.57718
[1mStep[0m  [330/339], [94mLoss[0m : 3.12890

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27675
[1mStep[0m  [33/339], [94mLoss[0m : 1.81440
[1mStep[0m  [66/339], [94mLoss[0m : 2.50563
[1mStep[0m  [99/339], [94mLoss[0m : 2.58713
[1mStep[0m  [132/339], [94mLoss[0m : 2.09118
[1mStep[0m  [165/339], [94mLoss[0m : 2.19419
[1mStep[0m  [198/339], [94mLoss[0m : 2.23666
[1mStep[0m  [231/339], [94mLoss[0m : 3.06215
[1mStep[0m  [264/339], [94mLoss[0m : 3.27041
[1mStep[0m  [297/339], [94mLoss[0m : 2.00649
[1mStep[0m  [330/339], [94mLoss[0m : 2.38494

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.02917
[1mStep[0m  [33/339], [94mLoss[0m : 2.01926
[1mStep[0m  [66/339], [94mLoss[0m : 2.37133
[1mStep[0m  [99/339], [94mLoss[0m : 2.62011
[1mStep[0m  [132/339], [94mLoss[0m : 2.30338
[1mStep[0m  [165/339], [94mLoss[0m : 3.15275
[1mStep[0m  [198/339], [94mLoss[0m : 2.80832
[1mStep[0m  [231/339], [94mLoss[0m : 2.40008
[1mStep[0m  [264/339], [94mLoss[0m : 3.08428
[1mStep[0m  [297/339], [94mLoss[0m : 2.62174
[1mStep[0m  [330/339], [94mLoss[0m : 2.34902

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53158
[1mStep[0m  [33/339], [94mLoss[0m : 3.27268
[1mStep[0m  [66/339], [94mLoss[0m : 2.56878
[1mStep[0m  [99/339], [94mLoss[0m : 2.33264
[1mStep[0m  [132/339], [94mLoss[0m : 2.60610
[1mStep[0m  [165/339], [94mLoss[0m : 3.18045
[1mStep[0m  [198/339], [94mLoss[0m : 2.05403
[1mStep[0m  [231/339], [94mLoss[0m : 2.78546
[1mStep[0m  [264/339], [94mLoss[0m : 2.64068
[1mStep[0m  [297/339], [94mLoss[0m : 2.51063
[1mStep[0m  [330/339], [94mLoss[0m : 2.06007

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00988
[1mStep[0m  [33/339], [94mLoss[0m : 2.72275
[1mStep[0m  [66/339], [94mLoss[0m : 2.43027
[1mStep[0m  [99/339], [94mLoss[0m : 3.53679
[1mStep[0m  [132/339], [94mLoss[0m : 2.81150
[1mStep[0m  [165/339], [94mLoss[0m : 2.59745
[1mStep[0m  [198/339], [94mLoss[0m : 2.55219
[1mStep[0m  [231/339], [94mLoss[0m : 1.78204
[1mStep[0m  [264/339], [94mLoss[0m : 2.72460
[1mStep[0m  [297/339], [94mLoss[0m : 2.84585
[1mStep[0m  [330/339], [94mLoss[0m : 3.03721

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.07341
[1mStep[0m  [33/339], [94mLoss[0m : 2.62167
[1mStep[0m  [66/339], [94mLoss[0m : 2.20691
[1mStep[0m  [99/339], [94mLoss[0m : 1.89829
[1mStep[0m  [132/339], [94mLoss[0m : 2.72687
[1mStep[0m  [165/339], [94mLoss[0m : 2.43981
[1mStep[0m  [198/339], [94mLoss[0m : 2.73125
[1mStep[0m  [231/339], [94mLoss[0m : 2.96344
[1mStep[0m  [264/339], [94mLoss[0m : 2.25544
[1mStep[0m  [297/339], [94mLoss[0m : 2.23366
[1mStep[0m  [330/339], [94mLoss[0m : 2.42645

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.382, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13018
[1mStep[0m  [33/339], [94mLoss[0m : 2.06679
[1mStep[0m  [66/339], [94mLoss[0m : 2.13513
[1mStep[0m  [99/339], [94mLoss[0m : 2.87882
[1mStep[0m  [132/339], [94mLoss[0m : 2.78228
[1mStep[0m  [165/339], [94mLoss[0m : 2.08059
[1mStep[0m  [198/339], [94mLoss[0m : 2.84486
[1mStep[0m  [231/339], [94mLoss[0m : 3.01898
[1mStep[0m  [264/339], [94mLoss[0m : 2.44166
[1mStep[0m  [297/339], [94mLoss[0m : 2.49092
[1mStep[0m  [330/339], [94mLoss[0m : 2.39566

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49054
[1mStep[0m  [33/339], [94mLoss[0m : 2.16995
[1mStep[0m  [66/339], [94mLoss[0m : 2.59994
[1mStep[0m  [99/339], [94mLoss[0m : 3.47408
[1mStep[0m  [132/339], [94mLoss[0m : 2.60079
[1mStep[0m  [165/339], [94mLoss[0m : 3.14474
[1mStep[0m  [198/339], [94mLoss[0m : 2.98598
[1mStep[0m  [231/339], [94mLoss[0m : 2.59085
[1mStep[0m  [264/339], [94mLoss[0m : 2.27098
[1mStep[0m  [297/339], [94mLoss[0m : 3.40721
[1mStep[0m  [330/339], [94mLoss[0m : 2.58179

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.372, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58727
[1mStep[0m  [33/339], [94mLoss[0m : 2.71520
[1mStep[0m  [66/339], [94mLoss[0m : 1.87050
[1mStep[0m  [99/339], [94mLoss[0m : 1.96742
[1mStep[0m  [132/339], [94mLoss[0m : 2.90094
[1mStep[0m  [165/339], [94mLoss[0m : 3.31490
[1mStep[0m  [198/339], [94mLoss[0m : 3.11669
[1mStep[0m  [231/339], [94mLoss[0m : 2.86065
[1mStep[0m  [264/339], [94mLoss[0m : 2.46778
[1mStep[0m  [297/339], [94mLoss[0m : 2.52253
[1mStep[0m  [330/339], [94mLoss[0m : 2.90568

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.396, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30882
[1mStep[0m  [33/339], [94mLoss[0m : 2.52691
[1mStep[0m  [66/339], [94mLoss[0m : 1.90535
[1mStep[0m  [99/339], [94mLoss[0m : 2.21695
[1mStep[0m  [132/339], [94mLoss[0m : 2.02507
[1mStep[0m  [165/339], [94mLoss[0m : 2.83789
[1mStep[0m  [198/339], [94mLoss[0m : 3.57406
[1mStep[0m  [231/339], [94mLoss[0m : 2.89843
[1mStep[0m  [264/339], [94mLoss[0m : 2.68442
[1mStep[0m  [297/339], [94mLoss[0m : 2.61063
[1mStep[0m  [330/339], [94mLoss[0m : 3.77978

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.386, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56171
[1mStep[0m  [33/339], [94mLoss[0m : 2.82401
[1mStep[0m  [66/339], [94mLoss[0m : 2.47095
[1mStep[0m  [99/339], [94mLoss[0m : 2.35282
[1mStep[0m  [132/339], [94mLoss[0m : 2.61849
[1mStep[0m  [165/339], [94mLoss[0m : 3.79850
[1mStep[0m  [198/339], [94mLoss[0m : 2.87013
[1mStep[0m  [231/339], [94mLoss[0m : 2.72834
[1mStep[0m  [264/339], [94mLoss[0m : 2.18613
[1mStep[0m  [297/339], [94mLoss[0m : 3.02268
[1mStep[0m  [330/339], [94mLoss[0m : 2.31641

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.387, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58587
[1mStep[0m  [33/339], [94mLoss[0m : 2.67772
[1mStep[0m  [66/339], [94mLoss[0m : 2.33422
[1mStep[0m  [99/339], [94mLoss[0m : 2.44854
[1mStep[0m  [132/339], [94mLoss[0m : 2.56201
[1mStep[0m  [165/339], [94mLoss[0m : 2.35199
[1mStep[0m  [198/339], [94mLoss[0m : 2.66296
[1mStep[0m  [231/339], [94mLoss[0m : 2.79761
[1mStep[0m  [264/339], [94mLoss[0m : 2.39945
[1mStep[0m  [297/339], [94mLoss[0m : 2.59095
[1mStep[0m  [330/339], [94mLoss[0m : 2.55544

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.383, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52605
[1mStep[0m  [33/339], [94mLoss[0m : 2.62972
[1mStep[0m  [66/339], [94mLoss[0m : 3.21952
[1mStep[0m  [99/339], [94mLoss[0m : 2.19128
[1mStep[0m  [132/339], [94mLoss[0m : 2.86165
[1mStep[0m  [165/339], [94mLoss[0m : 2.28996
[1mStep[0m  [198/339], [94mLoss[0m : 2.15217
[1mStep[0m  [231/339], [94mLoss[0m : 2.23505
[1mStep[0m  [264/339], [94mLoss[0m : 2.66312
[1mStep[0m  [297/339], [94mLoss[0m : 3.07821
[1mStep[0m  [330/339], [94mLoss[0m : 2.47511

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76606
[1mStep[0m  [33/339], [94mLoss[0m : 2.57372
[1mStep[0m  [66/339], [94mLoss[0m : 2.26410
[1mStep[0m  [99/339], [94mLoss[0m : 2.80610
[1mStep[0m  [132/339], [94mLoss[0m : 2.84200
[1mStep[0m  [165/339], [94mLoss[0m : 2.78260
[1mStep[0m  [198/339], [94mLoss[0m : 2.61133
[1mStep[0m  [231/339], [94mLoss[0m : 2.80627
[1mStep[0m  [264/339], [94mLoss[0m : 2.42474
[1mStep[0m  [297/339], [94mLoss[0m : 2.02436
[1mStep[0m  [330/339], [94mLoss[0m : 2.46525

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.379, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57804
[1mStep[0m  [33/339], [94mLoss[0m : 3.06141
[1mStep[0m  [66/339], [94mLoss[0m : 2.73737
[1mStep[0m  [99/339], [94mLoss[0m : 2.87483
[1mStep[0m  [132/339], [94mLoss[0m : 2.51875
[1mStep[0m  [165/339], [94mLoss[0m : 2.25186
[1mStep[0m  [198/339], [94mLoss[0m : 2.06708
[1mStep[0m  [231/339], [94mLoss[0m : 3.00044
[1mStep[0m  [264/339], [94mLoss[0m : 2.87900
[1mStep[0m  [297/339], [94mLoss[0m : 3.05634
[1mStep[0m  [330/339], [94mLoss[0m : 2.54044

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.411, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47332
[1mStep[0m  [33/339], [94mLoss[0m : 2.48802
[1mStep[0m  [66/339], [94mLoss[0m : 2.97959
[1mStep[0m  [99/339], [94mLoss[0m : 2.34483
[1mStep[0m  [132/339], [94mLoss[0m : 2.32219
[1mStep[0m  [165/339], [94mLoss[0m : 2.72111
[1mStep[0m  [198/339], [94mLoss[0m : 2.58132
[1mStep[0m  [231/339], [94mLoss[0m : 2.40237
[1mStep[0m  [264/339], [94mLoss[0m : 1.99158
[1mStep[0m  [297/339], [94mLoss[0m : 2.84612
[1mStep[0m  [330/339], [94mLoss[0m : 3.15796

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.410
====================================

Phase 1 - Evaluation MAE:  2.4098824579103857
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.02005
[1mStep[0m  [33/339], [94mLoss[0m : 2.94410
[1mStep[0m  [66/339], [94mLoss[0m : 2.42451
[1mStep[0m  [99/339], [94mLoss[0m : 2.52269
[1mStep[0m  [132/339], [94mLoss[0m : 3.23481
[1mStep[0m  [165/339], [94mLoss[0m : 3.16826
[1mStep[0m  [198/339], [94mLoss[0m : 2.75494
[1mStep[0m  [231/339], [94mLoss[0m : 2.43712
[1mStep[0m  [264/339], [94mLoss[0m : 2.31234
[1mStep[0m  [297/339], [94mLoss[0m : 2.42829
[1mStep[0m  [330/339], [94mLoss[0m : 2.84397

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48411
[1mStep[0m  [33/339], [94mLoss[0m : 2.59082
[1mStep[0m  [66/339], [94mLoss[0m : 2.49492
[1mStep[0m  [99/339], [94mLoss[0m : 2.25015
[1mStep[0m  [132/339], [94mLoss[0m : 2.84074
[1mStep[0m  [165/339], [94mLoss[0m : 2.21780
[1mStep[0m  [198/339], [94mLoss[0m : 3.41809
[1mStep[0m  [231/339], [94mLoss[0m : 2.66771
[1mStep[0m  [264/339], [94mLoss[0m : 2.50256
[1mStep[0m  [297/339], [94mLoss[0m : 2.26833
[1mStep[0m  [330/339], [94mLoss[0m : 2.79504

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36572
[1mStep[0m  [33/339], [94mLoss[0m : 2.48100
[1mStep[0m  [66/339], [94mLoss[0m : 2.87282
[1mStep[0m  [99/339], [94mLoss[0m : 2.57512
[1mStep[0m  [132/339], [94mLoss[0m : 2.25467
[1mStep[0m  [165/339], [94mLoss[0m : 2.19157
[1mStep[0m  [198/339], [94mLoss[0m : 2.83437
[1mStep[0m  [231/339], [94mLoss[0m : 2.53996
[1mStep[0m  [264/339], [94mLoss[0m : 3.35869
[1mStep[0m  [297/339], [94mLoss[0m : 2.86839
[1mStep[0m  [330/339], [94mLoss[0m : 2.71745

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.403, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18165
[1mStep[0m  [33/339], [94mLoss[0m : 2.99818
[1mStep[0m  [66/339], [94mLoss[0m : 2.07919
[1mStep[0m  [99/339], [94mLoss[0m : 2.71466
[1mStep[0m  [132/339], [94mLoss[0m : 2.41924
[1mStep[0m  [165/339], [94mLoss[0m : 2.47519
[1mStep[0m  [198/339], [94mLoss[0m : 2.39646
[1mStep[0m  [231/339], [94mLoss[0m : 2.20378
[1mStep[0m  [264/339], [94mLoss[0m : 2.47929
[1mStep[0m  [297/339], [94mLoss[0m : 2.73734
[1mStep[0m  [330/339], [94mLoss[0m : 2.70278

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10840
[1mStep[0m  [33/339], [94mLoss[0m : 2.33712
[1mStep[0m  [66/339], [94mLoss[0m : 2.64214
[1mStep[0m  [99/339], [94mLoss[0m : 2.21744
[1mStep[0m  [132/339], [94mLoss[0m : 2.33858
[1mStep[0m  [165/339], [94mLoss[0m : 2.49371
[1mStep[0m  [198/339], [94mLoss[0m : 2.38320
[1mStep[0m  [231/339], [94mLoss[0m : 2.23657
[1mStep[0m  [264/339], [94mLoss[0m : 2.65442
[1mStep[0m  [297/339], [94mLoss[0m : 1.82955
[1mStep[0m  [330/339], [94mLoss[0m : 2.58023

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.66207
[1mStep[0m  [33/339], [94mLoss[0m : 2.98289
[1mStep[0m  [66/339], [94mLoss[0m : 2.22378
[1mStep[0m  [99/339], [94mLoss[0m : 2.13781
[1mStep[0m  [132/339], [94mLoss[0m : 2.43142
[1mStep[0m  [165/339], [94mLoss[0m : 2.28902
[1mStep[0m  [198/339], [94mLoss[0m : 2.82852
[1mStep[0m  [231/339], [94mLoss[0m : 2.08070
[1mStep[0m  [264/339], [94mLoss[0m : 2.65197
[1mStep[0m  [297/339], [94mLoss[0m : 2.67036
[1mStep[0m  [330/339], [94mLoss[0m : 2.51385

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27478
[1mStep[0m  [33/339], [94mLoss[0m : 2.09003
[1mStep[0m  [66/339], [94mLoss[0m : 2.22224
[1mStep[0m  [99/339], [94mLoss[0m : 2.91360
[1mStep[0m  [132/339], [94mLoss[0m : 2.13746
[1mStep[0m  [165/339], [94mLoss[0m : 2.48440
[1mStep[0m  [198/339], [94mLoss[0m : 2.41362
[1mStep[0m  [231/339], [94mLoss[0m : 3.04136
[1mStep[0m  [264/339], [94mLoss[0m : 2.60368
[1mStep[0m  [297/339], [94mLoss[0m : 2.86702
[1mStep[0m  [330/339], [94mLoss[0m : 2.32011

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49309
[1mStep[0m  [33/339], [94mLoss[0m : 2.11642
[1mStep[0m  [66/339], [94mLoss[0m : 2.17209
[1mStep[0m  [99/339], [94mLoss[0m : 2.04758
[1mStep[0m  [132/339], [94mLoss[0m : 1.73887
[1mStep[0m  [165/339], [94mLoss[0m : 2.85033
[1mStep[0m  [198/339], [94mLoss[0m : 1.69213
[1mStep[0m  [231/339], [94mLoss[0m : 3.19319
[1mStep[0m  [264/339], [94mLoss[0m : 1.99680
[1mStep[0m  [297/339], [94mLoss[0m : 3.03234
[1mStep[0m  [330/339], [94mLoss[0m : 2.83002

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70284
[1mStep[0m  [33/339], [94mLoss[0m : 2.75821
[1mStep[0m  [66/339], [94mLoss[0m : 2.55948
[1mStep[0m  [99/339], [94mLoss[0m : 2.31788
[1mStep[0m  [132/339], [94mLoss[0m : 2.56581
[1mStep[0m  [165/339], [94mLoss[0m : 2.46849
[1mStep[0m  [198/339], [94mLoss[0m : 2.41235
[1mStep[0m  [231/339], [94mLoss[0m : 2.18421
[1mStep[0m  [264/339], [94mLoss[0m : 2.18185
[1mStep[0m  [297/339], [94mLoss[0m : 1.91648
[1mStep[0m  [330/339], [94mLoss[0m : 2.61149

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59627
[1mStep[0m  [33/339], [94mLoss[0m : 2.58443
[1mStep[0m  [66/339], [94mLoss[0m : 2.18863
[1mStep[0m  [99/339], [94mLoss[0m : 1.89967
[1mStep[0m  [132/339], [94mLoss[0m : 2.42949
[1mStep[0m  [165/339], [94mLoss[0m : 2.44539
[1mStep[0m  [198/339], [94mLoss[0m : 2.65475
[1mStep[0m  [231/339], [94mLoss[0m : 2.65973
[1mStep[0m  [264/339], [94mLoss[0m : 2.58414
[1mStep[0m  [297/339], [94mLoss[0m : 2.40175
[1mStep[0m  [330/339], [94mLoss[0m : 2.61819

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92394
[1mStep[0m  [33/339], [94mLoss[0m : 1.96859
[1mStep[0m  [66/339], [94mLoss[0m : 2.40960
[1mStep[0m  [99/339], [94mLoss[0m : 2.74474
[1mStep[0m  [132/339], [94mLoss[0m : 2.60926
[1mStep[0m  [165/339], [94mLoss[0m : 1.81775
[1mStep[0m  [198/339], [94mLoss[0m : 2.38468
[1mStep[0m  [231/339], [94mLoss[0m : 2.44807
[1mStep[0m  [264/339], [94mLoss[0m : 2.35367
[1mStep[0m  [297/339], [94mLoss[0m : 2.92523
[1mStep[0m  [330/339], [94mLoss[0m : 2.50712

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86032
[1mStep[0m  [33/339], [94mLoss[0m : 2.63774
[1mStep[0m  [66/339], [94mLoss[0m : 2.15966
[1mStep[0m  [99/339], [94mLoss[0m : 2.23331
[1mStep[0m  [132/339], [94mLoss[0m : 1.88220
[1mStep[0m  [165/339], [94mLoss[0m : 2.26841
[1mStep[0m  [198/339], [94mLoss[0m : 2.37814
[1mStep[0m  [231/339], [94mLoss[0m : 2.33387
[1mStep[0m  [264/339], [94mLoss[0m : 2.62620
[1mStep[0m  [297/339], [94mLoss[0m : 2.37926
[1mStep[0m  [330/339], [94mLoss[0m : 2.72004

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30368
[1mStep[0m  [33/339], [94mLoss[0m : 2.22738
[1mStep[0m  [66/339], [94mLoss[0m : 2.43206
[1mStep[0m  [99/339], [94mLoss[0m : 2.40107
[1mStep[0m  [132/339], [94mLoss[0m : 2.16209
[1mStep[0m  [165/339], [94mLoss[0m : 2.72591
[1mStep[0m  [198/339], [94mLoss[0m : 2.16153
[1mStep[0m  [231/339], [94mLoss[0m : 2.30200
[1mStep[0m  [264/339], [94mLoss[0m : 2.19079
[1mStep[0m  [297/339], [94mLoss[0m : 2.34675
[1mStep[0m  [330/339], [94mLoss[0m : 2.46742

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62651
[1mStep[0m  [33/339], [94mLoss[0m : 1.96058
[1mStep[0m  [66/339], [94mLoss[0m : 1.86427
[1mStep[0m  [99/339], [94mLoss[0m : 2.05205
[1mStep[0m  [132/339], [94mLoss[0m : 2.12790
[1mStep[0m  [165/339], [94mLoss[0m : 2.42855
[1mStep[0m  [198/339], [94mLoss[0m : 2.04301
[1mStep[0m  [231/339], [94mLoss[0m : 2.05792
[1mStep[0m  [264/339], [94mLoss[0m : 2.38693
[1mStep[0m  [297/339], [94mLoss[0m : 2.59648
[1mStep[0m  [330/339], [94mLoss[0m : 2.20412

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27618
[1mStep[0m  [33/339], [94mLoss[0m : 3.25706
[1mStep[0m  [66/339], [94mLoss[0m : 1.89656
[1mStep[0m  [99/339], [94mLoss[0m : 1.96846
[1mStep[0m  [132/339], [94mLoss[0m : 2.47528
[1mStep[0m  [165/339], [94mLoss[0m : 2.42922
[1mStep[0m  [198/339], [94mLoss[0m : 2.32703
[1mStep[0m  [231/339], [94mLoss[0m : 3.86669
[1mStep[0m  [264/339], [94mLoss[0m : 2.17384
[1mStep[0m  [297/339], [94mLoss[0m : 2.41775
[1mStep[0m  [330/339], [94mLoss[0m : 1.95655

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75753
[1mStep[0m  [33/339], [94mLoss[0m : 2.41578
[1mStep[0m  [66/339], [94mLoss[0m : 2.19191
[1mStep[0m  [99/339], [94mLoss[0m : 2.03411
[1mStep[0m  [132/339], [94mLoss[0m : 2.36065
[1mStep[0m  [165/339], [94mLoss[0m : 2.46072
[1mStep[0m  [198/339], [94mLoss[0m : 2.06874
[1mStep[0m  [231/339], [94mLoss[0m : 2.64728
[1mStep[0m  [264/339], [94mLoss[0m : 2.36607
[1mStep[0m  [297/339], [94mLoss[0m : 2.23517
[1mStep[0m  [330/339], [94mLoss[0m : 1.62543

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49702
[1mStep[0m  [33/339], [94mLoss[0m : 2.56754
[1mStep[0m  [66/339], [94mLoss[0m : 2.11055
[1mStep[0m  [99/339], [94mLoss[0m : 2.98777
[1mStep[0m  [132/339], [94mLoss[0m : 2.15542
[1mStep[0m  [165/339], [94mLoss[0m : 2.26803
[1mStep[0m  [198/339], [94mLoss[0m : 2.79764
[1mStep[0m  [231/339], [94mLoss[0m : 2.67275
[1mStep[0m  [264/339], [94mLoss[0m : 2.57250
[1mStep[0m  [297/339], [94mLoss[0m : 2.01284
[1mStep[0m  [330/339], [94mLoss[0m : 2.26618

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58036
[1mStep[0m  [33/339], [94mLoss[0m : 2.60650
[1mStep[0m  [66/339], [94mLoss[0m : 2.06345
[1mStep[0m  [99/339], [94mLoss[0m : 2.05763
[1mStep[0m  [132/339], [94mLoss[0m : 2.14564
[1mStep[0m  [165/339], [94mLoss[0m : 2.46945
[1mStep[0m  [198/339], [94mLoss[0m : 2.82742
[1mStep[0m  [231/339], [94mLoss[0m : 1.89271
[1mStep[0m  [264/339], [94mLoss[0m : 1.60648
[1mStep[0m  [297/339], [94mLoss[0m : 2.73847
[1mStep[0m  [330/339], [94mLoss[0m : 2.18582

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59555
[1mStep[0m  [33/339], [94mLoss[0m : 1.94746
[1mStep[0m  [66/339], [94mLoss[0m : 2.21387
[1mStep[0m  [99/339], [94mLoss[0m : 2.36071
[1mStep[0m  [132/339], [94mLoss[0m : 2.27834
[1mStep[0m  [165/339], [94mLoss[0m : 2.41509
[1mStep[0m  [198/339], [94mLoss[0m : 2.00709
[1mStep[0m  [231/339], [94mLoss[0m : 2.48050
[1mStep[0m  [264/339], [94mLoss[0m : 2.39882
[1mStep[0m  [297/339], [94mLoss[0m : 2.32805
[1mStep[0m  [330/339], [94mLoss[0m : 3.14016

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21987
[1mStep[0m  [33/339], [94mLoss[0m : 2.20787
[1mStep[0m  [66/339], [94mLoss[0m : 2.08495
[1mStep[0m  [99/339], [94mLoss[0m : 2.16649
[1mStep[0m  [132/339], [94mLoss[0m : 2.75854
[1mStep[0m  [165/339], [94mLoss[0m : 2.84523
[1mStep[0m  [198/339], [94mLoss[0m : 1.97039
[1mStep[0m  [231/339], [94mLoss[0m : 1.75043
[1mStep[0m  [264/339], [94mLoss[0m : 1.77117
[1mStep[0m  [297/339], [94mLoss[0m : 2.28908
[1mStep[0m  [330/339], [94mLoss[0m : 2.08008

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.414, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33059
[1mStep[0m  [33/339], [94mLoss[0m : 1.47903
[1mStep[0m  [66/339], [94mLoss[0m : 1.73664
[1mStep[0m  [99/339], [94mLoss[0m : 1.87576
[1mStep[0m  [132/339], [94mLoss[0m : 1.91082
[1mStep[0m  [165/339], [94mLoss[0m : 1.79427
[1mStep[0m  [198/339], [94mLoss[0m : 1.76748
[1mStep[0m  [231/339], [94mLoss[0m : 2.14874
[1mStep[0m  [264/339], [94mLoss[0m : 2.39999
[1mStep[0m  [297/339], [94mLoss[0m : 2.21090
[1mStep[0m  [330/339], [94mLoss[0m : 2.23058

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.464, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66412
[1mStep[0m  [33/339], [94mLoss[0m : 2.31389
[1mStep[0m  [66/339], [94mLoss[0m : 1.82084
[1mStep[0m  [99/339], [94mLoss[0m : 2.28857
[1mStep[0m  [132/339], [94mLoss[0m : 1.95514
[1mStep[0m  [165/339], [94mLoss[0m : 2.34588
[1mStep[0m  [198/339], [94mLoss[0m : 2.23412
[1mStep[0m  [231/339], [94mLoss[0m : 2.31160
[1mStep[0m  [264/339], [94mLoss[0m : 1.96637
[1mStep[0m  [297/339], [94mLoss[0m : 2.01681
[1mStep[0m  [330/339], [94mLoss[0m : 2.72102

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.438, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95164
[1mStep[0m  [33/339], [94mLoss[0m : 1.92248
[1mStep[0m  [66/339], [94mLoss[0m : 2.45292
[1mStep[0m  [99/339], [94mLoss[0m : 1.82033
[1mStep[0m  [132/339], [94mLoss[0m : 2.80121
[1mStep[0m  [165/339], [94mLoss[0m : 2.08866
[1mStep[0m  [198/339], [94mLoss[0m : 2.21473
[1mStep[0m  [231/339], [94mLoss[0m : 2.74689
[1mStep[0m  [264/339], [94mLoss[0m : 1.47553
[1mStep[0m  [297/339], [94mLoss[0m : 1.68266
[1mStep[0m  [330/339], [94mLoss[0m : 2.88511

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.517, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28251
[1mStep[0m  [33/339], [94mLoss[0m : 1.89013
[1mStep[0m  [66/339], [94mLoss[0m : 2.61839
[1mStep[0m  [99/339], [94mLoss[0m : 2.01272
[1mStep[0m  [132/339], [94mLoss[0m : 2.70048
[1mStep[0m  [165/339], [94mLoss[0m : 1.70722
[1mStep[0m  [198/339], [94mLoss[0m : 2.44229
[1mStep[0m  [231/339], [94mLoss[0m : 2.49947
[1mStep[0m  [264/339], [94mLoss[0m : 2.27124
[1mStep[0m  [297/339], [94mLoss[0m : 2.27623
[1mStep[0m  [330/339], [94mLoss[0m : 1.69372

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.273, [92mTest[0m: 2.458, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26804
[1mStep[0m  [33/339], [94mLoss[0m : 2.46809
[1mStep[0m  [66/339], [94mLoss[0m : 1.77412
[1mStep[0m  [99/339], [94mLoss[0m : 1.70808
[1mStep[0m  [132/339], [94mLoss[0m : 2.14975
[1mStep[0m  [165/339], [94mLoss[0m : 2.13228
[1mStep[0m  [198/339], [94mLoss[0m : 1.84873
[1mStep[0m  [231/339], [94mLoss[0m : 3.14882
[1mStep[0m  [264/339], [94mLoss[0m : 2.36988
[1mStep[0m  [297/339], [94mLoss[0m : 2.57833
[1mStep[0m  [330/339], [94mLoss[0m : 2.01347

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.272, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85342
[1mStep[0m  [33/339], [94mLoss[0m : 2.36488
[1mStep[0m  [66/339], [94mLoss[0m : 2.27409
[1mStep[0m  [99/339], [94mLoss[0m : 2.35316
[1mStep[0m  [132/339], [94mLoss[0m : 2.46715
[1mStep[0m  [165/339], [94mLoss[0m : 2.28770
[1mStep[0m  [198/339], [94mLoss[0m : 2.47119
[1mStep[0m  [231/339], [94mLoss[0m : 2.13688
[1mStep[0m  [264/339], [94mLoss[0m : 2.68091
[1mStep[0m  [297/339], [94mLoss[0m : 2.27490
[1mStep[0m  [330/339], [94mLoss[0m : 2.39201

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.448, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25315
[1mStep[0m  [33/339], [94mLoss[0m : 2.30813
[1mStep[0m  [66/339], [94mLoss[0m : 2.79954
[1mStep[0m  [99/339], [94mLoss[0m : 2.01886
[1mStep[0m  [132/339], [94mLoss[0m : 2.04458
[1mStep[0m  [165/339], [94mLoss[0m : 1.79030
[1mStep[0m  [198/339], [94mLoss[0m : 2.29098
[1mStep[0m  [231/339], [94mLoss[0m : 2.34629
[1mStep[0m  [264/339], [94mLoss[0m : 1.64693
[1mStep[0m  [297/339], [94mLoss[0m : 2.15559
[1mStep[0m  [330/339], [94mLoss[0m : 2.47081

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.450, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11849
[1mStep[0m  [33/339], [94mLoss[0m : 2.82321
[1mStep[0m  [66/339], [94mLoss[0m : 2.14935
[1mStep[0m  [99/339], [94mLoss[0m : 1.60681
[1mStep[0m  [132/339], [94mLoss[0m : 2.11422
[1mStep[0m  [165/339], [94mLoss[0m : 2.03715
[1mStep[0m  [198/339], [94mLoss[0m : 1.85879
[1mStep[0m  [231/339], [94mLoss[0m : 2.38618
[1mStep[0m  [264/339], [94mLoss[0m : 2.77998
[1mStep[0m  [297/339], [94mLoss[0m : 2.33994
[1mStep[0m  [330/339], [94mLoss[0m : 2.09407

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99847
[1mStep[0m  [33/339], [94mLoss[0m : 2.71375
[1mStep[0m  [66/339], [94mLoss[0m : 2.55404
[1mStep[0m  [99/339], [94mLoss[0m : 1.61807
[1mStep[0m  [132/339], [94mLoss[0m : 2.15931
[1mStep[0m  [165/339], [94mLoss[0m : 1.66274
[1mStep[0m  [198/339], [94mLoss[0m : 2.21389
[1mStep[0m  [231/339], [94mLoss[0m : 3.11713
[1mStep[0m  [264/339], [94mLoss[0m : 2.25827
[1mStep[0m  [297/339], [94mLoss[0m : 2.83306
[1mStep[0m  [330/339], [94mLoss[0m : 2.86877

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.260, [92mTest[0m: 2.448, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33826
[1mStep[0m  [33/339], [94mLoss[0m : 2.17568
[1mStep[0m  [66/339], [94mLoss[0m : 2.86555
[1mStep[0m  [99/339], [94mLoss[0m : 1.97387
[1mStep[0m  [132/339], [94mLoss[0m : 2.51372
[1mStep[0m  [165/339], [94mLoss[0m : 2.29302
[1mStep[0m  [198/339], [94mLoss[0m : 2.31145
[1mStep[0m  [231/339], [94mLoss[0m : 1.80005
[1mStep[0m  [264/339], [94mLoss[0m : 2.38749
[1mStep[0m  [297/339], [94mLoss[0m : 2.32339
[1mStep[0m  [330/339], [94mLoss[0m : 2.15776

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.458, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.434
====================================

Phase 2 - Evaluation MAE:  2.4335274527558184
MAE score P1       2.409882
MAE score P2       2.433527
loss                2.24281
learning_rate          0.01
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.34880
[1mStep[0m  [16/169], [94mLoss[0m : 4.88422
[1mStep[0m  [32/169], [94mLoss[0m : 2.99405
[1mStep[0m  [48/169], [94mLoss[0m : 2.60916
[1mStep[0m  [64/169], [94mLoss[0m : 2.56799
[1mStep[0m  [80/169], [94mLoss[0m : 2.43406
[1mStep[0m  [96/169], [94mLoss[0m : 2.60879
[1mStep[0m  [112/169], [94mLoss[0m : 2.33510
[1mStep[0m  [128/169], [94mLoss[0m : 3.21778
[1mStep[0m  [144/169], [94mLoss[0m : 2.59946
[1mStep[0m  [160/169], [94mLoss[0m : 2.43442

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.213, [92mTest[0m: 11.119, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69483
[1mStep[0m  [16/169], [94mLoss[0m : 2.68443
[1mStep[0m  [32/169], [94mLoss[0m : 3.06914
[1mStep[0m  [48/169], [94mLoss[0m : 2.35636
[1mStep[0m  [64/169], [94mLoss[0m : 2.42890
[1mStep[0m  [80/169], [94mLoss[0m : 2.23222
[1mStep[0m  [96/169], [94mLoss[0m : 2.57861
[1mStep[0m  [112/169], [94mLoss[0m : 2.54440
[1mStep[0m  [128/169], [94mLoss[0m : 2.69807
[1mStep[0m  [144/169], [94mLoss[0m : 2.46626
[1mStep[0m  [160/169], [94mLoss[0m : 2.71123

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.587, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03652
[1mStep[0m  [16/169], [94mLoss[0m : 2.48743
[1mStep[0m  [32/169], [94mLoss[0m : 2.49784
[1mStep[0m  [48/169], [94mLoss[0m : 2.66836
[1mStep[0m  [64/169], [94mLoss[0m : 2.57662
[1mStep[0m  [80/169], [94mLoss[0m : 2.53395
[1mStep[0m  [96/169], [94mLoss[0m : 2.67339
[1mStep[0m  [112/169], [94mLoss[0m : 2.38324
[1mStep[0m  [128/169], [94mLoss[0m : 2.23143
[1mStep[0m  [144/169], [94mLoss[0m : 2.48578
[1mStep[0m  [160/169], [94mLoss[0m : 2.57446

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.541, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72485
[1mStep[0m  [16/169], [94mLoss[0m : 2.22985
[1mStep[0m  [32/169], [94mLoss[0m : 2.43957
[1mStep[0m  [48/169], [94mLoss[0m : 2.16938
[1mStep[0m  [64/169], [94mLoss[0m : 2.36210
[1mStep[0m  [80/169], [94mLoss[0m : 2.00757
[1mStep[0m  [96/169], [94mLoss[0m : 2.49202
[1mStep[0m  [112/169], [94mLoss[0m : 2.81613
[1mStep[0m  [128/169], [94mLoss[0m : 1.94367
[1mStep[0m  [144/169], [94mLoss[0m : 2.25434
[1mStep[0m  [160/169], [94mLoss[0m : 2.37641

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27330
[1mStep[0m  [16/169], [94mLoss[0m : 2.45909
[1mStep[0m  [32/169], [94mLoss[0m : 2.40962
[1mStep[0m  [48/169], [94mLoss[0m : 2.20235
[1mStep[0m  [64/169], [94mLoss[0m : 1.77269
[1mStep[0m  [80/169], [94mLoss[0m : 2.48588
[1mStep[0m  [96/169], [94mLoss[0m : 2.64323
[1mStep[0m  [112/169], [94mLoss[0m : 2.43804
[1mStep[0m  [128/169], [94mLoss[0m : 2.52036
[1mStep[0m  [144/169], [94mLoss[0m : 2.04740
[1mStep[0m  [160/169], [94mLoss[0m : 2.54766

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36489
[1mStep[0m  [16/169], [94mLoss[0m : 2.67732
[1mStep[0m  [32/169], [94mLoss[0m : 2.31129
[1mStep[0m  [48/169], [94mLoss[0m : 2.47643
[1mStep[0m  [64/169], [94mLoss[0m : 2.40220
[1mStep[0m  [80/169], [94mLoss[0m : 2.55001
[1mStep[0m  [96/169], [94mLoss[0m : 2.98174
[1mStep[0m  [112/169], [94mLoss[0m : 2.55709
[1mStep[0m  [128/169], [94mLoss[0m : 2.21332
[1mStep[0m  [144/169], [94mLoss[0m : 2.56744
[1mStep[0m  [160/169], [94mLoss[0m : 2.25932

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.93402
[1mStep[0m  [16/169], [94mLoss[0m : 2.35108
[1mStep[0m  [32/169], [94mLoss[0m : 2.41054
[1mStep[0m  [48/169], [94mLoss[0m : 2.55746
[1mStep[0m  [64/169], [94mLoss[0m : 2.85335
[1mStep[0m  [80/169], [94mLoss[0m : 1.91104
[1mStep[0m  [96/169], [94mLoss[0m : 2.21685
[1mStep[0m  [112/169], [94mLoss[0m : 2.46957
[1mStep[0m  [128/169], [94mLoss[0m : 2.12453
[1mStep[0m  [144/169], [94mLoss[0m : 2.42198
[1mStep[0m  [160/169], [94mLoss[0m : 2.66993

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17646
[1mStep[0m  [16/169], [94mLoss[0m : 2.00925
[1mStep[0m  [32/169], [94mLoss[0m : 2.45659
[1mStep[0m  [48/169], [94mLoss[0m : 2.75126
[1mStep[0m  [64/169], [94mLoss[0m : 2.12793
[1mStep[0m  [80/169], [94mLoss[0m : 2.77829
[1mStep[0m  [96/169], [94mLoss[0m : 2.55889
[1mStep[0m  [112/169], [94mLoss[0m : 2.66334
[1mStep[0m  [128/169], [94mLoss[0m : 2.19068
[1mStep[0m  [144/169], [94mLoss[0m : 2.27285
[1mStep[0m  [160/169], [94mLoss[0m : 2.34612

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31322
[1mStep[0m  [16/169], [94mLoss[0m : 2.34784
[1mStep[0m  [32/169], [94mLoss[0m : 2.81266
[1mStep[0m  [48/169], [94mLoss[0m : 2.06697
[1mStep[0m  [64/169], [94mLoss[0m : 2.77744
[1mStep[0m  [80/169], [94mLoss[0m : 2.38022
[1mStep[0m  [96/169], [94mLoss[0m : 2.68643
[1mStep[0m  [112/169], [94mLoss[0m : 2.58426
[1mStep[0m  [128/169], [94mLoss[0m : 2.16968
[1mStep[0m  [144/169], [94mLoss[0m : 2.41803
[1mStep[0m  [160/169], [94mLoss[0m : 2.30485

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23151
[1mStep[0m  [16/169], [94mLoss[0m : 2.40506
[1mStep[0m  [32/169], [94mLoss[0m : 2.41504
[1mStep[0m  [48/169], [94mLoss[0m : 2.38030
[1mStep[0m  [64/169], [94mLoss[0m : 2.42799
[1mStep[0m  [80/169], [94mLoss[0m : 2.20133
[1mStep[0m  [96/169], [94mLoss[0m : 2.23095
[1mStep[0m  [112/169], [94mLoss[0m : 2.24298
[1mStep[0m  [128/169], [94mLoss[0m : 2.36321
[1mStep[0m  [144/169], [94mLoss[0m : 2.35188
[1mStep[0m  [160/169], [94mLoss[0m : 2.19383

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16521
[1mStep[0m  [16/169], [94mLoss[0m : 2.47411
[1mStep[0m  [32/169], [94mLoss[0m : 2.98110
[1mStep[0m  [48/169], [94mLoss[0m : 2.69338
[1mStep[0m  [64/169], [94mLoss[0m : 2.54623
[1mStep[0m  [80/169], [94mLoss[0m : 2.29027
[1mStep[0m  [96/169], [94mLoss[0m : 2.38816
[1mStep[0m  [112/169], [94mLoss[0m : 2.45941
[1mStep[0m  [128/169], [94mLoss[0m : 2.18482
[1mStep[0m  [144/169], [94mLoss[0m : 3.05266
[1mStep[0m  [160/169], [94mLoss[0m : 2.28133

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35246
[1mStep[0m  [16/169], [94mLoss[0m : 2.48220
[1mStep[0m  [32/169], [94mLoss[0m : 2.85954
[1mStep[0m  [48/169], [94mLoss[0m : 2.28041
[1mStep[0m  [64/169], [94mLoss[0m : 2.47013
[1mStep[0m  [80/169], [94mLoss[0m : 2.68312
[1mStep[0m  [96/169], [94mLoss[0m : 2.55113
[1mStep[0m  [112/169], [94mLoss[0m : 2.60125
[1mStep[0m  [128/169], [94mLoss[0m : 2.67259
[1mStep[0m  [144/169], [94mLoss[0m : 2.44265
[1mStep[0m  [160/169], [94mLoss[0m : 2.48573

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84062
[1mStep[0m  [16/169], [94mLoss[0m : 2.27024
[1mStep[0m  [32/169], [94mLoss[0m : 2.48786
[1mStep[0m  [48/169], [94mLoss[0m : 2.92268
[1mStep[0m  [64/169], [94mLoss[0m : 1.88259
[1mStep[0m  [80/169], [94mLoss[0m : 2.29926
[1mStep[0m  [96/169], [94mLoss[0m : 2.27360
[1mStep[0m  [112/169], [94mLoss[0m : 2.12510
[1mStep[0m  [128/169], [94mLoss[0m : 1.98032
[1mStep[0m  [144/169], [94mLoss[0m : 2.40025
[1mStep[0m  [160/169], [94mLoss[0m : 2.59267

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51346
[1mStep[0m  [16/169], [94mLoss[0m : 2.37886
[1mStep[0m  [32/169], [94mLoss[0m : 2.11654
[1mStep[0m  [48/169], [94mLoss[0m : 2.40599
[1mStep[0m  [64/169], [94mLoss[0m : 2.58737
[1mStep[0m  [80/169], [94mLoss[0m : 2.32271
[1mStep[0m  [96/169], [94mLoss[0m : 2.24950
[1mStep[0m  [112/169], [94mLoss[0m : 2.60534
[1mStep[0m  [128/169], [94mLoss[0m : 2.56504
[1mStep[0m  [144/169], [94mLoss[0m : 2.35186
[1mStep[0m  [160/169], [94mLoss[0m : 2.40408

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73727
[1mStep[0m  [16/169], [94mLoss[0m : 2.07510
[1mStep[0m  [32/169], [94mLoss[0m : 2.86087
[1mStep[0m  [48/169], [94mLoss[0m : 2.52646
[1mStep[0m  [64/169], [94mLoss[0m : 2.10732
[1mStep[0m  [80/169], [94mLoss[0m : 2.61151
[1mStep[0m  [96/169], [94mLoss[0m : 2.56801
[1mStep[0m  [112/169], [94mLoss[0m : 2.25090
[1mStep[0m  [128/169], [94mLoss[0m : 2.53227
[1mStep[0m  [144/169], [94mLoss[0m : 2.32768
[1mStep[0m  [160/169], [94mLoss[0m : 2.19959

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23126
[1mStep[0m  [16/169], [94mLoss[0m : 2.26216
[1mStep[0m  [32/169], [94mLoss[0m : 2.12584
[1mStep[0m  [48/169], [94mLoss[0m : 2.43188
[1mStep[0m  [64/169], [94mLoss[0m : 2.15682
[1mStep[0m  [80/169], [94mLoss[0m : 2.62110
[1mStep[0m  [96/169], [94mLoss[0m : 2.41981
[1mStep[0m  [112/169], [94mLoss[0m : 2.44758
[1mStep[0m  [128/169], [94mLoss[0m : 2.82199
[1mStep[0m  [144/169], [94mLoss[0m : 2.35130
[1mStep[0m  [160/169], [94mLoss[0m : 2.56410

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79323
[1mStep[0m  [16/169], [94mLoss[0m : 2.05438
[1mStep[0m  [32/169], [94mLoss[0m : 2.25561
[1mStep[0m  [48/169], [94mLoss[0m : 2.20787
[1mStep[0m  [64/169], [94mLoss[0m : 2.81792
[1mStep[0m  [80/169], [94mLoss[0m : 2.09270
[1mStep[0m  [96/169], [94mLoss[0m : 1.98212
[1mStep[0m  [112/169], [94mLoss[0m : 2.33262
[1mStep[0m  [128/169], [94mLoss[0m : 2.51364
[1mStep[0m  [144/169], [94mLoss[0m : 2.39390
[1mStep[0m  [160/169], [94mLoss[0m : 2.29375

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48776
[1mStep[0m  [16/169], [94mLoss[0m : 2.32205
[1mStep[0m  [32/169], [94mLoss[0m : 2.15999
[1mStep[0m  [48/169], [94mLoss[0m : 2.46523
[1mStep[0m  [64/169], [94mLoss[0m : 2.22189
[1mStep[0m  [80/169], [94mLoss[0m : 2.43009
[1mStep[0m  [96/169], [94mLoss[0m : 2.25320
[1mStep[0m  [112/169], [94mLoss[0m : 2.36449
[1mStep[0m  [128/169], [94mLoss[0m : 2.34625
[1mStep[0m  [144/169], [94mLoss[0m : 2.35231
[1mStep[0m  [160/169], [94mLoss[0m : 2.51412

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12339
[1mStep[0m  [16/169], [94mLoss[0m : 2.76608
[1mStep[0m  [32/169], [94mLoss[0m : 2.24916
[1mStep[0m  [48/169], [94mLoss[0m : 1.94358
[1mStep[0m  [64/169], [94mLoss[0m : 2.10357
[1mStep[0m  [80/169], [94mLoss[0m : 2.33506
[1mStep[0m  [96/169], [94mLoss[0m : 2.65856
[1mStep[0m  [112/169], [94mLoss[0m : 2.66465
[1mStep[0m  [128/169], [94mLoss[0m : 2.22843
[1mStep[0m  [144/169], [94mLoss[0m : 2.29622
[1mStep[0m  [160/169], [94mLoss[0m : 2.20402

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52911
[1mStep[0m  [16/169], [94mLoss[0m : 2.47435
[1mStep[0m  [32/169], [94mLoss[0m : 2.14999
[1mStep[0m  [48/169], [94mLoss[0m : 2.38304
[1mStep[0m  [64/169], [94mLoss[0m : 2.51919
[1mStep[0m  [80/169], [94mLoss[0m : 2.34941
[1mStep[0m  [96/169], [94mLoss[0m : 2.19697
[1mStep[0m  [112/169], [94mLoss[0m : 2.08230
[1mStep[0m  [128/169], [94mLoss[0m : 2.19562
[1mStep[0m  [144/169], [94mLoss[0m : 2.23997
[1mStep[0m  [160/169], [94mLoss[0m : 2.02175

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66066
[1mStep[0m  [16/169], [94mLoss[0m : 2.29839
[1mStep[0m  [32/169], [94mLoss[0m : 2.52690
[1mStep[0m  [48/169], [94mLoss[0m : 2.54873
[1mStep[0m  [64/169], [94mLoss[0m : 2.25829
[1mStep[0m  [80/169], [94mLoss[0m : 2.01688
[1mStep[0m  [96/169], [94mLoss[0m : 2.50859
[1mStep[0m  [112/169], [94mLoss[0m : 2.05779
[1mStep[0m  [128/169], [94mLoss[0m : 2.29033
[1mStep[0m  [144/169], [94mLoss[0m : 2.23604
[1mStep[0m  [160/169], [94mLoss[0m : 2.37360

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11135
[1mStep[0m  [16/169], [94mLoss[0m : 2.35893
[1mStep[0m  [32/169], [94mLoss[0m : 1.77989
[1mStep[0m  [48/169], [94mLoss[0m : 2.08389
[1mStep[0m  [64/169], [94mLoss[0m : 1.99934
[1mStep[0m  [80/169], [94mLoss[0m : 2.41990
[1mStep[0m  [96/169], [94mLoss[0m : 2.31401
[1mStep[0m  [112/169], [94mLoss[0m : 2.54785
[1mStep[0m  [128/169], [94mLoss[0m : 2.47516
[1mStep[0m  [144/169], [94mLoss[0m : 2.08515
[1mStep[0m  [160/169], [94mLoss[0m : 1.90086

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31277
[1mStep[0m  [16/169], [94mLoss[0m : 2.33196
[1mStep[0m  [32/169], [94mLoss[0m : 2.47361
[1mStep[0m  [48/169], [94mLoss[0m : 2.15212
[1mStep[0m  [64/169], [94mLoss[0m : 2.21772
[1mStep[0m  [80/169], [94mLoss[0m : 2.15402
[1mStep[0m  [96/169], [94mLoss[0m : 2.50844
[1mStep[0m  [112/169], [94mLoss[0m : 2.07781
[1mStep[0m  [128/169], [94mLoss[0m : 2.50734
[1mStep[0m  [144/169], [94mLoss[0m : 2.61426
[1mStep[0m  [160/169], [94mLoss[0m : 2.04814

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22086
[1mStep[0m  [16/169], [94mLoss[0m : 2.58290
[1mStep[0m  [32/169], [94mLoss[0m : 2.05860
[1mStep[0m  [48/169], [94mLoss[0m : 2.37630
[1mStep[0m  [64/169], [94mLoss[0m : 2.41565
[1mStep[0m  [80/169], [94mLoss[0m : 2.27331
[1mStep[0m  [96/169], [94mLoss[0m : 2.20867
[1mStep[0m  [112/169], [94mLoss[0m : 1.90290
[1mStep[0m  [128/169], [94mLoss[0m : 2.21554
[1mStep[0m  [144/169], [94mLoss[0m : 2.39971
[1mStep[0m  [160/169], [94mLoss[0m : 1.97079

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06257
[1mStep[0m  [16/169], [94mLoss[0m : 2.72399
[1mStep[0m  [32/169], [94mLoss[0m : 1.97846
[1mStep[0m  [48/169], [94mLoss[0m : 2.30023
[1mStep[0m  [64/169], [94mLoss[0m : 2.34027
[1mStep[0m  [80/169], [94mLoss[0m : 2.28986
[1mStep[0m  [96/169], [94mLoss[0m : 2.09975
[1mStep[0m  [112/169], [94mLoss[0m : 2.75350
[1mStep[0m  [128/169], [94mLoss[0m : 2.24610
[1mStep[0m  [144/169], [94mLoss[0m : 2.61600
[1mStep[0m  [160/169], [94mLoss[0m : 2.39711

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.341, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09353
[1mStep[0m  [16/169], [94mLoss[0m : 1.87379
[1mStep[0m  [32/169], [94mLoss[0m : 2.35024
[1mStep[0m  [48/169], [94mLoss[0m : 2.43609
[1mStep[0m  [64/169], [94mLoss[0m : 2.08883
[1mStep[0m  [80/169], [94mLoss[0m : 2.65294
[1mStep[0m  [96/169], [94mLoss[0m : 1.79524
[1mStep[0m  [112/169], [94mLoss[0m : 2.55114
[1mStep[0m  [128/169], [94mLoss[0m : 2.22117
[1mStep[0m  [144/169], [94mLoss[0m : 2.50007
[1mStep[0m  [160/169], [94mLoss[0m : 2.55188

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17298
[1mStep[0m  [16/169], [94mLoss[0m : 1.99684
[1mStep[0m  [32/169], [94mLoss[0m : 2.47410
[1mStep[0m  [48/169], [94mLoss[0m : 2.27333
[1mStep[0m  [64/169], [94mLoss[0m : 2.14509
[1mStep[0m  [80/169], [94mLoss[0m : 2.34837
[1mStep[0m  [96/169], [94mLoss[0m : 2.25997
[1mStep[0m  [112/169], [94mLoss[0m : 2.56252
[1mStep[0m  [128/169], [94mLoss[0m : 2.47080
[1mStep[0m  [144/169], [94mLoss[0m : 1.88461
[1mStep[0m  [160/169], [94mLoss[0m : 2.46124

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75244
[1mStep[0m  [16/169], [94mLoss[0m : 2.43057
[1mStep[0m  [32/169], [94mLoss[0m : 1.99197
[1mStep[0m  [48/169], [94mLoss[0m : 1.81551
[1mStep[0m  [64/169], [94mLoss[0m : 2.49191
[1mStep[0m  [80/169], [94mLoss[0m : 2.65315
[1mStep[0m  [96/169], [94mLoss[0m : 2.23687
[1mStep[0m  [112/169], [94mLoss[0m : 2.03227
[1mStep[0m  [128/169], [94mLoss[0m : 2.46030
[1mStep[0m  [144/169], [94mLoss[0m : 2.05444
[1mStep[0m  [160/169], [94mLoss[0m : 2.14226

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38987
[1mStep[0m  [16/169], [94mLoss[0m : 2.00714
[1mStep[0m  [32/169], [94mLoss[0m : 2.23309
[1mStep[0m  [48/169], [94mLoss[0m : 2.34627
[1mStep[0m  [64/169], [94mLoss[0m : 1.99109
[1mStep[0m  [80/169], [94mLoss[0m : 1.96050
[1mStep[0m  [96/169], [94mLoss[0m : 2.17498
[1mStep[0m  [112/169], [94mLoss[0m : 2.51327
[1mStep[0m  [128/169], [94mLoss[0m : 2.78328
[1mStep[0m  [144/169], [94mLoss[0m : 2.54149
[1mStep[0m  [160/169], [94mLoss[0m : 2.44025

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50051
[1mStep[0m  [16/169], [94mLoss[0m : 2.69762
[1mStep[0m  [32/169], [94mLoss[0m : 2.23459
[1mStep[0m  [48/169], [94mLoss[0m : 1.83748
[1mStep[0m  [64/169], [94mLoss[0m : 2.37753
[1mStep[0m  [80/169], [94mLoss[0m : 3.00930
[1mStep[0m  [96/169], [94mLoss[0m : 2.00480
[1mStep[0m  [112/169], [94mLoss[0m : 2.17275
[1mStep[0m  [128/169], [94mLoss[0m : 2.13532
[1mStep[0m  [144/169], [94mLoss[0m : 2.08344
[1mStep[0m  [160/169], [94mLoss[0m : 2.19164

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.318, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.345
====================================

Phase 1 - Evaluation MAE:  2.34523312321731
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.05940
[1mStep[0m  [16/169], [94mLoss[0m : 2.64439
[1mStep[0m  [32/169], [94mLoss[0m : 2.64389
[1mStep[0m  [48/169], [94mLoss[0m : 2.78826
[1mStep[0m  [64/169], [94mLoss[0m : 1.87780
[1mStep[0m  [80/169], [94mLoss[0m : 2.78156
[1mStep[0m  [96/169], [94mLoss[0m : 2.29603
[1mStep[0m  [112/169], [94mLoss[0m : 2.53202
[1mStep[0m  [128/169], [94mLoss[0m : 2.14298
[1mStep[0m  [144/169], [94mLoss[0m : 1.90152
[1mStep[0m  [160/169], [94mLoss[0m : 2.48795

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40487
[1mStep[0m  [16/169], [94mLoss[0m : 1.86606
[1mStep[0m  [32/169], [94mLoss[0m : 2.65988
[1mStep[0m  [48/169], [94mLoss[0m : 2.96160
[1mStep[0m  [64/169], [94mLoss[0m : 2.60942
[1mStep[0m  [80/169], [94mLoss[0m : 1.91608
[1mStep[0m  [96/169], [94mLoss[0m : 1.80902
[1mStep[0m  [112/169], [94mLoss[0m : 2.59820
[1mStep[0m  [128/169], [94mLoss[0m : 2.31029
[1mStep[0m  [144/169], [94mLoss[0m : 1.82650
[1mStep[0m  [160/169], [94mLoss[0m : 2.55300

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94685
[1mStep[0m  [16/169], [94mLoss[0m : 2.32706
[1mStep[0m  [32/169], [94mLoss[0m : 2.20009
[1mStep[0m  [48/169], [94mLoss[0m : 2.33738
[1mStep[0m  [64/169], [94mLoss[0m : 1.99261
[1mStep[0m  [80/169], [94mLoss[0m : 2.25394
[1mStep[0m  [96/169], [94mLoss[0m : 2.42619
[1mStep[0m  [112/169], [94mLoss[0m : 2.33337
[1mStep[0m  [128/169], [94mLoss[0m : 2.35571
[1mStep[0m  [144/169], [94mLoss[0m : 2.68966
[1mStep[0m  [160/169], [94mLoss[0m : 2.08310

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98473
[1mStep[0m  [16/169], [94mLoss[0m : 2.25285
[1mStep[0m  [32/169], [94mLoss[0m : 2.14905
[1mStep[0m  [48/169], [94mLoss[0m : 2.06887
[1mStep[0m  [64/169], [94mLoss[0m : 2.05859
[1mStep[0m  [80/169], [94mLoss[0m : 1.78356
[1mStep[0m  [96/169], [94mLoss[0m : 2.03040
[1mStep[0m  [112/169], [94mLoss[0m : 2.18419
[1mStep[0m  [128/169], [94mLoss[0m : 1.85153
[1mStep[0m  [144/169], [94mLoss[0m : 1.84242
[1mStep[0m  [160/169], [94mLoss[0m : 2.56020

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.130, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39101
[1mStep[0m  [16/169], [94mLoss[0m : 2.29219
[1mStep[0m  [32/169], [94mLoss[0m : 1.96102
[1mStep[0m  [48/169], [94mLoss[0m : 2.05973
[1mStep[0m  [64/169], [94mLoss[0m : 1.87932
[1mStep[0m  [80/169], [94mLoss[0m : 1.70019
[1mStep[0m  [96/169], [94mLoss[0m : 2.25105
[1mStep[0m  [112/169], [94mLoss[0m : 2.15621
[1mStep[0m  [128/169], [94mLoss[0m : 1.95263
[1mStep[0m  [144/169], [94mLoss[0m : 2.63075
[1mStep[0m  [160/169], [94mLoss[0m : 2.02160

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.069, [92mTest[0m: 2.486, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81440
[1mStep[0m  [16/169], [94mLoss[0m : 1.75213
[1mStep[0m  [32/169], [94mLoss[0m : 2.16208
[1mStep[0m  [48/169], [94mLoss[0m : 1.91061
[1mStep[0m  [64/169], [94mLoss[0m : 2.05676
[1mStep[0m  [80/169], [94mLoss[0m : 2.00340
[1mStep[0m  [96/169], [94mLoss[0m : 1.84708
[1mStep[0m  [112/169], [94mLoss[0m : 2.01664
[1mStep[0m  [128/169], [94mLoss[0m : 1.75699
[1mStep[0m  [144/169], [94mLoss[0m : 1.91567
[1mStep[0m  [160/169], [94mLoss[0m : 1.94091

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98060
[1mStep[0m  [16/169], [94mLoss[0m : 1.85665
[1mStep[0m  [32/169], [94mLoss[0m : 1.62875
[1mStep[0m  [48/169], [94mLoss[0m : 2.03322
[1mStep[0m  [64/169], [94mLoss[0m : 1.59522
[1mStep[0m  [80/169], [94mLoss[0m : 2.06377
[1mStep[0m  [96/169], [94mLoss[0m : 1.89996
[1mStep[0m  [112/169], [94mLoss[0m : 2.10564
[1mStep[0m  [128/169], [94mLoss[0m : 2.31197
[1mStep[0m  [144/169], [94mLoss[0m : 2.22485
[1mStep[0m  [160/169], [94mLoss[0m : 1.64880

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.473, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02923
[1mStep[0m  [16/169], [94mLoss[0m : 1.60646
[1mStep[0m  [32/169], [94mLoss[0m : 1.76460
[1mStep[0m  [48/169], [94mLoss[0m : 1.89959
[1mStep[0m  [64/169], [94mLoss[0m : 1.87091
[1mStep[0m  [80/169], [94mLoss[0m : 1.79629
[1mStep[0m  [96/169], [94mLoss[0m : 1.78956
[1mStep[0m  [112/169], [94mLoss[0m : 2.00240
[1mStep[0m  [128/169], [94mLoss[0m : 2.00728
[1mStep[0m  [144/169], [94mLoss[0m : 1.91333
[1mStep[0m  [160/169], [94mLoss[0m : 1.60735

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94435
[1mStep[0m  [16/169], [94mLoss[0m : 1.85905
[1mStep[0m  [32/169], [94mLoss[0m : 1.58608
[1mStep[0m  [48/169], [94mLoss[0m : 1.98562
[1mStep[0m  [64/169], [94mLoss[0m : 1.98314
[1mStep[0m  [80/169], [94mLoss[0m : 1.95993
[1mStep[0m  [96/169], [94mLoss[0m : 1.80232
[1mStep[0m  [112/169], [94mLoss[0m : 1.87467
[1mStep[0m  [128/169], [94mLoss[0m : 1.73364
[1mStep[0m  [144/169], [94mLoss[0m : 1.92585
[1mStep[0m  [160/169], [94mLoss[0m : 1.71666

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82028
[1mStep[0m  [16/169], [94mLoss[0m : 1.82469
[1mStep[0m  [32/169], [94mLoss[0m : 1.90585
[1mStep[0m  [48/169], [94mLoss[0m : 1.68760
[1mStep[0m  [64/169], [94mLoss[0m : 1.80754
[1mStep[0m  [80/169], [94mLoss[0m : 1.84515
[1mStep[0m  [96/169], [94mLoss[0m : 1.75470
[1mStep[0m  [112/169], [94mLoss[0m : 1.72089
[1mStep[0m  [128/169], [94mLoss[0m : 1.87711
[1mStep[0m  [144/169], [94mLoss[0m : 2.20104
[1mStep[0m  [160/169], [94mLoss[0m : 1.74277

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.450, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85255
[1mStep[0m  [16/169], [94mLoss[0m : 1.34399
[1mStep[0m  [32/169], [94mLoss[0m : 1.80603
[1mStep[0m  [48/169], [94mLoss[0m : 1.72335
[1mStep[0m  [64/169], [94mLoss[0m : 1.46413
[1mStep[0m  [80/169], [94mLoss[0m : 1.77735
[1mStep[0m  [96/169], [94mLoss[0m : 1.44221
[1mStep[0m  [112/169], [94mLoss[0m : 1.56330
[1mStep[0m  [128/169], [94mLoss[0m : 1.56219
[1mStep[0m  [144/169], [94mLoss[0m : 1.88925
[1mStep[0m  [160/169], [94mLoss[0m : 2.00832

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83555
[1mStep[0m  [16/169], [94mLoss[0m : 1.44418
[1mStep[0m  [32/169], [94mLoss[0m : 1.77064
[1mStep[0m  [48/169], [94mLoss[0m : 1.57283
[1mStep[0m  [64/169], [94mLoss[0m : 1.61596
[1mStep[0m  [80/169], [94mLoss[0m : 1.83891
[1mStep[0m  [96/169], [94mLoss[0m : 1.51286
[1mStep[0m  [112/169], [94mLoss[0m : 1.84564
[1mStep[0m  [128/169], [94mLoss[0m : 1.19135
[1mStep[0m  [144/169], [94mLoss[0m : 2.00705
[1mStep[0m  [160/169], [94mLoss[0m : 1.61046

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.487, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46729
[1mStep[0m  [16/169], [94mLoss[0m : 1.62453
[1mStep[0m  [32/169], [94mLoss[0m : 1.40530
[1mStep[0m  [48/169], [94mLoss[0m : 1.71599
[1mStep[0m  [64/169], [94mLoss[0m : 1.55397
[1mStep[0m  [80/169], [94mLoss[0m : 1.57370
[1mStep[0m  [96/169], [94mLoss[0m : 1.68839
[1mStep[0m  [112/169], [94mLoss[0m : 1.92436
[1mStep[0m  [128/169], [94mLoss[0m : 1.68457
[1mStep[0m  [144/169], [94mLoss[0m : 1.70549
[1mStep[0m  [160/169], [94mLoss[0m : 1.74528

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00307
[1mStep[0m  [16/169], [94mLoss[0m : 1.47993
[1mStep[0m  [32/169], [94mLoss[0m : 1.54148
[1mStep[0m  [48/169], [94mLoss[0m : 1.51262
[1mStep[0m  [64/169], [94mLoss[0m : 1.81725
[1mStep[0m  [80/169], [94mLoss[0m : 1.48184
[1mStep[0m  [96/169], [94mLoss[0m : 1.70383
[1mStep[0m  [112/169], [94mLoss[0m : 1.41517
[1mStep[0m  [128/169], [94mLoss[0m : 1.50235
[1mStep[0m  [144/169], [94mLoss[0m : 1.68391
[1mStep[0m  [160/169], [94mLoss[0m : 1.66865

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87149
[1mStep[0m  [16/169], [94mLoss[0m : 1.63576
[1mStep[0m  [32/169], [94mLoss[0m : 1.54775
[1mStep[0m  [48/169], [94mLoss[0m : 1.32642
[1mStep[0m  [64/169], [94mLoss[0m : 2.28816
[1mStep[0m  [80/169], [94mLoss[0m : 2.07350
[1mStep[0m  [96/169], [94mLoss[0m : 1.59134
[1mStep[0m  [112/169], [94mLoss[0m : 2.02051
[1mStep[0m  [128/169], [94mLoss[0m : 1.75660
[1mStep[0m  [144/169], [94mLoss[0m : 1.45953
[1mStep[0m  [160/169], [94mLoss[0m : 1.42268

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.609, [92mTest[0m: 2.525, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51353
[1mStep[0m  [16/169], [94mLoss[0m : 1.63406
[1mStep[0m  [32/169], [94mLoss[0m : 1.64849
[1mStep[0m  [48/169], [94mLoss[0m : 1.55939
[1mStep[0m  [64/169], [94mLoss[0m : 1.56520
[1mStep[0m  [80/169], [94mLoss[0m : 1.31687
[1mStep[0m  [96/169], [94mLoss[0m : 1.97529
[1mStep[0m  [112/169], [94mLoss[0m : 1.84725
[1mStep[0m  [128/169], [94mLoss[0m : 1.76762
[1mStep[0m  [144/169], [94mLoss[0m : 1.87341
[1mStep[0m  [160/169], [94mLoss[0m : 1.67261

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28339
[1mStep[0m  [16/169], [94mLoss[0m : 1.49614
[1mStep[0m  [32/169], [94mLoss[0m : 1.19307
[1mStep[0m  [48/169], [94mLoss[0m : 1.58952
[1mStep[0m  [64/169], [94mLoss[0m : 1.30944
[1mStep[0m  [80/169], [94mLoss[0m : 1.27335
[1mStep[0m  [96/169], [94mLoss[0m : 1.88830
[1mStep[0m  [112/169], [94mLoss[0m : 1.49570
[1mStep[0m  [128/169], [94mLoss[0m : 1.40503
[1mStep[0m  [144/169], [94mLoss[0m : 1.54007
[1mStep[0m  [160/169], [94mLoss[0m : 1.52127

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.544, [92mTest[0m: 2.502, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.43562
[1mStep[0m  [16/169], [94mLoss[0m : 1.44227
[1mStep[0m  [32/169], [94mLoss[0m : 1.35929
[1mStep[0m  [48/169], [94mLoss[0m : 1.66327
[1mStep[0m  [64/169], [94mLoss[0m : 1.26223
[1mStep[0m  [80/169], [94mLoss[0m : 1.41248
[1mStep[0m  [96/169], [94mLoss[0m : 1.47317
[1mStep[0m  [112/169], [94mLoss[0m : 1.67871
[1mStep[0m  [128/169], [94mLoss[0m : 1.60063
[1mStep[0m  [144/169], [94mLoss[0m : 1.48391
[1mStep[0m  [160/169], [94mLoss[0m : 1.62570

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.517, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42261
[1mStep[0m  [16/169], [94mLoss[0m : 1.61077
[1mStep[0m  [32/169], [94mLoss[0m : 1.26368
[1mStep[0m  [48/169], [94mLoss[0m : 1.43857
[1mStep[0m  [64/169], [94mLoss[0m : 1.41614
[1mStep[0m  [80/169], [94mLoss[0m : 1.33203
[1mStep[0m  [96/169], [94mLoss[0m : 1.66741
[1mStep[0m  [112/169], [94mLoss[0m : 1.33116
[1mStep[0m  [128/169], [94mLoss[0m : 1.53298
[1mStep[0m  [144/169], [94mLoss[0m : 1.75407
[1mStep[0m  [160/169], [94mLoss[0m : 1.72362

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.520, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91564
[1mStep[0m  [16/169], [94mLoss[0m : 1.23874
[1mStep[0m  [32/169], [94mLoss[0m : 1.90300
[1mStep[0m  [48/169], [94mLoss[0m : 1.56262
[1mStep[0m  [64/169], [94mLoss[0m : 1.42439
[1mStep[0m  [80/169], [94mLoss[0m : 1.78352
[1mStep[0m  [96/169], [94mLoss[0m : 1.33510
[1mStep[0m  [112/169], [94mLoss[0m : 1.41896
[1mStep[0m  [128/169], [94mLoss[0m : 1.51584
[1mStep[0m  [144/169], [94mLoss[0m : 1.56543
[1mStep[0m  [160/169], [94mLoss[0m : 1.86471

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34908
[1mStep[0m  [16/169], [94mLoss[0m : 1.42857
[1mStep[0m  [32/169], [94mLoss[0m : 1.54246
[1mStep[0m  [48/169], [94mLoss[0m : 1.55006
[1mStep[0m  [64/169], [94mLoss[0m : 1.42730
[1mStep[0m  [80/169], [94mLoss[0m : 1.51216
[1mStep[0m  [96/169], [94mLoss[0m : 1.36842
[1mStep[0m  [112/169], [94mLoss[0m : 1.43186
[1mStep[0m  [128/169], [94mLoss[0m : 1.36508
[1mStep[0m  [144/169], [94mLoss[0m : 1.20261
[1mStep[0m  [160/169], [94mLoss[0m : 1.56850

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.16136
[1mStep[0m  [16/169], [94mLoss[0m : 1.32832
[1mStep[0m  [32/169], [94mLoss[0m : 1.29637
[1mStep[0m  [48/169], [94mLoss[0m : 1.37560
[1mStep[0m  [64/169], [94mLoss[0m : 1.42998
[1mStep[0m  [80/169], [94mLoss[0m : 1.61383
[1mStep[0m  [96/169], [94mLoss[0m : 1.29926
[1mStep[0m  [112/169], [94mLoss[0m : 1.18435
[1mStep[0m  [128/169], [94mLoss[0m : 1.47067
[1mStep[0m  [144/169], [94mLoss[0m : 1.53221
[1mStep[0m  [160/169], [94mLoss[0m : 1.32546

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.396, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34770
[1mStep[0m  [16/169], [94mLoss[0m : 1.36221
[1mStep[0m  [32/169], [94mLoss[0m : 1.95478
[1mStep[0m  [48/169], [94mLoss[0m : 1.30985
[1mStep[0m  [64/169], [94mLoss[0m : 1.37007
[1mStep[0m  [80/169], [94mLoss[0m : 1.53148
[1mStep[0m  [96/169], [94mLoss[0m : 1.70761
[1mStep[0m  [112/169], [94mLoss[0m : 1.25815
[1mStep[0m  [128/169], [94mLoss[0m : 1.39660
[1mStep[0m  [144/169], [94mLoss[0m : 1.32231
[1mStep[0m  [160/169], [94mLoss[0m : 1.70800

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.412, [92mTest[0m: 2.488, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 22 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.521
====================================

Phase 2 - Evaluation MAE:  2.5210323291165486
MAE score P1      2.345233
MAE score P2      2.521032
loss              1.396126
learning_rate         0.01
batch_size              64
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay        0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.31195
[1mStep[0m  [16/169], [94mLoss[0m : 10.41051
[1mStep[0m  [32/169], [94mLoss[0m : 10.07373
[1mStep[0m  [48/169], [94mLoss[0m : 9.64199
[1mStep[0m  [64/169], [94mLoss[0m : 9.04071
[1mStep[0m  [80/169], [94mLoss[0m : 9.41855
[1mStep[0m  [96/169], [94mLoss[0m : 9.20469
[1mStep[0m  [112/169], [94mLoss[0m : 9.76047
[1mStep[0m  [128/169], [94mLoss[0m : 7.58847
[1mStep[0m  [144/169], [94mLoss[0m : 7.89857
[1mStep[0m  [160/169], [94mLoss[0m : 6.86928

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.297, [92mTest[0m: 10.795, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.89791
[1mStep[0m  [16/169], [94mLoss[0m : 7.68031
[1mStep[0m  [32/169], [94mLoss[0m : 5.46811
[1mStep[0m  [48/169], [94mLoss[0m : 6.13747
[1mStep[0m  [64/169], [94mLoss[0m : 5.64957
[1mStep[0m  [80/169], [94mLoss[0m : 5.14056
[1mStep[0m  [96/169], [94mLoss[0m : 5.17716
[1mStep[0m  [112/169], [94mLoss[0m : 4.69572
[1mStep[0m  [128/169], [94mLoss[0m : 3.90149
[1mStep[0m  [144/169], [94mLoss[0m : 3.89981
[1mStep[0m  [160/169], [94mLoss[0m : 4.06315

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.374, [92mTest[0m: 6.626, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.07760
[1mStep[0m  [16/169], [94mLoss[0m : 3.84254
[1mStep[0m  [32/169], [94mLoss[0m : 3.35496
[1mStep[0m  [48/169], [94mLoss[0m : 3.24187
[1mStep[0m  [64/169], [94mLoss[0m : 3.21926
[1mStep[0m  [80/169], [94mLoss[0m : 3.06030
[1mStep[0m  [96/169], [94mLoss[0m : 2.79969
[1mStep[0m  [112/169], [94mLoss[0m : 2.67259
[1mStep[0m  [128/169], [94mLoss[0m : 2.34538
[1mStep[0m  [144/169], [94mLoss[0m : 2.33196
[1mStep[0m  [160/169], [94mLoss[0m : 2.26178

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.932, [92mTest[0m: 2.940, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91194
[1mStep[0m  [16/169], [94mLoss[0m : 3.04345
[1mStep[0m  [32/169], [94mLoss[0m : 2.83669
[1mStep[0m  [48/169], [94mLoss[0m : 2.93614
[1mStep[0m  [64/169], [94mLoss[0m : 2.76471
[1mStep[0m  [80/169], [94mLoss[0m : 2.50598
[1mStep[0m  [96/169], [94mLoss[0m : 2.52619
[1mStep[0m  [112/169], [94mLoss[0m : 2.71466
[1mStep[0m  [128/169], [94mLoss[0m : 2.42879
[1mStep[0m  [144/169], [94mLoss[0m : 2.18373
[1mStep[0m  [160/169], [94mLoss[0m : 2.45832

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42693
[1mStep[0m  [16/169], [94mLoss[0m : 2.85054
[1mStep[0m  [32/169], [94mLoss[0m : 2.28329
[1mStep[0m  [48/169], [94mLoss[0m : 2.72660
[1mStep[0m  [64/169], [94mLoss[0m : 2.73116
[1mStep[0m  [80/169], [94mLoss[0m : 2.66960
[1mStep[0m  [96/169], [94mLoss[0m : 2.54423
[1mStep[0m  [112/169], [94mLoss[0m : 2.51203
[1mStep[0m  [128/169], [94mLoss[0m : 2.64194
[1mStep[0m  [144/169], [94mLoss[0m : 2.18988
[1mStep[0m  [160/169], [94mLoss[0m : 3.11054

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77709
[1mStep[0m  [16/169], [94mLoss[0m : 3.38160
[1mStep[0m  [32/169], [94mLoss[0m : 2.50156
[1mStep[0m  [48/169], [94mLoss[0m : 2.58143
[1mStep[0m  [64/169], [94mLoss[0m : 2.45581
[1mStep[0m  [80/169], [94mLoss[0m : 2.78211
[1mStep[0m  [96/169], [94mLoss[0m : 2.51301
[1mStep[0m  [112/169], [94mLoss[0m : 2.90120
[1mStep[0m  [128/169], [94mLoss[0m : 2.20769
[1mStep[0m  [144/169], [94mLoss[0m : 2.79003
[1mStep[0m  [160/169], [94mLoss[0m : 2.84470

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36076
[1mStep[0m  [16/169], [94mLoss[0m : 2.25011
[1mStep[0m  [32/169], [94mLoss[0m : 2.34564
[1mStep[0m  [48/169], [94mLoss[0m : 2.69598
[1mStep[0m  [64/169], [94mLoss[0m : 2.71528
[1mStep[0m  [80/169], [94mLoss[0m : 2.19296
[1mStep[0m  [96/169], [94mLoss[0m : 2.53883
[1mStep[0m  [112/169], [94mLoss[0m : 2.78152
[1mStep[0m  [128/169], [94mLoss[0m : 2.88538
[1mStep[0m  [144/169], [94mLoss[0m : 2.73040
[1mStep[0m  [160/169], [94mLoss[0m : 2.82473

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.588, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39802
[1mStep[0m  [16/169], [94mLoss[0m : 2.89215
[1mStep[0m  [32/169], [94mLoss[0m : 2.43749
[1mStep[0m  [48/169], [94mLoss[0m : 2.23713
[1mStep[0m  [64/169], [94mLoss[0m : 2.26012
[1mStep[0m  [80/169], [94mLoss[0m : 2.67993
[1mStep[0m  [96/169], [94mLoss[0m : 2.46467
[1mStep[0m  [112/169], [94mLoss[0m : 2.88209
[1mStep[0m  [128/169], [94mLoss[0m : 2.72725
[1mStep[0m  [144/169], [94mLoss[0m : 2.57689
[1mStep[0m  [160/169], [94mLoss[0m : 2.49616

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.45788
[1mStep[0m  [16/169], [94mLoss[0m : 2.43668
[1mStep[0m  [32/169], [94mLoss[0m : 2.44098
[1mStep[0m  [48/169], [94mLoss[0m : 2.57512
[1mStep[0m  [64/169], [94mLoss[0m : 2.62924
[1mStep[0m  [80/169], [94mLoss[0m : 2.76706
[1mStep[0m  [96/169], [94mLoss[0m : 2.30673
[1mStep[0m  [112/169], [94mLoss[0m : 3.09762
[1mStep[0m  [128/169], [94mLoss[0m : 2.53103
[1mStep[0m  [144/169], [94mLoss[0m : 2.98896
[1mStep[0m  [160/169], [94mLoss[0m : 2.17931

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31432
[1mStep[0m  [16/169], [94mLoss[0m : 2.89088
[1mStep[0m  [32/169], [94mLoss[0m : 2.77144
[1mStep[0m  [48/169], [94mLoss[0m : 2.64302
[1mStep[0m  [64/169], [94mLoss[0m : 2.49328
[1mStep[0m  [80/169], [94mLoss[0m : 2.23196
[1mStep[0m  [96/169], [94mLoss[0m : 3.06149
[1mStep[0m  [112/169], [94mLoss[0m : 2.10307
[1mStep[0m  [128/169], [94mLoss[0m : 2.18868
[1mStep[0m  [144/169], [94mLoss[0m : 2.39147
[1mStep[0m  [160/169], [94mLoss[0m : 2.43273

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.350, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42563
[1mStep[0m  [16/169], [94mLoss[0m : 2.62849
[1mStep[0m  [32/169], [94mLoss[0m : 2.36005
[1mStep[0m  [48/169], [94mLoss[0m : 2.20108
[1mStep[0m  [64/169], [94mLoss[0m : 2.84440
[1mStep[0m  [80/169], [94mLoss[0m : 2.10200
[1mStep[0m  [96/169], [94mLoss[0m : 2.34476
[1mStep[0m  [112/169], [94mLoss[0m : 2.40872
[1mStep[0m  [128/169], [94mLoss[0m : 2.96319
[1mStep[0m  [144/169], [94mLoss[0m : 2.64196
[1mStep[0m  [160/169], [94mLoss[0m : 2.55102

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63159
[1mStep[0m  [16/169], [94mLoss[0m : 2.53748
[1mStep[0m  [32/169], [94mLoss[0m : 2.93398
[1mStep[0m  [48/169], [94mLoss[0m : 2.57357
[1mStep[0m  [64/169], [94mLoss[0m : 2.30045
[1mStep[0m  [80/169], [94mLoss[0m : 2.62114
[1mStep[0m  [96/169], [94mLoss[0m : 2.70339
[1mStep[0m  [112/169], [94mLoss[0m : 2.40332
[1mStep[0m  [128/169], [94mLoss[0m : 2.42147
[1mStep[0m  [144/169], [94mLoss[0m : 2.76031
[1mStep[0m  [160/169], [94mLoss[0m : 2.16799

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17499
[1mStep[0m  [16/169], [94mLoss[0m : 2.48505
[1mStep[0m  [32/169], [94mLoss[0m : 2.45433
[1mStep[0m  [48/169], [94mLoss[0m : 2.71676
[1mStep[0m  [64/169], [94mLoss[0m : 2.94785
[1mStep[0m  [80/169], [94mLoss[0m : 2.08651
[1mStep[0m  [96/169], [94mLoss[0m : 2.03475
[1mStep[0m  [112/169], [94mLoss[0m : 2.60551
[1mStep[0m  [128/169], [94mLoss[0m : 2.58966
[1mStep[0m  [144/169], [94mLoss[0m : 2.93142
[1mStep[0m  [160/169], [94mLoss[0m : 2.30214

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57670
[1mStep[0m  [16/169], [94mLoss[0m : 2.18316
[1mStep[0m  [32/169], [94mLoss[0m : 2.42490
[1mStep[0m  [48/169], [94mLoss[0m : 2.75051
[1mStep[0m  [64/169], [94mLoss[0m : 2.74665
[1mStep[0m  [80/169], [94mLoss[0m : 1.96003
[1mStep[0m  [96/169], [94mLoss[0m : 2.58263
[1mStep[0m  [112/169], [94mLoss[0m : 2.49843
[1mStep[0m  [128/169], [94mLoss[0m : 2.06708
[1mStep[0m  [144/169], [94mLoss[0m : 2.67982
[1mStep[0m  [160/169], [94mLoss[0m : 2.17592

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52556
[1mStep[0m  [16/169], [94mLoss[0m : 2.85813
[1mStep[0m  [32/169], [94mLoss[0m : 2.34770
[1mStep[0m  [48/169], [94mLoss[0m : 2.69513
[1mStep[0m  [64/169], [94mLoss[0m : 2.17989
[1mStep[0m  [80/169], [94mLoss[0m : 2.33591
[1mStep[0m  [96/169], [94mLoss[0m : 2.99302
[1mStep[0m  [112/169], [94mLoss[0m : 2.36156
[1mStep[0m  [128/169], [94mLoss[0m : 2.24429
[1mStep[0m  [144/169], [94mLoss[0m : 2.49965
[1mStep[0m  [160/169], [94mLoss[0m : 2.14015

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24235
[1mStep[0m  [16/169], [94mLoss[0m : 2.62418
[1mStep[0m  [32/169], [94mLoss[0m : 2.24599
[1mStep[0m  [48/169], [94mLoss[0m : 2.70844
[1mStep[0m  [64/169], [94mLoss[0m : 1.97750
[1mStep[0m  [80/169], [94mLoss[0m : 2.27173
[1mStep[0m  [96/169], [94mLoss[0m : 2.01950
[1mStep[0m  [112/169], [94mLoss[0m : 2.21349
[1mStep[0m  [128/169], [94mLoss[0m : 2.42177
[1mStep[0m  [144/169], [94mLoss[0m : 1.78925
[1mStep[0m  [160/169], [94mLoss[0m : 2.64976

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39265
[1mStep[0m  [16/169], [94mLoss[0m : 2.62466
[1mStep[0m  [32/169], [94mLoss[0m : 2.45005
[1mStep[0m  [48/169], [94mLoss[0m : 2.07830
[1mStep[0m  [64/169], [94mLoss[0m : 2.28282
[1mStep[0m  [80/169], [94mLoss[0m : 2.28171
[1mStep[0m  [96/169], [94mLoss[0m : 2.11727
[1mStep[0m  [112/169], [94mLoss[0m : 2.52371
[1mStep[0m  [128/169], [94mLoss[0m : 2.62621
[1mStep[0m  [144/169], [94mLoss[0m : 2.33288
[1mStep[0m  [160/169], [94mLoss[0m : 2.31751

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09676
[1mStep[0m  [16/169], [94mLoss[0m : 2.94370
[1mStep[0m  [32/169], [94mLoss[0m : 2.44187
[1mStep[0m  [48/169], [94mLoss[0m : 2.30190
[1mStep[0m  [64/169], [94mLoss[0m : 2.81566
[1mStep[0m  [80/169], [94mLoss[0m : 2.36209
[1mStep[0m  [96/169], [94mLoss[0m : 2.04330
[1mStep[0m  [112/169], [94mLoss[0m : 2.16132
[1mStep[0m  [128/169], [94mLoss[0m : 2.91748
[1mStep[0m  [144/169], [94mLoss[0m : 2.55033
[1mStep[0m  [160/169], [94mLoss[0m : 2.64769

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29260
[1mStep[0m  [16/169], [94mLoss[0m : 2.91224
[1mStep[0m  [32/169], [94mLoss[0m : 2.40137
[1mStep[0m  [48/169], [94mLoss[0m : 2.57089
[1mStep[0m  [64/169], [94mLoss[0m : 2.83005
[1mStep[0m  [80/169], [94mLoss[0m : 2.08382
[1mStep[0m  [96/169], [94mLoss[0m : 2.55477
[1mStep[0m  [112/169], [94mLoss[0m : 2.52666
[1mStep[0m  [128/169], [94mLoss[0m : 2.60864
[1mStep[0m  [144/169], [94mLoss[0m : 2.15307
[1mStep[0m  [160/169], [94mLoss[0m : 2.68425

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25162
[1mStep[0m  [16/169], [94mLoss[0m : 2.45951
[1mStep[0m  [32/169], [94mLoss[0m : 2.34011
[1mStep[0m  [48/169], [94mLoss[0m : 2.48526
[1mStep[0m  [64/169], [94mLoss[0m : 2.73274
[1mStep[0m  [80/169], [94mLoss[0m : 2.55526
[1mStep[0m  [96/169], [94mLoss[0m : 2.67003
[1mStep[0m  [112/169], [94mLoss[0m : 2.26715
[1mStep[0m  [128/169], [94mLoss[0m : 2.96566
[1mStep[0m  [144/169], [94mLoss[0m : 2.24012
[1mStep[0m  [160/169], [94mLoss[0m : 2.36981

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23150
[1mStep[0m  [16/169], [94mLoss[0m : 2.36676
[1mStep[0m  [32/169], [94mLoss[0m : 2.68850
[1mStep[0m  [48/169], [94mLoss[0m : 2.53068
[1mStep[0m  [64/169], [94mLoss[0m : 2.73300
[1mStep[0m  [80/169], [94mLoss[0m : 2.23171
[1mStep[0m  [96/169], [94mLoss[0m : 2.21874
[1mStep[0m  [112/169], [94mLoss[0m : 2.63387
[1mStep[0m  [128/169], [94mLoss[0m : 2.30479
[1mStep[0m  [144/169], [94mLoss[0m : 2.10914
[1mStep[0m  [160/169], [94mLoss[0m : 2.18028

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58057
[1mStep[0m  [16/169], [94mLoss[0m : 2.36593
[1mStep[0m  [32/169], [94mLoss[0m : 2.43089
[1mStep[0m  [48/169], [94mLoss[0m : 2.19016
[1mStep[0m  [64/169], [94mLoss[0m : 2.19145
[1mStep[0m  [80/169], [94mLoss[0m : 2.75322
[1mStep[0m  [96/169], [94mLoss[0m : 2.55921
[1mStep[0m  [112/169], [94mLoss[0m : 2.23272
[1mStep[0m  [128/169], [94mLoss[0m : 2.31525
[1mStep[0m  [144/169], [94mLoss[0m : 2.22574
[1mStep[0m  [160/169], [94mLoss[0m : 2.32552

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55989
[1mStep[0m  [16/169], [94mLoss[0m : 2.44437
[1mStep[0m  [32/169], [94mLoss[0m : 2.46473
[1mStep[0m  [48/169], [94mLoss[0m : 2.39512
[1mStep[0m  [64/169], [94mLoss[0m : 2.18197
[1mStep[0m  [80/169], [94mLoss[0m : 2.06336
[1mStep[0m  [96/169], [94mLoss[0m : 2.53816
[1mStep[0m  [112/169], [94mLoss[0m : 2.71880
[1mStep[0m  [128/169], [94mLoss[0m : 2.58988
[1mStep[0m  [144/169], [94mLoss[0m : 2.24310
[1mStep[0m  [160/169], [94mLoss[0m : 2.43773

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22434
[1mStep[0m  [16/169], [94mLoss[0m : 2.81265
[1mStep[0m  [32/169], [94mLoss[0m : 2.32455
[1mStep[0m  [48/169], [94mLoss[0m : 2.71952
[1mStep[0m  [64/169], [94mLoss[0m : 2.88611
[1mStep[0m  [80/169], [94mLoss[0m : 2.47724
[1mStep[0m  [96/169], [94mLoss[0m : 2.54015
[1mStep[0m  [112/169], [94mLoss[0m : 2.56052
[1mStep[0m  [128/169], [94mLoss[0m : 2.56312
[1mStep[0m  [144/169], [94mLoss[0m : 2.74590
[1mStep[0m  [160/169], [94mLoss[0m : 2.56816

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23743
[1mStep[0m  [16/169], [94mLoss[0m : 2.52478
[1mStep[0m  [32/169], [94mLoss[0m : 2.33465
[1mStep[0m  [48/169], [94mLoss[0m : 2.57068
[1mStep[0m  [64/169], [94mLoss[0m : 2.59846
[1mStep[0m  [80/169], [94mLoss[0m : 2.87858
[1mStep[0m  [96/169], [94mLoss[0m : 2.38579
[1mStep[0m  [112/169], [94mLoss[0m : 2.36164
[1mStep[0m  [128/169], [94mLoss[0m : 2.23623
[1mStep[0m  [144/169], [94mLoss[0m : 2.11195
[1mStep[0m  [160/169], [94mLoss[0m : 2.33707

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.353, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71455
[1mStep[0m  [16/169], [94mLoss[0m : 2.05929
[1mStep[0m  [32/169], [94mLoss[0m : 2.94770
[1mStep[0m  [48/169], [94mLoss[0m : 2.46581
[1mStep[0m  [64/169], [94mLoss[0m : 2.17243
[1mStep[0m  [80/169], [94mLoss[0m : 1.77714
[1mStep[0m  [96/169], [94mLoss[0m : 2.45810
[1mStep[0m  [112/169], [94mLoss[0m : 2.32799
[1mStep[0m  [128/169], [94mLoss[0m : 2.47907
[1mStep[0m  [144/169], [94mLoss[0m : 2.49700
[1mStep[0m  [160/169], [94mLoss[0m : 2.13067

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68766
[1mStep[0m  [16/169], [94mLoss[0m : 2.35160
[1mStep[0m  [32/169], [94mLoss[0m : 2.13697
[1mStep[0m  [48/169], [94mLoss[0m : 2.37188
[1mStep[0m  [64/169], [94mLoss[0m : 2.60379
[1mStep[0m  [80/169], [94mLoss[0m : 2.39216
[1mStep[0m  [96/169], [94mLoss[0m : 2.57714
[1mStep[0m  [112/169], [94mLoss[0m : 2.74699
[1mStep[0m  [128/169], [94mLoss[0m : 2.49107
[1mStep[0m  [144/169], [94mLoss[0m : 2.67096
[1mStep[0m  [160/169], [94mLoss[0m : 2.73911

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.352, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51386
[1mStep[0m  [16/169], [94mLoss[0m : 2.20431
[1mStep[0m  [32/169], [94mLoss[0m : 2.42747
[1mStep[0m  [48/169], [94mLoss[0m : 2.92892
[1mStep[0m  [64/169], [94mLoss[0m : 2.62135
[1mStep[0m  [80/169], [94mLoss[0m : 2.55121
[1mStep[0m  [96/169], [94mLoss[0m : 2.76214
[1mStep[0m  [112/169], [94mLoss[0m : 2.59430
[1mStep[0m  [128/169], [94mLoss[0m : 2.40161
[1mStep[0m  [144/169], [94mLoss[0m : 1.93906
[1mStep[0m  [160/169], [94mLoss[0m : 2.57674

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43392
[1mStep[0m  [16/169], [94mLoss[0m : 1.95758
[1mStep[0m  [32/169], [94mLoss[0m : 2.55168
[1mStep[0m  [48/169], [94mLoss[0m : 2.57479
[1mStep[0m  [64/169], [94mLoss[0m : 2.11887
[1mStep[0m  [80/169], [94mLoss[0m : 2.63020
[1mStep[0m  [96/169], [94mLoss[0m : 2.45035
[1mStep[0m  [112/169], [94mLoss[0m : 2.10079
[1mStep[0m  [128/169], [94mLoss[0m : 2.34278
[1mStep[0m  [144/169], [94mLoss[0m : 2.14379
[1mStep[0m  [160/169], [94mLoss[0m : 2.15105

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.348, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36392
[1mStep[0m  [16/169], [94mLoss[0m : 2.52997
[1mStep[0m  [32/169], [94mLoss[0m : 2.59444
[1mStep[0m  [48/169], [94mLoss[0m : 2.50048
[1mStep[0m  [64/169], [94mLoss[0m : 2.50805
[1mStep[0m  [80/169], [94mLoss[0m : 2.25461
[1mStep[0m  [96/169], [94mLoss[0m : 2.59927
[1mStep[0m  [112/169], [94mLoss[0m : 2.96784
[1mStep[0m  [128/169], [94mLoss[0m : 2.92309
[1mStep[0m  [144/169], [94mLoss[0m : 2.42110
[1mStep[0m  [160/169], [94mLoss[0m : 2.50726

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.343
====================================

Phase 1 - Evaluation MAE:  2.3433587614979063
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 3.07396
[1mStep[0m  [16/169], [94mLoss[0m : 2.18039
[1mStep[0m  [32/169], [94mLoss[0m : 2.59658
[1mStep[0m  [48/169], [94mLoss[0m : 2.87659
[1mStep[0m  [64/169], [94mLoss[0m : 2.08790
[1mStep[0m  [80/169], [94mLoss[0m : 2.52473
[1mStep[0m  [96/169], [94mLoss[0m : 2.23726
[1mStep[0m  [112/169], [94mLoss[0m : 2.62731
[1mStep[0m  [128/169], [94mLoss[0m : 2.75173
[1mStep[0m  [144/169], [94mLoss[0m : 2.57939
[1mStep[0m  [160/169], [94mLoss[0m : 2.71080

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65406
[1mStep[0m  [16/169], [94mLoss[0m : 2.54546
[1mStep[0m  [32/169], [94mLoss[0m : 2.28856
[1mStep[0m  [48/169], [94mLoss[0m : 1.89816
[1mStep[0m  [64/169], [94mLoss[0m : 2.04433
[1mStep[0m  [80/169], [94mLoss[0m : 2.52133
[1mStep[0m  [96/169], [94mLoss[0m : 1.76721
[1mStep[0m  [112/169], [94mLoss[0m : 2.60088
[1mStep[0m  [128/169], [94mLoss[0m : 2.65185
[1mStep[0m  [144/169], [94mLoss[0m : 2.09801
[1mStep[0m  [160/169], [94mLoss[0m : 2.15498

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11394
[1mStep[0m  [16/169], [94mLoss[0m : 1.98743
[1mStep[0m  [32/169], [94mLoss[0m : 2.16702
[1mStep[0m  [48/169], [94mLoss[0m : 2.16582
[1mStep[0m  [64/169], [94mLoss[0m : 2.35249
[1mStep[0m  [80/169], [94mLoss[0m : 2.30181
[1mStep[0m  [96/169], [94mLoss[0m : 2.55177
[1mStep[0m  [112/169], [94mLoss[0m : 2.02060
[1mStep[0m  [128/169], [94mLoss[0m : 2.23485
[1mStep[0m  [144/169], [94mLoss[0m : 2.55224
[1mStep[0m  [160/169], [94mLoss[0m : 2.50779

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.301, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33493
[1mStep[0m  [16/169], [94mLoss[0m : 2.47473
[1mStep[0m  [32/169], [94mLoss[0m : 2.56568
[1mStep[0m  [48/169], [94mLoss[0m : 2.30760
[1mStep[0m  [64/169], [94mLoss[0m : 2.06818
[1mStep[0m  [80/169], [94mLoss[0m : 2.32310
[1mStep[0m  [96/169], [94mLoss[0m : 1.95991
[1mStep[0m  [112/169], [94mLoss[0m : 2.33169
[1mStep[0m  [128/169], [94mLoss[0m : 2.03273
[1mStep[0m  [144/169], [94mLoss[0m : 2.57199
[1mStep[0m  [160/169], [94mLoss[0m : 2.14390

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46306
[1mStep[0m  [16/169], [94mLoss[0m : 2.08525
[1mStep[0m  [32/169], [94mLoss[0m : 2.24105
[1mStep[0m  [48/169], [94mLoss[0m : 2.20432
[1mStep[0m  [64/169], [94mLoss[0m : 1.91776
[1mStep[0m  [80/169], [94mLoss[0m : 2.07234
[1mStep[0m  [96/169], [94mLoss[0m : 2.70270
[1mStep[0m  [112/169], [94mLoss[0m : 2.02270
[1mStep[0m  [128/169], [94mLoss[0m : 1.94486
[1mStep[0m  [144/169], [94mLoss[0m : 2.22758
[1mStep[0m  [160/169], [94mLoss[0m : 2.10571

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01397
[1mStep[0m  [16/169], [94mLoss[0m : 1.91583
[1mStep[0m  [32/169], [94mLoss[0m : 1.94815
[1mStep[0m  [48/169], [94mLoss[0m : 2.05503
[1mStep[0m  [64/169], [94mLoss[0m : 2.21102
[1mStep[0m  [80/169], [94mLoss[0m : 2.22793
[1mStep[0m  [96/169], [94mLoss[0m : 2.18665
[1mStep[0m  [112/169], [94mLoss[0m : 2.24834
[1mStep[0m  [128/169], [94mLoss[0m : 2.13768
[1mStep[0m  [144/169], [94mLoss[0m : 2.18849
[1mStep[0m  [160/169], [94mLoss[0m : 2.57703

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.134, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15877
[1mStep[0m  [16/169], [94mLoss[0m : 2.19305
[1mStep[0m  [32/169], [94mLoss[0m : 2.44406
[1mStep[0m  [48/169], [94mLoss[0m : 2.08022
[1mStep[0m  [64/169], [94mLoss[0m : 1.93086
[1mStep[0m  [80/169], [94mLoss[0m : 1.80517
[1mStep[0m  [96/169], [94mLoss[0m : 2.22991
[1mStep[0m  [112/169], [94mLoss[0m : 2.08164
[1mStep[0m  [128/169], [94mLoss[0m : 2.08863
[1mStep[0m  [144/169], [94mLoss[0m : 1.90719
[1mStep[0m  [160/169], [94mLoss[0m : 1.83812

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79130
[1mStep[0m  [16/169], [94mLoss[0m : 2.04936
[1mStep[0m  [32/169], [94mLoss[0m : 1.98544
[1mStep[0m  [48/169], [94mLoss[0m : 1.95352
[1mStep[0m  [64/169], [94mLoss[0m : 1.84393
[1mStep[0m  [80/169], [94mLoss[0m : 1.80721
[1mStep[0m  [96/169], [94mLoss[0m : 2.16047
[1mStep[0m  [112/169], [94mLoss[0m : 1.99954
[1mStep[0m  [128/169], [94mLoss[0m : 1.86436
[1mStep[0m  [144/169], [94mLoss[0m : 2.18021
[1mStep[0m  [160/169], [94mLoss[0m : 2.01375

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57931
[1mStep[0m  [16/169], [94mLoss[0m : 2.18006
[1mStep[0m  [32/169], [94mLoss[0m : 1.78233
[1mStep[0m  [48/169], [94mLoss[0m : 2.51031
[1mStep[0m  [64/169], [94mLoss[0m : 1.96520
[1mStep[0m  [80/169], [94mLoss[0m : 2.20681
[1mStep[0m  [96/169], [94mLoss[0m : 1.95949
[1mStep[0m  [112/169], [94mLoss[0m : 2.53139
[1mStep[0m  [128/169], [94mLoss[0m : 2.10196
[1mStep[0m  [144/169], [94mLoss[0m : 2.12859
[1mStep[0m  [160/169], [94mLoss[0m : 2.11948

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84004
[1mStep[0m  [16/169], [94mLoss[0m : 1.71765
[1mStep[0m  [32/169], [94mLoss[0m : 1.35361
[1mStep[0m  [48/169], [94mLoss[0m : 1.79289
[1mStep[0m  [64/169], [94mLoss[0m : 1.83800
[1mStep[0m  [80/169], [94mLoss[0m : 1.60450
[1mStep[0m  [96/169], [94mLoss[0m : 2.19602
[1mStep[0m  [112/169], [94mLoss[0m : 1.88301
[1mStep[0m  [128/169], [94mLoss[0m : 1.82436
[1mStep[0m  [144/169], [94mLoss[0m : 1.88469
[1mStep[0m  [160/169], [94mLoss[0m : 2.15692

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75073
[1mStep[0m  [16/169], [94mLoss[0m : 1.95309
[1mStep[0m  [32/169], [94mLoss[0m : 1.85349
[1mStep[0m  [48/169], [94mLoss[0m : 1.96973
[1mStep[0m  [64/169], [94mLoss[0m : 1.80055
[1mStep[0m  [80/169], [94mLoss[0m : 1.75706
[1mStep[0m  [96/169], [94mLoss[0m : 1.81754
[1mStep[0m  [112/169], [94mLoss[0m : 1.94770
[1mStep[0m  [128/169], [94mLoss[0m : 1.91284
[1mStep[0m  [144/169], [94mLoss[0m : 1.93671
[1mStep[0m  [160/169], [94mLoss[0m : 1.79039

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.871, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73941
[1mStep[0m  [16/169], [94mLoss[0m : 1.93936
[1mStep[0m  [32/169], [94mLoss[0m : 2.12497
[1mStep[0m  [48/169], [94mLoss[0m : 2.01440
[1mStep[0m  [64/169], [94mLoss[0m : 2.05385
[1mStep[0m  [80/169], [94mLoss[0m : 1.81235
[1mStep[0m  [96/169], [94mLoss[0m : 1.46499
[1mStep[0m  [112/169], [94mLoss[0m : 1.99735
[1mStep[0m  [128/169], [94mLoss[0m : 1.91939
[1mStep[0m  [144/169], [94mLoss[0m : 1.59176
[1mStep[0m  [160/169], [94mLoss[0m : 1.93249

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60928
[1mStep[0m  [16/169], [94mLoss[0m : 1.58686
[1mStep[0m  [32/169], [94mLoss[0m : 1.34763
[1mStep[0m  [48/169], [94mLoss[0m : 1.79480
[1mStep[0m  [64/169], [94mLoss[0m : 2.05618
[1mStep[0m  [80/169], [94mLoss[0m : 1.77930
[1mStep[0m  [96/169], [94mLoss[0m : 1.88227
[1mStep[0m  [112/169], [94mLoss[0m : 1.69481
[1mStep[0m  [128/169], [94mLoss[0m : 1.86719
[1mStep[0m  [144/169], [94mLoss[0m : 1.91854
[1mStep[0m  [160/169], [94mLoss[0m : 2.07739

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74038
[1mStep[0m  [16/169], [94mLoss[0m : 2.12084
[1mStep[0m  [32/169], [94mLoss[0m : 1.55290
[1mStep[0m  [48/169], [94mLoss[0m : 1.90991
[1mStep[0m  [64/169], [94mLoss[0m : 1.91909
[1mStep[0m  [80/169], [94mLoss[0m : 1.52993
[1mStep[0m  [96/169], [94mLoss[0m : 1.58106
[1mStep[0m  [112/169], [94mLoss[0m : 1.75849
[1mStep[0m  [128/169], [94mLoss[0m : 2.10653
[1mStep[0m  [144/169], [94mLoss[0m : 1.78061
[1mStep[0m  [160/169], [94mLoss[0m : 1.92289

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.485, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01406
[1mStep[0m  [16/169], [94mLoss[0m : 1.75897
[1mStep[0m  [32/169], [94mLoss[0m : 1.69915
[1mStep[0m  [48/169], [94mLoss[0m : 1.62289
[1mStep[0m  [64/169], [94mLoss[0m : 1.38777
[1mStep[0m  [80/169], [94mLoss[0m : 1.53936
[1mStep[0m  [96/169], [94mLoss[0m : 1.74064
[1mStep[0m  [112/169], [94mLoss[0m : 1.80880
[1mStep[0m  [128/169], [94mLoss[0m : 1.81749
[1mStep[0m  [144/169], [94mLoss[0m : 1.74620
[1mStep[0m  [160/169], [94mLoss[0m : 1.39553

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56658
[1mStep[0m  [16/169], [94mLoss[0m : 1.85699
[1mStep[0m  [32/169], [94mLoss[0m : 1.84324
[1mStep[0m  [48/169], [94mLoss[0m : 1.44598
[1mStep[0m  [64/169], [94mLoss[0m : 1.88598
[1mStep[0m  [80/169], [94mLoss[0m : 1.57778
[1mStep[0m  [96/169], [94mLoss[0m : 1.68303
[1mStep[0m  [112/169], [94mLoss[0m : 1.48829
[1mStep[0m  [128/169], [94mLoss[0m : 1.46363
[1mStep[0m  [144/169], [94mLoss[0m : 1.64585
[1mStep[0m  [160/169], [94mLoss[0m : 1.60296

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.676, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62971
[1mStep[0m  [16/169], [94mLoss[0m : 1.24140
[1mStep[0m  [32/169], [94mLoss[0m : 1.36171
[1mStep[0m  [48/169], [94mLoss[0m : 1.28729
[1mStep[0m  [64/169], [94mLoss[0m : 1.59040
[1mStep[0m  [80/169], [94mLoss[0m : 1.67544
[1mStep[0m  [96/169], [94mLoss[0m : 1.71143
[1mStep[0m  [112/169], [94mLoss[0m : 1.48979
[1mStep[0m  [128/169], [94mLoss[0m : 1.52802
[1mStep[0m  [144/169], [94mLoss[0m : 1.52402
[1mStep[0m  [160/169], [94mLoss[0m : 1.91677

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.566, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77454
[1mStep[0m  [16/169], [94mLoss[0m : 1.59288
[1mStep[0m  [32/169], [94mLoss[0m : 1.54616
[1mStep[0m  [48/169], [94mLoss[0m : 1.62766
[1mStep[0m  [64/169], [94mLoss[0m : 1.44933
[1mStep[0m  [80/169], [94mLoss[0m : 1.54390
[1mStep[0m  [96/169], [94mLoss[0m : 1.55466
[1mStep[0m  [112/169], [94mLoss[0m : 1.79597
[1mStep[0m  [128/169], [94mLoss[0m : 1.61086
[1mStep[0m  [144/169], [94mLoss[0m : 1.49703
[1mStep[0m  [160/169], [94mLoss[0m : 1.35698

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.505, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48856
[1mStep[0m  [16/169], [94mLoss[0m : 1.52835
[1mStep[0m  [32/169], [94mLoss[0m : 1.70447
[1mStep[0m  [48/169], [94mLoss[0m : 1.30625
[1mStep[0m  [64/169], [94mLoss[0m : 1.65493
[1mStep[0m  [80/169], [94mLoss[0m : 2.01041
[1mStep[0m  [96/169], [94mLoss[0m : 1.70819
[1mStep[0m  [112/169], [94mLoss[0m : 1.52423
[1mStep[0m  [128/169], [94mLoss[0m : 1.48769
[1mStep[0m  [144/169], [94mLoss[0m : 1.45128
[1mStep[0m  [160/169], [94mLoss[0m : 1.49254

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.504, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54582
[1mStep[0m  [16/169], [94mLoss[0m : 1.54269
[1mStep[0m  [32/169], [94mLoss[0m : 1.52922
[1mStep[0m  [48/169], [94mLoss[0m : 1.45632
[1mStep[0m  [64/169], [94mLoss[0m : 1.48069
[1mStep[0m  [80/169], [94mLoss[0m : 1.54731
[1mStep[0m  [96/169], [94mLoss[0m : 1.56660
[1mStep[0m  [112/169], [94mLoss[0m : 1.55793
[1mStep[0m  [128/169], [94mLoss[0m : 1.52039
[1mStep[0m  [144/169], [94mLoss[0m : 1.49320
[1mStep[0m  [160/169], [94mLoss[0m : 1.51149

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50307
[1mStep[0m  [16/169], [94mLoss[0m : 1.46859
[1mStep[0m  [32/169], [94mLoss[0m : 1.48739
[1mStep[0m  [48/169], [94mLoss[0m : 1.55753
[1mStep[0m  [64/169], [94mLoss[0m : 1.47979
[1mStep[0m  [80/169], [94mLoss[0m : 2.03868
[1mStep[0m  [96/169], [94mLoss[0m : 1.78098
[1mStep[0m  [112/169], [94mLoss[0m : 1.56319
[1mStep[0m  [128/169], [94mLoss[0m : 1.48802
[1mStep[0m  [144/169], [94mLoss[0m : 1.40306
[1mStep[0m  [160/169], [94mLoss[0m : 1.32611

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73535
[1mStep[0m  [16/169], [94mLoss[0m : 1.34144
[1mStep[0m  [32/169], [94mLoss[0m : 1.40651
[1mStep[0m  [48/169], [94mLoss[0m : 1.29902
[1mStep[0m  [64/169], [94mLoss[0m : 1.54263
[1mStep[0m  [80/169], [94mLoss[0m : 1.58810
[1mStep[0m  [96/169], [94mLoss[0m : 1.68015
[1mStep[0m  [112/169], [94mLoss[0m : 1.12644
[1mStep[0m  [128/169], [94mLoss[0m : 1.86187
[1mStep[0m  [144/169], [94mLoss[0m : 1.50770
[1mStep[0m  [160/169], [94mLoss[0m : 1.80051

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.509, [92mTest[0m: 2.459, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.37920
[1mStep[0m  [16/169], [94mLoss[0m : 1.21656
[1mStep[0m  [32/169], [94mLoss[0m : 1.55687
[1mStep[0m  [48/169], [94mLoss[0m : 1.34923
[1mStep[0m  [64/169], [94mLoss[0m : 1.46557
[1mStep[0m  [80/169], [94mLoss[0m : 1.58124
[1mStep[0m  [96/169], [94mLoss[0m : 1.22977
[1mStep[0m  [112/169], [94mLoss[0m : 1.27558
[1mStep[0m  [128/169], [94mLoss[0m : 1.55683
[1mStep[0m  [144/169], [94mLoss[0m : 1.38688
[1mStep[0m  [160/169], [94mLoss[0m : 1.56608

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.524, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.30240
[1mStep[0m  [16/169], [94mLoss[0m : 1.21302
[1mStep[0m  [32/169], [94mLoss[0m : 1.50743
[1mStep[0m  [48/169], [94mLoss[0m : 1.09297
[1mStep[0m  [64/169], [94mLoss[0m : 1.46161
[1mStep[0m  [80/169], [94mLoss[0m : 1.34279
[1mStep[0m  [96/169], [94mLoss[0m : 1.59031
[1mStep[0m  [112/169], [94mLoss[0m : 1.23630
[1mStep[0m  [128/169], [94mLoss[0m : 1.55983
[1mStep[0m  [144/169], [94mLoss[0m : 1.51004
[1mStep[0m  [160/169], [94mLoss[0m : 1.52547

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.436, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50246
[1mStep[0m  [16/169], [94mLoss[0m : 1.43822
[1mStep[0m  [32/169], [94mLoss[0m : 1.37411
[1mStep[0m  [48/169], [94mLoss[0m : 1.20836
[1mStep[0m  [64/169], [94mLoss[0m : 1.31341
[1mStep[0m  [80/169], [94mLoss[0m : 1.12835
[1mStep[0m  [96/169], [94mLoss[0m : 1.34490
[1mStep[0m  [112/169], [94mLoss[0m : 1.26905
[1mStep[0m  [128/169], [94mLoss[0m : 1.42427
[1mStep[0m  [144/169], [94mLoss[0m : 1.29448
[1mStep[0m  [160/169], [94mLoss[0m : 1.40260

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.440, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.26531
[1mStep[0m  [16/169], [94mLoss[0m : 1.26945
[1mStep[0m  [32/169], [94mLoss[0m : 1.32304
[1mStep[0m  [48/169], [94mLoss[0m : 1.38634
[1mStep[0m  [64/169], [94mLoss[0m : 1.36625
[1mStep[0m  [80/169], [94mLoss[0m : 1.50417
[1mStep[0m  [96/169], [94mLoss[0m : 1.77857
[1mStep[0m  [112/169], [94mLoss[0m : 1.38945
[1mStep[0m  [128/169], [94mLoss[0m : 0.91707
[1mStep[0m  [144/169], [94mLoss[0m : 1.70868
[1mStep[0m  [160/169], [94mLoss[0m : 1.55579

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.411, [92mTest[0m: 2.438, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.26057
[1mStep[0m  [16/169], [94mLoss[0m : 1.14767
[1mStep[0m  [32/169], [94mLoss[0m : 1.38022
[1mStep[0m  [48/169], [94mLoss[0m : 1.49236
[1mStep[0m  [64/169], [94mLoss[0m : 1.32963
[1mStep[0m  [80/169], [94mLoss[0m : 1.28682
[1mStep[0m  [96/169], [94mLoss[0m : 1.36446
[1mStep[0m  [112/169], [94mLoss[0m : 1.60657
[1mStep[0m  [128/169], [94mLoss[0m : 1.49265
[1mStep[0m  [144/169], [94mLoss[0m : 1.38441
[1mStep[0m  [160/169], [94mLoss[0m : 1.58770

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.389, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.4917681366205215
MAE score P1      2.343359
MAE score P2      2.491768
loss              1.388681
learning_rate         0.01
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 11.77534
[1mStep[0m  [33/339], [94mLoss[0m : 7.70229
[1mStep[0m  [66/339], [94mLoss[0m : 4.30793
[1mStep[0m  [99/339], [94mLoss[0m : 2.62027
[1mStep[0m  [132/339], [94mLoss[0m : 2.36818
[1mStep[0m  [165/339], [94mLoss[0m : 2.09055
[1mStep[0m  [198/339], [94mLoss[0m : 2.62422
[1mStep[0m  [231/339], [94mLoss[0m : 2.46708
[1mStep[0m  [264/339], [94mLoss[0m : 3.31726
[1mStep[0m  [297/339], [94mLoss[0m : 2.87896
[1mStep[0m  [330/339], [94mLoss[0m : 2.65550

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.830, [92mTest[0m: 10.837, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13002
[1mStep[0m  [33/339], [94mLoss[0m : 2.42096
[1mStep[0m  [66/339], [94mLoss[0m : 2.04145
[1mStep[0m  [99/339], [94mLoss[0m : 2.72428
[1mStep[0m  [132/339], [94mLoss[0m : 2.86300
[1mStep[0m  [165/339], [94mLoss[0m : 2.26738
[1mStep[0m  [198/339], [94mLoss[0m : 2.64609
[1mStep[0m  [231/339], [94mLoss[0m : 3.25115
[1mStep[0m  [264/339], [94mLoss[0m : 2.83283
[1mStep[0m  [297/339], [94mLoss[0m : 3.00874
[1mStep[0m  [330/339], [94mLoss[0m : 2.93043

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12858
[1mStep[0m  [33/339], [94mLoss[0m : 2.25721
[1mStep[0m  [66/339], [94mLoss[0m : 2.10849
[1mStep[0m  [99/339], [94mLoss[0m : 2.81647
[1mStep[0m  [132/339], [94mLoss[0m : 2.69739
[1mStep[0m  [165/339], [94mLoss[0m : 2.97255
[1mStep[0m  [198/339], [94mLoss[0m : 2.42188
[1mStep[0m  [231/339], [94mLoss[0m : 2.37767
[1mStep[0m  [264/339], [94mLoss[0m : 2.19965
[1mStep[0m  [297/339], [94mLoss[0m : 2.19734
[1mStep[0m  [330/339], [94mLoss[0m : 2.80589

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.439, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.05658
[1mStep[0m  [33/339], [94mLoss[0m : 2.46905
[1mStep[0m  [66/339], [94mLoss[0m : 2.50250
[1mStep[0m  [99/339], [94mLoss[0m : 2.41018
[1mStep[0m  [132/339], [94mLoss[0m : 2.53557
[1mStep[0m  [165/339], [94mLoss[0m : 2.25688
[1mStep[0m  [198/339], [94mLoss[0m : 2.13587
[1mStep[0m  [231/339], [94mLoss[0m : 2.44643
[1mStep[0m  [264/339], [94mLoss[0m : 2.96970
[1mStep[0m  [297/339], [94mLoss[0m : 3.30510
[1mStep[0m  [330/339], [94mLoss[0m : 2.76940

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.07793
[1mStep[0m  [33/339], [94mLoss[0m : 2.24077
[1mStep[0m  [66/339], [94mLoss[0m : 2.43379
[1mStep[0m  [99/339], [94mLoss[0m : 2.40190
[1mStep[0m  [132/339], [94mLoss[0m : 1.81627
[1mStep[0m  [165/339], [94mLoss[0m : 2.10495
[1mStep[0m  [198/339], [94mLoss[0m : 2.34324
[1mStep[0m  [231/339], [94mLoss[0m : 1.96756
[1mStep[0m  [264/339], [94mLoss[0m : 2.37877
[1mStep[0m  [297/339], [94mLoss[0m : 2.47228
[1mStep[0m  [330/339], [94mLoss[0m : 2.88611

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00669
[1mStep[0m  [33/339], [94mLoss[0m : 3.01139
[1mStep[0m  [66/339], [94mLoss[0m : 3.07145
[1mStep[0m  [99/339], [94mLoss[0m : 3.00287
[1mStep[0m  [132/339], [94mLoss[0m : 2.45697
[1mStep[0m  [165/339], [94mLoss[0m : 2.86547
[1mStep[0m  [198/339], [94mLoss[0m : 3.21071
[1mStep[0m  [231/339], [94mLoss[0m : 2.69814
[1mStep[0m  [264/339], [94mLoss[0m : 2.59833
[1mStep[0m  [297/339], [94mLoss[0m : 2.67250
[1mStep[0m  [330/339], [94mLoss[0m : 2.81730

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.386, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.41981
[1mStep[0m  [33/339], [94mLoss[0m : 2.36175
[1mStep[0m  [66/339], [94mLoss[0m : 2.96723
[1mStep[0m  [99/339], [94mLoss[0m : 2.25663
[1mStep[0m  [132/339], [94mLoss[0m : 2.43444
[1mStep[0m  [165/339], [94mLoss[0m : 2.43939
[1mStep[0m  [198/339], [94mLoss[0m : 2.53924
[1mStep[0m  [231/339], [94mLoss[0m : 3.41593
[1mStep[0m  [264/339], [94mLoss[0m : 2.41796
[1mStep[0m  [297/339], [94mLoss[0m : 2.82288
[1mStep[0m  [330/339], [94mLoss[0m : 2.13888

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49829
[1mStep[0m  [33/339], [94mLoss[0m : 2.72460
[1mStep[0m  [66/339], [94mLoss[0m : 2.24886
[1mStep[0m  [99/339], [94mLoss[0m : 2.77361
[1mStep[0m  [132/339], [94mLoss[0m : 2.08274
[1mStep[0m  [165/339], [94mLoss[0m : 2.65901
[1mStep[0m  [198/339], [94mLoss[0m : 2.69254
[1mStep[0m  [231/339], [94mLoss[0m : 2.10497
[1mStep[0m  [264/339], [94mLoss[0m : 1.93990
[1mStep[0m  [297/339], [94mLoss[0m : 2.84782
[1mStep[0m  [330/339], [94mLoss[0m : 2.46185

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.82011
[1mStep[0m  [33/339], [94mLoss[0m : 2.39763
[1mStep[0m  [66/339], [94mLoss[0m : 2.39515
[1mStep[0m  [99/339], [94mLoss[0m : 2.64622
[1mStep[0m  [132/339], [94mLoss[0m : 2.58113
[1mStep[0m  [165/339], [94mLoss[0m : 2.81244
[1mStep[0m  [198/339], [94mLoss[0m : 2.22126
[1mStep[0m  [231/339], [94mLoss[0m : 2.59071
[1mStep[0m  [264/339], [94mLoss[0m : 2.10432
[1mStep[0m  [297/339], [94mLoss[0m : 1.76654
[1mStep[0m  [330/339], [94mLoss[0m : 2.60664

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41328
[1mStep[0m  [33/339], [94mLoss[0m : 2.40051
[1mStep[0m  [66/339], [94mLoss[0m : 2.18244
[1mStep[0m  [99/339], [94mLoss[0m : 2.72016
[1mStep[0m  [132/339], [94mLoss[0m : 2.79729
[1mStep[0m  [165/339], [94mLoss[0m : 2.54239
[1mStep[0m  [198/339], [94mLoss[0m : 2.61689
[1mStep[0m  [231/339], [94mLoss[0m : 2.30729
[1mStep[0m  [264/339], [94mLoss[0m : 2.40556
[1mStep[0m  [297/339], [94mLoss[0m : 2.61269
[1mStep[0m  [330/339], [94mLoss[0m : 2.47434

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14478
[1mStep[0m  [33/339], [94mLoss[0m : 2.43334
[1mStep[0m  [66/339], [94mLoss[0m : 1.77682
[1mStep[0m  [99/339], [94mLoss[0m : 2.31398
[1mStep[0m  [132/339], [94mLoss[0m : 2.54376
[1mStep[0m  [165/339], [94mLoss[0m : 3.10530
[1mStep[0m  [198/339], [94mLoss[0m : 2.56395
[1mStep[0m  [231/339], [94mLoss[0m : 2.14112
[1mStep[0m  [264/339], [94mLoss[0m : 2.34815
[1mStep[0m  [297/339], [94mLoss[0m : 2.07165
[1mStep[0m  [330/339], [94mLoss[0m : 2.42822

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66475
[1mStep[0m  [33/339], [94mLoss[0m : 3.06177
[1mStep[0m  [66/339], [94mLoss[0m : 2.08115
[1mStep[0m  [99/339], [94mLoss[0m : 2.27917
[1mStep[0m  [132/339], [94mLoss[0m : 2.70630
[1mStep[0m  [165/339], [94mLoss[0m : 2.75203
[1mStep[0m  [198/339], [94mLoss[0m : 2.09306
[1mStep[0m  [231/339], [94mLoss[0m : 1.90435
[1mStep[0m  [264/339], [94mLoss[0m : 2.09099
[1mStep[0m  [297/339], [94mLoss[0m : 2.94895
[1mStep[0m  [330/339], [94mLoss[0m : 1.85919

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.314, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16096
[1mStep[0m  [33/339], [94mLoss[0m : 2.17408
[1mStep[0m  [66/339], [94mLoss[0m : 2.84989
[1mStep[0m  [99/339], [94mLoss[0m : 2.81779
[1mStep[0m  [132/339], [94mLoss[0m : 2.73733
[1mStep[0m  [165/339], [94mLoss[0m : 2.80733
[1mStep[0m  [198/339], [94mLoss[0m : 2.40483
[1mStep[0m  [231/339], [94mLoss[0m : 2.68298
[1mStep[0m  [264/339], [94mLoss[0m : 2.30659
[1mStep[0m  [297/339], [94mLoss[0m : 2.50424
[1mStep[0m  [330/339], [94mLoss[0m : 2.20719

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.08379
[1mStep[0m  [33/339], [94mLoss[0m : 2.00546
[1mStep[0m  [66/339], [94mLoss[0m : 2.39262
[1mStep[0m  [99/339], [94mLoss[0m : 3.01364
[1mStep[0m  [132/339], [94mLoss[0m : 2.23339
[1mStep[0m  [165/339], [94mLoss[0m : 2.09534
[1mStep[0m  [198/339], [94mLoss[0m : 2.82816
[1mStep[0m  [231/339], [94mLoss[0m : 2.12465
[1mStep[0m  [264/339], [94mLoss[0m : 2.70394
[1mStep[0m  [297/339], [94mLoss[0m : 2.32214
[1mStep[0m  [330/339], [94mLoss[0m : 2.50754

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.333, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48576
[1mStep[0m  [33/339], [94mLoss[0m : 2.77538
[1mStep[0m  [66/339], [94mLoss[0m : 2.14361
[1mStep[0m  [99/339], [94mLoss[0m : 1.91936
[1mStep[0m  [132/339], [94mLoss[0m : 2.09511
[1mStep[0m  [165/339], [94mLoss[0m : 2.11919
[1mStep[0m  [198/339], [94mLoss[0m : 2.10041
[1mStep[0m  [231/339], [94mLoss[0m : 2.57519
[1mStep[0m  [264/339], [94mLoss[0m : 2.59577
[1mStep[0m  [297/339], [94mLoss[0m : 2.74869
[1mStep[0m  [330/339], [94mLoss[0m : 2.58096

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21196
[1mStep[0m  [33/339], [94mLoss[0m : 2.43731
[1mStep[0m  [66/339], [94mLoss[0m : 2.92999
[1mStep[0m  [99/339], [94mLoss[0m : 2.99737
[1mStep[0m  [132/339], [94mLoss[0m : 1.84030
[1mStep[0m  [165/339], [94mLoss[0m : 2.15599
[1mStep[0m  [198/339], [94mLoss[0m : 2.10401
[1mStep[0m  [231/339], [94mLoss[0m : 2.63730
[1mStep[0m  [264/339], [94mLoss[0m : 2.85834
[1mStep[0m  [297/339], [94mLoss[0m : 2.92164
[1mStep[0m  [330/339], [94mLoss[0m : 2.28024

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57314
[1mStep[0m  [33/339], [94mLoss[0m : 1.89179
[1mStep[0m  [66/339], [94mLoss[0m : 2.04649
[1mStep[0m  [99/339], [94mLoss[0m : 2.31250
[1mStep[0m  [132/339], [94mLoss[0m : 2.25583
[1mStep[0m  [165/339], [94mLoss[0m : 2.60259
[1mStep[0m  [198/339], [94mLoss[0m : 2.49879
[1mStep[0m  [231/339], [94mLoss[0m : 1.88458
[1mStep[0m  [264/339], [94mLoss[0m : 2.13842
[1mStep[0m  [297/339], [94mLoss[0m : 3.10716
[1mStep[0m  [330/339], [94mLoss[0m : 2.13174

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62713
[1mStep[0m  [33/339], [94mLoss[0m : 3.05181
[1mStep[0m  [66/339], [94mLoss[0m : 2.17733
[1mStep[0m  [99/339], [94mLoss[0m : 2.54436
[1mStep[0m  [132/339], [94mLoss[0m : 2.60352
[1mStep[0m  [165/339], [94mLoss[0m : 2.21645
[1mStep[0m  [198/339], [94mLoss[0m : 1.66559
[1mStep[0m  [231/339], [94mLoss[0m : 1.90577
[1mStep[0m  [264/339], [94mLoss[0m : 3.08994
[1mStep[0m  [297/339], [94mLoss[0m : 2.84603
[1mStep[0m  [330/339], [94mLoss[0m : 2.54262

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63971
[1mStep[0m  [33/339], [94mLoss[0m : 2.38322
[1mStep[0m  [66/339], [94mLoss[0m : 1.99020
[1mStep[0m  [99/339], [94mLoss[0m : 2.31794
[1mStep[0m  [132/339], [94mLoss[0m : 3.49595
[1mStep[0m  [165/339], [94mLoss[0m : 2.44561
[1mStep[0m  [198/339], [94mLoss[0m : 1.89567
[1mStep[0m  [231/339], [94mLoss[0m : 2.09408
[1mStep[0m  [264/339], [94mLoss[0m : 2.17176
[1mStep[0m  [297/339], [94mLoss[0m : 2.27964
[1mStep[0m  [330/339], [94mLoss[0m : 2.02103

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19770
[1mStep[0m  [33/339], [94mLoss[0m : 2.13268
[1mStep[0m  [66/339], [94mLoss[0m : 1.46298
[1mStep[0m  [99/339], [94mLoss[0m : 2.03109
[1mStep[0m  [132/339], [94mLoss[0m : 2.37568
[1mStep[0m  [165/339], [94mLoss[0m : 2.49194
[1mStep[0m  [198/339], [94mLoss[0m : 1.92690
[1mStep[0m  [231/339], [94mLoss[0m : 2.83744
[1mStep[0m  [264/339], [94mLoss[0m : 1.86335
[1mStep[0m  [297/339], [94mLoss[0m : 2.96103
[1mStep[0m  [330/339], [94mLoss[0m : 2.43547

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29645
[1mStep[0m  [33/339], [94mLoss[0m : 2.53938
[1mStep[0m  [66/339], [94mLoss[0m : 2.23636
[1mStep[0m  [99/339], [94mLoss[0m : 2.06357
[1mStep[0m  [132/339], [94mLoss[0m : 1.85785
[1mStep[0m  [165/339], [94mLoss[0m : 2.41550
[1mStep[0m  [198/339], [94mLoss[0m : 2.72105
[1mStep[0m  [231/339], [94mLoss[0m : 2.35306
[1mStep[0m  [264/339], [94mLoss[0m : 2.02083
[1mStep[0m  [297/339], [94mLoss[0m : 2.72078
[1mStep[0m  [330/339], [94mLoss[0m : 2.51831

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.313, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00728
[1mStep[0m  [33/339], [94mLoss[0m : 2.56230
[1mStep[0m  [66/339], [94mLoss[0m : 2.77263
[1mStep[0m  [99/339], [94mLoss[0m : 2.54602
[1mStep[0m  [132/339], [94mLoss[0m : 1.70346
[1mStep[0m  [165/339], [94mLoss[0m : 2.04469
[1mStep[0m  [198/339], [94mLoss[0m : 2.25608
[1mStep[0m  [231/339], [94mLoss[0m : 2.00145
[1mStep[0m  [264/339], [94mLoss[0m : 2.25851
[1mStep[0m  [297/339], [94mLoss[0m : 2.58755
[1mStep[0m  [330/339], [94mLoss[0m : 1.96625

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62149
[1mStep[0m  [33/339], [94mLoss[0m : 2.50109
[1mStep[0m  [66/339], [94mLoss[0m : 2.30792
[1mStep[0m  [99/339], [94mLoss[0m : 1.97661
[1mStep[0m  [132/339], [94mLoss[0m : 2.71463
[1mStep[0m  [165/339], [94mLoss[0m : 2.29606
[1mStep[0m  [198/339], [94mLoss[0m : 2.61104
[1mStep[0m  [231/339], [94mLoss[0m : 1.93770
[1mStep[0m  [264/339], [94mLoss[0m : 2.21591
[1mStep[0m  [297/339], [94mLoss[0m : 1.81658
[1mStep[0m  [330/339], [94mLoss[0m : 2.73208

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34682
[1mStep[0m  [33/339], [94mLoss[0m : 3.01005
[1mStep[0m  [66/339], [94mLoss[0m : 2.34740
[1mStep[0m  [99/339], [94mLoss[0m : 2.07365
[1mStep[0m  [132/339], [94mLoss[0m : 2.48661
[1mStep[0m  [165/339], [94mLoss[0m : 2.73913
[1mStep[0m  [198/339], [94mLoss[0m : 2.60108
[1mStep[0m  [231/339], [94mLoss[0m : 2.01152
[1mStep[0m  [264/339], [94mLoss[0m : 2.21240
[1mStep[0m  [297/339], [94mLoss[0m : 2.26651
[1mStep[0m  [330/339], [94mLoss[0m : 1.69399

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47857
[1mStep[0m  [33/339], [94mLoss[0m : 3.10874
[1mStep[0m  [66/339], [94mLoss[0m : 2.33224
[1mStep[0m  [99/339], [94mLoss[0m : 2.40209
[1mStep[0m  [132/339], [94mLoss[0m : 2.22341
[1mStep[0m  [165/339], [94mLoss[0m : 2.75267
[1mStep[0m  [198/339], [94mLoss[0m : 2.25198
[1mStep[0m  [231/339], [94mLoss[0m : 2.64913
[1mStep[0m  [264/339], [94mLoss[0m : 2.39211
[1mStep[0m  [297/339], [94mLoss[0m : 2.37287
[1mStep[0m  [330/339], [94mLoss[0m : 2.43786

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99760
[1mStep[0m  [33/339], [94mLoss[0m : 2.41014
[1mStep[0m  [66/339], [94mLoss[0m : 2.42557
[1mStep[0m  [99/339], [94mLoss[0m : 2.99963
[1mStep[0m  [132/339], [94mLoss[0m : 1.69314
[1mStep[0m  [165/339], [94mLoss[0m : 2.45644
[1mStep[0m  [198/339], [94mLoss[0m : 2.89940
[1mStep[0m  [231/339], [94mLoss[0m : 2.27794
[1mStep[0m  [264/339], [94mLoss[0m : 2.42045
[1mStep[0m  [297/339], [94mLoss[0m : 2.80692
[1mStep[0m  [330/339], [94mLoss[0m : 2.20016

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.358, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29632
[1mStep[0m  [33/339], [94mLoss[0m : 1.55927
[1mStep[0m  [66/339], [94mLoss[0m : 2.12431
[1mStep[0m  [99/339], [94mLoss[0m : 2.77120
[1mStep[0m  [132/339], [94mLoss[0m : 2.64683
[1mStep[0m  [165/339], [94mLoss[0m : 2.61826
[1mStep[0m  [198/339], [94mLoss[0m : 2.88395
[1mStep[0m  [231/339], [94mLoss[0m : 2.76808
[1mStep[0m  [264/339], [94mLoss[0m : 2.26990
[1mStep[0m  [297/339], [94mLoss[0m : 2.07277
[1mStep[0m  [330/339], [94mLoss[0m : 2.07738

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.19410
[1mStep[0m  [33/339], [94mLoss[0m : 3.41552
[1mStep[0m  [66/339], [94mLoss[0m : 3.24180
[1mStep[0m  [99/339], [94mLoss[0m : 2.38181
[1mStep[0m  [132/339], [94mLoss[0m : 2.21039
[1mStep[0m  [165/339], [94mLoss[0m : 2.37627
[1mStep[0m  [198/339], [94mLoss[0m : 2.57745
[1mStep[0m  [231/339], [94mLoss[0m : 1.96229
[1mStep[0m  [264/339], [94mLoss[0m : 2.65679
[1mStep[0m  [297/339], [94mLoss[0m : 2.82083
[1mStep[0m  [330/339], [94mLoss[0m : 2.31300

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97969
[1mStep[0m  [33/339], [94mLoss[0m : 2.04927
[1mStep[0m  [66/339], [94mLoss[0m : 2.18478
[1mStep[0m  [99/339], [94mLoss[0m : 2.25378
[1mStep[0m  [132/339], [94mLoss[0m : 2.34420
[1mStep[0m  [165/339], [94mLoss[0m : 1.61195
[1mStep[0m  [198/339], [94mLoss[0m : 2.40688
[1mStep[0m  [231/339], [94mLoss[0m : 2.59625
[1mStep[0m  [264/339], [94mLoss[0m : 2.34520
[1mStep[0m  [297/339], [94mLoss[0m : 2.03588
[1mStep[0m  [330/339], [94mLoss[0m : 2.03743

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31121
[1mStep[0m  [33/339], [94mLoss[0m : 1.60641
[1mStep[0m  [66/339], [94mLoss[0m : 2.21257
[1mStep[0m  [99/339], [94mLoss[0m : 1.90131
[1mStep[0m  [132/339], [94mLoss[0m : 1.85825
[1mStep[0m  [165/339], [94mLoss[0m : 2.67859
[1mStep[0m  [198/339], [94mLoss[0m : 2.32324
[1mStep[0m  [231/339], [94mLoss[0m : 1.97964
[1mStep[0m  [264/339], [94mLoss[0m : 2.11126
[1mStep[0m  [297/339], [94mLoss[0m : 2.26489
[1mStep[0m  [330/339], [94mLoss[0m : 2.78857

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.349
====================================

Phase 1 - Evaluation MAE:  2.348716493201467
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 1.83628
[1mStep[0m  [33/339], [94mLoss[0m : 2.29554
[1mStep[0m  [66/339], [94mLoss[0m : 2.41436
[1mStep[0m  [99/339], [94mLoss[0m : 2.70478
[1mStep[0m  [132/339], [94mLoss[0m : 2.08055
[1mStep[0m  [165/339], [94mLoss[0m : 2.42104
[1mStep[0m  [198/339], [94mLoss[0m : 2.05653
[1mStep[0m  [231/339], [94mLoss[0m : 2.40118
[1mStep[0m  [264/339], [94mLoss[0m : 2.61559
[1mStep[0m  [297/339], [94mLoss[0m : 2.99403
[1mStep[0m  [330/339], [94mLoss[0m : 2.18844

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66659
[1mStep[0m  [33/339], [94mLoss[0m : 2.42569
[1mStep[0m  [66/339], [94mLoss[0m : 1.97596
[1mStep[0m  [99/339], [94mLoss[0m : 2.63428
[1mStep[0m  [132/339], [94mLoss[0m : 2.00558
[1mStep[0m  [165/339], [94mLoss[0m : 2.38915
[1mStep[0m  [198/339], [94mLoss[0m : 2.46901
[1mStep[0m  [231/339], [94mLoss[0m : 1.94111
[1mStep[0m  [264/339], [94mLoss[0m : 2.29941
[1mStep[0m  [297/339], [94mLoss[0m : 2.71273
[1mStep[0m  [330/339], [94mLoss[0m : 2.72442

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.05390
[1mStep[0m  [33/339], [94mLoss[0m : 1.60923
[1mStep[0m  [66/339], [94mLoss[0m : 2.59598
[1mStep[0m  [99/339], [94mLoss[0m : 3.00720
[1mStep[0m  [132/339], [94mLoss[0m : 2.07445
[1mStep[0m  [165/339], [94mLoss[0m : 2.79320
[1mStep[0m  [198/339], [94mLoss[0m : 2.21669
[1mStep[0m  [231/339], [94mLoss[0m : 2.56901
[1mStep[0m  [264/339], [94mLoss[0m : 2.37936
[1mStep[0m  [297/339], [94mLoss[0m : 1.85873
[1mStep[0m  [330/339], [94mLoss[0m : 2.75444

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69637
[1mStep[0m  [33/339], [94mLoss[0m : 2.85451
[1mStep[0m  [66/339], [94mLoss[0m : 2.38050
[1mStep[0m  [99/339], [94mLoss[0m : 2.50498
[1mStep[0m  [132/339], [94mLoss[0m : 2.32622
[1mStep[0m  [165/339], [94mLoss[0m : 2.36747
[1mStep[0m  [198/339], [94mLoss[0m : 2.02839
[1mStep[0m  [231/339], [94mLoss[0m : 2.07000
[1mStep[0m  [264/339], [94mLoss[0m : 2.52008
[1mStep[0m  [297/339], [94mLoss[0m : 1.71568
[1mStep[0m  [330/339], [94mLoss[0m : 1.80260

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39967
[1mStep[0m  [33/339], [94mLoss[0m : 2.66479
[1mStep[0m  [66/339], [94mLoss[0m : 1.60798
[1mStep[0m  [99/339], [94mLoss[0m : 2.46012
[1mStep[0m  [132/339], [94mLoss[0m : 2.15473
[1mStep[0m  [165/339], [94mLoss[0m : 2.23866
[1mStep[0m  [198/339], [94mLoss[0m : 2.05472
[1mStep[0m  [231/339], [94mLoss[0m : 2.15423
[1mStep[0m  [264/339], [94mLoss[0m : 2.33450
[1mStep[0m  [297/339], [94mLoss[0m : 2.17210
[1mStep[0m  [330/339], [94mLoss[0m : 2.43229

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.200, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34516
[1mStep[0m  [33/339], [94mLoss[0m : 1.67222
[1mStep[0m  [66/339], [94mLoss[0m : 1.95574
[1mStep[0m  [99/339], [94mLoss[0m : 1.75362
[1mStep[0m  [132/339], [94mLoss[0m : 2.54338
[1mStep[0m  [165/339], [94mLoss[0m : 2.50826
[1mStep[0m  [198/339], [94mLoss[0m : 1.75655
[1mStep[0m  [231/339], [94mLoss[0m : 1.81206
[1mStep[0m  [264/339], [94mLoss[0m : 2.24364
[1mStep[0m  [297/339], [94mLoss[0m : 2.53893
[1mStep[0m  [330/339], [94mLoss[0m : 1.85339

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49074
[1mStep[0m  [33/339], [94mLoss[0m : 1.85399
[1mStep[0m  [66/339], [94mLoss[0m : 2.15925
[1mStep[0m  [99/339], [94mLoss[0m : 2.06174
[1mStep[0m  [132/339], [94mLoss[0m : 2.03298
[1mStep[0m  [165/339], [94mLoss[0m : 1.96637
[1mStep[0m  [198/339], [94mLoss[0m : 1.91374
[1mStep[0m  [231/339], [94mLoss[0m : 2.12739
[1mStep[0m  [264/339], [94mLoss[0m : 1.77473
[1mStep[0m  [297/339], [94mLoss[0m : 2.24745
[1mStep[0m  [330/339], [94mLoss[0m : 2.15208

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78373
[1mStep[0m  [33/339], [94mLoss[0m : 2.15984
[1mStep[0m  [66/339], [94mLoss[0m : 1.95412
[1mStep[0m  [99/339], [94mLoss[0m : 1.87354
[1mStep[0m  [132/339], [94mLoss[0m : 2.63993
[1mStep[0m  [165/339], [94mLoss[0m : 1.55989
[1mStep[0m  [198/339], [94mLoss[0m : 1.72841
[1mStep[0m  [231/339], [94mLoss[0m : 1.90093
[1mStep[0m  [264/339], [94mLoss[0m : 2.50258
[1mStep[0m  [297/339], [94mLoss[0m : 2.03448
[1mStep[0m  [330/339], [94mLoss[0m : 2.66693

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61384
[1mStep[0m  [33/339], [94mLoss[0m : 2.17016
[1mStep[0m  [66/339], [94mLoss[0m : 1.58453
[1mStep[0m  [99/339], [94mLoss[0m : 2.90948
[1mStep[0m  [132/339], [94mLoss[0m : 2.07840
[1mStep[0m  [165/339], [94mLoss[0m : 2.11016
[1mStep[0m  [198/339], [94mLoss[0m : 2.24804
[1mStep[0m  [231/339], [94mLoss[0m : 1.56943
[1mStep[0m  [264/339], [94mLoss[0m : 2.04911
[1mStep[0m  [297/339], [94mLoss[0m : 1.89563
[1mStep[0m  [330/339], [94mLoss[0m : 1.60311

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.051, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51495
[1mStep[0m  [33/339], [94mLoss[0m : 1.23001
[1mStep[0m  [66/339], [94mLoss[0m : 1.89800
[1mStep[0m  [99/339], [94mLoss[0m : 1.76047
[1mStep[0m  [132/339], [94mLoss[0m : 2.42251
[1mStep[0m  [165/339], [94mLoss[0m : 2.62711
[1mStep[0m  [198/339], [94mLoss[0m : 2.62254
[1mStep[0m  [231/339], [94mLoss[0m : 2.29119
[1mStep[0m  [264/339], [94mLoss[0m : 2.25014
[1mStep[0m  [297/339], [94mLoss[0m : 1.62857
[1mStep[0m  [330/339], [94mLoss[0m : 1.56506

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03139
[1mStep[0m  [33/339], [94mLoss[0m : 2.30264
[1mStep[0m  [66/339], [94mLoss[0m : 1.77665
[1mStep[0m  [99/339], [94mLoss[0m : 2.39531
[1mStep[0m  [132/339], [94mLoss[0m : 2.22599
[1mStep[0m  [165/339], [94mLoss[0m : 2.10617
[1mStep[0m  [198/339], [94mLoss[0m : 2.19759
[1mStep[0m  [231/339], [94mLoss[0m : 1.70174
[1mStep[0m  [264/339], [94mLoss[0m : 1.87675
[1mStep[0m  [297/339], [94mLoss[0m : 2.03664
[1mStep[0m  [330/339], [94mLoss[0m : 2.20329

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60741
[1mStep[0m  [33/339], [94mLoss[0m : 1.56214
[1mStep[0m  [66/339], [94mLoss[0m : 2.31924
[1mStep[0m  [99/339], [94mLoss[0m : 1.64786
[1mStep[0m  [132/339], [94mLoss[0m : 1.38929
[1mStep[0m  [165/339], [94mLoss[0m : 1.40211
[1mStep[0m  [198/339], [94mLoss[0m : 1.81314
[1mStep[0m  [231/339], [94mLoss[0m : 1.47372
[1mStep[0m  [264/339], [94mLoss[0m : 2.22117
[1mStep[0m  [297/339], [94mLoss[0m : 1.74845
[1mStep[0m  [330/339], [94mLoss[0m : 2.21184

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.514, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.89859
[1mStep[0m  [33/339], [94mLoss[0m : 1.68326
[1mStep[0m  [66/339], [94mLoss[0m : 1.85884
[1mStep[0m  [99/339], [94mLoss[0m : 1.85493
[1mStep[0m  [132/339], [94mLoss[0m : 2.12377
[1mStep[0m  [165/339], [94mLoss[0m : 1.70580
[1mStep[0m  [198/339], [94mLoss[0m : 2.11212
[1mStep[0m  [231/339], [94mLoss[0m : 1.68887
[1mStep[0m  [264/339], [94mLoss[0m : 1.75851
[1mStep[0m  [297/339], [94mLoss[0m : 1.98314
[1mStep[0m  [330/339], [94mLoss[0m : 1.86589

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.503, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30458
[1mStep[0m  [33/339], [94mLoss[0m : 2.10146
[1mStep[0m  [66/339], [94mLoss[0m : 2.05926
[1mStep[0m  [99/339], [94mLoss[0m : 2.31426
[1mStep[0m  [132/339], [94mLoss[0m : 1.80293
[1mStep[0m  [165/339], [94mLoss[0m : 1.67065
[1mStep[0m  [198/339], [94mLoss[0m : 1.61038
[1mStep[0m  [231/339], [94mLoss[0m : 2.32438
[1mStep[0m  [264/339], [94mLoss[0m : 2.32122
[1mStep[0m  [297/339], [94mLoss[0m : 1.98403
[1mStep[0m  [330/339], [94mLoss[0m : 1.58766

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.60996
[1mStep[0m  [33/339], [94mLoss[0m : 1.65696
[1mStep[0m  [66/339], [94mLoss[0m : 1.97423
[1mStep[0m  [99/339], [94mLoss[0m : 1.57918
[1mStep[0m  [132/339], [94mLoss[0m : 1.49495
[1mStep[0m  [165/339], [94mLoss[0m : 1.71686
[1mStep[0m  [198/339], [94mLoss[0m : 2.15627
[1mStep[0m  [231/339], [94mLoss[0m : 2.28696
[1mStep[0m  [264/339], [94mLoss[0m : 1.69572
[1mStep[0m  [297/339], [94mLoss[0m : 1.81698
[1mStep[0m  [330/339], [94mLoss[0m : 1.27260

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.581, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82873
[1mStep[0m  [33/339], [94mLoss[0m : 1.69156
[1mStep[0m  [66/339], [94mLoss[0m : 1.66163
[1mStep[0m  [99/339], [94mLoss[0m : 1.73294
[1mStep[0m  [132/339], [94mLoss[0m : 2.30242
[1mStep[0m  [165/339], [94mLoss[0m : 2.15364
[1mStep[0m  [198/339], [94mLoss[0m : 1.48798
[1mStep[0m  [231/339], [94mLoss[0m : 1.38123
[1mStep[0m  [264/339], [94mLoss[0m : 1.83185
[1mStep[0m  [297/339], [94mLoss[0m : 1.94786
[1mStep[0m  [330/339], [94mLoss[0m : 1.80910

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39773
[1mStep[0m  [33/339], [94mLoss[0m : 1.60943
[1mStep[0m  [66/339], [94mLoss[0m : 1.87471
[1mStep[0m  [99/339], [94mLoss[0m : 1.89500
[1mStep[0m  [132/339], [94mLoss[0m : 1.76334
[1mStep[0m  [165/339], [94mLoss[0m : 1.99972
[1mStep[0m  [198/339], [94mLoss[0m : 2.09630
[1mStep[0m  [231/339], [94mLoss[0m : 1.68243
[1mStep[0m  [264/339], [94mLoss[0m : 1.91487
[1mStep[0m  [297/339], [94mLoss[0m : 1.68689
[1mStep[0m  [330/339], [94mLoss[0m : 1.55020

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.515, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31613
[1mStep[0m  [33/339], [94mLoss[0m : 2.02181
[1mStep[0m  [66/339], [94mLoss[0m : 1.55322
[1mStep[0m  [99/339], [94mLoss[0m : 1.74659
[1mStep[0m  [132/339], [94mLoss[0m : 1.96903
[1mStep[0m  [165/339], [94mLoss[0m : 1.21596
[1mStep[0m  [198/339], [94mLoss[0m : 1.24871
[1mStep[0m  [231/339], [94mLoss[0m : 2.26211
[1mStep[0m  [264/339], [94mLoss[0m : 1.94782
[1mStep[0m  [297/339], [94mLoss[0m : 2.28668
[1mStep[0m  [330/339], [94mLoss[0m : 1.60239

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69960
[1mStep[0m  [33/339], [94mLoss[0m : 1.57239
[1mStep[0m  [66/339], [94mLoss[0m : 1.30283
[1mStep[0m  [99/339], [94mLoss[0m : 1.48810
[1mStep[0m  [132/339], [94mLoss[0m : 1.46969
[1mStep[0m  [165/339], [94mLoss[0m : 1.44859
[1mStep[0m  [198/339], [94mLoss[0m : 1.38436
[1mStep[0m  [231/339], [94mLoss[0m : 1.33675
[1mStep[0m  [264/339], [94mLoss[0m : 1.87071
[1mStep[0m  [297/339], [94mLoss[0m : 1.85384
[1mStep[0m  [330/339], [94mLoss[0m : 1.43419

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.537, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77319
[1mStep[0m  [33/339], [94mLoss[0m : 2.06985
[1mStep[0m  [66/339], [94mLoss[0m : 1.53891
[1mStep[0m  [99/339], [94mLoss[0m : 1.48752
[1mStep[0m  [132/339], [94mLoss[0m : 2.02609
[1mStep[0m  [165/339], [94mLoss[0m : 1.79284
[1mStep[0m  [198/339], [94mLoss[0m : 1.78521
[1mStep[0m  [231/339], [94mLoss[0m : 1.98544
[1mStep[0m  [264/339], [94mLoss[0m : 1.46651
[1mStep[0m  [297/339], [94mLoss[0m : 1.66020
[1mStep[0m  [330/339], [94mLoss[0m : 1.49447

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.527, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.30836
[1mStep[0m  [33/339], [94mLoss[0m : 1.33667
[1mStep[0m  [66/339], [94mLoss[0m : 1.70570
[1mStep[0m  [99/339], [94mLoss[0m : 1.77824
[1mStep[0m  [132/339], [94mLoss[0m : 1.92680
[1mStep[0m  [165/339], [94mLoss[0m : 1.63376
[1mStep[0m  [198/339], [94mLoss[0m : 1.30855
[1mStep[0m  [231/339], [94mLoss[0m : 1.74868
[1mStep[0m  [264/339], [94mLoss[0m : 1.70776
[1mStep[0m  [297/339], [94mLoss[0m : 1.42992
[1mStep[0m  [330/339], [94mLoss[0m : 1.74081

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.560, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.49141
[1mStep[0m  [33/339], [94mLoss[0m : 1.63091
[1mStep[0m  [66/339], [94mLoss[0m : 2.01678
[1mStep[0m  [99/339], [94mLoss[0m : 1.72002
[1mStep[0m  [132/339], [94mLoss[0m : 1.30850
[1mStep[0m  [165/339], [94mLoss[0m : 1.99735
[1mStep[0m  [198/339], [94mLoss[0m : 1.40885
[1mStep[0m  [231/339], [94mLoss[0m : 1.61830
[1mStep[0m  [264/339], [94mLoss[0m : 1.84352
[1mStep[0m  [297/339], [94mLoss[0m : 1.67119
[1mStep[0m  [330/339], [94mLoss[0m : 2.03485

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.432, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57929
[1mStep[0m  [33/339], [94mLoss[0m : 1.29581
[1mStep[0m  [66/339], [94mLoss[0m : 1.17466
[1mStep[0m  [99/339], [94mLoss[0m : 1.82362
[1mStep[0m  [132/339], [94mLoss[0m : 1.22513
[1mStep[0m  [165/339], [94mLoss[0m : 1.54110
[1mStep[0m  [198/339], [94mLoss[0m : 1.20943
[1mStep[0m  [231/339], [94mLoss[0m : 1.62125
[1mStep[0m  [264/339], [94mLoss[0m : 1.70760
[1mStep[0m  [297/339], [94mLoss[0m : 1.99722
[1mStep[0m  [330/339], [94mLoss[0m : 1.48053

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.606, [92mTest[0m: 2.546, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78768
[1mStep[0m  [33/339], [94mLoss[0m : 1.68838
[1mStep[0m  [66/339], [94mLoss[0m : 1.71165
[1mStep[0m  [99/339], [94mLoss[0m : 2.48611
[1mStep[0m  [132/339], [94mLoss[0m : 1.84292
[1mStep[0m  [165/339], [94mLoss[0m : 1.71548
[1mStep[0m  [198/339], [94mLoss[0m : 1.42715
[1mStep[0m  [231/339], [94mLoss[0m : 1.41370
[1mStep[0m  [264/339], [94mLoss[0m : 1.81043
[1mStep[0m  [297/339], [94mLoss[0m : 1.89053
[1mStep[0m  [330/339], [94mLoss[0m : 1.88730

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.536, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.24222
[1mStep[0m  [33/339], [94mLoss[0m : 1.28813
[1mStep[0m  [66/339], [94mLoss[0m : 1.46737
[1mStep[0m  [99/339], [94mLoss[0m : 1.32288
[1mStep[0m  [132/339], [94mLoss[0m : 2.17060
[1mStep[0m  [165/339], [94mLoss[0m : 1.73776
[1mStep[0m  [198/339], [94mLoss[0m : 1.27311
[1mStep[0m  [231/339], [94mLoss[0m : 1.56392
[1mStep[0m  [264/339], [94mLoss[0m : 1.74947
[1mStep[0m  [297/339], [94mLoss[0m : 1.45972
[1mStep[0m  [330/339], [94mLoss[0m : 1.40920

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.536, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.30168
[1mStep[0m  [33/339], [94mLoss[0m : 1.86022
[1mStep[0m  [66/339], [94mLoss[0m : 1.58763
[1mStep[0m  [99/339], [94mLoss[0m : 1.71764
[1mStep[0m  [132/339], [94mLoss[0m : 1.33871
[1mStep[0m  [165/339], [94mLoss[0m : 1.75414
[1mStep[0m  [198/339], [94mLoss[0m : 1.64138
[1mStep[0m  [231/339], [94mLoss[0m : 1.49456
[1mStep[0m  [264/339], [94mLoss[0m : 1.91580
[1mStep[0m  [297/339], [94mLoss[0m : 1.87284
[1mStep[0m  [330/339], [94mLoss[0m : 1.19821

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.552, [92mTest[0m: 2.520, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61515
[1mStep[0m  [33/339], [94mLoss[0m : 1.81344
[1mStep[0m  [66/339], [94mLoss[0m : 1.15730
[1mStep[0m  [99/339], [94mLoss[0m : 1.45620
[1mStep[0m  [132/339], [94mLoss[0m : 1.51539
[1mStep[0m  [165/339], [94mLoss[0m : 1.72342
[1mStep[0m  [198/339], [94mLoss[0m : 1.32813
[1mStep[0m  [231/339], [94mLoss[0m : 1.86637
[1mStep[0m  [264/339], [94mLoss[0m : 1.61142
[1mStep[0m  [297/339], [94mLoss[0m : 1.59024
[1mStep[0m  [330/339], [94mLoss[0m : 1.78017

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.549, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41523
[1mStep[0m  [33/339], [94mLoss[0m : 1.68710
[1mStep[0m  [66/339], [94mLoss[0m : 1.25389
[1mStep[0m  [99/339], [94mLoss[0m : 1.60500
[1mStep[0m  [132/339], [94mLoss[0m : 1.19012
[1mStep[0m  [165/339], [94mLoss[0m : 1.18610
[1mStep[0m  [198/339], [94mLoss[0m : 1.26986
[1mStep[0m  [231/339], [94mLoss[0m : 1.68771
[1mStep[0m  [264/339], [94mLoss[0m : 1.88536
[1mStep[0m  [297/339], [94mLoss[0m : 1.34717
[1mStep[0m  [330/339], [94mLoss[0m : 1.63035

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.513, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56286
[1mStep[0m  [33/339], [94mLoss[0m : 1.47413
[1mStep[0m  [66/339], [94mLoss[0m : 1.25791
[1mStep[0m  [99/339], [94mLoss[0m : 2.04793
[1mStep[0m  [132/339], [94mLoss[0m : 1.45329
[1mStep[0m  [165/339], [94mLoss[0m : 1.82814
[1mStep[0m  [198/339], [94mLoss[0m : 1.72400
[1mStep[0m  [231/339], [94mLoss[0m : 1.57223
[1mStep[0m  [264/339], [94mLoss[0m : 1.58692
[1mStep[0m  [297/339], [94mLoss[0m : 1.54188
[1mStep[0m  [330/339], [94mLoss[0m : 1.38585

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.505, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75903
[1mStep[0m  [33/339], [94mLoss[0m : 1.88501
[1mStep[0m  [66/339], [94mLoss[0m : 1.21616
[1mStep[0m  [99/339], [94mLoss[0m : 1.66895
[1mStep[0m  [132/339], [94mLoss[0m : 1.76622
[1mStep[0m  [165/339], [94mLoss[0m : 1.46750
[1mStep[0m  [198/339], [94mLoss[0m : 1.31723
[1mStep[0m  [231/339], [94mLoss[0m : 1.32172
[1mStep[0m  [264/339], [94mLoss[0m : 1.98297
[1mStep[0m  [297/339], [94mLoss[0m : 1.16574
[1mStep[0m  [330/339], [94mLoss[0m : 1.57359

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.486
====================================

Phase 2 - Evaluation MAE:  2.485543451477996
MAE score P1       2.348716
MAE score P2       2.485543
loss               1.504037
learning_rate          0.01
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay          0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.01784
[1mStep[0m  [16/169], [94mLoss[0m : 10.81938
[1mStep[0m  [32/169], [94mLoss[0m : 11.65972
[1mStep[0m  [48/169], [94mLoss[0m : 9.57548
[1mStep[0m  [64/169], [94mLoss[0m : 9.97582
[1mStep[0m  [80/169], [94mLoss[0m : 9.84197
[1mStep[0m  [96/169], [94mLoss[0m : 9.14032
[1mStep[0m  [112/169], [94mLoss[0m : 9.67782
[1mStep[0m  [128/169], [94mLoss[0m : 8.25122
[1mStep[0m  [144/169], [94mLoss[0m : 9.05526
[1mStep[0m  [160/169], [94mLoss[0m : 7.63920

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.578, [92mTest[0m: 10.927, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.28459
[1mStep[0m  [16/169], [94mLoss[0m : 7.56233
[1mStep[0m  [32/169], [94mLoss[0m : 7.11750
[1mStep[0m  [48/169], [94mLoss[0m : 5.96220
[1mStep[0m  [64/169], [94mLoss[0m : 6.44969
[1mStep[0m  [80/169], [94mLoss[0m : 5.58264
[1mStep[0m  [96/169], [94mLoss[0m : 5.34963
[1mStep[0m  [112/169], [94mLoss[0m : 4.90717
[1mStep[0m  [128/169], [94mLoss[0m : 4.52937
[1mStep[0m  [144/169], [94mLoss[0m : 4.47565
[1mStep[0m  [160/169], [94mLoss[0m : 3.70377

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.841, [92mTest[0m: 7.236, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.43929
[1mStep[0m  [16/169], [94mLoss[0m : 3.44069
[1mStep[0m  [32/169], [94mLoss[0m : 2.86975
[1mStep[0m  [48/169], [94mLoss[0m : 2.60822
[1mStep[0m  [64/169], [94mLoss[0m : 2.83340
[1mStep[0m  [80/169], [94mLoss[0m : 2.61038
[1mStep[0m  [96/169], [94mLoss[0m : 2.70991
[1mStep[0m  [112/169], [94mLoss[0m : 2.36126
[1mStep[0m  [128/169], [94mLoss[0m : 2.99884
[1mStep[0m  [144/169], [94mLoss[0m : 2.95586
[1mStep[0m  [160/169], [94mLoss[0m : 2.51753

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.968, [92mTest[0m: 3.179, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63940
[1mStep[0m  [16/169], [94mLoss[0m : 2.50566
[1mStep[0m  [32/169], [94mLoss[0m : 2.54076
[1mStep[0m  [48/169], [94mLoss[0m : 3.14055
[1mStep[0m  [64/169], [94mLoss[0m : 2.72796
[1mStep[0m  [80/169], [94mLoss[0m : 3.20075
[1mStep[0m  [96/169], [94mLoss[0m : 2.57373
[1mStep[0m  [112/169], [94mLoss[0m : 2.53409
[1mStep[0m  [128/169], [94mLoss[0m : 3.45893
[1mStep[0m  [144/169], [94mLoss[0m : 2.88695
[1mStep[0m  [160/169], [94mLoss[0m : 2.55245

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31936
[1mStep[0m  [16/169], [94mLoss[0m : 2.84844
[1mStep[0m  [32/169], [94mLoss[0m : 2.52814
[1mStep[0m  [48/169], [94mLoss[0m : 2.95143
[1mStep[0m  [64/169], [94mLoss[0m : 2.41766
[1mStep[0m  [80/169], [94mLoss[0m : 2.30856
[1mStep[0m  [96/169], [94mLoss[0m : 2.02472
[1mStep[0m  [112/169], [94mLoss[0m : 3.02630
[1mStep[0m  [128/169], [94mLoss[0m : 2.18833
[1mStep[0m  [144/169], [94mLoss[0m : 2.55370
[1mStep[0m  [160/169], [94mLoss[0m : 2.73977

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74982
[1mStep[0m  [16/169], [94mLoss[0m : 2.73892
[1mStep[0m  [32/169], [94mLoss[0m : 2.38160
[1mStep[0m  [48/169], [94mLoss[0m : 2.35983
[1mStep[0m  [64/169], [94mLoss[0m : 2.57816
[1mStep[0m  [80/169], [94mLoss[0m : 2.58188
[1mStep[0m  [96/169], [94mLoss[0m : 2.93705
[1mStep[0m  [112/169], [94mLoss[0m : 3.31432
[1mStep[0m  [128/169], [94mLoss[0m : 2.46716
[1mStep[0m  [144/169], [94mLoss[0m : 2.38948
[1mStep[0m  [160/169], [94mLoss[0m : 2.34306

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61706
[1mStep[0m  [16/169], [94mLoss[0m : 2.69631
[1mStep[0m  [32/169], [94mLoss[0m : 2.69800
[1mStep[0m  [48/169], [94mLoss[0m : 2.51469
[1mStep[0m  [64/169], [94mLoss[0m : 2.51665
[1mStep[0m  [80/169], [94mLoss[0m : 2.74838
[1mStep[0m  [96/169], [94mLoss[0m : 2.83050
[1mStep[0m  [112/169], [94mLoss[0m : 2.44228
[1mStep[0m  [128/169], [94mLoss[0m : 3.04863
[1mStep[0m  [144/169], [94mLoss[0m : 2.37376
[1mStep[0m  [160/169], [94mLoss[0m : 2.55445

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83693
[1mStep[0m  [16/169], [94mLoss[0m : 2.84934
[1mStep[0m  [32/169], [94mLoss[0m : 2.17621
[1mStep[0m  [48/169], [94mLoss[0m : 2.31521
[1mStep[0m  [64/169], [94mLoss[0m : 2.66619
[1mStep[0m  [80/169], [94mLoss[0m : 2.96834
[1mStep[0m  [96/169], [94mLoss[0m : 2.43326
[1mStep[0m  [112/169], [94mLoss[0m : 2.75152
[1mStep[0m  [128/169], [94mLoss[0m : 2.71044
[1mStep[0m  [144/169], [94mLoss[0m : 2.82863
[1mStep[0m  [160/169], [94mLoss[0m : 2.56562

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.03155
[1mStep[0m  [16/169], [94mLoss[0m : 2.33713
[1mStep[0m  [32/169], [94mLoss[0m : 2.39436
[1mStep[0m  [48/169], [94mLoss[0m : 2.36690
[1mStep[0m  [64/169], [94mLoss[0m : 2.34656
[1mStep[0m  [80/169], [94mLoss[0m : 2.85208
[1mStep[0m  [96/169], [94mLoss[0m : 2.50888
[1mStep[0m  [112/169], [94mLoss[0m : 2.90176
[1mStep[0m  [128/169], [94mLoss[0m : 2.75445
[1mStep[0m  [144/169], [94mLoss[0m : 2.55101
[1mStep[0m  [160/169], [94mLoss[0m : 2.53210

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25777
[1mStep[0m  [16/169], [94mLoss[0m : 3.06553
[1mStep[0m  [32/169], [94mLoss[0m : 2.39080
[1mStep[0m  [48/169], [94mLoss[0m : 2.64205
[1mStep[0m  [64/169], [94mLoss[0m : 1.95416
[1mStep[0m  [80/169], [94mLoss[0m : 2.63026
[1mStep[0m  [96/169], [94mLoss[0m : 2.46140
[1mStep[0m  [112/169], [94mLoss[0m : 2.68458
[1mStep[0m  [128/169], [94mLoss[0m : 2.33290
[1mStep[0m  [144/169], [94mLoss[0m : 2.39565
[1mStep[0m  [160/169], [94mLoss[0m : 2.78233

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46075
[1mStep[0m  [16/169], [94mLoss[0m : 2.72400
[1mStep[0m  [32/169], [94mLoss[0m : 2.25796
[1mStep[0m  [48/169], [94mLoss[0m : 2.29238
[1mStep[0m  [64/169], [94mLoss[0m : 2.16829
[1mStep[0m  [80/169], [94mLoss[0m : 2.40546
[1mStep[0m  [96/169], [94mLoss[0m : 2.37749
[1mStep[0m  [112/169], [94mLoss[0m : 2.54394
[1mStep[0m  [128/169], [94mLoss[0m : 2.97870
[1mStep[0m  [144/169], [94mLoss[0m : 2.69899
[1mStep[0m  [160/169], [94mLoss[0m : 2.46918

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79115
[1mStep[0m  [16/169], [94mLoss[0m : 2.26802
[1mStep[0m  [32/169], [94mLoss[0m : 2.29282
[1mStep[0m  [48/169], [94mLoss[0m : 2.35356
[1mStep[0m  [64/169], [94mLoss[0m : 2.73911
[1mStep[0m  [80/169], [94mLoss[0m : 2.54130
[1mStep[0m  [96/169], [94mLoss[0m : 2.41367
[1mStep[0m  [112/169], [94mLoss[0m : 2.16344
[1mStep[0m  [128/169], [94mLoss[0m : 2.54087
[1mStep[0m  [144/169], [94mLoss[0m : 2.53144
[1mStep[0m  [160/169], [94mLoss[0m : 2.70537

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41930
[1mStep[0m  [16/169], [94mLoss[0m : 2.27140
[1mStep[0m  [32/169], [94mLoss[0m : 2.54155
[1mStep[0m  [48/169], [94mLoss[0m : 2.42623
[1mStep[0m  [64/169], [94mLoss[0m : 2.70076
[1mStep[0m  [80/169], [94mLoss[0m : 2.27083
[1mStep[0m  [96/169], [94mLoss[0m : 2.60387
[1mStep[0m  [112/169], [94mLoss[0m : 2.51686
[1mStep[0m  [128/169], [94mLoss[0m : 2.44715
[1mStep[0m  [144/169], [94mLoss[0m : 2.72718
[1mStep[0m  [160/169], [94mLoss[0m : 2.47759

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43528
[1mStep[0m  [16/169], [94mLoss[0m : 2.22214
[1mStep[0m  [32/169], [94mLoss[0m : 2.76377
[1mStep[0m  [48/169], [94mLoss[0m : 2.62819
[1mStep[0m  [64/169], [94mLoss[0m : 2.41995
[1mStep[0m  [80/169], [94mLoss[0m : 2.43005
[1mStep[0m  [96/169], [94mLoss[0m : 2.49041
[1mStep[0m  [112/169], [94mLoss[0m : 2.60237
[1mStep[0m  [128/169], [94mLoss[0m : 2.47333
[1mStep[0m  [144/169], [94mLoss[0m : 2.78027
[1mStep[0m  [160/169], [94mLoss[0m : 2.80102

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73859
[1mStep[0m  [16/169], [94mLoss[0m : 2.19758
[1mStep[0m  [32/169], [94mLoss[0m : 2.47799
[1mStep[0m  [48/169], [94mLoss[0m : 2.63477
[1mStep[0m  [64/169], [94mLoss[0m : 2.29076
[1mStep[0m  [80/169], [94mLoss[0m : 3.01755
[1mStep[0m  [96/169], [94mLoss[0m : 2.28496
[1mStep[0m  [112/169], [94mLoss[0m : 2.52261
[1mStep[0m  [128/169], [94mLoss[0m : 2.40755
[1mStep[0m  [144/169], [94mLoss[0m : 2.32113
[1mStep[0m  [160/169], [94mLoss[0m : 2.39850

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.314, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65206
[1mStep[0m  [16/169], [94mLoss[0m : 2.67523
[1mStep[0m  [32/169], [94mLoss[0m : 2.32180
[1mStep[0m  [48/169], [94mLoss[0m : 2.66999
[1mStep[0m  [64/169], [94mLoss[0m : 2.35105
[1mStep[0m  [80/169], [94mLoss[0m : 2.21906
[1mStep[0m  [96/169], [94mLoss[0m : 2.71059
[1mStep[0m  [112/169], [94mLoss[0m : 2.26874
[1mStep[0m  [128/169], [94mLoss[0m : 2.34266
[1mStep[0m  [144/169], [94mLoss[0m : 3.12996
[1mStep[0m  [160/169], [94mLoss[0m : 2.33231

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.312, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91665
[1mStep[0m  [16/169], [94mLoss[0m : 2.90474
[1mStep[0m  [32/169], [94mLoss[0m : 2.60337
[1mStep[0m  [48/169], [94mLoss[0m : 2.70945
[1mStep[0m  [64/169], [94mLoss[0m : 2.14457
[1mStep[0m  [80/169], [94mLoss[0m : 2.51251
[1mStep[0m  [96/169], [94mLoss[0m : 2.59808
[1mStep[0m  [112/169], [94mLoss[0m : 2.79927
[1mStep[0m  [128/169], [94mLoss[0m : 2.30757
[1mStep[0m  [144/169], [94mLoss[0m : 2.92582
[1mStep[0m  [160/169], [94mLoss[0m : 2.39316

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79664
[1mStep[0m  [16/169], [94mLoss[0m : 2.40477
[1mStep[0m  [32/169], [94mLoss[0m : 2.31010
[1mStep[0m  [48/169], [94mLoss[0m : 2.73607
[1mStep[0m  [64/169], [94mLoss[0m : 2.79636
[1mStep[0m  [80/169], [94mLoss[0m : 2.67109
[1mStep[0m  [96/169], [94mLoss[0m : 2.64517
[1mStep[0m  [112/169], [94mLoss[0m : 2.49582
[1mStep[0m  [128/169], [94mLoss[0m : 2.37361
[1mStep[0m  [144/169], [94mLoss[0m : 2.42144
[1mStep[0m  [160/169], [94mLoss[0m : 2.54328

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53269
[1mStep[0m  [16/169], [94mLoss[0m : 2.48023
[1mStep[0m  [32/169], [94mLoss[0m : 2.55211
[1mStep[0m  [48/169], [94mLoss[0m : 2.10331
[1mStep[0m  [64/169], [94mLoss[0m : 1.97055
[1mStep[0m  [80/169], [94mLoss[0m : 2.75799
[1mStep[0m  [96/169], [94mLoss[0m : 2.28999
[1mStep[0m  [112/169], [94mLoss[0m : 2.39973
[1mStep[0m  [128/169], [94mLoss[0m : 2.00181
[1mStep[0m  [144/169], [94mLoss[0m : 2.38851
[1mStep[0m  [160/169], [94mLoss[0m : 2.74871

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97197
[1mStep[0m  [16/169], [94mLoss[0m : 2.59475
[1mStep[0m  [32/169], [94mLoss[0m : 2.69037
[1mStep[0m  [48/169], [94mLoss[0m : 2.26280
[1mStep[0m  [64/169], [94mLoss[0m : 2.15473
[1mStep[0m  [80/169], [94mLoss[0m : 2.16387
[1mStep[0m  [96/169], [94mLoss[0m : 2.24481
[1mStep[0m  [112/169], [94mLoss[0m : 2.58894
[1mStep[0m  [128/169], [94mLoss[0m : 2.56027
[1mStep[0m  [144/169], [94mLoss[0m : 2.09355
[1mStep[0m  [160/169], [94mLoss[0m : 2.91287

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13336
[1mStep[0m  [16/169], [94mLoss[0m : 2.38257
[1mStep[0m  [32/169], [94mLoss[0m : 2.51380
[1mStep[0m  [48/169], [94mLoss[0m : 2.12224
[1mStep[0m  [64/169], [94mLoss[0m : 2.02405
[1mStep[0m  [80/169], [94mLoss[0m : 2.57889
[1mStep[0m  [96/169], [94mLoss[0m : 2.43142
[1mStep[0m  [112/169], [94mLoss[0m : 2.54334
[1mStep[0m  [128/169], [94mLoss[0m : 2.56908
[1mStep[0m  [144/169], [94mLoss[0m : 2.99337
[1mStep[0m  [160/169], [94mLoss[0m : 2.44433

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79076
[1mStep[0m  [16/169], [94mLoss[0m : 2.68161
[1mStep[0m  [32/169], [94mLoss[0m : 2.29441
[1mStep[0m  [48/169], [94mLoss[0m : 2.93948
[1mStep[0m  [64/169], [94mLoss[0m : 2.46477
[1mStep[0m  [80/169], [94mLoss[0m : 3.16300
[1mStep[0m  [96/169], [94mLoss[0m : 2.42584
[1mStep[0m  [112/169], [94mLoss[0m : 2.52104
[1mStep[0m  [128/169], [94mLoss[0m : 2.26517
[1mStep[0m  [144/169], [94mLoss[0m : 2.65464
[1mStep[0m  [160/169], [94mLoss[0m : 2.17142

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32837
[1mStep[0m  [16/169], [94mLoss[0m : 2.29465
[1mStep[0m  [32/169], [94mLoss[0m : 2.32533
[1mStep[0m  [48/169], [94mLoss[0m : 2.53384
[1mStep[0m  [64/169], [94mLoss[0m : 2.67339
[1mStep[0m  [80/169], [94mLoss[0m : 2.94399
[1mStep[0m  [96/169], [94mLoss[0m : 2.56595
[1mStep[0m  [112/169], [94mLoss[0m : 2.54989
[1mStep[0m  [128/169], [94mLoss[0m : 2.48957
[1mStep[0m  [144/169], [94mLoss[0m : 2.71950
[1mStep[0m  [160/169], [94mLoss[0m : 2.71081

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84373
[1mStep[0m  [16/169], [94mLoss[0m : 2.52812
[1mStep[0m  [32/169], [94mLoss[0m : 2.51522
[1mStep[0m  [48/169], [94mLoss[0m : 2.30284
[1mStep[0m  [64/169], [94mLoss[0m : 2.36196
[1mStep[0m  [80/169], [94mLoss[0m : 2.38861
[1mStep[0m  [96/169], [94mLoss[0m : 2.66677
[1mStep[0m  [112/169], [94mLoss[0m : 2.27695
[1mStep[0m  [128/169], [94mLoss[0m : 2.31453
[1mStep[0m  [144/169], [94mLoss[0m : 2.49931
[1mStep[0m  [160/169], [94mLoss[0m : 2.60159

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51910
[1mStep[0m  [16/169], [94mLoss[0m : 2.40933
[1mStep[0m  [32/169], [94mLoss[0m : 2.76797
[1mStep[0m  [48/169], [94mLoss[0m : 2.22408
[1mStep[0m  [64/169], [94mLoss[0m : 2.17449
[1mStep[0m  [80/169], [94mLoss[0m : 2.33612
[1mStep[0m  [96/169], [94mLoss[0m : 2.14951
[1mStep[0m  [112/169], [94mLoss[0m : 2.50836
[1mStep[0m  [128/169], [94mLoss[0m : 2.63773
[1mStep[0m  [144/169], [94mLoss[0m : 2.00740
[1mStep[0m  [160/169], [94mLoss[0m : 2.32636

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63723
[1mStep[0m  [16/169], [94mLoss[0m : 2.38265
[1mStep[0m  [32/169], [94mLoss[0m : 2.74718
[1mStep[0m  [48/169], [94mLoss[0m : 2.54781
[1mStep[0m  [64/169], [94mLoss[0m : 2.33113
[1mStep[0m  [80/169], [94mLoss[0m : 1.99303
[1mStep[0m  [96/169], [94mLoss[0m : 2.17254
[1mStep[0m  [112/169], [94mLoss[0m : 2.91736
[1mStep[0m  [128/169], [94mLoss[0m : 2.57164
[1mStep[0m  [144/169], [94mLoss[0m : 2.49667
[1mStep[0m  [160/169], [94mLoss[0m : 2.53608

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.320, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45185
[1mStep[0m  [16/169], [94mLoss[0m : 2.44405
[1mStep[0m  [32/169], [94mLoss[0m : 2.72549
[1mStep[0m  [48/169], [94mLoss[0m : 2.37480
[1mStep[0m  [64/169], [94mLoss[0m : 2.84114
[1mStep[0m  [80/169], [94mLoss[0m : 2.48165
[1mStep[0m  [96/169], [94mLoss[0m : 2.85963
[1mStep[0m  [112/169], [94mLoss[0m : 2.61647
[1mStep[0m  [128/169], [94mLoss[0m : 2.43219
[1mStep[0m  [144/169], [94mLoss[0m : 2.66124
[1mStep[0m  [160/169], [94mLoss[0m : 2.67639

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68839
[1mStep[0m  [16/169], [94mLoss[0m : 2.49477
[1mStep[0m  [32/169], [94mLoss[0m : 2.55810
[1mStep[0m  [48/169], [94mLoss[0m : 2.38719
[1mStep[0m  [64/169], [94mLoss[0m : 2.31715
[1mStep[0m  [80/169], [94mLoss[0m : 2.47712
[1mStep[0m  [96/169], [94mLoss[0m : 2.77433
[1mStep[0m  [112/169], [94mLoss[0m : 2.51630
[1mStep[0m  [128/169], [94mLoss[0m : 2.27054
[1mStep[0m  [144/169], [94mLoss[0m : 2.83694
[1mStep[0m  [160/169], [94mLoss[0m : 2.33317

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71327
[1mStep[0m  [16/169], [94mLoss[0m : 2.77016
[1mStep[0m  [32/169], [94mLoss[0m : 2.14619
[1mStep[0m  [48/169], [94mLoss[0m : 2.72259
[1mStep[0m  [64/169], [94mLoss[0m : 2.30929
[1mStep[0m  [80/169], [94mLoss[0m : 2.50317
[1mStep[0m  [96/169], [94mLoss[0m : 2.29631
[1mStep[0m  [112/169], [94mLoss[0m : 2.43275
[1mStep[0m  [128/169], [94mLoss[0m : 2.28864
[1mStep[0m  [144/169], [94mLoss[0m : 2.49756
[1mStep[0m  [160/169], [94mLoss[0m : 2.30165

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87213
[1mStep[0m  [16/169], [94mLoss[0m : 2.40839
[1mStep[0m  [32/169], [94mLoss[0m : 2.65394
[1mStep[0m  [48/169], [94mLoss[0m : 2.38189
[1mStep[0m  [64/169], [94mLoss[0m : 2.23748
[1mStep[0m  [80/169], [94mLoss[0m : 2.56564
[1mStep[0m  [96/169], [94mLoss[0m : 2.62465
[1mStep[0m  [112/169], [94mLoss[0m : 2.48021
[1mStep[0m  [128/169], [94mLoss[0m : 2.20343
[1mStep[0m  [144/169], [94mLoss[0m : 1.93183
[1mStep[0m  [160/169], [94mLoss[0m : 2.42991

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.319
====================================

Phase 1 - Evaluation MAE:  2.3190061173268726
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.78919
[1mStep[0m  [16/169], [94mLoss[0m : 2.57867
[1mStep[0m  [32/169], [94mLoss[0m : 2.30503
[1mStep[0m  [48/169], [94mLoss[0m : 2.50822
[1mStep[0m  [64/169], [94mLoss[0m : 2.06404
[1mStep[0m  [80/169], [94mLoss[0m : 2.26578
[1mStep[0m  [96/169], [94mLoss[0m : 2.79788
[1mStep[0m  [112/169], [94mLoss[0m : 2.61345
[1mStep[0m  [128/169], [94mLoss[0m : 2.32578
[1mStep[0m  [144/169], [94mLoss[0m : 2.32172
[1mStep[0m  [160/169], [94mLoss[0m : 2.47717

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39992
[1mStep[0m  [16/169], [94mLoss[0m : 2.61106
[1mStep[0m  [32/169], [94mLoss[0m : 2.58118
[1mStep[0m  [48/169], [94mLoss[0m : 2.48730
[1mStep[0m  [64/169], [94mLoss[0m : 2.78515
[1mStep[0m  [80/169], [94mLoss[0m : 2.69884
[1mStep[0m  [96/169], [94mLoss[0m : 2.37064
[1mStep[0m  [112/169], [94mLoss[0m : 2.18807
[1mStep[0m  [128/169], [94mLoss[0m : 2.11196
[1mStep[0m  [144/169], [94mLoss[0m : 2.90528
[1mStep[0m  [160/169], [94mLoss[0m : 2.22583

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93340
[1mStep[0m  [16/169], [94mLoss[0m : 2.18322
[1mStep[0m  [32/169], [94mLoss[0m : 2.42829
[1mStep[0m  [48/169], [94mLoss[0m : 2.04650
[1mStep[0m  [64/169], [94mLoss[0m : 2.54209
[1mStep[0m  [80/169], [94mLoss[0m : 2.21431
[1mStep[0m  [96/169], [94mLoss[0m : 2.56902
[1mStep[0m  [112/169], [94mLoss[0m : 2.50625
[1mStep[0m  [128/169], [94mLoss[0m : 2.23732
[1mStep[0m  [144/169], [94mLoss[0m : 1.92318
[1mStep[0m  [160/169], [94mLoss[0m : 2.13643

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90438
[1mStep[0m  [16/169], [94mLoss[0m : 2.24864
[1mStep[0m  [32/169], [94mLoss[0m : 2.42460
[1mStep[0m  [48/169], [94mLoss[0m : 2.20776
[1mStep[0m  [64/169], [94mLoss[0m : 1.97163
[1mStep[0m  [80/169], [94mLoss[0m : 2.27814
[1mStep[0m  [96/169], [94mLoss[0m : 2.09035
[1mStep[0m  [112/169], [94mLoss[0m : 2.30635
[1mStep[0m  [128/169], [94mLoss[0m : 2.19199
[1mStep[0m  [144/169], [94mLoss[0m : 2.09287
[1mStep[0m  [160/169], [94mLoss[0m : 2.49497

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.262, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35312
[1mStep[0m  [16/169], [94mLoss[0m : 2.03301
[1mStep[0m  [32/169], [94mLoss[0m : 2.41148
[1mStep[0m  [48/169], [94mLoss[0m : 2.22411
[1mStep[0m  [64/169], [94mLoss[0m : 2.17695
[1mStep[0m  [80/169], [94mLoss[0m : 2.04229
[1mStep[0m  [96/169], [94mLoss[0m : 2.58705
[1mStep[0m  [112/169], [94mLoss[0m : 2.19498
[1mStep[0m  [128/169], [94mLoss[0m : 2.32092
[1mStep[0m  [144/169], [94mLoss[0m : 2.12033
[1mStep[0m  [160/169], [94mLoss[0m : 1.84943

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07197
[1mStep[0m  [16/169], [94mLoss[0m : 1.74355
[1mStep[0m  [32/169], [94mLoss[0m : 1.96942
[1mStep[0m  [48/169], [94mLoss[0m : 1.94995
[1mStep[0m  [64/169], [94mLoss[0m : 1.51894
[1mStep[0m  [80/169], [94mLoss[0m : 2.43775
[1mStep[0m  [96/169], [94mLoss[0m : 2.45327
[1mStep[0m  [112/169], [94mLoss[0m : 2.46236
[1mStep[0m  [128/169], [94mLoss[0m : 1.95499
[1mStep[0m  [144/169], [94mLoss[0m : 1.99802
[1mStep[0m  [160/169], [94mLoss[0m : 2.05235

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07197
[1mStep[0m  [16/169], [94mLoss[0m : 1.94904
[1mStep[0m  [32/169], [94mLoss[0m : 1.76275
[1mStep[0m  [48/169], [94mLoss[0m : 2.76734
[1mStep[0m  [64/169], [94mLoss[0m : 1.96221
[1mStep[0m  [80/169], [94mLoss[0m : 2.49123
[1mStep[0m  [96/169], [94mLoss[0m : 2.31028
[1mStep[0m  [112/169], [94mLoss[0m : 1.85594
[1mStep[0m  [128/169], [94mLoss[0m : 2.16545
[1mStep[0m  [144/169], [94mLoss[0m : 2.46260
[1mStep[0m  [160/169], [94mLoss[0m : 2.47828

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82079
[1mStep[0m  [16/169], [94mLoss[0m : 1.56901
[1mStep[0m  [32/169], [94mLoss[0m : 1.82547
[1mStep[0m  [48/169], [94mLoss[0m : 2.14693
[1mStep[0m  [64/169], [94mLoss[0m : 2.08940
[1mStep[0m  [80/169], [94mLoss[0m : 1.94097
[1mStep[0m  [96/169], [94mLoss[0m : 1.78767
[1mStep[0m  [112/169], [94mLoss[0m : 2.19000
[1mStep[0m  [128/169], [94mLoss[0m : 2.06116
[1mStep[0m  [144/169], [94mLoss[0m : 1.76753
[1mStep[0m  [160/169], [94mLoss[0m : 2.35114

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74631
[1mStep[0m  [16/169], [94mLoss[0m : 2.29505
[1mStep[0m  [32/169], [94mLoss[0m : 1.98552
[1mStep[0m  [48/169], [94mLoss[0m : 1.96619
[1mStep[0m  [64/169], [94mLoss[0m : 2.21195
[1mStep[0m  [80/169], [94mLoss[0m : 1.87183
[1mStep[0m  [96/169], [94mLoss[0m : 1.71441
[1mStep[0m  [112/169], [94mLoss[0m : 1.98992
[1mStep[0m  [128/169], [94mLoss[0m : 2.12871
[1mStep[0m  [144/169], [94mLoss[0m : 1.63224
[1mStep[0m  [160/169], [94mLoss[0m : 2.16566

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62517
[1mStep[0m  [16/169], [94mLoss[0m : 1.68963
[1mStep[0m  [32/169], [94mLoss[0m : 2.34149
[1mStep[0m  [48/169], [94mLoss[0m : 1.82699
[1mStep[0m  [64/169], [94mLoss[0m : 1.91065
[1mStep[0m  [80/169], [94mLoss[0m : 2.05687
[1mStep[0m  [96/169], [94mLoss[0m : 1.84815
[1mStep[0m  [112/169], [94mLoss[0m : 1.83009
[1mStep[0m  [128/169], [94mLoss[0m : 2.36752
[1mStep[0m  [144/169], [94mLoss[0m : 1.74750
[1mStep[0m  [160/169], [94mLoss[0m : 2.29767

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96728
[1mStep[0m  [16/169], [94mLoss[0m : 2.03290
[1mStep[0m  [32/169], [94mLoss[0m : 1.86355
[1mStep[0m  [48/169], [94mLoss[0m : 1.69427
[1mStep[0m  [64/169], [94mLoss[0m : 2.01662
[1mStep[0m  [80/169], [94mLoss[0m : 1.82641
[1mStep[0m  [96/169], [94mLoss[0m : 2.05284
[1mStep[0m  [112/169], [94mLoss[0m : 1.78708
[1mStep[0m  [128/169], [94mLoss[0m : 2.07927
[1mStep[0m  [144/169], [94mLoss[0m : 1.81521
[1mStep[0m  [160/169], [94mLoss[0m : 2.43280

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92723
[1mStep[0m  [16/169], [94mLoss[0m : 2.11086
[1mStep[0m  [32/169], [94mLoss[0m : 1.75494
[1mStep[0m  [48/169], [94mLoss[0m : 2.03041
[1mStep[0m  [64/169], [94mLoss[0m : 1.43980
[1mStep[0m  [80/169], [94mLoss[0m : 1.75383
[1mStep[0m  [96/169], [94mLoss[0m : 1.88600
[1mStep[0m  [112/169], [94mLoss[0m : 1.63573
[1mStep[0m  [128/169], [94mLoss[0m : 2.14354
[1mStep[0m  [144/169], [94mLoss[0m : 1.90611
[1mStep[0m  [160/169], [94mLoss[0m : 1.82641

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94848
[1mStep[0m  [16/169], [94mLoss[0m : 1.71104
[1mStep[0m  [32/169], [94mLoss[0m : 1.80358
[1mStep[0m  [48/169], [94mLoss[0m : 1.92533
[1mStep[0m  [64/169], [94mLoss[0m : 1.29102
[1mStep[0m  [80/169], [94mLoss[0m : 1.91937
[1mStep[0m  [96/169], [94mLoss[0m : 1.49235
[1mStep[0m  [112/169], [94mLoss[0m : 1.59965
[1mStep[0m  [128/169], [94mLoss[0m : 1.75916
[1mStep[0m  [144/169], [94mLoss[0m : 1.92636
[1mStep[0m  [160/169], [94mLoss[0m : 1.83838

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.498, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86752
[1mStep[0m  [16/169], [94mLoss[0m : 2.07273
[1mStep[0m  [32/169], [94mLoss[0m : 1.75329
[1mStep[0m  [48/169], [94mLoss[0m : 1.56659
[1mStep[0m  [64/169], [94mLoss[0m : 1.89656
[1mStep[0m  [80/169], [94mLoss[0m : 2.15890
[1mStep[0m  [96/169], [94mLoss[0m : 1.66147
[1mStep[0m  [112/169], [94mLoss[0m : 1.76040
[1mStep[0m  [128/169], [94mLoss[0m : 1.71783
[1mStep[0m  [144/169], [94mLoss[0m : 1.73300
[1mStep[0m  [160/169], [94mLoss[0m : 1.74715

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.793, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40380
[1mStep[0m  [16/169], [94mLoss[0m : 1.78905
[1mStep[0m  [32/169], [94mLoss[0m : 1.90531
[1mStep[0m  [48/169], [94mLoss[0m : 1.61698
[1mStep[0m  [64/169], [94mLoss[0m : 1.64226
[1mStep[0m  [80/169], [94mLoss[0m : 1.74857
[1mStep[0m  [96/169], [94mLoss[0m : 1.83566
[1mStep[0m  [112/169], [94mLoss[0m : 1.56427
[1mStep[0m  [128/169], [94mLoss[0m : 1.42704
[1mStep[0m  [144/169], [94mLoss[0m : 1.49876
[1mStep[0m  [160/169], [94mLoss[0m : 1.52450

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.453, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85810
[1mStep[0m  [16/169], [94mLoss[0m : 1.62666
[1mStep[0m  [32/169], [94mLoss[0m : 1.77485
[1mStep[0m  [48/169], [94mLoss[0m : 1.38869
[1mStep[0m  [64/169], [94mLoss[0m : 1.35121
[1mStep[0m  [80/169], [94mLoss[0m : 1.60043
[1mStep[0m  [96/169], [94mLoss[0m : 1.52187
[1mStep[0m  [112/169], [94mLoss[0m : 2.25894
[1mStep[0m  [128/169], [94mLoss[0m : 1.77861
[1mStep[0m  [144/169], [94mLoss[0m : 1.69919
[1mStep[0m  [160/169], [94mLoss[0m : 1.85605

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68082
[1mStep[0m  [16/169], [94mLoss[0m : 1.94667
[1mStep[0m  [32/169], [94mLoss[0m : 1.43530
[1mStep[0m  [48/169], [94mLoss[0m : 1.48875
[1mStep[0m  [64/169], [94mLoss[0m : 1.53406
[1mStep[0m  [80/169], [94mLoss[0m : 1.81295
[1mStep[0m  [96/169], [94mLoss[0m : 1.67351
[1mStep[0m  [112/169], [94mLoss[0m : 1.18444
[1mStep[0m  [128/169], [94mLoss[0m : 1.54398
[1mStep[0m  [144/169], [94mLoss[0m : 1.86076
[1mStep[0m  [160/169], [94mLoss[0m : 1.48038

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.661, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47997
[1mStep[0m  [16/169], [94mLoss[0m : 1.78813
[1mStep[0m  [32/169], [94mLoss[0m : 1.33199
[1mStep[0m  [48/169], [94mLoss[0m : 1.61332
[1mStep[0m  [64/169], [94mLoss[0m : 1.52018
[1mStep[0m  [80/169], [94mLoss[0m : 1.74542
[1mStep[0m  [96/169], [94mLoss[0m : 1.63790
[1mStep[0m  [112/169], [94mLoss[0m : 1.84206
[1mStep[0m  [128/169], [94mLoss[0m : 1.73952
[1mStep[0m  [144/169], [94mLoss[0m : 2.03627
[1mStep[0m  [160/169], [94mLoss[0m : 1.43215

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.643, [92mTest[0m: 2.535, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34936
[1mStep[0m  [16/169], [94mLoss[0m : 1.30169
[1mStep[0m  [32/169], [94mLoss[0m : 1.61168
[1mStep[0m  [48/169], [94mLoss[0m : 1.38889
[1mStep[0m  [64/169], [94mLoss[0m : 1.68458
[1mStep[0m  [80/169], [94mLoss[0m : 1.52267
[1mStep[0m  [96/169], [94mLoss[0m : 1.54010
[1mStep[0m  [112/169], [94mLoss[0m : 1.69688
[1mStep[0m  [128/169], [94mLoss[0m : 1.74115
[1mStep[0m  [144/169], [94mLoss[0m : 1.60104
[1mStep[0m  [160/169], [94mLoss[0m : 1.54335

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68442
[1mStep[0m  [16/169], [94mLoss[0m : 1.73527
[1mStep[0m  [32/169], [94mLoss[0m : 1.58967
[1mStep[0m  [48/169], [94mLoss[0m : 1.52918
[1mStep[0m  [64/169], [94mLoss[0m : 1.47319
[1mStep[0m  [80/169], [94mLoss[0m : 1.69405
[1mStep[0m  [96/169], [94mLoss[0m : 1.53506
[1mStep[0m  [112/169], [94mLoss[0m : 1.52375
[1mStep[0m  [128/169], [94mLoss[0m : 1.43125
[1mStep[0m  [144/169], [94mLoss[0m : 1.69972
[1mStep[0m  [160/169], [94mLoss[0m : 1.40627

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.599, [92mTest[0m: 2.508, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56687
[1mStep[0m  [16/169], [94mLoss[0m : 1.51956
[1mStep[0m  [32/169], [94mLoss[0m : 1.17275
[1mStep[0m  [48/169], [94mLoss[0m : 1.81082
[1mStep[0m  [64/169], [94mLoss[0m : 1.64016
[1mStep[0m  [80/169], [94mLoss[0m : 1.33010
[1mStep[0m  [96/169], [94mLoss[0m : 1.75319
[1mStep[0m  [112/169], [94mLoss[0m : 1.41802
[1mStep[0m  [128/169], [94mLoss[0m : 1.28044
[1mStep[0m  [144/169], [94mLoss[0m : 1.61061
[1mStep[0m  [160/169], [94mLoss[0m : 1.63849

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.11548
[1mStep[0m  [16/169], [94mLoss[0m : 1.42509
[1mStep[0m  [32/169], [94mLoss[0m : 1.61992
[1mStep[0m  [48/169], [94mLoss[0m : 1.53810
[1mStep[0m  [64/169], [94mLoss[0m : 1.69365
[1mStep[0m  [80/169], [94mLoss[0m : 1.56547
[1mStep[0m  [96/169], [94mLoss[0m : 1.61806
[1mStep[0m  [112/169], [94mLoss[0m : 1.57423
[1mStep[0m  [128/169], [94mLoss[0m : 1.59448
[1mStep[0m  [144/169], [94mLoss[0m : 1.59326
[1mStep[0m  [160/169], [94mLoss[0m : 1.57241

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.513, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73128
[1mStep[0m  [16/169], [94mLoss[0m : 1.33579
[1mStep[0m  [32/169], [94mLoss[0m : 1.70549
[1mStep[0m  [48/169], [94mLoss[0m : 1.29334
[1mStep[0m  [64/169], [94mLoss[0m : 1.35251
[1mStep[0m  [80/169], [94mLoss[0m : 1.67735
[1mStep[0m  [96/169], [94mLoss[0m : 1.48998
[1mStep[0m  [112/169], [94mLoss[0m : 1.55790
[1mStep[0m  [128/169], [94mLoss[0m : 1.49358
[1mStep[0m  [144/169], [94mLoss[0m : 1.56965
[1mStep[0m  [160/169], [94mLoss[0m : 1.45046

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.503, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.30019
[1mStep[0m  [16/169], [94mLoss[0m : 1.49523
[1mStep[0m  [32/169], [94mLoss[0m : 1.27703
[1mStep[0m  [48/169], [94mLoss[0m : 1.31046
[1mStep[0m  [64/169], [94mLoss[0m : 1.04319
[1mStep[0m  [80/169], [94mLoss[0m : 1.83110
[1mStep[0m  [96/169], [94mLoss[0m : 1.58399
[1mStep[0m  [112/169], [94mLoss[0m : 1.25847
[1mStep[0m  [128/169], [94mLoss[0m : 1.49246
[1mStep[0m  [144/169], [94mLoss[0m : 1.51060
[1mStep[0m  [160/169], [94mLoss[0m : 1.62810

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48391
[1mStep[0m  [16/169], [94mLoss[0m : 1.36307
[1mStep[0m  [32/169], [94mLoss[0m : 1.63702
[1mStep[0m  [48/169], [94mLoss[0m : 1.59931
[1mStep[0m  [64/169], [94mLoss[0m : 1.36013
[1mStep[0m  [80/169], [94mLoss[0m : 1.70222
[1mStep[0m  [96/169], [94mLoss[0m : 1.45801
[1mStep[0m  [112/169], [94mLoss[0m : 1.42603
[1mStep[0m  [128/169], [94mLoss[0m : 1.68929
[1mStep[0m  [144/169], [94mLoss[0m : 1.66480
[1mStep[0m  [160/169], [94mLoss[0m : 1.60739

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.452, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.20345
[1mStep[0m  [16/169], [94mLoss[0m : 1.26081
[1mStep[0m  [32/169], [94mLoss[0m : 1.71137
[1mStep[0m  [48/169], [94mLoss[0m : 1.43437
[1mStep[0m  [64/169], [94mLoss[0m : 1.23492
[1mStep[0m  [80/169], [94mLoss[0m : 1.30135
[1mStep[0m  [96/169], [94mLoss[0m : 1.27456
[1mStep[0m  [112/169], [94mLoss[0m : 1.40384
[1mStep[0m  [128/169], [94mLoss[0m : 1.22917
[1mStep[0m  [144/169], [94mLoss[0m : 1.41636
[1mStep[0m  [160/169], [94mLoss[0m : 1.90489

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.424, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.484
====================================

Phase 2 - Evaluation MAE:  2.4836138018539975
MAE score P1        2.319006
MAE score P2        2.483614
loss                1.424167
learning_rate           0.01
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.1
weight_decay          0.0001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 10.46331
[1mStep[0m  [33/339], [94mLoss[0m : 3.77044
[1mStep[0m  [66/339], [94mLoss[0m : 2.41896
[1mStep[0m  [99/339], [94mLoss[0m : 3.39748
[1mStep[0m  [132/339], [94mLoss[0m : 2.77125
[1mStep[0m  [165/339], [94mLoss[0m : 2.84288
[1mStep[0m  [198/339], [94mLoss[0m : 2.57796
[1mStep[0m  [231/339], [94mLoss[0m : 3.40191
[1mStep[0m  [264/339], [94mLoss[0m : 2.78824
[1mStep[0m  [297/339], [94mLoss[0m : 2.60114
[1mStep[0m  [330/339], [94mLoss[0m : 2.75893

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.005, [92mTest[0m: 10.825, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16985
[1mStep[0m  [33/339], [94mLoss[0m : 2.33424
[1mStep[0m  [66/339], [94mLoss[0m : 1.91811
[1mStep[0m  [99/339], [94mLoss[0m : 2.14523
[1mStep[0m  [132/339], [94mLoss[0m : 2.62228
[1mStep[0m  [165/339], [94mLoss[0m : 2.81467
[1mStep[0m  [198/339], [94mLoss[0m : 2.60894
[1mStep[0m  [231/339], [94mLoss[0m : 2.45942
[1mStep[0m  [264/339], [94mLoss[0m : 2.63241
[1mStep[0m  [297/339], [94mLoss[0m : 2.25319
[1mStep[0m  [330/339], [94mLoss[0m : 1.89483

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61795
[1mStep[0m  [33/339], [94mLoss[0m : 2.11626
[1mStep[0m  [66/339], [94mLoss[0m : 1.69115
[1mStep[0m  [99/339], [94mLoss[0m : 2.14399
[1mStep[0m  [132/339], [94mLoss[0m : 2.95929
[1mStep[0m  [165/339], [94mLoss[0m : 3.11811
[1mStep[0m  [198/339], [94mLoss[0m : 2.58890
[1mStep[0m  [231/339], [94mLoss[0m : 2.89018
[1mStep[0m  [264/339], [94mLoss[0m : 2.80491
[1mStep[0m  [297/339], [94mLoss[0m : 2.19917
[1mStep[0m  [330/339], [94mLoss[0m : 2.93679

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28808
[1mStep[0m  [33/339], [94mLoss[0m : 2.44819
[1mStep[0m  [66/339], [94mLoss[0m : 2.29809
[1mStep[0m  [99/339], [94mLoss[0m : 2.27865
[1mStep[0m  [132/339], [94mLoss[0m : 2.08189
[1mStep[0m  [165/339], [94mLoss[0m : 1.99530
[1mStep[0m  [198/339], [94mLoss[0m : 2.20306
[1mStep[0m  [231/339], [94mLoss[0m : 2.43353
[1mStep[0m  [264/339], [94mLoss[0m : 3.32442
[1mStep[0m  [297/339], [94mLoss[0m : 1.71853
[1mStep[0m  [330/339], [94mLoss[0m : 2.28387

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14485
[1mStep[0m  [33/339], [94mLoss[0m : 2.57293
[1mStep[0m  [66/339], [94mLoss[0m : 1.95098
[1mStep[0m  [99/339], [94mLoss[0m : 2.91017
[1mStep[0m  [132/339], [94mLoss[0m : 2.65568
[1mStep[0m  [165/339], [94mLoss[0m : 2.34414
[1mStep[0m  [198/339], [94mLoss[0m : 1.92477
[1mStep[0m  [231/339], [94mLoss[0m : 2.66818
[1mStep[0m  [264/339], [94mLoss[0m : 2.76015
[1mStep[0m  [297/339], [94mLoss[0m : 3.10091
[1mStep[0m  [330/339], [94mLoss[0m : 2.30942

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35603
[1mStep[0m  [33/339], [94mLoss[0m : 2.49122
[1mStep[0m  [66/339], [94mLoss[0m : 2.49423
[1mStep[0m  [99/339], [94mLoss[0m : 2.36174
[1mStep[0m  [132/339], [94mLoss[0m : 2.63543
[1mStep[0m  [165/339], [94mLoss[0m : 2.76694
[1mStep[0m  [198/339], [94mLoss[0m : 2.45098
[1mStep[0m  [231/339], [94mLoss[0m : 2.10354
[1mStep[0m  [264/339], [94mLoss[0m : 3.20512
[1mStep[0m  [297/339], [94mLoss[0m : 2.21841
[1mStep[0m  [330/339], [94mLoss[0m : 2.72195

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.335, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52509
[1mStep[0m  [33/339], [94mLoss[0m : 2.09140
[1mStep[0m  [66/339], [94mLoss[0m : 2.66772
[1mStep[0m  [99/339], [94mLoss[0m : 2.35217
[1mStep[0m  [132/339], [94mLoss[0m : 2.68696
[1mStep[0m  [165/339], [94mLoss[0m : 2.25884
[1mStep[0m  [198/339], [94mLoss[0m : 2.01011
[1mStep[0m  [231/339], [94mLoss[0m : 3.11317
[1mStep[0m  [264/339], [94mLoss[0m : 2.24076
[1mStep[0m  [297/339], [94mLoss[0m : 2.23350
[1mStep[0m  [330/339], [94mLoss[0m : 2.01298

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87817
[1mStep[0m  [33/339], [94mLoss[0m : 2.26122
[1mStep[0m  [66/339], [94mLoss[0m : 2.31761
[1mStep[0m  [99/339], [94mLoss[0m : 3.68411
[1mStep[0m  [132/339], [94mLoss[0m : 2.19534
[1mStep[0m  [165/339], [94mLoss[0m : 3.01914
[1mStep[0m  [198/339], [94mLoss[0m : 2.59661
[1mStep[0m  [231/339], [94mLoss[0m : 2.25370
[1mStep[0m  [264/339], [94mLoss[0m : 2.26639
[1mStep[0m  [297/339], [94mLoss[0m : 2.99834
[1mStep[0m  [330/339], [94mLoss[0m : 1.88438

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45133
[1mStep[0m  [33/339], [94mLoss[0m : 2.82709
[1mStep[0m  [66/339], [94mLoss[0m : 2.67072
[1mStep[0m  [99/339], [94mLoss[0m : 2.60942
[1mStep[0m  [132/339], [94mLoss[0m : 2.39353
[1mStep[0m  [165/339], [94mLoss[0m : 2.17551
[1mStep[0m  [198/339], [94mLoss[0m : 2.43661
[1mStep[0m  [231/339], [94mLoss[0m : 1.82664
[1mStep[0m  [264/339], [94mLoss[0m : 2.40242
[1mStep[0m  [297/339], [94mLoss[0m : 2.34366
[1mStep[0m  [330/339], [94mLoss[0m : 3.01677

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25010
[1mStep[0m  [33/339], [94mLoss[0m : 2.52345
[1mStep[0m  [66/339], [94mLoss[0m : 2.40363
[1mStep[0m  [99/339], [94mLoss[0m : 2.09987
[1mStep[0m  [132/339], [94mLoss[0m : 2.74667
[1mStep[0m  [165/339], [94mLoss[0m : 2.17186
[1mStep[0m  [198/339], [94mLoss[0m : 2.80752
[1mStep[0m  [231/339], [94mLoss[0m : 2.90344
[1mStep[0m  [264/339], [94mLoss[0m : 3.13631
[1mStep[0m  [297/339], [94mLoss[0m : 1.89204
[1mStep[0m  [330/339], [94mLoss[0m : 2.57238

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.08233
[1mStep[0m  [33/339], [94mLoss[0m : 3.18518
[1mStep[0m  [66/339], [94mLoss[0m : 2.72852
[1mStep[0m  [99/339], [94mLoss[0m : 2.64331
[1mStep[0m  [132/339], [94mLoss[0m : 2.85460
[1mStep[0m  [165/339], [94mLoss[0m : 2.70295
[1mStep[0m  [198/339], [94mLoss[0m : 2.56062
[1mStep[0m  [231/339], [94mLoss[0m : 2.61693
[1mStep[0m  [264/339], [94mLoss[0m : 2.53825
[1mStep[0m  [297/339], [94mLoss[0m : 2.68369
[1mStep[0m  [330/339], [94mLoss[0m : 2.46301

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.19259
[1mStep[0m  [33/339], [94mLoss[0m : 2.81518
[1mStep[0m  [66/339], [94mLoss[0m : 2.94246
[1mStep[0m  [99/339], [94mLoss[0m : 2.83696
[1mStep[0m  [132/339], [94mLoss[0m : 2.84491
[1mStep[0m  [165/339], [94mLoss[0m : 2.64437
[1mStep[0m  [198/339], [94mLoss[0m : 2.05640
[1mStep[0m  [231/339], [94mLoss[0m : 1.84710
[1mStep[0m  [264/339], [94mLoss[0m : 2.74759
[1mStep[0m  [297/339], [94mLoss[0m : 2.97086
[1mStep[0m  [330/339], [94mLoss[0m : 2.85043

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.343, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77149
[1mStep[0m  [33/339], [94mLoss[0m : 2.56194
[1mStep[0m  [66/339], [94mLoss[0m : 2.23596
[1mStep[0m  [99/339], [94mLoss[0m : 1.92886
[1mStep[0m  [132/339], [94mLoss[0m : 2.64980
[1mStep[0m  [165/339], [94mLoss[0m : 1.76202
[1mStep[0m  [198/339], [94mLoss[0m : 3.69726
[1mStep[0m  [231/339], [94mLoss[0m : 2.83071
[1mStep[0m  [264/339], [94mLoss[0m : 2.34209
[1mStep[0m  [297/339], [94mLoss[0m : 2.37289
[1mStep[0m  [330/339], [94mLoss[0m : 1.92168

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29367
[1mStep[0m  [33/339], [94mLoss[0m : 2.23814
[1mStep[0m  [66/339], [94mLoss[0m : 2.15880
[1mStep[0m  [99/339], [94mLoss[0m : 3.21614
[1mStep[0m  [132/339], [94mLoss[0m : 2.56524
[1mStep[0m  [165/339], [94mLoss[0m : 2.08439
[1mStep[0m  [198/339], [94mLoss[0m : 2.29230
[1mStep[0m  [231/339], [94mLoss[0m : 3.19515
[1mStep[0m  [264/339], [94mLoss[0m : 2.62629
[1mStep[0m  [297/339], [94mLoss[0m : 2.81374
[1mStep[0m  [330/339], [94mLoss[0m : 1.87566

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97101
[1mStep[0m  [33/339], [94mLoss[0m : 2.66034
[1mStep[0m  [66/339], [94mLoss[0m : 1.90468
[1mStep[0m  [99/339], [94mLoss[0m : 2.64295
[1mStep[0m  [132/339], [94mLoss[0m : 2.67130
[1mStep[0m  [165/339], [94mLoss[0m : 2.59979
[1mStep[0m  [198/339], [94mLoss[0m : 2.85311
[1mStep[0m  [231/339], [94mLoss[0m : 2.72066
[1mStep[0m  [264/339], [94mLoss[0m : 2.60258
[1mStep[0m  [297/339], [94mLoss[0m : 2.66189
[1mStep[0m  [330/339], [94mLoss[0m : 2.90668

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77775
[1mStep[0m  [33/339], [94mLoss[0m : 2.99513
[1mStep[0m  [66/339], [94mLoss[0m : 2.84929
[1mStep[0m  [99/339], [94mLoss[0m : 1.97417
[1mStep[0m  [132/339], [94mLoss[0m : 2.59831
[1mStep[0m  [165/339], [94mLoss[0m : 2.26033
[1mStep[0m  [198/339], [94mLoss[0m : 2.39175
[1mStep[0m  [231/339], [94mLoss[0m : 2.50383
[1mStep[0m  [264/339], [94mLoss[0m : 2.23134
[1mStep[0m  [297/339], [94mLoss[0m : 2.70144
[1mStep[0m  [330/339], [94mLoss[0m : 3.23659

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42248
[1mStep[0m  [33/339], [94mLoss[0m : 2.32627
[1mStep[0m  [66/339], [94mLoss[0m : 2.10562
[1mStep[0m  [99/339], [94mLoss[0m : 2.78174
[1mStep[0m  [132/339], [94mLoss[0m : 2.11929
[1mStep[0m  [165/339], [94mLoss[0m : 2.44075
[1mStep[0m  [198/339], [94mLoss[0m : 2.34248
[1mStep[0m  [231/339], [94mLoss[0m : 2.65729
[1mStep[0m  [264/339], [94mLoss[0m : 2.66651
[1mStep[0m  [297/339], [94mLoss[0m : 2.25172
[1mStep[0m  [330/339], [94mLoss[0m : 1.82024

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35289
[1mStep[0m  [33/339], [94mLoss[0m : 3.04018
[1mStep[0m  [66/339], [94mLoss[0m : 2.43802
[1mStep[0m  [99/339], [94mLoss[0m : 3.12994
[1mStep[0m  [132/339], [94mLoss[0m : 2.20405
[1mStep[0m  [165/339], [94mLoss[0m : 2.74782
[1mStep[0m  [198/339], [94mLoss[0m : 2.32778
[1mStep[0m  [231/339], [94mLoss[0m : 2.69191
[1mStep[0m  [264/339], [94mLoss[0m : 2.48481
[1mStep[0m  [297/339], [94mLoss[0m : 2.26860
[1mStep[0m  [330/339], [94mLoss[0m : 2.40886

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62065
[1mStep[0m  [33/339], [94mLoss[0m : 2.59010
[1mStep[0m  [66/339], [94mLoss[0m : 2.36807
[1mStep[0m  [99/339], [94mLoss[0m : 2.41010
[1mStep[0m  [132/339], [94mLoss[0m : 2.28687
[1mStep[0m  [165/339], [94mLoss[0m : 2.60132
[1mStep[0m  [198/339], [94mLoss[0m : 2.94897
[1mStep[0m  [231/339], [94mLoss[0m : 1.84002
[1mStep[0m  [264/339], [94mLoss[0m : 2.04093
[1mStep[0m  [297/339], [94mLoss[0m : 2.62082
[1mStep[0m  [330/339], [94mLoss[0m : 2.35002

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53697
[1mStep[0m  [33/339], [94mLoss[0m : 2.16067
[1mStep[0m  [66/339], [94mLoss[0m : 2.66615
[1mStep[0m  [99/339], [94mLoss[0m : 2.84500
[1mStep[0m  [132/339], [94mLoss[0m : 2.44668
[1mStep[0m  [165/339], [94mLoss[0m : 3.04322
[1mStep[0m  [198/339], [94mLoss[0m : 2.90305
[1mStep[0m  [231/339], [94mLoss[0m : 2.87173
[1mStep[0m  [264/339], [94mLoss[0m : 2.93013
[1mStep[0m  [297/339], [94mLoss[0m : 2.02475
[1mStep[0m  [330/339], [94mLoss[0m : 2.69870

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.05879
[1mStep[0m  [33/339], [94mLoss[0m : 2.28208
[1mStep[0m  [66/339], [94mLoss[0m : 2.78699
[1mStep[0m  [99/339], [94mLoss[0m : 2.92602
[1mStep[0m  [132/339], [94mLoss[0m : 2.58786
[1mStep[0m  [165/339], [94mLoss[0m : 2.64276
[1mStep[0m  [198/339], [94mLoss[0m : 2.37923
[1mStep[0m  [231/339], [94mLoss[0m : 2.64767
[1mStep[0m  [264/339], [94mLoss[0m : 2.59928
[1mStep[0m  [297/339], [94mLoss[0m : 3.08771
[1mStep[0m  [330/339], [94mLoss[0m : 2.19264

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62453
[1mStep[0m  [33/339], [94mLoss[0m : 1.94521
[1mStep[0m  [66/339], [94mLoss[0m : 2.67232
[1mStep[0m  [99/339], [94mLoss[0m : 2.92990
[1mStep[0m  [132/339], [94mLoss[0m : 3.54148
[1mStep[0m  [165/339], [94mLoss[0m : 2.65415
[1mStep[0m  [198/339], [94mLoss[0m : 2.46397
[1mStep[0m  [231/339], [94mLoss[0m : 2.59547
[1mStep[0m  [264/339], [94mLoss[0m : 2.58814
[1mStep[0m  [297/339], [94mLoss[0m : 2.54807
[1mStep[0m  [330/339], [94mLoss[0m : 2.24194

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00779
[1mStep[0m  [33/339], [94mLoss[0m : 1.86888
[1mStep[0m  [66/339], [94mLoss[0m : 2.50103
[1mStep[0m  [99/339], [94mLoss[0m : 2.89877
[1mStep[0m  [132/339], [94mLoss[0m : 2.56924
[1mStep[0m  [165/339], [94mLoss[0m : 2.23691
[1mStep[0m  [198/339], [94mLoss[0m : 2.39464
[1mStep[0m  [231/339], [94mLoss[0m : 2.89081
[1mStep[0m  [264/339], [94mLoss[0m : 2.83276
[1mStep[0m  [297/339], [94mLoss[0m : 2.43707
[1mStep[0m  [330/339], [94mLoss[0m : 2.83408

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.349, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16041
[1mStep[0m  [33/339], [94mLoss[0m : 2.98027
[1mStep[0m  [66/339], [94mLoss[0m : 2.25241
[1mStep[0m  [99/339], [94mLoss[0m : 2.18153
[1mStep[0m  [132/339], [94mLoss[0m : 2.77118
[1mStep[0m  [165/339], [94mLoss[0m : 1.98345
[1mStep[0m  [198/339], [94mLoss[0m : 2.54513
[1mStep[0m  [231/339], [94mLoss[0m : 2.30912
[1mStep[0m  [264/339], [94mLoss[0m : 2.64110
[1mStep[0m  [297/339], [94mLoss[0m : 2.53780
[1mStep[0m  [330/339], [94mLoss[0m : 2.39016

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69934
[1mStep[0m  [33/339], [94mLoss[0m : 2.44975
[1mStep[0m  [66/339], [94mLoss[0m : 2.65581
[1mStep[0m  [99/339], [94mLoss[0m : 2.00490
[1mStep[0m  [132/339], [94mLoss[0m : 2.88581
[1mStep[0m  [165/339], [94mLoss[0m : 2.38697
[1mStep[0m  [198/339], [94mLoss[0m : 2.61887
[1mStep[0m  [231/339], [94mLoss[0m : 2.63455
[1mStep[0m  [264/339], [94mLoss[0m : 2.06798
[1mStep[0m  [297/339], [94mLoss[0m : 3.36544
[1mStep[0m  [330/339], [94mLoss[0m : 2.15999

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.366, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61624
[1mStep[0m  [33/339], [94mLoss[0m : 2.28662
[1mStep[0m  [66/339], [94mLoss[0m : 2.25187
[1mStep[0m  [99/339], [94mLoss[0m : 2.34601
[1mStep[0m  [132/339], [94mLoss[0m : 2.22475
[1mStep[0m  [165/339], [94mLoss[0m : 2.72506
[1mStep[0m  [198/339], [94mLoss[0m : 2.34599
[1mStep[0m  [231/339], [94mLoss[0m : 2.82955
[1mStep[0m  [264/339], [94mLoss[0m : 3.20490
[1mStep[0m  [297/339], [94mLoss[0m : 2.38695
[1mStep[0m  [330/339], [94mLoss[0m : 2.07604

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.367, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86617
[1mStep[0m  [33/339], [94mLoss[0m : 2.39876
[1mStep[0m  [66/339], [94mLoss[0m : 2.07345
[1mStep[0m  [99/339], [94mLoss[0m : 1.99755
[1mStep[0m  [132/339], [94mLoss[0m : 2.88855
[1mStep[0m  [165/339], [94mLoss[0m : 2.46905
[1mStep[0m  [198/339], [94mLoss[0m : 2.63249
[1mStep[0m  [231/339], [94mLoss[0m : 2.26688
[1mStep[0m  [264/339], [94mLoss[0m : 3.11998
[1mStep[0m  [297/339], [94mLoss[0m : 2.70778
[1mStep[0m  [330/339], [94mLoss[0m : 2.38999

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.55365
[1mStep[0m  [33/339], [94mLoss[0m : 2.61886
[1mStep[0m  [66/339], [94mLoss[0m : 2.29062
[1mStep[0m  [99/339], [94mLoss[0m : 2.56484
[1mStep[0m  [132/339], [94mLoss[0m : 2.96045
[1mStep[0m  [165/339], [94mLoss[0m : 2.37294
[1mStep[0m  [198/339], [94mLoss[0m : 2.94231
[1mStep[0m  [231/339], [94mLoss[0m : 2.39471
[1mStep[0m  [264/339], [94mLoss[0m : 3.23354
[1mStep[0m  [297/339], [94mLoss[0m : 2.52304
[1mStep[0m  [330/339], [94mLoss[0m : 2.60428

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.94551
[1mStep[0m  [33/339], [94mLoss[0m : 2.40160
[1mStep[0m  [66/339], [94mLoss[0m : 2.31882
[1mStep[0m  [99/339], [94mLoss[0m : 2.60997
[1mStep[0m  [132/339], [94mLoss[0m : 2.83600
[1mStep[0m  [165/339], [94mLoss[0m : 2.52266
[1mStep[0m  [198/339], [94mLoss[0m : 2.49989
[1mStep[0m  [231/339], [94mLoss[0m : 2.72924
[1mStep[0m  [264/339], [94mLoss[0m : 2.02799
[1mStep[0m  [297/339], [94mLoss[0m : 2.17802
[1mStep[0m  [330/339], [94mLoss[0m : 2.69122

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93486
[1mStep[0m  [33/339], [94mLoss[0m : 2.70213
[1mStep[0m  [66/339], [94mLoss[0m : 2.53582
[1mStep[0m  [99/339], [94mLoss[0m : 3.04897
[1mStep[0m  [132/339], [94mLoss[0m : 2.74590
[1mStep[0m  [165/339], [94mLoss[0m : 2.70425
[1mStep[0m  [198/339], [94mLoss[0m : 2.58330
[1mStep[0m  [231/339], [94mLoss[0m : 3.31894
[1mStep[0m  [264/339], [94mLoss[0m : 2.23727
[1mStep[0m  [297/339], [94mLoss[0m : 2.04579
[1mStep[0m  [330/339], [94mLoss[0m : 2.64055

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.359, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.367
====================================

Phase 1 - Evaluation MAE:  2.36715358020985
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.97067
[1mStep[0m  [33/339], [94mLoss[0m : 3.44934
[1mStep[0m  [66/339], [94mLoss[0m : 2.67763
[1mStep[0m  [99/339], [94mLoss[0m : 2.69505
[1mStep[0m  [132/339], [94mLoss[0m : 2.02569
[1mStep[0m  [165/339], [94mLoss[0m : 2.39444
[1mStep[0m  [198/339], [94mLoss[0m : 2.89984
[1mStep[0m  [231/339], [94mLoss[0m : 2.48578
[1mStep[0m  [264/339], [94mLoss[0m : 2.22650
[1mStep[0m  [297/339], [94mLoss[0m : 2.82701
[1mStep[0m  [330/339], [94mLoss[0m : 2.71379

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05599
[1mStep[0m  [33/339], [94mLoss[0m : 2.77278
[1mStep[0m  [66/339], [94mLoss[0m : 2.28156
[1mStep[0m  [99/339], [94mLoss[0m : 2.07815
[1mStep[0m  [132/339], [94mLoss[0m : 2.99473
[1mStep[0m  [165/339], [94mLoss[0m : 2.16674
[1mStep[0m  [198/339], [94mLoss[0m : 2.74985
[1mStep[0m  [231/339], [94mLoss[0m : 2.76921
[1mStep[0m  [264/339], [94mLoss[0m : 2.48311
[1mStep[0m  [297/339], [94mLoss[0m : 1.78493
[1mStep[0m  [330/339], [94mLoss[0m : 2.39632

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.414, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14207
[1mStep[0m  [33/339], [94mLoss[0m : 2.53949
[1mStep[0m  [66/339], [94mLoss[0m : 2.50978
[1mStep[0m  [99/339], [94mLoss[0m : 2.25687
[1mStep[0m  [132/339], [94mLoss[0m : 2.34742
[1mStep[0m  [165/339], [94mLoss[0m : 2.91493
[1mStep[0m  [198/339], [94mLoss[0m : 2.61597
[1mStep[0m  [231/339], [94mLoss[0m : 2.05804
[1mStep[0m  [264/339], [94mLoss[0m : 2.89298
[1mStep[0m  [297/339], [94mLoss[0m : 1.75365
[1mStep[0m  [330/339], [94mLoss[0m : 2.38043

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09033
[1mStep[0m  [33/339], [94mLoss[0m : 2.26544
[1mStep[0m  [66/339], [94mLoss[0m : 2.68005
[1mStep[0m  [99/339], [94mLoss[0m : 2.06880
[1mStep[0m  [132/339], [94mLoss[0m : 3.22700
[1mStep[0m  [165/339], [94mLoss[0m : 2.41162
[1mStep[0m  [198/339], [94mLoss[0m : 2.09168
[1mStep[0m  [231/339], [94mLoss[0m : 2.13552
[1mStep[0m  [264/339], [94mLoss[0m : 2.22833
[1mStep[0m  [297/339], [94mLoss[0m : 1.74546
[1mStep[0m  [330/339], [94mLoss[0m : 2.06441

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20794
[1mStep[0m  [33/339], [94mLoss[0m : 1.80079
[1mStep[0m  [66/339], [94mLoss[0m : 2.20704
[1mStep[0m  [99/339], [94mLoss[0m : 2.35327
[1mStep[0m  [132/339], [94mLoss[0m : 3.01816
[1mStep[0m  [165/339], [94mLoss[0m : 2.25865
[1mStep[0m  [198/339], [94mLoss[0m : 3.22247
[1mStep[0m  [231/339], [94mLoss[0m : 2.18175
[1mStep[0m  [264/339], [94mLoss[0m : 2.36125
[1mStep[0m  [297/339], [94mLoss[0m : 2.36104
[1mStep[0m  [330/339], [94mLoss[0m : 2.31462

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98053
[1mStep[0m  [33/339], [94mLoss[0m : 1.86314
[1mStep[0m  [66/339], [94mLoss[0m : 1.71402
[1mStep[0m  [99/339], [94mLoss[0m : 1.94835
[1mStep[0m  [132/339], [94mLoss[0m : 1.95606
[1mStep[0m  [165/339], [94mLoss[0m : 2.29803
[1mStep[0m  [198/339], [94mLoss[0m : 3.09008
[1mStep[0m  [231/339], [94mLoss[0m : 1.76553
[1mStep[0m  [264/339], [94mLoss[0m : 1.80659
[1mStep[0m  [297/339], [94mLoss[0m : 2.17917
[1mStep[0m  [330/339], [94mLoss[0m : 2.03994

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.71940
[1mStep[0m  [33/339], [94mLoss[0m : 2.94377
[1mStep[0m  [66/339], [94mLoss[0m : 1.82379
[1mStep[0m  [99/339], [94mLoss[0m : 1.34248
[1mStep[0m  [132/339], [94mLoss[0m : 1.88154
[1mStep[0m  [165/339], [94mLoss[0m : 2.68319
[1mStep[0m  [198/339], [94mLoss[0m : 1.76656
[1mStep[0m  [231/339], [94mLoss[0m : 2.09151
[1mStep[0m  [264/339], [94mLoss[0m : 2.01079
[1mStep[0m  [297/339], [94mLoss[0m : 2.62174
[1mStep[0m  [330/339], [94mLoss[0m : 2.20403

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.388, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57112
[1mStep[0m  [33/339], [94mLoss[0m : 2.53584
[1mStep[0m  [66/339], [94mLoss[0m : 2.22174
[1mStep[0m  [99/339], [94mLoss[0m : 1.99084
[1mStep[0m  [132/339], [94mLoss[0m : 1.81621
[1mStep[0m  [165/339], [94mLoss[0m : 2.03863
[1mStep[0m  [198/339], [94mLoss[0m : 2.71546
[1mStep[0m  [231/339], [94mLoss[0m : 2.77978
[1mStep[0m  [264/339], [94mLoss[0m : 2.50039
[1mStep[0m  [297/339], [94mLoss[0m : 2.69362
[1mStep[0m  [330/339], [94mLoss[0m : 2.85392

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09388
[1mStep[0m  [33/339], [94mLoss[0m : 2.15533
[1mStep[0m  [66/339], [94mLoss[0m : 1.86052
[1mStep[0m  [99/339], [94mLoss[0m : 1.82445
[1mStep[0m  [132/339], [94mLoss[0m : 2.22799
[1mStep[0m  [165/339], [94mLoss[0m : 1.75139
[1mStep[0m  [198/339], [94mLoss[0m : 1.78067
[1mStep[0m  [231/339], [94mLoss[0m : 2.78781
[1mStep[0m  [264/339], [94mLoss[0m : 2.02133
[1mStep[0m  [297/339], [94mLoss[0m : 2.13553
[1mStep[0m  [330/339], [94mLoss[0m : 3.28306

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.431, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79260
[1mStep[0m  [33/339], [94mLoss[0m : 1.56224
[1mStep[0m  [66/339], [94mLoss[0m : 1.67929
[1mStep[0m  [99/339], [94mLoss[0m : 2.80610
[1mStep[0m  [132/339], [94mLoss[0m : 2.62152
[1mStep[0m  [165/339], [94mLoss[0m : 2.23130
[1mStep[0m  [198/339], [94mLoss[0m : 2.35823
[1mStep[0m  [231/339], [94mLoss[0m : 1.60578
[1mStep[0m  [264/339], [94mLoss[0m : 1.90593
[1mStep[0m  [297/339], [94mLoss[0m : 1.89921
[1mStep[0m  [330/339], [94mLoss[0m : 2.70196

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.451, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64165
[1mStep[0m  [33/339], [94mLoss[0m : 2.43866
[1mStep[0m  [66/339], [94mLoss[0m : 1.96772
[1mStep[0m  [99/339], [94mLoss[0m : 1.74931
[1mStep[0m  [132/339], [94mLoss[0m : 1.77884
[1mStep[0m  [165/339], [94mLoss[0m : 2.13601
[1mStep[0m  [198/339], [94mLoss[0m : 2.09975
[1mStep[0m  [231/339], [94mLoss[0m : 2.23293
[1mStep[0m  [264/339], [94mLoss[0m : 2.74337
[1mStep[0m  [297/339], [94mLoss[0m : 2.28921
[1mStep[0m  [330/339], [94mLoss[0m : 2.52831

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39854
[1mStep[0m  [33/339], [94mLoss[0m : 2.05450
[1mStep[0m  [66/339], [94mLoss[0m : 2.07285
[1mStep[0m  [99/339], [94mLoss[0m : 2.70252
[1mStep[0m  [132/339], [94mLoss[0m : 1.63771
[1mStep[0m  [165/339], [94mLoss[0m : 2.85737
[1mStep[0m  [198/339], [94mLoss[0m : 1.50760
[1mStep[0m  [231/339], [94mLoss[0m : 1.90394
[1mStep[0m  [264/339], [94mLoss[0m : 2.19869
[1mStep[0m  [297/339], [94mLoss[0m : 2.16820
[1mStep[0m  [330/339], [94mLoss[0m : 1.94355

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.146, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.49608
[1mStep[0m  [33/339], [94mLoss[0m : 2.13814
[1mStep[0m  [66/339], [94mLoss[0m : 2.07657
[1mStep[0m  [99/339], [94mLoss[0m : 1.75606
[1mStep[0m  [132/339], [94mLoss[0m : 1.98531
[1mStep[0m  [165/339], [94mLoss[0m : 2.08030
[1mStep[0m  [198/339], [94mLoss[0m : 2.15737
[1mStep[0m  [231/339], [94mLoss[0m : 1.44283
[1mStep[0m  [264/339], [94mLoss[0m : 2.11215
[1mStep[0m  [297/339], [94mLoss[0m : 2.24583
[1mStep[0m  [330/339], [94mLoss[0m : 2.08627

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.412, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10662
[1mStep[0m  [33/339], [94mLoss[0m : 2.57272
[1mStep[0m  [66/339], [94mLoss[0m : 2.15425
[1mStep[0m  [99/339], [94mLoss[0m : 2.08060
[1mStep[0m  [132/339], [94mLoss[0m : 1.38810
[1mStep[0m  [165/339], [94mLoss[0m : 1.97841
[1mStep[0m  [198/339], [94mLoss[0m : 1.93400
[1mStep[0m  [231/339], [94mLoss[0m : 1.84716
[1mStep[0m  [264/339], [94mLoss[0m : 1.93237
[1mStep[0m  [297/339], [94mLoss[0m : 1.70024
[1mStep[0m  [330/339], [94mLoss[0m : 2.12644

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09381
[1mStep[0m  [33/339], [94mLoss[0m : 1.68074
[1mStep[0m  [66/339], [94mLoss[0m : 1.84613
[1mStep[0m  [99/339], [94mLoss[0m : 2.31552
[1mStep[0m  [132/339], [94mLoss[0m : 1.77948
[1mStep[0m  [165/339], [94mLoss[0m : 2.81684
[1mStep[0m  [198/339], [94mLoss[0m : 1.92630
[1mStep[0m  [231/339], [94mLoss[0m : 2.10524
[1mStep[0m  [264/339], [94mLoss[0m : 2.24759
[1mStep[0m  [297/339], [94mLoss[0m : 2.32656
[1mStep[0m  [330/339], [94mLoss[0m : 1.92913

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.097, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.27390
[1mStep[0m  [33/339], [94mLoss[0m : 2.91154
[1mStep[0m  [66/339], [94mLoss[0m : 2.02033
[1mStep[0m  [99/339], [94mLoss[0m : 1.75273
[1mStep[0m  [132/339], [94mLoss[0m : 2.22521
[1mStep[0m  [165/339], [94mLoss[0m : 1.67337
[1mStep[0m  [198/339], [94mLoss[0m : 1.81274
[1mStep[0m  [231/339], [94mLoss[0m : 2.69968
[1mStep[0m  [264/339], [94mLoss[0m : 2.17708
[1mStep[0m  [297/339], [94mLoss[0m : 1.93049
[1mStep[0m  [330/339], [94mLoss[0m : 2.29811

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.48059
[1mStep[0m  [33/339], [94mLoss[0m : 1.56946
[1mStep[0m  [66/339], [94mLoss[0m : 2.28688
[1mStep[0m  [99/339], [94mLoss[0m : 1.85558
[1mStep[0m  [132/339], [94mLoss[0m : 2.21112
[1mStep[0m  [165/339], [94mLoss[0m : 1.81241
[1mStep[0m  [198/339], [94mLoss[0m : 2.49912
[1mStep[0m  [231/339], [94mLoss[0m : 1.91306
[1mStep[0m  [264/339], [94mLoss[0m : 2.33863
[1mStep[0m  [297/339], [94mLoss[0m : 1.94661
[1mStep[0m  [330/339], [94mLoss[0m : 2.38758

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.073, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11062
[1mStep[0m  [33/339], [94mLoss[0m : 1.54101
[1mStep[0m  [66/339], [94mLoss[0m : 1.85595
[1mStep[0m  [99/339], [94mLoss[0m : 1.78014
[1mStep[0m  [132/339], [94mLoss[0m : 2.06457
[1mStep[0m  [165/339], [94mLoss[0m : 2.32518
[1mStep[0m  [198/339], [94mLoss[0m : 1.39615
[1mStep[0m  [231/339], [94mLoss[0m : 1.85533
[1mStep[0m  [264/339], [94mLoss[0m : 1.68122
[1mStep[0m  [297/339], [94mLoss[0m : 2.39990
[1mStep[0m  [330/339], [94mLoss[0m : 2.36900

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79959
[1mStep[0m  [33/339], [94mLoss[0m : 2.00803
[1mStep[0m  [66/339], [94mLoss[0m : 1.78551
[1mStep[0m  [99/339], [94mLoss[0m : 2.33308
[1mStep[0m  [132/339], [94mLoss[0m : 1.23366
[1mStep[0m  [165/339], [94mLoss[0m : 2.22114
[1mStep[0m  [198/339], [94mLoss[0m : 1.34050
[1mStep[0m  [231/339], [94mLoss[0m : 1.51643
[1mStep[0m  [264/339], [94mLoss[0m : 2.50742
[1mStep[0m  [297/339], [94mLoss[0m : 1.80111
[1mStep[0m  [330/339], [94mLoss[0m : 2.07570

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.052, [92mTest[0m: 2.406, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93417
[1mStep[0m  [33/339], [94mLoss[0m : 2.52181
[1mStep[0m  [66/339], [94mLoss[0m : 1.79904
[1mStep[0m  [99/339], [94mLoss[0m : 1.46229
[1mStep[0m  [132/339], [94mLoss[0m : 2.12425
[1mStep[0m  [165/339], [94mLoss[0m : 2.47965
[1mStep[0m  [198/339], [94mLoss[0m : 2.54996
[1mStep[0m  [231/339], [94mLoss[0m : 2.67588
[1mStep[0m  [264/339], [94mLoss[0m : 2.06737
[1mStep[0m  [297/339], [94mLoss[0m : 2.09437
[1mStep[0m  [330/339], [94mLoss[0m : 1.92634

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.452, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70295
[1mStep[0m  [33/339], [94mLoss[0m : 1.64410
[1mStep[0m  [66/339], [94mLoss[0m : 1.82290
[1mStep[0m  [99/339], [94mLoss[0m : 2.25183
[1mStep[0m  [132/339], [94mLoss[0m : 2.11705
[1mStep[0m  [165/339], [94mLoss[0m : 2.09222
[1mStep[0m  [198/339], [94mLoss[0m : 1.97207
[1mStep[0m  [231/339], [94mLoss[0m : 2.13609
[1mStep[0m  [264/339], [94mLoss[0m : 2.62981
[1mStep[0m  [297/339], [94mLoss[0m : 1.89453
[1mStep[0m  [330/339], [94mLoss[0m : 2.17394

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.423, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73609
[1mStep[0m  [33/339], [94mLoss[0m : 2.13269
[1mStep[0m  [66/339], [94mLoss[0m : 2.00644
[1mStep[0m  [99/339], [94mLoss[0m : 1.97805
[1mStep[0m  [132/339], [94mLoss[0m : 2.12235
[1mStep[0m  [165/339], [94mLoss[0m : 1.69696
[1mStep[0m  [198/339], [94mLoss[0m : 1.71807
[1mStep[0m  [231/339], [94mLoss[0m : 1.95826
[1mStep[0m  [264/339], [94mLoss[0m : 1.98691
[1mStep[0m  [297/339], [94mLoss[0m : 1.85863
[1mStep[0m  [330/339], [94mLoss[0m : 2.14844

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73712
[1mStep[0m  [33/339], [94mLoss[0m : 2.36161
[1mStep[0m  [66/339], [94mLoss[0m : 2.40813
[1mStep[0m  [99/339], [94mLoss[0m : 2.20206
[1mStep[0m  [132/339], [94mLoss[0m : 2.09668
[1mStep[0m  [165/339], [94mLoss[0m : 2.11086
[1mStep[0m  [198/339], [94mLoss[0m : 2.12488
[1mStep[0m  [231/339], [94mLoss[0m : 1.52810
[1mStep[0m  [264/339], [94mLoss[0m : 1.84869
[1mStep[0m  [297/339], [94mLoss[0m : 1.95749
[1mStep[0m  [330/339], [94mLoss[0m : 2.23681

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.447, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88307
[1mStep[0m  [33/339], [94mLoss[0m : 1.72189
[1mStep[0m  [66/339], [94mLoss[0m : 2.28182
[1mStep[0m  [99/339], [94mLoss[0m : 2.07038
[1mStep[0m  [132/339], [94mLoss[0m : 1.50343
[1mStep[0m  [165/339], [94mLoss[0m : 1.62050
[1mStep[0m  [198/339], [94mLoss[0m : 2.70738
[1mStep[0m  [231/339], [94mLoss[0m : 1.86309
[1mStep[0m  [264/339], [94mLoss[0m : 1.90058
[1mStep[0m  [297/339], [94mLoss[0m : 2.43384
[1mStep[0m  [330/339], [94mLoss[0m : 2.11113

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.995, [92mTest[0m: 2.442, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29489
[1mStep[0m  [33/339], [94mLoss[0m : 1.92284
[1mStep[0m  [66/339], [94mLoss[0m : 2.36316
[1mStep[0m  [99/339], [94mLoss[0m : 1.93685
[1mStep[0m  [132/339], [94mLoss[0m : 2.04803
[1mStep[0m  [165/339], [94mLoss[0m : 2.18487
[1mStep[0m  [198/339], [94mLoss[0m : 1.93660
[1mStep[0m  [231/339], [94mLoss[0m : 2.15288
[1mStep[0m  [264/339], [94mLoss[0m : 2.20747
[1mStep[0m  [297/339], [94mLoss[0m : 2.32157
[1mStep[0m  [330/339], [94mLoss[0m : 1.84054

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.471, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67833
[1mStep[0m  [33/339], [94mLoss[0m : 1.66696
[1mStep[0m  [66/339], [94mLoss[0m : 1.73199
[1mStep[0m  [99/339], [94mLoss[0m : 2.24037
[1mStep[0m  [132/339], [94mLoss[0m : 1.96611
[1mStep[0m  [165/339], [94mLoss[0m : 2.00920
[1mStep[0m  [198/339], [94mLoss[0m : 2.11704
[1mStep[0m  [231/339], [94mLoss[0m : 2.27023
[1mStep[0m  [264/339], [94mLoss[0m : 2.29995
[1mStep[0m  [297/339], [94mLoss[0m : 1.74907
[1mStep[0m  [330/339], [94mLoss[0m : 1.89412

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.981, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86004
[1mStep[0m  [33/339], [94mLoss[0m : 2.13494
[1mStep[0m  [66/339], [94mLoss[0m : 1.59260
[1mStep[0m  [99/339], [94mLoss[0m : 1.64210
[1mStep[0m  [132/339], [94mLoss[0m : 1.88692
[1mStep[0m  [165/339], [94mLoss[0m : 1.45042
[1mStep[0m  [198/339], [94mLoss[0m : 1.76950
[1mStep[0m  [231/339], [94mLoss[0m : 1.70548
[1mStep[0m  [264/339], [94mLoss[0m : 1.76570
[1mStep[0m  [297/339], [94mLoss[0m : 1.68981
[1mStep[0m  [330/339], [94mLoss[0m : 1.92932

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.967, [92mTest[0m: 2.456, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04510
[1mStep[0m  [33/339], [94mLoss[0m : 1.56808
[1mStep[0m  [66/339], [94mLoss[0m : 1.87276
[1mStep[0m  [99/339], [94mLoss[0m : 1.77077
[1mStep[0m  [132/339], [94mLoss[0m : 1.98316
[1mStep[0m  [165/339], [94mLoss[0m : 2.42042
[1mStep[0m  [198/339], [94mLoss[0m : 1.82996
[1mStep[0m  [231/339], [94mLoss[0m : 2.06479
[1mStep[0m  [264/339], [94mLoss[0m : 1.73085
[1mStep[0m  [297/339], [94mLoss[0m : 1.67670
[1mStep[0m  [330/339], [94mLoss[0m : 2.18392

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.429, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80306
[1mStep[0m  [33/339], [94mLoss[0m : 1.79790
[1mStep[0m  [66/339], [94mLoss[0m : 1.82960
[1mStep[0m  [99/339], [94mLoss[0m : 2.31826
[1mStep[0m  [132/339], [94mLoss[0m : 2.23931
[1mStep[0m  [165/339], [94mLoss[0m : 2.42840
[1mStep[0m  [198/339], [94mLoss[0m : 1.92282
[1mStep[0m  [231/339], [94mLoss[0m : 2.06307
[1mStep[0m  [264/339], [94mLoss[0m : 1.85420
[1mStep[0m  [297/339], [94mLoss[0m : 1.99569
[1mStep[0m  [330/339], [94mLoss[0m : 1.83076

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.485, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79023
[1mStep[0m  [33/339], [94mLoss[0m : 2.02651
[1mStep[0m  [66/339], [94mLoss[0m : 1.29643
[1mStep[0m  [99/339], [94mLoss[0m : 2.14504
[1mStep[0m  [132/339], [94mLoss[0m : 2.46543
[1mStep[0m  [165/339], [94mLoss[0m : 2.00364
[1mStep[0m  [198/339], [94mLoss[0m : 2.36326
[1mStep[0m  [231/339], [94mLoss[0m : 1.87658
[1mStep[0m  [264/339], [94mLoss[0m : 1.95024
[1mStep[0m  [297/339], [94mLoss[0m : 1.61091
[1mStep[0m  [330/339], [94mLoss[0m : 1.52484

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.479, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.454
====================================

Phase 2 - Evaluation MAE:  2.4540676796330816
MAE score P1        2.367154
MAE score P2        2.454068
loss                1.931875
learning_rate           0.01
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay            0.01
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.82825
[1mStep[0m  [16/169], [94mLoss[0m : 2.85008
[1mStep[0m  [32/169], [94mLoss[0m : 2.67756
[1mStep[0m  [48/169], [94mLoss[0m : 2.49482
[1mStep[0m  [64/169], [94mLoss[0m : 2.42448
[1mStep[0m  [80/169], [94mLoss[0m : 3.01230
[1mStep[0m  [96/169], [94mLoss[0m : 2.55891
[1mStep[0m  [112/169], [94mLoss[0m : 2.11561
[1mStep[0m  [128/169], [94mLoss[0m : 2.55240
[1mStep[0m  [144/169], [94mLoss[0m : 2.43361
[1mStep[0m  [160/169], [94mLoss[0m : 2.66427

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.745, [92mTest[0m: 10.992, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74789
[1mStep[0m  [16/169], [94mLoss[0m : 2.43471
[1mStep[0m  [32/169], [94mLoss[0m : 2.39043
[1mStep[0m  [48/169], [94mLoss[0m : 2.49838
[1mStep[0m  [64/169], [94mLoss[0m : 2.41040
[1mStep[0m  [80/169], [94mLoss[0m : 2.26994
[1mStep[0m  [96/169], [94mLoss[0m : 2.90660
[1mStep[0m  [112/169], [94mLoss[0m : 2.83389
[1mStep[0m  [128/169], [94mLoss[0m : 2.75404
[1mStep[0m  [144/169], [94mLoss[0m : 2.90148
[1mStep[0m  [160/169], [94mLoss[0m : 2.25425

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.377, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25654
[1mStep[0m  [16/169], [94mLoss[0m : 2.12005
[1mStep[0m  [32/169], [94mLoss[0m : 2.65738
[1mStep[0m  [48/169], [94mLoss[0m : 2.64800
[1mStep[0m  [64/169], [94mLoss[0m : 1.97016
[1mStep[0m  [80/169], [94mLoss[0m : 2.19201
[1mStep[0m  [96/169], [94mLoss[0m : 2.20032
[1mStep[0m  [112/169], [94mLoss[0m : 2.31540
[1mStep[0m  [128/169], [94mLoss[0m : 2.47599
[1mStep[0m  [144/169], [94mLoss[0m : 2.48103
[1mStep[0m  [160/169], [94mLoss[0m : 2.64366

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60851
[1mStep[0m  [16/169], [94mLoss[0m : 2.21678
[1mStep[0m  [32/169], [94mLoss[0m : 2.58056
[1mStep[0m  [48/169], [94mLoss[0m : 2.45091
[1mStep[0m  [64/169], [94mLoss[0m : 2.10873
[1mStep[0m  [80/169], [94mLoss[0m : 2.25297
[1mStep[0m  [96/169], [94mLoss[0m : 2.58424
[1mStep[0m  [112/169], [94mLoss[0m : 2.33663
[1mStep[0m  [128/169], [94mLoss[0m : 2.22525
[1mStep[0m  [144/169], [94mLoss[0m : 2.40408
[1mStep[0m  [160/169], [94mLoss[0m : 2.66751

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20269
[1mStep[0m  [16/169], [94mLoss[0m : 2.37054
[1mStep[0m  [32/169], [94mLoss[0m : 2.23438
[1mStep[0m  [48/169], [94mLoss[0m : 2.33006
[1mStep[0m  [64/169], [94mLoss[0m : 2.28628
[1mStep[0m  [80/169], [94mLoss[0m : 2.87122
[1mStep[0m  [96/169], [94mLoss[0m : 2.33151
[1mStep[0m  [112/169], [94mLoss[0m : 2.62584
[1mStep[0m  [128/169], [94mLoss[0m : 2.59761
[1mStep[0m  [144/169], [94mLoss[0m : 2.31048
[1mStep[0m  [160/169], [94mLoss[0m : 2.34051

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11865
[1mStep[0m  [16/169], [94mLoss[0m : 2.49321
[1mStep[0m  [32/169], [94mLoss[0m : 2.19071
[1mStep[0m  [48/169], [94mLoss[0m : 2.48179
[1mStep[0m  [64/169], [94mLoss[0m : 2.15819
[1mStep[0m  [80/169], [94mLoss[0m : 2.52392
[1mStep[0m  [96/169], [94mLoss[0m : 3.15793
[1mStep[0m  [112/169], [94mLoss[0m : 2.35155
[1mStep[0m  [128/169], [94mLoss[0m : 2.54696
[1mStep[0m  [144/169], [94mLoss[0m : 2.37740
[1mStep[0m  [160/169], [94mLoss[0m : 2.37405

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.325, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73472
[1mStep[0m  [16/169], [94mLoss[0m : 2.45197
[1mStep[0m  [32/169], [94mLoss[0m : 2.50954
[1mStep[0m  [48/169], [94mLoss[0m : 2.39267
[1mStep[0m  [64/169], [94mLoss[0m : 2.53945
[1mStep[0m  [80/169], [94mLoss[0m : 2.68712
[1mStep[0m  [96/169], [94mLoss[0m : 2.02584
[1mStep[0m  [112/169], [94mLoss[0m : 2.41839
[1mStep[0m  [128/169], [94mLoss[0m : 2.21432
[1mStep[0m  [144/169], [94mLoss[0m : 2.45425
[1mStep[0m  [160/169], [94mLoss[0m : 2.34826

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.324, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21740
[1mStep[0m  [16/169], [94mLoss[0m : 2.29577
[1mStep[0m  [32/169], [94mLoss[0m : 2.88751
[1mStep[0m  [48/169], [94mLoss[0m : 2.36433
[1mStep[0m  [64/169], [94mLoss[0m : 2.22495
[1mStep[0m  [80/169], [94mLoss[0m : 1.95204
[1mStep[0m  [96/169], [94mLoss[0m : 2.23332
[1mStep[0m  [112/169], [94mLoss[0m : 2.67951
[1mStep[0m  [128/169], [94mLoss[0m : 2.12504
[1mStep[0m  [144/169], [94mLoss[0m : 2.57585
[1mStep[0m  [160/169], [94mLoss[0m : 2.63724

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64879
[1mStep[0m  [16/169], [94mLoss[0m : 2.26774
[1mStep[0m  [32/169], [94mLoss[0m : 2.69222
[1mStep[0m  [48/169], [94mLoss[0m : 2.43549
[1mStep[0m  [64/169], [94mLoss[0m : 2.32742
[1mStep[0m  [80/169], [94mLoss[0m : 2.43408
[1mStep[0m  [96/169], [94mLoss[0m : 2.15878
[1mStep[0m  [112/169], [94mLoss[0m : 2.29473
[1mStep[0m  [128/169], [94mLoss[0m : 2.69655
[1mStep[0m  [144/169], [94mLoss[0m : 2.25478
[1mStep[0m  [160/169], [94mLoss[0m : 2.46163

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29308
[1mStep[0m  [16/169], [94mLoss[0m : 2.37661
[1mStep[0m  [32/169], [94mLoss[0m : 2.88613
[1mStep[0m  [48/169], [94mLoss[0m : 2.27896
[1mStep[0m  [64/169], [94mLoss[0m : 2.43845
[1mStep[0m  [80/169], [94mLoss[0m : 2.48244
[1mStep[0m  [96/169], [94mLoss[0m : 2.53973
[1mStep[0m  [112/169], [94mLoss[0m : 2.67852
[1mStep[0m  [128/169], [94mLoss[0m : 2.79867
[1mStep[0m  [144/169], [94mLoss[0m : 2.43916
[1mStep[0m  [160/169], [94mLoss[0m : 2.49818

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31297
[1mStep[0m  [16/169], [94mLoss[0m : 2.40945
[1mStep[0m  [32/169], [94mLoss[0m : 2.33588
[1mStep[0m  [48/169], [94mLoss[0m : 2.46572
[1mStep[0m  [64/169], [94mLoss[0m : 2.82516
[1mStep[0m  [80/169], [94mLoss[0m : 2.90163
[1mStep[0m  [96/169], [94mLoss[0m : 2.58011
[1mStep[0m  [112/169], [94mLoss[0m : 2.07738
[1mStep[0m  [128/169], [94mLoss[0m : 2.41826
[1mStep[0m  [144/169], [94mLoss[0m : 2.29652
[1mStep[0m  [160/169], [94mLoss[0m : 1.96897

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75603
[1mStep[0m  [16/169], [94mLoss[0m : 2.27223
[1mStep[0m  [32/169], [94mLoss[0m : 2.94316
[1mStep[0m  [48/169], [94mLoss[0m : 2.29902
[1mStep[0m  [64/169], [94mLoss[0m : 2.19390
[1mStep[0m  [80/169], [94mLoss[0m : 2.44507
[1mStep[0m  [96/169], [94mLoss[0m : 2.49931
[1mStep[0m  [112/169], [94mLoss[0m : 2.85471
[1mStep[0m  [128/169], [94mLoss[0m : 2.40929
[1mStep[0m  [144/169], [94mLoss[0m : 2.84269
[1mStep[0m  [160/169], [94mLoss[0m : 2.36345

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.326, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49132
[1mStep[0m  [16/169], [94mLoss[0m : 2.09434
[1mStep[0m  [32/169], [94mLoss[0m : 2.18417
[1mStep[0m  [48/169], [94mLoss[0m : 2.53715
[1mStep[0m  [64/169], [94mLoss[0m : 2.29641
[1mStep[0m  [80/169], [94mLoss[0m : 2.71058
[1mStep[0m  [96/169], [94mLoss[0m : 2.35264
[1mStep[0m  [112/169], [94mLoss[0m : 2.35973
[1mStep[0m  [128/169], [94mLoss[0m : 2.39430
[1mStep[0m  [144/169], [94mLoss[0m : 2.63257
[1mStep[0m  [160/169], [94mLoss[0m : 2.69398

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.321, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57396
[1mStep[0m  [16/169], [94mLoss[0m : 2.55171
[1mStep[0m  [32/169], [94mLoss[0m : 2.47908
[1mStep[0m  [48/169], [94mLoss[0m : 2.58537
[1mStep[0m  [64/169], [94mLoss[0m : 2.34424
[1mStep[0m  [80/169], [94mLoss[0m : 2.05566
[1mStep[0m  [96/169], [94mLoss[0m : 2.53364
[1mStep[0m  [112/169], [94mLoss[0m : 2.71285
[1mStep[0m  [128/169], [94mLoss[0m : 1.88865
[1mStep[0m  [144/169], [94mLoss[0m : 2.33433
[1mStep[0m  [160/169], [94mLoss[0m : 2.52689

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58225
[1mStep[0m  [16/169], [94mLoss[0m : 2.36803
[1mStep[0m  [32/169], [94mLoss[0m : 2.28860
[1mStep[0m  [48/169], [94mLoss[0m : 1.92286
[1mStep[0m  [64/169], [94mLoss[0m : 2.16780
[1mStep[0m  [80/169], [94mLoss[0m : 2.65355
[1mStep[0m  [96/169], [94mLoss[0m : 2.48751
[1mStep[0m  [112/169], [94mLoss[0m : 2.36718
[1mStep[0m  [128/169], [94mLoss[0m : 2.32169
[1mStep[0m  [144/169], [94mLoss[0m : 2.38304
[1mStep[0m  [160/169], [94mLoss[0m : 2.69936

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18277
[1mStep[0m  [16/169], [94mLoss[0m : 2.54811
[1mStep[0m  [32/169], [94mLoss[0m : 2.81333
[1mStep[0m  [48/169], [94mLoss[0m : 2.61741
[1mStep[0m  [64/169], [94mLoss[0m : 2.10950
[1mStep[0m  [80/169], [94mLoss[0m : 2.52455
[1mStep[0m  [96/169], [94mLoss[0m : 2.18406
[1mStep[0m  [112/169], [94mLoss[0m : 2.45369
[1mStep[0m  [128/169], [94mLoss[0m : 2.28690
[1mStep[0m  [144/169], [94mLoss[0m : 2.04273
[1mStep[0m  [160/169], [94mLoss[0m : 3.03778

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25332
[1mStep[0m  [16/169], [94mLoss[0m : 2.47349
[1mStep[0m  [32/169], [94mLoss[0m : 2.40540
[1mStep[0m  [48/169], [94mLoss[0m : 2.32011
[1mStep[0m  [64/169], [94mLoss[0m : 2.45587
[1mStep[0m  [80/169], [94mLoss[0m : 2.54002
[1mStep[0m  [96/169], [94mLoss[0m : 2.50079
[1mStep[0m  [112/169], [94mLoss[0m : 2.03495
[1mStep[0m  [128/169], [94mLoss[0m : 2.23955
[1mStep[0m  [144/169], [94mLoss[0m : 2.28430
[1mStep[0m  [160/169], [94mLoss[0m : 2.59089

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.318, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71957
[1mStep[0m  [16/169], [94mLoss[0m : 2.15673
[1mStep[0m  [32/169], [94mLoss[0m : 2.56731
[1mStep[0m  [48/169], [94mLoss[0m : 3.08944
[1mStep[0m  [64/169], [94mLoss[0m : 2.69453
[1mStep[0m  [80/169], [94mLoss[0m : 2.65429
[1mStep[0m  [96/169], [94mLoss[0m : 2.56528
[1mStep[0m  [112/169], [94mLoss[0m : 2.48686
[1mStep[0m  [128/169], [94mLoss[0m : 2.14437
[1mStep[0m  [144/169], [94mLoss[0m : 2.55785
[1mStep[0m  [160/169], [94mLoss[0m : 2.64891

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95333
[1mStep[0m  [16/169], [94mLoss[0m : 2.72560
[1mStep[0m  [32/169], [94mLoss[0m : 2.44286
[1mStep[0m  [48/169], [94mLoss[0m : 2.62607
[1mStep[0m  [64/169], [94mLoss[0m : 2.36058
[1mStep[0m  [80/169], [94mLoss[0m : 2.47676
[1mStep[0m  [96/169], [94mLoss[0m : 2.25479
[1mStep[0m  [112/169], [94mLoss[0m : 2.42093
[1mStep[0m  [128/169], [94mLoss[0m : 2.09556
[1mStep[0m  [144/169], [94mLoss[0m : 2.88312
[1mStep[0m  [160/169], [94mLoss[0m : 2.34579

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24998
[1mStep[0m  [16/169], [94mLoss[0m : 2.34156
[1mStep[0m  [32/169], [94mLoss[0m : 2.40421
[1mStep[0m  [48/169], [94mLoss[0m : 2.57404
[1mStep[0m  [64/169], [94mLoss[0m : 2.73258
[1mStep[0m  [80/169], [94mLoss[0m : 2.32022
[1mStep[0m  [96/169], [94mLoss[0m : 2.50982
[1mStep[0m  [112/169], [94mLoss[0m : 2.80797
[1mStep[0m  [128/169], [94mLoss[0m : 2.37124
[1mStep[0m  [144/169], [94mLoss[0m : 2.03064
[1mStep[0m  [160/169], [94mLoss[0m : 2.39414

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.331, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84441
[1mStep[0m  [16/169], [94mLoss[0m : 2.04834
[1mStep[0m  [32/169], [94mLoss[0m : 2.53413
[1mStep[0m  [48/169], [94mLoss[0m : 2.19346
[1mStep[0m  [64/169], [94mLoss[0m : 2.16730
[1mStep[0m  [80/169], [94mLoss[0m : 2.40731
[1mStep[0m  [96/169], [94mLoss[0m : 3.04212
[1mStep[0m  [112/169], [94mLoss[0m : 2.57838
[1mStep[0m  [128/169], [94mLoss[0m : 2.06160
[1mStep[0m  [144/169], [94mLoss[0m : 2.40000
[1mStep[0m  [160/169], [94mLoss[0m : 2.35336

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31575
[1mStep[0m  [16/169], [94mLoss[0m : 1.90281
[1mStep[0m  [32/169], [94mLoss[0m : 2.85794
[1mStep[0m  [48/169], [94mLoss[0m : 1.98588
[1mStep[0m  [64/169], [94mLoss[0m : 2.38089
[1mStep[0m  [80/169], [94mLoss[0m : 2.51935
[1mStep[0m  [96/169], [94mLoss[0m : 2.40630
[1mStep[0m  [112/169], [94mLoss[0m : 3.04995
[1mStep[0m  [128/169], [94mLoss[0m : 2.33600
[1mStep[0m  [144/169], [94mLoss[0m : 2.51825
[1mStep[0m  [160/169], [94mLoss[0m : 2.47371

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.327, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52842
[1mStep[0m  [16/169], [94mLoss[0m : 2.49493
[1mStep[0m  [32/169], [94mLoss[0m : 2.78248
[1mStep[0m  [48/169], [94mLoss[0m : 2.62220
[1mStep[0m  [64/169], [94mLoss[0m : 2.26592
[1mStep[0m  [80/169], [94mLoss[0m : 1.93969
[1mStep[0m  [96/169], [94mLoss[0m : 2.18019
[1mStep[0m  [112/169], [94mLoss[0m : 2.32057
[1mStep[0m  [128/169], [94mLoss[0m : 2.39189
[1mStep[0m  [144/169], [94mLoss[0m : 2.27088
[1mStep[0m  [160/169], [94mLoss[0m : 2.22540

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.362, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66857
[1mStep[0m  [16/169], [94mLoss[0m : 2.18500
[1mStep[0m  [32/169], [94mLoss[0m : 2.27119
[1mStep[0m  [48/169], [94mLoss[0m : 2.15527
[1mStep[0m  [64/169], [94mLoss[0m : 2.55247
[1mStep[0m  [80/169], [94mLoss[0m : 2.48705
[1mStep[0m  [96/169], [94mLoss[0m : 2.61769
[1mStep[0m  [112/169], [94mLoss[0m : 2.73965
[1mStep[0m  [128/169], [94mLoss[0m : 2.49171
[1mStep[0m  [144/169], [94mLoss[0m : 2.00734
[1mStep[0m  [160/169], [94mLoss[0m : 2.45297

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60345
[1mStep[0m  [16/169], [94mLoss[0m : 2.83347
[1mStep[0m  [32/169], [94mLoss[0m : 2.27772
[1mStep[0m  [48/169], [94mLoss[0m : 2.50266
[1mStep[0m  [64/169], [94mLoss[0m : 2.43014
[1mStep[0m  [80/169], [94mLoss[0m : 2.43781
[1mStep[0m  [96/169], [94mLoss[0m : 2.46909
[1mStep[0m  [112/169], [94mLoss[0m : 2.22926
[1mStep[0m  [128/169], [94mLoss[0m : 2.36547
[1mStep[0m  [144/169], [94mLoss[0m : 2.18777
[1mStep[0m  [160/169], [94mLoss[0m : 3.16247

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.363, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04853
[1mStep[0m  [16/169], [94mLoss[0m : 2.36330
[1mStep[0m  [32/169], [94mLoss[0m : 2.11324
[1mStep[0m  [48/169], [94mLoss[0m : 2.17338
[1mStep[0m  [64/169], [94mLoss[0m : 2.48910
[1mStep[0m  [80/169], [94mLoss[0m : 2.78940
[1mStep[0m  [96/169], [94mLoss[0m : 2.13307
[1mStep[0m  [112/169], [94mLoss[0m : 2.66857
[1mStep[0m  [128/169], [94mLoss[0m : 2.64430
[1mStep[0m  [144/169], [94mLoss[0m : 2.47816
[1mStep[0m  [160/169], [94mLoss[0m : 2.56201

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51491
[1mStep[0m  [16/169], [94mLoss[0m : 2.42881
[1mStep[0m  [32/169], [94mLoss[0m : 2.43675
[1mStep[0m  [48/169], [94mLoss[0m : 2.69601
[1mStep[0m  [64/169], [94mLoss[0m : 2.27640
[1mStep[0m  [80/169], [94mLoss[0m : 2.42881
[1mStep[0m  [96/169], [94mLoss[0m : 2.53576
[1mStep[0m  [112/169], [94mLoss[0m : 2.21848
[1mStep[0m  [128/169], [94mLoss[0m : 2.78605
[1mStep[0m  [144/169], [94mLoss[0m : 2.44155
[1mStep[0m  [160/169], [94mLoss[0m : 2.19304

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50882
[1mStep[0m  [16/169], [94mLoss[0m : 2.62387
[1mStep[0m  [32/169], [94mLoss[0m : 2.17487
[1mStep[0m  [48/169], [94mLoss[0m : 1.87491
[1mStep[0m  [64/169], [94mLoss[0m : 2.25453
[1mStep[0m  [80/169], [94mLoss[0m : 2.21067
[1mStep[0m  [96/169], [94mLoss[0m : 2.80973
[1mStep[0m  [112/169], [94mLoss[0m : 2.37203
[1mStep[0m  [128/169], [94mLoss[0m : 2.21368
[1mStep[0m  [144/169], [94mLoss[0m : 2.40025
[1mStep[0m  [160/169], [94mLoss[0m : 2.35287

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76426
[1mStep[0m  [16/169], [94mLoss[0m : 2.45054
[1mStep[0m  [32/169], [94mLoss[0m : 2.35491
[1mStep[0m  [48/169], [94mLoss[0m : 2.53644
[1mStep[0m  [64/169], [94mLoss[0m : 2.53480
[1mStep[0m  [80/169], [94mLoss[0m : 2.25950
[1mStep[0m  [96/169], [94mLoss[0m : 2.37067
[1mStep[0m  [112/169], [94mLoss[0m : 2.33580
[1mStep[0m  [128/169], [94mLoss[0m : 2.63750
[1mStep[0m  [144/169], [94mLoss[0m : 2.59433
[1mStep[0m  [160/169], [94mLoss[0m : 2.47606

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81206
[1mStep[0m  [16/169], [94mLoss[0m : 2.83235
[1mStep[0m  [32/169], [94mLoss[0m : 2.48822
[1mStep[0m  [48/169], [94mLoss[0m : 1.92919
[1mStep[0m  [64/169], [94mLoss[0m : 2.78117
[1mStep[0m  [80/169], [94mLoss[0m : 2.24752
[1mStep[0m  [96/169], [94mLoss[0m : 2.14583
[1mStep[0m  [112/169], [94mLoss[0m : 2.49515
[1mStep[0m  [128/169], [94mLoss[0m : 2.30331
[1mStep[0m  [144/169], [94mLoss[0m : 2.34143
[1mStep[0m  [160/169], [94mLoss[0m : 2.25479

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.324, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.359
====================================

Phase 1 - Evaluation MAE:  2.358548790216446
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.42159
[1mStep[0m  [16/169], [94mLoss[0m : 2.41089
[1mStep[0m  [32/169], [94mLoss[0m : 2.96252
[1mStep[0m  [48/169], [94mLoss[0m : 2.51143
[1mStep[0m  [64/169], [94mLoss[0m : 2.69967
[1mStep[0m  [80/169], [94mLoss[0m : 2.36514
[1mStep[0m  [96/169], [94mLoss[0m : 2.22370
[1mStep[0m  [112/169], [94mLoss[0m : 2.53819
[1mStep[0m  [128/169], [94mLoss[0m : 2.86483
[1mStep[0m  [144/169], [94mLoss[0m : 2.60856
[1mStep[0m  [160/169], [94mLoss[0m : 2.53352

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60242
[1mStep[0m  [16/169], [94mLoss[0m : 2.61437
[1mStep[0m  [32/169], [94mLoss[0m : 2.43445
[1mStep[0m  [48/169], [94mLoss[0m : 2.32743
[1mStep[0m  [64/169], [94mLoss[0m : 2.60061
[1mStep[0m  [80/169], [94mLoss[0m : 2.40102
[1mStep[0m  [96/169], [94mLoss[0m : 2.22279
[1mStep[0m  [112/169], [94mLoss[0m : 2.22184
[1mStep[0m  [128/169], [94mLoss[0m : 2.07240
[1mStep[0m  [144/169], [94mLoss[0m : 2.61233
[1mStep[0m  [160/169], [94mLoss[0m : 2.65231

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33284
[1mStep[0m  [16/169], [94mLoss[0m : 1.98256
[1mStep[0m  [32/169], [94mLoss[0m : 2.05440
[1mStep[0m  [48/169], [94mLoss[0m : 2.21490
[1mStep[0m  [64/169], [94mLoss[0m : 2.28196
[1mStep[0m  [80/169], [94mLoss[0m : 2.39592
[1mStep[0m  [96/169], [94mLoss[0m : 2.56708
[1mStep[0m  [112/169], [94mLoss[0m : 2.01008
[1mStep[0m  [128/169], [94mLoss[0m : 1.90189
[1mStep[0m  [144/169], [94mLoss[0m : 2.72105
[1mStep[0m  [160/169], [94mLoss[0m : 1.81681

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11442
[1mStep[0m  [16/169], [94mLoss[0m : 2.49105
[1mStep[0m  [32/169], [94mLoss[0m : 2.56718
[1mStep[0m  [48/169], [94mLoss[0m : 2.00849
[1mStep[0m  [64/169], [94mLoss[0m : 2.09405
[1mStep[0m  [80/169], [94mLoss[0m : 2.04090
[1mStep[0m  [96/169], [94mLoss[0m : 1.88092
[1mStep[0m  [112/169], [94mLoss[0m : 1.88253
[1mStep[0m  [128/169], [94mLoss[0m : 2.13511
[1mStep[0m  [144/169], [94mLoss[0m : 2.05434
[1mStep[0m  [160/169], [94mLoss[0m : 2.17589

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.393, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99535
[1mStep[0m  [16/169], [94mLoss[0m : 2.14868
[1mStep[0m  [32/169], [94mLoss[0m : 2.07620
[1mStep[0m  [48/169], [94mLoss[0m : 1.88167
[1mStep[0m  [64/169], [94mLoss[0m : 1.99279
[1mStep[0m  [80/169], [94mLoss[0m : 2.07507
[1mStep[0m  [96/169], [94mLoss[0m : 2.12688
[1mStep[0m  [112/169], [94mLoss[0m : 2.31096
[1mStep[0m  [128/169], [94mLoss[0m : 2.15782
[1mStep[0m  [144/169], [94mLoss[0m : 2.28786
[1mStep[0m  [160/169], [94mLoss[0m : 1.83514

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.123, [92mTest[0m: 2.433, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96728
[1mStep[0m  [16/169], [94mLoss[0m : 2.43549
[1mStep[0m  [32/169], [94mLoss[0m : 1.87914
[1mStep[0m  [48/169], [94mLoss[0m : 1.97582
[1mStep[0m  [64/169], [94mLoss[0m : 2.30906
[1mStep[0m  [80/169], [94mLoss[0m : 2.19766
[1mStep[0m  [96/169], [94mLoss[0m : 2.00926
[1mStep[0m  [112/169], [94mLoss[0m : 1.70299
[1mStep[0m  [128/169], [94mLoss[0m : 2.25175
[1mStep[0m  [144/169], [94mLoss[0m : 2.50802
[1mStep[0m  [160/169], [94mLoss[0m : 2.04326

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93979
[1mStep[0m  [16/169], [94mLoss[0m : 1.92411
[1mStep[0m  [32/169], [94mLoss[0m : 2.04321
[1mStep[0m  [48/169], [94mLoss[0m : 2.34342
[1mStep[0m  [64/169], [94mLoss[0m : 2.01892
[1mStep[0m  [80/169], [94mLoss[0m : 2.03121
[1mStep[0m  [96/169], [94mLoss[0m : 2.43691
[1mStep[0m  [112/169], [94mLoss[0m : 1.87739
[1mStep[0m  [128/169], [94mLoss[0m : 2.32460
[1mStep[0m  [144/169], [94mLoss[0m : 1.92085
[1mStep[0m  [160/169], [94mLoss[0m : 2.23526

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08028
[1mStep[0m  [16/169], [94mLoss[0m : 2.24463
[1mStep[0m  [32/169], [94mLoss[0m : 1.90316
[1mStep[0m  [48/169], [94mLoss[0m : 1.87063
[1mStep[0m  [64/169], [94mLoss[0m : 1.66000
[1mStep[0m  [80/169], [94mLoss[0m : 2.12858
[1mStep[0m  [96/169], [94mLoss[0m : 1.73568
[1mStep[0m  [112/169], [94mLoss[0m : 2.24766
[1mStep[0m  [128/169], [94mLoss[0m : 1.96173
[1mStep[0m  [144/169], [94mLoss[0m : 2.32133
[1mStep[0m  [160/169], [94mLoss[0m : 1.85542

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58453
[1mStep[0m  [16/169], [94mLoss[0m : 1.98830
[1mStep[0m  [32/169], [94mLoss[0m : 1.60897
[1mStep[0m  [48/169], [94mLoss[0m : 1.55360
[1mStep[0m  [64/169], [94mLoss[0m : 1.81094
[1mStep[0m  [80/169], [94mLoss[0m : 2.07411
[1mStep[0m  [96/169], [94mLoss[0m : 1.93591
[1mStep[0m  [112/169], [94mLoss[0m : 1.97614
[1mStep[0m  [128/169], [94mLoss[0m : 1.38660
[1mStep[0m  [144/169], [94mLoss[0m : 1.81347
[1mStep[0m  [160/169], [94mLoss[0m : 2.35383

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.424, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52957
[1mStep[0m  [16/169], [94mLoss[0m : 1.68335
[1mStep[0m  [32/169], [94mLoss[0m : 1.48932
[1mStep[0m  [48/169], [94mLoss[0m : 1.80645
[1mStep[0m  [64/169], [94mLoss[0m : 1.81898
[1mStep[0m  [80/169], [94mLoss[0m : 1.81532
[1mStep[0m  [96/169], [94mLoss[0m : 1.85556
[1mStep[0m  [112/169], [94mLoss[0m : 1.50889
[1mStep[0m  [128/169], [94mLoss[0m : 1.84775
[1mStep[0m  [144/169], [94mLoss[0m : 2.06318
[1mStep[0m  [160/169], [94mLoss[0m : 1.73967

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85718
[1mStep[0m  [16/169], [94mLoss[0m : 1.86553
[1mStep[0m  [32/169], [94mLoss[0m : 1.49993
[1mStep[0m  [48/169], [94mLoss[0m : 1.76053
[1mStep[0m  [64/169], [94mLoss[0m : 1.77502
[1mStep[0m  [80/169], [94mLoss[0m : 2.11074
[1mStep[0m  [96/169], [94mLoss[0m : 2.00366
[1mStep[0m  [112/169], [94mLoss[0m : 1.58739
[1mStep[0m  [128/169], [94mLoss[0m : 1.74932
[1mStep[0m  [144/169], [94mLoss[0m : 1.84601
[1mStep[0m  [160/169], [94mLoss[0m : 1.76188

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62584
[1mStep[0m  [16/169], [94mLoss[0m : 2.09405
[1mStep[0m  [32/169], [94mLoss[0m : 1.57168
[1mStep[0m  [48/169], [94mLoss[0m : 1.63166
[1mStep[0m  [64/169], [94mLoss[0m : 1.55020
[1mStep[0m  [80/169], [94mLoss[0m : 1.57873
[1mStep[0m  [96/169], [94mLoss[0m : 2.10395
[1mStep[0m  [112/169], [94mLoss[0m : 1.90465
[1mStep[0m  [128/169], [94mLoss[0m : 1.64476
[1mStep[0m  [144/169], [94mLoss[0m : 1.91638
[1mStep[0m  [160/169], [94mLoss[0m : 1.61713

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.447, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39417
[1mStep[0m  [16/169], [94mLoss[0m : 1.66700
[1mStep[0m  [32/169], [94mLoss[0m : 1.85748
[1mStep[0m  [48/169], [94mLoss[0m : 1.69205
[1mStep[0m  [64/169], [94mLoss[0m : 1.86914
[1mStep[0m  [80/169], [94mLoss[0m : 1.39992
[1mStep[0m  [96/169], [94mLoss[0m : 1.92487
[1mStep[0m  [112/169], [94mLoss[0m : 1.73040
[1mStep[0m  [128/169], [94mLoss[0m : 1.66844
[1mStep[0m  [144/169], [94mLoss[0m : 1.71618
[1mStep[0m  [160/169], [94mLoss[0m : 1.75931

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61558
[1mStep[0m  [16/169], [94mLoss[0m : 1.69143
[1mStep[0m  [32/169], [94mLoss[0m : 1.58176
[1mStep[0m  [48/169], [94mLoss[0m : 1.76720
[1mStep[0m  [64/169], [94mLoss[0m : 1.41831
[1mStep[0m  [80/169], [94mLoss[0m : 1.94832
[1mStep[0m  [96/169], [94mLoss[0m : 1.43085
[1mStep[0m  [112/169], [94mLoss[0m : 1.66325
[1mStep[0m  [128/169], [94mLoss[0m : 1.56391
[1mStep[0m  [144/169], [94mLoss[0m : 1.31863
[1mStep[0m  [160/169], [94mLoss[0m : 1.23943

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.658, [92mTest[0m: 2.516, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52426
[1mStep[0m  [16/169], [94mLoss[0m : 1.56342
[1mStep[0m  [32/169], [94mLoss[0m : 1.39444
[1mStep[0m  [48/169], [94mLoss[0m : 1.72507
[1mStep[0m  [64/169], [94mLoss[0m : 1.73941
[1mStep[0m  [80/169], [94mLoss[0m : 1.35664
[1mStep[0m  [96/169], [94mLoss[0m : 1.60591
[1mStep[0m  [112/169], [94mLoss[0m : 1.70782
[1mStep[0m  [128/169], [94mLoss[0m : 1.49005
[1mStep[0m  [144/169], [94mLoss[0m : 1.73892
[1mStep[0m  [160/169], [94mLoss[0m : 1.80777

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.653, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46486
[1mStep[0m  [16/169], [94mLoss[0m : 1.49937
[1mStep[0m  [32/169], [94mLoss[0m : 1.40876
[1mStep[0m  [48/169], [94mLoss[0m : 1.42154
[1mStep[0m  [64/169], [94mLoss[0m : 1.59780
[1mStep[0m  [80/169], [94mLoss[0m : 1.72524
[1mStep[0m  [96/169], [94mLoss[0m : 1.66405
[1mStep[0m  [112/169], [94mLoss[0m : 1.95874
[1mStep[0m  [128/169], [94mLoss[0m : 1.56549
[1mStep[0m  [144/169], [94mLoss[0m : 1.64366
[1mStep[0m  [160/169], [94mLoss[0m : 1.49290

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.464, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49790
[1mStep[0m  [16/169], [94mLoss[0m : 1.41059
[1mStep[0m  [32/169], [94mLoss[0m : 1.66068
[1mStep[0m  [48/169], [94mLoss[0m : 1.50944
[1mStep[0m  [64/169], [94mLoss[0m : 1.61629
[1mStep[0m  [80/169], [94mLoss[0m : 1.58079
[1mStep[0m  [96/169], [94mLoss[0m : 1.59533
[1mStep[0m  [112/169], [94mLoss[0m : 1.75599
[1mStep[0m  [128/169], [94mLoss[0m : 1.58330
[1mStep[0m  [144/169], [94mLoss[0m : 1.46930
[1mStep[0m  [160/169], [94mLoss[0m : 1.19608

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.570, [92mTest[0m: 2.478, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88504
[1mStep[0m  [16/169], [94mLoss[0m : 1.49015
[1mStep[0m  [32/169], [94mLoss[0m : 1.49838
[1mStep[0m  [48/169], [94mLoss[0m : 1.60478
[1mStep[0m  [64/169], [94mLoss[0m : 1.40619
[1mStep[0m  [80/169], [94mLoss[0m : 1.46970
[1mStep[0m  [96/169], [94mLoss[0m : 1.40320
[1mStep[0m  [112/169], [94mLoss[0m : 1.79217
[1mStep[0m  [128/169], [94mLoss[0m : 1.36270
[1mStep[0m  [144/169], [94mLoss[0m : 1.75981
[1mStep[0m  [160/169], [94mLoss[0m : 1.33892

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.540, [92mTest[0m: 2.474, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58503
[1mStep[0m  [16/169], [94mLoss[0m : 1.48013
[1mStep[0m  [32/169], [94mLoss[0m : 1.30408
[1mStep[0m  [48/169], [94mLoss[0m : 1.29315
[1mStep[0m  [64/169], [94mLoss[0m : 1.24295
[1mStep[0m  [80/169], [94mLoss[0m : 1.43622
[1mStep[0m  [96/169], [94mLoss[0m : 1.66491
[1mStep[0m  [112/169], [94mLoss[0m : 1.60701
[1mStep[0m  [128/169], [94mLoss[0m : 1.31904
[1mStep[0m  [144/169], [94mLoss[0m : 1.51212
[1mStep[0m  [160/169], [94mLoss[0m : 1.48871

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.523, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.29250
[1mStep[0m  [16/169], [94mLoss[0m : 1.65239
[1mStep[0m  [32/169], [94mLoss[0m : 1.22690
[1mStep[0m  [48/169], [94mLoss[0m : 1.77976
[1mStep[0m  [64/169], [94mLoss[0m : 1.37926
[1mStep[0m  [80/169], [94mLoss[0m : 1.34114
[1mStep[0m  [96/169], [94mLoss[0m : 1.50601
[1mStep[0m  [112/169], [94mLoss[0m : 1.64577
[1mStep[0m  [128/169], [94mLoss[0m : 1.42340
[1mStep[0m  [144/169], [94mLoss[0m : 1.81923
[1mStep[0m  [160/169], [94mLoss[0m : 1.62458

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.508, [92mTest[0m: 2.559, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42417
[1mStep[0m  [16/169], [94mLoss[0m : 1.37758
[1mStep[0m  [32/169], [94mLoss[0m : 1.32098
[1mStep[0m  [48/169], [94mLoss[0m : 1.33540
[1mStep[0m  [64/169], [94mLoss[0m : 1.79216
[1mStep[0m  [80/169], [94mLoss[0m : 1.55765
[1mStep[0m  [96/169], [94mLoss[0m : 1.26756
[1mStep[0m  [112/169], [94mLoss[0m : 1.55398
[1mStep[0m  [128/169], [94mLoss[0m : 2.06020
[1mStep[0m  [144/169], [94mLoss[0m : 1.45810
[1mStep[0m  [160/169], [94mLoss[0m : 1.63349

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.434, [92mTest[0m: 2.522, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46380
[1mStep[0m  [16/169], [94mLoss[0m : 1.36067
[1mStep[0m  [32/169], [94mLoss[0m : 1.20760
[1mStep[0m  [48/169], [94mLoss[0m : 1.59633
[1mStep[0m  [64/169], [94mLoss[0m : 1.69924
[1mStep[0m  [80/169], [94mLoss[0m : 1.28971
[1mStep[0m  [96/169], [94mLoss[0m : 1.29449
[1mStep[0m  [112/169], [94mLoss[0m : 1.41760
[1mStep[0m  [128/169], [94mLoss[0m : 1.94187
[1mStep[0m  [144/169], [94mLoss[0m : 1.37115
[1mStep[0m  [160/169], [94mLoss[0m : 1.48142

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.429, [92mTest[0m: 2.512, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.33772
[1mStep[0m  [16/169], [94mLoss[0m : 1.25220
[1mStep[0m  [32/169], [94mLoss[0m : 1.48330
[1mStep[0m  [48/169], [94mLoss[0m : 1.33872
[1mStep[0m  [64/169], [94mLoss[0m : 1.16023
[1mStep[0m  [80/169], [94mLoss[0m : 1.32881
[1mStep[0m  [96/169], [94mLoss[0m : 1.45834
[1mStep[0m  [112/169], [94mLoss[0m : 1.29043
[1mStep[0m  [128/169], [94mLoss[0m : 1.60851
[1mStep[0m  [144/169], [94mLoss[0m : 1.33394
[1mStep[0m  [160/169], [94mLoss[0m : 1.59445

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.399, [92mTest[0m: 2.474, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36357
[1mStep[0m  [16/169], [94mLoss[0m : 1.21975
[1mStep[0m  [32/169], [94mLoss[0m : 2.08191
[1mStep[0m  [48/169], [94mLoss[0m : 1.40181
[1mStep[0m  [64/169], [94mLoss[0m : 1.23824
[1mStep[0m  [80/169], [94mLoss[0m : 1.45884
[1mStep[0m  [96/169], [94mLoss[0m : 1.37681
[1mStep[0m  [112/169], [94mLoss[0m : 1.07118
[1mStep[0m  [128/169], [94mLoss[0m : 1.49546
[1mStep[0m  [144/169], [94mLoss[0m : 1.32041
[1mStep[0m  [160/169], [94mLoss[0m : 1.54501

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.391, [92mTest[0m: 2.513, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.511
====================================

Phase 2 - Evaluation MAE:  2.5106596648693085
MAE score P1      2.358549
MAE score P2       2.51066
loss              1.390745
learning_rate         0.01
batch_size              64
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.44974
[1mStep[0m  [16/169], [94mLoss[0m : 9.95296
[1mStep[0m  [32/169], [94mLoss[0m : 10.09770
[1mStep[0m  [48/169], [94mLoss[0m : 9.04329
[1mStep[0m  [64/169], [94mLoss[0m : 8.13234
[1mStep[0m  [80/169], [94mLoss[0m : 7.55557
[1mStep[0m  [96/169], [94mLoss[0m : 7.96419
[1mStep[0m  [112/169], [94mLoss[0m : 6.48380
[1mStep[0m  [128/169], [94mLoss[0m : 5.86955
[1mStep[0m  [144/169], [94mLoss[0m : 4.77880
[1mStep[0m  [160/169], [94mLoss[0m : 4.23163

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.636, [92mTest[0m: 10.785, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.49198
[1mStep[0m  [16/169], [94mLoss[0m : 3.17510
[1mStep[0m  [32/169], [94mLoss[0m : 3.12199
[1mStep[0m  [48/169], [94mLoss[0m : 2.80215
[1mStep[0m  [64/169], [94mLoss[0m : 2.59808
[1mStep[0m  [80/169], [94mLoss[0m : 2.17263
[1mStep[0m  [96/169], [94mLoss[0m : 2.38380
[1mStep[0m  [112/169], [94mLoss[0m : 2.82119
[1mStep[0m  [128/169], [94mLoss[0m : 2.82461
[1mStep[0m  [144/169], [94mLoss[0m : 2.62871
[1mStep[0m  [160/169], [94mLoss[0m : 2.90136

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.977, [92mTest[0m: 2.993, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.25485
[1mStep[0m  [16/169], [94mLoss[0m : 2.22946
[1mStep[0m  [32/169], [94mLoss[0m : 2.44363
[1mStep[0m  [48/169], [94mLoss[0m : 3.09058
[1mStep[0m  [64/169], [94mLoss[0m : 2.94262
[1mStep[0m  [80/169], [94mLoss[0m : 3.52871
[1mStep[0m  [96/169], [94mLoss[0m : 2.63825
[1mStep[0m  [112/169], [94mLoss[0m : 2.45197
[1mStep[0m  [128/169], [94mLoss[0m : 2.56466
[1mStep[0m  [144/169], [94mLoss[0m : 2.75291
[1mStep[0m  [160/169], [94mLoss[0m : 2.71359

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77509
[1mStep[0m  [16/169], [94mLoss[0m : 3.07034
[1mStep[0m  [32/169], [94mLoss[0m : 2.43880
[1mStep[0m  [48/169], [94mLoss[0m : 3.11547
[1mStep[0m  [64/169], [94mLoss[0m : 2.85011
[1mStep[0m  [80/169], [94mLoss[0m : 2.75230
[1mStep[0m  [96/169], [94mLoss[0m : 2.16430
[1mStep[0m  [112/169], [94mLoss[0m : 2.44815
[1mStep[0m  [128/169], [94mLoss[0m : 2.75379
[1mStep[0m  [144/169], [94mLoss[0m : 3.01313
[1mStep[0m  [160/169], [94mLoss[0m : 2.34866

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.401, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65113
[1mStep[0m  [16/169], [94mLoss[0m : 2.44406
[1mStep[0m  [32/169], [94mLoss[0m : 2.06133
[1mStep[0m  [48/169], [94mLoss[0m : 2.86840
[1mStep[0m  [64/169], [94mLoss[0m : 2.38111
[1mStep[0m  [80/169], [94mLoss[0m : 2.49796
[1mStep[0m  [96/169], [94mLoss[0m : 2.58158
[1mStep[0m  [112/169], [94mLoss[0m : 2.58741
[1mStep[0m  [128/169], [94mLoss[0m : 2.62516
[1mStep[0m  [144/169], [94mLoss[0m : 2.89134
[1mStep[0m  [160/169], [94mLoss[0m : 3.37340

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94892
[1mStep[0m  [16/169], [94mLoss[0m : 2.35495
[1mStep[0m  [32/169], [94mLoss[0m : 2.23688
[1mStep[0m  [48/169], [94mLoss[0m : 2.36036
[1mStep[0m  [64/169], [94mLoss[0m : 2.49164
[1mStep[0m  [80/169], [94mLoss[0m : 2.59872
[1mStep[0m  [96/169], [94mLoss[0m : 2.67650
[1mStep[0m  [112/169], [94mLoss[0m : 2.74551
[1mStep[0m  [128/169], [94mLoss[0m : 2.36336
[1mStep[0m  [144/169], [94mLoss[0m : 2.51985
[1mStep[0m  [160/169], [94mLoss[0m : 2.77848

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.366, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35681
[1mStep[0m  [16/169], [94mLoss[0m : 2.43051
[1mStep[0m  [32/169], [94mLoss[0m : 2.31751
[1mStep[0m  [48/169], [94mLoss[0m : 2.84297
[1mStep[0m  [64/169], [94mLoss[0m : 2.21051
[1mStep[0m  [80/169], [94mLoss[0m : 2.08794
[1mStep[0m  [96/169], [94mLoss[0m : 2.56714
[1mStep[0m  [112/169], [94mLoss[0m : 2.14579
[1mStep[0m  [128/169], [94mLoss[0m : 2.52985
[1mStep[0m  [144/169], [94mLoss[0m : 2.47918
[1mStep[0m  [160/169], [94mLoss[0m : 2.34093

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10657
[1mStep[0m  [16/169], [94mLoss[0m : 2.46672
[1mStep[0m  [32/169], [94mLoss[0m : 2.38726
[1mStep[0m  [48/169], [94mLoss[0m : 2.43220
[1mStep[0m  [64/169], [94mLoss[0m : 2.55230
[1mStep[0m  [80/169], [94mLoss[0m : 2.81178
[1mStep[0m  [96/169], [94mLoss[0m : 2.20348
[1mStep[0m  [112/169], [94mLoss[0m : 2.63817
[1mStep[0m  [128/169], [94mLoss[0m : 2.76605
[1mStep[0m  [144/169], [94mLoss[0m : 2.65397
[1mStep[0m  [160/169], [94mLoss[0m : 2.29445

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64063
[1mStep[0m  [16/169], [94mLoss[0m : 2.64343
[1mStep[0m  [32/169], [94mLoss[0m : 2.48030
[1mStep[0m  [48/169], [94mLoss[0m : 2.48017
[1mStep[0m  [64/169], [94mLoss[0m : 2.38216
[1mStep[0m  [80/169], [94mLoss[0m : 2.58395
[1mStep[0m  [96/169], [94mLoss[0m : 2.52928
[1mStep[0m  [112/169], [94mLoss[0m : 2.58041
[1mStep[0m  [128/169], [94mLoss[0m : 2.68468
[1mStep[0m  [144/169], [94mLoss[0m : 2.66226
[1mStep[0m  [160/169], [94mLoss[0m : 2.39171

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54542
[1mStep[0m  [16/169], [94mLoss[0m : 2.26368
[1mStep[0m  [32/169], [94mLoss[0m : 2.47191
[1mStep[0m  [48/169], [94mLoss[0m : 2.31769
[1mStep[0m  [64/169], [94mLoss[0m : 2.34018
[1mStep[0m  [80/169], [94mLoss[0m : 2.60442
[1mStep[0m  [96/169], [94mLoss[0m : 2.65989
[1mStep[0m  [112/169], [94mLoss[0m : 2.07554
[1mStep[0m  [128/169], [94mLoss[0m : 2.55713
[1mStep[0m  [144/169], [94mLoss[0m : 2.17297
[1mStep[0m  [160/169], [94mLoss[0m : 2.26148

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.00726
[1mStep[0m  [16/169], [94mLoss[0m : 2.72847
[1mStep[0m  [32/169], [94mLoss[0m : 2.55205
[1mStep[0m  [48/169], [94mLoss[0m : 2.17877
[1mStep[0m  [64/169], [94mLoss[0m : 2.35615
[1mStep[0m  [80/169], [94mLoss[0m : 2.54623
[1mStep[0m  [96/169], [94mLoss[0m : 2.81096
[1mStep[0m  [112/169], [94mLoss[0m : 1.81684
[1mStep[0m  [128/169], [94mLoss[0m : 2.14804
[1mStep[0m  [144/169], [94mLoss[0m : 2.75885
[1mStep[0m  [160/169], [94mLoss[0m : 2.49050

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73255
[1mStep[0m  [16/169], [94mLoss[0m : 2.58809
[1mStep[0m  [32/169], [94mLoss[0m : 2.00946
[1mStep[0m  [48/169], [94mLoss[0m : 2.54958
[1mStep[0m  [64/169], [94mLoss[0m : 2.72515
[1mStep[0m  [80/169], [94mLoss[0m : 2.11139
[1mStep[0m  [96/169], [94mLoss[0m : 2.67406
[1mStep[0m  [112/169], [94mLoss[0m : 2.76581
[1mStep[0m  [128/169], [94mLoss[0m : 2.36297
[1mStep[0m  [144/169], [94mLoss[0m : 2.38237
[1mStep[0m  [160/169], [94mLoss[0m : 2.87896

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.371, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60698
[1mStep[0m  [16/169], [94mLoss[0m : 2.55390
[1mStep[0m  [32/169], [94mLoss[0m : 2.40270
[1mStep[0m  [48/169], [94mLoss[0m : 2.66260
[1mStep[0m  [64/169], [94mLoss[0m : 2.85851
[1mStep[0m  [80/169], [94mLoss[0m : 3.03167
[1mStep[0m  [96/169], [94mLoss[0m : 2.95712
[1mStep[0m  [112/169], [94mLoss[0m : 2.84666
[1mStep[0m  [128/169], [94mLoss[0m : 2.56193
[1mStep[0m  [144/169], [94mLoss[0m : 2.38425
[1mStep[0m  [160/169], [94mLoss[0m : 2.10139

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44693
[1mStep[0m  [16/169], [94mLoss[0m : 2.51639
[1mStep[0m  [32/169], [94mLoss[0m : 2.36368
[1mStep[0m  [48/169], [94mLoss[0m : 2.41266
[1mStep[0m  [64/169], [94mLoss[0m : 2.56510
[1mStep[0m  [80/169], [94mLoss[0m : 2.05912
[1mStep[0m  [96/169], [94mLoss[0m : 2.29711
[1mStep[0m  [112/169], [94mLoss[0m : 2.69247
[1mStep[0m  [128/169], [94mLoss[0m : 2.31792
[1mStep[0m  [144/169], [94mLoss[0m : 2.57004
[1mStep[0m  [160/169], [94mLoss[0m : 2.17003

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.368, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90450
[1mStep[0m  [16/169], [94mLoss[0m : 2.40076
[1mStep[0m  [32/169], [94mLoss[0m : 2.51991
[1mStep[0m  [48/169], [94mLoss[0m : 2.73062
[1mStep[0m  [64/169], [94mLoss[0m : 2.47185
[1mStep[0m  [80/169], [94mLoss[0m : 2.21818
[1mStep[0m  [96/169], [94mLoss[0m : 2.47240
[1mStep[0m  [112/169], [94mLoss[0m : 2.28069
[1mStep[0m  [128/169], [94mLoss[0m : 2.49909
[1mStep[0m  [144/169], [94mLoss[0m : 2.43909
[1mStep[0m  [160/169], [94mLoss[0m : 2.38329

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41978
[1mStep[0m  [16/169], [94mLoss[0m : 2.19292
[1mStep[0m  [32/169], [94mLoss[0m : 2.31917
[1mStep[0m  [48/169], [94mLoss[0m : 2.29798
[1mStep[0m  [64/169], [94mLoss[0m : 2.23982
[1mStep[0m  [80/169], [94mLoss[0m : 2.24465
[1mStep[0m  [96/169], [94mLoss[0m : 2.68055
[1mStep[0m  [112/169], [94mLoss[0m : 2.16546
[1mStep[0m  [128/169], [94mLoss[0m : 2.32676
[1mStep[0m  [144/169], [94mLoss[0m : 2.69777
[1mStep[0m  [160/169], [94mLoss[0m : 2.03706

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.336, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30923
[1mStep[0m  [16/169], [94mLoss[0m : 2.40709
[1mStep[0m  [32/169], [94mLoss[0m : 2.65457
[1mStep[0m  [48/169], [94mLoss[0m : 2.78306
[1mStep[0m  [64/169], [94mLoss[0m : 2.68229
[1mStep[0m  [80/169], [94mLoss[0m : 2.09760
[1mStep[0m  [96/169], [94mLoss[0m : 2.20739
[1mStep[0m  [112/169], [94mLoss[0m : 2.00055
[1mStep[0m  [128/169], [94mLoss[0m : 2.72780
[1mStep[0m  [144/169], [94mLoss[0m : 2.18943
[1mStep[0m  [160/169], [94mLoss[0m : 2.35736

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36179
[1mStep[0m  [16/169], [94mLoss[0m : 2.23730
[1mStep[0m  [32/169], [94mLoss[0m : 2.38645
[1mStep[0m  [48/169], [94mLoss[0m : 2.49708
[1mStep[0m  [64/169], [94mLoss[0m : 2.29241
[1mStep[0m  [80/169], [94mLoss[0m : 2.25028
[1mStep[0m  [96/169], [94mLoss[0m : 2.30947
[1mStep[0m  [112/169], [94mLoss[0m : 2.49544
[1mStep[0m  [128/169], [94mLoss[0m : 2.47787
[1mStep[0m  [144/169], [94mLoss[0m : 2.66630
[1mStep[0m  [160/169], [94mLoss[0m : 2.20475

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42939
[1mStep[0m  [16/169], [94mLoss[0m : 2.39151
[1mStep[0m  [32/169], [94mLoss[0m : 2.37612
[1mStep[0m  [48/169], [94mLoss[0m : 2.39172
[1mStep[0m  [64/169], [94mLoss[0m : 2.27640
[1mStep[0m  [80/169], [94mLoss[0m : 2.43574
[1mStep[0m  [96/169], [94mLoss[0m : 2.69003
[1mStep[0m  [112/169], [94mLoss[0m : 2.97568
[1mStep[0m  [128/169], [94mLoss[0m : 2.39796
[1mStep[0m  [144/169], [94mLoss[0m : 2.57045
[1mStep[0m  [160/169], [94mLoss[0m : 2.27266

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29176
[1mStep[0m  [16/169], [94mLoss[0m : 2.29386
[1mStep[0m  [32/169], [94mLoss[0m : 1.82168
[1mStep[0m  [48/169], [94mLoss[0m : 2.10730
[1mStep[0m  [64/169], [94mLoss[0m : 2.18874
[1mStep[0m  [80/169], [94mLoss[0m : 2.40445
[1mStep[0m  [96/169], [94mLoss[0m : 2.39684
[1mStep[0m  [112/169], [94mLoss[0m : 2.44214
[1mStep[0m  [128/169], [94mLoss[0m : 2.43451
[1mStep[0m  [144/169], [94mLoss[0m : 2.77172
[1mStep[0m  [160/169], [94mLoss[0m : 2.40981

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.342, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76904
[1mStep[0m  [16/169], [94mLoss[0m : 2.89102
[1mStep[0m  [32/169], [94mLoss[0m : 2.42032
[1mStep[0m  [48/169], [94mLoss[0m : 2.33184
[1mStep[0m  [64/169], [94mLoss[0m : 1.95597
[1mStep[0m  [80/169], [94mLoss[0m : 2.22727
[1mStep[0m  [96/169], [94mLoss[0m : 2.52970
[1mStep[0m  [112/169], [94mLoss[0m : 2.28109
[1mStep[0m  [128/169], [94mLoss[0m : 2.02512
[1mStep[0m  [144/169], [94mLoss[0m : 2.50123
[1mStep[0m  [160/169], [94mLoss[0m : 2.90462

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.365, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85436
[1mStep[0m  [16/169], [94mLoss[0m : 2.31396
[1mStep[0m  [32/169], [94mLoss[0m : 2.12161
[1mStep[0m  [48/169], [94mLoss[0m : 2.34853
[1mStep[0m  [64/169], [94mLoss[0m : 2.30531
[1mStep[0m  [80/169], [94mLoss[0m : 2.26315
[1mStep[0m  [96/169], [94mLoss[0m : 2.31594
[1mStep[0m  [112/169], [94mLoss[0m : 2.14878
[1mStep[0m  [128/169], [94mLoss[0m : 2.65554
[1mStep[0m  [144/169], [94mLoss[0m : 2.61780
[1mStep[0m  [160/169], [94mLoss[0m : 2.44246

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18838
[1mStep[0m  [16/169], [94mLoss[0m : 2.77168
[1mStep[0m  [32/169], [94mLoss[0m : 2.15518
[1mStep[0m  [48/169], [94mLoss[0m : 2.65404
[1mStep[0m  [64/169], [94mLoss[0m : 2.77020
[1mStep[0m  [80/169], [94mLoss[0m : 1.96471
[1mStep[0m  [96/169], [94mLoss[0m : 2.71285
[1mStep[0m  [112/169], [94mLoss[0m : 2.14031
[1mStep[0m  [128/169], [94mLoss[0m : 2.49302
[1mStep[0m  [144/169], [94mLoss[0m : 2.50432
[1mStep[0m  [160/169], [94mLoss[0m : 2.17273

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99606
[1mStep[0m  [16/169], [94mLoss[0m : 2.50352
[1mStep[0m  [32/169], [94mLoss[0m : 2.82403
[1mStep[0m  [48/169], [94mLoss[0m : 2.91099
[1mStep[0m  [64/169], [94mLoss[0m : 2.31888
[1mStep[0m  [80/169], [94mLoss[0m : 2.02158
[1mStep[0m  [96/169], [94mLoss[0m : 2.56571
[1mStep[0m  [112/169], [94mLoss[0m : 2.37280
[1mStep[0m  [128/169], [94mLoss[0m : 2.59885
[1mStep[0m  [144/169], [94mLoss[0m : 2.25423
[1mStep[0m  [160/169], [94mLoss[0m : 2.17139

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21285
[1mStep[0m  [16/169], [94mLoss[0m : 2.46491
[1mStep[0m  [32/169], [94mLoss[0m : 2.43087
[1mStep[0m  [48/169], [94mLoss[0m : 2.37255
[1mStep[0m  [64/169], [94mLoss[0m : 1.97328
[1mStep[0m  [80/169], [94mLoss[0m : 2.37173
[1mStep[0m  [96/169], [94mLoss[0m : 2.04953
[1mStep[0m  [112/169], [94mLoss[0m : 2.47049
[1mStep[0m  [128/169], [94mLoss[0m : 2.37264
[1mStep[0m  [144/169], [94mLoss[0m : 2.50280
[1mStep[0m  [160/169], [94mLoss[0m : 1.91866

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.356, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72950
[1mStep[0m  [16/169], [94mLoss[0m : 2.61360
[1mStep[0m  [32/169], [94mLoss[0m : 2.64676
[1mStep[0m  [48/169], [94mLoss[0m : 2.02830
[1mStep[0m  [64/169], [94mLoss[0m : 2.49291
[1mStep[0m  [80/169], [94mLoss[0m : 2.89640
[1mStep[0m  [96/169], [94mLoss[0m : 2.69590
[1mStep[0m  [112/169], [94mLoss[0m : 2.30518
[1mStep[0m  [128/169], [94mLoss[0m : 1.98417
[1mStep[0m  [144/169], [94mLoss[0m : 2.51651
[1mStep[0m  [160/169], [94mLoss[0m : 2.37975

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85311
[1mStep[0m  [16/169], [94mLoss[0m : 2.64899
[1mStep[0m  [32/169], [94mLoss[0m : 2.02263
[1mStep[0m  [48/169], [94mLoss[0m : 2.55569
[1mStep[0m  [64/169], [94mLoss[0m : 2.43130
[1mStep[0m  [80/169], [94mLoss[0m : 2.52533
[1mStep[0m  [96/169], [94mLoss[0m : 2.34800
[1mStep[0m  [112/169], [94mLoss[0m : 2.38260
[1mStep[0m  [128/169], [94mLoss[0m : 2.16167
[1mStep[0m  [144/169], [94mLoss[0m : 2.79334
[1mStep[0m  [160/169], [94mLoss[0m : 2.18099

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65861
[1mStep[0m  [16/169], [94mLoss[0m : 2.02642
[1mStep[0m  [32/169], [94mLoss[0m : 2.35155
[1mStep[0m  [48/169], [94mLoss[0m : 2.21698
[1mStep[0m  [64/169], [94mLoss[0m : 2.50975
[1mStep[0m  [80/169], [94mLoss[0m : 2.69500
[1mStep[0m  [96/169], [94mLoss[0m : 2.56372
[1mStep[0m  [112/169], [94mLoss[0m : 2.58513
[1mStep[0m  [128/169], [94mLoss[0m : 2.20721
[1mStep[0m  [144/169], [94mLoss[0m : 2.83740
[1mStep[0m  [160/169], [94mLoss[0m : 2.14791

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56420
[1mStep[0m  [16/169], [94mLoss[0m : 2.22923
[1mStep[0m  [32/169], [94mLoss[0m : 2.61314
[1mStep[0m  [48/169], [94mLoss[0m : 2.76104
[1mStep[0m  [64/169], [94mLoss[0m : 2.54567
[1mStep[0m  [80/169], [94mLoss[0m : 2.45729
[1mStep[0m  [96/169], [94mLoss[0m : 2.53561
[1mStep[0m  [112/169], [94mLoss[0m : 2.54117
[1mStep[0m  [128/169], [94mLoss[0m : 2.12843
[1mStep[0m  [144/169], [94mLoss[0m : 2.76902
[1mStep[0m  [160/169], [94mLoss[0m : 2.27190

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13069
[1mStep[0m  [16/169], [94mLoss[0m : 1.97206
[1mStep[0m  [32/169], [94mLoss[0m : 2.35873
[1mStep[0m  [48/169], [94mLoss[0m : 2.08123
[1mStep[0m  [64/169], [94mLoss[0m : 2.01977
[1mStep[0m  [80/169], [94mLoss[0m : 2.38961
[1mStep[0m  [96/169], [94mLoss[0m : 2.22924
[1mStep[0m  [112/169], [94mLoss[0m : 2.49588
[1mStep[0m  [128/169], [94mLoss[0m : 2.23852
[1mStep[0m  [144/169], [94mLoss[0m : 1.89386
[1mStep[0m  [160/169], [94mLoss[0m : 2.38757

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.348
====================================

Phase 1 - Evaluation MAE:  2.347548704062189
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.18057
[1mStep[0m  [16/169], [94mLoss[0m : 2.44524
[1mStep[0m  [32/169], [94mLoss[0m : 2.46223
[1mStep[0m  [48/169], [94mLoss[0m : 2.29609
[1mStep[0m  [64/169], [94mLoss[0m : 2.11405
[1mStep[0m  [80/169], [94mLoss[0m : 2.66606
[1mStep[0m  [96/169], [94mLoss[0m : 2.72272
[1mStep[0m  [112/169], [94mLoss[0m : 2.76545
[1mStep[0m  [128/169], [94mLoss[0m : 1.82622
[1mStep[0m  [144/169], [94mLoss[0m : 2.24008
[1mStep[0m  [160/169], [94mLoss[0m : 2.22953

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19435
[1mStep[0m  [16/169], [94mLoss[0m : 2.65498
[1mStep[0m  [32/169], [94mLoss[0m : 2.43467
[1mStep[0m  [48/169], [94mLoss[0m : 2.21703
[1mStep[0m  [64/169], [94mLoss[0m : 2.21688
[1mStep[0m  [80/169], [94mLoss[0m : 2.72048
[1mStep[0m  [96/169], [94mLoss[0m : 2.41062
[1mStep[0m  [112/169], [94mLoss[0m : 2.67964
[1mStep[0m  [128/169], [94mLoss[0m : 2.41916
[1mStep[0m  [144/169], [94mLoss[0m : 2.24592
[1mStep[0m  [160/169], [94mLoss[0m : 2.42063

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09530
[1mStep[0m  [16/169], [94mLoss[0m : 2.18624
[1mStep[0m  [32/169], [94mLoss[0m : 1.93768
[1mStep[0m  [48/169], [94mLoss[0m : 2.00511
[1mStep[0m  [64/169], [94mLoss[0m : 2.08615
[1mStep[0m  [80/169], [94mLoss[0m : 2.34820
[1mStep[0m  [96/169], [94mLoss[0m : 2.00635
[1mStep[0m  [112/169], [94mLoss[0m : 1.98594
[1mStep[0m  [128/169], [94mLoss[0m : 2.78375
[1mStep[0m  [144/169], [94mLoss[0m : 1.76747
[1mStep[0m  [160/169], [94mLoss[0m : 2.52073

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.264, [92mTest[0m: 2.381, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10900
[1mStep[0m  [16/169], [94mLoss[0m : 2.05145
[1mStep[0m  [32/169], [94mLoss[0m : 2.08588
[1mStep[0m  [48/169], [94mLoss[0m : 1.85612
[1mStep[0m  [64/169], [94mLoss[0m : 2.12652
[1mStep[0m  [80/169], [94mLoss[0m : 2.65065
[1mStep[0m  [96/169], [94mLoss[0m : 2.24651
[1mStep[0m  [112/169], [94mLoss[0m : 2.72156
[1mStep[0m  [128/169], [94mLoss[0m : 2.35055
[1mStep[0m  [144/169], [94mLoss[0m : 2.76528
[1mStep[0m  [160/169], [94mLoss[0m : 2.45543

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25471
[1mStep[0m  [16/169], [94mLoss[0m : 2.30624
[1mStep[0m  [32/169], [94mLoss[0m : 2.55809
[1mStep[0m  [48/169], [94mLoss[0m : 2.29598
[1mStep[0m  [64/169], [94mLoss[0m : 2.58466
[1mStep[0m  [80/169], [94mLoss[0m : 2.03759
[1mStep[0m  [96/169], [94mLoss[0m : 2.07205
[1mStep[0m  [112/169], [94mLoss[0m : 2.13043
[1mStep[0m  [128/169], [94mLoss[0m : 2.29055
[1mStep[0m  [144/169], [94mLoss[0m : 1.95292
[1mStep[0m  [160/169], [94mLoss[0m : 1.87146

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29622
[1mStep[0m  [16/169], [94mLoss[0m : 1.81241
[1mStep[0m  [32/169], [94mLoss[0m : 1.94736
[1mStep[0m  [48/169], [94mLoss[0m : 2.04105
[1mStep[0m  [64/169], [94mLoss[0m : 1.79839
[1mStep[0m  [80/169], [94mLoss[0m : 2.66383
[1mStep[0m  [96/169], [94mLoss[0m : 1.96533
[1mStep[0m  [112/169], [94mLoss[0m : 1.88688
[1mStep[0m  [128/169], [94mLoss[0m : 1.80632
[1mStep[0m  [144/169], [94mLoss[0m : 2.44186
[1mStep[0m  [160/169], [94mLoss[0m : 2.14426

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.040, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37331
[1mStep[0m  [16/169], [94mLoss[0m : 1.97001
[1mStep[0m  [32/169], [94mLoss[0m : 2.27710
[1mStep[0m  [48/169], [94mLoss[0m : 1.83221
[1mStep[0m  [64/169], [94mLoss[0m : 1.88527
[1mStep[0m  [80/169], [94mLoss[0m : 1.97813
[1mStep[0m  [96/169], [94mLoss[0m : 2.14570
[1mStep[0m  [112/169], [94mLoss[0m : 1.92735
[1mStep[0m  [128/169], [94mLoss[0m : 2.09830
[1mStep[0m  [144/169], [94mLoss[0m : 1.80112
[1mStep[0m  [160/169], [94mLoss[0m : 1.96967

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.001, [92mTest[0m: 2.426, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60209
[1mStep[0m  [16/169], [94mLoss[0m : 1.71586
[1mStep[0m  [32/169], [94mLoss[0m : 2.13659
[1mStep[0m  [48/169], [94mLoss[0m : 2.32891
[1mStep[0m  [64/169], [94mLoss[0m : 1.98094
[1mStep[0m  [80/169], [94mLoss[0m : 2.30735
[1mStep[0m  [96/169], [94mLoss[0m : 1.74555
[1mStep[0m  [112/169], [94mLoss[0m : 1.66298
[1mStep[0m  [128/169], [94mLoss[0m : 2.13419
[1mStep[0m  [144/169], [94mLoss[0m : 1.76799
[1mStep[0m  [160/169], [94mLoss[0m : 2.02030

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.940, [92mTest[0m: 2.427, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36420
[1mStep[0m  [16/169], [94mLoss[0m : 1.70282
[1mStep[0m  [32/169], [94mLoss[0m : 1.75135
[1mStep[0m  [48/169], [94mLoss[0m : 1.95182
[1mStep[0m  [64/169], [94mLoss[0m : 1.97196
[1mStep[0m  [80/169], [94mLoss[0m : 1.96299
[1mStep[0m  [96/169], [94mLoss[0m : 1.87842
[1mStep[0m  [112/169], [94mLoss[0m : 2.05813
[1mStep[0m  [128/169], [94mLoss[0m : 1.87314
[1mStep[0m  [144/169], [94mLoss[0m : 1.99982
[1mStep[0m  [160/169], [94mLoss[0m : 1.95169

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79963
[1mStep[0m  [16/169], [94mLoss[0m : 1.92879
[1mStep[0m  [32/169], [94mLoss[0m : 1.56505
[1mStep[0m  [48/169], [94mLoss[0m : 1.78553
[1mStep[0m  [64/169], [94mLoss[0m : 2.22595
[1mStep[0m  [80/169], [94mLoss[0m : 2.35306
[1mStep[0m  [96/169], [94mLoss[0m : 1.79983
[1mStep[0m  [112/169], [94mLoss[0m : 2.14921
[1mStep[0m  [128/169], [94mLoss[0m : 1.93839
[1mStep[0m  [144/169], [94mLoss[0m : 1.63366
[1mStep[0m  [160/169], [94mLoss[0m : 1.87978

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92190
[1mStep[0m  [16/169], [94mLoss[0m : 2.05289
[1mStep[0m  [32/169], [94mLoss[0m : 1.63371
[1mStep[0m  [48/169], [94mLoss[0m : 1.64209
[1mStep[0m  [64/169], [94mLoss[0m : 1.87391
[1mStep[0m  [80/169], [94mLoss[0m : 1.45981
[1mStep[0m  [96/169], [94mLoss[0m : 2.37579
[1mStep[0m  [112/169], [94mLoss[0m : 1.49433
[1mStep[0m  [128/169], [94mLoss[0m : 1.60371
[1mStep[0m  [144/169], [94mLoss[0m : 1.93986
[1mStep[0m  [160/169], [94mLoss[0m : 1.75462

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70556
[1mStep[0m  [16/169], [94mLoss[0m : 1.49894
[1mStep[0m  [32/169], [94mLoss[0m : 1.66957
[1mStep[0m  [48/169], [94mLoss[0m : 1.61878
[1mStep[0m  [64/169], [94mLoss[0m : 1.80501
[1mStep[0m  [80/169], [94mLoss[0m : 1.80704
[1mStep[0m  [96/169], [94mLoss[0m : 1.71683
[1mStep[0m  [112/169], [94mLoss[0m : 1.62161
[1mStep[0m  [128/169], [94mLoss[0m : 1.89526
[1mStep[0m  [144/169], [94mLoss[0m : 1.93158
[1mStep[0m  [160/169], [94mLoss[0m : 2.03214

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.762, [92mTest[0m: 2.423, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59374
[1mStep[0m  [16/169], [94mLoss[0m : 1.61817
[1mStep[0m  [32/169], [94mLoss[0m : 1.83346
[1mStep[0m  [48/169], [94mLoss[0m : 1.59985
[1mStep[0m  [64/169], [94mLoss[0m : 1.87434
[1mStep[0m  [80/169], [94mLoss[0m : 1.77061
[1mStep[0m  [96/169], [94mLoss[0m : 1.82672
[1mStep[0m  [112/169], [94mLoss[0m : 1.56261
[1mStep[0m  [128/169], [94mLoss[0m : 1.47349
[1mStep[0m  [144/169], [94mLoss[0m : 1.93827
[1mStep[0m  [160/169], [94mLoss[0m : 1.75438

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.480, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59073
[1mStep[0m  [16/169], [94mLoss[0m : 1.51076
[1mStep[0m  [32/169], [94mLoss[0m : 1.58296
[1mStep[0m  [48/169], [94mLoss[0m : 1.56413
[1mStep[0m  [64/169], [94mLoss[0m : 1.82413
[1mStep[0m  [80/169], [94mLoss[0m : 2.09749
[1mStep[0m  [96/169], [94mLoss[0m : 1.61387
[1mStep[0m  [112/169], [94mLoss[0m : 1.54462
[1mStep[0m  [128/169], [94mLoss[0m : 1.66420
[1mStep[0m  [144/169], [94mLoss[0m : 1.48859
[1mStep[0m  [160/169], [94mLoss[0m : 1.76671

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.670, [92mTest[0m: 2.462, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78411
[1mStep[0m  [16/169], [94mLoss[0m : 1.29593
[1mStep[0m  [32/169], [94mLoss[0m : 1.73470
[1mStep[0m  [48/169], [94mLoss[0m : 1.37475
[1mStep[0m  [64/169], [94mLoss[0m : 1.35395
[1mStep[0m  [80/169], [94mLoss[0m : 1.95091
[1mStep[0m  [96/169], [94mLoss[0m : 1.81319
[1mStep[0m  [112/169], [94mLoss[0m : 1.66507
[1mStep[0m  [128/169], [94mLoss[0m : 1.72897
[1mStep[0m  [144/169], [94mLoss[0m : 1.77748
[1mStep[0m  [160/169], [94mLoss[0m : 1.77900

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48532
[1mStep[0m  [16/169], [94mLoss[0m : 1.62856
[1mStep[0m  [32/169], [94mLoss[0m : 1.50116
[1mStep[0m  [48/169], [94mLoss[0m : 1.43232
[1mStep[0m  [64/169], [94mLoss[0m : 1.31033
[1mStep[0m  [80/169], [94mLoss[0m : 1.50296
[1mStep[0m  [96/169], [94mLoss[0m : 1.48553
[1mStep[0m  [112/169], [94mLoss[0m : 1.84632
[1mStep[0m  [128/169], [94mLoss[0m : 1.56059
[1mStep[0m  [144/169], [94mLoss[0m : 1.66856
[1mStep[0m  [160/169], [94mLoss[0m : 1.77102

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.605, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40487
[1mStep[0m  [16/169], [94mLoss[0m : 1.69993
[1mStep[0m  [32/169], [94mLoss[0m : 1.39707
[1mStep[0m  [48/169], [94mLoss[0m : 1.58480
[1mStep[0m  [64/169], [94mLoss[0m : 1.47092
[1mStep[0m  [80/169], [94mLoss[0m : 1.23370
[1mStep[0m  [96/169], [94mLoss[0m : 1.65100
[1mStep[0m  [112/169], [94mLoss[0m : 1.37590
[1mStep[0m  [128/169], [94mLoss[0m : 1.52706
[1mStep[0m  [144/169], [94mLoss[0m : 1.78600
[1mStep[0m  [160/169], [94mLoss[0m : 1.61528

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.593, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67638
[1mStep[0m  [16/169], [94mLoss[0m : 1.73870
[1mStep[0m  [32/169], [94mLoss[0m : 1.30697
[1mStep[0m  [48/169], [94mLoss[0m : 1.52526
[1mStep[0m  [64/169], [94mLoss[0m : 1.51412
[1mStep[0m  [80/169], [94mLoss[0m : 1.51186
[1mStep[0m  [96/169], [94mLoss[0m : 1.61042
[1mStep[0m  [112/169], [94mLoss[0m : 1.34987
[1mStep[0m  [128/169], [94mLoss[0m : 1.51574
[1mStep[0m  [144/169], [94mLoss[0m : 1.85400
[1mStep[0m  [160/169], [94mLoss[0m : 1.41254

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.505, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38678
[1mStep[0m  [16/169], [94mLoss[0m : 1.42197
[1mStep[0m  [32/169], [94mLoss[0m : 1.67138
[1mStep[0m  [48/169], [94mLoss[0m : 1.64516
[1mStep[0m  [64/169], [94mLoss[0m : 1.47619
[1mStep[0m  [80/169], [94mLoss[0m : 1.38357
[1mStep[0m  [96/169], [94mLoss[0m : 1.20646
[1mStep[0m  [112/169], [94mLoss[0m : 1.82218
[1mStep[0m  [128/169], [94mLoss[0m : 1.44045
[1mStep[0m  [144/169], [94mLoss[0m : 1.73189
[1mStep[0m  [160/169], [94mLoss[0m : 1.65543

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56061
[1mStep[0m  [16/169], [94mLoss[0m : 1.58263
[1mStep[0m  [32/169], [94mLoss[0m : 1.46881
[1mStep[0m  [48/169], [94mLoss[0m : 1.20522
[1mStep[0m  [64/169], [94mLoss[0m : 1.54612
[1mStep[0m  [80/169], [94mLoss[0m : 1.25776
[1mStep[0m  [96/169], [94mLoss[0m : 1.73825
[1mStep[0m  [112/169], [94mLoss[0m : 1.69234
[1mStep[0m  [128/169], [94mLoss[0m : 1.44612
[1mStep[0m  [144/169], [94mLoss[0m : 1.76820
[1mStep[0m  [160/169], [94mLoss[0m : 1.32264

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.503, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59346
[1mStep[0m  [16/169], [94mLoss[0m : 1.46819
[1mStep[0m  [32/169], [94mLoss[0m : 1.46578
[1mStep[0m  [48/169], [94mLoss[0m : 1.69083
[1mStep[0m  [64/169], [94mLoss[0m : 1.73799
[1mStep[0m  [80/169], [94mLoss[0m : 1.39015
[1mStep[0m  [96/169], [94mLoss[0m : 1.37542
[1mStep[0m  [112/169], [94mLoss[0m : 1.49329
[1mStep[0m  [128/169], [94mLoss[0m : 1.31313
[1mStep[0m  [144/169], [94mLoss[0m : 1.83164
[1mStep[0m  [160/169], [94mLoss[0m : 1.49685

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.489, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39167
[1mStep[0m  [16/169], [94mLoss[0m : 1.65435
[1mStep[0m  [32/169], [94mLoss[0m : 1.54512
[1mStep[0m  [48/169], [94mLoss[0m : 1.39233
[1mStep[0m  [64/169], [94mLoss[0m : 1.95427
[1mStep[0m  [80/169], [94mLoss[0m : 1.23447
[1mStep[0m  [96/169], [94mLoss[0m : 1.56071
[1mStep[0m  [112/169], [94mLoss[0m : 1.47917
[1mStep[0m  [128/169], [94mLoss[0m : 1.18922
[1mStep[0m  [144/169], [94mLoss[0m : 1.29168
[1mStep[0m  [160/169], [94mLoss[0m : 1.61611

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.428, [92mTest[0m: 2.460, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55429
[1mStep[0m  [16/169], [94mLoss[0m : 1.31475
[1mStep[0m  [32/169], [94mLoss[0m : 1.05124
[1mStep[0m  [48/169], [94mLoss[0m : 1.28894
[1mStep[0m  [64/169], [94mLoss[0m : 1.24099
[1mStep[0m  [80/169], [94mLoss[0m : 1.24338
[1mStep[0m  [96/169], [94mLoss[0m : 1.56888
[1mStep[0m  [112/169], [94mLoss[0m : 1.65978
[1mStep[0m  [128/169], [94mLoss[0m : 1.52493
[1mStep[0m  [144/169], [94mLoss[0m : 1.54796
[1mStep[0m  [160/169], [94mLoss[0m : 1.40733

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.403, [92mTest[0m: 2.537, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41279
[1mStep[0m  [16/169], [94mLoss[0m : 1.27647
[1mStep[0m  [32/169], [94mLoss[0m : 1.45033
[1mStep[0m  [48/169], [94mLoss[0m : 1.29948
[1mStep[0m  [64/169], [94mLoss[0m : 1.19461
[1mStep[0m  [80/169], [94mLoss[0m : 1.24255
[1mStep[0m  [96/169], [94mLoss[0m : 1.62331
[1mStep[0m  [112/169], [94mLoss[0m : 1.24054
[1mStep[0m  [128/169], [94mLoss[0m : 1.11484
[1mStep[0m  [144/169], [94mLoss[0m : 1.82958
[1mStep[0m  [160/169], [94mLoss[0m : 1.43076

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.366, [92mTest[0m: 2.478, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55409
[1mStep[0m  [16/169], [94mLoss[0m : 1.44767
[1mStep[0m  [32/169], [94mLoss[0m : 1.14658
[1mStep[0m  [48/169], [94mLoss[0m : 1.40964
[1mStep[0m  [64/169], [94mLoss[0m : 1.45432
[1mStep[0m  [80/169], [94mLoss[0m : 1.46675
[1mStep[0m  [96/169], [94mLoss[0m : 1.38724
[1mStep[0m  [112/169], [94mLoss[0m : 1.43953
[1mStep[0m  [128/169], [94mLoss[0m : 1.64726
[1mStep[0m  [144/169], [94mLoss[0m : 1.34345
[1mStep[0m  [160/169], [94mLoss[0m : 1.13804

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.341, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.502
====================================

Phase 2 - Evaluation MAE:  2.5022576110703603
MAE score P1      2.347549
MAE score P2      2.502258
loss              1.340703
learning_rate         0.01
batch_size              64
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 11.69671
[1mStep[0m  [16/169], [94mLoss[0m : 8.25921
[1mStep[0m  [32/169], [94mLoss[0m : 5.93484
[1mStep[0m  [48/169], [94mLoss[0m : 3.37814
[1mStep[0m  [64/169], [94mLoss[0m : 3.45524
[1mStep[0m  [80/169], [94mLoss[0m : 2.97941
[1mStep[0m  [96/169], [94mLoss[0m : 3.12929
[1mStep[0m  [112/169], [94mLoss[0m : 2.35797
[1mStep[0m  [128/169], [94mLoss[0m : 3.10111
[1mStep[0m  [144/169], [94mLoss[0m : 2.76016
[1mStep[0m  [160/169], [94mLoss[0m : 2.44036

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.061, [92mTest[0m: 11.062, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70281
[1mStep[0m  [16/169], [94mLoss[0m : 2.39826
[1mStep[0m  [32/169], [94mLoss[0m : 3.00321
[1mStep[0m  [48/169], [94mLoss[0m : 2.85914
[1mStep[0m  [64/169], [94mLoss[0m : 2.50630
[1mStep[0m  [80/169], [94mLoss[0m : 2.32238
[1mStep[0m  [96/169], [94mLoss[0m : 2.55012
[1mStep[0m  [112/169], [94mLoss[0m : 2.54277
[1mStep[0m  [128/169], [94mLoss[0m : 2.79583
[1mStep[0m  [144/169], [94mLoss[0m : 2.80622
[1mStep[0m  [160/169], [94mLoss[0m : 2.38692

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34531
[1mStep[0m  [16/169], [94mLoss[0m : 2.66844
[1mStep[0m  [32/169], [94mLoss[0m : 2.64633
[1mStep[0m  [48/169], [94mLoss[0m : 2.36430
[1mStep[0m  [64/169], [94mLoss[0m : 2.19681
[1mStep[0m  [80/169], [94mLoss[0m : 2.69821
[1mStep[0m  [96/169], [94mLoss[0m : 2.64428
[1mStep[0m  [112/169], [94mLoss[0m : 2.56609
[1mStep[0m  [128/169], [94mLoss[0m : 2.48489
[1mStep[0m  [144/169], [94mLoss[0m : 2.32837
[1mStep[0m  [160/169], [94mLoss[0m : 2.92031

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36740
[1mStep[0m  [16/169], [94mLoss[0m : 2.53890
[1mStep[0m  [32/169], [94mLoss[0m : 2.74088
[1mStep[0m  [48/169], [94mLoss[0m : 2.93654
[1mStep[0m  [64/169], [94mLoss[0m : 2.76831
[1mStep[0m  [80/169], [94mLoss[0m : 2.70874
[1mStep[0m  [96/169], [94mLoss[0m : 2.11144
[1mStep[0m  [112/169], [94mLoss[0m : 2.60930
[1mStep[0m  [128/169], [94mLoss[0m : 3.09327
[1mStep[0m  [144/169], [94mLoss[0m : 2.60498
[1mStep[0m  [160/169], [94mLoss[0m : 2.32675

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55409
[1mStep[0m  [16/169], [94mLoss[0m : 3.14104
[1mStep[0m  [32/169], [94mLoss[0m : 2.76862
[1mStep[0m  [48/169], [94mLoss[0m : 2.37799
[1mStep[0m  [64/169], [94mLoss[0m : 3.19096
[1mStep[0m  [80/169], [94mLoss[0m : 2.48236
[1mStep[0m  [96/169], [94mLoss[0m : 2.77147
[1mStep[0m  [112/169], [94mLoss[0m : 2.36832
[1mStep[0m  [128/169], [94mLoss[0m : 2.38974
[1mStep[0m  [144/169], [94mLoss[0m : 2.60006
[1mStep[0m  [160/169], [94mLoss[0m : 2.59908

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29839
[1mStep[0m  [16/169], [94mLoss[0m : 2.51734
[1mStep[0m  [32/169], [94mLoss[0m : 2.56024
[1mStep[0m  [48/169], [94mLoss[0m : 2.44879
[1mStep[0m  [64/169], [94mLoss[0m : 2.65707
[1mStep[0m  [80/169], [94mLoss[0m : 2.64634
[1mStep[0m  [96/169], [94mLoss[0m : 2.61520
[1mStep[0m  [112/169], [94mLoss[0m : 2.26407
[1mStep[0m  [128/169], [94mLoss[0m : 2.77417
[1mStep[0m  [144/169], [94mLoss[0m : 2.62919
[1mStep[0m  [160/169], [94mLoss[0m : 2.63422

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47971
[1mStep[0m  [16/169], [94mLoss[0m : 2.42065
[1mStep[0m  [32/169], [94mLoss[0m : 2.63486
[1mStep[0m  [48/169], [94mLoss[0m : 2.88547
[1mStep[0m  [64/169], [94mLoss[0m : 2.51335
[1mStep[0m  [80/169], [94mLoss[0m : 2.79816
[1mStep[0m  [96/169], [94mLoss[0m : 2.82058
[1mStep[0m  [112/169], [94mLoss[0m : 2.87596
[1mStep[0m  [128/169], [94mLoss[0m : 2.74020
[1mStep[0m  [144/169], [94mLoss[0m : 2.35972
[1mStep[0m  [160/169], [94mLoss[0m : 2.53974

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34483
[1mStep[0m  [16/169], [94mLoss[0m : 2.03007
[1mStep[0m  [32/169], [94mLoss[0m : 2.61999
[1mStep[0m  [48/169], [94mLoss[0m : 2.47643
[1mStep[0m  [64/169], [94mLoss[0m : 2.05840
[1mStep[0m  [80/169], [94mLoss[0m : 2.49533
[1mStep[0m  [96/169], [94mLoss[0m : 2.41319
[1mStep[0m  [112/169], [94mLoss[0m : 2.37919
[1mStep[0m  [128/169], [94mLoss[0m : 2.42133
[1mStep[0m  [144/169], [94mLoss[0m : 2.90967
[1mStep[0m  [160/169], [94mLoss[0m : 2.54845

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.359, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34963
[1mStep[0m  [16/169], [94mLoss[0m : 2.32564
[1mStep[0m  [32/169], [94mLoss[0m : 2.61720
[1mStep[0m  [48/169], [94mLoss[0m : 2.43077
[1mStep[0m  [64/169], [94mLoss[0m : 2.38818
[1mStep[0m  [80/169], [94mLoss[0m : 2.29917
[1mStep[0m  [96/169], [94mLoss[0m : 2.74067
[1mStep[0m  [112/169], [94mLoss[0m : 2.65241
[1mStep[0m  [128/169], [94mLoss[0m : 2.22396
[1mStep[0m  [144/169], [94mLoss[0m : 2.95759
[1mStep[0m  [160/169], [94mLoss[0m : 2.70222

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34986
[1mStep[0m  [16/169], [94mLoss[0m : 2.88859
[1mStep[0m  [32/169], [94mLoss[0m : 2.53739
[1mStep[0m  [48/169], [94mLoss[0m : 2.46830
[1mStep[0m  [64/169], [94mLoss[0m : 2.63741
[1mStep[0m  [80/169], [94mLoss[0m : 2.25570
[1mStep[0m  [96/169], [94mLoss[0m : 2.24510
[1mStep[0m  [112/169], [94mLoss[0m : 2.45822
[1mStep[0m  [128/169], [94mLoss[0m : 2.67471
[1mStep[0m  [144/169], [94mLoss[0m : 3.19156
[1mStep[0m  [160/169], [94mLoss[0m : 2.37440

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34090
[1mStep[0m  [16/169], [94mLoss[0m : 2.29841
[1mStep[0m  [32/169], [94mLoss[0m : 2.87354
[1mStep[0m  [48/169], [94mLoss[0m : 2.89362
[1mStep[0m  [64/169], [94mLoss[0m : 2.33627
[1mStep[0m  [80/169], [94mLoss[0m : 2.67130
[1mStep[0m  [96/169], [94mLoss[0m : 2.34348
[1mStep[0m  [112/169], [94mLoss[0m : 2.49054
[1mStep[0m  [128/169], [94mLoss[0m : 2.05542
[1mStep[0m  [144/169], [94mLoss[0m : 2.62343
[1mStep[0m  [160/169], [94mLoss[0m : 2.58628

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45685
[1mStep[0m  [16/169], [94mLoss[0m : 2.32690
[1mStep[0m  [32/169], [94mLoss[0m : 2.37774
[1mStep[0m  [48/169], [94mLoss[0m : 2.31560
[1mStep[0m  [64/169], [94mLoss[0m : 2.58265
[1mStep[0m  [80/169], [94mLoss[0m : 2.49255
[1mStep[0m  [96/169], [94mLoss[0m : 2.41827
[1mStep[0m  [112/169], [94mLoss[0m : 2.52888
[1mStep[0m  [128/169], [94mLoss[0m : 2.33225
[1mStep[0m  [144/169], [94mLoss[0m : 2.50967
[1mStep[0m  [160/169], [94mLoss[0m : 2.18572

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.378, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82265
[1mStep[0m  [16/169], [94mLoss[0m : 2.33051
[1mStep[0m  [32/169], [94mLoss[0m : 2.52981
[1mStep[0m  [48/169], [94mLoss[0m : 2.22327
[1mStep[0m  [64/169], [94mLoss[0m : 2.46035
[1mStep[0m  [80/169], [94mLoss[0m : 2.29874
[1mStep[0m  [96/169], [94mLoss[0m : 2.75420
[1mStep[0m  [112/169], [94mLoss[0m : 3.21711
[1mStep[0m  [128/169], [94mLoss[0m : 2.46298
[1mStep[0m  [144/169], [94mLoss[0m : 2.03413
[1mStep[0m  [160/169], [94mLoss[0m : 2.95136

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.312, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51432
[1mStep[0m  [16/169], [94mLoss[0m : 2.33432
[1mStep[0m  [32/169], [94mLoss[0m : 2.75722
[1mStep[0m  [48/169], [94mLoss[0m : 2.54270
[1mStep[0m  [64/169], [94mLoss[0m : 2.56940
[1mStep[0m  [80/169], [94mLoss[0m : 2.25011
[1mStep[0m  [96/169], [94mLoss[0m : 2.57089
[1mStep[0m  [112/169], [94mLoss[0m : 2.70120
[1mStep[0m  [128/169], [94mLoss[0m : 2.35263
[1mStep[0m  [144/169], [94mLoss[0m : 2.99036
[1mStep[0m  [160/169], [94mLoss[0m : 2.69671

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.337, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32706
[1mStep[0m  [16/169], [94mLoss[0m : 2.44572
[1mStep[0m  [32/169], [94mLoss[0m : 2.01711
[1mStep[0m  [48/169], [94mLoss[0m : 2.40359
[1mStep[0m  [64/169], [94mLoss[0m : 2.30426
[1mStep[0m  [80/169], [94mLoss[0m : 2.61007
[1mStep[0m  [96/169], [94mLoss[0m : 2.47669
[1mStep[0m  [112/169], [94mLoss[0m : 2.14748
[1mStep[0m  [128/169], [94mLoss[0m : 3.00958
[1mStep[0m  [144/169], [94mLoss[0m : 2.45727
[1mStep[0m  [160/169], [94mLoss[0m : 2.63585

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45321
[1mStep[0m  [16/169], [94mLoss[0m : 2.53440
[1mStep[0m  [32/169], [94mLoss[0m : 2.50599
[1mStep[0m  [48/169], [94mLoss[0m : 2.65529
[1mStep[0m  [64/169], [94mLoss[0m : 2.52838
[1mStep[0m  [80/169], [94mLoss[0m : 2.00135
[1mStep[0m  [96/169], [94mLoss[0m : 2.34942
[1mStep[0m  [112/169], [94mLoss[0m : 2.51709
[1mStep[0m  [128/169], [94mLoss[0m : 2.39801
[1mStep[0m  [144/169], [94mLoss[0m : 2.28735
[1mStep[0m  [160/169], [94mLoss[0m : 2.42422

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.311, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68870
[1mStep[0m  [16/169], [94mLoss[0m : 2.73308
[1mStep[0m  [32/169], [94mLoss[0m : 2.23236
[1mStep[0m  [48/169], [94mLoss[0m : 2.21353
[1mStep[0m  [64/169], [94mLoss[0m : 2.65091
[1mStep[0m  [80/169], [94mLoss[0m : 2.54079
[1mStep[0m  [96/169], [94mLoss[0m : 2.01594
[1mStep[0m  [112/169], [94mLoss[0m : 2.22851
[1mStep[0m  [128/169], [94mLoss[0m : 2.09749
[1mStep[0m  [144/169], [94mLoss[0m : 2.37973
[1mStep[0m  [160/169], [94mLoss[0m : 2.03564

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.380, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54224
[1mStep[0m  [16/169], [94mLoss[0m : 2.59021
[1mStep[0m  [32/169], [94mLoss[0m : 2.19339
[1mStep[0m  [48/169], [94mLoss[0m : 2.52920
[1mStep[0m  [64/169], [94mLoss[0m : 2.44958
[1mStep[0m  [80/169], [94mLoss[0m : 2.23767
[1mStep[0m  [96/169], [94mLoss[0m : 2.45159
[1mStep[0m  [112/169], [94mLoss[0m : 2.75690
[1mStep[0m  [128/169], [94mLoss[0m : 2.76047
[1mStep[0m  [144/169], [94mLoss[0m : 2.71920
[1mStep[0m  [160/169], [94mLoss[0m : 2.67205

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.311, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23726
[1mStep[0m  [16/169], [94mLoss[0m : 2.29697
[1mStep[0m  [32/169], [94mLoss[0m : 2.74809
[1mStep[0m  [48/169], [94mLoss[0m : 2.50183
[1mStep[0m  [64/169], [94mLoss[0m : 1.82736
[1mStep[0m  [80/169], [94mLoss[0m : 2.28677
[1mStep[0m  [96/169], [94mLoss[0m : 2.38809
[1mStep[0m  [112/169], [94mLoss[0m : 2.33980
[1mStep[0m  [128/169], [94mLoss[0m : 2.09548
[1mStep[0m  [144/169], [94mLoss[0m : 2.69528
[1mStep[0m  [160/169], [94mLoss[0m : 2.50692

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.306, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79032
[1mStep[0m  [16/169], [94mLoss[0m : 2.01736
[1mStep[0m  [32/169], [94mLoss[0m : 2.66558
[1mStep[0m  [48/169], [94mLoss[0m : 2.82617
[1mStep[0m  [64/169], [94mLoss[0m : 2.54984
[1mStep[0m  [80/169], [94mLoss[0m : 2.34679
[1mStep[0m  [96/169], [94mLoss[0m : 2.48349
[1mStep[0m  [112/169], [94mLoss[0m : 2.20423
[1mStep[0m  [128/169], [94mLoss[0m : 2.35116
[1mStep[0m  [144/169], [94mLoss[0m : 2.48608
[1mStep[0m  [160/169], [94mLoss[0m : 2.62014

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.299, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91886
[1mStep[0m  [16/169], [94mLoss[0m : 2.32569
[1mStep[0m  [32/169], [94mLoss[0m : 2.59796
[1mStep[0m  [48/169], [94mLoss[0m : 2.30850
[1mStep[0m  [64/169], [94mLoss[0m : 2.32545
[1mStep[0m  [80/169], [94mLoss[0m : 2.42738
[1mStep[0m  [96/169], [94mLoss[0m : 2.26784
[1mStep[0m  [112/169], [94mLoss[0m : 2.31375
[1mStep[0m  [128/169], [94mLoss[0m : 2.45247
[1mStep[0m  [144/169], [94mLoss[0m : 1.96305
[1mStep[0m  [160/169], [94mLoss[0m : 2.39850

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55633
[1mStep[0m  [16/169], [94mLoss[0m : 2.90245
[1mStep[0m  [32/169], [94mLoss[0m : 2.39139
[1mStep[0m  [48/169], [94mLoss[0m : 2.48823
[1mStep[0m  [64/169], [94mLoss[0m : 2.14651
[1mStep[0m  [80/169], [94mLoss[0m : 2.17589
[1mStep[0m  [96/169], [94mLoss[0m : 2.57368
[1mStep[0m  [112/169], [94mLoss[0m : 2.59521
[1mStep[0m  [128/169], [94mLoss[0m : 2.52269
[1mStep[0m  [144/169], [94mLoss[0m : 3.08038
[1mStep[0m  [160/169], [94mLoss[0m : 2.04313

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.302, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53250
[1mStep[0m  [16/169], [94mLoss[0m : 2.51326
[1mStep[0m  [32/169], [94mLoss[0m : 2.23417
[1mStep[0m  [48/169], [94mLoss[0m : 2.29751
[1mStep[0m  [64/169], [94mLoss[0m : 2.19663
[1mStep[0m  [80/169], [94mLoss[0m : 2.58702
[1mStep[0m  [96/169], [94mLoss[0m : 2.45697
[1mStep[0m  [112/169], [94mLoss[0m : 2.39094
[1mStep[0m  [128/169], [94mLoss[0m : 2.84361
[1mStep[0m  [144/169], [94mLoss[0m : 2.26838
[1mStep[0m  [160/169], [94mLoss[0m : 1.98128

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42167
[1mStep[0m  [16/169], [94mLoss[0m : 2.61162
[1mStep[0m  [32/169], [94mLoss[0m : 2.35194
[1mStep[0m  [48/169], [94mLoss[0m : 2.49623
[1mStep[0m  [64/169], [94mLoss[0m : 2.25427
[1mStep[0m  [80/169], [94mLoss[0m : 2.13459
[1mStep[0m  [96/169], [94mLoss[0m : 2.62727
[1mStep[0m  [112/169], [94mLoss[0m : 2.61012
[1mStep[0m  [128/169], [94mLoss[0m : 1.99798
[1mStep[0m  [144/169], [94mLoss[0m : 2.05251
[1mStep[0m  [160/169], [94mLoss[0m : 2.39796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44867
[1mStep[0m  [16/169], [94mLoss[0m : 2.31265
[1mStep[0m  [32/169], [94mLoss[0m : 2.25548
[1mStep[0m  [48/169], [94mLoss[0m : 2.29632
[1mStep[0m  [64/169], [94mLoss[0m : 2.73846
[1mStep[0m  [80/169], [94mLoss[0m : 2.11200
[1mStep[0m  [96/169], [94mLoss[0m : 2.68714
[1mStep[0m  [112/169], [94mLoss[0m : 2.22785
[1mStep[0m  [128/169], [94mLoss[0m : 2.90630
[1mStep[0m  [144/169], [94mLoss[0m : 2.46584
[1mStep[0m  [160/169], [94mLoss[0m : 1.84797

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.303, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62406
[1mStep[0m  [16/169], [94mLoss[0m : 2.52749
[1mStep[0m  [32/169], [94mLoss[0m : 2.71786
[1mStep[0m  [48/169], [94mLoss[0m : 2.29006
[1mStep[0m  [64/169], [94mLoss[0m : 2.33810
[1mStep[0m  [80/169], [94mLoss[0m : 2.70218
[1mStep[0m  [96/169], [94mLoss[0m : 2.81219
[1mStep[0m  [112/169], [94mLoss[0m : 2.07491
[1mStep[0m  [128/169], [94mLoss[0m : 2.32780
[1mStep[0m  [144/169], [94mLoss[0m : 2.03234
[1mStep[0m  [160/169], [94mLoss[0m : 2.15213

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.285, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95015
[1mStep[0m  [16/169], [94mLoss[0m : 2.20423
[1mStep[0m  [32/169], [94mLoss[0m : 3.12512
[1mStep[0m  [48/169], [94mLoss[0m : 2.38820
[1mStep[0m  [64/169], [94mLoss[0m : 2.36016
[1mStep[0m  [80/169], [94mLoss[0m : 2.04182
[1mStep[0m  [96/169], [94mLoss[0m : 2.60227
[1mStep[0m  [112/169], [94mLoss[0m : 2.52589
[1mStep[0m  [128/169], [94mLoss[0m : 2.55728
[1mStep[0m  [144/169], [94mLoss[0m : 2.16849
[1mStep[0m  [160/169], [94mLoss[0m : 2.25089

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30256
[1mStep[0m  [16/169], [94mLoss[0m : 2.15183
[1mStep[0m  [32/169], [94mLoss[0m : 2.54553
[1mStep[0m  [48/169], [94mLoss[0m : 2.38901
[1mStep[0m  [64/169], [94mLoss[0m : 2.30369
[1mStep[0m  [80/169], [94mLoss[0m : 2.81907
[1mStep[0m  [96/169], [94mLoss[0m : 2.04325
[1mStep[0m  [112/169], [94mLoss[0m : 2.38977
[1mStep[0m  [128/169], [94mLoss[0m : 2.02821
[1mStep[0m  [144/169], [94mLoss[0m : 2.51467
[1mStep[0m  [160/169], [94mLoss[0m : 2.25628

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80150
[1mStep[0m  [16/169], [94mLoss[0m : 2.25638
[1mStep[0m  [32/169], [94mLoss[0m : 2.26021
[1mStep[0m  [48/169], [94mLoss[0m : 2.52973
[1mStep[0m  [64/169], [94mLoss[0m : 2.42109
[1mStep[0m  [80/169], [94mLoss[0m : 2.51777
[1mStep[0m  [96/169], [94mLoss[0m : 2.87493
[1mStep[0m  [112/169], [94mLoss[0m : 2.41811
[1mStep[0m  [128/169], [94mLoss[0m : 2.63950
[1mStep[0m  [144/169], [94mLoss[0m : 2.26115
[1mStep[0m  [160/169], [94mLoss[0m : 2.18663

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.304, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41354
[1mStep[0m  [16/169], [94mLoss[0m : 2.21439
[1mStep[0m  [32/169], [94mLoss[0m : 2.65798
[1mStep[0m  [48/169], [94mLoss[0m : 2.29911
[1mStep[0m  [64/169], [94mLoss[0m : 1.70032
[1mStep[0m  [80/169], [94mLoss[0m : 2.59824
[1mStep[0m  [96/169], [94mLoss[0m : 2.36796
[1mStep[0m  [112/169], [94mLoss[0m : 2.42897
[1mStep[0m  [128/169], [94mLoss[0m : 2.48363
[1mStep[0m  [144/169], [94mLoss[0m : 2.20602
[1mStep[0m  [160/169], [94mLoss[0m : 2.70260

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.296, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.309
====================================

Phase 1 - Evaluation MAE:  2.3092446625232697
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.49220
[1mStep[0m  [16/169], [94mLoss[0m : 2.93055
[1mStep[0m  [32/169], [94mLoss[0m : 2.34074
[1mStep[0m  [48/169], [94mLoss[0m : 2.39021
[1mStep[0m  [64/169], [94mLoss[0m : 2.80026
[1mStep[0m  [80/169], [94mLoss[0m : 2.66092
[1mStep[0m  [96/169], [94mLoss[0m : 2.69626
[1mStep[0m  [112/169], [94mLoss[0m : 2.26002
[1mStep[0m  [128/169], [94mLoss[0m : 2.70736
[1mStep[0m  [144/169], [94mLoss[0m : 2.67641
[1mStep[0m  [160/169], [94mLoss[0m : 2.77339

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.308, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31517
[1mStep[0m  [16/169], [94mLoss[0m : 2.17707
[1mStep[0m  [32/169], [94mLoss[0m : 2.62230
[1mStep[0m  [48/169], [94mLoss[0m : 2.45214
[1mStep[0m  [64/169], [94mLoss[0m : 2.42573
[1mStep[0m  [80/169], [94mLoss[0m : 2.38014
[1mStep[0m  [96/169], [94mLoss[0m : 2.42881
[1mStep[0m  [112/169], [94mLoss[0m : 2.31311
[1mStep[0m  [128/169], [94mLoss[0m : 2.15429
[1mStep[0m  [144/169], [94mLoss[0m : 2.42868
[1mStep[0m  [160/169], [94mLoss[0m : 2.03661

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72722
[1mStep[0m  [16/169], [94mLoss[0m : 2.00491
[1mStep[0m  [32/169], [94mLoss[0m : 2.20543
[1mStep[0m  [48/169], [94mLoss[0m : 2.53201
[1mStep[0m  [64/169], [94mLoss[0m : 2.48073
[1mStep[0m  [80/169], [94mLoss[0m : 2.42028
[1mStep[0m  [96/169], [94mLoss[0m : 2.52186
[1mStep[0m  [112/169], [94mLoss[0m : 2.19221
[1mStep[0m  [128/169], [94mLoss[0m : 2.04466
[1mStep[0m  [144/169], [94mLoss[0m : 2.77195
[1mStep[0m  [160/169], [94mLoss[0m : 1.93513

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.374, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01323
[1mStep[0m  [16/169], [94mLoss[0m : 1.95700
[1mStep[0m  [32/169], [94mLoss[0m : 2.74197
[1mStep[0m  [48/169], [94mLoss[0m : 2.10257
[1mStep[0m  [64/169], [94mLoss[0m : 1.82712
[1mStep[0m  [80/169], [94mLoss[0m : 2.36195
[1mStep[0m  [96/169], [94mLoss[0m : 1.70292
[1mStep[0m  [112/169], [94mLoss[0m : 2.29035
[1mStep[0m  [128/169], [94mLoss[0m : 2.15449
[1mStep[0m  [144/169], [94mLoss[0m : 2.63265
[1mStep[0m  [160/169], [94mLoss[0m : 2.17603

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82010
[1mStep[0m  [16/169], [94mLoss[0m : 2.28030
[1mStep[0m  [32/169], [94mLoss[0m : 2.48756
[1mStep[0m  [48/169], [94mLoss[0m : 2.11328
[1mStep[0m  [64/169], [94mLoss[0m : 2.89939
[1mStep[0m  [80/169], [94mLoss[0m : 2.49138
[1mStep[0m  [96/169], [94mLoss[0m : 2.10535
[1mStep[0m  [112/169], [94mLoss[0m : 1.72881
[1mStep[0m  [128/169], [94mLoss[0m : 2.36672
[1mStep[0m  [144/169], [94mLoss[0m : 2.22273
[1mStep[0m  [160/169], [94mLoss[0m : 1.95185

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.180, [92mTest[0m: 2.382, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14866
[1mStep[0m  [16/169], [94mLoss[0m : 2.24436
[1mStep[0m  [32/169], [94mLoss[0m : 2.41871
[1mStep[0m  [48/169], [94mLoss[0m : 2.69848
[1mStep[0m  [64/169], [94mLoss[0m : 1.75644
[1mStep[0m  [80/169], [94mLoss[0m : 1.92805
[1mStep[0m  [96/169], [94mLoss[0m : 2.80140
[1mStep[0m  [112/169], [94mLoss[0m : 1.64383
[1mStep[0m  [128/169], [94mLoss[0m : 1.82311
[1mStep[0m  [144/169], [94mLoss[0m : 2.16081
[1mStep[0m  [160/169], [94mLoss[0m : 2.78512

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41064
[1mStep[0m  [16/169], [94mLoss[0m : 2.02010
[1mStep[0m  [32/169], [94mLoss[0m : 1.93617
[1mStep[0m  [48/169], [94mLoss[0m : 2.00769
[1mStep[0m  [64/169], [94mLoss[0m : 2.03529
[1mStep[0m  [80/169], [94mLoss[0m : 2.14412
[1mStep[0m  [96/169], [94mLoss[0m : 2.17865
[1mStep[0m  [112/169], [94mLoss[0m : 2.74783
[1mStep[0m  [128/169], [94mLoss[0m : 2.21111
[1mStep[0m  [144/169], [94mLoss[0m : 2.45289
[1mStep[0m  [160/169], [94mLoss[0m : 2.36637

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75297
[1mStep[0m  [16/169], [94mLoss[0m : 1.64832
[1mStep[0m  [32/169], [94mLoss[0m : 2.50809
[1mStep[0m  [48/169], [94mLoss[0m : 1.95735
[1mStep[0m  [64/169], [94mLoss[0m : 1.79145
[1mStep[0m  [80/169], [94mLoss[0m : 1.82310
[1mStep[0m  [96/169], [94mLoss[0m : 1.95856
[1mStep[0m  [112/169], [94mLoss[0m : 2.31069
[1mStep[0m  [128/169], [94mLoss[0m : 2.03854
[1mStep[0m  [144/169], [94mLoss[0m : 2.21133
[1mStep[0m  [160/169], [94mLoss[0m : 1.95641

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.023, [92mTest[0m: 2.420, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03108
[1mStep[0m  [16/169], [94mLoss[0m : 2.12191
[1mStep[0m  [32/169], [94mLoss[0m : 1.94332
[1mStep[0m  [48/169], [94mLoss[0m : 2.16321
[1mStep[0m  [64/169], [94mLoss[0m : 2.32072
[1mStep[0m  [80/169], [94mLoss[0m : 2.01988
[1mStep[0m  [96/169], [94mLoss[0m : 1.53777
[1mStep[0m  [112/169], [94mLoss[0m : 2.00996
[1mStep[0m  [128/169], [94mLoss[0m : 1.95605
[1mStep[0m  [144/169], [94mLoss[0m : 2.59549
[1mStep[0m  [160/169], [94mLoss[0m : 1.91076

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.438, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01508
[1mStep[0m  [16/169], [94mLoss[0m : 1.80000
[1mStep[0m  [32/169], [94mLoss[0m : 1.69782
[1mStep[0m  [48/169], [94mLoss[0m : 1.74158
[1mStep[0m  [64/169], [94mLoss[0m : 1.75336
[1mStep[0m  [80/169], [94mLoss[0m : 2.40117
[1mStep[0m  [96/169], [94mLoss[0m : 1.60771
[1mStep[0m  [112/169], [94mLoss[0m : 2.11070
[1mStep[0m  [128/169], [94mLoss[0m : 1.92145
[1mStep[0m  [144/169], [94mLoss[0m : 1.90324
[1mStep[0m  [160/169], [94mLoss[0m : 1.69329

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64863
[1mStep[0m  [16/169], [94mLoss[0m : 1.67493
[1mStep[0m  [32/169], [94mLoss[0m : 1.76504
[1mStep[0m  [48/169], [94mLoss[0m : 1.90367
[1mStep[0m  [64/169], [94mLoss[0m : 1.76834
[1mStep[0m  [80/169], [94mLoss[0m : 1.74630
[1mStep[0m  [96/169], [94mLoss[0m : 1.78579
[1mStep[0m  [112/169], [94mLoss[0m : 1.78553
[1mStep[0m  [128/169], [94mLoss[0m : 1.99433
[1mStep[0m  [144/169], [94mLoss[0m : 1.56302
[1mStep[0m  [160/169], [94mLoss[0m : 2.09947

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.391, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52199
[1mStep[0m  [16/169], [94mLoss[0m : 1.66959
[1mStep[0m  [32/169], [94mLoss[0m : 1.81389
[1mStep[0m  [48/169], [94mLoss[0m : 1.86497
[1mStep[0m  [64/169], [94mLoss[0m : 1.84845
[1mStep[0m  [80/169], [94mLoss[0m : 1.88771
[1mStep[0m  [96/169], [94mLoss[0m : 1.78317
[1mStep[0m  [112/169], [94mLoss[0m : 2.15709
[1mStep[0m  [128/169], [94mLoss[0m : 1.84028
[1mStep[0m  [144/169], [94mLoss[0m : 1.97099
[1mStep[0m  [160/169], [94mLoss[0m : 1.70548

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92418
[1mStep[0m  [16/169], [94mLoss[0m : 1.70128
[1mStep[0m  [32/169], [94mLoss[0m : 1.51136
[1mStep[0m  [48/169], [94mLoss[0m : 2.13189
[1mStep[0m  [64/169], [94mLoss[0m : 1.96642
[1mStep[0m  [80/169], [94mLoss[0m : 1.76103
[1mStep[0m  [96/169], [94mLoss[0m : 1.89916
[1mStep[0m  [112/169], [94mLoss[0m : 1.61786
[1mStep[0m  [128/169], [94mLoss[0m : 1.82191
[1mStep[0m  [144/169], [94mLoss[0m : 1.93247
[1mStep[0m  [160/169], [94mLoss[0m : 1.95197

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71976
[1mStep[0m  [16/169], [94mLoss[0m : 2.24790
[1mStep[0m  [32/169], [94mLoss[0m : 1.68732
[1mStep[0m  [48/169], [94mLoss[0m : 1.66372
[1mStep[0m  [64/169], [94mLoss[0m : 1.92045
[1mStep[0m  [80/169], [94mLoss[0m : 2.04616
[1mStep[0m  [96/169], [94mLoss[0m : 1.67323
[1mStep[0m  [112/169], [94mLoss[0m : 2.04799
[1mStep[0m  [128/169], [94mLoss[0m : 1.39401
[1mStep[0m  [144/169], [94mLoss[0m : 1.94839
[1mStep[0m  [160/169], [94mLoss[0m : 1.84440

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.810, [92mTest[0m: 2.434, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73702
[1mStep[0m  [16/169], [94mLoss[0m : 1.54044
[1mStep[0m  [32/169], [94mLoss[0m : 1.78338
[1mStep[0m  [48/169], [94mLoss[0m : 1.42870
[1mStep[0m  [64/169], [94mLoss[0m : 1.82663
[1mStep[0m  [80/169], [94mLoss[0m : 1.76750
[1mStep[0m  [96/169], [94mLoss[0m : 1.69142
[1mStep[0m  [112/169], [94mLoss[0m : 1.75680
[1mStep[0m  [128/169], [94mLoss[0m : 2.00846
[1mStep[0m  [144/169], [94mLoss[0m : 1.62818
[1mStep[0m  [160/169], [94mLoss[0m : 2.06657

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.769, [92mTest[0m: 2.454, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74495
[1mStep[0m  [16/169], [94mLoss[0m : 1.61674
[1mStep[0m  [32/169], [94mLoss[0m : 1.38658
[1mStep[0m  [48/169], [94mLoss[0m : 1.56708
[1mStep[0m  [64/169], [94mLoss[0m : 1.62918
[1mStep[0m  [80/169], [94mLoss[0m : 2.00366
[1mStep[0m  [96/169], [94mLoss[0m : 1.77247
[1mStep[0m  [112/169], [94mLoss[0m : 1.48133
[1mStep[0m  [128/169], [94mLoss[0m : 1.80562
[1mStep[0m  [144/169], [94mLoss[0m : 1.68916
[1mStep[0m  [160/169], [94mLoss[0m : 1.97203

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.738, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83661
[1mStep[0m  [16/169], [94mLoss[0m : 1.24293
[1mStep[0m  [32/169], [94mLoss[0m : 1.55454
[1mStep[0m  [48/169], [94mLoss[0m : 1.46618
[1mStep[0m  [64/169], [94mLoss[0m : 1.90118
[1mStep[0m  [80/169], [94mLoss[0m : 1.80919
[1mStep[0m  [96/169], [94mLoss[0m : 1.66591
[1mStep[0m  [112/169], [94mLoss[0m : 1.81388
[1mStep[0m  [128/169], [94mLoss[0m : 1.87178
[1mStep[0m  [144/169], [94mLoss[0m : 1.72390
[1mStep[0m  [160/169], [94mLoss[0m : 1.72796

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.720, [92mTest[0m: 2.458, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.37278
[1mStep[0m  [16/169], [94mLoss[0m : 1.69691
[1mStep[0m  [32/169], [94mLoss[0m : 1.69919
[1mStep[0m  [48/169], [94mLoss[0m : 1.89570
[1mStep[0m  [64/169], [94mLoss[0m : 1.84963
[1mStep[0m  [80/169], [94mLoss[0m : 1.99007
[1mStep[0m  [96/169], [94mLoss[0m : 1.56962
[1mStep[0m  [112/169], [94mLoss[0m : 1.47304
[1mStep[0m  [128/169], [94mLoss[0m : 1.83402
[1mStep[0m  [144/169], [94mLoss[0m : 1.77021
[1mStep[0m  [160/169], [94mLoss[0m : 1.60521

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04467
[1mStep[0m  [16/169], [94mLoss[0m : 1.74946
[1mStep[0m  [32/169], [94mLoss[0m : 1.38881
[1mStep[0m  [48/169], [94mLoss[0m : 1.93588
[1mStep[0m  [64/169], [94mLoss[0m : 1.91385
[1mStep[0m  [80/169], [94mLoss[0m : 1.58180
[1mStep[0m  [96/169], [94mLoss[0m : 1.64773
[1mStep[0m  [112/169], [94mLoss[0m : 1.78610
[1mStep[0m  [128/169], [94mLoss[0m : 1.98264
[1mStep[0m  [144/169], [94mLoss[0m : 1.88715
[1mStep[0m  [160/169], [94mLoss[0m : 1.60879

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42709
[1mStep[0m  [16/169], [94mLoss[0m : 1.56458
[1mStep[0m  [32/169], [94mLoss[0m : 1.65649
[1mStep[0m  [48/169], [94mLoss[0m : 1.82660
[1mStep[0m  [64/169], [94mLoss[0m : 1.55574
[1mStep[0m  [80/169], [94mLoss[0m : 1.83306
[1mStep[0m  [96/169], [94mLoss[0m : 1.70190
[1mStep[0m  [112/169], [94mLoss[0m : 1.76743
[1mStep[0m  [128/169], [94mLoss[0m : 1.83166
[1mStep[0m  [144/169], [94mLoss[0m : 1.98835
[1mStep[0m  [160/169], [94mLoss[0m : 1.52785

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56196
[1mStep[0m  [16/169], [94mLoss[0m : 1.48811
[1mStep[0m  [32/169], [94mLoss[0m : 1.58732
[1mStep[0m  [48/169], [94mLoss[0m : 1.85280
[1mStep[0m  [64/169], [94mLoss[0m : 1.92195
[1mStep[0m  [80/169], [94mLoss[0m : 1.39689
[1mStep[0m  [96/169], [94mLoss[0m : 1.95311
[1mStep[0m  [112/169], [94mLoss[0m : 1.76708
[1mStep[0m  [128/169], [94mLoss[0m : 1.41230
[1mStep[0m  [144/169], [94mLoss[0m : 1.77980
[1mStep[0m  [160/169], [94mLoss[0m : 1.63265

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.542, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64807
[1mStep[0m  [16/169], [94mLoss[0m : 1.42699
[1mStep[0m  [32/169], [94mLoss[0m : 1.45699
[1mStep[0m  [48/169], [94mLoss[0m : 1.48349
[1mStep[0m  [64/169], [94mLoss[0m : 1.64753
[1mStep[0m  [80/169], [94mLoss[0m : 1.59673
[1mStep[0m  [96/169], [94mLoss[0m : 1.65648
[1mStep[0m  [112/169], [94mLoss[0m : 1.99826
[1mStep[0m  [128/169], [94mLoss[0m : 1.38076
[1mStep[0m  [144/169], [94mLoss[0m : 1.69259
[1mStep[0m  [160/169], [94mLoss[0m : 1.55688

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36352
[1mStep[0m  [16/169], [94mLoss[0m : 1.31841
[1mStep[0m  [32/169], [94mLoss[0m : 1.44925
[1mStep[0m  [48/169], [94mLoss[0m : 1.75528
[1mStep[0m  [64/169], [94mLoss[0m : 1.44407
[1mStep[0m  [80/169], [94mLoss[0m : 1.65127
[1mStep[0m  [96/169], [94mLoss[0m : 1.53366
[1mStep[0m  [112/169], [94mLoss[0m : 1.60723
[1mStep[0m  [128/169], [94mLoss[0m : 1.45163
[1mStep[0m  [144/169], [94mLoss[0m : 1.38129
[1mStep[0m  [160/169], [94mLoss[0m : 1.41131

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32188
[1mStep[0m  [16/169], [94mLoss[0m : 1.66496
[1mStep[0m  [32/169], [94mLoss[0m : 1.30451
[1mStep[0m  [48/169], [94mLoss[0m : 1.69205
[1mStep[0m  [64/169], [94mLoss[0m : 1.50862
[1mStep[0m  [80/169], [94mLoss[0m : 1.54757
[1mStep[0m  [96/169], [94mLoss[0m : 1.58304
[1mStep[0m  [112/169], [94mLoss[0m : 1.72358
[1mStep[0m  [128/169], [94mLoss[0m : 1.51392
[1mStep[0m  [144/169], [94mLoss[0m : 1.75604
[1mStep[0m  [160/169], [94mLoss[0m : 1.25063

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.572, [92mTest[0m: 2.460, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.44732
[1mStep[0m  [16/169], [94mLoss[0m : 1.94943
[1mStep[0m  [32/169], [94mLoss[0m : 1.78875
[1mStep[0m  [48/169], [94mLoss[0m : 1.46841
[1mStep[0m  [64/169], [94mLoss[0m : 1.38395
[1mStep[0m  [80/169], [94mLoss[0m : 1.56649
[1mStep[0m  [96/169], [94mLoss[0m : 1.45347
[1mStep[0m  [112/169], [94mLoss[0m : 1.52501
[1mStep[0m  [128/169], [94mLoss[0m : 1.28350
[1mStep[0m  [144/169], [94mLoss[0m : 1.55467
[1mStep[0m  [160/169], [94mLoss[0m : 1.66560

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56828
[1mStep[0m  [16/169], [94mLoss[0m : 1.51236
[1mStep[0m  [32/169], [94mLoss[0m : 1.41942
[1mStep[0m  [48/169], [94mLoss[0m : 1.81772
[1mStep[0m  [64/169], [94mLoss[0m : 1.66044
[1mStep[0m  [80/169], [94mLoss[0m : 1.98376
[1mStep[0m  [96/169], [94mLoss[0m : 1.62544
[1mStep[0m  [112/169], [94mLoss[0m : 1.68138
[1mStep[0m  [128/169], [94mLoss[0m : 1.52917
[1mStep[0m  [144/169], [94mLoss[0m : 1.69787
[1mStep[0m  [160/169], [94mLoss[0m : 1.60702

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.25054
[1mStep[0m  [16/169], [94mLoss[0m : 1.57613
[1mStep[0m  [32/169], [94mLoss[0m : 1.44718
[1mStep[0m  [48/169], [94mLoss[0m : 1.41744
[1mStep[0m  [64/169], [94mLoss[0m : 1.38848
[1mStep[0m  [80/169], [94mLoss[0m : 1.48114
[1mStep[0m  [96/169], [94mLoss[0m : 1.42812
[1mStep[0m  [112/169], [94mLoss[0m : 1.27967
[1mStep[0m  [128/169], [94mLoss[0m : 1.51137
[1mStep[0m  [144/169], [94mLoss[0m : 1.71191
[1mStep[0m  [160/169], [94mLoss[0m : 1.55849

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.527, [92mTest[0m: 2.490, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.35264
[1mStep[0m  [16/169], [94mLoss[0m : 1.39284
[1mStep[0m  [32/169], [94mLoss[0m : 1.42868
[1mStep[0m  [48/169], [94mLoss[0m : 1.43964
[1mStep[0m  [64/169], [94mLoss[0m : 1.55168
[1mStep[0m  [80/169], [94mLoss[0m : 1.17310
[1mStep[0m  [96/169], [94mLoss[0m : 1.64180
[1mStep[0m  [112/169], [94mLoss[0m : 1.40884
[1mStep[0m  [128/169], [94mLoss[0m : 1.46754
[1mStep[0m  [144/169], [94mLoss[0m : 1.36526
[1mStep[0m  [160/169], [94mLoss[0m : 1.38583

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40256
[1mStep[0m  [16/169], [94mLoss[0m : 1.41640
[1mStep[0m  [32/169], [94mLoss[0m : 1.49466
[1mStep[0m  [48/169], [94mLoss[0m : 1.86471
[1mStep[0m  [64/169], [94mLoss[0m : 1.64233
[1mStep[0m  [80/169], [94mLoss[0m : 1.54618
[1mStep[0m  [96/169], [94mLoss[0m : 1.33875
[1mStep[0m  [112/169], [94mLoss[0m : 1.31859
[1mStep[0m  [128/169], [94mLoss[0m : 1.57539
[1mStep[0m  [144/169], [94mLoss[0m : 1.47896
[1mStep[0m  [160/169], [94mLoss[0m : 1.53773

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.504, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41480
[1mStep[0m  [16/169], [94mLoss[0m : 1.49667
[1mStep[0m  [32/169], [94mLoss[0m : 1.55617
[1mStep[0m  [48/169], [94mLoss[0m : 1.17417
[1mStep[0m  [64/169], [94mLoss[0m : 1.61498
[1mStep[0m  [80/169], [94mLoss[0m : 1.42348
[1mStep[0m  [96/169], [94mLoss[0m : 1.38897
[1mStep[0m  [112/169], [94mLoss[0m : 1.30581
[1mStep[0m  [128/169], [94mLoss[0m : 1.58463
[1mStep[0m  [144/169], [94mLoss[0m : 1.31409
[1mStep[0m  [160/169], [94mLoss[0m : 1.39638

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.491, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.476
====================================

Phase 2 - Evaluation MAE:  2.475806568350111
MAE score P1      2.309245
MAE score P2      2.475807
loss              1.484231
learning_rate         0.01
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.69724
[1mStep[0m  [33/339], [94mLoss[0m : 10.37912
[1mStep[0m  [66/339], [94mLoss[0m : 8.73269
[1mStep[0m  [99/339], [94mLoss[0m : 6.68341
[1mStep[0m  [132/339], [94mLoss[0m : 6.00632
[1mStep[0m  [165/339], [94mLoss[0m : 5.30829
[1mStep[0m  [198/339], [94mLoss[0m : 3.39591
[1mStep[0m  [231/339], [94mLoss[0m : 2.76463
[1mStep[0m  [264/339], [94mLoss[0m : 2.96350
[1mStep[0m  [297/339], [94mLoss[0m : 2.52899
[1mStep[0m  [330/339], [94mLoss[0m : 2.55761

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.430, [92mTest[0m: 10.762, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15989
[1mStep[0m  [33/339], [94mLoss[0m : 3.50298
[1mStep[0m  [66/339], [94mLoss[0m : 2.50969
[1mStep[0m  [99/339], [94mLoss[0m : 2.78923
[1mStep[0m  [132/339], [94mLoss[0m : 2.83454
[1mStep[0m  [165/339], [94mLoss[0m : 3.16714
[1mStep[0m  [198/339], [94mLoss[0m : 1.96484
[1mStep[0m  [231/339], [94mLoss[0m : 2.34446
[1mStep[0m  [264/339], [94mLoss[0m : 2.97377
[1mStep[0m  [297/339], [94mLoss[0m : 2.23430
[1mStep[0m  [330/339], [94mLoss[0m : 2.21560

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.85859
[1mStep[0m  [33/339], [94mLoss[0m : 2.62072
[1mStep[0m  [66/339], [94mLoss[0m : 2.33997
[1mStep[0m  [99/339], [94mLoss[0m : 2.06747
[1mStep[0m  [132/339], [94mLoss[0m : 2.86647
[1mStep[0m  [165/339], [94mLoss[0m : 3.04336
[1mStep[0m  [198/339], [94mLoss[0m : 2.61406
[1mStep[0m  [231/339], [94mLoss[0m : 2.54982
[1mStep[0m  [264/339], [94mLoss[0m : 2.56526
[1mStep[0m  [297/339], [94mLoss[0m : 2.67787
[1mStep[0m  [330/339], [94mLoss[0m : 2.40334

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30066
[1mStep[0m  [33/339], [94mLoss[0m : 2.34329
[1mStep[0m  [66/339], [94mLoss[0m : 2.66618
[1mStep[0m  [99/339], [94mLoss[0m : 2.36718
[1mStep[0m  [132/339], [94mLoss[0m : 2.47407
[1mStep[0m  [165/339], [94mLoss[0m : 2.24968
[1mStep[0m  [198/339], [94mLoss[0m : 2.69207
[1mStep[0m  [231/339], [94mLoss[0m : 2.87268
[1mStep[0m  [264/339], [94mLoss[0m : 2.20544
[1mStep[0m  [297/339], [94mLoss[0m : 2.87203
[1mStep[0m  [330/339], [94mLoss[0m : 2.54865

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48241
[1mStep[0m  [33/339], [94mLoss[0m : 2.50172
[1mStep[0m  [66/339], [94mLoss[0m : 2.72279
[1mStep[0m  [99/339], [94mLoss[0m : 2.55858
[1mStep[0m  [132/339], [94mLoss[0m : 2.53197
[1mStep[0m  [165/339], [94mLoss[0m : 2.72438
[1mStep[0m  [198/339], [94mLoss[0m : 1.98954
[1mStep[0m  [231/339], [94mLoss[0m : 2.77889
[1mStep[0m  [264/339], [94mLoss[0m : 2.40975
[1mStep[0m  [297/339], [94mLoss[0m : 2.33310
[1mStep[0m  [330/339], [94mLoss[0m : 2.41963

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63526
[1mStep[0m  [33/339], [94mLoss[0m : 2.03994
[1mStep[0m  [66/339], [94mLoss[0m : 2.77575
[1mStep[0m  [99/339], [94mLoss[0m : 2.61020
[1mStep[0m  [132/339], [94mLoss[0m : 2.14722
[1mStep[0m  [165/339], [94mLoss[0m : 2.59419
[1mStep[0m  [198/339], [94mLoss[0m : 3.33308
[1mStep[0m  [231/339], [94mLoss[0m : 2.95234
[1mStep[0m  [264/339], [94mLoss[0m : 2.71114
[1mStep[0m  [297/339], [94mLoss[0m : 2.48365
[1mStep[0m  [330/339], [94mLoss[0m : 2.16703

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23773
[1mStep[0m  [33/339], [94mLoss[0m : 3.04597
[1mStep[0m  [66/339], [94mLoss[0m : 2.06673
[1mStep[0m  [99/339], [94mLoss[0m : 1.94504
[1mStep[0m  [132/339], [94mLoss[0m : 2.23820
[1mStep[0m  [165/339], [94mLoss[0m : 2.89898
[1mStep[0m  [198/339], [94mLoss[0m : 2.53131
[1mStep[0m  [231/339], [94mLoss[0m : 2.55051
[1mStep[0m  [264/339], [94mLoss[0m : 2.65001
[1mStep[0m  [297/339], [94mLoss[0m : 2.29665
[1mStep[0m  [330/339], [94mLoss[0m : 2.86350

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.83036
[1mStep[0m  [33/339], [94mLoss[0m : 2.17644
[1mStep[0m  [66/339], [94mLoss[0m : 2.68925
[1mStep[0m  [99/339], [94mLoss[0m : 2.06245
[1mStep[0m  [132/339], [94mLoss[0m : 2.21234
[1mStep[0m  [165/339], [94mLoss[0m : 2.83226
[1mStep[0m  [198/339], [94mLoss[0m : 2.33573
[1mStep[0m  [231/339], [94mLoss[0m : 2.50579
[1mStep[0m  [264/339], [94mLoss[0m : 2.77184
[1mStep[0m  [297/339], [94mLoss[0m : 3.11622
[1mStep[0m  [330/339], [94mLoss[0m : 1.95178

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56153
[1mStep[0m  [33/339], [94mLoss[0m : 2.38216
[1mStep[0m  [66/339], [94mLoss[0m : 2.67501
[1mStep[0m  [99/339], [94mLoss[0m : 2.26313
[1mStep[0m  [132/339], [94mLoss[0m : 2.44832
[1mStep[0m  [165/339], [94mLoss[0m : 3.07487
[1mStep[0m  [198/339], [94mLoss[0m : 2.17410
[1mStep[0m  [231/339], [94mLoss[0m : 2.60646
[1mStep[0m  [264/339], [94mLoss[0m : 1.74230
[1mStep[0m  [297/339], [94mLoss[0m : 2.83381
[1mStep[0m  [330/339], [94mLoss[0m : 2.36053

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10701
[1mStep[0m  [33/339], [94mLoss[0m : 1.93860
[1mStep[0m  [66/339], [94mLoss[0m : 2.94262
[1mStep[0m  [99/339], [94mLoss[0m : 2.19720
[1mStep[0m  [132/339], [94mLoss[0m : 2.51167
[1mStep[0m  [165/339], [94mLoss[0m : 2.60071
[1mStep[0m  [198/339], [94mLoss[0m : 2.80951
[1mStep[0m  [231/339], [94mLoss[0m : 2.59463
[1mStep[0m  [264/339], [94mLoss[0m : 2.13089
[1mStep[0m  [297/339], [94mLoss[0m : 2.50747
[1mStep[0m  [330/339], [94mLoss[0m : 2.29388

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.61148
[1mStep[0m  [33/339], [94mLoss[0m : 2.26755
[1mStep[0m  [66/339], [94mLoss[0m : 2.16728
[1mStep[0m  [99/339], [94mLoss[0m : 1.99688
[1mStep[0m  [132/339], [94mLoss[0m : 2.78752
[1mStep[0m  [165/339], [94mLoss[0m : 2.70475
[1mStep[0m  [198/339], [94mLoss[0m : 2.38202
[1mStep[0m  [231/339], [94mLoss[0m : 2.94928
[1mStep[0m  [264/339], [94mLoss[0m : 2.95313
[1mStep[0m  [297/339], [94mLoss[0m : 2.02917
[1mStep[0m  [330/339], [94mLoss[0m : 2.99776

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.55425
[1mStep[0m  [33/339], [94mLoss[0m : 2.60421
[1mStep[0m  [66/339], [94mLoss[0m : 3.28275
[1mStep[0m  [99/339], [94mLoss[0m : 2.86638
[1mStep[0m  [132/339], [94mLoss[0m : 2.54235
[1mStep[0m  [165/339], [94mLoss[0m : 2.62651
[1mStep[0m  [198/339], [94mLoss[0m : 2.61236
[1mStep[0m  [231/339], [94mLoss[0m : 1.96792
[1mStep[0m  [264/339], [94mLoss[0m : 2.65393
[1mStep[0m  [297/339], [94mLoss[0m : 2.66968
[1mStep[0m  [330/339], [94mLoss[0m : 2.65088

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33031
[1mStep[0m  [33/339], [94mLoss[0m : 2.37033
[1mStep[0m  [66/339], [94mLoss[0m : 2.52357
[1mStep[0m  [99/339], [94mLoss[0m : 2.16573
[1mStep[0m  [132/339], [94mLoss[0m : 2.26813
[1mStep[0m  [165/339], [94mLoss[0m : 3.00519
[1mStep[0m  [198/339], [94mLoss[0m : 2.82206
[1mStep[0m  [231/339], [94mLoss[0m : 2.40914
[1mStep[0m  [264/339], [94mLoss[0m : 2.29600
[1mStep[0m  [297/339], [94mLoss[0m : 2.33934
[1mStep[0m  [330/339], [94mLoss[0m : 2.19343

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70372
[1mStep[0m  [33/339], [94mLoss[0m : 2.08083
[1mStep[0m  [66/339], [94mLoss[0m : 2.24893
[1mStep[0m  [99/339], [94mLoss[0m : 2.19221
[1mStep[0m  [132/339], [94mLoss[0m : 1.68082
[1mStep[0m  [165/339], [94mLoss[0m : 2.23979
[1mStep[0m  [198/339], [94mLoss[0m : 2.43695
[1mStep[0m  [231/339], [94mLoss[0m : 2.26251
[1mStep[0m  [264/339], [94mLoss[0m : 2.26530
[1mStep[0m  [297/339], [94mLoss[0m : 2.42656
[1mStep[0m  [330/339], [94mLoss[0m : 2.45045

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88311
[1mStep[0m  [33/339], [94mLoss[0m : 2.66638
[1mStep[0m  [66/339], [94mLoss[0m : 2.18117
[1mStep[0m  [99/339], [94mLoss[0m : 2.76399
[1mStep[0m  [132/339], [94mLoss[0m : 2.29071
[1mStep[0m  [165/339], [94mLoss[0m : 2.46688
[1mStep[0m  [198/339], [94mLoss[0m : 2.28763
[1mStep[0m  [231/339], [94mLoss[0m : 2.13659
[1mStep[0m  [264/339], [94mLoss[0m : 2.08083
[1mStep[0m  [297/339], [94mLoss[0m : 1.99909
[1mStep[0m  [330/339], [94mLoss[0m : 2.39113

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.358, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14260
[1mStep[0m  [33/339], [94mLoss[0m : 1.87834
[1mStep[0m  [66/339], [94mLoss[0m : 1.85972
[1mStep[0m  [99/339], [94mLoss[0m : 2.65724
[1mStep[0m  [132/339], [94mLoss[0m : 2.86973
[1mStep[0m  [165/339], [94mLoss[0m : 2.29517
[1mStep[0m  [198/339], [94mLoss[0m : 2.18447
[1mStep[0m  [231/339], [94mLoss[0m : 2.17944
[1mStep[0m  [264/339], [94mLoss[0m : 2.33898
[1mStep[0m  [297/339], [94mLoss[0m : 2.68795
[1mStep[0m  [330/339], [94mLoss[0m : 2.14945

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.330, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37176
[1mStep[0m  [33/339], [94mLoss[0m : 1.75034
[1mStep[0m  [66/339], [94mLoss[0m : 1.92227
[1mStep[0m  [99/339], [94mLoss[0m : 2.31061
[1mStep[0m  [132/339], [94mLoss[0m : 2.03634
[1mStep[0m  [165/339], [94mLoss[0m : 2.28008
[1mStep[0m  [198/339], [94mLoss[0m : 2.57682
[1mStep[0m  [231/339], [94mLoss[0m : 2.09778
[1mStep[0m  [264/339], [94mLoss[0m : 3.43523
[1mStep[0m  [297/339], [94mLoss[0m : 2.50599
[1mStep[0m  [330/339], [94mLoss[0m : 2.81010

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.328, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25306
[1mStep[0m  [33/339], [94mLoss[0m : 2.55634
[1mStep[0m  [66/339], [94mLoss[0m : 2.24642
[1mStep[0m  [99/339], [94mLoss[0m : 2.37493
[1mStep[0m  [132/339], [94mLoss[0m : 3.26312
[1mStep[0m  [165/339], [94mLoss[0m : 2.57291
[1mStep[0m  [198/339], [94mLoss[0m : 1.96337
[1mStep[0m  [231/339], [94mLoss[0m : 2.80312
[1mStep[0m  [264/339], [94mLoss[0m : 2.33502
[1mStep[0m  [297/339], [94mLoss[0m : 2.78911
[1mStep[0m  [330/339], [94mLoss[0m : 1.88446

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10680
[1mStep[0m  [33/339], [94mLoss[0m : 2.69646
[1mStep[0m  [66/339], [94mLoss[0m : 1.77149
[1mStep[0m  [99/339], [94mLoss[0m : 2.48951
[1mStep[0m  [132/339], [94mLoss[0m : 2.55991
[1mStep[0m  [165/339], [94mLoss[0m : 2.24792
[1mStep[0m  [198/339], [94mLoss[0m : 2.25901
[1mStep[0m  [231/339], [94mLoss[0m : 2.14430
[1mStep[0m  [264/339], [94mLoss[0m : 3.48298
[1mStep[0m  [297/339], [94mLoss[0m : 1.94128
[1mStep[0m  [330/339], [94mLoss[0m : 2.08488

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.346, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.79028
[1mStep[0m  [33/339], [94mLoss[0m : 3.06116
[1mStep[0m  [66/339], [94mLoss[0m : 2.31875
[1mStep[0m  [99/339], [94mLoss[0m : 2.27346
[1mStep[0m  [132/339], [94mLoss[0m : 2.37011
[1mStep[0m  [165/339], [94mLoss[0m : 2.11558
[1mStep[0m  [198/339], [94mLoss[0m : 2.61813
[1mStep[0m  [231/339], [94mLoss[0m : 2.68203
[1mStep[0m  [264/339], [94mLoss[0m : 2.93366
[1mStep[0m  [297/339], [94mLoss[0m : 2.34994
[1mStep[0m  [330/339], [94mLoss[0m : 3.23295

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.368, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.73763
[1mStep[0m  [33/339], [94mLoss[0m : 2.27042
[1mStep[0m  [66/339], [94mLoss[0m : 2.03448
[1mStep[0m  [99/339], [94mLoss[0m : 2.36427
[1mStep[0m  [132/339], [94mLoss[0m : 2.99754
[1mStep[0m  [165/339], [94mLoss[0m : 2.44042
[1mStep[0m  [198/339], [94mLoss[0m : 2.06758
[1mStep[0m  [231/339], [94mLoss[0m : 2.12358
[1mStep[0m  [264/339], [94mLoss[0m : 2.87814
[1mStep[0m  [297/339], [94mLoss[0m : 2.53998
[1mStep[0m  [330/339], [94mLoss[0m : 3.03258

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99200
[1mStep[0m  [33/339], [94mLoss[0m : 2.67455
[1mStep[0m  [66/339], [94mLoss[0m : 2.09453
[1mStep[0m  [99/339], [94mLoss[0m : 1.99868
[1mStep[0m  [132/339], [94mLoss[0m : 2.03133
[1mStep[0m  [165/339], [94mLoss[0m : 2.30123
[1mStep[0m  [198/339], [94mLoss[0m : 2.16166
[1mStep[0m  [231/339], [94mLoss[0m : 2.16725
[1mStep[0m  [264/339], [94mLoss[0m : 2.84779
[1mStep[0m  [297/339], [94mLoss[0m : 1.78908
[1mStep[0m  [330/339], [94mLoss[0m : 2.13891

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11670
[1mStep[0m  [33/339], [94mLoss[0m : 2.28889
[1mStep[0m  [66/339], [94mLoss[0m : 2.61329
[1mStep[0m  [99/339], [94mLoss[0m : 2.49586
[1mStep[0m  [132/339], [94mLoss[0m : 2.36076
[1mStep[0m  [165/339], [94mLoss[0m : 1.85416
[1mStep[0m  [198/339], [94mLoss[0m : 2.68977
[1mStep[0m  [231/339], [94mLoss[0m : 2.37270
[1mStep[0m  [264/339], [94mLoss[0m : 2.56305
[1mStep[0m  [297/339], [94mLoss[0m : 2.43686
[1mStep[0m  [330/339], [94mLoss[0m : 2.69816

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98654
[1mStep[0m  [33/339], [94mLoss[0m : 1.98368
[1mStep[0m  [66/339], [94mLoss[0m : 2.26170
[1mStep[0m  [99/339], [94mLoss[0m : 2.58071
[1mStep[0m  [132/339], [94mLoss[0m : 2.34182
[1mStep[0m  [165/339], [94mLoss[0m : 2.00797
[1mStep[0m  [198/339], [94mLoss[0m : 2.56689
[1mStep[0m  [231/339], [94mLoss[0m : 1.70671
[1mStep[0m  [264/339], [94mLoss[0m : 2.19679
[1mStep[0m  [297/339], [94mLoss[0m : 2.75736
[1mStep[0m  [330/339], [94mLoss[0m : 2.21934

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.370, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11390
[1mStep[0m  [33/339], [94mLoss[0m : 2.81027
[1mStep[0m  [66/339], [94mLoss[0m : 2.57778
[1mStep[0m  [99/339], [94mLoss[0m : 2.13046
[1mStep[0m  [132/339], [94mLoss[0m : 1.87101
[1mStep[0m  [165/339], [94mLoss[0m : 2.37441
[1mStep[0m  [198/339], [94mLoss[0m : 2.06066
[1mStep[0m  [231/339], [94mLoss[0m : 2.37164
[1mStep[0m  [264/339], [94mLoss[0m : 2.55966
[1mStep[0m  [297/339], [94mLoss[0m : 2.76638
[1mStep[0m  [330/339], [94mLoss[0m : 2.83346

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.340, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16719
[1mStep[0m  [33/339], [94mLoss[0m : 2.65840
[1mStep[0m  [66/339], [94mLoss[0m : 2.36314
[1mStep[0m  [99/339], [94mLoss[0m : 2.40954
[1mStep[0m  [132/339], [94mLoss[0m : 2.05300
[1mStep[0m  [165/339], [94mLoss[0m : 2.33044
[1mStep[0m  [198/339], [94mLoss[0m : 2.72733
[1mStep[0m  [231/339], [94mLoss[0m : 2.55082
[1mStep[0m  [264/339], [94mLoss[0m : 1.50463
[1mStep[0m  [297/339], [94mLoss[0m : 2.94465
[1mStep[0m  [330/339], [94mLoss[0m : 2.28706

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43567
[1mStep[0m  [33/339], [94mLoss[0m : 3.40480
[1mStep[0m  [66/339], [94mLoss[0m : 2.98553
[1mStep[0m  [99/339], [94mLoss[0m : 2.28952
[1mStep[0m  [132/339], [94mLoss[0m : 2.14089
[1mStep[0m  [165/339], [94mLoss[0m : 2.29867
[1mStep[0m  [198/339], [94mLoss[0m : 2.53636
[1mStep[0m  [231/339], [94mLoss[0m : 2.96067
[1mStep[0m  [264/339], [94mLoss[0m : 2.69333
[1mStep[0m  [297/339], [94mLoss[0m : 2.16270
[1mStep[0m  [330/339], [94mLoss[0m : 2.19969

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.20614
[1mStep[0m  [33/339], [94mLoss[0m : 2.45093
[1mStep[0m  [66/339], [94mLoss[0m : 2.55681
[1mStep[0m  [99/339], [94mLoss[0m : 1.74927
[1mStep[0m  [132/339], [94mLoss[0m : 2.34887
[1mStep[0m  [165/339], [94mLoss[0m : 2.64518
[1mStep[0m  [198/339], [94mLoss[0m : 1.66578
[1mStep[0m  [231/339], [94mLoss[0m : 2.10878
[1mStep[0m  [264/339], [94mLoss[0m : 2.17689
[1mStep[0m  [297/339], [94mLoss[0m : 2.08555
[1mStep[0m  [330/339], [94mLoss[0m : 2.34605

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.364, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21695
[1mStep[0m  [33/339], [94mLoss[0m : 2.68177
[1mStep[0m  [66/339], [94mLoss[0m : 2.75844
[1mStep[0m  [99/339], [94mLoss[0m : 2.57341
[1mStep[0m  [132/339], [94mLoss[0m : 2.37047
[1mStep[0m  [165/339], [94mLoss[0m : 2.06753
[1mStep[0m  [198/339], [94mLoss[0m : 1.93192
[1mStep[0m  [231/339], [94mLoss[0m : 2.77245
[1mStep[0m  [264/339], [94mLoss[0m : 2.62179
[1mStep[0m  [297/339], [94mLoss[0m : 2.52328
[1mStep[0m  [330/339], [94mLoss[0m : 2.61063

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.332, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45891
[1mStep[0m  [33/339], [94mLoss[0m : 2.38559
[1mStep[0m  [66/339], [94mLoss[0m : 1.98605
[1mStep[0m  [99/339], [94mLoss[0m : 2.69841
[1mStep[0m  [132/339], [94mLoss[0m : 2.12962
[1mStep[0m  [165/339], [94mLoss[0m : 2.93290
[1mStep[0m  [198/339], [94mLoss[0m : 1.82381
[1mStep[0m  [231/339], [94mLoss[0m : 2.10985
[1mStep[0m  [264/339], [94mLoss[0m : 2.78740
[1mStep[0m  [297/339], [94mLoss[0m : 2.22833
[1mStep[0m  [330/339], [94mLoss[0m : 1.68721

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.334, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.351
====================================

Phase 1 - Evaluation MAE:  2.350537236812895
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 1.83345
[1mStep[0m  [33/339], [94mLoss[0m : 3.00529
[1mStep[0m  [66/339], [94mLoss[0m : 2.42118
[1mStep[0m  [99/339], [94mLoss[0m : 2.06548
[1mStep[0m  [132/339], [94mLoss[0m : 2.83674
[1mStep[0m  [165/339], [94mLoss[0m : 2.32227
[1mStep[0m  [198/339], [94mLoss[0m : 2.90217
[1mStep[0m  [231/339], [94mLoss[0m : 1.82741
[1mStep[0m  [264/339], [94mLoss[0m : 1.94177
[1mStep[0m  [297/339], [94mLoss[0m : 2.01961
[1mStep[0m  [330/339], [94mLoss[0m : 2.22913

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.353, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85837
[1mStep[0m  [33/339], [94mLoss[0m : 2.21018
[1mStep[0m  [66/339], [94mLoss[0m : 1.98495
[1mStep[0m  [99/339], [94mLoss[0m : 1.84378
[1mStep[0m  [132/339], [94mLoss[0m : 2.55207
[1mStep[0m  [165/339], [94mLoss[0m : 2.27351
[1mStep[0m  [198/339], [94mLoss[0m : 2.41046
[1mStep[0m  [231/339], [94mLoss[0m : 1.76949
[1mStep[0m  [264/339], [94mLoss[0m : 2.44468
[1mStep[0m  [297/339], [94mLoss[0m : 2.56320
[1mStep[0m  [330/339], [94mLoss[0m : 2.26134

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72965
[1mStep[0m  [33/339], [94mLoss[0m : 2.48653
[1mStep[0m  [66/339], [94mLoss[0m : 2.47411
[1mStep[0m  [99/339], [94mLoss[0m : 2.11450
[1mStep[0m  [132/339], [94mLoss[0m : 2.17278
[1mStep[0m  [165/339], [94mLoss[0m : 2.55207
[1mStep[0m  [198/339], [94mLoss[0m : 2.48531
[1mStep[0m  [231/339], [94mLoss[0m : 3.10128
[1mStep[0m  [264/339], [94mLoss[0m : 2.21435
[1mStep[0m  [297/339], [94mLoss[0m : 2.37317
[1mStep[0m  [330/339], [94mLoss[0m : 2.50714

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01889
[1mStep[0m  [33/339], [94mLoss[0m : 2.24769
[1mStep[0m  [66/339], [94mLoss[0m : 2.92050
[1mStep[0m  [99/339], [94mLoss[0m : 1.96368
[1mStep[0m  [132/339], [94mLoss[0m : 2.20990
[1mStep[0m  [165/339], [94mLoss[0m : 2.60614
[1mStep[0m  [198/339], [94mLoss[0m : 2.12238
[1mStep[0m  [231/339], [94mLoss[0m : 2.27387
[1mStep[0m  [264/339], [94mLoss[0m : 2.07458
[1mStep[0m  [297/339], [94mLoss[0m : 2.45916
[1mStep[0m  [330/339], [94mLoss[0m : 2.04770

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73408
[1mStep[0m  [33/339], [94mLoss[0m : 2.12790
[1mStep[0m  [66/339], [94mLoss[0m : 1.59175
[1mStep[0m  [99/339], [94mLoss[0m : 2.15350
[1mStep[0m  [132/339], [94mLoss[0m : 2.40931
[1mStep[0m  [165/339], [94mLoss[0m : 2.27872
[1mStep[0m  [198/339], [94mLoss[0m : 1.69197
[1mStep[0m  [231/339], [94mLoss[0m : 1.92884
[1mStep[0m  [264/339], [94mLoss[0m : 1.71972
[1mStep[0m  [297/339], [94mLoss[0m : 2.28211
[1mStep[0m  [330/339], [94mLoss[0m : 2.17647

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80570
[1mStep[0m  [33/339], [94mLoss[0m : 2.41046
[1mStep[0m  [66/339], [94mLoss[0m : 1.99383
[1mStep[0m  [99/339], [94mLoss[0m : 2.32905
[1mStep[0m  [132/339], [94mLoss[0m : 1.98345
[1mStep[0m  [165/339], [94mLoss[0m : 2.39451
[1mStep[0m  [198/339], [94mLoss[0m : 1.68132
[1mStep[0m  [231/339], [94mLoss[0m : 2.00893
[1mStep[0m  [264/339], [94mLoss[0m : 2.01862
[1mStep[0m  [297/339], [94mLoss[0m : 1.94682
[1mStep[0m  [330/339], [94mLoss[0m : 2.13764

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69413
[1mStep[0m  [33/339], [94mLoss[0m : 2.42918
[1mStep[0m  [66/339], [94mLoss[0m : 1.88442
[1mStep[0m  [99/339], [94mLoss[0m : 2.06592
[1mStep[0m  [132/339], [94mLoss[0m : 2.39577
[1mStep[0m  [165/339], [94mLoss[0m : 2.62416
[1mStep[0m  [198/339], [94mLoss[0m : 1.98644
[1mStep[0m  [231/339], [94mLoss[0m : 1.96365
[1mStep[0m  [264/339], [94mLoss[0m : 1.92924
[1mStep[0m  [297/339], [94mLoss[0m : 2.45045
[1mStep[0m  [330/339], [94mLoss[0m : 2.20703

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.390, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.33304
[1mStep[0m  [33/339], [94mLoss[0m : 2.32759
[1mStep[0m  [66/339], [94mLoss[0m : 1.56509
[1mStep[0m  [99/339], [94mLoss[0m : 1.57531
[1mStep[0m  [132/339], [94mLoss[0m : 1.84313
[1mStep[0m  [165/339], [94mLoss[0m : 2.04053
[1mStep[0m  [198/339], [94mLoss[0m : 2.31999
[1mStep[0m  [231/339], [94mLoss[0m : 2.73060
[1mStep[0m  [264/339], [94mLoss[0m : 2.56606
[1mStep[0m  [297/339], [94mLoss[0m : 1.88973
[1mStep[0m  [330/339], [94mLoss[0m : 2.20961

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40107
[1mStep[0m  [33/339], [94mLoss[0m : 1.48549
[1mStep[0m  [66/339], [94mLoss[0m : 2.36238
[1mStep[0m  [99/339], [94mLoss[0m : 2.34199
[1mStep[0m  [132/339], [94mLoss[0m : 2.20972
[1mStep[0m  [165/339], [94mLoss[0m : 1.46037
[1mStep[0m  [198/339], [94mLoss[0m : 2.23072
[1mStep[0m  [231/339], [94mLoss[0m : 1.97937
[1mStep[0m  [264/339], [94mLoss[0m : 1.68968
[1mStep[0m  [297/339], [94mLoss[0m : 1.44296
[1mStep[0m  [330/339], [94mLoss[0m : 1.83961

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84266
[1mStep[0m  [33/339], [94mLoss[0m : 1.84468
[1mStep[0m  [66/339], [94mLoss[0m : 2.16189
[1mStep[0m  [99/339], [94mLoss[0m : 1.64062
[1mStep[0m  [132/339], [94mLoss[0m : 1.54467
[1mStep[0m  [165/339], [94mLoss[0m : 1.94246
[1mStep[0m  [198/339], [94mLoss[0m : 2.37340
[1mStep[0m  [231/339], [94mLoss[0m : 2.06226
[1mStep[0m  [264/339], [94mLoss[0m : 2.20202
[1mStep[0m  [297/339], [94mLoss[0m : 1.87366
[1mStep[0m  [330/339], [94mLoss[0m : 1.44731

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.44467
[1mStep[0m  [33/339], [94mLoss[0m : 1.74942
[1mStep[0m  [66/339], [94mLoss[0m : 1.83683
[1mStep[0m  [99/339], [94mLoss[0m : 1.77801
[1mStep[0m  [132/339], [94mLoss[0m : 1.52475
[1mStep[0m  [165/339], [94mLoss[0m : 2.09501
[1mStep[0m  [198/339], [94mLoss[0m : 2.02759
[1mStep[0m  [231/339], [94mLoss[0m : 2.91321
[1mStep[0m  [264/339], [94mLoss[0m : 2.18606
[1mStep[0m  [297/339], [94mLoss[0m : 1.75583
[1mStep[0m  [330/339], [94mLoss[0m : 1.56687

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.437, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.54994
[1mStep[0m  [33/339], [94mLoss[0m : 1.87710
[1mStep[0m  [66/339], [94mLoss[0m : 1.53976
[1mStep[0m  [99/339], [94mLoss[0m : 2.05568
[1mStep[0m  [132/339], [94mLoss[0m : 1.70949
[1mStep[0m  [165/339], [94mLoss[0m : 1.67230
[1mStep[0m  [198/339], [94mLoss[0m : 2.15786
[1mStep[0m  [231/339], [94mLoss[0m : 1.58551
[1mStep[0m  [264/339], [94mLoss[0m : 1.54361
[1mStep[0m  [297/339], [94mLoss[0m : 1.96439
[1mStep[0m  [330/339], [94mLoss[0m : 1.89352

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.526, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85728
[1mStep[0m  [33/339], [94mLoss[0m : 1.51579
[1mStep[0m  [66/339], [94mLoss[0m : 2.21580
[1mStep[0m  [99/339], [94mLoss[0m : 2.08414
[1mStep[0m  [132/339], [94mLoss[0m : 1.15191
[1mStep[0m  [165/339], [94mLoss[0m : 2.43081
[1mStep[0m  [198/339], [94mLoss[0m : 1.76296
[1mStep[0m  [231/339], [94mLoss[0m : 2.09767
[1mStep[0m  [264/339], [94mLoss[0m : 1.49570
[1mStep[0m  [297/339], [94mLoss[0m : 1.76350
[1mStep[0m  [330/339], [94mLoss[0m : 2.48703

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.27076
[1mStep[0m  [33/339], [94mLoss[0m : 1.46580
[1mStep[0m  [66/339], [94mLoss[0m : 1.57281
[1mStep[0m  [99/339], [94mLoss[0m : 1.41676
[1mStep[0m  [132/339], [94mLoss[0m : 1.54887
[1mStep[0m  [165/339], [94mLoss[0m : 1.77071
[1mStep[0m  [198/339], [94mLoss[0m : 1.91545
[1mStep[0m  [231/339], [94mLoss[0m : 1.55655
[1mStep[0m  [264/339], [94mLoss[0m : 1.44584
[1mStep[0m  [297/339], [94mLoss[0m : 1.79209
[1mStep[0m  [330/339], [94mLoss[0m : 1.75939

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.481, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.86817
[1mStep[0m  [33/339], [94mLoss[0m : 1.84616
[1mStep[0m  [66/339], [94mLoss[0m : 1.70864
[1mStep[0m  [99/339], [94mLoss[0m : 1.26901
[1mStep[0m  [132/339], [94mLoss[0m : 1.76804
[1mStep[0m  [165/339], [94mLoss[0m : 1.58648
[1mStep[0m  [198/339], [94mLoss[0m : 2.44157
[1mStep[0m  [231/339], [94mLoss[0m : 1.89339
[1mStep[0m  [264/339], [94mLoss[0m : 1.26005
[1mStep[0m  [297/339], [94mLoss[0m : 1.57328
[1mStep[0m  [330/339], [94mLoss[0m : 1.76809

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56091
[1mStep[0m  [33/339], [94mLoss[0m : 1.40875
[1mStep[0m  [66/339], [94mLoss[0m : 1.21152
[1mStep[0m  [99/339], [94mLoss[0m : 1.68797
[1mStep[0m  [132/339], [94mLoss[0m : 1.77579
[1mStep[0m  [165/339], [94mLoss[0m : 1.80684
[1mStep[0m  [198/339], [94mLoss[0m : 2.54617
[1mStep[0m  [231/339], [94mLoss[0m : 1.70972
[1mStep[0m  [264/339], [94mLoss[0m : 1.48920
[1mStep[0m  [297/339], [94mLoss[0m : 2.19201
[1mStep[0m  [330/339], [94mLoss[0m : 1.59281

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.479, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61034
[1mStep[0m  [33/339], [94mLoss[0m : 1.53810
[1mStep[0m  [66/339], [94mLoss[0m : 2.05787
[1mStep[0m  [99/339], [94mLoss[0m : 1.81860
[1mStep[0m  [132/339], [94mLoss[0m : 1.44940
[1mStep[0m  [165/339], [94mLoss[0m : 1.62259
[1mStep[0m  [198/339], [94mLoss[0m : 2.00742
[1mStep[0m  [231/339], [94mLoss[0m : 1.58049
[1mStep[0m  [264/339], [94mLoss[0m : 1.71292
[1mStep[0m  [297/339], [94mLoss[0m : 1.37816
[1mStep[0m  [330/339], [94mLoss[0m : 1.67273

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.522, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62539
[1mStep[0m  [33/339], [94mLoss[0m : 1.97312
[1mStep[0m  [66/339], [94mLoss[0m : 1.27840
[1mStep[0m  [99/339], [94mLoss[0m : 1.49380
[1mStep[0m  [132/339], [94mLoss[0m : 1.80087
[1mStep[0m  [165/339], [94mLoss[0m : 1.54155
[1mStep[0m  [198/339], [94mLoss[0m : 1.19682
[1mStep[0m  [231/339], [94mLoss[0m : 2.08175
[1mStep[0m  [264/339], [94mLoss[0m : 1.86928
[1mStep[0m  [297/339], [94mLoss[0m : 1.83386
[1mStep[0m  [330/339], [94mLoss[0m : 1.69044

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.56102
[1mStep[0m  [33/339], [94mLoss[0m : 1.41668
[1mStep[0m  [66/339], [94mLoss[0m : 1.93518
[1mStep[0m  [99/339], [94mLoss[0m : 1.59630
[1mStep[0m  [132/339], [94mLoss[0m : 1.67967
[1mStep[0m  [165/339], [94mLoss[0m : 2.32126
[1mStep[0m  [198/339], [94mLoss[0m : 1.71913
[1mStep[0m  [231/339], [94mLoss[0m : 1.39419
[1mStep[0m  [264/339], [94mLoss[0m : 1.94028
[1mStep[0m  [297/339], [94mLoss[0m : 2.22075
[1mStep[0m  [330/339], [94mLoss[0m : 1.96139

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.544, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52189
[1mStep[0m  [33/339], [94mLoss[0m : 1.74991
[1mStep[0m  [66/339], [94mLoss[0m : 1.67393
[1mStep[0m  [99/339], [94mLoss[0m : 1.30258
[1mStep[0m  [132/339], [94mLoss[0m : 1.04624
[1mStep[0m  [165/339], [94mLoss[0m : 1.33564
[1mStep[0m  [198/339], [94mLoss[0m : 1.76552
[1mStep[0m  [231/339], [94mLoss[0m : 1.61803
[1mStep[0m  [264/339], [94mLoss[0m : 1.38459
[1mStep[0m  [297/339], [94mLoss[0m : 1.19409
[1mStep[0m  [330/339], [94mLoss[0m : 2.21175

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65494
[1mStep[0m  [33/339], [94mLoss[0m : 1.12472
[1mStep[0m  [66/339], [94mLoss[0m : 1.60663
[1mStep[0m  [99/339], [94mLoss[0m : 1.73818
[1mStep[0m  [132/339], [94mLoss[0m : 1.69809
[1mStep[0m  [165/339], [94mLoss[0m : 1.52021
[1mStep[0m  [198/339], [94mLoss[0m : 1.98746
[1mStep[0m  [231/339], [94mLoss[0m : 1.40367
[1mStep[0m  [264/339], [94mLoss[0m : 1.68674
[1mStep[0m  [297/339], [94mLoss[0m : 1.85246
[1mStep[0m  [330/339], [94mLoss[0m : 1.55408

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.484, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38892
[1mStep[0m  [33/339], [94mLoss[0m : 1.69129
[1mStep[0m  [66/339], [94mLoss[0m : 2.16292
[1mStep[0m  [99/339], [94mLoss[0m : 2.38044
[1mStep[0m  [132/339], [94mLoss[0m : 1.41306
[1mStep[0m  [165/339], [94mLoss[0m : 1.48541
[1mStep[0m  [198/339], [94mLoss[0m : 2.14441
[1mStep[0m  [231/339], [94mLoss[0m : 1.54221
[1mStep[0m  [264/339], [94mLoss[0m : 1.66697
[1mStep[0m  [297/339], [94mLoss[0m : 1.47937
[1mStep[0m  [330/339], [94mLoss[0m : 1.46071

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.546, [92mTest[0m: 2.507, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.38782
[1mStep[0m  [33/339], [94mLoss[0m : 1.42905
[1mStep[0m  [66/339], [94mLoss[0m : 1.73508
[1mStep[0m  [99/339], [94mLoss[0m : 1.33958
[1mStep[0m  [132/339], [94mLoss[0m : 1.42814
[1mStep[0m  [165/339], [94mLoss[0m : 1.58908
[1mStep[0m  [198/339], [94mLoss[0m : 1.48865
[1mStep[0m  [231/339], [94mLoss[0m : 1.73785
[1mStep[0m  [264/339], [94mLoss[0m : 1.65798
[1mStep[0m  [297/339], [94mLoss[0m : 1.26082
[1mStep[0m  [330/339], [94mLoss[0m : 1.30484

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.533, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39047
[1mStep[0m  [33/339], [94mLoss[0m : 1.17988
[1mStep[0m  [66/339], [94mLoss[0m : 1.52705
[1mStep[0m  [99/339], [94mLoss[0m : 1.47825
[1mStep[0m  [132/339], [94mLoss[0m : 1.66024
[1mStep[0m  [165/339], [94mLoss[0m : 1.33020
[1mStep[0m  [198/339], [94mLoss[0m : 1.84506
[1mStep[0m  [231/339], [94mLoss[0m : 1.82887
[1mStep[0m  [264/339], [94mLoss[0m : 1.39719
[1mStep[0m  [297/339], [94mLoss[0m : 1.70983
[1mStep[0m  [330/339], [94mLoss[0m : 1.68319

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.487, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75429
[1mStep[0m  [33/339], [94mLoss[0m : 1.26198
[1mStep[0m  [66/339], [94mLoss[0m : 1.44110
[1mStep[0m  [99/339], [94mLoss[0m : 1.32729
[1mStep[0m  [132/339], [94mLoss[0m : 1.56290
[1mStep[0m  [165/339], [94mLoss[0m : 1.08572
[1mStep[0m  [198/339], [94mLoss[0m : 1.58945
[1mStep[0m  [231/339], [94mLoss[0m : 1.21875
[1mStep[0m  [264/339], [94mLoss[0m : 1.16497
[1mStep[0m  [297/339], [94mLoss[0m : 1.25012
[1mStep[0m  [330/339], [94mLoss[0m : 1.31201

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.489, [92mTest[0m: 2.502, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.21757
[1mStep[0m  [33/339], [94mLoss[0m : 1.23682
[1mStep[0m  [66/339], [94mLoss[0m : 1.62157
[1mStep[0m  [99/339], [94mLoss[0m : 1.19444
[1mStep[0m  [132/339], [94mLoss[0m : 1.52675
[1mStep[0m  [165/339], [94mLoss[0m : 1.90841
[1mStep[0m  [198/339], [94mLoss[0m : 1.60068
[1mStep[0m  [231/339], [94mLoss[0m : 1.55723
[1mStep[0m  [264/339], [94mLoss[0m : 1.84729
[1mStep[0m  [297/339], [94mLoss[0m : 1.49064
[1mStep[0m  [330/339], [94mLoss[0m : 2.02501

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.463, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67204
[1mStep[0m  [33/339], [94mLoss[0m : 1.81769
[1mStep[0m  [66/339], [94mLoss[0m : 1.31945
[1mStep[0m  [99/339], [94mLoss[0m : 1.19325
[1mStep[0m  [132/339], [94mLoss[0m : 0.89382
[1mStep[0m  [165/339], [94mLoss[0m : 1.68819
[1mStep[0m  [198/339], [94mLoss[0m : 1.28289
[1mStep[0m  [231/339], [94mLoss[0m : 1.16656
[1mStep[0m  [264/339], [94mLoss[0m : 1.34045
[1mStep[0m  [297/339], [94mLoss[0m : 1.54598
[1mStep[0m  [330/339], [94mLoss[0m : 1.01681

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.442, [92mTest[0m: 2.521, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67896
[1mStep[0m  [33/339], [94mLoss[0m : 1.52449
[1mStep[0m  [66/339], [94mLoss[0m : 1.48631
[1mStep[0m  [99/339], [94mLoss[0m : 1.63232
[1mStep[0m  [132/339], [94mLoss[0m : 1.90145
[1mStep[0m  [165/339], [94mLoss[0m : 1.46956
[1mStep[0m  [198/339], [94mLoss[0m : 1.93480
[1mStep[0m  [231/339], [94mLoss[0m : 1.98305
[1mStep[0m  [264/339], [94mLoss[0m : 1.40927
[1mStep[0m  [297/339], [94mLoss[0m : 1.06826
[1mStep[0m  [330/339], [94mLoss[0m : 1.37286

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.439, [92mTest[0m: 2.500, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.506
====================================

Phase 2 - Evaluation MAE:  2.506451562442611
MAE score P1      2.350537
MAE score P2      2.506452
loss              1.439067
learning_rate         0.01
batch_size              32
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 9.68877
[1mStep[0m  [16/169], [94mLoss[0m : 8.91917
[1mStep[0m  [32/169], [94mLoss[0m : 7.65946
[1mStep[0m  [48/169], [94mLoss[0m : 6.16758
[1mStep[0m  [64/169], [94mLoss[0m : 3.92253
[1mStep[0m  [80/169], [94mLoss[0m : 2.79874
[1mStep[0m  [96/169], [94mLoss[0m : 2.74664
[1mStep[0m  [112/169], [94mLoss[0m : 3.19776
[1mStep[0m  [128/169], [94mLoss[0m : 2.85930
[1mStep[0m  [144/169], [94mLoss[0m : 2.54711
[1mStep[0m  [160/169], [94mLoss[0m : 2.50923

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.633, [92mTest[0m: 10.784, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.17895
[1mStep[0m  [16/169], [94mLoss[0m : 2.72741
[1mStep[0m  [32/169], [94mLoss[0m : 2.65662
[1mStep[0m  [48/169], [94mLoss[0m : 2.78436
[1mStep[0m  [64/169], [94mLoss[0m : 2.74804
[1mStep[0m  [80/169], [94mLoss[0m : 2.35198
[1mStep[0m  [96/169], [94mLoss[0m : 2.59397
[1mStep[0m  [112/169], [94mLoss[0m : 2.61015
[1mStep[0m  [128/169], [94mLoss[0m : 2.66162
[1mStep[0m  [144/169], [94mLoss[0m : 2.32740
[1mStep[0m  [160/169], [94mLoss[0m : 2.72966

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.460, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16262
[1mStep[0m  [16/169], [94mLoss[0m : 2.69571
[1mStep[0m  [32/169], [94mLoss[0m : 2.54845
[1mStep[0m  [48/169], [94mLoss[0m : 2.44768
[1mStep[0m  [64/169], [94mLoss[0m : 2.50671
[1mStep[0m  [80/169], [94mLoss[0m : 2.74875
[1mStep[0m  [96/169], [94mLoss[0m : 2.57314
[1mStep[0m  [112/169], [94mLoss[0m : 2.50902
[1mStep[0m  [128/169], [94mLoss[0m : 2.50314
[1mStep[0m  [144/169], [94mLoss[0m : 2.74652
[1mStep[0m  [160/169], [94mLoss[0m : 2.38657

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.407, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61411
[1mStep[0m  [16/169], [94mLoss[0m : 2.38153
[1mStep[0m  [32/169], [94mLoss[0m : 2.23975
[1mStep[0m  [48/169], [94mLoss[0m : 2.54149
[1mStep[0m  [64/169], [94mLoss[0m : 2.52037
[1mStep[0m  [80/169], [94mLoss[0m : 2.70775
[1mStep[0m  [96/169], [94mLoss[0m : 2.31491
[1mStep[0m  [112/169], [94mLoss[0m : 2.40794
[1mStep[0m  [128/169], [94mLoss[0m : 2.93706
[1mStep[0m  [144/169], [94mLoss[0m : 2.70842
[1mStep[0m  [160/169], [94mLoss[0m : 2.34684

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.369, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94567
[1mStep[0m  [16/169], [94mLoss[0m : 2.12472
[1mStep[0m  [32/169], [94mLoss[0m : 2.28670
[1mStep[0m  [48/169], [94mLoss[0m : 2.35762
[1mStep[0m  [64/169], [94mLoss[0m : 2.77205
[1mStep[0m  [80/169], [94mLoss[0m : 2.54431
[1mStep[0m  [96/169], [94mLoss[0m : 3.08743
[1mStep[0m  [112/169], [94mLoss[0m : 2.62693
[1mStep[0m  [128/169], [94mLoss[0m : 2.60708
[1mStep[0m  [144/169], [94mLoss[0m : 2.83538
[1mStep[0m  [160/169], [94mLoss[0m : 2.57216

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21494
[1mStep[0m  [16/169], [94mLoss[0m : 3.21072
[1mStep[0m  [32/169], [94mLoss[0m : 2.23171
[1mStep[0m  [48/169], [94mLoss[0m : 2.83534
[1mStep[0m  [64/169], [94mLoss[0m : 2.25176
[1mStep[0m  [80/169], [94mLoss[0m : 2.45478
[1mStep[0m  [96/169], [94mLoss[0m : 2.40126
[1mStep[0m  [112/169], [94mLoss[0m : 2.51315
[1mStep[0m  [128/169], [94mLoss[0m : 2.26480
[1mStep[0m  [144/169], [94mLoss[0m : 2.74654
[1mStep[0m  [160/169], [94mLoss[0m : 2.16010

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70429
[1mStep[0m  [16/169], [94mLoss[0m : 2.54729
[1mStep[0m  [32/169], [94mLoss[0m : 1.98243
[1mStep[0m  [48/169], [94mLoss[0m : 2.69918
[1mStep[0m  [64/169], [94mLoss[0m : 2.17363
[1mStep[0m  [80/169], [94mLoss[0m : 2.44061
[1mStep[0m  [96/169], [94mLoss[0m : 2.93549
[1mStep[0m  [112/169], [94mLoss[0m : 2.48199
[1mStep[0m  [128/169], [94mLoss[0m : 2.48273
[1mStep[0m  [144/169], [94mLoss[0m : 2.92710
[1mStep[0m  [160/169], [94mLoss[0m : 2.88921

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.383, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94927
[1mStep[0m  [16/169], [94mLoss[0m : 2.52824
[1mStep[0m  [32/169], [94mLoss[0m : 2.49682
[1mStep[0m  [48/169], [94mLoss[0m : 2.45340
[1mStep[0m  [64/169], [94mLoss[0m : 2.58376
[1mStep[0m  [80/169], [94mLoss[0m : 2.63011
[1mStep[0m  [96/169], [94mLoss[0m : 2.52164
[1mStep[0m  [112/169], [94mLoss[0m : 2.34577
[1mStep[0m  [128/169], [94mLoss[0m : 2.61096
[1mStep[0m  [144/169], [94mLoss[0m : 2.21091
[1mStep[0m  [160/169], [94mLoss[0m : 2.62688

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.331, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88014
[1mStep[0m  [16/169], [94mLoss[0m : 2.21881
[1mStep[0m  [32/169], [94mLoss[0m : 2.12050
[1mStep[0m  [48/169], [94mLoss[0m : 2.02303
[1mStep[0m  [64/169], [94mLoss[0m : 2.66682
[1mStep[0m  [80/169], [94mLoss[0m : 2.70530
[1mStep[0m  [96/169], [94mLoss[0m : 2.06694
[1mStep[0m  [112/169], [94mLoss[0m : 2.46939
[1mStep[0m  [128/169], [94mLoss[0m : 2.83435
[1mStep[0m  [144/169], [94mLoss[0m : 2.51800
[1mStep[0m  [160/169], [94mLoss[0m : 2.64045

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38853
[1mStep[0m  [16/169], [94mLoss[0m : 2.28104
[1mStep[0m  [32/169], [94mLoss[0m : 2.12938
[1mStep[0m  [48/169], [94mLoss[0m : 2.30022
[1mStep[0m  [64/169], [94mLoss[0m : 2.14784
[1mStep[0m  [80/169], [94mLoss[0m : 2.43971
[1mStep[0m  [96/169], [94mLoss[0m : 2.34206
[1mStep[0m  [112/169], [94mLoss[0m : 2.84173
[1mStep[0m  [128/169], [94mLoss[0m : 2.36970
[1mStep[0m  [144/169], [94mLoss[0m : 2.44594
[1mStep[0m  [160/169], [94mLoss[0m : 2.58247

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25007
[1mStep[0m  [16/169], [94mLoss[0m : 1.78922
[1mStep[0m  [32/169], [94mLoss[0m : 2.61818
[1mStep[0m  [48/169], [94mLoss[0m : 2.35251
[1mStep[0m  [64/169], [94mLoss[0m : 2.89864
[1mStep[0m  [80/169], [94mLoss[0m : 2.46996
[1mStep[0m  [96/169], [94mLoss[0m : 2.34511
[1mStep[0m  [112/169], [94mLoss[0m : 2.60793
[1mStep[0m  [128/169], [94mLoss[0m : 2.47132
[1mStep[0m  [144/169], [94mLoss[0m : 2.59946
[1mStep[0m  [160/169], [94mLoss[0m : 2.19862

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.362, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30802
[1mStep[0m  [16/169], [94mLoss[0m : 2.40391
[1mStep[0m  [32/169], [94mLoss[0m : 2.65427
[1mStep[0m  [48/169], [94mLoss[0m : 2.13238
[1mStep[0m  [64/169], [94mLoss[0m : 2.36926
[1mStep[0m  [80/169], [94mLoss[0m : 2.54190
[1mStep[0m  [96/169], [94mLoss[0m : 2.14088
[1mStep[0m  [112/169], [94mLoss[0m : 2.45882
[1mStep[0m  [128/169], [94mLoss[0m : 2.84241
[1mStep[0m  [144/169], [94mLoss[0m : 2.59329
[1mStep[0m  [160/169], [94mLoss[0m : 2.60352

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23420
[1mStep[0m  [16/169], [94mLoss[0m : 2.13723
[1mStep[0m  [32/169], [94mLoss[0m : 2.37426
[1mStep[0m  [48/169], [94mLoss[0m : 2.47881
[1mStep[0m  [64/169], [94mLoss[0m : 2.54859
[1mStep[0m  [80/169], [94mLoss[0m : 2.37132
[1mStep[0m  [96/169], [94mLoss[0m : 2.81242
[1mStep[0m  [112/169], [94mLoss[0m : 2.30074
[1mStep[0m  [128/169], [94mLoss[0m : 2.71806
[1mStep[0m  [144/169], [94mLoss[0m : 2.49598
[1mStep[0m  [160/169], [94mLoss[0m : 2.48900

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.349, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.09028
[1mStep[0m  [16/169], [94mLoss[0m : 2.48063
[1mStep[0m  [32/169], [94mLoss[0m : 2.29830
[1mStep[0m  [48/169], [94mLoss[0m : 2.49391
[1mStep[0m  [64/169], [94mLoss[0m : 2.41720
[1mStep[0m  [80/169], [94mLoss[0m : 2.59218
[1mStep[0m  [96/169], [94mLoss[0m : 2.25130
[1mStep[0m  [112/169], [94mLoss[0m : 2.39712
[1mStep[0m  [128/169], [94mLoss[0m : 2.51492
[1mStep[0m  [144/169], [94mLoss[0m : 3.05907
[1mStep[0m  [160/169], [94mLoss[0m : 2.43660

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44942
[1mStep[0m  [16/169], [94mLoss[0m : 2.82028
[1mStep[0m  [32/169], [94mLoss[0m : 2.47629
[1mStep[0m  [48/169], [94mLoss[0m : 2.09685
[1mStep[0m  [64/169], [94mLoss[0m : 2.14022
[1mStep[0m  [80/169], [94mLoss[0m : 2.59540
[1mStep[0m  [96/169], [94mLoss[0m : 2.36346
[1mStep[0m  [112/169], [94mLoss[0m : 2.05315
[1mStep[0m  [128/169], [94mLoss[0m : 2.49007
[1mStep[0m  [144/169], [94mLoss[0m : 2.30848
[1mStep[0m  [160/169], [94mLoss[0m : 2.69657

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.365, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44702
[1mStep[0m  [16/169], [94mLoss[0m : 2.54604
[1mStep[0m  [32/169], [94mLoss[0m : 2.43346
[1mStep[0m  [48/169], [94mLoss[0m : 2.64232
[1mStep[0m  [64/169], [94mLoss[0m : 2.73711
[1mStep[0m  [80/169], [94mLoss[0m : 2.53387
[1mStep[0m  [96/169], [94mLoss[0m : 2.56883
[1mStep[0m  [112/169], [94mLoss[0m : 2.46425
[1mStep[0m  [128/169], [94mLoss[0m : 2.43625
[1mStep[0m  [144/169], [94mLoss[0m : 2.26428
[1mStep[0m  [160/169], [94mLoss[0m : 2.35320

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.355, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55502
[1mStep[0m  [16/169], [94mLoss[0m : 2.17899
[1mStep[0m  [32/169], [94mLoss[0m : 2.92039
[1mStep[0m  [48/169], [94mLoss[0m : 2.33031
[1mStep[0m  [64/169], [94mLoss[0m : 2.55629
[1mStep[0m  [80/169], [94mLoss[0m : 2.35376
[1mStep[0m  [96/169], [94mLoss[0m : 2.16070
[1mStep[0m  [112/169], [94mLoss[0m : 2.45072
[1mStep[0m  [128/169], [94mLoss[0m : 2.39592
[1mStep[0m  [144/169], [94mLoss[0m : 2.26997
[1mStep[0m  [160/169], [94mLoss[0m : 2.42406

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.334, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26419
[1mStep[0m  [16/169], [94mLoss[0m : 2.62688
[1mStep[0m  [32/169], [94mLoss[0m : 2.13330
[1mStep[0m  [48/169], [94mLoss[0m : 2.40054
[1mStep[0m  [64/169], [94mLoss[0m : 2.45125
[1mStep[0m  [80/169], [94mLoss[0m : 2.74715
[1mStep[0m  [96/169], [94mLoss[0m : 2.67689
[1mStep[0m  [112/169], [94mLoss[0m : 2.37886
[1mStep[0m  [128/169], [94mLoss[0m : 2.23797
[1mStep[0m  [144/169], [94mLoss[0m : 2.26154
[1mStep[0m  [160/169], [94mLoss[0m : 2.28858

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60442
[1mStep[0m  [16/169], [94mLoss[0m : 2.25300
[1mStep[0m  [32/169], [94mLoss[0m : 2.17406
[1mStep[0m  [48/169], [94mLoss[0m : 2.92271
[1mStep[0m  [64/169], [94mLoss[0m : 2.57388
[1mStep[0m  [80/169], [94mLoss[0m : 2.43115
[1mStep[0m  [96/169], [94mLoss[0m : 2.39533
[1mStep[0m  [112/169], [94mLoss[0m : 2.44516
[1mStep[0m  [128/169], [94mLoss[0m : 2.50075
[1mStep[0m  [144/169], [94mLoss[0m : 2.01783
[1mStep[0m  [160/169], [94mLoss[0m : 2.95564

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77284
[1mStep[0m  [16/169], [94mLoss[0m : 2.07774
[1mStep[0m  [32/169], [94mLoss[0m : 2.16219
[1mStep[0m  [48/169], [94mLoss[0m : 2.26913
[1mStep[0m  [64/169], [94mLoss[0m : 2.48492
[1mStep[0m  [80/169], [94mLoss[0m : 2.20780
[1mStep[0m  [96/169], [94mLoss[0m : 2.83868
[1mStep[0m  [112/169], [94mLoss[0m : 2.33645
[1mStep[0m  [128/169], [94mLoss[0m : 1.99569
[1mStep[0m  [144/169], [94mLoss[0m : 2.44745
[1mStep[0m  [160/169], [94mLoss[0m : 2.63380

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.354, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57089
[1mStep[0m  [16/169], [94mLoss[0m : 2.78611
[1mStep[0m  [32/169], [94mLoss[0m : 2.16899
[1mStep[0m  [48/169], [94mLoss[0m : 2.55025
[1mStep[0m  [64/169], [94mLoss[0m : 2.45931
[1mStep[0m  [80/169], [94mLoss[0m : 1.87695
[1mStep[0m  [96/169], [94mLoss[0m : 2.68053
[1mStep[0m  [112/169], [94mLoss[0m : 2.09606
[1mStep[0m  [128/169], [94mLoss[0m : 2.65212
[1mStep[0m  [144/169], [94mLoss[0m : 2.36271
[1mStep[0m  [160/169], [94mLoss[0m : 2.47914

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.333, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38318
[1mStep[0m  [16/169], [94mLoss[0m : 2.32174
[1mStep[0m  [32/169], [94mLoss[0m : 2.50221
[1mStep[0m  [48/169], [94mLoss[0m : 2.30673
[1mStep[0m  [64/169], [94mLoss[0m : 2.68702
[1mStep[0m  [80/169], [94mLoss[0m : 2.53759
[1mStep[0m  [96/169], [94mLoss[0m : 2.44065
[1mStep[0m  [112/169], [94mLoss[0m : 2.75561
[1mStep[0m  [128/169], [94mLoss[0m : 2.21573
[1mStep[0m  [144/169], [94mLoss[0m : 2.38862
[1mStep[0m  [160/169], [94mLoss[0m : 2.22279

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.347, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28423
[1mStep[0m  [16/169], [94mLoss[0m : 2.71254
[1mStep[0m  [32/169], [94mLoss[0m : 2.32675
[1mStep[0m  [48/169], [94mLoss[0m : 2.39075
[1mStep[0m  [64/169], [94mLoss[0m : 1.96653
[1mStep[0m  [80/169], [94mLoss[0m : 2.42374
[1mStep[0m  [96/169], [94mLoss[0m : 2.21157
[1mStep[0m  [112/169], [94mLoss[0m : 2.38706
[1mStep[0m  [128/169], [94mLoss[0m : 2.31396
[1mStep[0m  [144/169], [94mLoss[0m : 2.35681
[1mStep[0m  [160/169], [94mLoss[0m : 2.25253

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17810
[1mStep[0m  [16/169], [94mLoss[0m : 2.29982
[1mStep[0m  [32/169], [94mLoss[0m : 2.01260
[1mStep[0m  [48/169], [94mLoss[0m : 3.13105
[1mStep[0m  [64/169], [94mLoss[0m : 2.21204
[1mStep[0m  [80/169], [94mLoss[0m : 2.33570
[1mStep[0m  [96/169], [94mLoss[0m : 2.63323
[1mStep[0m  [112/169], [94mLoss[0m : 2.29224
[1mStep[0m  [128/169], [94mLoss[0m : 2.41755
[1mStep[0m  [144/169], [94mLoss[0m : 3.13958
[1mStep[0m  [160/169], [94mLoss[0m : 2.40224

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.336, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06849
[1mStep[0m  [16/169], [94mLoss[0m : 2.89070
[1mStep[0m  [32/169], [94mLoss[0m : 2.58812
[1mStep[0m  [48/169], [94mLoss[0m : 2.28841
[1mStep[0m  [64/169], [94mLoss[0m : 1.96520
[1mStep[0m  [80/169], [94mLoss[0m : 2.63219
[1mStep[0m  [96/169], [94mLoss[0m : 2.35668
[1mStep[0m  [112/169], [94mLoss[0m : 1.90254
[1mStep[0m  [128/169], [94mLoss[0m : 2.82904
[1mStep[0m  [144/169], [94mLoss[0m : 2.09415
[1mStep[0m  [160/169], [94mLoss[0m : 2.24295

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.345, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29922
[1mStep[0m  [16/169], [94mLoss[0m : 2.69379
[1mStep[0m  [32/169], [94mLoss[0m : 2.47411
[1mStep[0m  [48/169], [94mLoss[0m : 2.01866
[1mStep[0m  [64/169], [94mLoss[0m : 2.11220
[1mStep[0m  [80/169], [94mLoss[0m : 2.72382
[1mStep[0m  [96/169], [94mLoss[0m : 2.12516
[1mStep[0m  [112/169], [94mLoss[0m : 2.25944
[1mStep[0m  [128/169], [94mLoss[0m : 2.13309
[1mStep[0m  [144/169], [94mLoss[0m : 2.12035
[1mStep[0m  [160/169], [94mLoss[0m : 2.10903

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.330, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46854
[1mStep[0m  [16/169], [94mLoss[0m : 2.40972
[1mStep[0m  [32/169], [94mLoss[0m : 3.02260
[1mStep[0m  [48/169], [94mLoss[0m : 2.35980
[1mStep[0m  [64/169], [94mLoss[0m : 2.21967
[1mStep[0m  [80/169], [94mLoss[0m : 2.18373
[1mStep[0m  [96/169], [94mLoss[0m : 2.22755
[1mStep[0m  [112/169], [94mLoss[0m : 3.13321
[1mStep[0m  [128/169], [94mLoss[0m : 2.16213
[1mStep[0m  [144/169], [94mLoss[0m : 2.72774
[1mStep[0m  [160/169], [94mLoss[0m : 2.07274

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.338, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42880
[1mStep[0m  [16/169], [94mLoss[0m : 2.41836
[1mStep[0m  [32/169], [94mLoss[0m : 2.71376
[1mStep[0m  [48/169], [94mLoss[0m : 2.23593
[1mStep[0m  [64/169], [94mLoss[0m : 2.36662
[1mStep[0m  [80/169], [94mLoss[0m : 2.31479
[1mStep[0m  [96/169], [94mLoss[0m : 2.30510
[1mStep[0m  [112/169], [94mLoss[0m : 2.58828
[1mStep[0m  [128/169], [94mLoss[0m : 2.27623
[1mStep[0m  [144/169], [94mLoss[0m : 2.20999
[1mStep[0m  [160/169], [94mLoss[0m : 2.33279

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38820
[1mStep[0m  [16/169], [94mLoss[0m : 2.55911
[1mStep[0m  [32/169], [94mLoss[0m : 1.92823
[1mStep[0m  [48/169], [94mLoss[0m : 2.62703
[1mStep[0m  [64/169], [94mLoss[0m : 2.41822
[1mStep[0m  [80/169], [94mLoss[0m : 2.15973
[1mStep[0m  [96/169], [94mLoss[0m : 2.46261
[1mStep[0m  [112/169], [94mLoss[0m : 2.28404
[1mStep[0m  [128/169], [94mLoss[0m : 2.62764
[1mStep[0m  [144/169], [94mLoss[0m : 2.72581
[1mStep[0m  [160/169], [94mLoss[0m : 2.59111

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.322, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51334
[1mStep[0m  [16/169], [94mLoss[0m : 2.15593
[1mStep[0m  [32/169], [94mLoss[0m : 2.13815
[1mStep[0m  [48/169], [94mLoss[0m : 2.11664
[1mStep[0m  [64/169], [94mLoss[0m : 2.77793
[1mStep[0m  [80/169], [94mLoss[0m : 2.52553
[1mStep[0m  [96/169], [94mLoss[0m : 2.15012
[1mStep[0m  [112/169], [94mLoss[0m : 2.31951
[1mStep[0m  [128/169], [94mLoss[0m : 2.04065
[1mStep[0m  [144/169], [94mLoss[0m : 2.31017
[1mStep[0m  [160/169], [94mLoss[0m : 2.70812

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.343
====================================

Phase 1 - Evaluation MAE:  2.3429778622729436
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.34848
[1mStep[0m  [16/169], [94mLoss[0m : 2.51848
[1mStep[0m  [32/169], [94mLoss[0m : 2.44431
[1mStep[0m  [48/169], [94mLoss[0m : 2.63676
[1mStep[0m  [64/169], [94mLoss[0m : 2.61579
[1mStep[0m  [80/169], [94mLoss[0m : 2.38493
[1mStep[0m  [96/169], [94mLoss[0m : 2.37570
[1mStep[0m  [112/169], [94mLoss[0m : 2.68758
[1mStep[0m  [128/169], [94mLoss[0m : 2.70608
[1mStep[0m  [144/169], [94mLoss[0m : 2.45356
[1mStep[0m  [160/169], [94mLoss[0m : 2.80688

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56788
[1mStep[0m  [16/169], [94mLoss[0m : 2.06279
[1mStep[0m  [32/169], [94mLoss[0m : 2.27844
[1mStep[0m  [48/169], [94mLoss[0m : 2.67079
[1mStep[0m  [64/169], [94mLoss[0m : 2.60955
[1mStep[0m  [80/169], [94mLoss[0m : 2.50621
[1mStep[0m  [96/169], [94mLoss[0m : 2.48490
[1mStep[0m  [112/169], [94mLoss[0m : 2.27139
[1mStep[0m  [128/169], [94mLoss[0m : 2.76457
[1mStep[0m  [144/169], [94mLoss[0m : 2.22531
[1mStep[0m  [160/169], [94mLoss[0m : 2.47097

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36330
[1mStep[0m  [16/169], [94mLoss[0m : 2.48578
[1mStep[0m  [32/169], [94mLoss[0m : 2.43163
[1mStep[0m  [48/169], [94mLoss[0m : 2.80689
[1mStep[0m  [64/169], [94mLoss[0m : 2.82376
[1mStep[0m  [80/169], [94mLoss[0m : 2.18038
[1mStep[0m  [96/169], [94mLoss[0m : 2.20950
[1mStep[0m  [112/169], [94mLoss[0m : 2.34422
[1mStep[0m  [128/169], [94mLoss[0m : 1.92172
[1mStep[0m  [144/169], [94mLoss[0m : 1.80506
[1mStep[0m  [160/169], [94mLoss[0m : 2.23068

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28134
[1mStep[0m  [16/169], [94mLoss[0m : 2.40601
[1mStep[0m  [32/169], [94mLoss[0m : 2.20720
[1mStep[0m  [48/169], [94mLoss[0m : 2.32214
[1mStep[0m  [64/169], [94mLoss[0m : 2.65184
[1mStep[0m  [80/169], [94mLoss[0m : 2.49971
[1mStep[0m  [96/169], [94mLoss[0m : 1.90033
[1mStep[0m  [112/169], [94mLoss[0m : 2.21527
[1mStep[0m  [128/169], [94mLoss[0m : 2.04037
[1mStep[0m  [144/169], [94mLoss[0m : 2.56031
[1mStep[0m  [160/169], [94mLoss[0m : 2.04227

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32941
[1mStep[0m  [16/169], [94mLoss[0m : 2.10419
[1mStep[0m  [32/169], [94mLoss[0m : 1.84617
[1mStep[0m  [48/169], [94mLoss[0m : 2.18937
[1mStep[0m  [64/169], [94mLoss[0m : 2.13615
[1mStep[0m  [80/169], [94mLoss[0m : 2.13100
[1mStep[0m  [96/169], [94mLoss[0m : 1.96780
[1mStep[0m  [112/169], [94mLoss[0m : 2.55576
[1mStep[0m  [128/169], [94mLoss[0m : 1.86601
[1mStep[0m  [144/169], [94mLoss[0m : 2.01472
[1mStep[0m  [160/169], [94mLoss[0m : 2.41771

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05950
[1mStep[0m  [16/169], [94mLoss[0m : 2.16230
[1mStep[0m  [32/169], [94mLoss[0m : 1.94451
[1mStep[0m  [48/169], [94mLoss[0m : 1.96642
[1mStep[0m  [64/169], [94mLoss[0m : 2.08102
[1mStep[0m  [80/169], [94mLoss[0m : 1.95500
[1mStep[0m  [96/169], [94mLoss[0m : 2.35012
[1mStep[0m  [112/169], [94mLoss[0m : 2.26118
[1mStep[0m  [128/169], [94mLoss[0m : 1.96469
[1mStep[0m  [144/169], [94mLoss[0m : 2.15716
[1mStep[0m  [160/169], [94mLoss[0m : 2.08874

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.361, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76720
[1mStep[0m  [16/169], [94mLoss[0m : 2.00522
[1mStep[0m  [32/169], [94mLoss[0m : 2.27411
[1mStep[0m  [48/169], [94mLoss[0m : 2.24400
[1mStep[0m  [64/169], [94mLoss[0m : 1.67665
[1mStep[0m  [80/169], [94mLoss[0m : 2.00100
[1mStep[0m  [96/169], [94mLoss[0m : 2.51647
[1mStep[0m  [112/169], [94mLoss[0m : 2.42295
[1mStep[0m  [128/169], [94mLoss[0m : 2.02613
[1mStep[0m  [144/169], [94mLoss[0m : 2.21820
[1mStep[0m  [160/169], [94mLoss[0m : 2.13875

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39364
[1mStep[0m  [16/169], [94mLoss[0m : 2.22801
[1mStep[0m  [32/169], [94mLoss[0m : 1.96461
[1mStep[0m  [48/169], [94mLoss[0m : 2.14669
[1mStep[0m  [64/169], [94mLoss[0m : 1.81502
[1mStep[0m  [80/169], [94mLoss[0m : 2.27919
[1mStep[0m  [96/169], [94mLoss[0m : 2.58866
[1mStep[0m  [112/169], [94mLoss[0m : 1.91387
[1mStep[0m  [128/169], [94mLoss[0m : 1.98297
[1mStep[0m  [144/169], [94mLoss[0m : 1.74857
[1mStep[0m  [160/169], [94mLoss[0m : 1.96457

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85839
[1mStep[0m  [16/169], [94mLoss[0m : 1.94482
[1mStep[0m  [32/169], [94mLoss[0m : 1.60325
[1mStep[0m  [48/169], [94mLoss[0m : 1.75064
[1mStep[0m  [64/169], [94mLoss[0m : 2.15162
[1mStep[0m  [80/169], [94mLoss[0m : 2.14504
[1mStep[0m  [96/169], [94mLoss[0m : 2.11136
[1mStep[0m  [112/169], [94mLoss[0m : 2.02074
[1mStep[0m  [128/169], [94mLoss[0m : 1.92672
[1mStep[0m  [144/169], [94mLoss[0m : 2.11847
[1mStep[0m  [160/169], [94mLoss[0m : 1.93625

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.409, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82223
[1mStep[0m  [16/169], [94mLoss[0m : 2.20947
[1mStep[0m  [32/169], [94mLoss[0m : 2.18161
[1mStep[0m  [48/169], [94mLoss[0m : 2.18401
[1mStep[0m  [64/169], [94mLoss[0m : 1.93731
[1mStep[0m  [80/169], [94mLoss[0m : 2.22317
[1mStep[0m  [96/169], [94mLoss[0m : 1.88166
[1mStep[0m  [112/169], [94mLoss[0m : 1.99564
[1mStep[0m  [128/169], [94mLoss[0m : 2.10421
[1mStep[0m  [144/169], [94mLoss[0m : 1.69292
[1mStep[0m  [160/169], [94mLoss[0m : 1.85133

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.969, [92mTest[0m: 2.399, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01169
[1mStep[0m  [16/169], [94mLoss[0m : 2.01808
[1mStep[0m  [32/169], [94mLoss[0m : 1.84256
[1mStep[0m  [48/169], [94mLoss[0m : 2.06895
[1mStep[0m  [64/169], [94mLoss[0m : 2.19413
[1mStep[0m  [80/169], [94mLoss[0m : 1.70316
[1mStep[0m  [96/169], [94mLoss[0m : 2.50339
[1mStep[0m  [112/169], [94mLoss[0m : 2.04871
[1mStep[0m  [128/169], [94mLoss[0m : 2.01628
[1mStep[0m  [144/169], [94mLoss[0m : 1.70082
[1mStep[0m  [160/169], [94mLoss[0m : 1.80391

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.937, [92mTest[0m: 2.445, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98634
[1mStep[0m  [16/169], [94mLoss[0m : 2.12611
[1mStep[0m  [32/169], [94mLoss[0m : 1.72871
[1mStep[0m  [48/169], [94mLoss[0m : 2.08121
[1mStep[0m  [64/169], [94mLoss[0m : 1.51650
[1mStep[0m  [80/169], [94mLoss[0m : 1.67206
[1mStep[0m  [96/169], [94mLoss[0m : 1.65240
[1mStep[0m  [112/169], [94mLoss[0m : 2.00310
[1mStep[0m  [128/169], [94mLoss[0m : 1.78208
[1mStep[0m  [144/169], [94mLoss[0m : 1.81817
[1mStep[0m  [160/169], [94mLoss[0m : 2.13646

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02594
[1mStep[0m  [16/169], [94mLoss[0m : 1.65931
[1mStep[0m  [32/169], [94mLoss[0m : 1.81516
[1mStep[0m  [48/169], [94mLoss[0m : 1.80149
[1mStep[0m  [64/169], [94mLoss[0m : 1.81632
[1mStep[0m  [80/169], [94mLoss[0m : 2.13640
[1mStep[0m  [96/169], [94mLoss[0m : 1.89344
[1mStep[0m  [112/169], [94mLoss[0m : 2.06154
[1mStep[0m  [128/169], [94mLoss[0m : 1.89458
[1mStep[0m  [144/169], [94mLoss[0m : 2.01815
[1mStep[0m  [160/169], [94mLoss[0m : 1.77205

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.457, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26433
[1mStep[0m  [16/169], [94mLoss[0m : 1.72971
[1mStep[0m  [32/169], [94mLoss[0m : 1.84919
[1mStep[0m  [48/169], [94mLoss[0m : 2.03906
[1mStep[0m  [64/169], [94mLoss[0m : 1.86619
[1mStep[0m  [80/169], [94mLoss[0m : 1.80268
[1mStep[0m  [96/169], [94mLoss[0m : 1.76363
[1mStep[0m  [112/169], [94mLoss[0m : 2.03037
[1mStep[0m  [128/169], [94mLoss[0m : 1.95761
[1mStep[0m  [144/169], [94mLoss[0m : 2.00628
[1mStep[0m  [160/169], [94mLoss[0m : 1.81036

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.441, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73823
[1mStep[0m  [16/169], [94mLoss[0m : 1.80675
[1mStep[0m  [32/169], [94mLoss[0m : 2.02267
[1mStep[0m  [48/169], [94mLoss[0m : 1.96983
[1mStep[0m  [64/169], [94mLoss[0m : 1.70018
[1mStep[0m  [80/169], [94mLoss[0m : 1.86189
[1mStep[0m  [96/169], [94mLoss[0m : 1.60571
[1mStep[0m  [112/169], [94mLoss[0m : 2.30428
[1mStep[0m  [128/169], [94mLoss[0m : 1.83465
[1mStep[0m  [144/169], [94mLoss[0m : 1.71322
[1mStep[0m  [160/169], [94mLoss[0m : 1.76099

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.484, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46956
[1mStep[0m  [16/169], [94mLoss[0m : 1.48530
[1mStep[0m  [32/169], [94mLoss[0m : 1.87377
[1mStep[0m  [48/169], [94mLoss[0m : 1.65761
[1mStep[0m  [64/169], [94mLoss[0m : 1.72509
[1mStep[0m  [80/169], [94mLoss[0m : 1.82644
[1mStep[0m  [96/169], [94mLoss[0m : 1.77542
[1mStep[0m  [112/169], [94mLoss[0m : 1.84091
[1mStep[0m  [128/169], [94mLoss[0m : 2.07786
[1mStep[0m  [144/169], [94mLoss[0m : 1.60838
[1mStep[0m  [160/169], [94mLoss[0m : 1.79018

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.748, [92mTest[0m: 2.468, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20929
[1mStep[0m  [16/169], [94mLoss[0m : 1.82963
[1mStep[0m  [32/169], [94mLoss[0m : 1.95714
[1mStep[0m  [48/169], [94mLoss[0m : 1.50954
[1mStep[0m  [64/169], [94mLoss[0m : 1.46726
[1mStep[0m  [80/169], [94mLoss[0m : 1.63550
[1mStep[0m  [96/169], [94mLoss[0m : 1.25638
[1mStep[0m  [112/169], [94mLoss[0m : 1.79386
[1mStep[0m  [128/169], [94mLoss[0m : 2.19689
[1mStep[0m  [144/169], [94mLoss[0m : 1.71883
[1mStep[0m  [160/169], [94mLoss[0m : 1.97690

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.482, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17379
[1mStep[0m  [16/169], [94mLoss[0m : 1.57546
[1mStep[0m  [32/169], [94mLoss[0m : 1.61512
[1mStep[0m  [48/169], [94mLoss[0m : 1.81697
[1mStep[0m  [64/169], [94mLoss[0m : 1.94486
[1mStep[0m  [80/169], [94mLoss[0m : 1.62744
[1mStep[0m  [96/169], [94mLoss[0m : 1.65701
[1mStep[0m  [112/169], [94mLoss[0m : 1.79015
[1mStep[0m  [128/169], [94mLoss[0m : 1.54185
[1mStep[0m  [144/169], [94mLoss[0m : 1.84314
[1mStep[0m  [160/169], [94mLoss[0m : 1.97651

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.436, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79852
[1mStep[0m  [16/169], [94mLoss[0m : 1.93586
[1mStep[0m  [32/169], [94mLoss[0m : 1.60240
[1mStep[0m  [48/169], [94mLoss[0m : 1.59625
[1mStep[0m  [64/169], [94mLoss[0m : 1.95519
[1mStep[0m  [80/169], [94mLoss[0m : 1.58018
[1mStep[0m  [96/169], [94mLoss[0m : 1.94910
[1mStep[0m  [112/169], [94mLoss[0m : 1.71360
[1mStep[0m  [128/169], [94mLoss[0m : 1.33599
[1mStep[0m  [144/169], [94mLoss[0m : 2.02341
[1mStep[0m  [160/169], [94mLoss[0m : 1.48759

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.656, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47386
[1mStep[0m  [16/169], [94mLoss[0m : 1.58531
[1mStep[0m  [32/169], [94mLoss[0m : 1.42976
[1mStep[0m  [48/169], [94mLoss[0m : 1.97009
[1mStep[0m  [64/169], [94mLoss[0m : 1.60879
[1mStep[0m  [80/169], [94mLoss[0m : 1.81350
[1mStep[0m  [96/169], [94mLoss[0m : 1.54132
[1mStep[0m  [112/169], [94mLoss[0m : 1.54041
[1mStep[0m  [128/169], [94mLoss[0m : 1.70556
[1mStep[0m  [144/169], [94mLoss[0m : 1.67091
[1mStep[0m  [160/169], [94mLoss[0m : 2.01797

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.641, [92mTest[0m: 2.476, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89123
[1mStep[0m  [16/169], [94mLoss[0m : 1.44692
[1mStep[0m  [32/169], [94mLoss[0m : 1.71856
[1mStep[0m  [48/169], [94mLoss[0m : 1.70629
[1mStep[0m  [64/169], [94mLoss[0m : 1.78200
[1mStep[0m  [80/169], [94mLoss[0m : 1.76673
[1mStep[0m  [96/169], [94mLoss[0m : 1.64226
[1mStep[0m  [112/169], [94mLoss[0m : 1.49898
[1mStep[0m  [128/169], [94mLoss[0m : 1.46347
[1mStep[0m  [144/169], [94mLoss[0m : 1.72310
[1mStep[0m  [160/169], [94mLoss[0m : 1.57963

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.518, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02656
[1mStep[0m  [16/169], [94mLoss[0m : 1.62127
[1mStep[0m  [32/169], [94mLoss[0m : 1.83059
[1mStep[0m  [48/169], [94mLoss[0m : 1.71775
[1mStep[0m  [64/169], [94mLoss[0m : 1.52220
[1mStep[0m  [80/169], [94mLoss[0m : 1.54117
[1mStep[0m  [96/169], [94mLoss[0m : 1.79422
[1mStep[0m  [112/169], [94mLoss[0m : 1.58735
[1mStep[0m  [128/169], [94mLoss[0m : 1.58901
[1mStep[0m  [144/169], [94mLoss[0m : 1.49426
[1mStep[0m  [160/169], [94mLoss[0m : 1.79562

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.557, [92mTest[0m: 2.499, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47405
[1mStep[0m  [16/169], [94mLoss[0m : 1.48413
[1mStep[0m  [32/169], [94mLoss[0m : 1.18226
[1mStep[0m  [48/169], [94mLoss[0m : 1.60003
[1mStep[0m  [64/169], [94mLoss[0m : 1.70639
[1mStep[0m  [80/169], [94mLoss[0m : 1.37042
[1mStep[0m  [96/169], [94mLoss[0m : 1.46787
[1mStep[0m  [112/169], [94mLoss[0m : 1.82563
[1mStep[0m  [128/169], [94mLoss[0m : 1.74605
[1mStep[0m  [144/169], [94mLoss[0m : 1.58459
[1mStep[0m  [160/169], [94mLoss[0m : 1.52625

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.488, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42042
[1mStep[0m  [16/169], [94mLoss[0m : 1.50950
[1mStep[0m  [32/169], [94mLoss[0m : 1.22702
[1mStep[0m  [48/169], [94mLoss[0m : 1.62929
[1mStep[0m  [64/169], [94mLoss[0m : 1.99839
[1mStep[0m  [80/169], [94mLoss[0m : 1.52978
[1mStep[0m  [96/169], [94mLoss[0m : 1.62867
[1mStep[0m  [112/169], [94mLoss[0m : 1.52052
[1mStep[0m  [128/169], [94mLoss[0m : 1.24782
[1mStep[0m  [144/169], [94mLoss[0m : 1.37090
[1mStep[0m  [160/169], [94mLoss[0m : 1.58703

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.522, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.24889
[1mStep[0m  [16/169], [94mLoss[0m : 1.11199
[1mStep[0m  [32/169], [94mLoss[0m : 1.43578
[1mStep[0m  [48/169], [94mLoss[0m : 1.55939
[1mStep[0m  [64/169], [94mLoss[0m : 1.52488
[1mStep[0m  [80/169], [94mLoss[0m : 1.64962
[1mStep[0m  [96/169], [94mLoss[0m : 1.49682
[1mStep[0m  [112/169], [94mLoss[0m : 1.52985
[1mStep[0m  [128/169], [94mLoss[0m : 1.60876
[1mStep[0m  [144/169], [94mLoss[0m : 1.61415
[1mStep[0m  [160/169], [94mLoss[0m : 1.22533

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28766
[1mStep[0m  [16/169], [94mLoss[0m : 1.36959
[1mStep[0m  [32/169], [94mLoss[0m : 1.49575
[1mStep[0m  [48/169], [94mLoss[0m : 1.42215
[1mStep[0m  [64/169], [94mLoss[0m : 1.35607
[1mStep[0m  [80/169], [94mLoss[0m : 1.47045
[1mStep[0m  [96/169], [94mLoss[0m : 1.49748
[1mStep[0m  [112/169], [94mLoss[0m : 1.35687
[1mStep[0m  [128/169], [94mLoss[0m : 1.86189
[1mStep[0m  [144/169], [94mLoss[0m : 1.79213
[1mStep[0m  [160/169], [94mLoss[0m : 1.43509

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.470, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.43858
[1mStep[0m  [16/169], [94mLoss[0m : 1.57365
[1mStep[0m  [32/169], [94mLoss[0m : 1.44756
[1mStep[0m  [48/169], [94mLoss[0m : 1.21871
[1mStep[0m  [64/169], [94mLoss[0m : 1.50543
[1mStep[0m  [80/169], [94mLoss[0m : 1.59359
[1mStep[0m  [96/169], [94mLoss[0m : 1.51166
[1mStep[0m  [112/169], [94mLoss[0m : 1.42639
[1mStep[0m  [128/169], [94mLoss[0m : 1.46767
[1mStep[0m  [144/169], [94mLoss[0m : 1.20288
[1mStep[0m  [160/169], [94mLoss[0m : 1.73387

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.512, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36799
[1mStep[0m  [16/169], [94mLoss[0m : 1.31533
[1mStep[0m  [32/169], [94mLoss[0m : 1.35286
[1mStep[0m  [48/169], [94mLoss[0m : 1.31854
[1mStep[0m  [64/169], [94mLoss[0m : 1.50999
[1mStep[0m  [80/169], [94mLoss[0m : 1.12566
[1mStep[0m  [96/169], [94mLoss[0m : 1.47410
[1mStep[0m  [112/169], [94mLoss[0m : 1.28228
[1mStep[0m  [128/169], [94mLoss[0m : 1.54685
[1mStep[0m  [144/169], [94mLoss[0m : 1.56779
[1mStep[0m  [160/169], [94mLoss[0m : 1.71963

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.430, [92mTest[0m: 2.494, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56757
[1mStep[0m  [16/169], [94mLoss[0m : 1.18254
[1mStep[0m  [32/169], [94mLoss[0m : 1.26082
[1mStep[0m  [48/169], [94mLoss[0m : 1.55094
[1mStep[0m  [64/169], [94mLoss[0m : 1.69500
[1mStep[0m  [80/169], [94mLoss[0m : 1.52504
[1mStep[0m  [96/169], [94mLoss[0m : 1.31278
[1mStep[0m  [112/169], [94mLoss[0m : 1.43763
[1mStep[0m  [128/169], [94mLoss[0m : 1.34356
[1mStep[0m  [144/169], [94mLoss[0m : 1.59414
[1mStep[0m  [160/169], [94mLoss[0m : 1.38899

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.439, [92mTest[0m: 2.526, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 28 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.514
====================================

Phase 2 - Evaluation MAE:  2.513940006494522
MAE score P1       2.342978
MAE score P2        2.51394
loss               1.430241
learning_rate          0.01
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 12, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.24111
[1mStep[0m  [16/169], [94mLoss[0m : 2.39187
[1mStep[0m  [32/169], [94mLoss[0m : 2.43996
[1mStep[0m  [48/169], [94mLoss[0m : 2.34610
[1mStep[0m  [64/169], [94mLoss[0m : 2.38040
[1mStep[0m  [80/169], [94mLoss[0m : 2.37725
[1mStep[0m  [96/169], [94mLoss[0m : 2.51919
[1mStep[0m  [112/169], [94mLoss[0m : 2.58068
[1mStep[0m  [128/169], [94mLoss[0m : 2.24686
[1mStep[0m  [144/169], [94mLoss[0m : 2.41180
[1mStep[0m  [160/169], [94mLoss[0m : 2.32254

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.857, [92mTest[0m: 11.183, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32911
[1mStep[0m  [16/169], [94mLoss[0m : 2.64659
[1mStep[0m  [32/169], [94mLoss[0m : 2.70539
[1mStep[0m  [48/169], [94mLoss[0m : 2.27969
[1mStep[0m  [64/169], [94mLoss[0m : 3.09561
[1mStep[0m  [80/169], [94mLoss[0m : 2.38298
[1mStep[0m  [96/169], [94mLoss[0m : 2.31114
[1mStep[0m  [112/169], [94mLoss[0m : 2.61473
[1mStep[0m  [128/169], [94mLoss[0m : 2.68253
[1mStep[0m  [144/169], [94mLoss[0m : 2.35985
[1mStep[0m  [160/169], [94mLoss[0m : 2.73866

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.375, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.03478
[1mStep[0m  [16/169], [94mLoss[0m : 2.62935
[1mStep[0m  [32/169], [94mLoss[0m : 2.10059
[1mStep[0m  [48/169], [94mLoss[0m : 2.41685
[1mStep[0m  [64/169], [94mLoss[0m : 2.63014
[1mStep[0m  [80/169], [94mLoss[0m : 2.54731
[1mStep[0m  [96/169], [94mLoss[0m : 2.34650
[1mStep[0m  [112/169], [94mLoss[0m : 2.50814
[1mStep[0m  [128/169], [94mLoss[0m : 2.44474
[1mStep[0m  [144/169], [94mLoss[0m : 2.19328
[1mStep[0m  [160/169], [94mLoss[0m : 2.38810

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70781
[1mStep[0m  [16/169], [94mLoss[0m : 2.24962
[1mStep[0m  [32/169], [94mLoss[0m : 2.51685
[1mStep[0m  [48/169], [94mLoss[0m : 2.62747
[1mStep[0m  [64/169], [94mLoss[0m : 2.57487
[1mStep[0m  [80/169], [94mLoss[0m : 2.41503
[1mStep[0m  [96/169], [94mLoss[0m : 2.24201
[1mStep[0m  [112/169], [94mLoss[0m : 2.43324
[1mStep[0m  [128/169], [94mLoss[0m : 2.75703
[1mStep[0m  [144/169], [94mLoss[0m : 2.11046
[1mStep[0m  [160/169], [94mLoss[0m : 2.17441

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49348
[1mStep[0m  [16/169], [94mLoss[0m : 2.51437
[1mStep[0m  [32/169], [94mLoss[0m : 2.15306
[1mStep[0m  [48/169], [94mLoss[0m : 2.51950
[1mStep[0m  [64/169], [94mLoss[0m : 2.00011
[1mStep[0m  [80/169], [94mLoss[0m : 2.30978
[1mStep[0m  [96/169], [94mLoss[0m : 2.35160
[1mStep[0m  [112/169], [94mLoss[0m : 2.43681
[1mStep[0m  [128/169], [94mLoss[0m : 2.45301
[1mStep[0m  [144/169], [94mLoss[0m : 2.49393
[1mStep[0m  [160/169], [94mLoss[0m : 2.37011

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79566
[1mStep[0m  [16/169], [94mLoss[0m : 2.68616
[1mStep[0m  [32/169], [94mLoss[0m : 2.76278
[1mStep[0m  [48/169], [94mLoss[0m : 2.33922
[1mStep[0m  [64/169], [94mLoss[0m : 2.16713
[1mStep[0m  [80/169], [94mLoss[0m : 2.76428
[1mStep[0m  [96/169], [94mLoss[0m : 2.49446
[1mStep[0m  [112/169], [94mLoss[0m : 2.48007
[1mStep[0m  [128/169], [94mLoss[0m : 2.05273
[1mStep[0m  [144/169], [94mLoss[0m : 2.40061
[1mStep[0m  [160/169], [94mLoss[0m : 2.77862

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.588, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69128
[1mStep[0m  [16/169], [94mLoss[0m : 3.05976
[1mStep[0m  [32/169], [94mLoss[0m : 2.14705
[1mStep[0m  [48/169], [94mLoss[0m : 2.18571
[1mStep[0m  [64/169], [94mLoss[0m : 2.48761
[1mStep[0m  [80/169], [94mLoss[0m : 2.11408
[1mStep[0m  [96/169], [94mLoss[0m : 1.85739
[1mStep[0m  [112/169], [94mLoss[0m : 2.45985
[1mStep[0m  [128/169], [94mLoss[0m : 2.78911
[1mStep[0m  [144/169], [94mLoss[0m : 2.25554
[1mStep[0m  [160/169], [94mLoss[0m : 2.43216

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.471, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56043
[1mStep[0m  [16/169], [94mLoss[0m : 2.78098
[1mStep[0m  [32/169], [94mLoss[0m : 2.41780
[1mStep[0m  [48/169], [94mLoss[0m : 2.74005
[1mStep[0m  [64/169], [94mLoss[0m : 2.11723
[1mStep[0m  [80/169], [94mLoss[0m : 2.14180
[1mStep[0m  [96/169], [94mLoss[0m : 2.64575
[1mStep[0m  [112/169], [94mLoss[0m : 2.53144
[1mStep[0m  [128/169], [94mLoss[0m : 2.09391
[1mStep[0m  [144/169], [94mLoss[0m : 2.23523
[1mStep[0m  [160/169], [94mLoss[0m : 2.24756

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50080
[1mStep[0m  [16/169], [94mLoss[0m : 1.94925
[1mStep[0m  [32/169], [94mLoss[0m : 2.21156
[1mStep[0m  [48/169], [94mLoss[0m : 2.64056
[1mStep[0m  [64/169], [94mLoss[0m : 2.54107
[1mStep[0m  [80/169], [94mLoss[0m : 2.65682
[1mStep[0m  [96/169], [94mLoss[0m : 2.67968
[1mStep[0m  [112/169], [94mLoss[0m : 2.53984
[1mStep[0m  [128/169], [94mLoss[0m : 2.35290
[1mStep[0m  [144/169], [94mLoss[0m : 2.41038
[1mStep[0m  [160/169], [94mLoss[0m : 2.55041

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.430, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82339
[1mStep[0m  [16/169], [94mLoss[0m : 2.18284
[1mStep[0m  [32/169], [94mLoss[0m : 2.65430
[1mStep[0m  [48/169], [94mLoss[0m : 2.13578
[1mStep[0m  [64/169], [94mLoss[0m : 2.75759
[1mStep[0m  [80/169], [94mLoss[0m : 2.57863
[1mStep[0m  [96/169], [94mLoss[0m : 2.70771
[1mStep[0m  [112/169], [94mLoss[0m : 2.37423
[1mStep[0m  [128/169], [94mLoss[0m : 2.51629
[1mStep[0m  [144/169], [94mLoss[0m : 2.36386
[1mStep[0m  [160/169], [94mLoss[0m : 2.06792

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29129
[1mStep[0m  [16/169], [94mLoss[0m : 2.54180
[1mStep[0m  [32/169], [94mLoss[0m : 2.25531
[1mStep[0m  [48/169], [94mLoss[0m : 2.25795
[1mStep[0m  [64/169], [94mLoss[0m : 2.67140
[1mStep[0m  [80/169], [94mLoss[0m : 2.38651
[1mStep[0m  [96/169], [94mLoss[0m : 2.37932
[1mStep[0m  [112/169], [94mLoss[0m : 2.78434
[1mStep[0m  [128/169], [94mLoss[0m : 2.47932
[1mStep[0m  [144/169], [94mLoss[0m : 2.58832
[1mStep[0m  [160/169], [94mLoss[0m : 2.90167

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.340, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52701
[1mStep[0m  [16/169], [94mLoss[0m : 1.99195
[1mStep[0m  [32/169], [94mLoss[0m : 2.44831
[1mStep[0m  [48/169], [94mLoss[0m : 2.65530
[1mStep[0m  [64/169], [94mLoss[0m : 2.59065
[1mStep[0m  [80/169], [94mLoss[0m : 2.23621
[1mStep[0m  [96/169], [94mLoss[0m : 2.54001
[1mStep[0m  [112/169], [94mLoss[0m : 2.38658
[1mStep[0m  [128/169], [94mLoss[0m : 2.66439
[1mStep[0m  [144/169], [94mLoss[0m : 2.69807
[1mStep[0m  [160/169], [94mLoss[0m : 2.52957

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00446
[1mStep[0m  [16/169], [94mLoss[0m : 2.52581
[1mStep[0m  [32/169], [94mLoss[0m : 2.04173
[1mStep[0m  [48/169], [94mLoss[0m : 2.40443
[1mStep[0m  [64/169], [94mLoss[0m : 2.57760
[1mStep[0m  [80/169], [94mLoss[0m : 2.67205
[1mStep[0m  [96/169], [94mLoss[0m : 2.56106
[1mStep[0m  [112/169], [94mLoss[0m : 2.39717
[1mStep[0m  [128/169], [94mLoss[0m : 2.59782
[1mStep[0m  [144/169], [94mLoss[0m : 2.06313
[1mStep[0m  [160/169], [94mLoss[0m : 2.79412

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98232
[1mStep[0m  [16/169], [94mLoss[0m : 2.69177
[1mStep[0m  [32/169], [94mLoss[0m : 2.37852
[1mStep[0m  [48/169], [94mLoss[0m : 2.47316
[1mStep[0m  [64/169], [94mLoss[0m : 2.19966
[1mStep[0m  [80/169], [94mLoss[0m : 2.29121
[1mStep[0m  [96/169], [94mLoss[0m : 3.26953
[1mStep[0m  [112/169], [94mLoss[0m : 2.39264
[1mStep[0m  [128/169], [94mLoss[0m : 2.54446
[1mStep[0m  [144/169], [94mLoss[0m : 2.69601
[1mStep[0m  [160/169], [94mLoss[0m : 2.74080

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.561, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12003
[1mStep[0m  [16/169], [94mLoss[0m : 2.61324
[1mStep[0m  [32/169], [94mLoss[0m : 2.27505
[1mStep[0m  [48/169], [94mLoss[0m : 2.52781
[1mStep[0m  [64/169], [94mLoss[0m : 2.40107
[1mStep[0m  [80/169], [94mLoss[0m : 2.89257
[1mStep[0m  [96/169], [94mLoss[0m : 2.34494
[1mStep[0m  [112/169], [94mLoss[0m : 2.64925
[1mStep[0m  [128/169], [94mLoss[0m : 2.32219
[1mStep[0m  [144/169], [94mLoss[0m : 2.36614
[1mStep[0m  [160/169], [94mLoss[0m : 2.31279

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.322, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33636
[1mStep[0m  [16/169], [94mLoss[0m : 2.63083
[1mStep[0m  [32/169], [94mLoss[0m : 2.45706
[1mStep[0m  [48/169], [94mLoss[0m : 2.17013
[1mStep[0m  [64/169], [94mLoss[0m : 2.31994
[1mStep[0m  [80/169], [94mLoss[0m : 2.51180
[1mStep[0m  [96/169], [94mLoss[0m : 2.27037
[1mStep[0m  [112/169], [94mLoss[0m : 2.52169
[1mStep[0m  [128/169], [94mLoss[0m : 2.10880
[1mStep[0m  [144/169], [94mLoss[0m : 2.86981
[1mStep[0m  [160/169], [94mLoss[0m : 2.55386

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.341, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51573
[1mStep[0m  [16/169], [94mLoss[0m : 2.57637
[1mStep[0m  [32/169], [94mLoss[0m : 2.60302
[1mStep[0m  [48/169], [94mLoss[0m : 2.77598
[1mStep[0m  [64/169], [94mLoss[0m : 2.55982
[1mStep[0m  [80/169], [94mLoss[0m : 2.25818
[1mStep[0m  [96/169], [94mLoss[0m : 1.99512
[1mStep[0m  [112/169], [94mLoss[0m : 2.16735
[1mStep[0m  [128/169], [94mLoss[0m : 2.22875
[1mStep[0m  [144/169], [94mLoss[0m : 2.40521
[1mStep[0m  [160/169], [94mLoss[0m : 2.53012

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31271
[1mStep[0m  [16/169], [94mLoss[0m : 2.80340
[1mStep[0m  [32/169], [94mLoss[0m : 2.88576
[1mStep[0m  [48/169], [94mLoss[0m : 2.55819
[1mStep[0m  [64/169], [94mLoss[0m : 2.29890
[1mStep[0m  [80/169], [94mLoss[0m : 2.37188
[1mStep[0m  [96/169], [94mLoss[0m : 2.49149
[1mStep[0m  [112/169], [94mLoss[0m : 2.29394
[1mStep[0m  [128/169], [94mLoss[0m : 2.37950
[1mStep[0m  [144/169], [94mLoss[0m : 2.32563
[1mStep[0m  [160/169], [94mLoss[0m : 2.52695

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20396
[1mStep[0m  [16/169], [94mLoss[0m : 1.95204
[1mStep[0m  [32/169], [94mLoss[0m : 2.66789
[1mStep[0m  [48/169], [94mLoss[0m : 2.65488
[1mStep[0m  [64/169], [94mLoss[0m : 2.99078
[1mStep[0m  [80/169], [94mLoss[0m : 2.83742
[1mStep[0m  [96/169], [94mLoss[0m : 2.34847
[1mStep[0m  [112/169], [94mLoss[0m : 2.17693
[1mStep[0m  [128/169], [94mLoss[0m : 2.16754
[1mStep[0m  [144/169], [94mLoss[0m : 2.39632
[1mStep[0m  [160/169], [94mLoss[0m : 2.62721

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49444
[1mStep[0m  [16/169], [94mLoss[0m : 2.99342
[1mStep[0m  [32/169], [94mLoss[0m : 2.61272
[1mStep[0m  [48/169], [94mLoss[0m : 2.31434
[1mStep[0m  [64/169], [94mLoss[0m : 2.29177
[1mStep[0m  [80/169], [94mLoss[0m : 2.88744
[1mStep[0m  [96/169], [94mLoss[0m : 2.34700
[1mStep[0m  [112/169], [94mLoss[0m : 2.15899
[1mStep[0m  [128/169], [94mLoss[0m : 1.98212
[1mStep[0m  [144/169], [94mLoss[0m : 2.49915
[1mStep[0m  [160/169], [94mLoss[0m : 2.31382

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.433, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41800
[1mStep[0m  [16/169], [94mLoss[0m : 2.19784
[1mStep[0m  [32/169], [94mLoss[0m : 2.71483
[1mStep[0m  [48/169], [94mLoss[0m : 2.32239
[1mStep[0m  [64/169], [94mLoss[0m : 2.30354
[1mStep[0m  [80/169], [94mLoss[0m : 2.47152
[1mStep[0m  [96/169], [94mLoss[0m : 2.05973
[1mStep[0m  [112/169], [94mLoss[0m : 2.81547
[1mStep[0m  [128/169], [94mLoss[0m : 2.55256
[1mStep[0m  [144/169], [94mLoss[0m : 2.13981
[1mStep[0m  [160/169], [94mLoss[0m : 2.28137

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.319, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64632
[1mStep[0m  [16/169], [94mLoss[0m : 2.45271
[1mStep[0m  [32/169], [94mLoss[0m : 2.05835
[1mStep[0m  [48/169], [94mLoss[0m : 2.13600
[1mStep[0m  [64/169], [94mLoss[0m : 2.45750
[1mStep[0m  [80/169], [94mLoss[0m : 2.29909
[1mStep[0m  [96/169], [94mLoss[0m : 2.77585
[1mStep[0m  [112/169], [94mLoss[0m : 2.30493
[1mStep[0m  [128/169], [94mLoss[0m : 2.39237
[1mStep[0m  [144/169], [94mLoss[0m : 3.14035
[1mStep[0m  [160/169], [94mLoss[0m : 2.63491

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.380, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10149
[1mStep[0m  [16/169], [94mLoss[0m : 1.98437
[1mStep[0m  [32/169], [94mLoss[0m : 2.23120
[1mStep[0m  [48/169], [94mLoss[0m : 2.53368
[1mStep[0m  [64/169], [94mLoss[0m : 2.23169
[1mStep[0m  [80/169], [94mLoss[0m : 2.30479
[1mStep[0m  [96/169], [94mLoss[0m : 2.48400
[1mStep[0m  [112/169], [94mLoss[0m : 2.35163
[1mStep[0m  [128/169], [94mLoss[0m : 2.45677
[1mStep[0m  [144/169], [94mLoss[0m : 2.50967
[1mStep[0m  [160/169], [94mLoss[0m : 2.36170

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.316, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25662
[1mStep[0m  [16/169], [94mLoss[0m : 2.29170
[1mStep[0m  [32/169], [94mLoss[0m : 2.28355
[1mStep[0m  [48/169], [94mLoss[0m : 2.79500
[1mStep[0m  [64/169], [94mLoss[0m : 2.21162
[1mStep[0m  [80/169], [94mLoss[0m : 2.19744
[1mStep[0m  [96/169], [94mLoss[0m : 2.21756
[1mStep[0m  [112/169], [94mLoss[0m : 2.47253
[1mStep[0m  [128/169], [94mLoss[0m : 2.83859
[1mStep[0m  [144/169], [94mLoss[0m : 2.29053
[1mStep[0m  [160/169], [94mLoss[0m : 2.49289

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.350, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15628
[1mStep[0m  [16/169], [94mLoss[0m : 2.41769
[1mStep[0m  [32/169], [94mLoss[0m : 2.09774
[1mStep[0m  [48/169], [94mLoss[0m : 2.45401
[1mStep[0m  [64/169], [94mLoss[0m : 2.34370
[1mStep[0m  [80/169], [94mLoss[0m : 3.16964
[1mStep[0m  [96/169], [94mLoss[0m : 1.99144
[1mStep[0m  [112/169], [94mLoss[0m : 2.33612
[1mStep[0m  [128/169], [94mLoss[0m : 2.70078
[1mStep[0m  [144/169], [94mLoss[0m : 2.56144
[1mStep[0m  [160/169], [94mLoss[0m : 2.69071

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.389, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63774
[1mStep[0m  [16/169], [94mLoss[0m : 2.24899
[1mStep[0m  [32/169], [94mLoss[0m : 2.18818
[1mStep[0m  [48/169], [94mLoss[0m : 2.39816
[1mStep[0m  [64/169], [94mLoss[0m : 2.25048
[1mStep[0m  [80/169], [94mLoss[0m : 2.42906
[1mStep[0m  [96/169], [94mLoss[0m : 2.87066
[1mStep[0m  [112/169], [94mLoss[0m : 2.22388
[1mStep[0m  [128/169], [94mLoss[0m : 2.52583
[1mStep[0m  [144/169], [94mLoss[0m : 2.34371
[1mStep[0m  [160/169], [94mLoss[0m : 2.25720

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.312, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43872
[1mStep[0m  [16/169], [94mLoss[0m : 2.07188
[1mStep[0m  [32/169], [94mLoss[0m : 2.28678
[1mStep[0m  [48/169], [94mLoss[0m : 2.28674
[1mStep[0m  [64/169], [94mLoss[0m : 2.18614
[1mStep[0m  [80/169], [94mLoss[0m : 2.68183
[1mStep[0m  [96/169], [94mLoss[0m : 2.44779
[1mStep[0m  [112/169], [94mLoss[0m : 2.83759
[1mStep[0m  [128/169], [94mLoss[0m : 2.45403
[1mStep[0m  [144/169], [94mLoss[0m : 2.44565
[1mStep[0m  [160/169], [94mLoss[0m : 2.57916

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.315, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24966
[1mStep[0m  [16/169], [94mLoss[0m : 2.36391
[1mStep[0m  [32/169], [94mLoss[0m : 2.18965
[1mStep[0m  [48/169], [94mLoss[0m : 2.93702
[1mStep[0m  [64/169], [94mLoss[0m : 2.94138
[1mStep[0m  [80/169], [94mLoss[0m : 2.67587
[1mStep[0m  [96/169], [94mLoss[0m : 2.23167
[1mStep[0m  [112/169], [94mLoss[0m : 2.83151
[1mStep[0m  [128/169], [94mLoss[0m : 2.49713
[1mStep[0m  [144/169], [94mLoss[0m : 2.78289
[1mStep[0m  [160/169], [94mLoss[0m : 2.38086

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.329, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06098
[1mStep[0m  [16/169], [94mLoss[0m : 2.78833
[1mStep[0m  [32/169], [94mLoss[0m : 2.10465
[1mStep[0m  [48/169], [94mLoss[0m : 2.12761
[1mStep[0m  [64/169], [94mLoss[0m : 2.35449
[1mStep[0m  [80/169], [94mLoss[0m : 2.59597
[1mStep[0m  [96/169], [94mLoss[0m : 2.40140
[1mStep[0m  [112/169], [94mLoss[0m : 2.23921
[1mStep[0m  [128/169], [94mLoss[0m : 2.25444
[1mStep[0m  [144/169], [94mLoss[0m : 2.34489
[1mStep[0m  [160/169], [94mLoss[0m : 2.65350

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.371, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20028
[1mStep[0m  [16/169], [94mLoss[0m : 2.64004
[1mStep[0m  [32/169], [94mLoss[0m : 2.84816
[1mStep[0m  [48/169], [94mLoss[0m : 2.50263
[1mStep[0m  [64/169], [94mLoss[0m : 2.06009
[1mStep[0m  [80/169], [94mLoss[0m : 2.31903
[1mStep[0m  [96/169], [94mLoss[0m : 2.47462
[1mStep[0m  [112/169], [94mLoss[0m : 2.06862
[1mStep[0m  [128/169], [94mLoss[0m : 2.01464
[1mStep[0m  [144/169], [94mLoss[0m : 2.73737
[1mStep[0m  [160/169], [94mLoss[0m : 1.86349

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.416, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.319
====================================

Phase 1 - Evaluation MAE:  2.3189552617924556
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.57393
[1mStep[0m  [16/169], [94mLoss[0m : 2.74787
[1mStep[0m  [32/169], [94mLoss[0m : 2.29782
[1mStep[0m  [48/169], [94mLoss[0m : 2.23614
[1mStep[0m  [64/169], [94mLoss[0m : 2.44348
[1mStep[0m  [80/169], [94mLoss[0m : 2.83685
[1mStep[0m  [96/169], [94mLoss[0m : 1.77838
[1mStep[0m  [112/169], [94mLoss[0m : 2.90674
[1mStep[0m  [128/169], [94mLoss[0m : 2.92899
[1mStep[0m  [144/169], [94mLoss[0m : 2.19556
[1mStep[0m  [160/169], [94mLoss[0m : 2.08917

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29077
[1mStep[0m  [16/169], [94mLoss[0m : 2.24680
[1mStep[0m  [32/169], [94mLoss[0m : 2.76428
[1mStep[0m  [48/169], [94mLoss[0m : 2.20798
[1mStep[0m  [64/169], [94mLoss[0m : 2.04021
[1mStep[0m  [80/169], [94mLoss[0m : 2.10053
[1mStep[0m  [96/169], [94mLoss[0m : 2.42167
[1mStep[0m  [112/169], [94mLoss[0m : 2.73384
[1mStep[0m  [128/169], [94mLoss[0m : 2.53332
[1mStep[0m  [144/169], [94mLoss[0m : 2.57519
[1mStep[0m  [160/169], [94mLoss[0m : 2.57475

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.398, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00326
[1mStep[0m  [16/169], [94mLoss[0m : 1.90195
[1mStep[0m  [32/169], [94mLoss[0m : 2.28191
[1mStep[0m  [48/169], [94mLoss[0m : 1.91858
[1mStep[0m  [64/169], [94mLoss[0m : 2.32828
[1mStep[0m  [80/169], [94mLoss[0m : 2.28300
[1mStep[0m  [96/169], [94mLoss[0m : 1.98555
[1mStep[0m  [112/169], [94mLoss[0m : 2.26352
[1mStep[0m  [128/169], [94mLoss[0m : 2.49557
[1mStep[0m  [144/169], [94mLoss[0m : 2.22468
[1mStep[0m  [160/169], [94mLoss[0m : 1.98118

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.347, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81829
[1mStep[0m  [16/169], [94mLoss[0m : 2.07131
[1mStep[0m  [32/169], [94mLoss[0m : 2.14083
[1mStep[0m  [48/169], [94mLoss[0m : 2.42567
[1mStep[0m  [64/169], [94mLoss[0m : 2.20705
[1mStep[0m  [80/169], [94mLoss[0m : 2.12332
[1mStep[0m  [96/169], [94mLoss[0m : 2.04364
[1mStep[0m  [112/169], [94mLoss[0m : 2.23002
[1mStep[0m  [128/169], [94mLoss[0m : 2.33500
[1mStep[0m  [144/169], [94mLoss[0m : 2.13409
[1mStep[0m  [160/169], [94mLoss[0m : 1.99306

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.196, [92mTest[0m: 2.367, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97105
[1mStep[0m  [16/169], [94mLoss[0m : 1.49488
[1mStep[0m  [32/169], [94mLoss[0m : 2.17323
[1mStep[0m  [48/169], [94mLoss[0m : 1.94136
[1mStep[0m  [64/169], [94mLoss[0m : 1.98872
[1mStep[0m  [80/169], [94mLoss[0m : 1.97627
[1mStep[0m  [96/169], [94mLoss[0m : 1.94115
[1mStep[0m  [112/169], [94mLoss[0m : 1.79945
[1mStep[0m  [128/169], [94mLoss[0m : 2.13214
[1mStep[0m  [144/169], [94mLoss[0m : 2.51675
[1mStep[0m  [160/169], [94mLoss[0m : 2.32911

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.136, [92mTest[0m: 2.372, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04566
[1mStep[0m  [16/169], [94mLoss[0m : 2.01510
[1mStep[0m  [32/169], [94mLoss[0m : 2.56785
[1mStep[0m  [48/169], [94mLoss[0m : 1.90025
[1mStep[0m  [64/169], [94mLoss[0m : 2.59240
[1mStep[0m  [80/169], [94mLoss[0m : 1.96144
[1mStep[0m  [96/169], [94mLoss[0m : 2.42412
[1mStep[0m  [112/169], [94mLoss[0m : 1.85843
[1mStep[0m  [128/169], [94mLoss[0m : 1.86885
[1mStep[0m  [144/169], [94mLoss[0m : 2.27439
[1mStep[0m  [160/169], [94mLoss[0m : 1.77882

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.499, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11378
[1mStep[0m  [16/169], [94mLoss[0m : 1.93789
[1mStep[0m  [32/169], [94mLoss[0m : 2.01206
[1mStep[0m  [48/169], [94mLoss[0m : 1.62522
[1mStep[0m  [64/169], [94mLoss[0m : 1.97632
[1mStep[0m  [80/169], [94mLoss[0m : 1.90154
[1mStep[0m  [96/169], [94mLoss[0m : 1.99107
[1mStep[0m  [112/169], [94mLoss[0m : 2.12307
[1mStep[0m  [128/169], [94mLoss[0m : 1.79159
[1mStep[0m  [144/169], [94mLoss[0m : 2.18191
[1mStep[0m  [160/169], [94mLoss[0m : 2.03369

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.415, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08257
[1mStep[0m  [16/169], [94mLoss[0m : 1.81545
[1mStep[0m  [32/169], [94mLoss[0m : 2.27844
[1mStep[0m  [48/169], [94mLoss[0m : 2.14692
[1mStep[0m  [64/169], [94mLoss[0m : 2.09164
[1mStep[0m  [80/169], [94mLoss[0m : 2.06060
[1mStep[0m  [96/169], [94mLoss[0m : 1.86582
[1mStep[0m  [112/169], [94mLoss[0m : 1.72235
[1mStep[0m  [128/169], [94mLoss[0m : 2.27833
[1mStep[0m  [144/169], [94mLoss[0m : 1.74199
[1mStep[0m  [160/169], [94mLoss[0m : 2.04553

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77824
[1mStep[0m  [16/169], [94mLoss[0m : 1.68506
[1mStep[0m  [32/169], [94mLoss[0m : 1.92147
[1mStep[0m  [48/169], [94mLoss[0m : 1.98927
[1mStep[0m  [64/169], [94mLoss[0m : 1.91688
[1mStep[0m  [80/169], [94mLoss[0m : 1.66807
[1mStep[0m  [96/169], [94mLoss[0m : 1.72451
[1mStep[0m  [112/169], [94mLoss[0m : 1.69596
[1mStep[0m  [128/169], [94mLoss[0m : 1.65231
[1mStep[0m  [144/169], [94mLoss[0m : 2.12601
[1mStep[0m  [160/169], [94mLoss[0m : 2.07486

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.448, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12501
[1mStep[0m  [16/169], [94mLoss[0m : 2.08840
[1mStep[0m  [32/169], [94mLoss[0m : 1.56025
[1mStep[0m  [48/169], [94mLoss[0m : 2.15044
[1mStep[0m  [64/169], [94mLoss[0m : 1.76413
[1mStep[0m  [80/169], [94mLoss[0m : 2.27726
[1mStep[0m  [96/169], [94mLoss[0m : 1.82421
[1mStep[0m  [112/169], [94mLoss[0m : 1.49369
[1mStep[0m  [128/169], [94mLoss[0m : 1.74540
[1mStep[0m  [144/169], [94mLoss[0m : 2.03882
[1mStep[0m  [160/169], [94mLoss[0m : 1.93846

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.853, [92mTest[0m: 2.444, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74613
[1mStep[0m  [16/169], [94mLoss[0m : 1.86867
[1mStep[0m  [32/169], [94mLoss[0m : 1.91787
[1mStep[0m  [48/169], [94mLoss[0m : 1.70246
[1mStep[0m  [64/169], [94mLoss[0m : 1.79780
[1mStep[0m  [80/169], [94mLoss[0m : 1.59013
[1mStep[0m  [96/169], [94mLoss[0m : 1.67236
[1mStep[0m  [112/169], [94mLoss[0m : 2.02744
[1mStep[0m  [128/169], [94mLoss[0m : 1.52565
[1mStep[0m  [144/169], [94mLoss[0m : 1.92074
[1mStep[0m  [160/169], [94mLoss[0m : 2.19966

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.483, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57940
[1mStep[0m  [16/169], [94mLoss[0m : 1.99439
[1mStep[0m  [32/169], [94mLoss[0m : 1.59356
[1mStep[0m  [48/169], [94mLoss[0m : 2.28627
[1mStep[0m  [64/169], [94mLoss[0m : 1.43867
[1mStep[0m  [80/169], [94mLoss[0m : 1.70787
[1mStep[0m  [96/169], [94mLoss[0m : 1.67090
[1mStep[0m  [112/169], [94mLoss[0m : 1.89607
[1mStep[0m  [128/169], [94mLoss[0m : 1.97902
[1mStep[0m  [144/169], [94mLoss[0m : 1.49583
[1mStep[0m  [160/169], [94mLoss[0m : 2.08147

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.469, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49439
[1mStep[0m  [16/169], [94mLoss[0m : 1.52265
[1mStep[0m  [32/169], [94mLoss[0m : 1.54832
[1mStep[0m  [48/169], [94mLoss[0m : 2.01381
[1mStep[0m  [64/169], [94mLoss[0m : 2.05628
[1mStep[0m  [80/169], [94mLoss[0m : 1.52274
[1mStep[0m  [96/169], [94mLoss[0m : 1.96477
[1mStep[0m  [112/169], [94mLoss[0m : 1.36815
[1mStep[0m  [128/169], [94mLoss[0m : 1.38405
[1mStep[0m  [144/169], [94mLoss[0m : 2.06273
[1mStep[0m  [160/169], [94mLoss[0m : 1.75246

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.512, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62573
[1mStep[0m  [16/169], [94mLoss[0m : 1.51715
[1mStep[0m  [32/169], [94mLoss[0m : 1.57116
[1mStep[0m  [48/169], [94mLoss[0m : 1.36892
[1mStep[0m  [64/169], [94mLoss[0m : 1.78347
[1mStep[0m  [80/169], [94mLoss[0m : 1.82900
[1mStep[0m  [96/169], [94mLoss[0m : 1.72677
[1mStep[0m  [112/169], [94mLoss[0m : 1.48475
[1mStep[0m  [128/169], [94mLoss[0m : 1.65524
[1mStep[0m  [144/169], [94mLoss[0m : 2.09285
[1mStep[0m  [160/169], [94mLoss[0m : 1.52984

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.467, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74107
[1mStep[0m  [16/169], [94mLoss[0m : 1.74051
[1mStep[0m  [32/169], [94mLoss[0m : 1.80379
[1mStep[0m  [48/169], [94mLoss[0m : 1.53767
[1mStep[0m  [64/169], [94mLoss[0m : 1.52360
[1mStep[0m  [80/169], [94mLoss[0m : 1.56106
[1mStep[0m  [96/169], [94mLoss[0m : 1.33353
[1mStep[0m  [112/169], [94mLoss[0m : 1.38145
[1mStep[0m  [128/169], [94mLoss[0m : 1.77928
[1mStep[0m  [144/169], [94mLoss[0m : 2.01687
[1mStep[0m  [160/169], [94mLoss[0m : 1.82429

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.563, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41940
[1mStep[0m  [16/169], [94mLoss[0m : 1.74173
[1mStep[0m  [32/169], [94mLoss[0m : 1.60674
[1mStep[0m  [48/169], [94mLoss[0m : 1.35520
[1mStep[0m  [64/169], [94mLoss[0m : 1.57153
[1mStep[0m  [80/169], [94mLoss[0m : 1.42904
[1mStep[0m  [96/169], [94mLoss[0m : 1.67760
[1mStep[0m  [112/169], [94mLoss[0m : 1.74845
[1mStep[0m  [128/169], [94mLoss[0m : 1.64859
[1mStep[0m  [144/169], [94mLoss[0m : 1.85790
[1mStep[0m  [160/169], [94mLoss[0m : 1.74612

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.459, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52147
[1mStep[0m  [16/169], [94mLoss[0m : 1.58793
[1mStep[0m  [32/169], [94mLoss[0m : 2.08051
[1mStep[0m  [48/169], [94mLoss[0m : 1.55032
[1mStep[0m  [64/169], [94mLoss[0m : 1.62337
[1mStep[0m  [80/169], [94mLoss[0m : 1.57008
[1mStep[0m  [96/169], [94mLoss[0m : 1.38805
[1mStep[0m  [112/169], [94mLoss[0m : 1.72721
[1mStep[0m  [128/169], [94mLoss[0m : 1.44567
[1mStep[0m  [144/169], [94mLoss[0m : 1.55215
[1mStep[0m  [160/169], [94mLoss[0m : 1.53885

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.496, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61090
[1mStep[0m  [16/169], [94mLoss[0m : 1.50462
[1mStep[0m  [32/169], [94mLoss[0m : 1.59658
[1mStep[0m  [48/169], [94mLoss[0m : 1.59238
[1mStep[0m  [64/169], [94mLoss[0m : 1.43764
[1mStep[0m  [80/169], [94mLoss[0m : 1.60048
[1mStep[0m  [96/169], [94mLoss[0m : 1.62095
[1mStep[0m  [112/169], [94mLoss[0m : 1.30462
[1mStep[0m  [128/169], [94mLoss[0m : 1.58686
[1mStep[0m  [144/169], [94mLoss[0m : 1.88834
[1mStep[0m  [160/169], [94mLoss[0m : 1.60088

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.571, [92mTest[0m: 2.494, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28334
[1mStep[0m  [16/169], [94mLoss[0m : 2.28401
[1mStep[0m  [32/169], [94mLoss[0m : 1.92811
[1mStep[0m  [48/169], [94mLoss[0m : 1.41841
[1mStep[0m  [64/169], [94mLoss[0m : 2.07741
[1mStep[0m  [80/169], [94mLoss[0m : 1.13755
[1mStep[0m  [96/169], [94mLoss[0m : 1.19999
[1mStep[0m  [112/169], [94mLoss[0m : 1.13677
[1mStep[0m  [128/169], [94mLoss[0m : 1.68119
[1mStep[0m  [144/169], [94mLoss[0m : 1.55289
[1mStep[0m  [160/169], [94mLoss[0m : 1.35111

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.565, [92mTest[0m: 2.550, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.31217
[1mStep[0m  [16/169], [94mLoss[0m : 1.54822
[1mStep[0m  [32/169], [94mLoss[0m : 1.74761
[1mStep[0m  [48/169], [94mLoss[0m : 1.40676
[1mStep[0m  [64/169], [94mLoss[0m : 1.82400
[1mStep[0m  [80/169], [94mLoss[0m : 1.20852
[1mStep[0m  [96/169], [94mLoss[0m : 1.54387
[1mStep[0m  [112/169], [94mLoss[0m : 1.53475
[1mStep[0m  [128/169], [94mLoss[0m : 1.41590
[1mStep[0m  [144/169], [94mLoss[0m : 1.64771
[1mStep[0m  [160/169], [94mLoss[0m : 2.08817

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.564, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48148
[1mStep[0m  [16/169], [94mLoss[0m : 1.43615
[1mStep[0m  [32/169], [94mLoss[0m : 1.72023
[1mStep[0m  [48/169], [94mLoss[0m : 1.59702
[1mStep[0m  [64/169], [94mLoss[0m : 1.56049
[1mStep[0m  [80/169], [94mLoss[0m : 1.24863
[1mStep[0m  [96/169], [94mLoss[0m : 1.23668
[1mStep[0m  [112/169], [94mLoss[0m : 1.26688
[1mStep[0m  [128/169], [94mLoss[0m : 1.52004
[1mStep[0m  [144/169], [94mLoss[0m : 1.72588
[1mStep[0m  [160/169], [94mLoss[0m : 1.47295

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.455, [92mTest[0m: 2.486, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.14529
[1mStep[0m  [16/169], [94mLoss[0m : 1.67101
[1mStep[0m  [32/169], [94mLoss[0m : 1.87578
[1mStep[0m  [48/169], [94mLoss[0m : 1.60549
[1mStep[0m  [64/169], [94mLoss[0m : 1.49182
[1mStep[0m  [80/169], [94mLoss[0m : 1.43672
[1mStep[0m  [96/169], [94mLoss[0m : 1.43752
[1mStep[0m  [112/169], [94mLoss[0m : 1.48296
[1mStep[0m  [128/169], [94mLoss[0m : 1.57348
[1mStep[0m  [144/169], [94mLoss[0m : 1.58550
[1mStep[0m  [160/169], [94mLoss[0m : 1.73995

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.481, [92mTest[0m: 2.512, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58096
[1mStep[0m  [16/169], [94mLoss[0m : 1.38393
[1mStep[0m  [32/169], [94mLoss[0m : 1.06797
[1mStep[0m  [48/169], [94mLoss[0m : 1.43120
[1mStep[0m  [64/169], [94mLoss[0m : 1.34870
[1mStep[0m  [80/169], [94mLoss[0m : 1.57283
[1mStep[0m  [96/169], [94mLoss[0m : 1.25738
[1mStep[0m  [112/169], [94mLoss[0m : 1.41050
[1mStep[0m  [128/169], [94mLoss[0m : 1.77429
[1mStep[0m  [144/169], [94mLoss[0m : 1.90153
[1mStep[0m  [160/169], [94mLoss[0m : 1.33803

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.437, [92mTest[0m: 2.501, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08092
[1mStep[0m  [16/169], [94mLoss[0m : 1.14254
[1mStep[0m  [32/169], [94mLoss[0m : 1.14078
[1mStep[0m  [48/169], [94mLoss[0m : 1.35292
[1mStep[0m  [64/169], [94mLoss[0m : 1.48934
[1mStep[0m  [80/169], [94mLoss[0m : 1.42154
[1mStep[0m  [96/169], [94mLoss[0m : 1.42186
[1mStep[0m  [112/169], [94mLoss[0m : 1.26898
[1mStep[0m  [128/169], [94mLoss[0m : 1.77361
[1mStep[0m  [144/169], [94mLoss[0m : 1.34927
[1mStep[0m  [160/169], [94mLoss[0m : 1.40409

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.424, [92mTest[0m: 2.533, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.520
====================================

Phase 2 - Evaluation MAE:  2.5204599840300426
MAE score P1      2.318955
MAE score P2       2.52046
loss              1.424273
learning_rate         0.01
batch_size              64
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.46918
[1mStep[0m  [16/169], [94mLoss[0m : 9.30649
[1mStep[0m  [32/169], [94mLoss[0m : 7.19720
[1mStep[0m  [48/169], [94mLoss[0m : 4.95700
[1mStep[0m  [64/169], [94mLoss[0m : 3.76087
[1mStep[0m  [80/169], [94mLoss[0m : 3.25011
[1mStep[0m  [96/169], [94mLoss[0m : 2.92623
[1mStep[0m  [112/169], [94mLoss[0m : 2.94347
[1mStep[0m  [128/169], [94mLoss[0m : 2.78413
[1mStep[0m  [144/169], [94mLoss[0m : 3.12294
[1mStep[0m  [160/169], [94mLoss[0m : 2.80156

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.447, [92mTest[0m: 10.815, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66434
[1mStep[0m  [16/169], [94mLoss[0m : 2.70313
[1mStep[0m  [32/169], [94mLoss[0m : 2.77127
[1mStep[0m  [48/169], [94mLoss[0m : 2.46765
[1mStep[0m  [64/169], [94mLoss[0m : 2.68092
[1mStep[0m  [80/169], [94mLoss[0m : 2.93068
[1mStep[0m  [96/169], [94mLoss[0m : 2.34426
[1mStep[0m  [112/169], [94mLoss[0m : 2.62979
[1mStep[0m  [128/169], [94mLoss[0m : 2.98841
[1mStep[0m  [144/169], [94mLoss[0m : 2.49691
[1mStep[0m  [160/169], [94mLoss[0m : 2.34770

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.466, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68323
[1mStep[0m  [16/169], [94mLoss[0m : 2.96226
[1mStep[0m  [32/169], [94mLoss[0m : 2.29241
[1mStep[0m  [48/169], [94mLoss[0m : 2.54789
[1mStep[0m  [64/169], [94mLoss[0m : 2.82682
[1mStep[0m  [80/169], [94mLoss[0m : 2.66422
[1mStep[0m  [96/169], [94mLoss[0m : 2.28210
[1mStep[0m  [112/169], [94mLoss[0m : 2.00377
[1mStep[0m  [128/169], [94mLoss[0m : 2.59044
[1mStep[0m  [144/169], [94mLoss[0m : 2.57169
[1mStep[0m  [160/169], [94mLoss[0m : 2.59624

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44426
[1mStep[0m  [16/169], [94mLoss[0m : 2.79402
[1mStep[0m  [32/169], [94mLoss[0m : 2.45713
[1mStep[0m  [48/169], [94mLoss[0m : 2.34939
[1mStep[0m  [64/169], [94mLoss[0m : 2.68574
[1mStep[0m  [80/169], [94mLoss[0m : 2.95625
[1mStep[0m  [96/169], [94mLoss[0m : 2.35200
[1mStep[0m  [112/169], [94mLoss[0m : 2.49105
[1mStep[0m  [128/169], [94mLoss[0m : 2.12177
[1mStep[0m  [144/169], [94mLoss[0m : 2.32026
[1mStep[0m  [160/169], [94mLoss[0m : 2.55256

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91590
[1mStep[0m  [16/169], [94mLoss[0m : 2.55470
[1mStep[0m  [32/169], [94mLoss[0m : 2.55325
[1mStep[0m  [48/169], [94mLoss[0m : 2.16575
[1mStep[0m  [64/169], [94mLoss[0m : 2.92335
[1mStep[0m  [80/169], [94mLoss[0m : 2.69898
[1mStep[0m  [96/169], [94mLoss[0m : 2.64950
[1mStep[0m  [112/169], [94mLoss[0m : 2.88240
[1mStep[0m  [128/169], [94mLoss[0m : 2.57726
[1mStep[0m  [144/169], [94mLoss[0m : 2.22637
[1mStep[0m  [160/169], [94mLoss[0m : 2.53745

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.402, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50508
[1mStep[0m  [16/169], [94mLoss[0m : 2.23354
[1mStep[0m  [32/169], [94mLoss[0m : 2.52491
[1mStep[0m  [48/169], [94mLoss[0m : 2.40706
[1mStep[0m  [64/169], [94mLoss[0m : 2.30277
[1mStep[0m  [80/169], [94mLoss[0m : 2.08027
[1mStep[0m  [96/169], [94mLoss[0m : 2.58423
[1mStep[0m  [112/169], [94mLoss[0m : 2.57513
[1mStep[0m  [128/169], [94mLoss[0m : 2.47352
[1mStep[0m  [144/169], [94mLoss[0m : 2.31738
[1mStep[0m  [160/169], [94mLoss[0m : 2.76014

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.385, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74933
[1mStep[0m  [16/169], [94mLoss[0m : 2.61139
[1mStep[0m  [32/169], [94mLoss[0m : 2.49666
[1mStep[0m  [48/169], [94mLoss[0m : 2.68010
[1mStep[0m  [64/169], [94mLoss[0m : 2.61191
[1mStep[0m  [80/169], [94mLoss[0m : 2.75043
[1mStep[0m  [96/169], [94mLoss[0m : 2.70408
[1mStep[0m  [112/169], [94mLoss[0m : 1.96549
[1mStep[0m  [128/169], [94mLoss[0m : 2.49796
[1mStep[0m  [144/169], [94mLoss[0m : 2.51755
[1mStep[0m  [160/169], [94mLoss[0m : 2.89153

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25142
[1mStep[0m  [16/169], [94mLoss[0m : 2.36986
[1mStep[0m  [32/169], [94mLoss[0m : 2.52683
[1mStep[0m  [48/169], [94mLoss[0m : 2.42509
[1mStep[0m  [64/169], [94mLoss[0m : 2.40902
[1mStep[0m  [80/169], [94mLoss[0m : 2.28437
[1mStep[0m  [96/169], [94mLoss[0m : 2.78898
[1mStep[0m  [112/169], [94mLoss[0m : 2.77194
[1mStep[0m  [128/169], [94mLoss[0m : 2.15620
[1mStep[0m  [144/169], [94mLoss[0m : 2.61108
[1mStep[0m  [160/169], [94mLoss[0m : 2.42100

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.348, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62074
[1mStep[0m  [16/169], [94mLoss[0m : 2.70948
[1mStep[0m  [32/169], [94mLoss[0m : 2.40144
[1mStep[0m  [48/169], [94mLoss[0m : 2.54102
[1mStep[0m  [64/169], [94mLoss[0m : 2.57424
[1mStep[0m  [80/169], [94mLoss[0m : 2.83889
[1mStep[0m  [96/169], [94mLoss[0m : 1.99665
[1mStep[0m  [112/169], [94mLoss[0m : 2.40512
[1mStep[0m  [128/169], [94mLoss[0m : 2.03496
[1mStep[0m  [144/169], [94mLoss[0m : 2.33077
[1mStep[0m  [160/169], [94mLoss[0m : 2.59848

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.351, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57620
[1mStep[0m  [16/169], [94mLoss[0m : 2.13872
[1mStep[0m  [32/169], [94mLoss[0m : 2.23699
[1mStep[0m  [48/169], [94mLoss[0m : 2.52128
[1mStep[0m  [64/169], [94mLoss[0m : 2.47188
[1mStep[0m  [80/169], [94mLoss[0m : 2.48166
[1mStep[0m  [96/169], [94mLoss[0m : 2.22796
[1mStep[0m  [112/169], [94mLoss[0m : 2.73011
[1mStep[0m  [128/169], [94mLoss[0m : 2.46973
[1mStep[0m  [144/169], [94mLoss[0m : 2.40245
[1mStep[0m  [160/169], [94mLoss[0m : 2.35185

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.342, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54296
[1mStep[0m  [16/169], [94mLoss[0m : 2.24276
[1mStep[0m  [32/169], [94mLoss[0m : 2.26281
[1mStep[0m  [48/169], [94mLoss[0m : 2.39563
[1mStep[0m  [64/169], [94mLoss[0m : 2.36738
[1mStep[0m  [80/169], [94mLoss[0m : 3.05044
[1mStep[0m  [96/169], [94mLoss[0m : 2.58815
[1mStep[0m  [112/169], [94mLoss[0m : 2.59826
[1mStep[0m  [128/169], [94mLoss[0m : 2.70023
[1mStep[0m  [144/169], [94mLoss[0m : 2.38244
[1mStep[0m  [160/169], [94mLoss[0m : 2.49058

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.360, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28034
[1mStep[0m  [16/169], [94mLoss[0m : 2.52635
[1mStep[0m  [32/169], [94mLoss[0m : 2.51192
[1mStep[0m  [48/169], [94mLoss[0m : 2.61079
[1mStep[0m  [64/169], [94mLoss[0m : 2.36747
[1mStep[0m  [80/169], [94mLoss[0m : 2.65731
[1mStep[0m  [96/169], [94mLoss[0m : 2.42289
[1mStep[0m  [112/169], [94mLoss[0m : 2.16622
[1mStep[0m  [128/169], [94mLoss[0m : 2.45532
[1mStep[0m  [144/169], [94mLoss[0m : 2.26983
[1mStep[0m  [160/169], [94mLoss[0m : 2.41087

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.370, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35244
[1mStep[0m  [16/169], [94mLoss[0m : 2.36999
[1mStep[0m  [32/169], [94mLoss[0m : 2.36894
[1mStep[0m  [48/169], [94mLoss[0m : 2.21004
[1mStep[0m  [64/169], [94mLoss[0m : 2.28022
[1mStep[0m  [80/169], [94mLoss[0m : 2.29893
[1mStep[0m  [96/169], [94mLoss[0m : 2.01363
[1mStep[0m  [112/169], [94mLoss[0m : 2.44540
[1mStep[0m  [128/169], [94mLoss[0m : 3.11491
[1mStep[0m  [144/169], [94mLoss[0m : 2.51661
[1mStep[0m  [160/169], [94mLoss[0m : 2.85983

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.352, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19857
[1mStep[0m  [16/169], [94mLoss[0m : 2.30738
[1mStep[0m  [32/169], [94mLoss[0m : 2.14989
[1mStep[0m  [48/169], [94mLoss[0m : 2.39007
[1mStep[0m  [64/169], [94mLoss[0m : 2.71677
[1mStep[0m  [80/169], [94mLoss[0m : 2.19833
[1mStep[0m  [96/169], [94mLoss[0m : 2.10484
[1mStep[0m  [112/169], [94mLoss[0m : 2.46035
[1mStep[0m  [128/169], [94mLoss[0m : 2.87741
[1mStep[0m  [144/169], [94mLoss[0m : 2.54623
[1mStep[0m  [160/169], [94mLoss[0m : 2.46861

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.363, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24782
[1mStep[0m  [16/169], [94mLoss[0m : 2.12924
[1mStep[0m  [32/169], [94mLoss[0m : 2.32523
[1mStep[0m  [48/169], [94mLoss[0m : 2.31852
[1mStep[0m  [64/169], [94mLoss[0m : 2.42691
[1mStep[0m  [80/169], [94mLoss[0m : 1.99883
[1mStep[0m  [96/169], [94mLoss[0m : 2.51943
[1mStep[0m  [112/169], [94mLoss[0m : 2.71054
[1mStep[0m  [128/169], [94mLoss[0m : 2.54345
[1mStep[0m  [144/169], [94mLoss[0m : 2.32839
[1mStep[0m  [160/169], [94mLoss[0m : 2.33406

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56893
[1mStep[0m  [16/169], [94mLoss[0m : 2.19080
[1mStep[0m  [32/169], [94mLoss[0m : 3.17207
[1mStep[0m  [48/169], [94mLoss[0m : 2.00115
[1mStep[0m  [64/169], [94mLoss[0m : 2.42402
[1mStep[0m  [80/169], [94mLoss[0m : 2.37020
[1mStep[0m  [96/169], [94mLoss[0m : 3.30020
[1mStep[0m  [112/169], [94mLoss[0m : 2.42440
[1mStep[0m  [128/169], [94mLoss[0m : 2.48063
[1mStep[0m  [144/169], [94mLoss[0m : 2.91576
[1mStep[0m  [160/169], [94mLoss[0m : 2.19167

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.376, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19063
[1mStep[0m  [16/169], [94mLoss[0m : 1.80301
[1mStep[0m  [32/169], [94mLoss[0m : 2.56871
[1mStep[0m  [48/169], [94mLoss[0m : 2.45852
[1mStep[0m  [64/169], [94mLoss[0m : 2.03626
[1mStep[0m  [80/169], [94mLoss[0m : 2.41364
[1mStep[0m  [96/169], [94mLoss[0m : 2.13834
[1mStep[0m  [112/169], [94mLoss[0m : 2.37894
[1mStep[0m  [128/169], [94mLoss[0m : 2.33841
[1mStep[0m  [144/169], [94mLoss[0m : 2.52519
[1mStep[0m  [160/169], [94mLoss[0m : 2.63614

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.344, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.10871
[1mStep[0m  [16/169], [94mLoss[0m : 2.59081
[1mStep[0m  [32/169], [94mLoss[0m : 2.41744
[1mStep[0m  [48/169], [94mLoss[0m : 2.22236
[1mStep[0m  [64/169], [94mLoss[0m : 2.36270
[1mStep[0m  [80/169], [94mLoss[0m : 2.22777
[1mStep[0m  [96/169], [94mLoss[0m : 2.05450
[1mStep[0m  [112/169], [94mLoss[0m : 2.15381
[1mStep[0m  [128/169], [94mLoss[0m : 2.35196
[1mStep[0m  [144/169], [94mLoss[0m : 2.15969
[1mStep[0m  [160/169], [94mLoss[0m : 2.19460

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25008
[1mStep[0m  [16/169], [94mLoss[0m : 2.37527
[1mStep[0m  [32/169], [94mLoss[0m : 2.66754
[1mStep[0m  [48/169], [94mLoss[0m : 2.44055
[1mStep[0m  [64/169], [94mLoss[0m : 2.65875
[1mStep[0m  [80/169], [94mLoss[0m : 2.24053
[1mStep[0m  [96/169], [94mLoss[0m : 2.64304
[1mStep[0m  [112/169], [94mLoss[0m : 2.15240
[1mStep[0m  [128/169], [94mLoss[0m : 2.47115
[1mStep[0m  [144/169], [94mLoss[0m : 2.34476
[1mStep[0m  [160/169], [94mLoss[0m : 2.36656

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.452, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06865
[1mStep[0m  [16/169], [94mLoss[0m : 2.28668
[1mStep[0m  [32/169], [94mLoss[0m : 2.66626
[1mStep[0m  [48/169], [94mLoss[0m : 2.40135
[1mStep[0m  [64/169], [94mLoss[0m : 2.47453
[1mStep[0m  [80/169], [94mLoss[0m : 2.72559
[1mStep[0m  [96/169], [94mLoss[0m : 2.70441
[1mStep[0m  [112/169], [94mLoss[0m : 1.90567
[1mStep[0m  [128/169], [94mLoss[0m : 2.23467
[1mStep[0m  [144/169], [94mLoss[0m : 2.52559
[1mStep[0m  [160/169], [94mLoss[0m : 2.44200

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.335, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78752
[1mStep[0m  [16/169], [94mLoss[0m : 2.20752
[1mStep[0m  [32/169], [94mLoss[0m : 2.45177
[1mStep[0m  [48/169], [94mLoss[0m : 2.50537
[1mStep[0m  [64/169], [94mLoss[0m : 2.41195
[1mStep[0m  [80/169], [94mLoss[0m : 2.53633
[1mStep[0m  [96/169], [94mLoss[0m : 2.46658
[1mStep[0m  [112/169], [94mLoss[0m : 2.79019
[1mStep[0m  [128/169], [94mLoss[0m : 2.24789
[1mStep[0m  [144/169], [94mLoss[0m : 2.53388
[1mStep[0m  [160/169], [94mLoss[0m : 2.52100

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.337, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98262
[1mStep[0m  [16/169], [94mLoss[0m : 2.35270
[1mStep[0m  [32/169], [94mLoss[0m : 2.53446
[1mStep[0m  [48/169], [94mLoss[0m : 2.45334
[1mStep[0m  [64/169], [94mLoss[0m : 2.30037
[1mStep[0m  [80/169], [94mLoss[0m : 2.70664
[1mStep[0m  [96/169], [94mLoss[0m : 2.20501
[1mStep[0m  [112/169], [94mLoss[0m : 2.39606
[1mStep[0m  [128/169], [94mLoss[0m : 2.14065
[1mStep[0m  [144/169], [94mLoss[0m : 2.28174
[1mStep[0m  [160/169], [94mLoss[0m : 2.43323

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52407
[1mStep[0m  [16/169], [94mLoss[0m : 2.38298
[1mStep[0m  [32/169], [94mLoss[0m : 2.45011
[1mStep[0m  [48/169], [94mLoss[0m : 2.66557
[1mStep[0m  [64/169], [94mLoss[0m : 2.68600
[1mStep[0m  [80/169], [94mLoss[0m : 2.66181
[1mStep[0m  [96/169], [94mLoss[0m : 2.10843
[1mStep[0m  [112/169], [94mLoss[0m : 2.01715
[1mStep[0m  [128/169], [94mLoss[0m : 2.09964
[1mStep[0m  [144/169], [94mLoss[0m : 2.79762
[1mStep[0m  [160/169], [94mLoss[0m : 2.28335

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.343, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48448
[1mStep[0m  [16/169], [94mLoss[0m : 2.36319
[1mStep[0m  [32/169], [94mLoss[0m : 2.29328
[1mStep[0m  [48/169], [94mLoss[0m : 2.21278
[1mStep[0m  [64/169], [94mLoss[0m : 2.25074
[1mStep[0m  [80/169], [94mLoss[0m : 2.07692
[1mStep[0m  [96/169], [94mLoss[0m : 2.40941
[1mStep[0m  [112/169], [94mLoss[0m : 2.48708
[1mStep[0m  [128/169], [94mLoss[0m : 1.90596
[1mStep[0m  [144/169], [94mLoss[0m : 2.26389
[1mStep[0m  [160/169], [94mLoss[0m : 2.57620

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.344, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23539
[1mStep[0m  [16/169], [94mLoss[0m : 2.46928
[1mStep[0m  [32/169], [94mLoss[0m : 1.62800
[1mStep[0m  [48/169], [94mLoss[0m : 2.52402
[1mStep[0m  [64/169], [94mLoss[0m : 2.32480
[1mStep[0m  [80/169], [94mLoss[0m : 2.19762
[1mStep[0m  [96/169], [94mLoss[0m : 2.20994
[1mStep[0m  [112/169], [94mLoss[0m : 2.33783
[1mStep[0m  [128/169], [94mLoss[0m : 2.47032
[1mStep[0m  [144/169], [94mLoss[0m : 2.48147
[1mStep[0m  [160/169], [94mLoss[0m : 2.75375

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.365, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31605
[1mStep[0m  [16/169], [94mLoss[0m : 2.17498
[1mStep[0m  [32/169], [94mLoss[0m : 2.35315
[1mStep[0m  [48/169], [94mLoss[0m : 2.59878
[1mStep[0m  [64/169], [94mLoss[0m : 2.49080
[1mStep[0m  [80/169], [94mLoss[0m : 2.24192
[1mStep[0m  [96/169], [94mLoss[0m : 2.45360
[1mStep[0m  [112/169], [94mLoss[0m : 2.48838
[1mStep[0m  [128/169], [94mLoss[0m : 2.67841
[1mStep[0m  [144/169], [94mLoss[0m : 2.40159
[1mStep[0m  [160/169], [94mLoss[0m : 2.49683

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.323, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18240
[1mStep[0m  [16/169], [94mLoss[0m : 2.44721
[1mStep[0m  [32/169], [94mLoss[0m : 2.63343
[1mStep[0m  [48/169], [94mLoss[0m : 2.74493
[1mStep[0m  [64/169], [94mLoss[0m : 2.22210
[1mStep[0m  [80/169], [94mLoss[0m : 2.14057
[1mStep[0m  [96/169], [94mLoss[0m : 2.52188
[1mStep[0m  [112/169], [94mLoss[0m : 2.51237
[1mStep[0m  [128/169], [94mLoss[0m : 2.38575
[1mStep[0m  [144/169], [94mLoss[0m : 2.48574
[1mStep[0m  [160/169], [94mLoss[0m : 2.63785

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.351, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26772
[1mStep[0m  [16/169], [94mLoss[0m : 2.08676
[1mStep[0m  [32/169], [94mLoss[0m : 2.61140
[1mStep[0m  [48/169], [94mLoss[0m : 2.46423
[1mStep[0m  [64/169], [94mLoss[0m : 2.68195
[1mStep[0m  [80/169], [94mLoss[0m : 2.03549
[1mStep[0m  [96/169], [94mLoss[0m : 2.61996
[1mStep[0m  [112/169], [94mLoss[0m : 2.49260
[1mStep[0m  [128/169], [94mLoss[0m : 2.05713
[1mStep[0m  [144/169], [94mLoss[0m : 2.43836
[1mStep[0m  [160/169], [94mLoss[0m : 3.02627

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.402, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08267
[1mStep[0m  [16/169], [94mLoss[0m : 3.06545
[1mStep[0m  [32/169], [94mLoss[0m : 2.58677
[1mStep[0m  [48/169], [94mLoss[0m : 2.66550
[1mStep[0m  [64/169], [94mLoss[0m : 2.07245
[1mStep[0m  [80/169], [94mLoss[0m : 2.05815
[1mStep[0m  [96/169], [94mLoss[0m : 2.56224
[1mStep[0m  [112/169], [94mLoss[0m : 1.95697
[1mStep[0m  [128/169], [94mLoss[0m : 2.79358
[1mStep[0m  [144/169], [94mLoss[0m : 2.28130
[1mStep[0m  [160/169], [94mLoss[0m : 2.85639

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.361, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31986
[1mStep[0m  [16/169], [94mLoss[0m : 2.33525
[1mStep[0m  [32/169], [94mLoss[0m : 2.61401
[1mStep[0m  [48/169], [94mLoss[0m : 1.96478
[1mStep[0m  [64/169], [94mLoss[0m : 2.72509
[1mStep[0m  [80/169], [94mLoss[0m : 2.66803
[1mStep[0m  [96/169], [94mLoss[0m : 2.42745
[1mStep[0m  [112/169], [94mLoss[0m : 2.44215
[1mStep[0m  [128/169], [94mLoss[0m : 2.49074
[1mStep[0m  [144/169], [94mLoss[0m : 2.51396
[1mStep[0m  [160/169], [94mLoss[0m : 2.50076

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.346, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.391
====================================

Phase 1 - Evaluation MAE:  2.3908541990177974
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.38928
[1mStep[0m  [16/169], [94mLoss[0m : 2.68185
[1mStep[0m  [32/169], [94mLoss[0m : 2.62883
[1mStep[0m  [48/169], [94mLoss[0m : 2.45143
[1mStep[0m  [64/169], [94mLoss[0m : 2.57605
[1mStep[0m  [80/169], [94mLoss[0m : 2.91570
[1mStep[0m  [96/169], [94mLoss[0m : 2.45285
[1mStep[0m  [112/169], [94mLoss[0m : 2.58272
[1mStep[0m  [128/169], [94mLoss[0m : 2.18471
[1mStep[0m  [144/169], [94mLoss[0m : 2.59315
[1mStep[0m  [160/169], [94mLoss[0m : 1.99587

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54127
[1mStep[0m  [16/169], [94mLoss[0m : 2.39070
[1mStep[0m  [32/169], [94mLoss[0m : 2.10374
[1mStep[0m  [48/169], [94mLoss[0m : 2.28106
[1mStep[0m  [64/169], [94mLoss[0m : 2.41187
[1mStep[0m  [80/169], [94mLoss[0m : 2.15783
[1mStep[0m  [96/169], [94mLoss[0m : 2.18313
[1mStep[0m  [112/169], [94mLoss[0m : 2.16888
[1mStep[0m  [128/169], [94mLoss[0m : 2.42370
[1mStep[0m  [144/169], [94mLoss[0m : 2.16954
[1mStep[0m  [160/169], [94mLoss[0m : 2.13674

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.373, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25996
[1mStep[0m  [16/169], [94mLoss[0m : 1.75187
[1mStep[0m  [32/169], [94mLoss[0m : 2.34553
[1mStep[0m  [48/169], [94mLoss[0m : 2.25540
[1mStep[0m  [64/169], [94mLoss[0m : 1.98423
[1mStep[0m  [80/169], [94mLoss[0m : 1.94381
[1mStep[0m  [96/169], [94mLoss[0m : 2.26329
[1mStep[0m  [112/169], [94mLoss[0m : 2.37432
[1mStep[0m  [128/169], [94mLoss[0m : 2.22145
[1mStep[0m  [144/169], [94mLoss[0m : 2.59476
[1mStep[0m  [160/169], [94mLoss[0m : 2.31888

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.364, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83005
[1mStep[0m  [16/169], [94mLoss[0m : 2.12313
[1mStep[0m  [32/169], [94mLoss[0m : 1.82272
[1mStep[0m  [48/169], [94mLoss[0m : 2.10995
[1mStep[0m  [64/169], [94mLoss[0m : 2.60061
[1mStep[0m  [80/169], [94mLoss[0m : 2.89754
[1mStep[0m  [96/169], [94mLoss[0m : 2.31292
[1mStep[0m  [112/169], [94mLoss[0m : 1.97045
[1mStep[0m  [128/169], [94mLoss[0m : 2.09775
[1mStep[0m  [144/169], [94mLoss[0m : 2.42727
[1mStep[0m  [160/169], [94mLoss[0m : 2.32888

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20728
[1mStep[0m  [16/169], [94mLoss[0m : 2.26952
[1mStep[0m  [32/169], [94mLoss[0m : 2.06393
[1mStep[0m  [48/169], [94mLoss[0m : 2.74056
[1mStep[0m  [64/169], [94mLoss[0m : 2.44042
[1mStep[0m  [80/169], [94mLoss[0m : 2.47051
[1mStep[0m  [96/169], [94mLoss[0m : 1.95787
[1mStep[0m  [112/169], [94mLoss[0m : 2.69402
[1mStep[0m  [128/169], [94mLoss[0m : 2.01776
[1mStep[0m  [144/169], [94mLoss[0m : 1.97238
[1mStep[0m  [160/169], [94mLoss[0m : 2.41073

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99375
[1mStep[0m  [16/169], [94mLoss[0m : 1.76157
[1mStep[0m  [32/169], [94mLoss[0m : 1.92374
[1mStep[0m  [48/169], [94mLoss[0m : 1.86479
[1mStep[0m  [64/169], [94mLoss[0m : 1.94375
[1mStep[0m  [80/169], [94mLoss[0m : 2.01654
[1mStep[0m  [96/169], [94mLoss[0m : 2.44957
[1mStep[0m  [112/169], [94mLoss[0m : 2.05538
[1mStep[0m  [128/169], [94mLoss[0m : 1.65780
[1mStep[0m  [144/169], [94mLoss[0m : 2.13730
[1mStep[0m  [160/169], [94mLoss[0m : 2.18425

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.397, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01098
[1mStep[0m  [16/169], [94mLoss[0m : 2.09072
[1mStep[0m  [32/169], [94mLoss[0m : 2.30258
[1mStep[0m  [48/169], [94mLoss[0m : 1.78644
[1mStep[0m  [64/169], [94mLoss[0m : 2.43022
[1mStep[0m  [80/169], [94mLoss[0m : 2.22802
[1mStep[0m  [96/169], [94mLoss[0m : 2.01685
[1mStep[0m  [112/169], [94mLoss[0m : 2.13567
[1mStep[0m  [128/169], [94mLoss[0m : 2.24555
[1mStep[0m  [144/169], [94mLoss[0m : 1.86627
[1mStep[0m  [160/169], [94mLoss[0m : 2.54215

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.387, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03422
[1mStep[0m  [16/169], [94mLoss[0m : 1.96281
[1mStep[0m  [32/169], [94mLoss[0m : 1.80480
[1mStep[0m  [48/169], [94mLoss[0m : 1.99284
[1mStep[0m  [64/169], [94mLoss[0m : 1.74220
[1mStep[0m  [80/169], [94mLoss[0m : 1.70496
[1mStep[0m  [96/169], [94mLoss[0m : 2.03391
[1mStep[0m  [112/169], [94mLoss[0m : 1.73327
[1mStep[0m  [128/169], [94mLoss[0m : 2.24141
[1mStep[0m  [144/169], [94mLoss[0m : 2.33386
[1mStep[0m  [160/169], [94mLoss[0m : 2.27854

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.461, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03935
[1mStep[0m  [16/169], [94mLoss[0m : 2.10665
[1mStep[0m  [32/169], [94mLoss[0m : 2.26632
[1mStep[0m  [48/169], [94mLoss[0m : 2.54816
[1mStep[0m  [64/169], [94mLoss[0m : 2.08753
[1mStep[0m  [80/169], [94mLoss[0m : 1.91344
[1mStep[0m  [96/169], [94mLoss[0m : 2.38759
[1mStep[0m  [112/169], [94mLoss[0m : 2.13957
[1mStep[0m  [128/169], [94mLoss[0m : 2.06852
[1mStep[0m  [144/169], [94mLoss[0m : 1.64055
[1mStep[0m  [160/169], [94mLoss[0m : 2.18158

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.470, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82823
[1mStep[0m  [16/169], [94mLoss[0m : 1.88712
[1mStep[0m  [32/169], [94mLoss[0m : 2.14418
[1mStep[0m  [48/169], [94mLoss[0m : 1.84312
[1mStep[0m  [64/169], [94mLoss[0m : 1.58908
[1mStep[0m  [80/169], [94mLoss[0m : 1.92597
[1mStep[0m  [96/169], [94mLoss[0m : 1.82055
[1mStep[0m  [112/169], [94mLoss[0m : 2.60225
[1mStep[0m  [128/169], [94mLoss[0m : 1.73836
[1mStep[0m  [144/169], [94mLoss[0m : 2.00791
[1mStep[0m  [160/169], [94mLoss[0m : 2.09503

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.930, [92mTest[0m: 2.443, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83577
[1mStep[0m  [16/169], [94mLoss[0m : 2.00883
[1mStep[0m  [32/169], [94mLoss[0m : 1.86447
[1mStep[0m  [48/169], [94mLoss[0m : 1.83218
[1mStep[0m  [64/169], [94mLoss[0m : 1.71857
[1mStep[0m  [80/169], [94mLoss[0m : 2.14287
[1mStep[0m  [96/169], [94mLoss[0m : 1.65389
[1mStep[0m  [112/169], [94mLoss[0m : 1.84578
[1mStep[0m  [128/169], [94mLoss[0m : 1.67228
[1mStep[0m  [144/169], [94mLoss[0m : 1.75839
[1mStep[0m  [160/169], [94mLoss[0m : 1.65254

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.900, [92mTest[0m: 2.475, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76176
[1mStep[0m  [16/169], [94mLoss[0m : 1.63193
[1mStep[0m  [32/169], [94mLoss[0m : 1.78683
[1mStep[0m  [48/169], [94mLoss[0m : 1.72793
[1mStep[0m  [64/169], [94mLoss[0m : 1.81157
[1mStep[0m  [80/169], [94mLoss[0m : 1.74679
[1mStep[0m  [96/169], [94mLoss[0m : 1.77972
[1mStep[0m  [112/169], [94mLoss[0m : 1.60322
[1mStep[0m  [128/169], [94mLoss[0m : 1.73530
[1mStep[0m  [144/169], [94mLoss[0m : 1.58960
[1mStep[0m  [160/169], [94mLoss[0m : 1.64497

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73879
[1mStep[0m  [16/169], [94mLoss[0m : 1.52343
[1mStep[0m  [32/169], [94mLoss[0m : 1.84421
[1mStep[0m  [48/169], [94mLoss[0m : 1.48597
[1mStep[0m  [64/169], [94mLoss[0m : 1.87167
[1mStep[0m  [80/169], [94mLoss[0m : 2.04079
[1mStep[0m  [96/169], [94mLoss[0m : 1.69540
[1mStep[0m  [112/169], [94mLoss[0m : 2.03278
[1mStep[0m  [128/169], [94mLoss[0m : 2.06235
[1mStep[0m  [144/169], [94mLoss[0m : 2.06925
[1mStep[0m  [160/169], [94mLoss[0m : 1.89371

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.802, [92mTest[0m: 2.455, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77095
[1mStep[0m  [16/169], [94mLoss[0m : 1.66639
[1mStep[0m  [32/169], [94mLoss[0m : 1.73993
[1mStep[0m  [48/169], [94mLoss[0m : 1.96929
[1mStep[0m  [64/169], [94mLoss[0m : 1.86028
[1mStep[0m  [80/169], [94mLoss[0m : 1.62169
[1mStep[0m  [96/169], [94mLoss[0m : 1.56317
[1mStep[0m  [112/169], [94mLoss[0m : 1.85566
[1mStep[0m  [128/169], [94mLoss[0m : 1.68896
[1mStep[0m  [144/169], [94mLoss[0m : 2.05191
[1mStep[0m  [160/169], [94mLoss[0m : 1.60369

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.425, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39389
[1mStep[0m  [16/169], [94mLoss[0m : 1.52467
[1mStep[0m  [32/169], [94mLoss[0m : 1.62966
[1mStep[0m  [48/169], [94mLoss[0m : 1.42554
[1mStep[0m  [64/169], [94mLoss[0m : 1.72150
[1mStep[0m  [80/169], [94mLoss[0m : 1.71001
[1mStep[0m  [96/169], [94mLoss[0m : 1.90736
[1mStep[0m  [112/169], [94mLoss[0m : 1.48115
[1mStep[0m  [128/169], [94mLoss[0m : 1.91688
[1mStep[0m  [144/169], [94mLoss[0m : 1.78600
[1mStep[0m  [160/169], [94mLoss[0m : 1.87190

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.428, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54957
[1mStep[0m  [16/169], [94mLoss[0m : 1.54506
[1mStep[0m  [32/169], [94mLoss[0m : 1.82709
[1mStep[0m  [48/169], [94mLoss[0m : 1.43406
[1mStep[0m  [64/169], [94mLoss[0m : 1.60977
[1mStep[0m  [80/169], [94mLoss[0m : 1.64869
[1mStep[0m  [96/169], [94mLoss[0m : 1.88528
[1mStep[0m  [112/169], [94mLoss[0m : 1.96411
[1mStep[0m  [128/169], [94mLoss[0m : 1.54944
[1mStep[0m  [144/169], [94mLoss[0m : 1.68577
[1mStep[0m  [160/169], [94mLoss[0m : 1.59535

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.721, [92mTest[0m: 2.507, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41322
[1mStep[0m  [16/169], [94mLoss[0m : 1.84721
[1mStep[0m  [32/169], [94mLoss[0m : 1.65856
[1mStep[0m  [48/169], [94mLoss[0m : 1.74399
[1mStep[0m  [64/169], [94mLoss[0m : 1.77602
[1mStep[0m  [80/169], [94mLoss[0m : 1.56022
[1mStep[0m  [96/169], [94mLoss[0m : 1.46158
[1mStep[0m  [112/169], [94mLoss[0m : 1.58042
[1mStep[0m  [128/169], [94mLoss[0m : 1.77539
[1mStep[0m  [144/169], [94mLoss[0m : 1.86826
[1mStep[0m  [160/169], [94mLoss[0m : 1.77422

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.493, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.29079
[1mStep[0m  [16/169], [94mLoss[0m : 1.73431
[1mStep[0m  [32/169], [94mLoss[0m : 1.92876
[1mStep[0m  [48/169], [94mLoss[0m : 1.42561
[1mStep[0m  [64/169], [94mLoss[0m : 1.39095
[1mStep[0m  [80/169], [94mLoss[0m : 1.59150
[1mStep[0m  [96/169], [94mLoss[0m : 1.56722
[1mStep[0m  [112/169], [94mLoss[0m : 1.52906
[1mStep[0m  [128/169], [94mLoss[0m : 1.95629
[1mStep[0m  [144/169], [94mLoss[0m : 1.69695
[1mStep[0m  [160/169], [94mLoss[0m : 1.53957

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.490, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50919
[1mStep[0m  [16/169], [94mLoss[0m : 1.05415
[1mStep[0m  [32/169], [94mLoss[0m : 1.24475
[1mStep[0m  [48/169], [94mLoss[0m : 1.21004
[1mStep[0m  [64/169], [94mLoss[0m : 2.22962
[1mStep[0m  [80/169], [94mLoss[0m : 1.66748
[1mStep[0m  [96/169], [94mLoss[0m : 1.58958
[1mStep[0m  [112/169], [94mLoss[0m : 1.62348
[1mStep[0m  [128/169], [94mLoss[0m : 1.61527
[1mStep[0m  [144/169], [94mLoss[0m : 1.82162
[1mStep[0m  [160/169], [94mLoss[0m : 1.75059

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.491, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41846
[1mStep[0m  [16/169], [94mLoss[0m : 1.38193
[1mStep[0m  [32/169], [94mLoss[0m : 1.49390
[1mStep[0m  [48/169], [94mLoss[0m : 1.53589
[1mStep[0m  [64/169], [94mLoss[0m : 1.44267
[1mStep[0m  [80/169], [94mLoss[0m : 1.73300
[1mStep[0m  [96/169], [94mLoss[0m : 1.62591
[1mStep[0m  [112/169], [94mLoss[0m : 1.84011
[1mStep[0m  [128/169], [94mLoss[0m : 1.65093
[1mStep[0m  [144/169], [94mLoss[0m : 1.69570
[1mStep[0m  [160/169], [94mLoss[0m : 1.58787

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.480, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46653
[1mStep[0m  [16/169], [94mLoss[0m : 1.65624
[1mStep[0m  [32/169], [94mLoss[0m : 1.43163
[1mStep[0m  [48/169], [94mLoss[0m : 1.42652
[1mStep[0m  [64/169], [94mLoss[0m : 1.64702
[1mStep[0m  [80/169], [94mLoss[0m : 1.64853
[1mStep[0m  [96/169], [94mLoss[0m : 1.67799
[1mStep[0m  [112/169], [94mLoss[0m : 1.87716
[1mStep[0m  [128/169], [94mLoss[0m : 1.59002
[1mStep[0m  [144/169], [94mLoss[0m : 1.56668
[1mStep[0m  [160/169], [94mLoss[0m : 1.67550

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.533, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55616
[1mStep[0m  [16/169], [94mLoss[0m : 1.26695
[1mStep[0m  [32/169], [94mLoss[0m : 1.33409
[1mStep[0m  [48/169], [94mLoss[0m : 1.47443
[1mStep[0m  [64/169], [94mLoss[0m : 1.47234
[1mStep[0m  [80/169], [94mLoss[0m : 1.50015
[1mStep[0m  [96/169], [94mLoss[0m : 1.70393
[1mStep[0m  [112/169], [94mLoss[0m : 1.80921
[1mStep[0m  [128/169], [94mLoss[0m : 1.62637
[1mStep[0m  [144/169], [94mLoss[0m : 1.58800
[1mStep[0m  [160/169], [94mLoss[0m : 1.72315

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.504, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.37136
[1mStep[0m  [16/169], [94mLoss[0m : 1.35015
[1mStep[0m  [32/169], [94mLoss[0m : 1.39573
[1mStep[0m  [48/169], [94mLoss[0m : 1.40814
[1mStep[0m  [64/169], [94mLoss[0m : 1.44291
[1mStep[0m  [80/169], [94mLoss[0m : 1.62743
[1mStep[0m  [96/169], [94mLoss[0m : 1.52002
[1mStep[0m  [112/169], [94mLoss[0m : 1.66707
[1mStep[0m  [128/169], [94mLoss[0m : 1.54297
[1mStep[0m  [144/169], [94mLoss[0m : 1.68875
[1mStep[0m  [160/169], [94mLoss[0m : 1.85572

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.458, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41677
[1mStep[0m  [16/169], [94mLoss[0m : 1.50448
[1mStep[0m  [32/169], [94mLoss[0m : 1.46120
[1mStep[0m  [48/169], [94mLoss[0m : 1.05914
[1mStep[0m  [64/169], [94mLoss[0m : 1.54496
[1mStep[0m  [80/169], [94mLoss[0m : 1.39129
[1mStep[0m  [96/169], [94mLoss[0m : 1.91698
[1mStep[0m  [112/169], [94mLoss[0m : 1.41507
[1mStep[0m  [128/169], [94mLoss[0m : 1.51967
[1mStep[0m  [144/169], [94mLoss[0m : 1.25797
[1mStep[0m  [160/169], [94mLoss[0m : 1.43012

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.493, [92mTest[0m: 2.508, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53178
[1mStep[0m  [16/169], [94mLoss[0m : 1.27132
[1mStep[0m  [32/169], [94mLoss[0m : 1.29585
[1mStep[0m  [48/169], [94mLoss[0m : 1.65194
[1mStep[0m  [64/169], [94mLoss[0m : 1.48890
[1mStep[0m  [80/169], [94mLoss[0m : 1.31799
[1mStep[0m  [96/169], [94mLoss[0m : 1.48979
[1mStep[0m  [112/169], [94mLoss[0m : 1.28347
[1mStep[0m  [128/169], [94mLoss[0m : 1.44759
[1mStep[0m  [144/169], [94mLoss[0m : 1.20242
[1mStep[0m  [160/169], [94mLoss[0m : 1.20806

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.453, [92mTest[0m: 2.516, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39786
[1mStep[0m  [16/169], [94mLoss[0m : 1.30555
[1mStep[0m  [32/169], [94mLoss[0m : 1.62119
[1mStep[0m  [48/169], [94mLoss[0m : 1.39011
[1mStep[0m  [64/169], [94mLoss[0m : 1.28588
[1mStep[0m  [80/169], [94mLoss[0m : 1.83291
[1mStep[0m  [96/169], [94mLoss[0m : 1.50956
[1mStep[0m  [112/169], [94mLoss[0m : 1.50843
[1mStep[0m  [128/169], [94mLoss[0m : 1.60315
[1mStep[0m  [144/169], [94mLoss[0m : 0.97479
[1mStep[0m  [160/169], [94mLoss[0m : 1.71435

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.434, [92mTest[0m: 2.519, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.22328
[1mStep[0m  [16/169], [94mLoss[0m : 1.22119
[1mStep[0m  [32/169], [94mLoss[0m : 1.59976
[1mStep[0m  [48/169], [94mLoss[0m : 1.47374
[1mStep[0m  [64/169], [94mLoss[0m : 1.59680
[1mStep[0m  [80/169], [94mLoss[0m : 1.35956
[1mStep[0m  [96/169], [94mLoss[0m : 1.33178
[1mStep[0m  [112/169], [94mLoss[0m : 1.48803
[1mStep[0m  [128/169], [94mLoss[0m : 1.68998
[1mStep[0m  [144/169], [94mLoss[0m : 1.37151
[1mStep[0m  [160/169], [94mLoss[0m : 1.57442

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.405, [92mTest[0m: 2.506, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.29973
[1mStep[0m  [16/169], [94mLoss[0m : 1.42763
[1mStep[0m  [32/169], [94mLoss[0m : 1.60009
[1mStep[0m  [48/169], [94mLoss[0m : 1.30432
[1mStep[0m  [64/169], [94mLoss[0m : 1.42149
[1mStep[0m  [80/169], [94mLoss[0m : 1.40226
[1mStep[0m  [96/169], [94mLoss[0m : 1.35956
[1mStep[0m  [112/169], [94mLoss[0m : 1.07919
[1mStep[0m  [128/169], [94mLoss[0m : 1.41678
[1mStep[0m  [144/169], [94mLoss[0m : 1.31586
[1mStep[0m  [160/169], [94mLoss[0m : 1.45005

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.418, [92mTest[0m: 2.495, [96mlr[0m: 0.009000000000000001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.546
====================================

Phase 2 - Evaluation MAE:  2.546312004327774
MAE score P1      2.390854
MAE score P2      2.546312
loss              1.405171
learning_rate         0.01
batch_size              64
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 9.64375
[1mStep[0m  [33/339], [94mLoss[0m : 3.17895
[1mStep[0m  [66/339], [94mLoss[0m : 2.32313
[1mStep[0m  [99/339], [94mLoss[0m : 2.53219
[1mStep[0m  [132/339], [94mLoss[0m : 2.74438
[1mStep[0m  [165/339], [94mLoss[0m : 3.24194
[1mStep[0m  [198/339], [94mLoss[0m : 2.59884
[1mStep[0m  [231/339], [94mLoss[0m : 2.58601
[1mStep[0m  [264/339], [94mLoss[0m : 2.03493
[1mStep[0m  [297/339], [94mLoss[0m : 2.42334
[1mStep[0m  [330/339], [94mLoss[0m : 2.89733

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.955, [92mTest[0m: 10.768, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62204
[1mStep[0m  [33/339], [94mLoss[0m : 2.37714
[1mStep[0m  [66/339], [94mLoss[0m : 2.64888
[1mStep[0m  [99/339], [94mLoss[0m : 2.43423
[1mStep[0m  [132/339], [94mLoss[0m : 3.25277
[1mStep[0m  [165/339], [94mLoss[0m : 2.32828
[1mStep[0m  [198/339], [94mLoss[0m : 2.21440
[1mStep[0m  [231/339], [94mLoss[0m : 3.17506
[1mStep[0m  [264/339], [94mLoss[0m : 2.06653
[1mStep[0m  [297/339], [94mLoss[0m : 2.44135
[1mStep[0m  [330/339], [94mLoss[0m : 2.36682

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.389, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32260
[1mStep[0m  [33/339], [94mLoss[0m : 2.30401
[1mStep[0m  [66/339], [94mLoss[0m : 2.82523
[1mStep[0m  [99/339], [94mLoss[0m : 2.52754
[1mStep[0m  [132/339], [94mLoss[0m : 2.00174
[1mStep[0m  [165/339], [94mLoss[0m : 3.05401
[1mStep[0m  [198/339], [94mLoss[0m : 2.48696
[1mStep[0m  [231/339], [94mLoss[0m : 2.41444
[1mStep[0m  [264/339], [94mLoss[0m : 2.34005
[1mStep[0m  [297/339], [94mLoss[0m : 1.73117
[1mStep[0m  [330/339], [94mLoss[0m : 2.00504

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.418, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07520
[1mStep[0m  [33/339], [94mLoss[0m : 2.28405
[1mStep[0m  [66/339], [94mLoss[0m : 2.04991
[1mStep[0m  [99/339], [94mLoss[0m : 2.90611
[1mStep[0m  [132/339], [94mLoss[0m : 2.35634
[1mStep[0m  [165/339], [94mLoss[0m : 3.08033
[1mStep[0m  [198/339], [94mLoss[0m : 2.49689
[1mStep[0m  [231/339], [94mLoss[0m : 2.14211
[1mStep[0m  [264/339], [94mLoss[0m : 2.78610
[1mStep[0m  [297/339], [94mLoss[0m : 2.12871
[1mStep[0m  [330/339], [94mLoss[0m : 2.41773

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.408, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31597
[1mStep[0m  [33/339], [94mLoss[0m : 2.49570
[1mStep[0m  [66/339], [94mLoss[0m : 2.72973
[1mStep[0m  [99/339], [94mLoss[0m : 3.11335
[1mStep[0m  [132/339], [94mLoss[0m : 2.86019
[1mStep[0m  [165/339], [94mLoss[0m : 2.48544
[1mStep[0m  [198/339], [94mLoss[0m : 2.70760
[1mStep[0m  [231/339], [94mLoss[0m : 2.04536
[1mStep[0m  [264/339], [94mLoss[0m : 2.09131
[1mStep[0m  [297/339], [94mLoss[0m : 1.88950
[1mStep[0m  [330/339], [94mLoss[0m : 2.71462

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.338, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09649
[1mStep[0m  [33/339], [94mLoss[0m : 2.26303
[1mStep[0m  [66/339], [94mLoss[0m : 2.35043
[1mStep[0m  [99/339], [94mLoss[0m : 2.54688
[1mStep[0m  [132/339], [94mLoss[0m : 2.61467
[1mStep[0m  [165/339], [94mLoss[0m : 2.46301
[1mStep[0m  [198/339], [94mLoss[0m : 2.39013
[1mStep[0m  [231/339], [94mLoss[0m : 2.27717
[1mStep[0m  [264/339], [94mLoss[0m : 2.09355
[1mStep[0m  [297/339], [94mLoss[0m : 2.27787
[1mStep[0m  [330/339], [94mLoss[0m : 3.07631

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.319, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.92942
[1mStep[0m  [33/339], [94mLoss[0m : 2.82554
[1mStep[0m  [66/339], [94mLoss[0m : 2.20410
[1mStep[0m  [99/339], [94mLoss[0m : 2.88046
[1mStep[0m  [132/339], [94mLoss[0m : 3.48081
[1mStep[0m  [165/339], [94mLoss[0m : 2.43397
[1mStep[0m  [198/339], [94mLoss[0m : 2.53405
[1mStep[0m  [231/339], [94mLoss[0m : 2.70826
[1mStep[0m  [264/339], [94mLoss[0m : 2.26009
[1mStep[0m  [297/339], [94mLoss[0m : 2.76284
[1mStep[0m  [330/339], [94mLoss[0m : 2.78566

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.357, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34470
[1mStep[0m  [33/339], [94mLoss[0m : 2.08467
[1mStep[0m  [66/339], [94mLoss[0m : 1.98420
[1mStep[0m  [99/339], [94mLoss[0m : 2.09040
[1mStep[0m  [132/339], [94mLoss[0m : 2.20874
[1mStep[0m  [165/339], [94mLoss[0m : 2.66959
[1mStep[0m  [198/339], [94mLoss[0m : 1.97050
[1mStep[0m  [231/339], [94mLoss[0m : 2.26542
[1mStep[0m  [264/339], [94mLoss[0m : 2.62469
[1mStep[0m  [297/339], [94mLoss[0m : 2.83354
[1mStep[0m  [330/339], [94mLoss[0m : 2.20612

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.329, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75680
[1mStep[0m  [33/339], [94mLoss[0m : 2.11233
[1mStep[0m  [66/339], [94mLoss[0m : 2.27485
[1mStep[0m  [99/339], [94mLoss[0m : 2.40167
[1mStep[0m  [132/339], [94mLoss[0m : 2.79696
[1mStep[0m  [165/339], [94mLoss[0m : 2.85790
[1mStep[0m  [198/339], [94mLoss[0m : 2.83309
[1mStep[0m  [231/339], [94mLoss[0m : 3.07620
[1mStep[0m  [264/339], [94mLoss[0m : 2.49998
[1mStep[0m  [297/339], [94mLoss[0m : 2.01702
[1mStep[0m  [330/339], [94mLoss[0m : 2.09192

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.315, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10271
[1mStep[0m  [33/339], [94mLoss[0m : 1.92458
[1mStep[0m  [66/339], [94mLoss[0m : 1.84611
[1mStep[0m  [99/339], [94mLoss[0m : 2.43667
[1mStep[0m  [132/339], [94mLoss[0m : 2.32698
[1mStep[0m  [165/339], [94mLoss[0m : 2.69623
[1mStep[0m  [198/339], [94mLoss[0m : 2.47267
[1mStep[0m  [231/339], [94mLoss[0m : 1.98521
[1mStep[0m  [264/339], [94mLoss[0m : 2.44139
[1mStep[0m  [297/339], [94mLoss[0m : 2.55278
[1mStep[0m  [330/339], [94mLoss[0m : 2.11211

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.395, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97488
[1mStep[0m  [33/339], [94mLoss[0m : 2.41373
[1mStep[0m  [66/339], [94mLoss[0m : 2.10683
[1mStep[0m  [99/339], [94mLoss[0m : 2.95447
[1mStep[0m  [132/339], [94mLoss[0m : 2.47159
[1mStep[0m  [165/339], [94mLoss[0m : 2.67954
[1mStep[0m  [198/339], [94mLoss[0m : 2.13830
[1mStep[0m  [231/339], [94mLoss[0m : 2.78116
[1mStep[0m  [264/339], [94mLoss[0m : 2.34146
[1mStep[0m  [297/339], [94mLoss[0m : 3.01775
[1mStep[0m  [330/339], [94mLoss[0m : 2.88810

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01004
[1mStep[0m  [33/339], [94mLoss[0m : 2.56893
[1mStep[0m  [66/339], [94mLoss[0m : 1.62368
[1mStep[0m  [99/339], [94mLoss[0m : 2.04795
[1mStep[0m  [132/339], [94mLoss[0m : 1.52260
[1mStep[0m  [165/339], [94mLoss[0m : 2.21447
[1mStep[0m  [198/339], [94mLoss[0m : 2.33394
[1mStep[0m  [231/339], [94mLoss[0m : 2.57578
[1mStep[0m  [264/339], [94mLoss[0m : 2.16542
[1mStep[0m  [297/339], [94mLoss[0m : 3.13559
[1mStep[0m  [330/339], [94mLoss[0m : 2.51722

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.339, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49855
[1mStep[0m  [33/339], [94mLoss[0m : 2.09159
[1mStep[0m  [66/339], [94mLoss[0m : 2.25248
[1mStep[0m  [99/339], [94mLoss[0m : 3.27745
[1mStep[0m  [132/339], [94mLoss[0m : 2.69657
[1mStep[0m  [165/339], [94mLoss[0m : 2.03854
[1mStep[0m  [198/339], [94mLoss[0m : 2.88729
[1mStep[0m  [231/339], [94mLoss[0m : 2.09671
[1mStep[0m  [264/339], [94mLoss[0m : 2.53294
[1mStep[0m  [297/339], [94mLoss[0m : 2.03967
[1mStep[0m  [330/339], [94mLoss[0m : 1.94367

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22140
[1mStep[0m  [33/339], [94mLoss[0m : 2.87556
[1mStep[0m  [66/339], [94mLoss[0m : 2.55675
[1mStep[0m  [99/339], [94mLoss[0m : 2.44007
[1mStep[0m  [132/339], [94mLoss[0m : 2.27495
[1mStep[0m  [165/339], [94mLoss[0m : 2.43705
[1mStep[0m  [198/339], [94mLoss[0m : 2.88486
[1mStep[0m  [231/339], [94mLoss[0m : 2.44146
[1mStep[0m  [264/339], [94mLoss[0m : 2.21887
[1mStep[0m  [297/339], [94mLoss[0m : 2.68915
[1mStep[0m  [330/339], [94mLoss[0m : 3.22730

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.354, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.97350
[1mStep[0m  [33/339], [94mLoss[0m : 2.39579
[1mStep[0m  [66/339], [94mLoss[0m : 3.64839
[1mStep[0m  [99/339], [94mLoss[0m : 2.11065
[1mStep[0m  [132/339], [94mLoss[0m : 2.41394
[1mStep[0m  [165/339], [94mLoss[0m : 2.52472
[1mStep[0m  [198/339], [94mLoss[0m : 2.50893
[1mStep[0m  [231/339], [94mLoss[0m : 2.21904
[1mStep[0m  [264/339], [94mLoss[0m : 1.99319
[1mStep[0m  [297/339], [94mLoss[0m : 1.63961
[1mStep[0m  [330/339], [94mLoss[0m : 3.17165

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.320, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52450
[1mStep[0m  [33/339], [94mLoss[0m : 2.85172
[1mStep[0m  [66/339], [94mLoss[0m : 2.19995
[1mStep[0m  [99/339], [94mLoss[0m : 2.67036
[1mStep[0m  [132/339], [94mLoss[0m : 2.63728
[1mStep[0m  [165/339], [94mLoss[0m : 2.63227
[1mStep[0m  [198/339], [94mLoss[0m : 2.76598
[1mStep[0m  [231/339], [94mLoss[0m : 2.69978
[1mStep[0m  [264/339], [94mLoss[0m : 2.18995
[1mStep[0m  [297/339], [94mLoss[0m : 2.28904
[1mStep[0m  [330/339], [94mLoss[0m : 2.04121

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.327, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32376
[1mStep[0m  [33/339], [94mLoss[0m : 3.10289
[1mStep[0m  [66/339], [94mLoss[0m : 3.01964
[1mStep[0m  [99/339], [94mLoss[0m : 2.90868
[1mStep[0m  [132/339], [94mLoss[0m : 2.65415
[1mStep[0m  [165/339], [94mLoss[0m : 2.62871
[1mStep[0m  [198/339], [94mLoss[0m : 2.65518
[1mStep[0m  [231/339], [94mLoss[0m : 3.10969
[1mStep[0m  [264/339], [94mLoss[0m : 2.35271
[1mStep[0m  [297/339], [94mLoss[0m : 3.00892
[1mStep[0m  [330/339], [94mLoss[0m : 2.49344

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.384, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01825
[1mStep[0m  [33/339], [94mLoss[0m : 2.20796
[1mStep[0m  [66/339], [94mLoss[0m : 2.74675
[1mStep[0m  [99/339], [94mLoss[0m : 2.95526
[1mStep[0m  [132/339], [94mLoss[0m : 2.41150
[1mStep[0m  [165/339], [94mLoss[0m : 2.30153
[1mStep[0m  [198/339], [94mLoss[0m : 2.99452
[1mStep[0m  [231/339], [94mLoss[0m : 2.17693
[1mStep[0m  [264/339], [94mLoss[0m : 2.61786
[1mStep[0m  [297/339], [94mLoss[0m : 2.34322
[1mStep[0m  [330/339], [94mLoss[0m : 2.88166

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.345, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20984
[1mStep[0m  [33/339], [94mLoss[0m : 2.41508
[1mStep[0m  [66/339], [94mLoss[0m : 2.57894
[1mStep[0m  [99/339], [94mLoss[0m : 2.27343
[1mStep[0m  [132/339], [94mLoss[0m : 2.90947
[1mStep[0m  [165/339], [94mLoss[0m : 2.22120
[1mStep[0m  [198/339], [94mLoss[0m : 2.72161
[1mStep[0m  [231/339], [94mLoss[0m : 2.80841
[1mStep[0m  [264/339], [94mLoss[0m : 2.52741
[1mStep[0m  [297/339], [94mLoss[0m : 1.95962
[1mStep[0m  [330/339], [94mLoss[0m : 2.18856

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.323, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81112
[1mStep[0m  [33/339], [94mLoss[0m : 2.01467
[1mStep[0m  [66/339], [94mLoss[0m : 2.70820
[1mStep[0m  [99/339], [94mLoss[0m : 1.95928
[1mStep[0m  [132/339], [94mLoss[0m : 2.04396
[1mStep[0m  [165/339], [94mLoss[0m : 2.19941
[1mStep[0m  [198/339], [94mLoss[0m : 2.20701
[1mStep[0m  [231/339], [94mLoss[0m : 2.22395
[1mStep[0m  [264/339], [94mLoss[0m : 1.89421
[1mStep[0m  [297/339], [94mLoss[0m : 2.08546
[1mStep[0m  [330/339], [94mLoss[0m : 2.26542

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41175
[1mStep[0m  [33/339], [94mLoss[0m : 2.07889
[1mStep[0m  [66/339], [94mLoss[0m : 2.30322
[1mStep[0m  [99/339], [94mLoss[0m : 2.65504
[1mStep[0m  [132/339], [94mLoss[0m : 2.89037
[1mStep[0m  [165/339], [94mLoss[0m : 2.62932
[1mStep[0m  [198/339], [94mLoss[0m : 2.46966
[1mStep[0m  [231/339], [94mLoss[0m : 2.50425
[1mStep[0m  [264/339], [94mLoss[0m : 2.13828
[1mStep[0m  [297/339], [94mLoss[0m : 2.19868
[1mStep[0m  [330/339], [94mLoss[0m : 2.18282

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.415, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34590
[1mStep[0m  [33/339], [94mLoss[0m : 3.07851
[1mStep[0m  [66/339], [94mLoss[0m : 2.43922
[1mStep[0m  [99/339], [94mLoss[0m : 2.75902
[1mStep[0m  [132/339], [94mLoss[0m : 2.17714
[1mStep[0m  [165/339], [94mLoss[0m : 2.42840
[1mStep[0m  [198/339], [94mLoss[0m : 2.74127
[1mStep[0m  [231/339], [94mLoss[0m : 2.31839
[1mStep[0m  [264/339], [94mLoss[0m : 2.56563
[1mStep[0m  [297/339], [94mLoss[0m : 2.70089
[1mStep[0m  [330/339], [94mLoss[0m : 2.26848

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.339, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40653
[1mStep[0m  [33/339], [94mLoss[0m : 2.52718
[1mStep[0m  [66/339], [94mLoss[0m : 2.46965
[1mStep[0m  [99/339], [94mLoss[0m : 2.32629
[1mStep[0m  [132/339], [94mLoss[0m : 2.65340
[1mStep[0m  [165/339], [94mLoss[0m : 2.70192
[1mStep[0m  [198/339], [94mLoss[0m : 2.91786
[1mStep[0m  [231/339], [94mLoss[0m : 2.92351
[1mStep[0m  [264/339], [94mLoss[0m : 3.33573
[1mStep[0m  [297/339], [94mLoss[0m : 2.35762
[1mStep[0m  [330/339], [94mLoss[0m : 2.69131

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.314, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64720
[1mStep[0m  [33/339], [94mLoss[0m : 2.80694
[1mStep[0m  [66/339], [94mLoss[0m : 2.40044
[1mStep[0m  [99/339], [94mLoss[0m : 2.58935
[1mStep[0m  [132/339], [94mLoss[0m : 2.40008
[1mStep[0m  [165/339], [94mLoss[0m : 2.48956
[1mStep[0m  [198/339], [94mLoss[0m : 2.41254
[1mStep[0m  [231/339], [94mLoss[0m : 2.54212
[1mStep[0m  [264/339], [94mLoss[0m : 2.97853
[1mStep[0m  [297/339], [94mLoss[0m : 2.64215
[1mStep[0m  [330/339], [94mLoss[0m : 2.65906

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.301, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39718
[1mStep[0m  [33/339], [94mLoss[0m : 2.41939
[1mStep[0m  [66/339], [94mLoss[0m : 1.60662
[1mStep[0m  [99/339], [94mLoss[0m : 2.63146
[1mStep[0m  [132/339], [94mLoss[0m : 2.12290
[1mStep[0m  [165/339], [94mLoss[0m : 2.29200
[1mStep[0m  [198/339], [94mLoss[0m : 2.48247
[1mStep[0m  [231/339], [94mLoss[0m : 2.79618
[1mStep[0m  [264/339], [94mLoss[0m : 2.77484
[1mStep[0m  [297/339], [94mLoss[0m : 2.60275
[1mStep[0m  [330/339], [94mLoss[0m : 2.16184

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49255
[1mStep[0m  [33/339], [94mLoss[0m : 2.93231
[1mStep[0m  [66/339], [94mLoss[0m : 2.40898
[1mStep[0m  [99/339], [94mLoss[0m : 2.85908
[1mStep[0m  [132/339], [94mLoss[0m : 2.30253
[1mStep[0m  [165/339], [94mLoss[0m : 2.42294
[1mStep[0m  [198/339], [94mLoss[0m : 2.36142
[1mStep[0m  [231/339], [94mLoss[0m : 1.95567
[1mStep[0m  [264/339], [94mLoss[0m : 1.72313
[1mStep[0m  [297/339], [94mLoss[0m : 2.64102
[1mStep[0m  [330/339], [94mLoss[0m : 2.23419

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.317, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44807
[1mStep[0m  [33/339], [94mLoss[0m : 2.61665
[1mStep[0m  [66/339], [94mLoss[0m : 2.39642
[1mStep[0m  [99/339], [94mLoss[0m : 2.29240
[1mStep[0m  [132/339], [94mLoss[0m : 2.62153
[1mStep[0m  [165/339], [94mLoss[0m : 2.29898
[1mStep[0m  [198/339], [94mLoss[0m : 2.62657
[1mStep[0m  [231/339], [94mLoss[0m : 2.06328
[1mStep[0m  [264/339], [94mLoss[0m : 3.01354
[1mStep[0m  [297/339], [94mLoss[0m : 2.00956
[1mStep[0m  [330/339], [94mLoss[0m : 2.07022

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.375, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16338
[1mStep[0m  [33/339], [94mLoss[0m : 2.01802
[1mStep[0m  [66/339], [94mLoss[0m : 2.56589
[1mStep[0m  [99/339], [94mLoss[0m : 2.51663
[1mStep[0m  [132/339], [94mLoss[0m : 2.27355
[1mStep[0m  [165/339], [94mLoss[0m : 2.57536
[1mStep[0m  [198/339], [94mLoss[0m : 1.87213
[1mStep[0m  [231/339], [94mLoss[0m : 3.33479
[1mStep[0m  [264/339], [94mLoss[0m : 2.56823
[1mStep[0m  [297/339], [94mLoss[0m : 2.44945
[1mStep[0m  [330/339], [94mLoss[0m : 2.37843

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.321, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10484
[1mStep[0m  [33/339], [94mLoss[0m : 2.40658
[1mStep[0m  [66/339], [94mLoss[0m : 2.86183
[1mStep[0m  [99/339], [94mLoss[0m : 2.80060
[1mStep[0m  [132/339], [94mLoss[0m : 2.24858
[1mStep[0m  [165/339], [94mLoss[0m : 2.22220
[1mStep[0m  [198/339], [94mLoss[0m : 1.79772
[1mStep[0m  [231/339], [94mLoss[0m : 2.73453
[1mStep[0m  [264/339], [94mLoss[0m : 2.09166
[1mStep[0m  [297/339], [94mLoss[0m : 2.49864
[1mStep[0m  [330/339], [94mLoss[0m : 1.95126

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.313, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63707
[1mStep[0m  [33/339], [94mLoss[0m : 2.33695
[1mStep[0m  [66/339], [94mLoss[0m : 2.38673
[1mStep[0m  [99/339], [94mLoss[0m : 2.26228
[1mStep[0m  [132/339], [94mLoss[0m : 2.94783
[1mStep[0m  [165/339], [94mLoss[0m : 2.63660
[1mStep[0m  [198/339], [94mLoss[0m : 2.80803
[1mStep[0m  [231/339], [94mLoss[0m : 2.69137
[1mStep[0m  [264/339], [94mLoss[0m : 3.48397
[1mStep[0m  [297/339], [94mLoss[0m : 2.29757
[1mStep[0m  [330/339], [94mLoss[0m : 2.25367

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.325, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.332
====================================

Phase 1 - Evaluation MAE:  2.3323416488360516
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.01
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 2.09501
[1mStep[0m  [33/339], [94mLoss[0m : 2.50962
[1mStep[0m  [66/339], [94mLoss[0m : 2.21151
[1mStep[0m  [99/339], [94mLoss[0m : 2.07473
[1mStep[0m  [132/339], [94mLoss[0m : 2.35435
[1mStep[0m  [165/339], [94mLoss[0m : 2.37776
[1mStep[0m  [198/339], [94mLoss[0m : 2.60047
[1mStep[0m  [231/339], [94mLoss[0m : 3.01289
[1mStep[0m  [264/339], [94mLoss[0m : 1.97361
[1mStep[0m  [297/339], [94mLoss[0m : 1.95004
[1mStep[0m  [330/339], [94mLoss[0m : 2.16792

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.332, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.69392
[1mStep[0m  [33/339], [94mLoss[0m : 1.95767
[1mStep[0m  [66/339], [94mLoss[0m : 2.53064
[1mStep[0m  [99/339], [94mLoss[0m : 2.48088
[1mStep[0m  [132/339], [94mLoss[0m : 1.91617
[1mStep[0m  [165/339], [94mLoss[0m : 2.47878
[1mStep[0m  [198/339], [94mLoss[0m : 2.27747
[1mStep[0m  [231/339], [94mLoss[0m : 2.86879
[1mStep[0m  [264/339], [94mLoss[0m : 2.75189
[1mStep[0m  [297/339], [94mLoss[0m : 2.42375
[1mStep[0m  [330/339], [94mLoss[0m : 2.42583

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.404, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98548
[1mStep[0m  [33/339], [94mLoss[0m : 3.08668
[1mStep[0m  [66/339], [94mLoss[0m : 2.18695
[1mStep[0m  [99/339], [94mLoss[0m : 3.34498
[1mStep[0m  [132/339], [94mLoss[0m : 2.26425
[1mStep[0m  [165/339], [94mLoss[0m : 2.21950
[1mStep[0m  [198/339], [94mLoss[0m : 2.96531
[1mStep[0m  [231/339], [94mLoss[0m : 2.26568
[1mStep[0m  [264/339], [94mLoss[0m : 2.48292
[1mStep[0m  [297/339], [94mLoss[0m : 2.38346
[1mStep[0m  [330/339], [94mLoss[0m : 2.69229

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.400, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26561
[1mStep[0m  [33/339], [94mLoss[0m : 2.76322
[1mStep[0m  [66/339], [94mLoss[0m : 2.37679
[1mStep[0m  [99/339], [94mLoss[0m : 1.86313
[1mStep[0m  [132/339], [94mLoss[0m : 2.64667
[1mStep[0m  [165/339], [94mLoss[0m : 2.96076
[1mStep[0m  [198/339], [94mLoss[0m : 2.37873
[1mStep[0m  [231/339], [94mLoss[0m : 2.95670
[1mStep[0m  [264/339], [94mLoss[0m : 2.58046
[1mStep[0m  [297/339], [94mLoss[0m : 2.36352
[1mStep[0m  [330/339], [94mLoss[0m : 2.43359

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.396, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.24751
[1mStep[0m  [33/339], [94mLoss[0m : 1.89935
[1mStep[0m  [66/339], [94mLoss[0m : 2.10874
[1mStep[0m  [99/339], [94mLoss[0m : 2.32057
[1mStep[0m  [132/339], [94mLoss[0m : 2.51591
[1mStep[0m  [165/339], [94mLoss[0m : 3.02122
[1mStep[0m  [198/339], [94mLoss[0m : 2.71113
[1mStep[0m  [231/339], [94mLoss[0m : 2.45007
[1mStep[0m  [264/339], [94mLoss[0m : 2.47697
[1mStep[0m  [297/339], [94mLoss[0m : 2.49413
[1mStep[0m  [330/339], [94mLoss[0m : 2.91175

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.501, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44706
[1mStep[0m  [33/339], [94mLoss[0m : 2.81623
[1mStep[0m  [66/339], [94mLoss[0m : 2.41670
[1mStep[0m  [99/339], [94mLoss[0m : 2.92178
[1mStep[0m  [132/339], [94mLoss[0m : 2.19655
[1mStep[0m  [165/339], [94mLoss[0m : 2.57423
[1mStep[0m  [198/339], [94mLoss[0m : 2.37213
[1mStep[0m  [231/339], [94mLoss[0m : 2.58473
[1mStep[0m  [264/339], [94mLoss[0m : 3.25718
[1mStep[0m  [297/339], [94mLoss[0m : 2.16609
[1mStep[0m  [330/339], [94mLoss[0m : 2.16553

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.405, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42329
[1mStep[0m  [33/339], [94mLoss[0m : 2.40100
[1mStep[0m  [66/339], [94mLoss[0m : 3.21900
[1mStep[0m  [99/339], [94mLoss[0m : 2.22524
[1mStep[0m  [132/339], [94mLoss[0m : 2.08182
[1mStep[0m  [165/339], [94mLoss[0m : 2.61527
[1mStep[0m  [198/339], [94mLoss[0m : 2.18750
[1mStep[0m  [231/339], [94mLoss[0m : 2.46679
[1mStep[0m  [264/339], [94mLoss[0m : 2.16819
[1mStep[0m  [297/339], [94mLoss[0m : 1.65743
[1mStep[0m  [330/339], [94mLoss[0m : 2.99567

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.465, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93304
[1mStep[0m  [33/339], [94mLoss[0m : 2.03826
[1mStep[0m  [66/339], [94mLoss[0m : 2.14416
[1mStep[0m  [99/339], [94mLoss[0m : 2.44205
[1mStep[0m  [132/339], [94mLoss[0m : 2.83883
[1mStep[0m  [165/339], [94mLoss[0m : 1.80198
[1mStep[0m  [198/339], [94mLoss[0m : 2.17935
[1mStep[0m  [231/339], [94mLoss[0m : 2.38003
[1mStep[0m  [264/339], [94mLoss[0m : 2.26976
[1mStep[0m  [297/339], [94mLoss[0m : 2.60115
[1mStep[0m  [330/339], [94mLoss[0m : 2.78759

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.421, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33138
[1mStep[0m  [33/339], [94mLoss[0m : 2.50485
[1mStep[0m  [66/339], [94mLoss[0m : 2.11692
[1mStep[0m  [99/339], [94mLoss[0m : 2.30142
[1mStep[0m  [132/339], [94mLoss[0m : 2.90672
[1mStep[0m  [165/339], [94mLoss[0m : 2.27637
[1mStep[0m  [198/339], [94mLoss[0m : 2.85259
[1mStep[0m  [231/339], [94mLoss[0m : 2.42039
[1mStep[0m  [264/339], [94mLoss[0m : 2.47878
[1mStep[0m  [297/339], [94mLoss[0m : 2.28506
[1mStep[0m  [330/339], [94mLoss[0m : 2.25183

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.456, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47979
[1mStep[0m  [33/339], [94mLoss[0m : 2.08978
[1mStep[0m  [66/339], [94mLoss[0m : 2.27207
[1mStep[0m  [99/339], [94mLoss[0m : 2.14548
[1mStep[0m  [132/339], [94mLoss[0m : 2.78741
[1mStep[0m  [165/339], [94mLoss[0m : 2.99860
[1mStep[0m  [198/339], [94mLoss[0m : 2.58526
[1mStep[0m  [231/339], [94mLoss[0m : 2.47269
[1mStep[0m  [264/339], [94mLoss[0m : 2.03845
[1mStep[0m  [297/339], [94mLoss[0m : 2.36921
[1mStep[0m  [330/339], [94mLoss[0m : 2.26340

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.440, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36446
[1mStep[0m  [33/339], [94mLoss[0m : 1.91243
[1mStep[0m  [66/339], [94mLoss[0m : 1.80572
[1mStep[0m  [99/339], [94mLoss[0m : 1.94838
[1mStep[0m  [132/339], [94mLoss[0m : 2.53422
[1mStep[0m  [165/339], [94mLoss[0m : 2.78202
[1mStep[0m  [198/339], [94mLoss[0m : 2.32195
[1mStep[0m  [231/339], [94mLoss[0m : 2.23265
[1mStep[0m  [264/339], [94mLoss[0m : 2.88139
[1mStep[0m  [297/339], [94mLoss[0m : 2.66614
[1mStep[0m  [330/339], [94mLoss[0m : 3.21160

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.476, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03453
[1mStep[0m  [33/339], [94mLoss[0m : 2.58677
[1mStep[0m  [66/339], [94mLoss[0m : 2.39953
[1mStep[0m  [99/339], [94mLoss[0m : 2.84957
[1mStep[0m  [132/339], [94mLoss[0m : 2.84006
[1mStep[0m  [165/339], [94mLoss[0m : 2.64265
[1mStep[0m  [198/339], [94mLoss[0m : 2.07680
[1mStep[0m  [231/339], [94mLoss[0m : 2.67236
[1mStep[0m  [264/339], [94mLoss[0m : 2.10922
[1mStep[0m  [297/339], [94mLoss[0m : 2.19441
[1mStep[0m  [330/339], [94mLoss[0m : 3.15938

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40699
[1mStep[0m  [33/339], [94mLoss[0m : 2.25151
[1mStep[0m  [66/339], [94mLoss[0m : 2.66573
[1mStep[0m  [99/339], [94mLoss[0m : 2.21889
[1mStep[0m  [132/339], [94mLoss[0m : 1.60723
[1mStep[0m  [165/339], [94mLoss[0m : 2.45301
[1mStep[0m  [198/339], [94mLoss[0m : 2.91957
[1mStep[0m  [231/339], [94mLoss[0m : 2.10238
[1mStep[0m  [264/339], [94mLoss[0m : 2.73380
[1mStep[0m  [297/339], [94mLoss[0m : 2.92314
[1mStep[0m  [330/339], [94mLoss[0m : 3.38696

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.432, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22939
[1mStep[0m  [33/339], [94mLoss[0m : 2.69229
[1mStep[0m  [66/339], [94mLoss[0m : 2.44200
[1mStep[0m  [99/339], [94mLoss[0m : 2.02481
[1mStep[0m  [132/339], [94mLoss[0m : 2.43547
[1mStep[0m  [165/339], [94mLoss[0m : 2.29071
[1mStep[0m  [198/339], [94mLoss[0m : 2.56914
[1mStep[0m  [231/339], [94mLoss[0m : 2.22305
[1mStep[0m  [264/339], [94mLoss[0m : 3.26397
[1mStep[0m  [297/339], [94mLoss[0m : 2.48261
[1mStep[0m  [330/339], [94mLoss[0m : 2.81513

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.463, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47524
[1mStep[0m  [33/339], [94mLoss[0m : 2.98123
[1mStep[0m  [66/339], [94mLoss[0m : 2.12405
[1mStep[0m  [99/339], [94mLoss[0m : 2.85728
[1mStep[0m  [132/339], [94mLoss[0m : 2.19336
[1mStep[0m  [165/339], [94mLoss[0m : 2.77511
[1mStep[0m  [198/339], [94mLoss[0m : 2.95258
[1mStep[0m  [231/339], [94mLoss[0m : 2.34872
[1mStep[0m  [264/339], [94mLoss[0m : 2.66119
[1mStep[0m  [297/339], [94mLoss[0m : 2.64385
[1mStep[0m  [330/339], [94mLoss[0m : 3.38339

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.442, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.02422
[1mStep[0m  [33/339], [94mLoss[0m : 1.75332
[1mStep[0m  [66/339], [94mLoss[0m : 2.46440
[1mStep[0m  [99/339], [94mLoss[0m : 2.57567
[1mStep[0m  [132/339], [94mLoss[0m : 2.27783
[1mStep[0m  [165/339], [94mLoss[0m : 3.04004
[1mStep[0m  [198/339], [94mLoss[0m : 2.58476
[1mStep[0m  [231/339], [94mLoss[0m : 2.43877
[1mStep[0m  [264/339], [94mLoss[0m : 3.11878
[1mStep[0m  [297/339], [94mLoss[0m : 2.26918
[1mStep[0m  [330/339], [94mLoss[0m : 2.42867

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.449, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02897
[1mStep[0m  [33/339], [94mLoss[0m : 2.56330
[1mStep[0m  [66/339], [94mLoss[0m : 2.67040
[1mStep[0m  [99/339], [94mLoss[0m : 2.94325
[1mStep[0m  [132/339], [94mLoss[0m : 2.49153
[1mStep[0m  [165/339], [94mLoss[0m : 2.95737
[1mStep[0m  [198/339], [94mLoss[0m : 3.31699
[1mStep[0m  [231/339], [94mLoss[0m : 1.76011
[1mStep[0m  [264/339], [94mLoss[0m : 2.73501
[1mStep[0m  [297/339], [94mLoss[0m : 2.77965
[1mStep[0m  [330/339], [94mLoss[0m : 2.53436

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.410, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17669
[1mStep[0m  [33/339], [94mLoss[0m : 2.66797
[1mStep[0m  [66/339], [94mLoss[0m : 2.72284
[1mStep[0m  [99/339], [94mLoss[0m : 2.24610
[1mStep[0m  [132/339], [94mLoss[0m : 1.68762
[1mStep[0m  [165/339], [94mLoss[0m : 2.17762
[1mStep[0m  [198/339], [94mLoss[0m : 2.35258
[1mStep[0m  [231/339], [94mLoss[0m : 2.53476
[1mStep[0m  [264/339], [94mLoss[0m : 2.44784
[1mStep[0m  [297/339], [94mLoss[0m : 2.08292
[1mStep[0m  [330/339], [94mLoss[0m : 2.14608

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.429, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46452
[1mStep[0m  [33/339], [94mLoss[0m : 2.90231
[1mStep[0m  [66/339], [94mLoss[0m : 2.44679
[1mStep[0m  [99/339], [94mLoss[0m : 2.82211
[1mStep[0m  [132/339], [94mLoss[0m : 2.01186
[1mStep[0m  [165/339], [94mLoss[0m : 2.55218
[1mStep[0m  [198/339], [94mLoss[0m : 2.21421
[1mStep[0m  [231/339], [94mLoss[0m : 2.08556
[1mStep[0m  [264/339], [94mLoss[0m : 2.56732
[1mStep[0m  [297/339], [94mLoss[0m : 2.75878
[1mStep[0m  [330/339], [94mLoss[0m : 3.24802

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.416, [96mlr[0m: 0.01
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13465
[1mStep[0m  [33/339], [94mLoss[0m : 2.25751
[1mStep[0m  [66/339], [94mLoss[0m : 2.83090
[1mStep[0m  [99/339], [94mLoss[0m : 2.47877
[1mStep[0m  [132/339], [94mLoss[0m : 2.19834
[1mStep[0m  [165/339], [94mLoss[0m : 2.95005
[1mStep[0m  [198/339], [94mLoss[0m : 2.45805
[1mStep[0m  [231/339], [94mLoss[0m : 2.83549
[1mStep[0m  [264/339], [94mLoss[0m : 2.37182
[1mStep[0m  [297/339], [94mLoss[0m : 2.49651
[1mStep[0m  [330/339], [94mLoss[0m : 2.45410

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.410, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35150
[1mStep[0m  [33/339], [94mLoss[0m : 3.07484
[1mStep[0m  [66/339], [94mLoss[0m : 2.38954
[1mStep[0m  [99/339], [94mLoss[0m : 2.89230
[1mStep[0m  [132/339], [94mLoss[0m : 1.96651
[1mStep[0m  [165/339], [94mLoss[0m : 2.31264
[1mStep[0m  [198/339], [94mLoss[0m : 3.29489
[1mStep[0m  [231/339], [94mLoss[0m : 2.59577
[1mStep[0m  [264/339], [94mLoss[0m : 2.77715
[1mStep[0m  [297/339], [94mLoss[0m : 2.90864
[1mStep[0m  [330/339], [94mLoss[0m : 2.89376

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.440, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07718
[1mStep[0m  [33/339], [94mLoss[0m : 2.74880
[1mStep[0m  [66/339], [94mLoss[0m : 2.35062
[1mStep[0m  [99/339], [94mLoss[0m : 2.00196
[1mStep[0m  [132/339], [94mLoss[0m : 2.70838
[1mStep[0m  [165/339], [94mLoss[0m : 2.94869
[1mStep[0m  [198/339], [94mLoss[0m : 2.49737
[1mStep[0m  [231/339], [94mLoss[0m : 2.50378
[1mStep[0m  [264/339], [94mLoss[0m : 2.35188
[1mStep[0m  [297/339], [94mLoss[0m : 2.64887
[1mStep[0m  [330/339], [94mLoss[0m : 2.46105

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.392, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.18917
[1mStep[0m  [33/339], [94mLoss[0m : 2.55510
[1mStep[0m  [66/339], [94mLoss[0m : 2.84734
[1mStep[0m  [99/339], [94mLoss[0m : 2.22651
[1mStep[0m  [132/339], [94mLoss[0m : 2.18692
[1mStep[0m  [165/339], [94mLoss[0m : 2.92348
[1mStep[0m  [198/339], [94mLoss[0m : 2.08629
[1mStep[0m  [231/339], [94mLoss[0m : 3.10242
[1mStep[0m  [264/339], [94mLoss[0m : 2.86513
[1mStep[0m  [297/339], [94mLoss[0m : 2.24430
[1mStep[0m  [330/339], [94mLoss[0m : 2.87978

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.403, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50788
[1mStep[0m  [33/339], [94mLoss[0m : 2.38913
[1mStep[0m  [66/339], [94mLoss[0m : 2.21719
[1mStep[0m  [99/339], [94mLoss[0m : 2.89687
[1mStep[0m  [132/339], [94mLoss[0m : 2.41430
[1mStep[0m  [165/339], [94mLoss[0m : 2.37398
[1mStep[0m  [198/339], [94mLoss[0m : 2.15185
[1mStep[0m  [231/339], [94mLoss[0m : 2.74860
[1mStep[0m  [264/339], [94mLoss[0m : 2.08930
[1mStep[0m  [297/339], [94mLoss[0m : 1.93943
[1mStep[0m  [330/339], [94mLoss[0m : 2.33006

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.425, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93908
[1mStep[0m  [33/339], [94mLoss[0m : 2.73039
[1mStep[0m  [66/339], [94mLoss[0m : 3.06700
[1mStep[0m  [99/339], [94mLoss[0m : 2.48210
[1mStep[0m  [132/339], [94mLoss[0m : 2.15864
[1mStep[0m  [165/339], [94mLoss[0m : 2.72260
[1mStep[0m  [198/339], [94mLoss[0m : 2.88354
[1mStep[0m  [231/339], [94mLoss[0m : 2.77873
[1mStep[0m  [264/339], [94mLoss[0m : 2.73958
[1mStep[0m  [297/339], [94mLoss[0m : 2.85607
[1mStep[0m  [330/339], [94mLoss[0m : 1.99816

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.422, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00415
[1mStep[0m  [33/339], [94mLoss[0m : 2.56037
[1mStep[0m  [66/339], [94mLoss[0m : 1.97011
[1mStep[0m  [99/339], [94mLoss[0m : 3.03610
[1mStep[0m  [132/339], [94mLoss[0m : 2.02505
[1mStep[0m  [165/339], [94mLoss[0m : 2.40062
[1mStep[0m  [198/339], [94mLoss[0m : 3.11732
[1mStep[0m  [231/339], [94mLoss[0m : 2.56702
[1mStep[0m  [264/339], [94mLoss[0m : 2.89613
[1mStep[0m  [297/339], [94mLoss[0m : 2.80961
[1mStep[0m  [330/339], [94mLoss[0m : 2.33543

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.449, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25921
[1mStep[0m  [33/339], [94mLoss[0m : 2.75119
[1mStep[0m  [66/339], [94mLoss[0m : 2.57171
[1mStep[0m  [99/339], [94mLoss[0m : 2.54924
[1mStep[0m  [132/339], [94mLoss[0m : 2.32346
[1mStep[0m  [165/339], [94mLoss[0m : 3.26857
[1mStep[0m  [198/339], [94mLoss[0m : 2.55896
[1mStep[0m  [231/339], [94mLoss[0m : 2.84911
[1mStep[0m  [264/339], [94mLoss[0m : 2.56673
[1mStep[0m  [297/339], [94mLoss[0m : 1.94820
[1mStep[0m  [330/339], [94mLoss[0m : 2.11801

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.409, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.72219
[1mStep[0m  [33/339], [94mLoss[0m : 2.97930
[1mStep[0m  [66/339], [94mLoss[0m : 2.19498
[1mStep[0m  [99/339], [94mLoss[0m : 2.82052
[1mStep[0m  [132/339], [94mLoss[0m : 2.47577
[1mStep[0m  [165/339], [94mLoss[0m : 2.38862
[1mStep[0m  [198/339], [94mLoss[0m : 1.75290
[1mStep[0m  [231/339], [94mLoss[0m : 2.44859
[1mStep[0m  [264/339], [94mLoss[0m : 2.11554
[1mStep[0m  [297/339], [94mLoss[0m : 2.23688
[1mStep[0m  [330/339], [94mLoss[0m : 2.84314

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.462, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81470
[1mStep[0m  [33/339], [94mLoss[0m : 2.28782
[1mStep[0m  [66/339], [94mLoss[0m : 2.77094
[1mStep[0m  [99/339], [94mLoss[0m : 3.45210
[1mStep[0m  [132/339], [94mLoss[0m : 2.79588
[1mStep[0m  [165/339], [94mLoss[0m : 2.70814
[1mStep[0m  [198/339], [94mLoss[0m : 2.86355
[1mStep[0m  [231/339], [94mLoss[0m : 2.03242
[1mStep[0m  [264/339], [94mLoss[0m : 2.53414
[1mStep[0m  [297/339], [94mLoss[0m : 2.43952
[1mStep[0m  [330/339], [94mLoss[0m : 2.40646

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.457, [96mlr[0m: 0.009000000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.11516
[1mStep[0m  [33/339], [94mLoss[0m : 3.12480
[1mStep[0m  [66/339], [94mLoss[0m : 2.34703
[1mStep[0m  [99/339], [94mLoss[0m : 2.96197
[1mStep[0m  [132/339], [94mLoss[0m : 2.40552
[1mStep[0m  [165/339], [94mLoss[0m : 2.22846
[1mStep[0m  [198/339], [94mLoss[0m : 2.33212
[1mStep[0m  [231/339], [94mLoss[0m : 2.50593
[1mStep[0m  [264/339], [94mLoss[0m : 2.92538
[1mStep[0m  [297/339], [94mLoss[0m : 2.54205
[1mStep[0m  [330/339], [94mLoss[0m : 2.46004

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.454, [96mlr[0m: 0.009000000000000001
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 2 - Evaluation MAE:  2.3973036118313273
MAE score P1       2.332342
MAE score P2       2.397304
loss               2.475186
learning_rate          0.01
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.9
weight_decay           0.01
Name: 15, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
