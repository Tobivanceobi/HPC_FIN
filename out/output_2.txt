no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  2
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.14886
[1mStep[0m  [4/42], [94mLoss[0m : 10.77842
[1mStep[0m  [8/42], [94mLoss[0m : 10.80040
[1mStep[0m  [12/42], [94mLoss[0m : 10.85955
[1mStep[0m  [16/42], [94mLoss[0m : 10.85389
[1mStep[0m  [20/42], [94mLoss[0m : 11.23703
[1mStep[0m  [24/42], [94mLoss[0m : 11.10439
[1mStep[0m  [28/42], [94mLoss[0m : 10.98098
[1mStep[0m  [32/42], [94mLoss[0m : 11.23938
[1mStep[0m  [36/42], [94mLoss[0m : 10.97577
[1mStep[0m  [40/42], [94mLoss[0m : 10.54851

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.959, [92mTest[0m: 11.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95928
[1mStep[0m  [4/42], [94mLoss[0m : 11.02774
[1mStep[0m  [8/42], [94mLoss[0m : 10.88586
[1mStep[0m  [12/42], [94mLoss[0m : 10.99393
[1mStep[0m  [16/42], [94mLoss[0m : 10.51716
[1mStep[0m  [20/42], [94mLoss[0m : 10.72927
[1mStep[0m  [24/42], [94mLoss[0m : 10.74377
[1mStep[0m  [28/42], [94mLoss[0m : 10.53458
[1mStep[0m  [32/42], [94mLoss[0m : 11.45419
[1mStep[0m  [36/42], [94mLoss[0m : 10.96968
[1mStep[0m  [40/42], [94mLoss[0m : 10.73656

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.909, [92mTest[0m: 10.958, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.77987
[1mStep[0m  [4/42], [94mLoss[0m : 10.98351
[1mStep[0m  [8/42], [94mLoss[0m : 10.82436
[1mStep[0m  [12/42], [94mLoss[0m : 11.04049
[1mStep[0m  [16/42], [94mLoss[0m : 10.91989
[1mStep[0m  [20/42], [94mLoss[0m : 10.31039
[1mStep[0m  [24/42], [94mLoss[0m : 11.20276
[1mStep[0m  [28/42], [94mLoss[0m : 10.89279
[1mStep[0m  [32/42], [94mLoss[0m : 10.75026
[1mStep[0m  [36/42], [94mLoss[0m : 10.93252
[1mStep[0m  [40/42], [94mLoss[0m : 10.83945

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.865, [92mTest[0m: 10.910, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68850
[1mStep[0m  [4/42], [94mLoss[0m : 10.85796
[1mStep[0m  [8/42], [94mLoss[0m : 10.91482
[1mStep[0m  [12/42], [94mLoss[0m : 10.72335
[1mStep[0m  [16/42], [94mLoss[0m : 10.45465
[1mStep[0m  [20/42], [94mLoss[0m : 10.99144
[1mStep[0m  [24/42], [94mLoss[0m : 11.02206
[1mStep[0m  [28/42], [94mLoss[0m : 10.98921
[1mStep[0m  [32/42], [94mLoss[0m : 10.68344
[1mStep[0m  [36/42], [94mLoss[0m : 10.50375
[1mStep[0m  [40/42], [94mLoss[0m : 10.63157

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72724
[1mStep[0m  [4/42], [94mLoss[0m : 10.50538
[1mStep[0m  [8/42], [94mLoss[0m : 10.90174
[1mStep[0m  [12/42], [94mLoss[0m : 10.62675
[1mStep[0m  [16/42], [94mLoss[0m : 11.10463
[1mStep[0m  [20/42], [94mLoss[0m : 10.72574
[1mStep[0m  [24/42], [94mLoss[0m : 10.54896
[1mStep[0m  [28/42], [94mLoss[0m : 10.65029
[1mStep[0m  [32/42], [94mLoss[0m : 10.89061
[1mStep[0m  [36/42], [94mLoss[0m : 10.94533
[1mStep[0m  [40/42], [94mLoss[0m : 10.93065

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.762, [92mTest[0m: 10.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93127
[1mStep[0m  [4/42], [94mLoss[0m : 10.76274
[1mStep[0m  [8/42], [94mLoss[0m : 10.63788
[1mStep[0m  [12/42], [94mLoss[0m : 10.86382
[1mStep[0m  [16/42], [94mLoss[0m : 10.40416
[1mStep[0m  [20/42], [94mLoss[0m : 10.90406
[1mStep[0m  [24/42], [94mLoss[0m : 10.69790
[1mStep[0m  [28/42], [94mLoss[0m : 10.74744
[1mStep[0m  [32/42], [94mLoss[0m : 10.83853
[1mStep[0m  [36/42], [94mLoss[0m : 10.86574
[1mStep[0m  [40/42], [94mLoss[0m : 10.40856

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.705, [92mTest[0m: 10.728, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.88126
[1mStep[0m  [4/42], [94mLoss[0m : 10.68148
[1mStep[0m  [8/42], [94mLoss[0m : 10.82281
[1mStep[0m  [12/42], [94mLoss[0m : 10.66851
[1mStep[0m  [16/42], [94mLoss[0m : 10.97451
[1mStep[0m  [20/42], [94mLoss[0m : 10.43741
[1mStep[0m  [24/42], [94mLoss[0m : 10.47323
[1mStep[0m  [28/42], [94mLoss[0m : 10.65827
[1mStep[0m  [32/42], [94mLoss[0m : 10.74710
[1mStep[0m  [36/42], [94mLoss[0m : 10.73613
[1mStep[0m  [40/42], [94mLoss[0m : 10.75013

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83923
[1mStep[0m  [4/42], [94mLoss[0m : 10.40605
[1mStep[0m  [8/42], [94mLoss[0m : 10.43605
[1mStep[0m  [12/42], [94mLoss[0m : 10.30395
[1mStep[0m  [16/42], [94mLoss[0m : 10.33706
[1mStep[0m  [20/42], [94mLoss[0m : 10.45051
[1mStep[0m  [24/42], [94mLoss[0m : 10.79154
[1mStep[0m  [28/42], [94mLoss[0m : 10.52950
[1mStep[0m  [32/42], [94mLoss[0m : 10.86360
[1mStep[0m  [36/42], [94mLoss[0m : 10.76009
[1mStep[0m  [40/42], [94mLoss[0m : 10.41399

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.606, [92mTest[0m: 10.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66784
[1mStep[0m  [4/42], [94mLoss[0m : 10.47186
[1mStep[0m  [8/42], [94mLoss[0m : 10.05151
[1mStep[0m  [12/42], [94mLoss[0m : 10.35043
[1mStep[0m  [16/42], [94mLoss[0m : 10.22364
[1mStep[0m  [20/42], [94mLoss[0m : 10.43506
[1mStep[0m  [24/42], [94mLoss[0m : 10.44515
[1mStep[0m  [28/42], [94mLoss[0m : 10.93332
[1mStep[0m  [32/42], [94mLoss[0m : 10.48164
[1mStep[0m  [36/42], [94mLoss[0m : 10.76203
[1mStep[0m  [40/42], [94mLoss[0m : 10.68228

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.554, [92mTest[0m: 10.538, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64385
[1mStep[0m  [4/42], [94mLoss[0m : 10.49400
[1mStep[0m  [8/42], [94mLoss[0m : 10.31431
[1mStep[0m  [12/42], [94mLoss[0m : 10.98713
[1mStep[0m  [16/42], [94mLoss[0m : 10.44500
[1mStep[0m  [20/42], [94mLoss[0m : 10.34261
[1mStep[0m  [24/42], [94mLoss[0m : 10.60029
[1mStep[0m  [28/42], [94mLoss[0m : 10.64419
[1mStep[0m  [32/42], [94mLoss[0m : 10.59218
[1mStep[0m  [36/42], [94mLoss[0m : 10.18012
[1mStep[0m  [40/42], [94mLoss[0m : 10.31413

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.513, [92mTest[0m: 10.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54778
[1mStep[0m  [4/42], [94mLoss[0m : 10.45470
[1mStep[0m  [8/42], [94mLoss[0m : 10.39700
[1mStep[0m  [12/42], [94mLoss[0m : 10.51066
[1mStep[0m  [16/42], [94mLoss[0m : 10.15458
[1mStep[0m  [20/42], [94mLoss[0m : 10.52315
[1mStep[0m  [24/42], [94mLoss[0m : 10.53786
[1mStep[0m  [28/42], [94mLoss[0m : 10.36984
[1mStep[0m  [32/42], [94mLoss[0m : 10.12010
[1mStep[0m  [36/42], [94mLoss[0m : 10.14920
[1mStep[0m  [40/42], [94mLoss[0m : 10.21013

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.451, [92mTest[0m: 10.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.31694
[1mStep[0m  [4/42], [94mLoss[0m : 10.22753
[1mStep[0m  [8/42], [94mLoss[0m : 10.44250
[1mStep[0m  [12/42], [94mLoss[0m : 10.72453
[1mStep[0m  [16/42], [94mLoss[0m : 9.94226
[1mStep[0m  [20/42], [94mLoss[0m : 10.66010
[1mStep[0m  [24/42], [94mLoss[0m : 10.16251
[1mStep[0m  [28/42], [94mLoss[0m : 10.91122
[1mStep[0m  [32/42], [94mLoss[0m : 10.53340
[1mStep[0m  [36/42], [94mLoss[0m : 10.16879
[1mStep[0m  [40/42], [94mLoss[0m : 10.36436

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.404, [92mTest[0m: 10.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46812
[1mStep[0m  [4/42], [94mLoss[0m : 10.46646
[1mStep[0m  [8/42], [94mLoss[0m : 10.26988
[1mStep[0m  [12/42], [94mLoss[0m : 10.26428
[1mStep[0m  [16/42], [94mLoss[0m : 10.51453
[1mStep[0m  [20/42], [94mLoss[0m : 10.54365
[1mStep[0m  [24/42], [94mLoss[0m : 10.22685
[1mStep[0m  [28/42], [94mLoss[0m : 10.23890
[1mStep[0m  [32/42], [94mLoss[0m : 10.77267
[1mStep[0m  [36/42], [94mLoss[0m : 10.29917
[1mStep[0m  [40/42], [94mLoss[0m : 10.11182

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.357, [92mTest[0m: 10.298, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65484
[1mStep[0m  [4/42], [94mLoss[0m : 10.32351
[1mStep[0m  [8/42], [94mLoss[0m : 10.22656
[1mStep[0m  [12/42], [94mLoss[0m : 10.38923
[1mStep[0m  [16/42], [94mLoss[0m : 10.43720
[1mStep[0m  [20/42], [94mLoss[0m : 10.27222
[1mStep[0m  [24/42], [94mLoss[0m : 10.50814
[1mStep[0m  [28/42], [94mLoss[0m : 10.47310
[1mStep[0m  [32/42], [94mLoss[0m : 10.10511
[1mStep[0m  [36/42], [94mLoss[0m : 10.05655
[1mStep[0m  [40/42], [94mLoss[0m : 10.05807

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.299, [92mTest[0m: 10.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.12794
[1mStep[0m  [4/42], [94mLoss[0m : 10.27704
[1mStep[0m  [8/42], [94mLoss[0m : 10.28656
[1mStep[0m  [12/42], [94mLoss[0m : 10.13140
[1mStep[0m  [16/42], [94mLoss[0m : 10.31776
[1mStep[0m  [20/42], [94mLoss[0m : 10.19854
[1mStep[0m  [24/42], [94mLoss[0m : 10.19061
[1mStep[0m  [28/42], [94mLoss[0m : 10.16799
[1mStep[0m  [32/42], [94mLoss[0m : 9.97458
[1mStep[0m  [36/42], [94mLoss[0m : 10.23825
[1mStep[0m  [40/42], [94mLoss[0m : 10.59367

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.242, [92mTest[0m: 10.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.29083
[1mStep[0m  [4/42], [94mLoss[0m : 10.16343
[1mStep[0m  [8/42], [94mLoss[0m : 10.13510
[1mStep[0m  [12/42], [94mLoss[0m : 10.23861
[1mStep[0m  [16/42], [94mLoss[0m : 10.37805
[1mStep[0m  [20/42], [94mLoss[0m : 10.27251
[1mStep[0m  [24/42], [94mLoss[0m : 10.39419
[1mStep[0m  [28/42], [94mLoss[0m : 10.27154
[1mStep[0m  [32/42], [94mLoss[0m : 10.29347
[1mStep[0m  [36/42], [94mLoss[0m : 10.27083
[1mStep[0m  [40/42], [94mLoss[0m : 9.96311

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.198, [92mTest[0m: 10.114, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.97891
[1mStep[0m  [4/42], [94mLoss[0m : 10.82668
[1mStep[0m  [8/42], [94mLoss[0m : 10.04988
[1mStep[0m  [12/42], [94mLoss[0m : 10.22516
[1mStep[0m  [16/42], [94mLoss[0m : 10.38004
[1mStep[0m  [20/42], [94mLoss[0m : 10.04117
[1mStep[0m  [24/42], [94mLoss[0m : 10.49323
[1mStep[0m  [28/42], [94mLoss[0m : 9.86268
[1mStep[0m  [32/42], [94mLoss[0m : 10.30226
[1mStep[0m  [36/42], [94mLoss[0m : 9.46080
[1mStep[0m  [40/42], [94mLoss[0m : 10.17970

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.142, [92mTest[0m: 10.023, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26304
[1mStep[0m  [4/42], [94mLoss[0m : 10.20771
[1mStep[0m  [8/42], [94mLoss[0m : 10.18167
[1mStep[0m  [12/42], [94mLoss[0m : 9.91331
[1mStep[0m  [16/42], [94mLoss[0m : 10.11563
[1mStep[0m  [20/42], [94mLoss[0m : 10.23470
[1mStep[0m  [24/42], [94mLoss[0m : 10.04135
[1mStep[0m  [28/42], [94mLoss[0m : 10.05363
[1mStep[0m  [32/42], [94mLoss[0m : 10.49143
[1mStep[0m  [36/42], [94mLoss[0m : 10.61354
[1mStep[0m  [40/42], [94mLoss[0m : 9.60671

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.089, [92mTest[0m: 9.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44465
[1mStep[0m  [4/42], [94mLoss[0m : 9.81152
[1mStep[0m  [8/42], [94mLoss[0m : 10.02423
[1mStep[0m  [12/42], [94mLoss[0m : 9.85007
[1mStep[0m  [16/42], [94mLoss[0m : 9.81760
[1mStep[0m  [20/42], [94mLoss[0m : 9.89320
[1mStep[0m  [24/42], [94mLoss[0m : 9.97069
[1mStep[0m  [28/42], [94mLoss[0m : 9.61405
[1mStep[0m  [32/42], [94mLoss[0m : 10.38861
[1mStep[0m  [36/42], [94mLoss[0m : 10.01866
[1mStep[0m  [40/42], [94mLoss[0m : 10.22405

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.033, [92mTest[0m: 9.920, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.92690
[1mStep[0m  [4/42], [94mLoss[0m : 10.17947
[1mStep[0m  [8/42], [94mLoss[0m : 9.94674
[1mStep[0m  [12/42], [94mLoss[0m : 9.91976
[1mStep[0m  [16/42], [94mLoss[0m : 10.08929
[1mStep[0m  [20/42], [94mLoss[0m : 10.02877
[1mStep[0m  [24/42], [94mLoss[0m : 9.71028
[1mStep[0m  [28/42], [94mLoss[0m : 10.11727
[1mStep[0m  [32/42], [94mLoss[0m : 10.45922
[1mStep[0m  [36/42], [94mLoss[0m : 9.97125
[1mStep[0m  [40/42], [94mLoss[0m : 9.75274

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.980, [92mTest[0m: 9.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.18071
[1mStep[0m  [4/42], [94mLoss[0m : 10.03657
[1mStep[0m  [8/42], [94mLoss[0m : 9.72530
[1mStep[0m  [12/42], [94mLoss[0m : 10.01635
[1mStep[0m  [16/42], [94mLoss[0m : 9.65096
[1mStep[0m  [20/42], [94mLoss[0m : 9.87024
[1mStep[0m  [24/42], [94mLoss[0m : 9.94394
[1mStep[0m  [28/42], [94mLoss[0m : 10.07765
[1mStep[0m  [32/42], [94mLoss[0m : 10.40762
[1mStep[0m  [36/42], [94mLoss[0m : 9.53219
[1mStep[0m  [40/42], [94mLoss[0m : 9.62024

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.934, [92mTest[0m: 9.770, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.13135
[1mStep[0m  [4/42], [94mLoss[0m : 9.94877
[1mStep[0m  [8/42], [94mLoss[0m : 9.60817
[1mStep[0m  [12/42], [94mLoss[0m : 9.56949
[1mStep[0m  [16/42], [94mLoss[0m : 10.17368
[1mStep[0m  [20/42], [94mLoss[0m : 9.92009
[1mStep[0m  [24/42], [94mLoss[0m : 9.74101
[1mStep[0m  [28/42], [94mLoss[0m : 9.61256
[1mStep[0m  [32/42], [94mLoss[0m : 10.21307
[1mStep[0m  [36/42], [94mLoss[0m : 10.13322
[1mStep[0m  [40/42], [94mLoss[0m : 9.87961

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.870, [92mTest[0m: 9.692, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.96108
[1mStep[0m  [4/42], [94mLoss[0m : 9.98815
[1mStep[0m  [8/42], [94mLoss[0m : 9.99281
[1mStep[0m  [12/42], [94mLoss[0m : 9.52489
[1mStep[0m  [16/42], [94mLoss[0m : 9.32711
[1mStep[0m  [20/42], [94mLoss[0m : 9.89324
[1mStep[0m  [24/42], [94mLoss[0m : 9.75704
[1mStep[0m  [28/42], [94mLoss[0m : 9.67664
[1mStep[0m  [32/42], [94mLoss[0m : 10.00811
[1mStep[0m  [36/42], [94mLoss[0m : 10.06447
[1mStep[0m  [40/42], [94mLoss[0m : 9.46191

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.822, [92mTest[0m: 9.640, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.38715
[1mStep[0m  [4/42], [94mLoss[0m : 10.02058
[1mStep[0m  [8/42], [94mLoss[0m : 9.82681
[1mStep[0m  [12/42], [94mLoss[0m : 9.88414
[1mStep[0m  [16/42], [94mLoss[0m : 10.08613
[1mStep[0m  [20/42], [94mLoss[0m : 9.88467
[1mStep[0m  [24/42], [94mLoss[0m : 9.77698
[1mStep[0m  [28/42], [94mLoss[0m : 9.64132
[1mStep[0m  [32/42], [94mLoss[0m : 9.80521
[1mStep[0m  [36/42], [94mLoss[0m : 9.87467
[1mStep[0m  [40/42], [94mLoss[0m : 9.71570

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.774, [92mTest[0m: 9.591, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.21773
[1mStep[0m  [4/42], [94mLoss[0m : 9.77859
[1mStep[0m  [8/42], [94mLoss[0m : 9.64065
[1mStep[0m  [12/42], [94mLoss[0m : 9.85203
[1mStep[0m  [16/42], [94mLoss[0m : 9.51581
[1mStep[0m  [20/42], [94mLoss[0m : 9.33973
[1mStep[0m  [24/42], [94mLoss[0m : 9.46435
[1mStep[0m  [28/42], [94mLoss[0m : 9.81695
[1mStep[0m  [32/42], [94mLoss[0m : 9.84052
[1mStep[0m  [36/42], [94mLoss[0m : 9.87293
[1mStep[0m  [40/42], [94mLoss[0m : 10.03867

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.712, [92mTest[0m: 9.515, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89224
[1mStep[0m  [4/42], [94mLoss[0m : 9.94701
[1mStep[0m  [8/42], [94mLoss[0m : 9.56583
[1mStep[0m  [12/42], [94mLoss[0m : 9.76952
[1mStep[0m  [16/42], [94mLoss[0m : 10.00143
[1mStep[0m  [20/42], [94mLoss[0m : 9.50064
[1mStep[0m  [24/42], [94mLoss[0m : 9.41945
[1mStep[0m  [28/42], [94mLoss[0m : 9.64650
[1mStep[0m  [32/42], [94mLoss[0m : 9.30487
[1mStep[0m  [36/42], [94mLoss[0m : 9.86725
[1mStep[0m  [40/42], [94mLoss[0m : 9.99980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.671, [92mTest[0m: 9.482, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.58315
[1mStep[0m  [4/42], [94mLoss[0m : 9.93517
[1mStep[0m  [8/42], [94mLoss[0m : 9.63700
[1mStep[0m  [12/42], [94mLoss[0m : 9.43216
[1mStep[0m  [16/42], [94mLoss[0m : 9.51027
[1mStep[0m  [20/42], [94mLoss[0m : 9.69471
[1mStep[0m  [24/42], [94mLoss[0m : 9.33057
[1mStep[0m  [28/42], [94mLoss[0m : 9.34218
[1mStep[0m  [32/42], [94mLoss[0m : 9.86767
[1mStep[0m  [36/42], [94mLoss[0m : 9.88712
[1mStep[0m  [40/42], [94mLoss[0m : 9.70340

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.609, [92mTest[0m: 9.399, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.92065
[1mStep[0m  [4/42], [94mLoss[0m : 9.66718
[1mStep[0m  [8/42], [94mLoss[0m : 9.42178
[1mStep[0m  [12/42], [94mLoss[0m : 9.45368
[1mStep[0m  [16/42], [94mLoss[0m : 9.48011
[1mStep[0m  [20/42], [94mLoss[0m : 9.46608
[1mStep[0m  [24/42], [94mLoss[0m : 9.64698
[1mStep[0m  [28/42], [94mLoss[0m : 9.56832
[1mStep[0m  [32/42], [94mLoss[0m : 9.50998
[1mStep[0m  [36/42], [94mLoss[0m : 9.50530
[1mStep[0m  [40/42], [94mLoss[0m : 9.13164

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.565, [92mTest[0m: 9.337, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.42589
[1mStep[0m  [4/42], [94mLoss[0m : 9.06823
[1mStep[0m  [8/42], [94mLoss[0m : 9.04670
[1mStep[0m  [12/42], [94mLoss[0m : 9.61650
[1mStep[0m  [16/42], [94mLoss[0m : 9.66317
[1mStep[0m  [20/42], [94mLoss[0m : 9.75733
[1mStep[0m  [24/42], [94mLoss[0m : 9.41029
[1mStep[0m  [28/42], [94mLoss[0m : 9.46039
[1mStep[0m  [32/42], [94mLoss[0m : 9.59560
[1mStep[0m  [36/42], [94mLoss[0m : 9.16854
[1mStep[0m  [40/42], [94mLoss[0m : 9.55299

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.502, [92mTest[0m: 9.287, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.53617
[1mStep[0m  [4/42], [94mLoss[0m : 9.35870
[1mStep[0m  [8/42], [94mLoss[0m : 9.33490
[1mStep[0m  [12/42], [94mLoss[0m : 9.74553
[1mStep[0m  [16/42], [94mLoss[0m : 9.34545
[1mStep[0m  [20/42], [94mLoss[0m : 9.17325
[1mStep[0m  [24/42], [94mLoss[0m : 9.49717
[1mStep[0m  [28/42], [94mLoss[0m : 9.11686
[1mStep[0m  [32/42], [94mLoss[0m : 9.60720
[1mStep[0m  [36/42], [94mLoss[0m : 9.67464
[1mStep[0m  [40/42], [94mLoss[0m : 9.42043

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.449, [92mTest[0m: 9.176, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.135
====================================

Phase 1 - Evaluation MAE:  9.135056018829346
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 9.74642
[1mStep[0m  [4/42], [94mLoss[0m : 9.21467
[1mStep[0m  [8/42], [94mLoss[0m : 9.38771
[1mStep[0m  [12/42], [94mLoss[0m : 9.32545
[1mStep[0m  [16/42], [94mLoss[0m : 9.17768
[1mStep[0m  [20/42], [94mLoss[0m : 9.33011
[1mStep[0m  [24/42], [94mLoss[0m : 9.38073
[1mStep[0m  [28/42], [94mLoss[0m : 8.95224
[1mStep[0m  [32/42], [94mLoss[0m : 9.48577
[1mStep[0m  [36/42], [94mLoss[0m : 9.55123
[1mStep[0m  [40/42], [94mLoss[0m : 9.10945

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.404, [92mTest[0m: 9.136, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.18679
[1mStep[0m  [4/42], [94mLoss[0m : 9.30293
[1mStep[0m  [8/42], [94mLoss[0m : 8.76954
[1mStep[0m  [12/42], [94mLoss[0m : 9.08573
[1mStep[0m  [16/42], [94mLoss[0m : 9.48486
[1mStep[0m  [20/42], [94mLoss[0m : 9.19190
[1mStep[0m  [24/42], [94mLoss[0m : 9.39890
[1mStep[0m  [28/42], [94mLoss[0m : 9.32054
[1mStep[0m  [32/42], [94mLoss[0m : 9.33874
[1mStep[0m  [36/42], [94mLoss[0m : 9.76363
[1mStep[0m  [40/42], [94mLoss[0m : 9.17422

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.337, [92mTest[0m: 9.057, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.44462
[1mStep[0m  [4/42], [94mLoss[0m : 9.24698
[1mStep[0m  [8/42], [94mLoss[0m : 9.56040
[1mStep[0m  [12/42], [94mLoss[0m : 9.15267
[1mStep[0m  [16/42], [94mLoss[0m : 9.14201
[1mStep[0m  [20/42], [94mLoss[0m : 9.54796
[1mStep[0m  [24/42], [94mLoss[0m : 8.99405
[1mStep[0m  [28/42], [94mLoss[0m : 9.37428
[1mStep[0m  [32/42], [94mLoss[0m : 9.32370
[1mStep[0m  [36/42], [94mLoss[0m : 9.61613
[1mStep[0m  [40/42], [94mLoss[0m : 9.01132

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.253, [92mTest[0m: 8.938, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.93830
[1mStep[0m  [4/42], [94mLoss[0m : 8.80034
[1mStep[0m  [8/42], [94mLoss[0m : 9.07688
[1mStep[0m  [12/42], [94mLoss[0m : 9.59789
[1mStep[0m  [16/42], [94mLoss[0m : 9.00634
[1mStep[0m  [20/42], [94mLoss[0m : 9.36516
[1mStep[0m  [24/42], [94mLoss[0m : 9.75541
[1mStep[0m  [28/42], [94mLoss[0m : 9.21993
[1mStep[0m  [32/42], [94mLoss[0m : 9.12764
[1mStep[0m  [36/42], [94mLoss[0m : 8.75079
[1mStep[0m  [40/42], [94mLoss[0m : 9.07916

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.185, [92mTest[0m: 8.919, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.37086
[1mStep[0m  [4/42], [94mLoss[0m : 9.15414
[1mStep[0m  [8/42], [94mLoss[0m : 9.34238
[1mStep[0m  [12/42], [94mLoss[0m : 9.15928
[1mStep[0m  [16/42], [94mLoss[0m : 8.76147
[1mStep[0m  [20/42], [94mLoss[0m : 8.85564
[1mStep[0m  [24/42], [94mLoss[0m : 8.98237
[1mStep[0m  [28/42], [94mLoss[0m : 9.34145
[1mStep[0m  [32/42], [94mLoss[0m : 9.15435
[1mStep[0m  [36/42], [94mLoss[0m : 9.30606
[1mStep[0m  [40/42], [94mLoss[0m : 9.11103

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.103, [92mTest[0m: 8.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.38144
[1mStep[0m  [4/42], [94mLoss[0m : 8.96119
[1mStep[0m  [8/42], [94mLoss[0m : 9.32930
[1mStep[0m  [12/42], [94mLoss[0m : 8.96048
[1mStep[0m  [16/42], [94mLoss[0m : 8.85251
[1mStep[0m  [20/42], [94mLoss[0m : 9.22604
[1mStep[0m  [24/42], [94mLoss[0m : 9.49219
[1mStep[0m  [28/42], [94mLoss[0m : 8.63371
[1mStep[0m  [32/42], [94mLoss[0m : 8.73911
[1mStep[0m  [36/42], [94mLoss[0m : 8.94430
[1mStep[0m  [40/42], [94mLoss[0m : 9.07032

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.027, [92mTest[0m: 8.725, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.61309
[1mStep[0m  [4/42], [94mLoss[0m : 8.58518
[1mStep[0m  [8/42], [94mLoss[0m : 8.81858
[1mStep[0m  [12/42], [94mLoss[0m : 8.73478
[1mStep[0m  [16/42], [94mLoss[0m : 9.38273
[1mStep[0m  [20/42], [94mLoss[0m : 9.10275
[1mStep[0m  [24/42], [94mLoss[0m : 8.44885
[1mStep[0m  [28/42], [94mLoss[0m : 8.88032
[1mStep[0m  [32/42], [94mLoss[0m : 8.67159
[1mStep[0m  [36/42], [94mLoss[0m : 9.01540
[1mStep[0m  [40/42], [94mLoss[0m : 8.97312

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.937, [92mTest[0m: 8.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.75648
[1mStep[0m  [4/42], [94mLoss[0m : 8.98937
[1mStep[0m  [8/42], [94mLoss[0m : 8.93847
[1mStep[0m  [12/42], [94mLoss[0m : 9.05759
[1mStep[0m  [16/42], [94mLoss[0m : 8.99182
[1mStep[0m  [20/42], [94mLoss[0m : 8.50613
[1mStep[0m  [24/42], [94mLoss[0m : 9.15090
[1mStep[0m  [28/42], [94mLoss[0m : 9.15388
[1mStep[0m  [32/42], [94mLoss[0m : 8.97621
[1mStep[0m  [36/42], [94mLoss[0m : 8.72092
[1mStep[0m  [40/42], [94mLoss[0m : 8.62566

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.851, [92mTest[0m: 8.626, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.87136
[1mStep[0m  [4/42], [94mLoss[0m : 8.54465
[1mStep[0m  [8/42], [94mLoss[0m : 8.81410
[1mStep[0m  [12/42], [94mLoss[0m : 8.81770
[1mStep[0m  [16/42], [94mLoss[0m : 8.55994
[1mStep[0m  [20/42], [94mLoss[0m : 8.42027
[1mStep[0m  [24/42], [94mLoss[0m : 8.77720
[1mStep[0m  [28/42], [94mLoss[0m : 9.00434
[1mStep[0m  [32/42], [94mLoss[0m : 8.91903
[1mStep[0m  [36/42], [94mLoss[0m : 8.86461
[1mStep[0m  [40/42], [94mLoss[0m : 8.12131

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.767, [92mTest[0m: 8.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.73751
[1mStep[0m  [4/42], [94mLoss[0m : 8.55779
[1mStep[0m  [8/42], [94mLoss[0m : 8.60423
[1mStep[0m  [12/42], [94mLoss[0m : 8.72233
[1mStep[0m  [16/42], [94mLoss[0m : 8.84093
[1mStep[0m  [20/42], [94mLoss[0m : 8.63879
[1mStep[0m  [24/42], [94mLoss[0m : 8.68874
[1mStep[0m  [28/42], [94mLoss[0m : 9.00864
[1mStep[0m  [32/42], [94mLoss[0m : 9.01344
[1mStep[0m  [36/42], [94mLoss[0m : 8.92234
[1mStep[0m  [40/42], [94mLoss[0m : 9.06942

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.684, [92mTest[0m: 8.428, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34633
[1mStep[0m  [4/42], [94mLoss[0m : 8.74780
[1mStep[0m  [8/42], [94mLoss[0m : 8.39550
[1mStep[0m  [12/42], [94mLoss[0m : 8.62765
[1mStep[0m  [16/42], [94mLoss[0m : 8.41746
[1mStep[0m  [20/42], [94mLoss[0m : 8.51240
[1mStep[0m  [24/42], [94mLoss[0m : 8.55627
[1mStep[0m  [28/42], [94mLoss[0m : 8.49718
[1mStep[0m  [32/42], [94mLoss[0m : 8.61635
[1mStep[0m  [36/42], [94mLoss[0m : 8.73983
[1mStep[0m  [40/42], [94mLoss[0m : 8.73490

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.584, [92mTest[0m: 8.320, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27121
[1mStep[0m  [4/42], [94mLoss[0m : 8.38789
[1mStep[0m  [8/42], [94mLoss[0m : 8.83226
[1mStep[0m  [12/42], [94mLoss[0m : 8.22137
[1mStep[0m  [16/42], [94mLoss[0m : 8.77205
[1mStep[0m  [20/42], [94mLoss[0m : 7.96128
[1mStep[0m  [24/42], [94mLoss[0m : 8.81882
[1mStep[0m  [28/42], [94mLoss[0m : 8.86123
[1mStep[0m  [32/42], [94mLoss[0m : 8.08850
[1mStep[0m  [36/42], [94mLoss[0m : 8.32664
[1mStep[0m  [40/42], [94mLoss[0m : 8.44946

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.487, [92mTest[0m: 8.139, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.68060
[1mStep[0m  [4/42], [94mLoss[0m : 8.35192
[1mStep[0m  [8/42], [94mLoss[0m : 8.41813
[1mStep[0m  [12/42], [94mLoss[0m : 8.40141
[1mStep[0m  [16/42], [94mLoss[0m : 8.48001
[1mStep[0m  [20/42], [94mLoss[0m : 8.53914
[1mStep[0m  [24/42], [94mLoss[0m : 8.43782
[1mStep[0m  [28/42], [94mLoss[0m : 8.51986
[1mStep[0m  [32/42], [94mLoss[0m : 8.42336
[1mStep[0m  [36/42], [94mLoss[0m : 7.84286
[1mStep[0m  [40/42], [94mLoss[0m : 7.97661

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.388, [92mTest[0m: 8.034, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.08319
[1mStep[0m  [4/42], [94mLoss[0m : 8.18322
[1mStep[0m  [8/42], [94mLoss[0m : 8.04729
[1mStep[0m  [12/42], [94mLoss[0m : 8.31660
[1mStep[0m  [16/42], [94mLoss[0m : 8.44638
[1mStep[0m  [20/42], [94mLoss[0m : 8.32544
[1mStep[0m  [24/42], [94mLoss[0m : 8.31187
[1mStep[0m  [28/42], [94mLoss[0m : 8.20170
[1mStep[0m  [32/42], [94mLoss[0m : 7.92099
[1mStep[0m  [36/42], [94mLoss[0m : 7.96688
[1mStep[0m  [40/42], [94mLoss[0m : 8.39500

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.271, [92mTest[0m: 7.784, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.69716
[1mStep[0m  [4/42], [94mLoss[0m : 7.98368
[1mStep[0m  [8/42], [94mLoss[0m : 8.66813
[1mStep[0m  [12/42], [94mLoss[0m : 8.24698
[1mStep[0m  [16/42], [94mLoss[0m : 7.91613
[1mStep[0m  [20/42], [94mLoss[0m : 8.27258
[1mStep[0m  [24/42], [94mLoss[0m : 8.36733
[1mStep[0m  [28/42], [94mLoss[0m : 8.46010
[1mStep[0m  [32/42], [94mLoss[0m : 8.26761
[1mStep[0m  [36/42], [94mLoss[0m : 8.16849
[1mStep[0m  [40/42], [94mLoss[0m : 8.25758

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.175, [92mTest[0m: 7.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.28985
[1mStep[0m  [4/42], [94mLoss[0m : 8.00718
[1mStep[0m  [8/42], [94mLoss[0m : 7.98947
[1mStep[0m  [12/42], [94mLoss[0m : 8.24952
[1mStep[0m  [16/42], [94mLoss[0m : 7.96203
[1mStep[0m  [20/42], [94mLoss[0m : 7.91030
[1mStep[0m  [24/42], [94mLoss[0m : 8.04957
[1mStep[0m  [28/42], [94mLoss[0m : 8.28158
[1mStep[0m  [32/42], [94mLoss[0m : 8.10500
[1mStep[0m  [36/42], [94mLoss[0m : 7.77929
[1mStep[0m  [40/42], [94mLoss[0m : 8.44515

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.072, [92mTest[0m: 7.719, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.00047
[1mStep[0m  [4/42], [94mLoss[0m : 7.89109
[1mStep[0m  [8/42], [94mLoss[0m : 8.14502
[1mStep[0m  [12/42], [94mLoss[0m : 7.96097
[1mStep[0m  [16/42], [94mLoss[0m : 7.77873
[1mStep[0m  [20/42], [94mLoss[0m : 8.13437
[1mStep[0m  [24/42], [94mLoss[0m : 8.13416
[1mStep[0m  [28/42], [94mLoss[0m : 7.99284
[1mStep[0m  [32/42], [94mLoss[0m : 8.12122
[1mStep[0m  [36/42], [94mLoss[0m : 7.61804
[1mStep[0m  [40/42], [94mLoss[0m : 7.73253

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.928, [92mTest[0m: 7.703, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.49596
[1mStep[0m  [4/42], [94mLoss[0m : 7.56905
[1mStep[0m  [8/42], [94mLoss[0m : 8.01459
[1mStep[0m  [12/42], [94mLoss[0m : 8.14543
[1mStep[0m  [16/42], [94mLoss[0m : 8.00408
[1mStep[0m  [20/42], [94mLoss[0m : 8.10384
[1mStep[0m  [24/42], [94mLoss[0m : 7.77933
[1mStep[0m  [28/42], [94mLoss[0m : 7.47772
[1mStep[0m  [32/42], [94mLoss[0m : 7.56972
[1mStep[0m  [36/42], [94mLoss[0m : 7.55718
[1mStep[0m  [40/42], [94mLoss[0m : 8.01760

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.824, [92mTest[0m: 7.684, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.79246
[1mStep[0m  [4/42], [94mLoss[0m : 7.89720
[1mStep[0m  [8/42], [94mLoss[0m : 7.36882
[1mStep[0m  [12/42], [94mLoss[0m : 7.76575
[1mStep[0m  [16/42], [94mLoss[0m : 7.90474
[1mStep[0m  [20/42], [94mLoss[0m : 7.79548
[1mStep[0m  [24/42], [94mLoss[0m : 7.92833
[1mStep[0m  [28/42], [94mLoss[0m : 7.55974
[1mStep[0m  [32/42], [94mLoss[0m : 7.49498
[1mStep[0m  [36/42], [94mLoss[0m : 7.68853
[1mStep[0m  [40/42], [94mLoss[0m : 7.67868

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.704, [92mTest[0m: 7.399, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.89553
[1mStep[0m  [4/42], [94mLoss[0m : 7.86521
[1mStep[0m  [8/42], [94mLoss[0m : 7.59772
[1mStep[0m  [12/42], [94mLoss[0m : 7.95143
[1mStep[0m  [16/42], [94mLoss[0m : 8.01385
[1mStep[0m  [20/42], [94mLoss[0m : 7.64874
[1mStep[0m  [24/42], [94mLoss[0m : 7.37233
[1mStep[0m  [28/42], [94mLoss[0m : 7.91833
[1mStep[0m  [32/42], [94mLoss[0m : 7.53619
[1mStep[0m  [36/42], [94mLoss[0m : 7.65939
[1mStep[0m  [40/42], [94mLoss[0m : 7.28111

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.566, [92mTest[0m: 7.049, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.18639
[1mStep[0m  [4/42], [94mLoss[0m : 7.31963
[1mStep[0m  [8/42], [94mLoss[0m : 7.19640
[1mStep[0m  [12/42], [94mLoss[0m : 7.12149
[1mStep[0m  [16/42], [94mLoss[0m : 7.78609
[1mStep[0m  [20/42], [94mLoss[0m : 7.16277
[1mStep[0m  [24/42], [94mLoss[0m : 7.42988
[1mStep[0m  [28/42], [94mLoss[0m : 7.21249
[1mStep[0m  [32/42], [94mLoss[0m : 7.19919
[1mStep[0m  [36/42], [94mLoss[0m : 7.33474
[1mStep[0m  [40/42], [94mLoss[0m : 7.30594

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.430, [92mTest[0m: 6.842, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.04524
[1mStep[0m  [4/42], [94mLoss[0m : 7.20624
[1mStep[0m  [8/42], [94mLoss[0m : 7.34905
[1mStep[0m  [12/42], [94mLoss[0m : 7.56426
[1mStep[0m  [16/42], [94mLoss[0m : 7.36344
[1mStep[0m  [20/42], [94mLoss[0m : 7.30453
[1mStep[0m  [24/42], [94mLoss[0m : 7.42925
[1mStep[0m  [28/42], [94mLoss[0m : 7.35044
[1mStep[0m  [32/42], [94mLoss[0m : 7.61241
[1mStep[0m  [36/42], [94mLoss[0m : 7.40846
[1mStep[0m  [40/42], [94mLoss[0m : 7.54021

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.311, [92mTest[0m: 6.721, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.08588
[1mStep[0m  [4/42], [94mLoss[0m : 6.97458
[1mStep[0m  [8/42], [94mLoss[0m : 7.06464
[1mStep[0m  [12/42], [94mLoss[0m : 7.33935
[1mStep[0m  [16/42], [94mLoss[0m : 7.17266
[1mStep[0m  [20/42], [94mLoss[0m : 6.92734
[1mStep[0m  [24/42], [94mLoss[0m : 7.44491
[1mStep[0m  [28/42], [94mLoss[0m : 7.48629
[1mStep[0m  [32/42], [94mLoss[0m : 7.39384
[1mStep[0m  [36/42], [94mLoss[0m : 7.27644
[1mStep[0m  [40/42], [94mLoss[0m : 7.38210

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.203, [92mTest[0m: 7.065, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.35527
[1mStep[0m  [4/42], [94mLoss[0m : 6.96015
[1mStep[0m  [8/42], [94mLoss[0m : 6.75774
[1mStep[0m  [12/42], [94mLoss[0m : 6.66897
[1mStep[0m  [16/42], [94mLoss[0m : 7.10687
[1mStep[0m  [20/42], [94mLoss[0m : 7.15359
[1mStep[0m  [24/42], [94mLoss[0m : 7.00220
[1mStep[0m  [28/42], [94mLoss[0m : 7.22461
[1mStep[0m  [32/42], [94mLoss[0m : 7.14078
[1mStep[0m  [36/42], [94mLoss[0m : 6.92707
[1mStep[0m  [40/42], [94mLoss[0m : 7.40125

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.052, [92mTest[0m: 6.650, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.14628
[1mStep[0m  [4/42], [94mLoss[0m : 6.75161
[1mStep[0m  [8/42], [94mLoss[0m : 7.10757
[1mStep[0m  [12/42], [94mLoss[0m : 6.98055
[1mStep[0m  [16/42], [94mLoss[0m : 7.03687
[1mStep[0m  [20/42], [94mLoss[0m : 6.69817
[1mStep[0m  [24/42], [94mLoss[0m : 7.00858
[1mStep[0m  [28/42], [94mLoss[0m : 6.93712
[1mStep[0m  [32/42], [94mLoss[0m : 6.94500
[1mStep[0m  [36/42], [94mLoss[0m : 6.88188
[1mStep[0m  [40/42], [94mLoss[0m : 6.66045

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.929, [92mTest[0m: 7.404, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.72409
[1mStep[0m  [4/42], [94mLoss[0m : 6.71218
[1mStep[0m  [8/42], [94mLoss[0m : 6.76634
[1mStep[0m  [12/42], [94mLoss[0m : 6.89981
[1mStep[0m  [16/42], [94mLoss[0m : 7.09396
[1mStep[0m  [20/42], [94mLoss[0m : 6.74817
[1mStep[0m  [24/42], [94mLoss[0m : 6.66259
[1mStep[0m  [28/42], [94mLoss[0m : 6.94469
[1mStep[0m  [32/42], [94mLoss[0m : 6.88898
[1mStep[0m  [36/42], [94mLoss[0m : 6.81970
[1mStep[0m  [40/42], [94mLoss[0m : 6.67675

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.798, [92mTest[0m: 6.676, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.61577
[1mStep[0m  [4/42], [94mLoss[0m : 6.87702
[1mStep[0m  [8/42], [94mLoss[0m : 6.95135
[1mStep[0m  [12/42], [94mLoss[0m : 6.69622
[1mStep[0m  [16/42], [94mLoss[0m : 6.50911
[1mStep[0m  [20/42], [94mLoss[0m : 6.32963
[1mStep[0m  [24/42], [94mLoss[0m : 6.63527
[1mStep[0m  [28/42], [94mLoss[0m : 6.79995
[1mStep[0m  [32/42], [94mLoss[0m : 6.12429
[1mStep[0m  [36/42], [94mLoss[0m : 6.62545
[1mStep[0m  [40/42], [94mLoss[0m : 6.59952

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.659, [92mTest[0m: 5.979, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.80070
[1mStep[0m  [4/42], [94mLoss[0m : 6.62878
[1mStep[0m  [8/42], [94mLoss[0m : 6.85628
[1mStep[0m  [12/42], [94mLoss[0m : 6.71708
[1mStep[0m  [16/42], [94mLoss[0m : 6.52368
[1mStep[0m  [20/42], [94mLoss[0m : 6.20575
[1mStep[0m  [24/42], [94mLoss[0m : 6.44873
[1mStep[0m  [28/42], [94mLoss[0m : 6.20889
[1mStep[0m  [32/42], [94mLoss[0m : 6.35417
[1mStep[0m  [36/42], [94mLoss[0m : 6.06854
[1mStep[0m  [40/42], [94mLoss[0m : 6.39585

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.534, [92mTest[0m: 5.753, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.40455
[1mStep[0m  [4/42], [94mLoss[0m : 6.22917
[1mStep[0m  [8/42], [94mLoss[0m : 6.30591
[1mStep[0m  [12/42], [94mLoss[0m : 6.23987
[1mStep[0m  [16/42], [94mLoss[0m : 6.27320
[1mStep[0m  [20/42], [94mLoss[0m : 6.30171
[1mStep[0m  [24/42], [94mLoss[0m : 6.26619
[1mStep[0m  [28/42], [94mLoss[0m : 6.56841
[1mStep[0m  [32/42], [94mLoss[0m : 6.22323
[1mStep[0m  [36/42], [94mLoss[0m : 6.55166
[1mStep[0m  [40/42], [94mLoss[0m : 6.54610

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.392, [92mTest[0m: 6.206, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.41240
[1mStep[0m  [4/42], [94mLoss[0m : 6.13819
[1mStep[0m  [8/42], [94mLoss[0m : 6.02787
[1mStep[0m  [12/42], [94mLoss[0m : 6.29822
[1mStep[0m  [16/42], [94mLoss[0m : 6.40415
[1mStep[0m  [20/42], [94mLoss[0m : 6.48345
[1mStep[0m  [24/42], [94mLoss[0m : 6.44408
[1mStep[0m  [28/42], [94mLoss[0m : 6.29343
[1mStep[0m  [32/42], [94mLoss[0m : 6.21328
[1mStep[0m  [36/42], [94mLoss[0m : 6.05769
[1mStep[0m  [40/42], [94mLoss[0m : 6.22536

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.264, [92mTest[0m: 5.712, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.493
====================================

Phase 2 - Evaluation MAE:  5.492551667349679
MAE score P1       9.135056
MAE score P2       5.492552
loss               6.263828
learning_rate        0.0001
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.29097
[1mStep[0m  [4/42], [94mLoss[0m : 10.79070
[1mStep[0m  [8/42], [94mLoss[0m : 10.82632
[1mStep[0m  [12/42], [94mLoss[0m : 10.83553
[1mStep[0m  [16/42], [94mLoss[0m : 10.74669
[1mStep[0m  [20/42], [94mLoss[0m : 10.93804
[1mStep[0m  [24/42], [94mLoss[0m : 10.87062
[1mStep[0m  [28/42], [94mLoss[0m : 11.01699
[1mStep[0m  [32/42], [94mLoss[0m : 11.14258
[1mStep[0m  [36/42], [94mLoss[0m : 11.16607
[1mStep[0m  [40/42], [94mLoss[0m : 11.00310

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.884, [92mTest[0m: 10.881, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.89340
[1mStep[0m  [4/42], [94mLoss[0m : 10.77799
[1mStep[0m  [8/42], [94mLoss[0m : 11.04500
[1mStep[0m  [12/42], [94mLoss[0m : 10.92506
[1mStep[0m  [16/42], [94mLoss[0m : 10.91495
[1mStep[0m  [20/42], [94mLoss[0m : 11.13556
[1mStep[0m  [24/42], [94mLoss[0m : 10.55239
[1mStep[0m  [28/42], [94mLoss[0m : 10.87190
[1mStep[0m  [32/42], [94mLoss[0m : 10.64488
[1mStep[0m  [36/42], [94mLoss[0m : 11.21749
[1mStep[0m  [40/42], [94mLoss[0m : 10.53345

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.877, [92mTest[0m: 10.855, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.27366
[1mStep[0m  [4/42], [94mLoss[0m : 10.97961
[1mStep[0m  [8/42], [94mLoss[0m : 10.86272
[1mStep[0m  [12/42], [94mLoss[0m : 10.82567
[1mStep[0m  [16/42], [94mLoss[0m : 10.50784
[1mStep[0m  [20/42], [94mLoss[0m : 10.45154
[1mStep[0m  [24/42], [94mLoss[0m : 10.93779
[1mStep[0m  [28/42], [94mLoss[0m : 10.63904
[1mStep[0m  [32/42], [94mLoss[0m : 10.95066
[1mStep[0m  [36/42], [94mLoss[0m : 10.96494
[1mStep[0m  [40/42], [94mLoss[0m : 11.21676

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.878, [92mTest[0m: 10.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64837
[1mStep[0m  [4/42], [94mLoss[0m : 11.22951
[1mStep[0m  [8/42], [94mLoss[0m : 10.83832
[1mStep[0m  [12/42], [94mLoss[0m : 11.13727
[1mStep[0m  [16/42], [94mLoss[0m : 10.62876
[1mStep[0m  [20/42], [94mLoss[0m : 10.87721
[1mStep[0m  [24/42], [94mLoss[0m : 10.75627
[1mStep[0m  [28/42], [94mLoss[0m : 11.10610
[1mStep[0m  [32/42], [94mLoss[0m : 10.99218
[1mStep[0m  [36/42], [94mLoss[0m : 10.76828
[1mStep[0m  [40/42], [94mLoss[0m : 10.71655

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.859, [92mTest[0m: 10.827, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.80294
[1mStep[0m  [4/42], [94mLoss[0m : 10.70114
[1mStep[0m  [8/42], [94mLoss[0m : 10.56056
[1mStep[0m  [12/42], [94mLoss[0m : 10.72788
[1mStep[0m  [16/42], [94mLoss[0m : 10.88287
[1mStep[0m  [20/42], [94mLoss[0m : 10.73635
[1mStep[0m  [24/42], [94mLoss[0m : 10.90298
[1mStep[0m  [28/42], [94mLoss[0m : 10.99397
[1mStep[0m  [32/42], [94mLoss[0m : 10.81456
[1mStep[0m  [36/42], [94mLoss[0m : 10.99935
[1mStep[0m  [40/42], [94mLoss[0m : 10.66139

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.851, [92mTest[0m: 10.818, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.73235
[1mStep[0m  [4/42], [94mLoss[0m : 11.07841
[1mStep[0m  [8/42], [94mLoss[0m : 10.64846
[1mStep[0m  [12/42], [94mLoss[0m : 11.03841
[1mStep[0m  [16/42], [94mLoss[0m : 10.72557
[1mStep[0m  [20/42], [94mLoss[0m : 10.73817
[1mStep[0m  [24/42], [94mLoss[0m : 10.55043
[1mStep[0m  [28/42], [94mLoss[0m : 11.06888
[1mStep[0m  [32/42], [94mLoss[0m : 10.98488
[1mStep[0m  [36/42], [94mLoss[0m : 11.10704
[1mStep[0m  [40/42], [94mLoss[0m : 10.52088

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.854, [92mTest[0m: 10.808, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74078
[1mStep[0m  [4/42], [94mLoss[0m : 10.70468
[1mStep[0m  [8/42], [94mLoss[0m : 10.78222
[1mStep[0m  [12/42], [94mLoss[0m : 11.00231
[1mStep[0m  [16/42], [94mLoss[0m : 11.14746
[1mStep[0m  [20/42], [94mLoss[0m : 11.18316
[1mStep[0m  [24/42], [94mLoss[0m : 10.89886
[1mStep[0m  [28/42], [94mLoss[0m : 11.03209
[1mStep[0m  [32/42], [94mLoss[0m : 10.94958
[1mStep[0m  [36/42], [94mLoss[0m : 10.96170
[1mStep[0m  [40/42], [94mLoss[0m : 11.25727

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.839, [92mTest[0m: 10.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90724
[1mStep[0m  [4/42], [94mLoss[0m : 11.00129
[1mStep[0m  [8/42], [94mLoss[0m : 10.94127
[1mStep[0m  [12/42], [94mLoss[0m : 10.74817
[1mStep[0m  [16/42], [94mLoss[0m : 10.94641
[1mStep[0m  [20/42], [94mLoss[0m : 10.66480
[1mStep[0m  [24/42], [94mLoss[0m : 10.31401
[1mStep[0m  [28/42], [94mLoss[0m : 10.87317
[1mStep[0m  [32/42], [94mLoss[0m : 10.43647
[1mStep[0m  [36/42], [94mLoss[0m : 10.96241
[1mStep[0m  [40/42], [94mLoss[0m : 10.89420

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.833, [92mTest[0m: 10.784, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.48029
[1mStep[0m  [4/42], [94mLoss[0m : 10.77368
[1mStep[0m  [8/42], [94mLoss[0m : 10.45002
[1mStep[0m  [12/42], [94mLoss[0m : 10.71426
[1mStep[0m  [16/42], [94mLoss[0m : 11.20607
[1mStep[0m  [20/42], [94mLoss[0m : 11.00491
[1mStep[0m  [24/42], [94mLoss[0m : 10.67806
[1mStep[0m  [28/42], [94mLoss[0m : 10.73141
[1mStep[0m  [32/42], [94mLoss[0m : 10.83274
[1mStep[0m  [36/42], [94mLoss[0m : 10.88913
[1mStep[0m  [40/42], [94mLoss[0m : 10.66430

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.827, [92mTest[0m: 10.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68888
[1mStep[0m  [4/42], [94mLoss[0m : 11.16204
[1mStep[0m  [8/42], [94mLoss[0m : 11.09318
[1mStep[0m  [12/42], [94mLoss[0m : 10.70968
[1mStep[0m  [16/42], [94mLoss[0m : 10.76889
[1mStep[0m  [20/42], [94mLoss[0m : 10.56881
[1mStep[0m  [24/42], [94mLoss[0m : 10.64406
[1mStep[0m  [28/42], [94mLoss[0m : 10.99980
[1mStep[0m  [32/42], [94mLoss[0m : 10.62244
[1mStep[0m  [36/42], [94mLoss[0m : 10.90961
[1mStep[0m  [40/42], [94mLoss[0m : 10.77585

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.814, [92mTest[0m: 10.768, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72385
[1mStep[0m  [4/42], [94mLoss[0m : 10.56615
[1mStep[0m  [8/42], [94mLoss[0m : 10.93426
[1mStep[0m  [12/42], [94mLoss[0m : 10.61544
[1mStep[0m  [16/42], [94mLoss[0m : 11.08376
[1mStep[0m  [20/42], [94mLoss[0m : 10.54391
[1mStep[0m  [24/42], [94mLoss[0m : 10.65737
[1mStep[0m  [28/42], [94mLoss[0m : 10.64329
[1mStep[0m  [32/42], [94mLoss[0m : 11.16809
[1mStep[0m  [36/42], [94mLoss[0m : 10.74879
[1mStep[0m  [40/42], [94mLoss[0m : 10.77038

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.809, [92mTest[0m: 10.751, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.17704
[1mStep[0m  [4/42], [94mLoss[0m : 10.59450
[1mStep[0m  [8/42], [94mLoss[0m : 10.85640
[1mStep[0m  [12/42], [94mLoss[0m : 10.84429
[1mStep[0m  [16/42], [94mLoss[0m : 10.92452
[1mStep[0m  [20/42], [94mLoss[0m : 10.99685
[1mStep[0m  [24/42], [94mLoss[0m : 11.04145
[1mStep[0m  [28/42], [94mLoss[0m : 10.68008
[1mStep[0m  [32/42], [94mLoss[0m : 10.80749
[1mStep[0m  [36/42], [94mLoss[0m : 10.65131
[1mStep[0m  [40/42], [94mLoss[0m : 10.62359

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.801, [92mTest[0m: 10.750, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.41366
[1mStep[0m  [4/42], [94mLoss[0m : 10.91367
[1mStep[0m  [8/42], [94mLoss[0m : 10.78679
[1mStep[0m  [12/42], [94mLoss[0m : 11.21527
[1mStep[0m  [16/42], [94mLoss[0m : 10.75990
[1mStep[0m  [20/42], [94mLoss[0m : 11.00455
[1mStep[0m  [24/42], [94mLoss[0m : 10.78586
[1mStep[0m  [28/42], [94mLoss[0m : 10.40884
[1mStep[0m  [32/42], [94mLoss[0m : 11.07462
[1mStep[0m  [36/42], [94mLoss[0m : 10.73660
[1mStep[0m  [40/42], [94mLoss[0m : 11.02908

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.792, [92mTest[0m: 10.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.82892
[1mStep[0m  [4/42], [94mLoss[0m : 10.77796
[1mStep[0m  [8/42], [94mLoss[0m : 10.62649
[1mStep[0m  [12/42], [94mLoss[0m : 10.84272
[1mStep[0m  [16/42], [94mLoss[0m : 10.39803
[1mStep[0m  [20/42], [94mLoss[0m : 10.46525
[1mStep[0m  [24/42], [94mLoss[0m : 10.70511
[1mStep[0m  [28/42], [94mLoss[0m : 10.65591
[1mStep[0m  [32/42], [94mLoss[0m : 10.70905
[1mStep[0m  [36/42], [94mLoss[0m : 11.17489
[1mStep[0m  [40/42], [94mLoss[0m : 10.71452

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.796, [92mTest[0m: 10.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.04396
[1mStep[0m  [4/42], [94mLoss[0m : 10.77663
[1mStep[0m  [8/42], [94mLoss[0m : 10.66807
[1mStep[0m  [12/42], [94mLoss[0m : 10.88173
[1mStep[0m  [16/42], [94mLoss[0m : 11.42698
[1mStep[0m  [20/42], [94mLoss[0m : 10.62604
[1mStep[0m  [24/42], [94mLoss[0m : 10.69164
[1mStep[0m  [28/42], [94mLoss[0m : 10.96688
[1mStep[0m  [32/42], [94mLoss[0m : 10.80309
[1mStep[0m  [36/42], [94mLoss[0m : 10.86479
[1mStep[0m  [40/42], [94mLoss[0m : 10.78904

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.780, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.75671
[1mStep[0m  [4/42], [94mLoss[0m : 11.17005
[1mStep[0m  [8/42], [94mLoss[0m : 10.74482
[1mStep[0m  [12/42], [94mLoss[0m : 10.81051
[1mStep[0m  [16/42], [94mLoss[0m : 10.43866
[1mStep[0m  [20/42], [94mLoss[0m : 11.09269
[1mStep[0m  [24/42], [94mLoss[0m : 10.36600
[1mStep[0m  [28/42], [94mLoss[0m : 10.69828
[1mStep[0m  [32/42], [94mLoss[0m : 10.48422
[1mStep[0m  [36/42], [94mLoss[0m : 10.26976
[1mStep[0m  [40/42], [94mLoss[0m : 10.69287

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.773, [92mTest[0m: 10.705, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.59241
[1mStep[0m  [4/42], [94mLoss[0m : 10.85986
[1mStep[0m  [8/42], [94mLoss[0m : 10.96819
[1mStep[0m  [12/42], [94mLoss[0m : 11.12679
[1mStep[0m  [16/42], [94mLoss[0m : 11.07443
[1mStep[0m  [20/42], [94mLoss[0m : 10.92800
[1mStep[0m  [24/42], [94mLoss[0m : 10.76762
[1mStep[0m  [28/42], [94mLoss[0m : 10.60290
[1mStep[0m  [32/42], [94mLoss[0m : 11.07036
[1mStep[0m  [36/42], [94mLoss[0m : 10.85672
[1mStep[0m  [40/42], [94mLoss[0m : 10.76578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.758, [92mTest[0m: 10.687, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54282
[1mStep[0m  [4/42], [94mLoss[0m : 11.01760
[1mStep[0m  [8/42], [94mLoss[0m : 10.59185
[1mStep[0m  [12/42], [94mLoss[0m : 10.49924
[1mStep[0m  [16/42], [94mLoss[0m : 10.69470
[1mStep[0m  [20/42], [94mLoss[0m : 10.79575
[1mStep[0m  [24/42], [94mLoss[0m : 11.09349
[1mStep[0m  [28/42], [94mLoss[0m : 10.70281
[1mStep[0m  [32/42], [94mLoss[0m : 10.64661
[1mStep[0m  [36/42], [94mLoss[0m : 10.75362
[1mStep[0m  [40/42], [94mLoss[0m : 11.02573

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.756, [92mTest[0m: 10.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22802
[1mStep[0m  [4/42], [94mLoss[0m : 10.45096
[1mStep[0m  [8/42], [94mLoss[0m : 10.98789
[1mStep[0m  [12/42], [94mLoss[0m : 10.84134
[1mStep[0m  [16/42], [94mLoss[0m : 10.83161
[1mStep[0m  [20/42], [94mLoss[0m : 10.50131
[1mStep[0m  [24/42], [94mLoss[0m : 10.73709
[1mStep[0m  [28/42], [94mLoss[0m : 10.85029
[1mStep[0m  [32/42], [94mLoss[0m : 10.93176
[1mStep[0m  [36/42], [94mLoss[0m : 10.94089
[1mStep[0m  [40/42], [94mLoss[0m : 10.64942

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.750, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54043
[1mStep[0m  [4/42], [94mLoss[0m : 10.50919
[1mStep[0m  [8/42], [94mLoss[0m : 10.68618
[1mStep[0m  [12/42], [94mLoss[0m : 10.78241
[1mStep[0m  [16/42], [94mLoss[0m : 10.53014
[1mStep[0m  [20/42], [94mLoss[0m : 10.57847
[1mStep[0m  [24/42], [94mLoss[0m : 10.88592
[1mStep[0m  [28/42], [94mLoss[0m : 10.80506
[1mStep[0m  [32/42], [94mLoss[0m : 10.81952
[1mStep[0m  [36/42], [94mLoss[0m : 10.81339
[1mStep[0m  [40/42], [94mLoss[0m : 11.01963

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.739, [92mTest[0m: 10.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93285
[1mStep[0m  [4/42], [94mLoss[0m : 10.59078
[1mStep[0m  [8/42], [94mLoss[0m : 10.50148
[1mStep[0m  [12/42], [94mLoss[0m : 10.32232
[1mStep[0m  [16/42], [94mLoss[0m : 10.90967
[1mStep[0m  [20/42], [94mLoss[0m : 10.74945
[1mStep[0m  [24/42], [94mLoss[0m : 10.56187
[1mStep[0m  [28/42], [94mLoss[0m : 10.84480
[1mStep[0m  [32/42], [94mLoss[0m : 10.53157
[1mStep[0m  [36/42], [94mLoss[0m : 10.70782
[1mStep[0m  [40/42], [94mLoss[0m : 10.89692

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.737, [92mTest[0m: 10.651, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52299
[1mStep[0m  [4/42], [94mLoss[0m : 10.82083
[1mStep[0m  [8/42], [94mLoss[0m : 11.22150
[1mStep[0m  [12/42], [94mLoss[0m : 10.46144
[1mStep[0m  [16/42], [94mLoss[0m : 10.64870
[1mStep[0m  [20/42], [94mLoss[0m : 10.92176
[1mStep[0m  [24/42], [94mLoss[0m : 10.98070
[1mStep[0m  [28/42], [94mLoss[0m : 10.20478
[1mStep[0m  [32/42], [94mLoss[0m : 10.56101
[1mStep[0m  [36/42], [94mLoss[0m : 11.11230
[1mStep[0m  [40/42], [94mLoss[0m : 10.76825

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.730, [92mTest[0m: 10.648, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74762
[1mStep[0m  [4/42], [94mLoss[0m : 10.56399
[1mStep[0m  [8/42], [94mLoss[0m : 10.97062
[1mStep[0m  [12/42], [94mLoss[0m : 10.43349
[1mStep[0m  [16/42], [94mLoss[0m : 10.71937
[1mStep[0m  [20/42], [94mLoss[0m : 10.61290
[1mStep[0m  [24/42], [94mLoss[0m : 10.31258
[1mStep[0m  [28/42], [94mLoss[0m : 10.40442
[1mStep[0m  [32/42], [94mLoss[0m : 10.70070
[1mStep[0m  [36/42], [94mLoss[0m : 10.93706
[1mStep[0m  [40/42], [94mLoss[0m : 10.70933

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.629, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81169
[1mStep[0m  [4/42], [94mLoss[0m : 10.43835
[1mStep[0m  [8/42], [94mLoss[0m : 10.43169
[1mStep[0m  [12/42], [94mLoss[0m : 10.95687
[1mStep[0m  [16/42], [94mLoss[0m : 11.11370
[1mStep[0m  [20/42], [94mLoss[0m : 10.63716
[1mStep[0m  [24/42], [94mLoss[0m : 10.59543
[1mStep[0m  [28/42], [94mLoss[0m : 10.97098
[1mStep[0m  [32/42], [94mLoss[0m : 10.61399
[1mStep[0m  [36/42], [94mLoss[0m : 10.81859
[1mStep[0m  [40/42], [94mLoss[0m : 10.75899

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.616, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71842
[1mStep[0m  [4/42], [94mLoss[0m : 10.89182
[1mStep[0m  [8/42], [94mLoss[0m : 10.82740
[1mStep[0m  [12/42], [94mLoss[0m : 10.68703
[1mStep[0m  [16/42], [94mLoss[0m : 10.53223
[1mStep[0m  [20/42], [94mLoss[0m : 10.69779
[1mStep[0m  [24/42], [94mLoss[0m : 10.88498
[1mStep[0m  [28/42], [94mLoss[0m : 10.47288
[1mStep[0m  [32/42], [94mLoss[0m : 10.71738
[1mStep[0m  [36/42], [94mLoss[0m : 10.66961
[1mStep[0m  [40/42], [94mLoss[0m : 10.40284

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.703, [92mTest[0m: 10.611, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76578
[1mStep[0m  [4/42], [94mLoss[0m : 10.99917
[1mStep[0m  [8/42], [94mLoss[0m : 10.64724
[1mStep[0m  [12/42], [94mLoss[0m : 11.06954
[1mStep[0m  [16/42], [94mLoss[0m : 10.39252
[1mStep[0m  [20/42], [94mLoss[0m : 10.72430
[1mStep[0m  [24/42], [94mLoss[0m : 10.77894
[1mStep[0m  [28/42], [94mLoss[0m : 10.90866
[1mStep[0m  [32/42], [94mLoss[0m : 10.32106
[1mStep[0m  [36/42], [94mLoss[0m : 10.85853
[1mStep[0m  [40/42], [94mLoss[0m : 10.57830

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.703, [92mTest[0m: 10.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57327
[1mStep[0m  [4/42], [94mLoss[0m : 10.88097
[1mStep[0m  [8/42], [94mLoss[0m : 10.38960
[1mStep[0m  [12/42], [94mLoss[0m : 11.07002
[1mStep[0m  [16/42], [94mLoss[0m : 10.57295
[1mStep[0m  [20/42], [94mLoss[0m : 11.02427
[1mStep[0m  [24/42], [94mLoss[0m : 10.95166
[1mStep[0m  [28/42], [94mLoss[0m : 10.66524
[1mStep[0m  [32/42], [94mLoss[0m : 10.52188
[1mStep[0m  [36/42], [94mLoss[0m : 10.68009
[1mStep[0m  [40/42], [94mLoss[0m : 10.43784

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.690, [92mTest[0m: 10.603, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.86742
[1mStep[0m  [4/42], [94mLoss[0m : 10.63706
[1mStep[0m  [8/42], [94mLoss[0m : 10.50108
[1mStep[0m  [12/42], [94mLoss[0m : 10.23445
[1mStep[0m  [16/42], [94mLoss[0m : 10.39645
[1mStep[0m  [20/42], [94mLoss[0m : 10.64370
[1mStep[0m  [24/42], [94mLoss[0m : 10.90696
[1mStep[0m  [28/42], [94mLoss[0m : 10.47787
[1mStep[0m  [32/42], [94mLoss[0m : 10.57481
[1mStep[0m  [36/42], [94mLoss[0m : 10.57150
[1mStep[0m  [40/42], [94mLoss[0m : 10.58786

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.684, [92mTest[0m: 10.585, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.05484
[1mStep[0m  [4/42], [94mLoss[0m : 10.50696
[1mStep[0m  [8/42], [94mLoss[0m : 11.19440
[1mStep[0m  [12/42], [94mLoss[0m : 10.81574
[1mStep[0m  [16/42], [94mLoss[0m : 10.61337
[1mStep[0m  [20/42], [94mLoss[0m : 10.54786
[1mStep[0m  [24/42], [94mLoss[0m : 10.43473
[1mStep[0m  [28/42], [94mLoss[0m : 10.60684
[1mStep[0m  [32/42], [94mLoss[0m : 11.08573
[1mStep[0m  [36/42], [94mLoss[0m : 11.12082
[1mStep[0m  [40/42], [94mLoss[0m : 10.82210

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.674, [92mTest[0m: 10.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.67024
[1mStep[0m  [4/42], [94mLoss[0m : 10.38850
[1mStep[0m  [8/42], [94mLoss[0m : 10.67858
[1mStep[0m  [12/42], [94mLoss[0m : 10.59480
[1mStep[0m  [16/42], [94mLoss[0m : 10.57345
[1mStep[0m  [20/42], [94mLoss[0m : 10.68223
[1mStep[0m  [24/42], [94mLoss[0m : 10.62391
[1mStep[0m  [28/42], [94mLoss[0m : 10.56908
[1mStep[0m  [32/42], [94mLoss[0m : 10.75543
[1mStep[0m  [36/42], [94mLoss[0m : 10.36164
[1mStep[0m  [40/42], [94mLoss[0m : 10.52295

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.672, [92mTest[0m: 10.566, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.551
====================================

Phase 1 - Evaluation MAE:  10.55103063583374
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.67169
[1mStep[0m  [4/42], [94mLoss[0m : 10.78387
[1mStep[0m  [8/42], [94mLoss[0m : 10.51139
[1mStep[0m  [12/42], [94mLoss[0m : 10.46146
[1mStep[0m  [16/42], [94mLoss[0m : 10.36124
[1mStep[0m  [20/42], [94mLoss[0m : 10.37919
[1mStep[0m  [24/42], [94mLoss[0m : 10.61598
[1mStep[0m  [28/42], [94mLoss[0m : 10.40846
[1mStep[0m  [32/42], [94mLoss[0m : 10.76405
[1mStep[0m  [36/42], [94mLoss[0m : 10.32141
[1mStep[0m  [40/42], [94mLoss[0m : 10.65012

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.664, [92mTest[0m: 10.554, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36341
[1mStep[0m  [4/42], [94mLoss[0m : 10.61286
[1mStep[0m  [8/42], [94mLoss[0m : 10.17721
[1mStep[0m  [12/42], [94mLoss[0m : 10.56511
[1mStep[0m  [16/42], [94mLoss[0m : 10.53091
[1mStep[0m  [20/42], [94mLoss[0m : 10.74704
[1mStep[0m  [24/42], [94mLoss[0m : 10.32933
[1mStep[0m  [28/42], [94mLoss[0m : 10.84183
[1mStep[0m  [32/42], [94mLoss[0m : 10.52687
[1mStep[0m  [36/42], [94mLoss[0m : 10.55876
[1mStep[0m  [40/42], [94mLoss[0m : 10.94393

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.656, [92mTest[0m: 10.543, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42557
[1mStep[0m  [4/42], [94mLoss[0m : 11.07545
[1mStep[0m  [8/42], [94mLoss[0m : 10.25146
[1mStep[0m  [12/42], [94mLoss[0m : 10.77527
[1mStep[0m  [16/42], [94mLoss[0m : 10.47996
[1mStep[0m  [20/42], [94mLoss[0m : 10.58104
[1mStep[0m  [24/42], [94mLoss[0m : 10.59127
[1mStep[0m  [28/42], [94mLoss[0m : 10.39625
[1mStep[0m  [32/42], [94mLoss[0m : 10.29954
[1mStep[0m  [36/42], [94mLoss[0m : 11.03351
[1mStep[0m  [40/42], [94mLoss[0m : 10.28074

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.638, [92mTest[0m: 10.533, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.69282
[1mStep[0m  [4/42], [94mLoss[0m : 10.73623
[1mStep[0m  [8/42], [94mLoss[0m : 10.42695
[1mStep[0m  [12/42], [94mLoss[0m : 10.79336
[1mStep[0m  [16/42], [94mLoss[0m : 10.43546
[1mStep[0m  [20/42], [94mLoss[0m : 10.45008
[1mStep[0m  [24/42], [94mLoss[0m : 10.39310
[1mStep[0m  [28/42], [94mLoss[0m : 10.73402
[1mStep[0m  [32/42], [94mLoss[0m : 10.74221
[1mStep[0m  [36/42], [94mLoss[0m : 10.64696
[1mStep[0m  [40/42], [94mLoss[0m : 10.48261

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.501, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53645
[1mStep[0m  [4/42], [94mLoss[0m : 10.65483
[1mStep[0m  [8/42], [94mLoss[0m : 10.70843
[1mStep[0m  [12/42], [94mLoss[0m : 10.31495
[1mStep[0m  [16/42], [94mLoss[0m : 10.89660
[1mStep[0m  [20/42], [94mLoss[0m : 11.03880
[1mStep[0m  [24/42], [94mLoss[0m : 11.03059
[1mStep[0m  [28/42], [94mLoss[0m : 10.61895
[1mStep[0m  [32/42], [94mLoss[0m : 10.39930
[1mStep[0m  [36/42], [94mLoss[0m : 10.60074
[1mStep[0m  [40/42], [94mLoss[0m : 10.91899

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.620, [92mTest[0m: 10.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.60346
[1mStep[0m  [4/42], [94mLoss[0m : 10.43086
[1mStep[0m  [8/42], [94mLoss[0m : 10.67119
[1mStep[0m  [12/42], [94mLoss[0m : 10.60637
[1mStep[0m  [16/42], [94mLoss[0m : 10.60252
[1mStep[0m  [20/42], [94mLoss[0m : 10.54004
[1mStep[0m  [24/42], [94mLoss[0m : 10.51592
[1mStep[0m  [28/42], [94mLoss[0m : 10.98554
[1mStep[0m  [32/42], [94mLoss[0m : 10.47999
[1mStep[0m  [36/42], [94mLoss[0m : 10.89244
[1mStep[0m  [40/42], [94mLoss[0m : 10.48759

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.61826
[1mStep[0m  [4/42], [94mLoss[0m : 10.76237
[1mStep[0m  [8/42], [94mLoss[0m : 10.38324
[1mStep[0m  [12/42], [94mLoss[0m : 10.52896
[1mStep[0m  [16/42], [94mLoss[0m : 10.54657
[1mStep[0m  [20/42], [94mLoss[0m : 10.67563
[1mStep[0m  [24/42], [94mLoss[0m : 10.90547
[1mStep[0m  [28/42], [94mLoss[0m : 10.61711
[1mStep[0m  [32/42], [94mLoss[0m : 10.21358
[1mStep[0m  [36/42], [94mLoss[0m : 10.46737
[1mStep[0m  [40/42], [94mLoss[0m : 10.45376

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.595, [92mTest[0m: 10.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72571
[1mStep[0m  [4/42], [94mLoss[0m : 10.23549
[1mStep[0m  [8/42], [94mLoss[0m : 10.55261
[1mStep[0m  [12/42], [94mLoss[0m : 10.56063
[1mStep[0m  [16/42], [94mLoss[0m : 10.67134
[1mStep[0m  [20/42], [94mLoss[0m : 10.59073
[1mStep[0m  [24/42], [94mLoss[0m : 10.55791
[1mStep[0m  [28/42], [94mLoss[0m : 10.81516
[1mStep[0m  [32/42], [94mLoss[0m : 10.89930
[1mStep[0m  [36/42], [94mLoss[0m : 10.67141
[1mStep[0m  [40/42], [94mLoss[0m : 10.42903

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.578, [92mTest[0m: 10.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53540
[1mStep[0m  [4/42], [94mLoss[0m : 10.88851
[1mStep[0m  [8/42], [94mLoss[0m : 10.85409
[1mStep[0m  [12/42], [94mLoss[0m : 10.27610
[1mStep[0m  [16/42], [94mLoss[0m : 10.55863
[1mStep[0m  [20/42], [94mLoss[0m : 10.38450
[1mStep[0m  [24/42], [94mLoss[0m : 10.29898
[1mStep[0m  [28/42], [94mLoss[0m : 10.59332
[1mStep[0m  [32/42], [94mLoss[0m : 10.55607
[1mStep[0m  [36/42], [94mLoss[0m : 10.41835
[1mStep[0m  [40/42], [94mLoss[0m : 10.61581

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.561, [92mTest[0m: 10.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32948
[1mStep[0m  [4/42], [94mLoss[0m : 10.28448
[1mStep[0m  [8/42], [94mLoss[0m : 10.74677
[1mStep[0m  [12/42], [94mLoss[0m : 10.57002
[1mStep[0m  [16/42], [94mLoss[0m : 10.27009
[1mStep[0m  [20/42], [94mLoss[0m : 10.16340
[1mStep[0m  [24/42], [94mLoss[0m : 10.33082
[1mStep[0m  [28/42], [94mLoss[0m : 10.68523
[1mStep[0m  [32/42], [94mLoss[0m : 10.59195
[1mStep[0m  [36/42], [94mLoss[0m : 10.61486
[1mStep[0m  [40/42], [94mLoss[0m : 10.14453

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.557, [92mTest[0m: 10.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.58879
[1mStep[0m  [4/42], [94mLoss[0m : 10.36447
[1mStep[0m  [8/42], [94mLoss[0m : 10.57848
[1mStep[0m  [12/42], [94mLoss[0m : 10.41941
[1mStep[0m  [16/42], [94mLoss[0m : 10.46214
[1mStep[0m  [20/42], [94mLoss[0m : 10.32670
[1mStep[0m  [24/42], [94mLoss[0m : 10.38933
[1mStep[0m  [28/42], [94mLoss[0m : 10.72557
[1mStep[0m  [32/42], [94mLoss[0m : 10.33887
[1mStep[0m  [36/42], [94mLoss[0m : 10.59208
[1mStep[0m  [40/42], [94mLoss[0m : 10.73798

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.542, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23008
[1mStep[0m  [4/42], [94mLoss[0m : 10.51946
[1mStep[0m  [8/42], [94mLoss[0m : 10.28447
[1mStep[0m  [12/42], [94mLoss[0m : 10.41266
[1mStep[0m  [16/42], [94mLoss[0m : 10.50855
[1mStep[0m  [20/42], [94mLoss[0m : 10.93315
[1mStep[0m  [24/42], [94mLoss[0m : 10.24876
[1mStep[0m  [28/42], [94mLoss[0m : 10.80182
[1mStep[0m  [32/42], [94mLoss[0m : 10.64515
[1mStep[0m  [36/42], [94mLoss[0m : 10.54438
[1mStep[0m  [40/42], [94mLoss[0m : 10.50639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.536, [92mTest[0m: 10.398, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46193
[1mStep[0m  [4/42], [94mLoss[0m : 10.37159
[1mStep[0m  [8/42], [94mLoss[0m : 10.67695
[1mStep[0m  [12/42], [94mLoss[0m : 10.30134
[1mStep[0m  [16/42], [94mLoss[0m : 10.41175
[1mStep[0m  [20/42], [94mLoss[0m : 10.34120
[1mStep[0m  [24/42], [94mLoss[0m : 10.44190
[1mStep[0m  [28/42], [94mLoss[0m : 10.27051
[1mStep[0m  [32/42], [94mLoss[0m : 10.50588
[1mStep[0m  [36/42], [94mLoss[0m : 10.30075
[1mStep[0m  [40/42], [94mLoss[0m : 10.13384

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.388, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36350
[1mStep[0m  [4/42], [94mLoss[0m : 10.62087
[1mStep[0m  [8/42], [94mLoss[0m : 10.82612
[1mStep[0m  [12/42], [94mLoss[0m : 10.37389
[1mStep[0m  [16/42], [94mLoss[0m : 10.27040
[1mStep[0m  [20/42], [94mLoss[0m : 10.64999
[1mStep[0m  [24/42], [94mLoss[0m : 10.56230
[1mStep[0m  [28/42], [94mLoss[0m : 10.52598
[1mStep[0m  [32/42], [94mLoss[0m : 10.40293
[1mStep[0m  [36/42], [94mLoss[0m : 10.39368
[1mStep[0m  [40/42], [94mLoss[0m : 10.70567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.497, [92mTest[0m: 10.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26057
[1mStep[0m  [4/42], [94mLoss[0m : 10.55352
[1mStep[0m  [8/42], [94mLoss[0m : 10.27639
[1mStep[0m  [12/42], [94mLoss[0m : 10.68398
[1mStep[0m  [16/42], [94mLoss[0m : 10.53403
[1mStep[0m  [20/42], [94mLoss[0m : 10.68661
[1mStep[0m  [24/42], [94mLoss[0m : 10.50122
[1mStep[0m  [28/42], [94mLoss[0m : 10.50782
[1mStep[0m  [32/42], [94mLoss[0m : 10.55922
[1mStep[0m  [36/42], [94mLoss[0m : 10.62578
[1mStep[0m  [40/42], [94mLoss[0m : 10.08010

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.482, [92mTest[0m: 10.371, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64451
[1mStep[0m  [4/42], [94mLoss[0m : 10.48927
[1mStep[0m  [8/42], [94mLoss[0m : 10.41912
[1mStep[0m  [12/42], [94mLoss[0m : 10.46181
[1mStep[0m  [16/42], [94mLoss[0m : 10.49047
[1mStep[0m  [20/42], [94mLoss[0m : 10.33095
[1mStep[0m  [24/42], [94mLoss[0m : 10.85958
[1mStep[0m  [28/42], [94mLoss[0m : 10.51812
[1mStep[0m  [32/42], [94mLoss[0m : 10.67965
[1mStep[0m  [36/42], [94mLoss[0m : 10.27052
[1mStep[0m  [40/42], [94mLoss[0m : 10.49875

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.470, [92mTest[0m: 10.361, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.89598
[1mStep[0m  [4/42], [94mLoss[0m : 10.44367
[1mStep[0m  [8/42], [94mLoss[0m : 10.60505
[1mStep[0m  [12/42], [94mLoss[0m : 10.37758
[1mStep[0m  [16/42], [94mLoss[0m : 10.50598
[1mStep[0m  [20/42], [94mLoss[0m : 10.20710
[1mStep[0m  [24/42], [94mLoss[0m : 10.38902
[1mStep[0m  [28/42], [94mLoss[0m : 10.69140
[1mStep[0m  [32/42], [94mLoss[0m : 10.78268
[1mStep[0m  [36/42], [94mLoss[0m : 10.31072
[1mStep[0m  [40/42], [94mLoss[0m : 10.17398

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.460, [92mTest[0m: 10.340, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.24460
[1mStep[0m  [4/42], [94mLoss[0m : 10.61376
[1mStep[0m  [8/42], [94mLoss[0m : 10.48022
[1mStep[0m  [12/42], [94mLoss[0m : 10.02723
[1mStep[0m  [16/42], [94mLoss[0m : 10.77702
[1mStep[0m  [20/42], [94mLoss[0m : 9.97850
[1mStep[0m  [24/42], [94mLoss[0m : 10.20188
[1mStep[0m  [28/42], [94mLoss[0m : 10.34150
[1mStep[0m  [32/42], [94mLoss[0m : 10.72417
[1mStep[0m  [36/42], [94mLoss[0m : 10.25457
[1mStep[0m  [40/42], [94mLoss[0m : 9.98488

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.443, [92mTest[0m: 10.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57015
[1mStep[0m  [4/42], [94mLoss[0m : 10.27035
[1mStep[0m  [8/42], [94mLoss[0m : 10.42882
[1mStep[0m  [12/42], [94mLoss[0m : 10.65877
[1mStep[0m  [16/42], [94mLoss[0m : 10.03500
[1mStep[0m  [20/42], [94mLoss[0m : 10.58255
[1mStep[0m  [24/42], [94mLoss[0m : 10.15423
[1mStep[0m  [28/42], [94mLoss[0m : 10.23915
[1mStep[0m  [32/42], [94mLoss[0m : 10.34532
[1mStep[0m  [36/42], [94mLoss[0m : 10.48684
[1mStep[0m  [40/42], [94mLoss[0m : 10.73666

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.426, [92mTest[0m: 10.288, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.79449
[1mStep[0m  [4/42], [94mLoss[0m : 10.66517
[1mStep[0m  [8/42], [94mLoss[0m : 10.57917
[1mStep[0m  [12/42], [94mLoss[0m : 10.61826
[1mStep[0m  [16/42], [94mLoss[0m : 10.67463
[1mStep[0m  [20/42], [94mLoss[0m : 10.50487
[1mStep[0m  [24/42], [94mLoss[0m : 10.19836
[1mStep[0m  [28/42], [94mLoss[0m : 10.11141
[1mStep[0m  [32/42], [94mLoss[0m : 10.61264
[1mStep[0m  [36/42], [94mLoss[0m : 10.21922
[1mStep[0m  [40/42], [94mLoss[0m : 10.45100

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.410, [92mTest[0m: 10.297, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.70908
[1mStep[0m  [4/42], [94mLoss[0m : 11.11978
[1mStep[0m  [8/42], [94mLoss[0m : 10.71813
[1mStep[0m  [12/42], [94mLoss[0m : 10.35740
[1mStep[0m  [16/42], [94mLoss[0m : 10.24630
[1mStep[0m  [20/42], [94mLoss[0m : 10.25779
[1mStep[0m  [24/42], [94mLoss[0m : 10.44284
[1mStep[0m  [28/42], [94mLoss[0m : 10.55289
[1mStep[0m  [32/42], [94mLoss[0m : 10.15582
[1mStep[0m  [36/42], [94mLoss[0m : 10.40741
[1mStep[0m  [40/42], [94mLoss[0m : 10.50510

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.260, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51223
[1mStep[0m  [4/42], [94mLoss[0m : 10.12063
[1mStep[0m  [8/42], [94mLoss[0m : 10.60136
[1mStep[0m  [12/42], [94mLoss[0m : 10.26774
[1mStep[0m  [16/42], [94mLoss[0m : 10.13853
[1mStep[0m  [20/42], [94mLoss[0m : 10.38877
[1mStep[0m  [24/42], [94mLoss[0m : 10.17423
[1mStep[0m  [28/42], [94mLoss[0m : 10.28179
[1mStep[0m  [32/42], [94mLoss[0m : 10.71244
[1mStep[0m  [36/42], [94mLoss[0m : 10.39647
[1mStep[0m  [40/42], [94mLoss[0m : 10.36506

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.382, [92mTest[0m: 10.272, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68544
[1mStep[0m  [4/42], [94mLoss[0m : 10.16725
[1mStep[0m  [8/42], [94mLoss[0m : 10.31884
[1mStep[0m  [12/42], [94mLoss[0m : 10.37614
[1mStep[0m  [16/42], [94mLoss[0m : 10.76433
[1mStep[0m  [20/42], [94mLoss[0m : 10.49255
[1mStep[0m  [24/42], [94mLoss[0m : 10.46292
[1mStep[0m  [28/42], [94mLoss[0m : 10.53828
[1mStep[0m  [32/42], [94mLoss[0m : 10.20187
[1mStep[0m  [36/42], [94mLoss[0m : 10.38574
[1mStep[0m  [40/42], [94mLoss[0m : 10.38345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.368, [92mTest[0m: 10.231, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.62543
[1mStep[0m  [4/42], [94mLoss[0m : 10.64053
[1mStep[0m  [8/42], [94mLoss[0m : 10.01714
[1mStep[0m  [12/42], [94mLoss[0m : 10.22435
[1mStep[0m  [16/42], [94mLoss[0m : 10.12448
[1mStep[0m  [20/42], [94mLoss[0m : 10.29277
[1mStep[0m  [24/42], [94mLoss[0m : 10.37702
[1mStep[0m  [28/42], [94mLoss[0m : 10.29115
[1mStep[0m  [32/42], [94mLoss[0m : 10.41033
[1mStep[0m  [36/42], [94mLoss[0m : 10.48519
[1mStep[0m  [40/42], [94mLoss[0m : 10.59065

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.346, [92mTest[0m: 10.212, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63689
[1mStep[0m  [4/42], [94mLoss[0m : 9.96818
[1mStep[0m  [8/42], [94mLoss[0m : 10.22194
[1mStep[0m  [12/42], [94mLoss[0m : 10.30500
[1mStep[0m  [16/42], [94mLoss[0m : 10.34773
[1mStep[0m  [20/42], [94mLoss[0m : 10.94298
[1mStep[0m  [24/42], [94mLoss[0m : 10.13616
[1mStep[0m  [28/42], [94mLoss[0m : 10.32853
[1mStep[0m  [32/42], [94mLoss[0m : 10.34101
[1mStep[0m  [36/42], [94mLoss[0m : 9.96710
[1mStep[0m  [40/42], [94mLoss[0m : 10.42264

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.340, [92mTest[0m: 10.233, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.47504
[1mStep[0m  [4/42], [94mLoss[0m : 10.21550
[1mStep[0m  [8/42], [94mLoss[0m : 10.15707
[1mStep[0m  [12/42], [94mLoss[0m : 10.26975
[1mStep[0m  [16/42], [94mLoss[0m : 10.41798
[1mStep[0m  [20/42], [94mLoss[0m : 10.20389
[1mStep[0m  [24/42], [94mLoss[0m : 10.12100
[1mStep[0m  [28/42], [94mLoss[0m : 10.58753
[1mStep[0m  [32/42], [94mLoss[0m : 10.26712
[1mStep[0m  [36/42], [94mLoss[0m : 10.06223
[1mStep[0m  [40/42], [94mLoss[0m : 10.53684

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.325, [92mTest[0m: 10.218, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65139
[1mStep[0m  [4/42], [94mLoss[0m : 10.76105
[1mStep[0m  [8/42], [94mLoss[0m : 10.14128
[1mStep[0m  [12/42], [94mLoss[0m : 10.38742
[1mStep[0m  [16/42], [94mLoss[0m : 10.31944
[1mStep[0m  [20/42], [94mLoss[0m : 10.56918
[1mStep[0m  [24/42], [94mLoss[0m : 10.00078
[1mStep[0m  [28/42], [94mLoss[0m : 10.39537
[1mStep[0m  [32/42], [94mLoss[0m : 10.40629
[1mStep[0m  [36/42], [94mLoss[0m : 10.31640
[1mStep[0m  [40/42], [94mLoss[0m : 10.27395

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.316, [92mTest[0m: 10.160, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15471
[1mStep[0m  [4/42], [94mLoss[0m : 10.12125
[1mStep[0m  [8/42], [94mLoss[0m : 10.36208
[1mStep[0m  [12/42], [94mLoss[0m : 10.14924
[1mStep[0m  [16/42], [94mLoss[0m : 10.04927
[1mStep[0m  [20/42], [94mLoss[0m : 10.01548
[1mStep[0m  [24/42], [94mLoss[0m : 10.41338
[1mStep[0m  [28/42], [94mLoss[0m : 10.17721
[1mStep[0m  [32/42], [94mLoss[0m : 10.01675
[1mStep[0m  [36/42], [94mLoss[0m : 10.25153
[1mStep[0m  [40/42], [94mLoss[0m : 10.47995

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.304, [92mTest[0m: 10.172, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.98189
[1mStep[0m  [4/42], [94mLoss[0m : 10.41049
[1mStep[0m  [8/42], [94mLoss[0m : 10.37721
[1mStep[0m  [12/42], [94mLoss[0m : 10.48646
[1mStep[0m  [16/42], [94mLoss[0m : 10.26812
[1mStep[0m  [20/42], [94mLoss[0m : 10.52621
[1mStep[0m  [24/42], [94mLoss[0m : 10.39232
[1mStep[0m  [28/42], [94mLoss[0m : 10.20516
[1mStep[0m  [32/42], [94mLoss[0m : 10.21775
[1mStep[0m  [36/42], [94mLoss[0m : 10.58236
[1mStep[0m  [40/42], [94mLoss[0m : 10.52077

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.281, [92mTest[0m: 10.188, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32043
[1mStep[0m  [4/42], [94mLoss[0m : 10.39856
[1mStep[0m  [8/42], [94mLoss[0m : 10.55984
[1mStep[0m  [12/42], [94mLoss[0m : 10.49690
[1mStep[0m  [16/42], [94mLoss[0m : 10.10979
[1mStep[0m  [20/42], [94mLoss[0m : 10.13434
[1mStep[0m  [24/42], [94mLoss[0m : 10.47996
[1mStep[0m  [28/42], [94mLoss[0m : 10.27427
[1mStep[0m  [32/42], [94mLoss[0m : 10.09857
[1mStep[0m  [36/42], [94mLoss[0m : 10.29100
[1mStep[0m  [40/42], [94mLoss[0m : 10.18056

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.279, [92mTest[0m: 10.168, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.112
====================================

Phase 2 - Evaluation MAE:  10.112481662205287
MAE score P1      10.551031
MAE score P2      10.112482
loss               10.27871
learning_rate        0.0001
batch_size              256
hidden_sizes          [250]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.97274
[1mStep[0m  [2/21], [94mLoss[0m : 10.88113
[1mStep[0m  [4/21], [94mLoss[0m : 10.74964
[1mStep[0m  [6/21], [94mLoss[0m : 11.01050
[1mStep[0m  [8/21], [94mLoss[0m : 10.83226
[1mStep[0m  [10/21], [94mLoss[0m : 10.80913
[1mStep[0m  [12/21], [94mLoss[0m : 11.09075
[1mStep[0m  [14/21], [94mLoss[0m : 11.02156
[1mStep[0m  [16/21], [94mLoss[0m : 10.70485
[1mStep[0m  [18/21], [94mLoss[0m : 10.93973
[1mStep[0m  [20/21], [94mLoss[0m : 10.93730

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.878, [92mTest[0m: 10.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99590
[1mStep[0m  [2/21], [94mLoss[0m : 11.07378
[1mStep[0m  [4/21], [94mLoss[0m : 10.77475
[1mStep[0m  [6/21], [94mLoss[0m : 11.11478
[1mStep[0m  [8/21], [94mLoss[0m : 10.99633
[1mStep[0m  [10/21], [94mLoss[0m : 11.30933
[1mStep[0m  [12/21], [94mLoss[0m : 10.79878
[1mStep[0m  [14/21], [94mLoss[0m : 11.01757
[1mStep[0m  [16/21], [94mLoss[0m : 10.62740
[1mStep[0m  [18/21], [94mLoss[0m : 10.79787
[1mStep[0m  [20/21], [94mLoss[0m : 10.69762

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.865, [92mTest[0m: 10.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74831
[1mStep[0m  [2/21], [94mLoss[0m : 10.88974
[1mStep[0m  [4/21], [94mLoss[0m : 10.68114
[1mStep[0m  [6/21], [94mLoss[0m : 10.85163
[1mStep[0m  [8/21], [94mLoss[0m : 11.04019
[1mStep[0m  [10/21], [94mLoss[0m : 11.21480
[1mStep[0m  [12/21], [94mLoss[0m : 10.87417
[1mStep[0m  [14/21], [94mLoss[0m : 11.06043
[1mStep[0m  [16/21], [94mLoss[0m : 10.72441
[1mStep[0m  [18/21], [94mLoss[0m : 10.98875
[1mStep[0m  [20/21], [94mLoss[0m : 10.41391

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.866, [92mTest[0m: 10.854, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.83000
[1mStep[0m  [2/21], [94mLoss[0m : 10.84226
[1mStep[0m  [4/21], [94mLoss[0m : 10.83471
[1mStep[0m  [6/21], [94mLoss[0m : 10.66128
[1mStep[0m  [8/21], [94mLoss[0m : 11.13410
[1mStep[0m  [10/21], [94mLoss[0m : 10.55437
[1mStep[0m  [12/21], [94mLoss[0m : 11.00154
[1mStep[0m  [14/21], [94mLoss[0m : 10.82792
[1mStep[0m  [16/21], [94mLoss[0m : 10.82428
[1mStep[0m  [18/21], [94mLoss[0m : 10.87931
[1mStep[0m  [20/21], [94mLoss[0m : 10.58682

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.852, [92mTest[0m: 10.861, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70198
[1mStep[0m  [2/21], [94mLoss[0m : 10.70302
[1mStep[0m  [4/21], [94mLoss[0m : 10.75599
[1mStep[0m  [6/21], [94mLoss[0m : 10.91622
[1mStep[0m  [8/21], [94mLoss[0m : 10.91733
[1mStep[0m  [10/21], [94mLoss[0m : 10.98491
[1mStep[0m  [12/21], [94mLoss[0m : 10.98895
[1mStep[0m  [14/21], [94mLoss[0m : 10.69456
[1mStep[0m  [16/21], [94mLoss[0m : 10.83839
[1mStep[0m  [18/21], [94mLoss[0m : 10.84143
[1mStep[0m  [20/21], [94mLoss[0m : 10.72627

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.853, [92mTest[0m: 10.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.10728
[1mStep[0m  [2/21], [94mLoss[0m : 10.99936
[1mStep[0m  [4/21], [94mLoss[0m : 10.72207
[1mStep[0m  [6/21], [94mLoss[0m : 10.98158
[1mStep[0m  [8/21], [94mLoss[0m : 10.54177
[1mStep[0m  [10/21], [94mLoss[0m : 10.88531
[1mStep[0m  [12/21], [94mLoss[0m : 10.81360
[1mStep[0m  [14/21], [94mLoss[0m : 10.91006
[1mStep[0m  [16/21], [94mLoss[0m : 10.85317
[1mStep[0m  [18/21], [94mLoss[0m : 10.65299
[1mStep[0m  [20/21], [94mLoss[0m : 10.98329

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.844, [92mTest[0m: 10.822, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72010
[1mStep[0m  [2/21], [94mLoss[0m : 10.79080
[1mStep[0m  [4/21], [94mLoss[0m : 10.94122
[1mStep[0m  [6/21], [94mLoss[0m : 10.69622
[1mStep[0m  [8/21], [94mLoss[0m : 10.78810
[1mStep[0m  [10/21], [94mLoss[0m : 10.98349
[1mStep[0m  [12/21], [94mLoss[0m : 10.98677
[1mStep[0m  [14/21], [94mLoss[0m : 10.77587
[1mStep[0m  [16/21], [94mLoss[0m : 11.15075
[1mStep[0m  [18/21], [94mLoss[0m : 10.65161
[1mStep[0m  [20/21], [94mLoss[0m : 10.80118

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.846, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77000
[1mStep[0m  [2/21], [94mLoss[0m : 11.09324
[1mStep[0m  [4/21], [94mLoss[0m : 11.09779
[1mStep[0m  [6/21], [94mLoss[0m : 10.98582
[1mStep[0m  [8/21], [94mLoss[0m : 10.77237
[1mStep[0m  [10/21], [94mLoss[0m : 10.82796
[1mStep[0m  [12/21], [94mLoss[0m : 10.76569
[1mStep[0m  [14/21], [94mLoss[0m : 10.76996
[1mStep[0m  [16/21], [94mLoss[0m : 10.80751
[1mStep[0m  [18/21], [94mLoss[0m : 10.70316
[1mStep[0m  [20/21], [94mLoss[0m : 11.26837

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.14415
[1mStep[0m  [2/21], [94mLoss[0m : 10.77735
[1mStep[0m  [4/21], [94mLoss[0m : 10.84417
[1mStep[0m  [6/21], [94mLoss[0m : 10.66930
[1mStep[0m  [8/21], [94mLoss[0m : 10.88417
[1mStep[0m  [10/21], [94mLoss[0m : 10.83279
[1mStep[0m  [12/21], [94mLoss[0m : 10.59071
[1mStep[0m  [14/21], [94mLoss[0m : 10.79601
[1mStep[0m  [16/21], [94mLoss[0m : 10.85921
[1mStep[0m  [18/21], [94mLoss[0m : 11.11592
[1mStep[0m  [20/21], [94mLoss[0m : 10.61709

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82457
[1mStep[0m  [2/21], [94mLoss[0m : 10.83295
[1mStep[0m  [4/21], [94mLoss[0m : 10.57803
[1mStep[0m  [6/21], [94mLoss[0m : 10.68434
[1mStep[0m  [8/21], [94mLoss[0m : 10.84425
[1mStep[0m  [10/21], [94mLoss[0m : 10.95699
[1mStep[0m  [12/21], [94mLoss[0m : 10.74031
[1mStep[0m  [14/21], [94mLoss[0m : 10.69013
[1mStep[0m  [16/21], [94mLoss[0m : 10.62759
[1mStep[0m  [18/21], [94mLoss[0m : 11.08183
[1mStep[0m  [20/21], [94mLoss[0m : 10.86856

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.829, [92mTest[0m: 10.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99295
[1mStep[0m  [2/21], [94mLoss[0m : 11.05340
[1mStep[0m  [4/21], [94mLoss[0m : 10.82736
[1mStep[0m  [6/21], [94mLoss[0m : 10.75204
[1mStep[0m  [8/21], [94mLoss[0m : 10.73418
[1mStep[0m  [10/21], [94mLoss[0m : 10.70277
[1mStep[0m  [12/21], [94mLoss[0m : 10.90381
[1mStep[0m  [14/21], [94mLoss[0m : 10.83977
[1mStep[0m  [16/21], [94mLoss[0m : 10.59590
[1mStep[0m  [18/21], [94mLoss[0m : 11.04809
[1mStep[0m  [20/21], [94mLoss[0m : 11.04772

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74655
[1mStep[0m  [2/21], [94mLoss[0m : 11.16893
[1mStep[0m  [4/21], [94mLoss[0m : 10.88108
[1mStep[0m  [6/21], [94mLoss[0m : 10.67094
[1mStep[0m  [8/21], [94mLoss[0m : 10.70607
[1mStep[0m  [10/21], [94mLoss[0m : 10.83278
[1mStep[0m  [12/21], [94mLoss[0m : 10.87657
[1mStep[0m  [14/21], [94mLoss[0m : 10.48971
[1mStep[0m  [16/21], [94mLoss[0m : 10.74745
[1mStep[0m  [18/21], [94mLoss[0m : 10.68479
[1mStep[0m  [20/21], [94mLoss[0m : 10.58746

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.809, [92mTest[0m: 10.796, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.95784
[1mStep[0m  [2/21], [94mLoss[0m : 10.72557
[1mStep[0m  [4/21], [94mLoss[0m : 10.83907
[1mStep[0m  [6/21], [94mLoss[0m : 10.76302
[1mStep[0m  [8/21], [94mLoss[0m : 10.46969
[1mStep[0m  [10/21], [94mLoss[0m : 10.75492
[1mStep[0m  [12/21], [94mLoss[0m : 10.72535
[1mStep[0m  [14/21], [94mLoss[0m : 10.66922
[1mStep[0m  [16/21], [94mLoss[0m : 10.97342
[1mStep[0m  [18/21], [94mLoss[0m : 10.86422
[1mStep[0m  [20/21], [94mLoss[0m : 11.04569

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.813, [92mTest[0m: 10.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88750
[1mStep[0m  [2/21], [94mLoss[0m : 10.92489
[1mStep[0m  [4/21], [94mLoss[0m : 10.78721
[1mStep[0m  [6/21], [94mLoss[0m : 10.84597
[1mStep[0m  [8/21], [94mLoss[0m : 10.94038
[1mStep[0m  [10/21], [94mLoss[0m : 10.84404
[1mStep[0m  [12/21], [94mLoss[0m : 10.85047
[1mStep[0m  [14/21], [94mLoss[0m : 10.64125
[1mStep[0m  [16/21], [94mLoss[0m : 10.79605
[1mStep[0m  [18/21], [94mLoss[0m : 10.67026
[1mStep[0m  [20/21], [94mLoss[0m : 10.64728

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.791, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62194
[1mStep[0m  [2/21], [94mLoss[0m : 10.86858
[1mStep[0m  [4/21], [94mLoss[0m : 10.45924
[1mStep[0m  [6/21], [94mLoss[0m : 10.42319
[1mStep[0m  [8/21], [94mLoss[0m : 10.69595
[1mStep[0m  [10/21], [94mLoss[0m : 11.02474
[1mStep[0m  [12/21], [94mLoss[0m : 10.85176
[1mStep[0m  [14/21], [94mLoss[0m : 10.72346
[1mStep[0m  [16/21], [94mLoss[0m : 10.76348
[1mStep[0m  [18/21], [94mLoss[0m : 10.88430
[1mStep[0m  [20/21], [94mLoss[0m : 10.60996

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.804, [92mTest[0m: 10.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88978
[1mStep[0m  [2/21], [94mLoss[0m : 10.99958
[1mStep[0m  [4/21], [94mLoss[0m : 10.62653
[1mStep[0m  [6/21], [94mLoss[0m : 10.74958
[1mStep[0m  [8/21], [94mLoss[0m : 11.03302
[1mStep[0m  [10/21], [94mLoss[0m : 10.91824
[1mStep[0m  [12/21], [94mLoss[0m : 10.82502
[1mStep[0m  [14/21], [94mLoss[0m : 10.76007
[1mStep[0m  [16/21], [94mLoss[0m : 10.91199
[1mStep[0m  [18/21], [94mLoss[0m : 10.69528
[1mStep[0m  [20/21], [94mLoss[0m : 10.91981

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.798, [92mTest[0m: 10.770, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.14226
[1mStep[0m  [2/21], [94mLoss[0m : 10.49918
[1mStep[0m  [4/21], [94mLoss[0m : 11.11569
[1mStep[0m  [6/21], [94mLoss[0m : 10.90954
[1mStep[0m  [8/21], [94mLoss[0m : 10.74473
[1mStep[0m  [10/21], [94mLoss[0m : 10.99401
[1mStep[0m  [12/21], [94mLoss[0m : 10.94290
[1mStep[0m  [14/21], [94mLoss[0m : 10.62062
[1mStep[0m  [16/21], [94mLoss[0m : 10.86662
[1mStep[0m  [18/21], [94mLoss[0m : 10.60919
[1mStep[0m  [20/21], [94mLoss[0m : 10.88444

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.790, [92mTest[0m: 10.761, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68396
[1mStep[0m  [2/21], [94mLoss[0m : 10.88458
[1mStep[0m  [4/21], [94mLoss[0m : 10.92850
[1mStep[0m  [6/21], [94mLoss[0m : 10.79274
[1mStep[0m  [8/21], [94mLoss[0m : 10.68675
[1mStep[0m  [10/21], [94mLoss[0m : 10.83173
[1mStep[0m  [12/21], [94mLoss[0m : 10.80496
[1mStep[0m  [14/21], [94mLoss[0m : 11.08104
[1mStep[0m  [16/21], [94mLoss[0m : 10.73668
[1mStep[0m  [18/21], [94mLoss[0m : 10.54101
[1mStep[0m  [20/21], [94mLoss[0m : 10.73926

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.789, [92mTest[0m: 10.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81469
[1mStep[0m  [2/21], [94mLoss[0m : 10.66331
[1mStep[0m  [4/21], [94mLoss[0m : 10.68940
[1mStep[0m  [6/21], [94mLoss[0m : 11.10332
[1mStep[0m  [8/21], [94mLoss[0m : 10.72316
[1mStep[0m  [10/21], [94mLoss[0m : 10.76016
[1mStep[0m  [12/21], [94mLoss[0m : 10.60785
[1mStep[0m  [14/21], [94mLoss[0m : 10.79904
[1mStep[0m  [16/21], [94mLoss[0m : 10.93945
[1mStep[0m  [18/21], [94mLoss[0m : 11.19710
[1mStep[0m  [20/21], [94mLoss[0m : 10.57405

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.777, [92mTest[0m: 10.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60272
[1mStep[0m  [2/21], [94mLoss[0m : 10.66787
[1mStep[0m  [4/21], [94mLoss[0m : 10.67510
[1mStep[0m  [6/21], [94mLoss[0m : 10.80907
[1mStep[0m  [8/21], [94mLoss[0m : 10.69847
[1mStep[0m  [10/21], [94mLoss[0m : 11.08875
[1mStep[0m  [12/21], [94mLoss[0m : 11.18092
[1mStep[0m  [14/21], [94mLoss[0m : 10.57906
[1mStep[0m  [16/21], [94mLoss[0m : 10.70693
[1mStep[0m  [18/21], [94mLoss[0m : 10.95984
[1mStep[0m  [20/21], [94mLoss[0m : 11.02462

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.747, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47818
[1mStep[0m  [2/21], [94mLoss[0m : 10.83256
[1mStep[0m  [4/21], [94mLoss[0m : 10.84228
[1mStep[0m  [6/21], [94mLoss[0m : 10.63134
[1mStep[0m  [8/21], [94mLoss[0m : 10.85241
[1mStep[0m  [10/21], [94mLoss[0m : 10.86176
[1mStep[0m  [12/21], [94mLoss[0m : 10.83864
[1mStep[0m  [14/21], [94mLoss[0m : 10.61283
[1mStep[0m  [16/21], [94mLoss[0m : 10.89151
[1mStep[0m  [18/21], [94mLoss[0m : 11.07480
[1mStep[0m  [20/21], [94mLoss[0m : 10.95278

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.752, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59239
[1mStep[0m  [2/21], [94mLoss[0m : 10.68226
[1mStep[0m  [4/21], [94mLoss[0m : 10.81708
[1mStep[0m  [6/21], [94mLoss[0m : 10.71097
[1mStep[0m  [8/21], [94mLoss[0m : 10.83643
[1mStep[0m  [10/21], [94mLoss[0m : 10.51433
[1mStep[0m  [12/21], [94mLoss[0m : 10.58420
[1mStep[0m  [14/21], [94mLoss[0m : 10.62643
[1mStep[0m  [16/21], [94mLoss[0m : 10.79380
[1mStep[0m  [18/21], [94mLoss[0m : 11.10456
[1mStep[0m  [20/21], [94mLoss[0m : 10.81960

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.769, [92mTest[0m: 10.748, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69480
[1mStep[0m  [2/21], [94mLoss[0m : 10.42045
[1mStep[0m  [4/21], [94mLoss[0m : 10.79322
[1mStep[0m  [6/21], [94mLoss[0m : 10.89722
[1mStep[0m  [8/21], [94mLoss[0m : 11.04687
[1mStep[0m  [10/21], [94mLoss[0m : 10.54037
[1mStep[0m  [12/21], [94mLoss[0m : 10.87222
[1mStep[0m  [14/21], [94mLoss[0m : 11.08016
[1mStep[0m  [16/21], [94mLoss[0m : 10.54079
[1mStep[0m  [18/21], [94mLoss[0m : 10.62245
[1mStep[0m  [20/21], [94mLoss[0m : 11.00585

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.770, [92mTest[0m: 10.732, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70584
[1mStep[0m  [2/21], [94mLoss[0m : 10.77142
[1mStep[0m  [4/21], [94mLoss[0m : 10.97752
[1mStep[0m  [6/21], [94mLoss[0m : 10.63959
[1mStep[0m  [8/21], [94mLoss[0m : 10.86617
[1mStep[0m  [10/21], [94mLoss[0m : 10.77967
[1mStep[0m  [12/21], [94mLoss[0m : 10.68880
[1mStep[0m  [14/21], [94mLoss[0m : 10.78868
[1mStep[0m  [16/21], [94mLoss[0m : 10.67839
[1mStep[0m  [18/21], [94mLoss[0m : 10.32012
[1mStep[0m  [20/21], [94mLoss[0m : 10.44729

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.714, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77361
[1mStep[0m  [2/21], [94mLoss[0m : 10.58418
[1mStep[0m  [4/21], [94mLoss[0m : 10.98382
[1mStep[0m  [6/21], [94mLoss[0m : 10.75637
[1mStep[0m  [8/21], [94mLoss[0m : 10.75233
[1mStep[0m  [10/21], [94mLoss[0m : 10.79197
[1mStep[0m  [12/21], [94mLoss[0m : 10.66937
[1mStep[0m  [14/21], [94mLoss[0m : 10.63094
[1mStep[0m  [16/21], [94mLoss[0m : 10.50897
[1mStep[0m  [18/21], [94mLoss[0m : 10.50279
[1mStep[0m  [20/21], [94mLoss[0m : 10.92007

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.713, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62030
[1mStep[0m  [2/21], [94mLoss[0m : 10.71788
[1mStep[0m  [4/21], [94mLoss[0m : 10.73981
[1mStep[0m  [6/21], [94mLoss[0m : 10.73691
[1mStep[0m  [8/21], [94mLoss[0m : 10.63311
[1mStep[0m  [10/21], [94mLoss[0m : 10.76714
[1mStep[0m  [12/21], [94mLoss[0m : 10.72818
[1mStep[0m  [14/21], [94mLoss[0m : 10.97577
[1mStep[0m  [16/21], [94mLoss[0m : 10.81167
[1mStep[0m  [18/21], [94mLoss[0m : 10.85871
[1mStep[0m  [20/21], [94mLoss[0m : 10.78481

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.756, [92mTest[0m: 10.717, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81124
[1mStep[0m  [2/21], [94mLoss[0m : 10.73811
[1mStep[0m  [4/21], [94mLoss[0m : 10.87908
[1mStep[0m  [6/21], [94mLoss[0m : 10.97276
[1mStep[0m  [8/21], [94mLoss[0m : 10.83374
[1mStep[0m  [10/21], [94mLoss[0m : 10.72976
[1mStep[0m  [12/21], [94mLoss[0m : 10.92404
[1mStep[0m  [14/21], [94mLoss[0m : 10.65051
[1mStep[0m  [16/21], [94mLoss[0m : 10.67105
[1mStep[0m  [18/21], [94mLoss[0m : 10.58202
[1mStep[0m  [20/21], [94mLoss[0m : 10.82082

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.716, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71320
[1mStep[0m  [2/21], [94mLoss[0m : 11.05269
[1mStep[0m  [4/21], [94mLoss[0m : 10.85292
[1mStep[0m  [6/21], [94mLoss[0m : 10.74537
[1mStep[0m  [8/21], [94mLoss[0m : 10.85665
[1mStep[0m  [10/21], [94mLoss[0m : 10.39832
[1mStep[0m  [12/21], [94mLoss[0m : 10.74593
[1mStep[0m  [14/21], [94mLoss[0m : 10.60846
[1mStep[0m  [16/21], [94mLoss[0m : 10.85725
[1mStep[0m  [18/21], [94mLoss[0m : 10.71033
[1mStep[0m  [20/21], [94mLoss[0m : 10.61804

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.733, [92mTest[0m: 10.700, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63210
[1mStep[0m  [2/21], [94mLoss[0m : 10.79954
[1mStep[0m  [4/21], [94mLoss[0m : 10.61071
[1mStep[0m  [6/21], [94mLoss[0m : 10.76204
[1mStep[0m  [8/21], [94mLoss[0m : 10.61598
[1mStep[0m  [10/21], [94mLoss[0m : 10.84784
[1mStep[0m  [12/21], [94mLoss[0m : 10.69270
[1mStep[0m  [14/21], [94mLoss[0m : 10.86425
[1mStep[0m  [16/21], [94mLoss[0m : 10.75255
[1mStep[0m  [18/21], [94mLoss[0m : 10.88775
[1mStep[0m  [20/21], [94mLoss[0m : 10.70501

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.736, [92mTest[0m: 10.686, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82901
[1mStep[0m  [2/21], [94mLoss[0m : 10.96388
[1mStep[0m  [4/21], [94mLoss[0m : 10.68603
[1mStep[0m  [6/21], [94mLoss[0m : 10.71745
[1mStep[0m  [8/21], [94mLoss[0m : 10.73454
[1mStep[0m  [10/21], [94mLoss[0m : 10.70594
[1mStep[0m  [12/21], [94mLoss[0m : 10.48967
[1mStep[0m  [14/21], [94mLoss[0m : 10.44853
[1mStep[0m  [16/21], [94mLoss[0m : 10.75822
[1mStep[0m  [18/21], [94mLoss[0m : 10.92238
[1mStep[0m  [20/21], [94mLoss[0m : 10.78607

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.728, [92mTest[0m: 10.700, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.675
====================================

Phase 1 - Evaluation MAE:  10.675141743251256
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.54160
[1mStep[0m  [2/21], [94mLoss[0m : 11.00776
[1mStep[0m  [4/21], [94mLoss[0m : 10.74431
[1mStep[0m  [6/21], [94mLoss[0m : 10.71904
[1mStep[0m  [8/21], [94mLoss[0m : 10.86288
[1mStep[0m  [10/21], [94mLoss[0m : 10.89097
[1mStep[0m  [12/21], [94mLoss[0m : 10.49796
[1mStep[0m  [14/21], [94mLoss[0m : 10.82873
[1mStep[0m  [16/21], [94mLoss[0m : 10.57245
[1mStep[0m  [18/21], [94mLoss[0m : 10.76244
[1mStep[0m  [20/21], [94mLoss[0m : 10.73077

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.722, [92mTest[0m: 10.690, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65586
[1mStep[0m  [2/21], [94mLoss[0m : 10.59143
[1mStep[0m  [4/21], [94mLoss[0m : 10.62750
[1mStep[0m  [6/21], [94mLoss[0m : 10.71857
[1mStep[0m  [8/21], [94mLoss[0m : 10.52463
[1mStep[0m  [10/21], [94mLoss[0m : 10.62641
[1mStep[0m  [12/21], [94mLoss[0m : 11.00536
[1mStep[0m  [14/21], [94mLoss[0m : 10.82856
[1mStep[0m  [16/21], [94mLoss[0m : 10.79477
[1mStep[0m  [18/21], [94mLoss[0m : 10.68137
[1mStep[0m  [20/21], [94mLoss[0m : 10.76824

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.718, [92mTest[0m: 10.690, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.41569
[1mStep[0m  [2/21], [94mLoss[0m : 10.63443
[1mStep[0m  [4/21], [94mLoss[0m : 10.77760
[1mStep[0m  [6/21], [94mLoss[0m : 10.64114
[1mStep[0m  [8/21], [94mLoss[0m : 10.97195
[1mStep[0m  [10/21], [94mLoss[0m : 10.65229
[1mStep[0m  [12/21], [94mLoss[0m : 10.61144
[1mStep[0m  [14/21], [94mLoss[0m : 11.10056
[1mStep[0m  [16/21], [94mLoss[0m : 10.57015
[1mStep[0m  [18/21], [94mLoss[0m : 10.62700
[1mStep[0m  [20/21], [94mLoss[0m : 10.68847

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.712, [92mTest[0m: 10.678, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58421
[1mStep[0m  [2/21], [94mLoss[0m : 10.75341
[1mStep[0m  [4/21], [94mLoss[0m : 10.58926
[1mStep[0m  [6/21], [94mLoss[0m : 10.79803
[1mStep[0m  [8/21], [94mLoss[0m : 10.69674
[1mStep[0m  [10/21], [94mLoss[0m : 10.80168
[1mStep[0m  [12/21], [94mLoss[0m : 10.70765
[1mStep[0m  [14/21], [94mLoss[0m : 10.85095
[1mStep[0m  [16/21], [94mLoss[0m : 10.82498
[1mStep[0m  [18/21], [94mLoss[0m : 10.83031
[1mStep[0m  [20/21], [94mLoss[0m : 10.54176

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.699, [92mTest[0m: 10.673, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51044
[1mStep[0m  [2/21], [94mLoss[0m : 10.71379
[1mStep[0m  [4/21], [94mLoss[0m : 10.83385
[1mStep[0m  [6/21], [94mLoss[0m : 11.01928
[1mStep[0m  [8/21], [94mLoss[0m : 10.88634
[1mStep[0m  [10/21], [94mLoss[0m : 10.93216
[1mStep[0m  [12/21], [94mLoss[0m : 10.74979
[1mStep[0m  [14/21], [94mLoss[0m : 10.64549
[1mStep[0m  [16/21], [94mLoss[0m : 10.82732
[1mStep[0m  [18/21], [94mLoss[0m : 10.67013
[1mStep[0m  [20/21], [94mLoss[0m : 10.57536

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50694
[1mStep[0m  [2/21], [94mLoss[0m : 10.59331
[1mStep[0m  [4/21], [94mLoss[0m : 10.60526
[1mStep[0m  [6/21], [94mLoss[0m : 10.44475
[1mStep[0m  [8/21], [94mLoss[0m : 10.74913
[1mStep[0m  [10/21], [94mLoss[0m : 10.86604
[1mStep[0m  [12/21], [94mLoss[0m : 10.63548
[1mStep[0m  [14/21], [94mLoss[0m : 10.71393
[1mStep[0m  [16/21], [94mLoss[0m : 10.57153
[1mStep[0m  [18/21], [94mLoss[0m : 10.76623
[1mStep[0m  [20/21], [94mLoss[0m : 10.87947

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.693, [92mTest[0m: 10.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88377
[1mStep[0m  [2/21], [94mLoss[0m : 10.60244
[1mStep[0m  [4/21], [94mLoss[0m : 10.43854
[1mStep[0m  [6/21], [94mLoss[0m : 10.80113
[1mStep[0m  [8/21], [94mLoss[0m : 10.85663
[1mStep[0m  [10/21], [94mLoss[0m : 10.78886
[1mStep[0m  [12/21], [94mLoss[0m : 10.54982
[1mStep[0m  [14/21], [94mLoss[0m : 10.70654
[1mStep[0m  [16/21], [94mLoss[0m : 10.60905
[1mStep[0m  [18/21], [94mLoss[0m : 10.56289
[1mStep[0m  [20/21], [94mLoss[0m : 10.61591

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.696, [92mTest[0m: 10.651, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67408
[1mStep[0m  [2/21], [94mLoss[0m : 10.57002
[1mStep[0m  [4/21], [94mLoss[0m : 10.73054
[1mStep[0m  [6/21], [94mLoss[0m : 10.66355
[1mStep[0m  [8/21], [94mLoss[0m : 10.54968
[1mStep[0m  [10/21], [94mLoss[0m : 10.76022
[1mStep[0m  [12/21], [94mLoss[0m : 10.80333
[1mStep[0m  [14/21], [94mLoss[0m : 10.56370
[1mStep[0m  [16/21], [94mLoss[0m : 10.73612
[1mStep[0m  [18/21], [94mLoss[0m : 10.90380
[1mStep[0m  [20/21], [94mLoss[0m : 10.81951

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.691, [92mTest[0m: 10.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51993
[1mStep[0m  [2/21], [94mLoss[0m : 10.71368
[1mStep[0m  [4/21], [94mLoss[0m : 10.67369
[1mStep[0m  [6/21], [94mLoss[0m : 10.58509
[1mStep[0m  [8/21], [94mLoss[0m : 11.00939
[1mStep[0m  [10/21], [94mLoss[0m : 10.68914
[1mStep[0m  [12/21], [94mLoss[0m : 10.65144
[1mStep[0m  [14/21], [94mLoss[0m : 10.45688
[1mStep[0m  [16/21], [94mLoss[0m : 10.61646
[1mStep[0m  [18/21], [94mLoss[0m : 11.00187
[1mStep[0m  [20/21], [94mLoss[0m : 10.79640

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.649, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69953
[1mStep[0m  [2/21], [94mLoss[0m : 10.69909
[1mStep[0m  [4/21], [94mLoss[0m : 10.69924
[1mStep[0m  [6/21], [94mLoss[0m : 10.56339
[1mStep[0m  [8/21], [94mLoss[0m : 10.60132
[1mStep[0m  [10/21], [94mLoss[0m : 10.54918
[1mStep[0m  [12/21], [94mLoss[0m : 10.70408
[1mStep[0m  [14/21], [94mLoss[0m : 10.39499
[1mStep[0m  [16/21], [94mLoss[0m : 10.66983
[1mStep[0m  [18/21], [94mLoss[0m : 10.76100
[1mStep[0m  [20/21], [94mLoss[0m : 10.80244

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.677, [92mTest[0m: 10.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.64780
[1mStep[0m  [2/21], [94mLoss[0m : 10.58250
[1mStep[0m  [4/21], [94mLoss[0m : 10.91204
[1mStep[0m  [6/21], [94mLoss[0m : 10.72967
[1mStep[0m  [8/21], [94mLoss[0m : 10.58343
[1mStep[0m  [10/21], [94mLoss[0m : 10.58646
[1mStep[0m  [12/21], [94mLoss[0m : 10.58706
[1mStep[0m  [14/21], [94mLoss[0m : 10.54700
[1mStep[0m  [16/21], [94mLoss[0m : 10.67874
[1mStep[0m  [18/21], [94mLoss[0m : 10.79268
[1mStep[0m  [20/21], [94mLoss[0m : 10.45678

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.667, [92mTest[0m: 10.624, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79998
[1mStep[0m  [2/21], [94mLoss[0m : 10.58756
[1mStep[0m  [4/21], [94mLoss[0m : 10.64288
[1mStep[0m  [6/21], [94mLoss[0m : 10.63147
[1mStep[0m  [8/21], [94mLoss[0m : 10.56773
[1mStep[0m  [10/21], [94mLoss[0m : 10.65458
[1mStep[0m  [12/21], [94mLoss[0m : 10.63181
[1mStep[0m  [14/21], [94mLoss[0m : 10.83534
[1mStep[0m  [16/21], [94mLoss[0m : 10.69713
[1mStep[0m  [18/21], [94mLoss[0m : 10.88961
[1mStep[0m  [20/21], [94mLoss[0m : 10.49954

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.655, [92mTest[0m: 10.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54333
[1mStep[0m  [2/21], [94mLoss[0m : 10.62961
[1mStep[0m  [4/21], [94mLoss[0m : 10.60168
[1mStep[0m  [6/21], [94mLoss[0m : 10.52088
[1mStep[0m  [8/21], [94mLoss[0m : 11.01348
[1mStep[0m  [10/21], [94mLoss[0m : 10.50978
[1mStep[0m  [12/21], [94mLoss[0m : 10.59679
[1mStep[0m  [14/21], [94mLoss[0m : 10.73445
[1mStep[0m  [16/21], [94mLoss[0m : 11.00506
[1mStep[0m  [18/21], [94mLoss[0m : 10.49004
[1mStep[0m  [20/21], [94mLoss[0m : 10.46120

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.660, [92mTest[0m: 10.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.84697
[1mStep[0m  [2/21], [94mLoss[0m : 10.50829
[1mStep[0m  [4/21], [94mLoss[0m : 10.54857
[1mStep[0m  [6/21], [94mLoss[0m : 10.51216
[1mStep[0m  [8/21], [94mLoss[0m : 10.86953
[1mStep[0m  [10/21], [94mLoss[0m : 10.65349
[1mStep[0m  [12/21], [94mLoss[0m : 10.57055
[1mStep[0m  [14/21], [94mLoss[0m : 10.59487
[1mStep[0m  [16/21], [94mLoss[0m : 10.63639
[1mStep[0m  [18/21], [94mLoss[0m : 10.85189
[1mStep[0m  [20/21], [94mLoss[0m : 10.69067

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.656, [92mTest[0m: 10.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59219
[1mStep[0m  [2/21], [94mLoss[0m : 10.59704
[1mStep[0m  [4/21], [94mLoss[0m : 10.54610
[1mStep[0m  [6/21], [94mLoss[0m : 10.59817
[1mStep[0m  [8/21], [94mLoss[0m : 10.59160
[1mStep[0m  [10/21], [94mLoss[0m : 10.91362
[1mStep[0m  [12/21], [94mLoss[0m : 10.51462
[1mStep[0m  [14/21], [94mLoss[0m : 10.62655
[1mStep[0m  [16/21], [94mLoss[0m : 10.71026
[1mStep[0m  [18/21], [94mLoss[0m : 10.61810
[1mStep[0m  [20/21], [94mLoss[0m : 10.70721

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.654, [92mTest[0m: 10.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75887
[1mStep[0m  [2/21], [94mLoss[0m : 10.62904
[1mStep[0m  [4/21], [94mLoss[0m : 10.42445
[1mStep[0m  [6/21], [94mLoss[0m : 10.91044
[1mStep[0m  [8/21], [94mLoss[0m : 10.76417
[1mStep[0m  [10/21], [94mLoss[0m : 10.38429
[1mStep[0m  [12/21], [94mLoss[0m : 10.56402
[1mStep[0m  [14/21], [94mLoss[0m : 10.65086
[1mStep[0m  [16/21], [94mLoss[0m : 10.75037
[1mStep[0m  [18/21], [94mLoss[0m : 10.64445
[1mStep[0m  [20/21], [94mLoss[0m : 10.37893

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.643, [92mTest[0m: 10.594, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.53658
[1mStep[0m  [2/21], [94mLoss[0m : 10.67337
[1mStep[0m  [4/21], [94mLoss[0m : 10.67324
[1mStep[0m  [6/21], [94mLoss[0m : 10.58317
[1mStep[0m  [8/21], [94mLoss[0m : 10.49213
[1mStep[0m  [10/21], [94mLoss[0m : 10.81606
[1mStep[0m  [12/21], [94mLoss[0m : 10.82809
[1mStep[0m  [14/21], [94mLoss[0m : 10.44040
[1mStep[0m  [16/21], [94mLoss[0m : 10.70665
[1mStep[0m  [18/21], [94mLoss[0m : 10.59467
[1mStep[0m  [20/21], [94mLoss[0m : 10.76260

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.637, [92mTest[0m: 10.573, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82868
[1mStep[0m  [2/21], [94mLoss[0m : 10.58622
[1mStep[0m  [4/21], [94mLoss[0m : 10.74097
[1mStep[0m  [6/21], [94mLoss[0m : 10.29476
[1mStep[0m  [8/21], [94mLoss[0m : 10.50923
[1mStep[0m  [10/21], [94mLoss[0m : 10.47945
[1mStep[0m  [12/21], [94mLoss[0m : 10.39509
[1mStep[0m  [14/21], [94mLoss[0m : 10.79780
[1mStep[0m  [16/21], [94mLoss[0m : 10.65111
[1mStep[0m  [18/21], [94mLoss[0m : 10.76157
[1mStep[0m  [20/21], [94mLoss[0m : 10.86201

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48328
[1mStep[0m  [2/21], [94mLoss[0m : 10.72875
[1mStep[0m  [4/21], [94mLoss[0m : 10.60909
[1mStep[0m  [6/21], [94mLoss[0m : 10.76690
[1mStep[0m  [8/21], [94mLoss[0m : 10.43271
[1mStep[0m  [10/21], [94mLoss[0m : 10.60721
[1mStep[0m  [12/21], [94mLoss[0m : 10.53446
[1mStep[0m  [14/21], [94mLoss[0m : 10.55334
[1mStep[0m  [16/21], [94mLoss[0m : 10.62318
[1mStep[0m  [18/21], [94mLoss[0m : 10.77534
[1mStep[0m  [20/21], [94mLoss[0m : 10.70536

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.630, [92mTest[0m: 10.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61111
[1mStep[0m  [2/21], [94mLoss[0m : 10.67504
[1mStep[0m  [4/21], [94mLoss[0m : 10.68191
[1mStep[0m  [6/21], [94mLoss[0m : 10.57961
[1mStep[0m  [8/21], [94mLoss[0m : 10.29584
[1mStep[0m  [10/21], [94mLoss[0m : 10.73256
[1mStep[0m  [12/21], [94mLoss[0m : 10.62190
[1mStep[0m  [14/21], [94mLoss[0m : 10.48454
[1mStep[0m  [16/21], [94mLoss[0m : 10.47728
[1mStep[0m  [18/21], [94mLoss[0m : 10.45530
[1mStep[0m  [20/21], [94mLoss[0m : 10.72444

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.626, [92mTest[0m: 10.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63621
[1mStep[0m  [2/21], [94mLoss[0m : 10.60907
[1mStep[0m  [4/21], [94mLoss[0m : 10.73018
[1mStep[0m  [6/21], [94mLoss[0m : 10.81149
[1mStep[0m  [8/21], [94mLoss[0m : 10.45297
[1mStep[0m  [10/21], [94mLoss[0m : 10.77956
[1mStep[0m  [12/21], [94mLoss[0m : 10.46272
[1mStep[0m  [14/21], [94mLoss[0m : 10.69327
[1mStep[0m  [16/21], [94mLoss[0m : 10.67217
[1mStep[0m  [18/21], [94mLoss[0m : 10.42138
[1mStep[0m  [20/21], [94mLoss[0m : 10.60683

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.621, [92mTest[0m: 10.549, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61765
[1mStep[0m  [2/21], [94mLoss[0m : 10.55948
[1mStep[0m  [4/21], [94mLoss[0m : 10.45725
[1mStep[0m  [6/21], [94mLoss[0m : 10.74974
[1mStep[0m  [8/21], [94mLoss[0m : 10.59873
[1mStep[0m  [10/21], [94mLoss[0m : 10.72410
[1mStep[0m  [12/21], [94mLoss[0m : 11.05609
[1mStep[0m  [14/21], [94mLoss[0m : 10.43437
[1mStep[0m  [16/21], [94mLoss[0m : 10.36877
[1mStep[0m  [18/21], [94mLoss[0m : 10.60204
[1mStep[0m  [20/21], [94mLoss[0m : 10.57128

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74508
[1mStep[0m  [2/21], [94mLoss[0m : 10.71399
[1mStep[0m  [4/21], [94mLoss[0m : 10.64035
[1mStep[0m  [6/21], [94mLoss[0m : 10.68986
[1mStep[0m  [8/21], [94mLoss[0m : 10.52063
[1mStep[0m  [10/21], [94mLoss[0m : 10.62170
[1mStep[0m  [12/21], [94mLoss[0m : 10.32346
[1mStep[0m  [14/21], [94mLoss[0m : 10.59635
[1mStep[0m  [16/21], [94mLoss[0m : 10.63542
[1mStep[0m  [18/21], [94mLoss[0m : 10.85183
[1mStep[0m  [20/21], [94mLoss[0m : 10.47790

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.546, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39762
[1mStep[0m  [2/21], [94mLoss[0m : 10.68655
[1mStep[0m  [4/21], [94mLoss[0m : 10.58248
[1mStep[0m  [6/21], [94mLoss[0m : 10.27944
[1mStep[0m  [8/21], [94mLoss[0m : 10.53896
[1mStep[0m  [10/21], [94mLoss[0m : 10.73231
[1mStep[0m  [12/21], [94mLoss[0m : 10.66086
[1mStep[0m  [14/21], [94mLoss[0m : 11.07967
[1mStep[0m  [16/21], [94mLoss[0m : 10.59218
[1mStep[0m  [18/21], [94mLoss[0m : 10.88344
[1mStep[0m  [20/21], [94mLoss[0m : 10.47388

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.531, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.57820
[1mStep[0m  [2/21], [94mLoss[0m : 10.27556
[1mStep[0m  [4/21], [94mLoss[0m : 10.26284
[1mStep[0m  [6/21], [94mLoss[0m : 10.63518
[1mStep[0m  [8/21], [94mLoss[0m : 10.51893
[1mStep[0m  [10/21], [94mLoss[0m : 10.51458
[1mStep[0m  [12/21], [94mLoss[0m : 10.56329
[1mStep[0m  [14/21], [94mLoss[0m : 10.57010
[1mStep[0m  [16/21], [94mLoss[0m : 10.66742
[1mStep[0m  [18/21], [94mLoss[0m : 10.56212
[1mStep[0m  [20/21], [94mLoss[0m : 10.58125

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.532, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.35584
[1mStep[0m  [2/21], [94mLoss[0m : 10.44556
[1mStep[0m  [4/21], [94mLoss[0m : 10.55929
[1mStep[0m  [6/21], [94mLoss[0m : 10.28414
[1mStep[0m  [8/21], [94mLoss[0m : 10.48156
[1mStep[0m  [10/21], [94mLoss[0m : 10.87212
[1mStep[0m  [12/21], [94mLoss[0m : 10.59735
[1mStep[0m  [14/21], [94mLoss[0m : 10.33922
[1mStep[0m  [16/21], [94mLoss[0m : 10.77232
[1mStep[0m  [18/21], [94mLoss[0m : 10.62677
[1mStep[0m  [20/21], [94mLoss[0m : 10.41629

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.593, [92mTest[0m: 10.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.43050
[1mStep[0m  [2/21], [94mLoss[0m : 10.76192
[1mStep[0m  [4/21], [94mLoss[0m : 10.84576
[1mStep[0m  [6/21], [94mLoss[0m : 10.44848
[1mStep[0m  [8/21], [94mLoss[0m : 10.59559
[1mStep[0m  [10/21], [94mLoss[0m : 10.47361
[1mStep[0m  [12/21], [94mLoss[0m : 10.58932
[1mStep[0m  [14/21], [94mLoss[0m : 10.59059
[1mStep[0m  [16/21], [94mLoss[0m : 10.71431
[1mStep[0m  [18/21], [94mLoss[0m : 10.54884
[1mStep[0m  [20/21], [94mLoss[0m : 10.48903

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.582, [92mTest[0m: 10.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79590
[1mStep[0m  [2/21], [94mLoss[0m : 10.31556
[1mStep[0m  [4/21], [94mLoss[0m : 10.68075
[1mStep[0m  [6/21], [94mLoss[0m : 10.76614
[1mStep[0m  [8/21], [94mLoss[0m : 10.43988
[1mStep[0m  [10/21], [94mLoss[0m : 10.23541
[1mStep[0m  [12/21], [94mLoss[0m : 10.57053
[1mStep[0m  [14/21], [94mLoss[0m : 10.67169
[1mStep[0m  [16/21], [94mLoss[0m : 10.44151
[1mStep[0m  [18/21], [94mLoss[0m : 10.46642
[1mStep[0m  [20/21], [94mLoss[0m : 10.75868

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.583, [92mTest[0m: 10.515, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72321
[1mStep[0m  [2/21], [94mLoss[0m : 10.51277
[1mStep[0m  [4/21], [94mLoss[0m : 10.48100
[1mStep[0m  [6/21], [94mLoss[0m : 10.74359
[1mStep[0m  [8/21], [94mLoss[0m : 10.67933
[1mStep[0m  [10/21], [94mLoss[0m : 10.34026
[1mStep[0m  [12/21], [94mLoss[0m : 10.36293
[1mStep[0m  [14/21], [94mLoss[0m : 10.53677
[1mStep[0m  [16/21], [94mLoss[0m : 10.60943
[1mStep[0m  [18/21], [94mLoss[0m : 10.74717
[1mStep[0m  [20/21], [94mLoss[0m : 10.60499

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.567, [92mTest[0m: 10.509, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.31405
[1mStep[0m  [2/21], [94mLoss[0m : 10.59729
[1mStep[0m  [4/21], [94mLoss[0m : 10.46753
[1mStep[0m  [6/21], [94mLoss[0m : 10.60018
[1mStep[0m  [8/21], [94mLoss[0m : 10.37397
[1mStep[0m  [10/21], [94mLoss[0m : 10.50184
[1mStep[0m  [12/21], [94mLoss[0m : 10.82570
[1mStep[0m  [14/21], [94mLoss[0m : 10.34484
[1mStep[0m  [16/21], [94mLoss[0m : 10.34978
[1mStep[0m  [18/21], [94mLoss[0m : 10.88975
[1mStep[0m  [20/21], [94mLoss[0m : 10.78924

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.576, [92mTest[0m: 10.491, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.504
====================================

Phase 2 - Evaluation MAE:  10.50377709524972
MAE score P1       10.675142
MAE score P2       10.503777
loss               10.567032
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay          0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.72153
[1mStep[0m  [4/42], [94mLoss[0m : 11.14010
[1mStep[0m  [8/42], [94mLoss[0m : 11.07225
[1mStep[0m  [12/42], [94mLoss[0m : 10.76218
[1mStep[0m  [16/42], [94mLoss[0m : 10.78569
[1mStep[0m  [20/42], [94mLoss[0m : 11.09395
[1mStep[0m  [24/42], [94mLoss[0m : 10.67431
[1mStep[0m  [28/42], [94mLoss[0m : 11.25818
[1mStep[0m  [32/42], [94mLoss[0m : 10.85147
[1mStep[0m  [36/42], [94mLoss[0m : 10.76558
[1mStep[0m  [40/42], [94mLoss[0m : 10.36812

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.925, [92mTest[0m: 11.146, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.77716
[1mStep[0m  [4/42], [94mLoss[0m : 10.85846
[1mStep[0m  [8/42], [94mLoss[0m : 11.14678
[1mStep[0m  [12/42], [94mLoss[0m : 11.02400
[1mStep[0m  [16/42], [94mLoss[0m : 10.71466
[1mStep[0m  [20/42], [94mLoss[0m : 11.17821
[1mStep[0m  [24/42], [94mLoss[0m : 10.70963
[1mStep[0m  [28/42], [94mLoss[0m : 10.83047
[1mStep[0m  [32/42], [94mLoss[0m : 10.41055
[1mStep[0m  [36/42], [94mLoss[0m : 11.06853
[1mStep[0m  [40/42], [94mLoss[0m : 10.80063

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.921, [92mTest[0m: 10.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.62021
[1mStep[0m  [4/42], [94mLoss[0m : 10.72482
[1mStep[0m  [8/42], [94mLoss[0m : 10.92020
[1mStep[0m  [12/42], [94mLoss[0m : 11.37157
[1mStep[0m  [16/42], [94mLoss[0m : 10.79471
[1mStep[0m  [20/42], [94mLoss[0m : 10.61018
[1mStep[0m  [24/42], [94mLoss[0m : 10.90607
[1mStep[0m  [28/42], [94mLoss[0m : 10.84479
[1mStep[0m  [32/42], [94mLoss[0m : 11.30593
[1mStep[0m  [36/42], [94mLoss[0m : 11.27610
[1mStep[0m  [40/42], [94mLoss[0m : 10.74107

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.910, [92mTest[0m: 10.945, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42619
[1mStep[0m  [4/42], [94mLoss[0m : 10.57798
[1mStep[0m  [8/42], [94mLoss[0m : 10.79698
[1mStep[0m  [12/42], [94mLoss[0m : 11.09361
[1mStep[0m  [16/42], [94mLoss[0m : 10.46673
[1mStep[0m  [20/42], [94mLoss[0m : 10.86550
[1mStep[0m  [24/42], [94mLoss[0m : 10.67565
[1mStep[0m  [28/42], [94mLoss[0m : 10.33904
[1mStep[0m  [32/42], [94mLoss[0m : 10.85115
[1mStep[0m  [36/42], [94mLoss[0m : 10.99854
[1mStep[0m  [40/42], [94mLoss[0m : 10.97437

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.904, [92mTest[0m: 10.917, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.27668
[1mStep[0m  [4/42], [94mLoss[0m : 10.47236
[1mStep[0m  [8/42], [94mLoss[0m : 10.62294
[1mStep[0m  [12/42], [94mLoss[0m : 10.89215
[1mStep[0m  [16/42], [94mLoss[0m : 10.75298
[1mStep[0m  [20/42], [94mLoss[0m : 10.64790
[1mStep[0m  [24/42], [94mLoss[0m : 10.86356
[1mStep[0m  [28/42], [94mLoss[0m : 10.32779
[1mStep[0m  [32/42], [94mLoss[0m : 11.12031
[1mStep[0m  [36/42], [94mLoss[0m : 11.16612
[1mStep[0m  [40/42], [94mLoss[0m : 10.68160

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.911, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.19475
[1mStep[0m  [4/42], [94mLoss[0m : 10.80805
[1mStep[0m  [8/42], [94mLoss[0m : 10.41185
[1mStep[0m  [12/42], [94mLoss[0m : 10.84962
[1mStep[0m  [16/42], [94mLoss[0m : 10.87784
[1mStep[0m  [20/42], [94mLoss[0m : 11.07770
[1mStep[0m  [24/42], [94mLoss[0m : 11.03628
[1mStep[0m  [28/42], [94mLoss[0m : 11.09545
[1mStep[0m  [32/42], [94mLoss[0m : 10.87600
[1mStep[0m  [36/42], [94mLoss[0m : 10.96433
[1mStep[0m  [40/42], [94mLoss[0m : 10.97426

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.884, [92mTest[0m: 10.915, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.77847
[1mStep[0m  [4/42], [94mLoss[0m : 10.75077
[1mStep[0m  [8/42], [94mLoss[0m : 10.66595
[1mStep[0m  [12/42], [94mLoss[0m : 10.82129
[1mStep[0m  [16/42], [94mLoss[0m : 10.90729
[1mStep[0m  [20/42], [94mLoss[0m : 11.12855
[1mStep[0m  [24/42], [94mLoss[0m : 10.72354
[1mStep[0m  [28/42], [94mLoss[0m : 10.71703
[1mStep[0m  [32/42], [94mLoss[0m : 10.84565
[1mStep[0m  [36/42], [94mLoss[0m : 11.00301
[1mStep[0m  [40/42], [94mLoss[0m : 10.76984

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.881, [92mTest[0m: 10.886, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.59427
[1mStep[0m  [4/42], [94mLoss[0m : 10.41480
[1mStep[0m  [8/42], [94mLoss[0m : 10.36096
[1mStep[0m  [12/42], [94mLoss[0m : 11.03665
[1mStep[0m  [16/42], [94mLoss[0m : 10.97492
[1mStep[0m  [20/42], [94mLoss[0m : 11.29395
[1mStep[0m  [24/42], [94mLoss[0m : 10.81358
[1mStep[0m  [28/42], [94mLoss[0m : 10.63507
[1mStep[0m  [32/42], [94mLoss[0m : 10.97157
[1mStep[0m  [36/42], [94mLoss[0m : 11.25482
[1mStep[0m  [40/42], [94mLoss[0m : 11.21238

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.868, [92mTest[0m: 10.862, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.06708
[1mStep[0m  [4/42], [94mLoss[0m : 10.61529
[1mStep[0m  [8/42], [94mLoss[0m : 10.85519
[1mStep[0m  [12/42], [94mLoss[0m : 10.90153
[1mStep[0m  [16/42], [94mLoss[0m : 11.05409
[1mStep[0m  [20/42], [94mLoss[0m : 11.01442
[1mStep[0m  [24/42], [94mLoss[0m : 10.95034
[1mStep[0m  [28/42], [94mLoss[0m : 10.83769
[1mStep[0m  [32/42], [94mLoss[0m : 10.73950
[1mStep[0m  [36/42], [94mLoss[0m : 11.11109
[1mStep[0m  [40/42], [94mLoss[0m : 10.76816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.868, [92mTest[0m: 10.869, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68627
[1mStep[0m  [4/42], [94mLoss[0m : 10.89530
[1mStep[0m  [8/42], [94mLoss[0m : 10.72541
[1mStep[0m  [12/42], [94mLoss[0m : 11.00792
[1mStep[0m  [16/42], [94mLoss[0m : 11.30960
[1mStep[0m  [20/42], [94mLoss[0m : 10.98240
[1mStep[0m  [24/42], [94mLoss[0m : 11.00320
[1mStep[0m  [28/42], [94mLoss[0m : 10.98123
[1mStep[0m  [32/42], [94mLoss[0m : 10.87017
[1mStep[0m  [36/42], [94mLoss[0m : 10.79856
[1mStep[0m  [40/42], [94mLoss[0m : 10.70355

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.857, [92mTest[0m: 10.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.89857
[1mStep[0m  [4/42], [94mLoss[0m : 10.86314
[1mStep[0m  [8/42], [94mLoss[0m : 10.96946
[1mStep[0m  [12/42], [94mLoss[0m : 11.18436
[1mStep[0m  [16/42], [94mLoss[0m : 10.71327
[1mStep[0m  [20/42], [94mLoss[0m : 10.84617
[1mStep[0m  [24/42], [94mLoss[0m : 10.91520
[1mStep[0m  [28/42], [94mLoss[0m : 10.76764
[1mStep[0m  [32/42], [94mLoss[0m : 10.80130
[1mStep[0m  [36/42], [94mLoss[0m : 10.90463
[1mStep[0m  [40/42], [94mLoss[0m : 10.51612

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.848, [92mTest[0m: 10.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.14871
[1mStep[0m  [4/42], [94mLoss[0m : 10.83908
[1mStep[0m  [8/42], [94mLoss[0m : 11.10882
[1mStep[0m  [12/42], [94mLoss[0m : 11.11201
[1mStep[0m  [16/42], [94mLoss[0m : 10.68972
[1mStep[0m  [20/42], [94mLoss[0m : 10.73173
[1mStep[0m  [24/42], [94mLoss[0m : 10.71036
[1mStep[0m  [28/42], [94mLoss[0m : 10.87076
[1mStep[0m  [32/42], [94mLoss[0m : 10.90586
[1mStep[0m  [36/42], [94mLoss[0m : 10.65055
[1mStep[0m  [40/42], [94mLoss[0m : 10.42995

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.843, [92mTest[0m: 10.844, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.04430
[1mStep[0m  [4/42], [94mLoss[0m : 10.92739
[1mStep[0m  [8/42], [94mLoss[0m : 10.82386
[1mStep[0m  [12/42], [94mLoss[0m : 10.28621
[1mStep[0m  [16/42], [94mLoss[0m : 10.79950
[1mStep[0m  [20/42], [94mLoss[0m : 11.15023
[1mStep[0m  [24/42], [94mLoss[0m : 11.25574
[1mStep[0m  [28/42], [94mLoss[0m : 10.76231
[1mStep[0m  [32/42], [94mLoss[0m : 11.05777
[1mStep[0m  [36/42], [94mLoss[0m : 10.42788
[1mStep[0m  [40/42], [94mLoss[0m : 10.55328

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.837, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66549
[1mStep[0m  [4/42], [94mLoss[0m : 11.05198
[1mStep[0m  [8/42], [94mLoss[0m : 11.01795
[1mStep[0m  [12/42], [94mLoss[0m : 10.62166
[1mStep[0m  [16/42], [94mLoss[0m : 10.56435
[1mStep[0m  [20/42], [94mLoss[0m : 10.92084
[1mStep[0m  [24/42], [94mLoss[0m : 10.66253
[1mStep[0m  [28/42], [94mLoss[0m : 10.51763
[1mStep[0m  [32/42], [94mLoss[0m : 10.86128
[1mStep[0m  [36/42], [94mLoss[0m : 11.07262
[1mStep[0m  [40/42], [94mLoss[0m : 10.58090

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.835, [92mTest[0m: 10.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94022
[1mStep[0m  [4/42], [94mLoss[0m : 11.03173
[1mStep[0m  [8/42], [94mLoss[0m : 10.77665
[1mStep[0m  [12/42], [94mLoss[0m : 10.26032
[1mStep[0m  [16/42], [94mLoss[0m : 10.28759
[1mStep[0m  [20/42], [94mLoss[0m : 10.87223
[1mStep[0m  [24/42], [94mLoss[0m : 10.61835
[1mStep[0m  [28/42], [94mLoss[0m : 10.53079
[1mStep[0m  [32/42], [94mLoss[0m : 10.55703
[1mStep[0m  [36/42], [94mLoss[0m : 10.86450
[1mStep[0m  [40/42], [94mLoss[0m : 10.84678

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.820, [92mTest[0m: 10.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.98138
[1mStep[0m  [4/42], [94mLoss[0m : 10.88906
[1mStep[0m  [8/42], [94mLoss[0m : 10.88895
[1mStep[0m  [12/42], [94mLoss[0m : 10.70160
[1mStep[0m  [16/42], [94mLoss[0m : 11.25661
[1mStep[0m  [20/42], [94mLoss[0m : 10.68548
[1mStep[0m  [24/42], [94mLoss[0m : 10.79802
[1mStep[0m  [28/42], [94mLoss[0m : 10.70029
[1mStep[0m  [32/42], [94mLoss[0m : 10.81820
[1mStep[0m  [36/42], [94mLoss[0m : 10.94487
[1mStep[0m  [40/42], [94mLoss[0m : 10.60432

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.814, [92mTest[0m: 10.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63752
[1mStep[0m  [4/42], [94mLoss[0m : 10.52975
[1mStep[0m  [8/42], [94mLoss[0m : 10.67454
[1mStep[0m  [12/42], [94mLoss[0m : 10.90941
[1mStep[0m  [16/42], [94mLoss[0m : 10.79511
[1mStep[0m  [20/42], [94mLoss[0m : 10.98380
[1mStep[0m  [24/42], [94mLoss[0m : 10.90849
[1mStep[0m  [28/42], [94mLoss[0m : 10.53138
[1mStep[0m  [32/42], [94mLoss[0m : 10.62202
[1mStep[0m  [36/42], [94mLoss[0m : 11.10981
[1mStep[0m  [40/42], [94mLoss[0m : 10.76153

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.814, [92mTest[0m: 10.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90617
[1mStep[0m  [4/42], [94mLoss[0m : 10.72078
[1mStep[0m  [8/42], [94mLoss[0m : 10.59801
[1mStep[0m  [12/42], [94mLoss[0m : 11.00550
[1mStep[0m  [16/42], [94mLoss[0m : 10.78853
[1mStep[0m  [20/42], [94mLoss[0m : 10.76859
[1mStep[0m  [24/42], [94mLoss[0m : 11.10537
[1mStep[0m  [28/42], [94mLoss[0m : 10.98039
[1mStep[0m  [32/42], [94mLoss[0m : 10.23752
[1mStep[0m  [36/42], [94mLoss[0m : 11.05658
[1mStep[0m  [40/42], [94mLoss[0m : 10.65995

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.803, [92mTest[0m: 10.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71402
[1mStep[0m  [4/42], [94mLoss[0m : 10.66478
[1mStep[0m  [8/42], [94mLoss[0m : 10.71315
[1mStep[0m  [12/42], [94mLoss[0m : 10.64529
[1mStep[0m  [16/42], [94mLoss[0m : 10.96800
[1mStep[0m  [20/42], [94mLoss[0m : 10.68221
[1mStep[0m  [24/42], [94mLoss[0m : 10.58173
[1mStep[0m  [28/42], [94mLoss[0m : 11.17440
[1mStep[0m  [32/42], [94mLoss[0m : 10.67338
[1mStep[0m  [36/42], [94mLoss[0m : 10.79759
[1mStep[0m  [40/42], [94mLoss[0m : 10.60334

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.788, [92mTest[0m: 10.765, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.88835
[1mStep[0m  [4/42], [94mLoss[0m : 10.89340
[1mStep[0m  [8/42], [94mLoss[0m : 11.10067
[1mStep[0m  [12/42], [94mLoss[0m : 10.89262
[1mStep[0m  [16/42], [94mLoss[0m : 11.22790
[1mStep[0m  [20/42], [94mLoss[0m : 11.20659
[1mStep[0m  [24/42], [94mLoss[0m : 10.55995
[1mStep[0m  [28/42], [94mLoss[0m : 10.51524
[1mStep[0m  [32/42], [94mLoss[0m : 10.98121
[1mStep[0m  [36/42], [94mLoss[0m : 10.86436
[1mStep[0m  [40/42], [94mLoss[0m : 10.83232

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.755, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54186
[1mStep[0m  [4/42], [94mLoss[0m : 10.84299
[1mStep[0m  [8/42], [94mLoss[0m : 10.50556
[1mStep[0m  [12/42], [94mLoss[0m : 10.43484
[1mStep[0m  [16/42], [94mLoss[0m : 10.71189
[1mStep[0m  [20/42], [94mLoss[0m : 10.75245
[1mStep[0m  [24/42], [94mLoss[0m : 10.72453
[1mStep[0m  [28/42], [94mLoss[0m : 10.66613
[1mStep[0m  [32/42], [94mLoss[0m : 10.51451
[1mStep[0m  [36/42], [94mLoss[0m : 10.71318
[1mStep[0m  [40/42], [94mLoss[0m : 10.61744

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.764, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83154
[1mStep[0m  [4/42], [94mLoss[0m : 10.56755
[1mStep[0m  [8/42], [94mLoss[0m : 10.67550
[1mStep[0m  [12/42], [94mLoss[0m : 11.10114
[1mStep[0m  [16/42], [94mLoss[0m : 11.08630
[1mStep[0m  [20/42], [94mLoss[0m : 10.78342
[1mStep[0m  [24/42], [94mLoss[0m : 11.11979
[1mStep[0m  [28/42], [94mLoss[0m : 10.43578
[1mStep[0m  [32/42], [94mLoss[0m : 10.47813
[1mStep[0m  [36/42], [94mLoss[0m : 10.86464
[1mStep[0m  [40/42], [94mLoss[0m : 11.01775

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.730, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.61968
[1mStep[0m  [4/42], [94mLoss[0m : 10.95244
[1mStep[0m  [8/42], [94mLoss[0m : 10.52533
[1mStep[0m  [12/42], [94mLoss[0m : 11.04622
[1mStep[0m  [16/42], [94mLoss[0m : 10.72653
[1mStep[0m  [20/42], [94mLoss[0m : 11.00746
[1mStep[0m  [24/42], [94mLoss[0m : 10.80243
[1mStep[0m  [28/42], [94mLoss[0m : 10.84607
[1mStep[0m  [32/42], [94mLoss[0m : 10.67608
[1mStep[0m  [36/42], [94mLoss[0m : 10.74879
[1mStep[0m  [40/42], [94mLoss[0m : 10.52440

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.778, [92mTest[0m: 10.736, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.70886
[1mStep[0m  [4/42], [94mLoss[0m : 11.03251
[1mStep[0m  [8/42], [94mLoss[0m : 10.78406
[1mStep[0m  [12/42], [94mLoss[0m : 10.90676
[1mStep[0m  [16/42], [94mLoss[0m : 10.34190
[1mStep[0m  [20/42], [94mLoss[0m : 10.42206
[1mStep[0m  [24/42], [94mLoss[0m : 11.11842
[1mStep[0m  [28/42], [94mLoss[0m : 10.77837
[1mStep[0m  [32/42], [94mLoss[0m : 10.29754
[1mStep[0m  [36/42], [94mLoss[0m : 10.86096
[1mStep[0m  [40/42], [94mLoss[0m : 10.78263

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.761, [92mTest[0m: 10.715, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.61175
[1mStep[0m  [4/42], [94mLoss[0m : 10.94249
[1mStep[0m  [8/42], [94mLoss[0m : 10.58421
[1mStep[0m  [12/42], [94mLoss[0m : 10.63185
[1mStep[0m  [16/42], [94mLoss[0m : 10.83105
[1mStep[0m  [20/42], [94mLoss[0m : 10.63843
[1mStep[0m  [24/42], [94mLoss[0m : 10.96694
[1mStep[0m  [28/42], [94mLoss[0m : 10.86076
[1mStep[0m  [32/42], [94mLoss[0m : 10.90348
[1mStep[0m  [36/42], [94mLoss[0m : 10.51714
[1mStep[0m  [40/42], [94mLoss[0m : 10.87903

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.747, [92mTest[0m: 10.719, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81853
[1mStep[0m  [4/42], [94mLoss[0m : 11.19484
[1mStep[0m  [8/42], [94mLoss[0m : 10.07345
[1mStep[0m  [12/42], [94mLoss[0m : 11.03221
[1mStep[0m  [16/42], [94mLoss[0m : 10.69333
[1mStep[0m  [20/42], [94mLoss[0m : 10.07979
[1mStep[0m  [24/42], [94mLoss[0m : 10.64798
[1mStep[0m  [28/42], [94mLoss[0m : 10.91741
[1mStep[0m  [32/42], [94mLoss[0m : 10.86045
[1mStep[0m  [36/42], [94mLoss[0m : 11.06795
[1mStep[0m  [40/42], [94mLoss[0m : 11.09558

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.686, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.55224
[1mStep[0m  [4/42], [94mLoss[0m : 10.76329
[1mStep[0m  [8/42], [94mLoss[0m : 10.75630
[1mStep[0m  [12/42], [94mLoss[0m : 10.80152
[1mStep[0m  [16/42], [94mLoss[0m : 10.83883
[1mStep[0m  [20/42], [94mLoss[0m : 10.77891
[1mStep[0m  [24/42], [94mLoss[0m : 10.93861
[1mStep[0m  [28/42], [94mLoss[0m : 11.29542
[1mStep[0m  [32/42], [94mLoss[0m : 10.77259
[1mStep[0m  [36/42], [94mLoss[0m : 10.51094
[1mStep[0m  [40/42], [94mLoss[0m : 10.84523

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.732, [92mTest[0m: 10.687, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94518
[1mStep[0m  [4/42], [94mLoss[0m : 11.05505
[1mStep[0m  [8/42], [94mLoss[0m : 10.41157
[1mStep[0m  [12/42], [94mLoss[0m : 10.70779
[1mStep[0m  [16/42], [94mLoss[0m : 10.67073
[1mStep[0m  [20/42], [94mLoss[0m : 10.75368
[1mStep[0m  [24/42], [94mLoss[0m : 10.46687
[1mStep[0m  [28/42], [94mLoss[0m : 10.81259
[1mStep[0m  [32/42], [94mLoss[0m : 10.73920
[1mStep[0m  [36/42], [94mLoss[0m : 11.11749
[1mStep[0m  [40/42], [94mLoss[0m : 10.62394

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.724, [92mTest[0m: 10.678, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.07890
[1mStep[0m  [4/42], [94mLoss[0m : 10.99300
[1mStep[0m  [8/42], [94mLoss[0m : 10.58931
[1mStep[0m  [12/42], [94mLoss[0m : 10.63676
[1mStep[0m  [16/42], [94mLoss[0m : 10.80959
[1mStep[0m  [20/42], [94mLoss[0m : 10.41155
[1mStep[0m  [24/42], [94mLoss[0m : 10.54420
[1mStep[0m  [28/42], [94mLoss[0m : 10.97119
[1mStep[0m  [32/42], [94mLoss[0m : 10.78132
[1mStep[0m  [36/42], [94mLoss[0m : 10.69838
[1mStep[0m  [40/42], [94mLoss[0m : 10.56231

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.673, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53680
[1mStep[0m  [4/42], [94mLoss[0m : 10.53171
[1mStep[0m  [8/42], [94mLoss[0m : 10.78208
[1mStep[0m  [12/42], [94mLoss[0m : 10.68220
[1mStep[0m  [16/42], [94mLoss[0m : 10.40004
[1mStep[0m  [20/42], [94mLoss[0m : 10.62460
[1mStep[0m  [24/42], [94mLoss[0m : 10.95391
[1mStep[0m  [28/42], [94mLoss[0m : 10.87111
[1mStep[0m  [32/42], [94mLoss[0m : 10.71559
[1mStep[0m  [36/42], [94mLoss[0m : 10.83139
[1mStep[0m  [40/42], [94mLoss[0m : 10.65990

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.663, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.667
====================================

Phase 1 - Evaluation MAE:  10.66709818158831
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.66540
[1mStep[0m  [4/42], [94mLoss[0m : 10.98633
[1mStep[0m  [8/42], [94mLoss[0m : 10.75247
[1mStep[0m  [12/42], [94mLoss[0m : 10.96118
[1mStep[0m  [16/42], [94mLoss[0m : 11.01003
[1mStep[0m  [20/42], [94mLoss[0m : 10.61064
[1mStep[0m  [24/42], [94mLoss[0m : 10.70921
[1mStep[0m  [28/42], [94mLoss[0m : 10.83910
[1mStep[0m  [32/42], [94mLoss[0m : 10.68097
[1mStep[0m  [36/42], [94mLoss[0m : 10.71783
[1mStep[0m  [40/42], [94mLoss[0m : 10.40071

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.711, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63689
[1mStep[0m  [4/42], [94mLoss[0m : 10.81911
[1mStep[0m  [8/42], [94mLoss[0m : 10.52502
[1mStep[0m  [12/42], [94mLoss[0m : 11.10652
[1mStep[0m  [16/42], [94mLoss[0m : 10.46415
[1mStep[0m  [20/42], [94mLoss[0m : 10.47359
[1mStep[0m  [24/42], [94mLoss[0m : 10.88811
[1mStep[0m  [28/42], [94mLoss[0m : 10.67453
[1mStep[0m  [32/42], [94mLoss[0m : 10.91177
[1mStep[0m  [36/42], [94mLoss[0m : 10.97283
[1mStep[0m  [40/42], [94mLoss[0m : 10.96105

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.697, [92mTest[0m: 10.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81547
[1mStep[0m  [4/42], [94mLoss[0m : 10.33032
[1mStep[0m  [8/42], [94mLoss[0m : 10.77144
[1mStep[0m  [12/42], [94mLoss[0m : 10.78884
[1mStep[0m  [16/42], [94mLoss[0m : 11.05325
[1mStep[0m  [20/42], [94mLoss[0m : 10.75397
[1mStep[0m  [24/42], [94mLoss[0m : 10.68144
[1mStep[0m  [28/42], [94mLoss[0m : 10.54898
[1mStep[0m  [32/42], [94mLoss[0m : 11.23777
[1mStep[0m  [36/42], [94mLoss[0m : 10.85961
[1mStep[0m  [40/42], [94mLoss[0m : 10.77172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.686, [92mTest[0m: 10.628, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.28807
[1mStep[0m  [4/42], [94mLoss[0m : 10.91521
[1mStep[0m  [8/42], [94mLoss[0m : 10.85265
[1mStep[0m  [12/42], [94mLoss[0m : 10.86128
[1mStep[0m  [16/42], [94mLoss[0m : 10.65905
[1mStep[0m  [20/42], [94mLoss[0m : 10.68260
[1mStep[0m  [24/42], [94mLoss[0m : 10.49931
[1mStep[0m  [28/42], [94mLoss[0m : 10.45931
[1mStep[0m  [32/42], [94mLoss[0m : 10.76502
[1mStep[0m  [36/42], [94mLoss[0m : 10.55606
[1mStep[0m  [40/42], [94mLoss[0m : 10.47764

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.667, [92mTest[0m: 10.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81214
[1mStep[0m  [4/42], [94mLoss[0m : 10.61258
[1mStep[0m  [8/42], [94mLoss[0m : 10.84539
[1mStep[0m  [12/42], [94mLoss[0m : 10.78892
[1mStep[0m  [16/42], [94mLoss[0m : 10.52262
[1mStep[0m  [20/42], [94mLoss[0m : 10.49782
[1mStep[0m  [24/42], [94mLoss[0m : 10.97137
[1mStep[0m  [28/42], [94mLoss[0m : 10.43366
[1mStep[0m  [32/42], [94mLoss[0m : 10.52034
[1mStep[0m  [36/42], [94mLoss[0m : 10.88127
[1mStep[0m  [40/42], [94mLoss[0m : 10.72039

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.655, [92mTest[0m: 10.595, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66520
[1mStep[0m  [4/42], [94mLoss[0m : 10.73272
[1mStep[0m  [8/42], [94mLoss[0m : 10.71704
[1mStep[0m  [12/42], [94mLoss[0m : 10.84937
[1mStep[0m  [16/42], [94mLoss[0m : 10.54333
[1mStep[0m  [20/42], [94mLoss[0m : 10.42440
[1mStep[0m  [24/42], [94mLoss[0m : 10.53331
[1mStep[0m  [28/42], [94mLoss[0m : 10.56059
[1mStep[0m  [32/42], [94mLoss[0m : 10.61421
[1mStep[0m  [36/42], [94mLoss[0m : 10.51164
[1mStep[0m  [40/42], [94mLoss[0m : 10.44902

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.653, [92mTest[0m: 10.584, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.73075
[1mStep[0m  [4/42], [94mLoss[0m : 10.81313
[1mStep[0m  [8/42], [94mLoss[0m : 10.86994
[1mStep[0m  [12/42], [94mLoss[0m : 10.89146
[1mStep[0m  [16/42], [94mLoss[0m : 10.62186
[1mStep[0m  [20/42], [94mLoss[0m : 10.68149
[1mStep[0m  [24/42], [94mLoss[0m : 10.74063
[1mStep[0m  [28/42], [94mLoss[0m : 10.71700
[1mStep[0m  [32/42], [94mLoss[0m : 10.51030
[1mStep[0m  [36/42], [94mLoss[0m : 10.72802
[1mStep[0m  [40/42], [94mLoss[0m : 10.80492

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.58501
[1mStep[0m  [4/42], [94mLoss[0m : 10.06435
[1mStep[0m  [8/42], [94mLoss[0m : 10.77169
[1mStep[0m  [12/42], [94mLoss[0m : 10.80161
[1mStep[0m  [16/42], [94mLoss[0m : 11.07670
[1mStep[0m  [20/42], [94mLoss[0m : 10.67800
[1mStep[0m  [24/42], [94mLoss[0m : 10.74183
[1mStep[0m  [28/42], [94mLoss[0m : 11.05719
[1mStep[0m  [32/42], [94mLoss[0m : 10.68260
[1mStep[0m  [36/42], [94mLoss[0m : 10.76222
[1mStep[0m  [40/42], [94mLoss[0m : 10.18670

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.621, [92mTest[0m: 10.552, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71711
[1mStep[0m  [4/42], [94mLoss[0m : 10.64510
[1mStep[0m  [8/42], [94mLoss[0m : 10.62071
[1mStep[0m  [12/42], [94mLoss[0m : 10.70366
[1mStep[0m  [16/42], [94mLoss[0m : 10.41588
[1mStep[0m  [20/42], [94mLoss[0m : 10.42479
[1mStep[0m  [24/42], [94mLoss[0m : 10.51253
[1mStep[0m  [28/42], [94mLoss[0m : 10.40645
[1mStep[0m  [32/42], [94mLoss[0m : 10.36548
[1mStep[0m  [36/42], [94mLoss[0m : 10.75307
[1mStep[0m  [40/42], [94mLoss[0m : 10.53824

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.609, [92mTest[0m: 10.544, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.86982
[1mStep[0m  [4/42], [94mLoss[0m : 10.95301
[1mStep[0m  [8/42], [94mLoss[0m : 10.26538
[1mStep[0m  [12/42], [94mLoss[0m : 10.63351
[1mStep[0m  [16/42], [94mLoss[0m : 10.57205
[1mStep[0m  [20/42], [94mLoss[0m : 10.66785
[1mStep[0m  [24/42], [94mLoss[0m : 10.56589
[1mStep[0m  [28/42], [94mLoss[0m : 10.34713
[1mStep[0m  [32/42], [94mLoss[0m : 10.69556
[1mStep[0m  [36/42], [94mLoss[0m : 10.77344
[1mStep[0m  [40/42], [94mLoss[0m : 10.86139

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.509, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26269
[1mStep[0m  [4/42], [94mLoss[0m : 10.71778
[1mStep[0m  [8/42], [94mLoss[0m : 10.77591
[1mStep[0m  [12/42], [94mLoss[0m : 10.43625
[1mStep[0m  [16/42], [94mLoss[0m : 10.37524
[1mStep[0m  [20/42], [94mLoss[0m : 11.21559
[1mStep[0m  [24/42], [94mLoss[0m : 10.84689
[1mStep[0m  [28/42], [94mLoss[0m : 10.47523
[1mStep[0m  [32/42], [94mLoss[0m : 10.76384
[1mStep[0m  [36/42], [94mLoss[0m : 10.35021
[1mStep[0m  [40/42], [94mLoss[0m : 10.33899

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.596, [92mTest[0m: 10.522, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.69842
[1mStep[0m  [4/42], [94mLoss[0m : 10.88983
[1mStep[0m  [8/42], [94mLoss[0m : 10.19995
[1mStep[0m  [12/42], [94mLoss[0m : 10.34530
[1mStep[0m  [16/42], [94mLoss[0m : 10.92550
[1mStep[0m  [20/42], [94mLoss[0m : 11.20634
[1mStep[0m  [24/42], [94mLoss[0m : 10.48941
[1mStep[0m  [28/42], [94mLoss[0m : 10.66304
[1mStep[0m  [32/42], [94mLoss[0m : 10.60554
[1mStep[0m  [36/42], [94mLoss[0m : 10.49410
[1mStep[0m  [40/42], [94mLoss[0m : 10.35530

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.579, [92mTest[0m: 10.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.04917
[1mStep[0m  [4/42], [94mLoss[0m : 10.87174
[1mStep[0m  [8/42], [94mLoss[0m : 10.74674
[1mStep[0m  [12/42], [94mLoss[0m : 10.96698
[1mStep[0m  [16/42], [94mLoss[0m : 10.54000
[1mStep[0m  [20/42], [94mLoss[0m : 10.78543
[1mStep[0m  [24/42], [94mLoss[0m : 11.13826
[1mStep[0m  [28/42], [94mLoss[0m : 10.08565
[1mStep[0m  [32/42], [94mLoss[0m : 10.39847
[1mStep[0m  [36/42], [94mLoss[0m : 10.54762
[1mStep[0m  [40/42], [94mLoss[0m : 10.39638

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.573, [92mTest[0m: 10.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.21835
[1mStep[0m  [4/42], [94mLoss[0m : 10.25059
[1mStep[0m  [8/42], [94mLoss[0m : 10.64458
[1mStep[0m  [12/42], [94mLoss[0m : 10.49605
[1mStep[0m  [16/42], [94mLoss[0m : 10.68735
[1mStep[0m  [20/42], [94mLoss[0m : 10.75904
[1mStep[0m  [24/42], [94mLoss[0m : 10.50891
[1mStep[0m  [28/42], [94mLoss[0m : 10.41369
[1mStep[0m  [32/42], [94mLoss[0m : 10.58956
[1mStep[0m  [36/42], [94mLoss[0m : 10.61364
[1mStep[0m  [40/42], [94mLoss[0m : 10.40985

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.555, [92mTest[0m: 10.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.30483
[1mStep[0m  [4/42], [94mLoss[0m : 10.50121
[1mStep[0m  [8/42], [94mLoss[0m : 10.57077
[1mStep[0m  [12/42], [94mLoss[0m : 10.46767
[1mStep[0m  [16/42], [94mLoss[0m : 10.37267
[1mStep[0m  [20/42], [94mLoss[0m : 10.35999
[1mStep[0m  [24/42], [94mLoss[0m : 11.14554
[1mStep[0m  [28/42], [94mLoss[0m : 10.75051
[1mStep[0m  [32/42], [94mLoss[0m : 10.81975
[1mStep[0m  [36/42], [94mLoss[0m : 10.52736
[1mStep[0m  [40/42], [94mLoss[0m : 10.62543

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.546, [92mTest[0m: 10.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.80108
[1mStep[0m  [4/42], [94mLoss[0m : 10.66266
[1mStep[0m  [8/42], [94mLoss[0m : 10.35791
[1mStep[0m  [12/42], [94mLoss[0m : 10.28813
[1mStep[0m  [16/42], [94mLoss[0m : 10.61641
[1mStep[0m  [20/42], [94mLoss[0m : 10.41549
[1mStep[0m  [24/42], [94mLoss[0m : 10.72873
[1mStep[0m  [28/42], [94mLoss[0m : 10.25451
[1mStep[0m  [32/42], [94mLoss[0m : 10.42797
[1mStep[0m  [36/42], [94mLoss[0m : 10.17884
[1mStep[0m  [40/42], [94mLoss[0m : 10.52163

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.524, [92mTest[0m: 10.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53312
[1mStep[0m  [4/42], [94mLoss[0m : 10.73431
[1mStep[0m  [8/42], [94mLoss[0m : 10.69379
[1mStep[0m  [12/42], [94mLoss[0m : 10.38017
[1mStep[0m  [16/42], [94mLoss[0m : 10.54764
[1mStep[0m  [20/42], [94mLoss[0m : 10.79007
[1mStep[0m  [24/42], [94mLoss[0m : 10.19130
[1mStep[0m  [28/42], [94mLoss[0m : 10.01816
[1mStep[0m  [32/42], [94mLoss[0m : 10.50139
[1mStep[0m  [36/42], [94mLoss[0m : 10.30575
[1mStep[0m  [40/42], [94mLoss[0m : 10.65108

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.515, [92mTest[0m: 10.424, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.58907
[1mStep[0m  [4/42], [94mLoss[0m : 10.38058
[1mStep[0m  [8/42], [94mLoss[0m : 10.39525
[1mStep[0m  [12/42], [94mLoss[0m : 10.26720
[1mStep[0m  [16/42], [94mLoss[0m : 10.50277
[1mStep[0m  [20/42], [94mLoss[0m : 10.72924
[1mStep[0m  [24/42], [94mLoss[0m : 10.38703
[1mStep[0m  [28/42], [94mLoss[0m : 10.19082
[1mStep[0m  [32/42], [94mLoss[0m : 10.61732
[1mStep[0m  [36/42], [94mLoss[0m : 10.33467
[1mStep[0m  [40/42], [94mLoss[0m : 10.43834

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.502, [92mTest[0m: 10.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.20287
[1mStep[0m  [4/42], [94mLoss[0m : 10.32375
[1mStep[0m  [8/42], [94mLoss[0m : 10.48461
[1mStep[0m  [12/42], [94mLoss[0m : 10.42535
[1mStep[0m  [16/42], [94mLoss[0m : 10.62687
[1mStep[0m  [20/42], [94mLoss[0m : 10.71616
[1mStep[0m  [24/42], [94mLoss[0m : 10.57730
[1mStep[0m  [28/42], [94mLoss[0m : 10.44722
[1mStep[0m  [32/42], [94mLoss[0m : 10.49480
[1mStep[0m  [36/42], [94mLoss[0m : 10.67018
[1mStep[0m  [40/42], [94mLoss[0m : 10.68810

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.483, [92mTest[0m: 10.414, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81868
[1mStep[0m  [4/42], [94mLoss[0m : 10.53097
[1mStep[0m  [8/42], [94mLoss[0m : 10.26009
[1mStep[0m  [12/42], [94mLoss[0m : 10.39744
[1mStep[0m  [16/42], [94mLoss[0m : 10.72131
[1mStep[0m  [20/42], [94mLoss[0m : 10.35000
[1mStep[0m  [24/42], [94mLoss[0m : 10.42015
[1mStep[0m  [28/42], [94mLoss[0m : 10.77026
[1mStep[0m  [32/42], [94mLoss[0m : 10.24926
[1mStep[0m  [36/42], [94mLoss[0m : 10.50674
[1mStep[0m  [40/42], [94mLoss[0m : 10.71733

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.473, [92mTest[0m: 10.364, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95390
[1mStep[0m  [4/42], [94mLoss[0m : 10.27020
[1mStep[0m  [8/42], [94mLoss[0m : 10.50774
[1mStep[0m  [12/42], [94mLoss[0m : 10.66438
[1mStep[0m  [16/42], [94mLoss[0m : 10.31098
[1mStep[0m  [20/42], [94mLoss[0m : 10.53895
[1mStep[0m  [24/42], [94mLoss[0m : 10.51916
[1mStep[0m  [28/42], [94mLoss[0m : 10.77312
[1mStep[0m  [32/42], [94mLoss[0m : 10.29575
[1mStep[0m  [36/42], [94mLoss[0m : 10.21775
[1mStep[0m  [40/42], [94mLoss[0m : 10.29030

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.459, [92mTest[0m: 10.374, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66611
[1mStep[0m  [4/42], [94mLoss[0m : 10.60231
[1mStep[0m  [8/42], [94mLoss[0m : 10.56211
[1mStep[0m  [12/42], [94mLoss[0m : 10.59464
[1mStep[0m  [16/42], [94mLoss[0m : 10.43337
[1mStep[0m  [20/42], [94mLoss[0m : 10.59333
[1mStep[0m  [24/42], [94mLoss[0m : 10.05285
[1mStep[0m  [28/42], [94mLoss[0m : 10.25581
[1mStep[0m  [32/42], [94mLoss[0m : 10.44471
[1mStep[0m  [36/42], [94mLoss[0m : 10.26333
[1mStep[0m  [40/42], [94mLoss[0m : 10.57736

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.445, [92mTest[0m: 10.357, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.27615
[1mStep[0m  [4/42], [94mLoss[0m : 10.48598
[1mStep[0m  [8/42], [94mLoss[0m : 10.22335
[1mStep[0m  [12/42], [94mLoss[0m : 10.08603
[1mStep[0m  [16/42], [94mLoss[0m : 10.17543
[1mStep[0m  [20/42], [94mLoss[0m : 10.38121
[1mStep[0m  [24/42], [94mLoss[0m : 10.52851
[1mStep[0m  [28/42], [94mLoss[0m : 10.59716
[1mStep[0m  [32/42], [94mLoss[0m : 10.64098
[1mStep[0m  [36/42], [94mLoss[0m : 10.57419
[1mStep[0m  [40/42], [94mLoss[0m : 11.13778

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.436, [92mTest[0m: 10.334, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.04091
[1mStep[0m  [4/42], [94mLoss[0m : 10.83652
[1mStep[0m  [8/42], [94mLoss[0m : 10.65817
[1mStep[0m  [12/42], [94mLoss[0m : 10.29949
[1mStep[0m  [16/42], [94mLoss[0m : 9.99353
[1mStep[0m  [20/42], [94mLoss[0m : 10.52305
[1mStep[0m  [24/42], [94mLoss[0m : 10.68321
[1mStep[0m  [28/42], [94mLoss[0m : 10.68887
[1mStep[0m  [32/42], [94mLoss[0m : 10.87938
[1mStep[0m  [36/42], [94mLoss[0m : 10.17659
[1mStep[0m  [40/42], [94mLoss[0m : 10.06048

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.425, [92mTest[0m: 10.331, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.55840
[1mStep[0m  [4/42], [94mLoss[0m : 10.35014
[1mStep[0m  [8/42], [94mLoss[0m : 10.54432
[1mStep[0m  [12/42], [94mLoss[0m : 10.04972
[1mStep[0m  [16/42], [94mLoss[0m : 10.40333
[1mStep[0m  [20/42], [94mLoss[0m : 10.33767
[1mStep[0m  [24/42], [94mLoss[0m : 10.50962
[1mStep[0m  [28/42], [94mLoss[0m : 10.50058
[1mStep[0m  [32/42], [94mLoss[0m : 10.88678
[1mStep[0m  [36/42], [94mLoss[0m : 10.14452
[1mStep[0m  [40/42], [94mLoss[0m : 10.18733

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.405, [92mTest[0m: 10.282, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19150
[1mStep[0m  [4/42], [94mLoss[0m : 10.48657
[1mStep[0m  [8/42], [94mLoss[0m : 10.80142
[1mStep[0m  [12/42], [94mLoss[0m : 10.10112
[1mStep[0m  [16/42], [94mLoss[0m : 10.09066
[1mStep[0m  [20/42], [94mLoss[0m : 10.62686
[1mStep[0m  [24/42], [94mLoss[0m : 10.26846
[1mStep[0m  [28/42], [94mLoss[0m : 10.50280
[1mStep[0m  [32/42], [94mLoss[0m : 10.14159
[1mStep[0m  [36/42], [94mLoss[0m : 10.07163
[1mStep[0m  [40/42], [94mLoss[0m : 10.39761

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.396, [92mTest[0m: 10.306, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.39243
[1mStep[0m  [4/42], [94mLoss[0m : 10.31412
[1mStep[0m  [8/42], [94mLoss[0m : 10.28929
[1mStep[0m  [12/42], [94mLoss[0m : 10.35023
[1mStep[0m  [16/42], [94mLoss[0m : 10.60060
[1mStep[0m  [20/42], [94mLoss[0m : 10.01559
[1mStep[0m  [24/42], [94mLoss[0m : 10.28966
[1mStep[0m  [28/42], [94mLoss[0m : 10.49040
[1mStep[0m  [32/42], [94mLoss[0m : 10.18031
[1mStep[0m  [36/42], [94mLoss[0m : 10.10081
[1mStep[0m  [40/42], [94mLoss[0m : 10.33084

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.271, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23538
[1mStep[0m  [4/42], [94mLoss[0m : 10.30000
[1mStep[0m  [8/42], [94mLoss[0m : 10.35364
[1mStep[0m  [12/42], [94mLoss[0m : 10.52666
[1mStep[0m  [16/42], [94mLoss[0m : 10.16922
[1mStep[0m  [20/42], [94mLoss[0m : 10.84308
[1mStep[0m  [24/42], [94mLoss[0m : 10.20992
[1mStep[0m  [28/42], [94mLoss[0m : 10.11676
[1mStep[0m  [32/42], [94mLoss[0m : 10.69149
[1mStep[0m  [36/42], [94mLoss[0m : 10.59163
[1mStep[0m  [40/42], [94mLoss[0m : 10.23760

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.368, [92mTest[0m: 10.276, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.27044
[1mStep[0m  [4/42], [94mLoss[0m : 10.02172
[1mStep[0m  [8/42], [94mLoss[0m : 10.51917
[1mStep[0m  [12/42], [94mLoss[0m : 10.16768
[1mStep[0m  [16/42], [94mLoss[0m : 10.35534
[1mStep[0m  [20/42], [94mLoss[0m : 10.32109
[1mStep[0m  [24/42], [94mLoss[0m : 10.25410
[1mStep[0m  [28/42], [94mLoss[0m : 10.00744
[1mStep[0m  [32/42], [94mLoss[0m : 10.43894
[1mStep[0m  [36/42], [94mLoss[0m : 10.58595
[1mStep[0m  [40/42], [94mLoss[0m : 10.29630

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.352, [92mTest[0m: 10.249, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.41781
[1mStep[0m  [4/42], [94mLoss[0m : 10.63538
[1mStep[0m  [8/42], [94mLoss[0m : 10.50893
[1mStep[0m  [12/42], [94mLoss[0m : 10.62538
[1mStep[0m  [16/42], [94mLoss[0m : 10.44756
[1mStep[0m  [20/42], [94mLoss[0m : 10.52062
[1mStep[0m  [24/42], [94mLoss[0m : 9.73729
[1mStep[0m  [28/42], [94mLoss[0m : 9.69461
[1mStep[0m  [32/42], [94mLoss[0m : 10.59444
[1mStep[0m  [36/42], [94mLoss[0m : 10.33304
[1mStep[0m  [40/42], [94mLoss[0m : 10.15460

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.339, [92mTest[0m: 10.244, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.217
====================================

Phase 2 - Evaluation MAE:  10.217419283730644
MAE score P1      10.667098
MAE score P2      10.217419
loss              10.339014
learning_rate        0.0001
batch_size              256
hidden_sizes          [100]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 3, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.12153
[1mStep[0m  [4/42], [94mLoss[0m : 11.31394
[1mStep[0m  [8/42], [94mLoss[0m : 11.26901
[1mStep[0m  [12/42], [94mLoss[0m : 10.76888
[1mStep[0m  [16/42], [94mLoss[0m : 10.61751
[1mStep[0m  [20/42], [94mLoss[0m : 10.86526
[1mStep[0m  [24/42], [94mLoss[0m : 10.92130
[1mStep[0m  [28/42], [94mLoss[0m : 10.83137
[1mStep[0m  [32/42], [94mLoss[0m : 10.46839
[1mStep[0m  [36/42], [94mLoss[0m : 11.06954
[1mStep[0m  [40/42], [94mLoss[0m : 11.04015

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.900, [92mTest[0m: 10.791, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87519
[1mStep[0m  [4/42], [94mLoss[0m : 10.60203
[1mStep[0m  [8/42], [94mLoss[0m : 10.95785
[1mStep[0m  [12/42], [94mLoss[0m : 10.95693
[1mStep[0m  [16/42], [94mLoss[0m : 10.65236
[1mStep[0m  [20/42], [94mLoss[0m : 10.65232
[1mStep[0m  [24/42], [94mLoss[0m : 10.93017
[1mStep[0m  [28/42], [94mLoss[0m : 10.84460
[1mStep[0m  [32/42], [94mLoss[0m : 11.11719
[1mStep[0m  [36/42], [94mLoss[0m : 11.22384
[1mStep[0m  [40/42], [94mLoss[0m : 11.10111

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.859, [92mTest[0m: 10.895, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68004
[1mStep[0m  [4/42], [94mLoss[0m : 10.70459
[1mStep[0m  [8/42], [94mLoss[0m : 10.64865
[1mStep[0m  [12/42], [94mLoss[0m : 10.76558
[1mStep[0m  [16/42], [94mLoss[0m : 10.97787
[1mStep[0m  [20/42], [94mLoss[0m : 10.94673
[1mStep[0m  [24/42], [94mLoss[0m : 10.89382
[1mStep[0m  [28/42], [94mLoss[0m : 10.31602
[1mStep[0m  [32/42], [94mLoss[0m : 10.43530
[1mStep[0m  [36/42], [94mLoss[0m : 11.32563
[1mStep[0m  [40/42], [94mLoss[0m : 10.87505

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.808, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.99494
[1mStep[0m  [4/42], [94mLoss[0m : 10.84263
[1mStep[0m  [8/42], [94mLoss[0m : 10.52052
[1mStep[0m  [12/42], [94mLoss[0m : 10.75883
[1mStep[0m  [16/42], [94mLoss[0m : 10.70651
[1mStep[0m  [20/42], [94mLoss[0m : 10.57359
[1mStep[0m  [24/42], [94mLoss[0m : 10.84466
[1mStep[0m  [28/42], [94mLoss[0m : 11.11523
[1mStep[0m  [32/42], [94mLoss[0m : 10.79014
[1mStep[0m  [36/42], [94mLoss[0m : 10.69892
[1mStep[0m  [40/42], [94mLoss[0m : 10.42036

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.773, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94497
[1mStep[0m  [4/42], [94mLoss[0m : 10.58010
[1mStep[0m  [8/42], [94mLoss[0m : 10.72295
[1mStep[0m  [12/42], [94mLoss[0m : 10.75602
[1mStep[0m  [16/42], [94mLoss[0m : 10.79348
[1mStep[0m  [20/42], [94mLoss[0m : 10.55411
[1mStep[0m  [24/42], [94mLoss[0m : 10.99052
[1mStep[0m  [28/42], [94mLoss[0m : 10.71303
[1mStep[0m  [32/42], [94mLoss[0m : 10.80384
[1mStep[0m  [36/42], [94mLoss[0m : 11.00308
[1mStep[0m  [40/42], [94mLoss[0m : 10.09999

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23635
[1mStep[0m  [4/42], [94mLoss[0m : 10.63965
[1mStep[0m  [8/42], [94mLoss[0m : 10.83551
[1mStep[0m  [12/42], [94mLoss[0m : 10.64469
[1mStep[0m  [16/42], [94mLoss[0m : 11.15883
[1mStep[0m  [20/42], [94mLoss[0m : 10.56781
[1mStep[0m  [24/42], [94mLoss[0m : 10.96777
[1mStep[0m  [28/42], [94mLoss[0m : 10.87955
[1mStep[0m  [32/42], [94mLoss[0m : 10.80344
[1mStep[0m  [36/42], [94mLoss[0m : 10.54668
[1mStep[0m  [40/42], [94mLoss[0m : 10.43334

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.695, [92mTest[0m: 10.786, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36719
[1mStep[0m  [4/42], [94mLoss[0m : 10.90341
[1mStep[0m  [8/42], [94mLoss[0m : 10.67081
[1mStep[0m  [12/42], [94mLoss[0m : 10.77538
[1mStep[0m  [16/42], [94mLoss[0m : 10.53699
[1mStep[0m  [20/42], [94mLoss[0m : 10.52908
[1mStep[0m  [24/42], [94mLoss[0m : 10.74871
[1mStep[0m  [28/42], [94mLoss[0m : 10.69335
[1mStep[0m  [32/42], [94mLoss[0m : 10.68401
[1mStep[0m  [36/42], [94mLoss[0m : 10.58900
[1mStep[0m  [40/42], [94mLoss[0m : 10.27931

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.96537
[1mStep[0m  [4/42], [94mLoss[0m : 10.33982
[1mStep[0m  [8/42], [94mLoss[0m : 10.25046
[1mStep[0m  [12/42], [94mLoss[0m : 10.59969
[1mStep[0m  [16/42], [94mLoss[0m : 10.73532
[1mStep[0m  [20/42], [94mLoss[0m : 10.34148
[1mStep[0m  [24/42], [94mLoss[0m : 10.72094
[1mStep[0m  [28/42], [94mLoss[0m : 10.67103
[1mStep[0m  [32/42], [94mLoss[0m : 10.62884
[1mStep[0m  [36/42], [94mLoss[0m : 10.16185
[1mStep[0m  [40/42], [94mLoss[0m : 10.61957

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.744, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.34617
[1mStep[0m  [4/42], [94mLoss[0m : 10.93506
[1mStep[0m  [8/42], [94mLoss[0m : 10.54105
[1mStep[0m  [12/42], [94mLoss[0m : 10.56591
[1mStep[0m  [16/42], [94mLoss[0m : 10.55564
[1mStep[0m  [20/42], [94mLoss[0m : 10.37780
[1mStep[0m  [24/42], [94mLoss[0m : 10.99674
[1mStep[0m  [28/42], [94mLoss[0m : 10.44082
[1mStep[0m  [32/42], [94mLoss[0m : 10.53447
[1mStep[0m  [36/42], [94mLoss[0m : 10.50145
[1mStep[0m  [40/42], [94mLoss[0m : 10.79132

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.577, [92mTest[0m: 10.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.80452
[1mStep[0m  [4/42], [94mLoss[0m : 10.74386
[1mStep[0m  [8/42], [94mLoss[0m : 10.54487
[1mStep[0m  [12/42], [94mLoss[0m : 10.15576
[1mStep[0m  [16/42], [94mLoss[0m : 10.33423
[1mStep[0m  [20/42], [94mLoss[0m : 10.37797
[1mStep[0m  [24/42], [94mLoss[0m : 10.82754
[1mStep[0m  [28/42], [94mLoss[0m : 10.63427
[1mStep[0m  [32/42], [94mLoss[0m : 10.23741
[1mStep[0m  [36/42], [94mLoss[0m : 10.68878
[1mStep[0m  [40/42], [94mLoss[0m : 10.34723

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.538, [92mTest[0m: 10.700, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.02080
[1mStep[0m  [4/42], [94mLoss[0m : 10.79540
[1mStep[0m  [8/42], [94mLoss[0m : 10.25086
[1mStep[0m  [12/42], [94mLoss[0m : 10.65207
[1mStep[0m  [16/42], [94mLoss[0m : 10.64751
[1mStep[0m  [20/42], [94mLoss[0m : 10.33317
[1mStep[0m  [24/42], [94mLoss[0m : 10.34248
[1mStep[0m  [28/42], [94mLoss[0m : 10.33091
[1mStep[0m  [32/42], [94mLoss[0m : 10.55436
[1mStep[0m  [36/42], [94mLoss[0m : 9.95325
[1mStep[0m  [40/42], [94mLoss[0m : 10.35828

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.492, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.09255
[1mStep[0m  [4/42], [94mLoss[0m : 10.24044
[1mStep[0m  [8/42], [94mLoss[0m : 10.36125
[1mStep[0m  [12/42], [94mLoss[0m : 10.56795
[1mStep[0m  [16/42], [94mLoss[0m : 10.31314
[1mStep[0m  [20/42], [94mLoss[0m : 10.54470
[1mStep[0m  [24/42], [94mLoss[0m : 10.22614
[1mStep[0m  [28/42], [94mLoss[0m : 10.53904
[1mStep[0m  [32/42], [94mLoss[0m : 10.59310
[1mStep[0m  [36/42], [94mLoss[0m : 10.36687
[1mStep[0m  [40/42], [94mLoss[0m : 10.48069

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.456, [92mTest[0m: 10.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49878
[1mStep[0m  [4/42], [94mLoss[0m : 10.65268
[1mStep[0m  [8/42], [94mLoss[0m : 10.80461
[1mStep[0m  [12/42], [94mLoss[0m : 10.23112
[1mStep[0m  [16/42], [94mLoss[0m : 10.36834
[1mStep[0m  [20/42], [94mLoss[0m : 10.29390
[1mStep[0m  [24/42], [94mLoss[0m : 10.37379
[1mStep[0m  [28/42], [94mLoss[0m : 10.38906
[1mStep[0m  [32/42], [94mLoss[0m : 10.50187
[1mStep[0m  [36/42], [94mLoss[0m : 10.33601
[1mStep[0m  [40/42], [94mLoss[0m : 10.56500

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.421, [92mTest[0m: 10.610, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.91941
[1mStep[0m  [4/42], [94mLoss[0m : 10.38238
[1mStep[0m  [8/42], [94mLoss[0m : 10.41831
[1mStep[0m  [12/42], [94mLoss[0m : 10.66580
[1mStep[0m  [16/42], [94mLoss[0m : 9.90515
[1mStep[0m  [20/42], [94mLoss[0m : 10.58666
[1mStep[0m  [24/42], [94mLoss[0m : 10.67388
[1mStep[0m  [28/42], [94mLoss[0m : 10.44431
[1mStep[0m  [32/42], [94mLoss[0m : 10.40823
[1mStep[0m  [36/42], [94mLoss[0m : 10.34847
[1mStep[0m  [40/42], [94mLoss[0m : 10.59049

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.374, [92mTest[0m: 10.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49866
[1mStep[0m  [4/42], [94mLoss[0m : 10.49462
[1mStep[0m  [8/42], [94mLoss[0m : 10.27741
[1mStep[0m  [12/42], [94mLoss[0m : 10.19382
[1mStep[0m  [16/42], [94mLoss[0m : 10.35324
[1mStep[0m  [20/42], [94mLoss[0m : 10.06346
[1mStep[0m  [24/42], [94mLoss[0m : 10.25773
[1mStep[0m  [28/42], [94mLoss[0m : 10.39276
[1mStep[0m  [32/42], [94mLoss[0m : 10.54745
[1mStep[0m  [36/42], [94mLoss[0m : 10.32751
[1mStep[0m  [40/42], [94mLoss[0m : 10.32203

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.343, [92mTest[0m: 10.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83733
[1mStep[0m  [4/42], [94mLoss[0m : 10.30380
[1mStep[0m  [8/42], [94mLoss[0m : 10.33003
[1mStep[0m  [12/42], [94mLoss[0m : 10.39496
[1mStep[0m  [16/42], [94mLoss[0m : 9.95794
[1mStep[0m  [20/42], [94mLoss[0m : 10.36157
[1mStep[0m  [24/42], [94mLoss[0m : 10.66506
[1mStep[0m  [28/42], [94mLoss[0m : 10.18810
[1mStep[0m  [32/42], [94mLoss[0m : 10.04789
[1mStep[0m  [36/42], [94mLoss[0m : 10.43315
[1mStep[0m  [40/42], [94mLoss[0m : 10.46148

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.302, [92mTest[0m: 10.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.30927
[1mStep[0m  [4/42], [94mLoss[0m : 10.00556
[1mStep[0m  [8/42], [94mLoss[0m : 10.62854
[1mStep[0m  [12/42], [94mLoss[0m : 10.50850
[1mStep[0m  [16/42], [94mLoss[0m : 10.09414
[1mStep[0m  [20/42], [94mLoss[0m : 10.42710
[1mStep[0m  [24/42], [94mLoss[0m : 9.59169
[1mStep[0m  [28/42], [94mLoss[0m : 9.88105
[1mStep[0m  [32/42], [94mLoss[0m : 10.72156
[1mStep[0m  [36/42], [94mLoss[0m : 10.08642
[1mStep[0m  [40/42], [94mLoss[0m : 10.35387

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.261, [92mTest[0m: 10.504, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.07948
[1mStep[0m  [4/42], [94mLoss[0m : 10.09748
[1mStep[0m  [8/42], [94mLoss[0m : 10.80640
[1mStep[0m  [12/42], [94mLoss[0m : 10.44366
[1mStep[0m  [16/42], [94mLoss[0m : 10.07197
[1mStep[0m  [20/42], [94mLoss[0m : 9.94861
[1mStep[0m  [24/42], [94mLoss[0m : 10.51818
[1mStep[0m  [28/42], [94mLoss[0m : 10.14928
[1mStep[0m  [32/42], [94mLoss[0m : 10.19316
[1mStep[0m  [36/42], [94mLoss[0m : 10.36688
[1mStep[0m  [40/42], [94mLoss[0m : 10.17469

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.221, [92mTest[0m: 10.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.71318
[1mStep[0m  [4/42], [94mLoss[0m : 10.72612
[1mStep[0m  [8/42], [94mLoss[0m : 10.01075
[1mStep[0m  [12/42], [94mLoss[0m : 10.09010
[1mStep[0m  [16/42], [94mLoss[0m : 9.82122
[1mStep[0m  [20/42], [94mLoss[0m : 10.03999
[1mStep[0m  [24/42], [94mLoss[0m : 10.09345
[1mStep[0m  [28/42], [94mLoss[0m : 9.89609
[1mStep[0m  [32/42], [94mLoss[0m : 10.19890
[1mStep[0m  [36/42], [94mLoss[0m : 10.18476
[1mStep[0m  [40/42], [94mLoss[0m : 9.73318

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.169, [92mTest[0m: 10.449, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08770
[1mStep[0m  [4/42], [94mLoss[0m : 9.88201
[1mStep[0m  [8/42], [94mLoss[0m : 10.38825
[1mStep[0m  [12/42], [94mLoss[0m : 10.41947
[1mStep[0m  [16/42], [94mLoss[0m : 10.02236
[1mStep[0m  [20/42], [94mLoss[0m : 10.37136
[1mStep[0m  [24/42], [94mLoss[0m : 10.32348
[1mStep[0m  [28/42], [94mLoss[0m : 10.30266
[1mStep[0m  [32/42], [94mLoss[0m : 10.49705
[1mStep[0m  [36/42], [94mLoss[0m : 10.05863
[1mStep[0m  [40/42], [94mLoss[0m : 10.05600

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.142, [92mTest[0m: 10.432, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.24840
[1mStep[0m  [4/42], [94mLoss[0m : 9.77245
[1mStep[0m  [8/42], [94mLoss[0m : 10.01143
[1mStep[0m  [12/42], [94mLoss[0m : 10.23731
[1mStep[0m  [16/42], [94mLoss[0m : 10.30610
[1mStep[0m  [20/42], [94mLoss[0m : 10.01186
[1mStep[0m  [24/42], [94mLoss[0m : 10.04740
[1mStep[0m  [28/42], [94mLoss[0m : 9.89917
[1mStep[0m  [32/42], [94mLoss[0m : 10.19786
[1mStep[0m  [36/42], [94mLoss[0m : 10.18151
[1mStep[0m  [40/42], [94mLoss[0m : 9.93931

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.107, [92mTest[0m: 10.413, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.07175
[1mStep[0m  [4/42], [94mLoss[0m : 9.93738
[1mStep[0m  [8/42], [94mLoss[0m : 9.97917
[1mStep[0m  [12/42], [94mLoss[0m : 9.90469
[1mStep[0m  [16/42], [94mLoss[0m : 9.86652
[1mStep[0m  [20/42], [94mLoss[0m : 10.31023
[1mStep[0m  [24/42], [94mLoss[0m : 10.54728
[1mStep[0m  [28/42], [94mLoss[0m : 10.31196
[1mStep[0m  [32/42], [94mLoss[0m : 9.81259
[1mStep[0m  [36/42], [94mLoss[0m : 10.17707
[1mStep[0m  [40/42], [94mLoss[0m : 10.25171

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.056, [92mTest[0m: 10.400, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89814
[1mStep[0m  [4/42], [94mLoss[0m : 9.61906
[1mStep[0m  [8/42], [94mLoss[0m : 10.03689
[1mStep[0m  [12/42], [94mLoss[0m : 9.81701
[1mStep[0m  [16/42], [94mLoss[0m : 10.45988
[1mStep[0m  [20/42], [94mLoss[0m : 10.36931
[1mStep[0m  [24/42], [94mLoss[0m : 9.81646
[1mStep[0m  [28/42], [94mLoss[0m : 10.24384
[1mStep[0m  [32/42], [94mLoss[0m : 9.79387
[1mStep[0m  [36/42], [94mLoss[0m : 9.83181
[1mStep[0m  [40/42], [94mLoss[0m : 9.57380

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.017, [92mTest[0m: 10.362, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.97983
[1mStep[0m  [4/42], [94mLoss[0m : 10.25405
[1mStep[0m  [8/42], [94mLoss[0m : 10.10392
[1mStep[0m  [12/42], [94mLoss[0m : 9.71083
[1mStep[0m  [16/42], [94mLoss[0m : 9.92206
[1mStep[0m  [20/42], [94mLoss[0m : 10.15201
[1mStep[0m  [24/42], [94mLoss[0m : 9.91735
[1mStep[0m  [28/42], [94mLoss[0m : 9.77770
[1mStep[0m  [32/42], [94mLoss[0m : 9.89157
[1mStep[0m  [36/42], [94mLoss[0m : 10.11929
[1mStep[0m  [40/42], [94mLoss[0m : 10.29533

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.988, [92mTest[0m: 10.339, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89144
[1mStep[0m  [4/42], [94mLoss[0m : 9.85434
[1mStep[0m  [8/42], [94mLoss[0m : 10.12562
[1mStep[0m  [12/42], [94mLoss[0m : 9.57314
[1mStep[0m  [16/42], [94mLoss[0m : 9.93107
[1mStep[0m  [20/42], [94mLoss[0m : 10.12623
[1mStep[0m  [24/42], [94mLoss[0m : 10.07885
[1mStep[0m  [28/42], [94mLoss[0m : 9.43859
[1mStep[0m  [32/42], [94mLoss[0m : 10.38773
[1mStep[0m  [36/42], [94mLoss[0m : 10.05840
[1mStep[0m  [40/42], [94mLoss[0m : 10.02038

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.953, [92mTest[0m: 10.300, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.14147
[1mStep[0m  [4/42], [94mLoss[0m : 9.80652
[1mStep[0m  [8/42], [94mLoss[0m : 9.50668
[1mStep[0m  [12/42], [94mLoss[0m : 9.57237
[1mStep[0m  [16/42], [94mLoss[0m : 9.86262
[1mStep[0m  [20/42], [94mLoss[0m : 10.18554
[1mStep[0m  [24/42], [94mLoss[0m : 9.70165
[1mStep[0m  [28/42], [94mLoss[0m : 9.79915
[1mStep[0m  [32/42], [94mLoss[0m : 9.75851
[1mStep[0m  [36/42], [94mLoss[0m : 10.03486
[1mStep[0m  [40/42], [94mLoss[0m : 9.86318

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.917, [92mTest[0m: 10.282, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.76803
[1mStep[0m  [4/42], [94mLoss[0m : 10.43327
[1mStep[0m  [8/42], [94mLoss[0m : 9.93233
[1mStep[0m  [12/42], [94mLoss[0m : 9.66183
[1mStep[0m  [16/42], [94mLoss[0m : 9.88472
[1mStep[0m  [20/42], [94mLoss[0m : 9.63311
[1mStep[0m  [24/42], [94mLoss[0m : 9.82112
[1mStep[0m  [28/42], [94mLoss[0m : 10.17269
[1mStep[0m  [32/42], [94mLoss[0m : 9.65363
[1mStep[0m  [36/42], [94mLoss[0m : 10.04811
[1mStep[0m  [40/42], [94mLoss[0m : 9.96277

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.887, [92mTest[0m: 10.264, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.72285
[1mStep[0m  [4/42], [94mLoss[0m : 9.56265
[1mStep[0m  [8/42], [94mLoss[0m : 10.37634
[1mStep[0m  [12/42], [94mLoss[0m : 9.69368
[1mStep[0m  [16/42], [94mLoss[0m : 9.76360
[1mStep[0m  [20/42], [94mLoss[0m : 9.77897
[1mStep[0m  [24/42], [94mLoss[0m : 10.09138
[1mStep[0m  [28/42], [94mLoss[0m : 9.85653
[1mStep[0m  [32/42], [94mLoss[0m : 10.22177
[1mStep[0m  [36/42], [94mLoss[0m : 9.90973
[1mStep[0m  [40/42], [94mLoss[0m : 9.72152

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.846, [92mTest[0m: 10.216, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.83505
[1mStep[0m  [4/42], [94mLoss[0m : 9.71465
[1mStep[0m  [8/42], [94mLoss[0m : 9.98337
[1mStep[0m  [12/42], [94mLoss[0m : 10.02248
[1mStep[0m  [16/42], [94mLoss[0m : 10.15690
[1mStep[0m  [20/42], [94mLoss[0m : 10.14792
[1mStep[0m  [24/42], [94mLoss[0m : 9.70015
[1mStep[0m  [28/42], [94mLoss[0m : 9.74174
[1mStep[0m  [32/42], [94mLoss[0m : 9.75387
[1mStep[0m  [36/42], [94mLoss[0m : 9.76179
[1mStep[0m  [40/42], [94mLoss[0m : 10.05179

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.821, [92mTest[0m: 10.225, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89723
[1mStep[0m  [4/42], [94mLoss[0m : 9.76577
[1mStep[0m  [8/42], [94mLoss[0m : 9.61481
[1mStep[0m  [12/42], [94mLoss[0m : 9.57117
[1mStep[0m  [16/42], [94mLoss[0m : 9.77509
[1mStep[0m  [20/42], [94mLoss[0m : 9.75256
[1mStep[0m  [24/42], [94mLoss[0m : 9.36875
[1mStep[0m  [28/42], [94mLoss[0m : 9.92948
[1mStep[0m  [32/42], [94mLoss[0m : 9.68495
[1mStep[0m  [36/42], [94mLoss[0m : 9.93747
[1mStep[0m  [40/42], [94mLoss[0m : 9.59794

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.783, [92mTest[0m: 10.205, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.164
====================================

Phase 1 - Evaluation MAE:  10.16397762298584
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 9.85610
[1mStep[0m  [4/42], [94mLoss[0m : 9.40708
[1mStep[0m  [8/42], [94mLoss[0m : 9.86716
[1mStep[0m  [12/42], [94mLoss[0m : 9.77869
[1mStep[0m  [16/42], [94mLoss[0m : 10.38183
[1mStep[0m  [20/42], [94mLoss[0m : 9.73495
[1mStep[0m  [24/42], [94mLoss[0m : 9.97785
[1mStep[0m  [28/42], [94mLoss[0m : 9.75752
[1mStep[0m  [32/42], [94mLoss[0m : 9.62058
[1mStep[0m  [36/42], [94mLoss[0m : 9.71523
[1mStep[0m  [40/42], [94mLoss[0m : 9.69144

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.745, [92mTest[0m: 10.173, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.42085
[1mStep[0m  [4/42], [94mLoss[0m : 9.65606
[1mStep[0m  [8/42], [94mLoss[0m : 9.79599
[1mStep[0m  [12/42], [94mLoss[0m : 9.39939
[1mStep[0m  [16/42], [94mLoss[0m : 9.80493
[1mStep[0m  [20/42], [94mLoss[0m : 9.98033
[1mStep[0m  [24/42], [94mLoss[0m : 9.81753
[1mStep[0m  [28/42], [94mLoss[0m : 9.74007
[1mStep[0m  [32/42], [94mLoss[0m : 9.59831
[1mStep[0m  [36/42], [94mLoss[0m : 9.62068
[1mStep[0m  [40/42], [94mLoss[0m : 9.71370

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.689, [92mTest[0m: 10.140, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25013
[1mStep[0m  [4/42], [94mLoss[0m : 9.62840
[1mStep[0m  [8/42], [94mLoss[0m : 9.98411
[1mStep[0m  [12/42], [94mLoss[0m : 9.49652
[1mStep[0m  [16/42], [94mLoss[0m : 9.70937
[1mStep[0m  [20/42], [94mLoss[0m : 9.79995
[1mStep[0m  [24/42], [94mLoss[0m : 9.55861
[1mStep[0m  [28/42], [94mLoss[0m : 9.40409
[1mStep[0m  [32/42], [94mLoss[0m : 9.92662
[1mStep[0m  [36/42], [94mLoss[0m : 9.68961
[1mStep[0m  [40/42], [94mLoss[0m : 9.92052

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.668, [92mTest[0m: 10.125, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.56463
[1mStep[0m  [4/42], [94mLoss[0m : 9.85890
[1mStep[0m  [8/42], [94mLoss[0m : 9.58863
[1mStep[0m  [12/42], [94mLoss[0m : 9.75465
[1mStep[0m  [16/42], [94mLoss[0m : 9.36004
[1mStep[0m  [20/42], [94mLoss[0m : 9.81622
[1mStep[0m  [24/42], [94mLoss[0m : 9.94336
[1mStep[0m  [28/42], [94mLoss[0m : 9.90593
[1mStep[0m  [32/42], [94mLoss[0m : 9.59847
[1mStep[0m  [36/42], [94mLoss[0m : 9.76892
[1mStep[0m  [40/42], [94mLoss[0m : 9.44859

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.618, [92mTest[0m: 10.081, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.35895
[1mStep[0m  [4/42], [94mLoss[0m : 9.51757
[1mStep[0m  [8/42], [94mLoss[0m : 9.42073
[1mStep[0m  [12/42], [94mLoss[0m : 9.54847
[1mStep[0m  [16/42], [94mLoss[0m : 9.72483
[1mStep[0m  [20/42], [94mLoss[0m : 9.67258
[1mStep[0m  [24/42], [94mLoss[0m : 9.37834
[1mStep[0m  [28/42], [94mLoss[0m : 9.27089
[1mStep[0m  [32/42], [94mLoss[0m : 9.56799
[1mStep[0m  [36/42], [94mLoss[0m : 9.73348
[1mStep[0m  [40/42], [94mLoss[0m : 10.07509

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.570, [92mTest[0m: 10.065, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.82274
[1mStep[0m  [4/42], [94mLoss[0m : 9.42803
[1mStep[0m  [8/42], [94mLoss[0m : 9.45292
[1mStep[0m  [12/42], [94mLoss[0m : 9.51494
[1mStep[0m  [16/42], [94mLoss[0m : 9.42123
[1mStep[0m  [20/42], [94mLoss[0m : 9.31581
[1mStep[0m  [24/42], [94mLoss[0m : 9.68664
[1mStep[0m  [28/42], [94mLoss[0m : 9.59554
[1mStep[0m  [32/42], [94mLoss[0m : 9.46979
[1mStep[0m  [36/42], [94mLoss[0m : 9.56361
[1mStep[0m  [40/42], [94mLoss[0m : 9.46489

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.538, [92mTest[0m: 10.027, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.41776
[1mStep[0m  [4/42], [94mLoss[0m : 9.54813
[1mStep[0m  [8/42], [94mLoss[0m : 9.35065
[1mStep[0m  [12/42], [94mLoss[0m : 9.82894
[1mStep[0m  [16/42], [94mLoss[0m : 9.45920
[1mStep[0m  [20/42], [94mLoss[0m : 9.83610
[1mStep[0m  [24/42], [94mLoss[0m : 9.40082
[1mStep[0m  [28/42], [94mLoss[0m : 9.50518
[1mStep[0m  [32/42], [94mLoss[0m : 9.41078
[1mStep[0m  [36/42], [94mLoss[0m : 9.64602
[1mStep[0m  [40/42], [94mLoss[0m : 9.47332

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.494, [92mTest[0m: 10.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.16853
[1mStep[0m  [4/42], [94mLoss[0m : 9.16209
[1mStep[0m  [8/42], [94mLoss[0m : 9.34724
[1mStep[0m  [12/42], [94mLoss[0m : 9.13991
[1mStep[0m  [16/42], [94mLoss[0m : 9.55203
[1mStep[0m  [20/42], [94mLoss[0m : 9.22749
[1mStep[0m  [24/42], [94mLoss[0m : 9.54992
[1mStep[0m  [28/42], [94mLoss[0m : 9.57694
[1mStep[0m  [32/42], [94mLoss[0m : 9.65254
[1mStep[0m  [36/42], [94mLoss[0m : 9.53939
[1mStep[0m  [40/42], [94mLoss[0m : 9.67394

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.453, [92mTest[0m: 9.970, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.23087
[1mStep[0m  [4/42], [94mLoss[0m : 9.47136
[1mStep[0m  [8/42], [94mLoss[0m : 9.67070
[1mStep[0m  [12/42], [94mLoss[0m : 9.32250
[1mStep[0m  [16/42], [94mLoss[0m : 9.77349
[1mStep[0m  [20/42], [94mLoss[0m : 9.26451
[1mStep[0m  [24/42], [94mLoss[0m : 9.50221
[1mStep[0m  [28/42], [94mLoss[0m : 9.24042
[1mStep[0m  [32/42], [94mLoss[0m : 9.23860
[1mStep[0m  [36/42], [94mLoss[0m : 9.10586
[1mStep[0m  [40/42], [94mLoss[0m : 9.40102

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.407, [92mTest[0m: 9.964, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.33664
[1mStep[0m  [4/42], [94mLoss[0m : 9.23968
[1mStep[0m  [8/42], [94mLoss[0m : 9.07200
[1mStep[0m  [12/42], [94mLoss[0m : 9.76265
[1mStep[0m  [16/42], [94mLoss[0m : 9.32150
[1mStep[0m  [20/42], [94mLoss[0m : 9.17542
[1mStep[0m  [24/42], [94mLoss[0m : 9.14755
[1mStep[0m  [28/42], [94mLoss[0m : 9.52300
[1mStep[0m  [32/42], [94mLoss[0m : 9.17962
[1mStep[0m  [36/42], [94mLoss[0m : 9.18531
[1mStep[0m  [40/42], [94mLoss[0m : 9.37112

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.372, [92mTest[0m: 9.924, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.49378
[1mStep[0m  [4/42], [94mLoss[0m : 9.34739
[1mStep[0m  [8/42], [94mLoss[0m : 9.17244
[1mStep[0m  [12/42], [94mLoss[0m : 9.06691
[1mStep[0m  [16/42], [94mLoss[0m : 8.88321
[1mStep[0m  [20/42], [94mLoss[0m : 9.14615
[1mStep[0m  [24/42], [94mLoss[0m : 9.37684
[1mStep[0m  [28/42], [94mLoss[0m : 9.05407
[1mStep[0m  [32/42], [94mLoss[0m : 9.42937
[1mStep[0m  [36/42], [94mLoss[0m : 9.50788
[1mStep[0m  [40/42], [94mLoss[0m : 9.70448

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.336, [92mTest[0m: 9.913, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.19883
[1mStep[0m  [4/42], [94mLoss[0m : 9.54823
[1mStep[0m  [8/42], [94mLoss[0m : 9.37698
[1mStep[0m  [12/42], [94mLoss[0m : 9.56484
[1mStep[0m  [16/42], [94mLoss[0m : 9.69699
[1mStep[0m  [20/42], [94mLoss[0m : 9.23769
[1mStep[0m  [24/42], [94mLoss[0m : 9.50031
[1mStep[0m  [28/42], [94mLoss[0m : 9.31708
[1mStep[0m  [32/42], [94mLoss[0m : 9.17329
[1mStep[0m  [36/42], [94mLoss[0m : 8.79389
[1mStep[0m  [40/42], [94mLoss[0m : 9.03378

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.285, [92mTest[0m: 9.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.32199
[1mStep[0m  [4/42], [94mLoss[0m : 9.27810
[1mStep[0m  [8/42], [94mLoss[0m : 9.05099
[1mStep[0m  [12/42], [94mLoss[0m : 9.24012
[1mStep[0m  [16/42], [94mLoss[0m : 9.17247
[1mStep[0m  [20/42], [94mLoss[0m : 9.56086
[1mStep[0m  [24/42], [94mLoss[0m : 9.11979
[1mStep[0m  [28/42], [94mLoss[0m : 9.25823
[1mStep[0m  [32/42], [94mLoss[0m : 9.22352
[1mStep[0m  [36/42], [94mLoss[0m : 9.05626
[1mStep[0m  [40/42], [94mLoss[0m : 9.17000

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.245, [92mTest[0m: 9.836, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.10716
[1mStep[0m  [4/42], [94mLoss[0m : 9.06871
[1mStep[0m  [8/42], [94mLoss[0m : 8.92988
[1mStep[0m  [12/42], [94mLoss[0m : 9.52522
[1mStep[0m  [16/42], [94mLoss[0m : 9.57223
[1mStep[0m  [20/42], [94mLoss[0m : 9.44252
[1mStep[0m  [24/42], [94mLoss[0m : 8.97800
[1mStep[0m  [28/42], [94mLoss[0m : 9.05726
[1mStep[0m  [32/42], [94mLoss[0m : 9.12377
[1mStep[0m  [36/42], [94mLoss[0m : 9.44790
[1mStep[0m  [40/42], [94mLoss[0m : 9.40503

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.207, [92mTest[0m: 9.846, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.56612
[1mStep[0m  [4/42], [94mLoss[0m : 9.21243
[1mStep[0m  [8/42], [94mLoss[0m : 9.21318
[1mStep[0m  [12/42], [94mLoss[0m : 9.00439
[1mStep[0m  [16/42], [94mLoss[0m : 9.62065
[1mStep[0m  [20/42], [94mLoss[0m : 8.96366
[1mStep[0m  [24/42], [94mLoss[0m : 9.44309
[1mStep[0m  [28/42], [94mLoss[0m : 9.12127
[1mStep[0m  [32/42], [94mLoss[0m : 8.86866
[1mStep[0m  [36/42], [94mLoss[0m : 8.92144
[1mStep[0m  [40/42], [94mLoss[0m : 9.18804

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.163, [92mTest[0m: 9.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.77580
[1mStep[0m  [4/42], [94mLoss[0m : 9.00971
[1mStep[0m  [8/42], [94mLoss[0m : 9.46877
[1mStep[0m  [12/42], [94mLoss[0m : 9.17599
[1mStep[0m  [16/42], [94mLoss[0m : 9.13379
[1mStep[0m  [20/42], [94mLoss[0m : 9.26422
[1mStep[0m  [24/42], [94mLoss[0m : 8.73542
[1mStep[0m  [28/42], [94mLoss[0m : 9.27909
[1mStep[0m  [32/42], [94mLoss[0m : 8.95960
[1mStep[0m  [36/42], [94mLoss[0m : 9.26702
[1mStep[0m  [40/42], [94mLoss[0m : 9.39121

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.114, [92mTest[0m: 9.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.04250
[1mStep[0m  [4/42], [94mLoss[0m : 8.93574
[1mStep[0m  [8/42], [94mLoss[0m : 9.00344
[1mStep[0m  [12/42], [94mLoss[0m : 9.22950
[1mStep[0m  [16/42], [94mLoss[0m : 9.28178
[1mStep[0m  [20/42], [94mLoss[0m : 8.72283
[1mStep[0m  [24/42], [94mLoss[0m : 9.48130
[1mStep[0m  [28/42], [94mLoss[0m : 9.41331
[1mStep[0m  [32/42], [94mLoss[0m : 8.91623
[1mStep[0m  [36/42], [94mLoss[0m : 8.66046
[1mStep[0m  [40/42], [94mLoss[0m : 9.27566

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.071, [92mTest[0m: 9.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.82203
[1mStep[0m  [4/42], [94mLoss[0m : 8.99204
[1mStep[0m  [8/42], [94mLoss[0m : 8.83321
[1mStep[0m  [12/42], [94mLoss[0m : 8.87404
[1mStep[0m  [16/42], [94mLoss[0m : 8.81034
[1mStep[0m  [20/42], [94mLoss[0m : 8.98832
[1mStep[0m  [24/42], [94mLoss[0m : 9.24955
[1mStep[0m  [28/42], [94mLoss[0m : 8.95237
[1mStep[0m  [32/42], [94mLoss[0m : 8.49825
[1mStep[0m  [36/42], [94mLoss[0m : 9.27731
[1mStep[0m  [40/42], [94mLoss[0m : 9.28014

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.034, [92mTest[0m: 9.705, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.91331
[1mStep[0m  [4/42], [94mLoss[0m : 9.20936
[1mStep[0m  [8/42], [94mLoss[0m : 9.13023
[1mStep[0m  [12/42], [94mLoss[0m : 8.41422
[1mStep[0m  [16/42], [94mLoss[0m : 8.86589
[1mStep[0m  [20/42], [94mLoss[0m : 9.18148
[1mStep[0m  [24/42], [94mLoss[0m : 8.98829
[1mStep[0m  [28/42], [94mLoss[0m : 8.86043
[1mStep[0m  [32/42], [94mLoss[0m : 8.80093
[1mStep[0m  [36/42], [94mLoss[0m : 8.76257
[1mStep[0m  [40/42], [94mLoss[0m : 9.08343

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.000, [92mTest[0m: 9.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.02684
[1mStep[0m  [4/42], [94mLoss[0m : 9.15798
[1mStep[0m  [8/42], [94mLoss[0m : 8.89514
[1mStep[0m  [12/42], [94mLoss[0m : 8.85740
[1mStep[0m  [16/42], [94mLoss[0m : 9.13307
[1mStep[0m  [20/42], [94mLoss[0m : 9.02323
[1mStep[0m  [24/42], [94mLoss[0m : 9.32370
[1mStep[0m  [28/42], [94mLoss[0m : 8.51533
[1mStep[0m  [32/42], [94mLoss[0m : 8.91657
[1mStep[0m  [36/42], [94mLoss[0m : 9.03089
[1mStep[0m  [40/42], [94mLoss[0m : 8.77369

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.954, [92mTest[0m: 9.653, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.57487
[1mStep[0m  [4/42], [94mLoss[0m : 8.97078
[1mStep[0m  [8/42], [94mLoss[0m : 9.18969
[1mStep[0m  [12/42], [94mLoss[0m : 8.52434
[1mStep[0m  [16/42], [94mLoss[0m : 8.62042
[1mStep[0m  [20/42], [94mLoss[0m : 8.87373
[1mStep[0m  [24/42], [94mLoss[0m : 9.21574
[1mStep[0m  [28/42], [94mLoss[0m : 8.67924
[1mStep[0m  [32/42], [94mLoss[0m : 9.06813
[1mStep[0m  [36/42], [94mLoss[0m : 8.76092
[1mStep[0m  [40/42], [94mLoss[0m : 8.41549

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.906, [92mTest[0m: 9.621, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.61407
[1mStep[0m  [4/42], [94mLoss[0m : 8.76340
[1mStep[0m  [8/42], [94mLoss[0m : 9.20844
[1mStep[0m  [12/42], [94mLoss[0m : 8.52460
[1mStep[0m  [16/42], [94mLoss[0m : 8.67944
[1mStep[0m  [20/42], [94mLoss[0m : 9.10365
[1mStep[0m  [24/42], [94mLoss[0m : 8.93739
[1mStep[0m  [28/42], [94mLoss[0m : 8.86641
[1mStep[0m  [32/42], [94mLoss[0m : 8.68265
[1mStep[0m  [36/42], [94mLoss[0m : 9.02055
[1mStep[0m  [40/42], [94mLoss[0m : 8.96376

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.862, [92mTest[0m: 9.588, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.15359
[1mStep[0m  [4/42], [94mLoss[0m : 9.08757
[1mStep[0m  [8/42], [94mLoss[0m : 8.82226
[1mStep[0m  [12/42], [94mLoss[0m : 8.54888
[1mStep[0m  [16/42], [94mLoss[0m : 8.83061
[1mStep[0m  [20/42], [94mLoss[0m : 8.45126
[1mStep[0m  [24/42], [94mLoss[0m : 8.91776
[1mStep[0m  [28/42], [94mLoss[0m : 9.34885
[1mStep[0m  [32/42], [94mLoss[0m : 8.89139
[1mStep[0m  [36/42], [94mLoss[0m : 9.20481
[1mStep[0m  [40/42], [94mLoss[0m : 8.83532

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.848, [92mTest[0m: 9.586, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.79856
[1mStep[0m  [4/42], [94mLoss[0m : 8.88811
[1mStep[0m  [8/42], [94mLoss[0m : 8.76776
[1mStep[0m  [12/42], [94mLoss[0m : 8.88333
[1mStep[0m  [16/42], [94mLoss[0m : 8.76653
[1mStep[0m  [20/42], [94mLoss[0m : 8.97457
[1mStep[0m  [24/42], [94mLoss[0m : 8.74147
[1mStep[0m  [28/42], [94mLoss[0m : 8.44333
[1mStep[0m  [32/42], [94mLoss[0m : 8.92744
[1mStep[0m  [36/42], [94mLoss[0m : 8.63469
[1mStep[0m  [40/42], [94mLoss[0m : 8.87473

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.786, [92mTest[0m: 9.549, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.57985
[1mStep[0m  [4/42], [94mLoss[0m : 8.78377
[1mStep[0m  [8/42], [94mLoss[0m : 9.09785
[1mStep[0m  [12/42], [94mLoss[0m : 8.83167
[1mStep[0m  [16/42], [94mLoss[0m : 8.53061
[1mStep[0m  [20/42], [94mLoss[0m : 8.50992
[1mStep[0m  [24/42], [94mLoss[0m : 8.39976
[1mStep[0m  [28/42], [94mLoss[0m : 9.24045
[1mStep[0m  [32/42], [94mLoss[0m : 8.70182
[1mStep[0m  [36/42], [94mLoss[0m : 8.80371
[1mStep[0m  [40/42], [94mLoss[0m : 8.79557

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.765, [92mTest[0m: 9.528, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.46004
[1mStep[0m  [4/42], [94mLoss[0m : 8.37962
[1mStep[0m  [8/42], [94mLoss[0m : 8.71728
[1mStep[0m  [12/42], [94mLoss[0m : 8.22898
[1mStep[0m  [16/42], [94mLoss[0m : 8.32275
[1mStep[0m  [20/42], [94mLoss[0m : 8.79015
[1mStep[0m  [24/42], [94mLoss[0m : 9.27909
[1mStep[0m  [28/42], [94mLoss[0m : 9.02377
[1mStep[0m  [32/42], [94mLoss[0m : 8.85800
[1mStep[0m  [36/42], [94mLoss[0m : 8.50378
[1mStep[0m  [40/42], [94mLoss[0m : 8.51830

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.725, [92mTest[0m: 9.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.78631
[1mStep[0m  [4/42], [94mLoss[0m : 8.54681
[1mStep[0m  [8/42], [94mLoss[0m : 8.69218
[1mStep[0m  [12/42], [94mLoss[0m : 8.47693
[1mStep[0m  [16/42], [94mLoss[0m : 8.67109
[1mStep[0m  [20/42], [94mLoss[0m : 8.69287
[1mStep[0m  [24/42], [94mLoss[0m : 8.91830
[1mStep[0m  [28/42], [94mLoss[0m : 8.41299
[1mStep[0m  [32/42], [94mLoss[0m : 8.83738
[1mStep[0m  [36/42], [94mLoss[0m : 8.48502
[1mStep[0m  [40/42], [94mLoss[0m : 8.66078

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.692, [92mTest[0m: 9.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.77548
[1mStep[0m  [4/42], [94mLoss[0m : 8.78243
[1mStep[0m  [8/42], [94mLoss[0m : 9.05019
[1mStep[0m  [12/42], [94mLoss[0m : 8.69549
[1mStep[0m  [16/42], [94mLoss[0m : 8.17504
[1mStep[0m  [20/42], [94mLoss[0m : 8.77215
[1mStep[0m  [24/42], [94mLoss[0m : 8.70778
[1mStep[0m  [28/42], [94mLoss[0m : 8.70661
[1mStep[0m  [32/42], [94mLoss[0m : 8.34630
[1mStep[0m  [36/42], [94mLoss[0m : 8.27882
[1mStep[0m  [40/42], [94mLoss[0m : 8.73814

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.640, [92mTest[0m: 9.464, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.54859
[1mStep[0m  [4/42], [94mLoss[0m : 8.50443
[1mStep[0m  [8/42], [94mLoss[0m : 8.97059
[1mStep[0m  [12/42], [94mLoss[0m : 8.29153
[1mStep[0m  [16/42], [94mLoss[0m : 8.10748
[1mStep[0m  [20/42], [94mLoss[0m : 8.84461
[1mStep[0m  [24/42], [94mLoss[0m : 8.38232
[1mStep[0m  [28/42], [94mLoss[0m : 8.73668
[1mStep[0m  [32/42], [94mLoss[0m : 8.59699
[1mStep[0m  [36/42], [94mLoss[0m : 8.45662
[1mStep[0m  [40/42], [94mLoss[0m : 8.43146

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.611, [92mTest[0m: 9.418, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.72782
[1mStep[0m  [4/42], [94mLoss[0m : 8.64755
[1mStep[0m  [8/42], [94mLoss[0m : 8.62979
[1mStep[0m  [12/42], [94mLoss[0m : 8.69781
[1mStep[0m  [16/42], [94mLoss[0m : 8.65550
[1mStep[0m  [20/42], [94mLoss[0m : 8.15139
[1mStep[0m  [24/42], [94mLoss[0m : 8.26405
[1mStep[0m  [28/42], [94mLoss[0m : 8.41271
[1mStep[0m  [32/42], [94mLoss[0m : 8.02639
[1mStep[0m  [36/42], [94mLoss[0m : 8.51423
[1mStep[0m  [40/42], [94mLoss[0m : 8.83687

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.574, [92mTest[0m: 9.368, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.393
====================================

Phase 2 - Evaluation MAE:  9.39284883226667
MAE score P1      10.163978
MAE score P2       9.392849
loss               8.573877
learning_rate        0.0001
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.42835
[1mStep[0m  [4/42], [94mLoss[0m : 10.61558
[1mStep[0m  [8/42], [94mLoss[0m : 10.79872
[1mStep[0m  [12/42], [94mLoss[0m : 10.63146
[1mStep[0m  [16/42], [94mLoss[0m : 10.25769
[1mStep[0m  [20/42], [94mLoss[0m : 10.79447
[1mStep[0m  [24/42], [94mLoss[0m : 10.32593
[1mStep[0m  [28/42], [94mLoss[0m : 10.58788
[1mStep[0m  [32/42], [94mLoss[0m : 10.75148
[1mStep[0m  [36/42], [94mLoss[0m : 10.28097
[1mStep[0m  [40/42], [94mLoss[0m : 10.53701

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.612, [92mTest[0m: 10.739, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54094
[1mStep[0m  [4/42], [94mLoss[0m : 10.28995
[1mStep[0m  [8/42], [94mLoss[0m : 9.99058
[1mStep[0m  [12/42], [94mLoss[0m : 10.23445
[1mStep[0m  [16/42], [94mLoss[0m : 10.56219
[1mStep[0m  [20/42], [94mLoss[0m : 10.63872
[1mStep[0m  [24/42], [94mLoss[0m : 10.82374
[1mStep[0m  [28/42], [94mLoss[0m : 10.15373
[1mStep[0m  [32/42], [94mLoss[0m : 10.18765
[1mStep[0m  [36/42], [94mLoss[0m : 10.03912
[1mStep[0m  [40/42], [94mLoss[0m : 10.25289

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.312, [92mTest[0m: 10.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36591
[1mStep[0m  [4/42], [94mLoss[0m : 10.27045
[1mStep[0m  [8/42], [94mLoss[0m : 10.14332
[1mStep[0m  [12/42], [94mLoss[0m : 10.30312
[1mStep[0m  [16/42], [94mLoss[0m : 9.70197
[1mStep[0m  [20/42], [94mLoss[0m : 10.18738
[1mStep[0m  [24/42], [94mLoss[0m : 10.31340
[1mStep[0m  [28/42], [94mLoss[0m : 9.74188
[1mStep[0m  [32/42], [94mLoss[0m : 10.12939
[1mStep[0m  [36/42], [94mLoss[0m : 9.51778
[1mStep[0m  [40/42], [94mLoss[0m : 9.71171

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.014, [92mTest[0m: 10.159, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.87954
[1mStep[0m  [4/42], [94mLoss[0m : 9.94926
[1mStep[0m  [8/42], [94mLoss[0m : 9.34963
[1mStep[0m  [12/42], [94mLoss[0m : 9.91695
[1mStep[0m  [16/42], [94mLoss[0m : 10.06371
[1mStep[0m  [20/42], [94mLoss[0m : 9.63603
[1mStep[0m  [24/42], [94mLoss[0m : 9.51238
[1mStep[0m  [28/42], [94mLoss[0m : 9.80516
[1mStep[0m  [32/42], [94mLoss[0m : 9.44312
[1mStep[0m  [36/42], [94mLoss[0m : 9.58319
[1mStep[0m  [40/42], [94mLoss[0m : 9.65602

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.713, [92mTest[0m: 9.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.61541
[1mStep[0m  [4/42], [94mLoss[0m : 9.68524
[1mStep[0m  [8/42], [94mLoss[0m : 9.36119
[1mStep[0m  [12/42], [94mLoss[0m : 9.69390
[1mStep[0m  [16/42], [94mLoss[0m : 9.79874
[1mStep[0m  [20/42], [94mLoss[0m : 9.42612
[1mStep[0m  [24/42], [94mLoss[0m : 9.33735
[1mStep[0m  [28/42], [94mLoss[0m : 9.10670
[1mStep[0m  [32/42], [94mLoss[0m : 9.48583
[1mStep[0m  [36/42], [94mLoss[0m : 9.13603
[1mStep[0m  [40/42], [94mLoss[0m : 9.56347

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.426, [92mTest[0m: 9.559, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.15894
[1mStep[0m  [4/42], [94mLoss[0m : 9.18082
[1mStep[0m  [8/42], [94mLoss[0m : 9.39003
[1mStep[0m  [12/42], [94mLoss[0m : 9.25844
[1mStep[0m  [16/42], [94mLoss[0m : 8.68418
[1mStep[0m  [20/42], [94mLoss[0m : 9.28806
[1mStep[0m  [24/42], [94mLoss[0m : 8.91386
[1mStep[0m  [28/42], [94mLoss[0m : 8.97920
[1mStep[0m  [32/42], [94mLoss[0m : 8.78570
[1mStep[0m  [36/42], [94mLoss[0m : 9.27561
[1mStep[0m  [40/42], [94mLoss[0m : 8.80035

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.116, [92mTest[0m: 9.273, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.24310
[1mStep[0m  [4/42], [94mLoss[0m : 8.85328
[1mStep[0m  [8/42], [94mLoss[0m : 8.83371
[1mStep[0m  [12/42], [94mLoss[0m : 8.90263
[1mStep[0m  [16/42], [94mLoss[0m : 8.54614
[1mStep[0m  [20/42], [94mLoss[0m : 9.22724
[1mStep[0m  [24/42], [94mLoss[0m : 9.06009
[1mStep[0m  [28/42], [94mLoss[0m : 8.65374
[1mStep[0m  [32/42], [94mLoss[0m : 8.47448
[1mStep[0m  [36/42], [94mLoss[0m : 9.28395
[1mStep[0m  [40/42], [94mLoss[0m : 8.74849

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.826, [92mTest[0m: 8.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.95146
[1mStep[0m  [4/42], [94mLoss[0m : 8.95861
[1mStep[0m  [8/42], [94mLoss[0m : 8.60078
[1mStep[0m  [12/42], [94mLoss[0m : 8.72912
[1mStep[0m  [16/42], [94mLoss[0m : 8.82262
[1mStep[0m  [20/42], [94mLoss[0m : 8.19971
[1mStep[0m  [24/42], [94mLoss[0m : 8.58667
[1mStep[0m  [28/42], [94mLoss[0m : 8.31043
[1mStep[0m  [32/42], [94mLoss[0m : 8.25469
[1mStep[0m  [36/42], [94mLoss[0m : 8.22256
[1mStep[0m  [40/42], [94mLoss[0m : 8.39613

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.531, [92mTest[0m: 8.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.08965
[1mStep[0m  [4/42], [94mLoss[0m : 8.38806
[1mStep[0m  [8/42], [94mLoss[0m : 8.08690
[1mStep[0m  [12/42], [94mLoss[0m : 8.35745
[1mStep[0m  [16/42], [94mLoss[0m : 8.31530
[1mStep[0m  [20/42], [94mLoss[0m : 8.72135
[1mStep[0m  [24/42], [94mLoss[0m : 8.08814
[1mStep[0m  [28/42], [94mLoss[0m : 7.65234
[1mStep[0m  [32/42], [94mLoss[0m : 7.88272
[1mStep[0m  [36/42], [94mLoss[0m : 7.99241
[1mStep[0m  [40/42], [94mLoss[0m : 7.93120

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.232, [92mTest[0m: 8.380, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.28630
[1mStep[0m  [4/42], [94mLoss[0m : 8.21904
[1mStep[0m  [8/42], [94mLoss[0m : 8.16469
[1mStep[0m  [12/42], [94mLoss[0m : 7.90255
[1mStep[0m  [16/42], [94mLoss[0m : 7.71704
[1mStep[0m  [20/42], [94mLoss[0m : 8.00183
[1mStep[0m  [24/42], [94mLoss[0m : 7.96581
[1mStep[0m  [28/42], [94mLoss[0m : 7.78522
[1mStep[0m  [32/42], [94mLoss[0m : 7.70836
[1mStep[0m  [36/42], [94mLoss[0m : 7.88638
[1mStep[0m  [40/42], [94mLoss[0m : 7.57099

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.933, [92mTest[0m: 8.084, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.71555
[1mStep[0m  [4/42], [94mLoss[0m : 7.85726
[1mStep[0m  [8/42], [94mLoss[0m : 8.47708
[1mStep[0m  [12/42], [94mLoss[0m : 7.91628
[1mStep[0m  [16/42], [94mLoss[0m : 7.81118
[1mStep[0m  [20/42], [94mLoss[0m : 7.80555
[1mStep[0m  [24/42], [94mLoss[0m : 7.68438
[1mStep[0m  [28/42], [94mLoss[0m : 7.75787
[1mStep[0m  [32/42], [94mLoss[0m : 7.52871
[1mStep[0m  [36/42], [94mLoss[0m : 7.60199
[1mStep[0m  [40/42], [94mLoss[0m : 7.28962

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.640, [92mTest[0m: 7.788, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.62493
[1mStep[0m  [4/42], [94mLoss[0m : 7.23880
[1mStep[0m  [8/42], [94mLoss[0m : 7.67016
[1mStep[0m  [12/42], [94mLoss[0m : 7.01439
[1mStep[0m  [16/42], [94mLoss[0m : 7.29452
[1mStep[0m  [20/42], [94mLoss[0m : 7.54633
[1mStep[0m  [24/42], [94mLoss[0m : 7.06222
[1mStep[0m  [28/42], [94mLoss[0m : 7.41838
[1mStep[0m  [32/42], [94mLoss[0m : 7.32889
[1mStep[0m  [36/42], [94mLoss[0m : 6.97666
[1mStep[0m  [40/42], [94mLoss[0m : 7.09699

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.340, [92mTest[0m: 7.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.16512
[1mStep[0m  [4/42], [94mLoss[0m : 7.24602
[1mStep[0m  [8/42], [94mLoss[0m : 7.26078
[1mStep[0m  [12/42], [94mLoss[0m : 7.26022
[1mStep[0m  [16/42], [94mLoss[0m : 7.15885
[1mStep[0m  [20/42], [94mLoss[0m : 7.09283
[1mStep[0m  [24/42], [94mLoss[0m : 6.62443
[1mStep[0m  [28/42], [94mLoss[0m : 6.88240
[1mStep[0m  [32/42], [94mLoss[0m : 7.19834
[1mStep[0m  [36/42], [94mLoss[0m : 6.93018
[1mStep[0m  [40/42], [94mLoss[0m : 6.89108

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.052, [92mTest[0m: 7.180, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.85378
[1mStep[0m  [4/42], [94mLoss[0m : 7.18618
[1mStep[0m  [8/42], [94mLoss[0m : 6.82466
[1mStep[0m  [12/42], [94mLoss[0m : 7.22139
[1mStep[0m  [16/42], [94mLoss[0m : 6.49808
[1mStep[0m  [20/42], [94mLoss[0m : 6.86631
[1mStep[0m  [24/42], [94mLoss[0m : 7.09953
[1mStep[0m  [28/42], [94mLoss[0m : 6.70739
[1mStep[0m  [32/42], [94mLoss[0m : 7.21760
[1mStep[0m  [36/42], [94mLoss[0m : 6.58899
[1mStep[0m  [40/42], [94mLoss[0m : 6.40137

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.749, [92mTest[0m: 6.892, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.76801
[1mStep[0m  [4/42], [94mLoss[0m : 6.46766
[1mStep[0m  [8/42], [94mLoss[0m : 6.33843
[1mStep[0m  [12/42], [94mLoss[0m : 6.66509
[1mStep[0m  [16/42], [94mLoss[0m : 6.41244
[1mStep[0m  [20/42], [94mLoss[0m : 6.39473
[1mStep[0m  [24/42], [94mLoss[0m : 6.30677
[1mStep[0m  [28/42], [94mLoss[0m : 6.76435
[1mStep[0m  [32/42], [94mLoss[0m : 6.36608
[1mStep[0m  [36/42], [94mLoss[0m : 6.18814
[1mStep[0m  [40/42], [94mLoss[0m : 6.38092

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.463, [92mTest[0m: 6.591, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.07854
[1mStep[0m  [4/42], [94mLoss[0m : 6.31717
[1mStep[0m  [8/42], [94mLoss[0m : 5.90738
[1mStep[0m  [12/42], [94mLoss[0m : 6.02787
[1mStep[0m  [16/42], [94mLoss[0m : 5.93833
[1mStep[0m  [20/42], [94mLoss[0m : 6.19827
[1mStep[0m  [24/42], [94mLoss[0m : 6.09776
[1mStep[0m  [28/42], [94mLoss[0m : 6.05623
[1mStep[0m  [32/42], [94mLoss[0m : 6.34535
[1mStep[0m  [36/42], [94mLoss[0m : 6.36982
[1mStep[0m  [40/42], [94mLoss[0m : 5.82581

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.153, [92mTest[0m: 6.295, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.01453
[1mStep[0m  [4/42], [94mLoss[0m : 6.24755
[1mStep[0m  [8/42], [94mLoss[0m : 6.02895
[1mStep[0m  [12/42], [94mLoss[0m : 6.08800
[1mStep[0m  [16/42], [94mLoss[0m : 5.74163
[1mStep[0m  [20/42], [94mLoss[0m : 5.99664
[1mStep[0m  [24/42], [94mLoss[0m : 5.73278
[1mStep[0m  [28/42], [94mLoss[0m : 5.57532
[1mStep[0m  [32/42], [94mLoss[0m : 6.27762
[1mStep[0m  [36/42], [94mLoss[0m : 5.64905
[1mStep[0m  [40/42], [94mLoss[0m : 5.58615

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.863, [92mTest[0m: 6.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.98441
[1mStep[0m  [4/42], [94mLoss[0m : 5.51898
[1mStep[0m  [8/42], [94mLoss[0m : 5.74593
[1mStep[0m  [12/42], [94mLoss[0m : 5.86241
[1mStep[0m  [16/42], [94mLoss[0m : 5.50586
[1mStep[0m  [20/42], [94mLoss[0m : 5.47349
[1mStep[0m  [24/42], [94mLoss[0m : 5.42639
[1mStep[0m  [28/42], [94mLoss[0m : 5.27374
[1mStep[0m  [32/42], [94mLoss[0m : 5.57438
[1mStep[0m  [36/42], [94mLoss[0m : 5.38998
[1mStep[0m  [40/42], [94mLoss[0m : 5.55876

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.571, [92mTest[0m: 5.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.58663
[1mStep[0m  [4/42], [94mLoss[0m : 5.54073
[1mStep[0m  [8/42], [94mLoss[0m : 5.83874
[1mStep[0m  [12/42], [94mLoss[0m : 5.57879
[1mStep[0m  [16/42], [94mLoss[0m : 4.98563
[1mStep[0m  [20/42], [94mLoss[0m : 5.35295
[1mStep[0m  [24/42], [94mLoss[0m : 5.17555
[1mStep[0m  [28/42], [94mLoss[0m : 5.18754
[1mStep[0m  [32/42], [94mLoss[0m : 5.27333
[1mStep[0m  [36/42], [94mLoss[0m : 5.50778
[1mStep[0m  [40/42], [94mLoss[0m : 4.92559

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.294, [92mTest[0m: 5.433, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.43915
[1mStep[0m  [4/42], [94mLoss[0m : 4.75846
[1mStep[0m  [8/42], [94mLoss[0m : 5.24241
[1mStep[0m  [12/42], [94mLoss[0m : 5.20868
[1mStep[0m  [16/42], [94mLoss[0m : 5.18595
[1mStep[0m  [20/42], [94mLoss[0m : 4.68909
[1mStep[0m  [24/42], [94mLoss[0m : 5.06141
[1mStep[0m  [28/42], [94mLoss[0m : 5.01231
[1mStep[0m  [32/42], [94mLoss[0m : 5.23012
[1mStep[0m  [36/42], [94mLoss[0m : 4.66909
[1mStep[0m  [40/42], [94mLoss[0m : 4.62204

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.037, [92mTest[0m: 5.161, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.88524
[1mStep[0m  [4/42], [94mLoss[0m : 5.05542
[1mStep[0m  [8/42], [94mLoss[0m : 4.83716
[1mStep[0m  [12/42], [94mLoss[0m : 4.99625
[1mStep[0m  [16/42], [94mLoss[0m : 4.72458
[1mStep[0m  [20/42], [94mLoss[0m : 4.76160
[1mStep[0m  [24/42], [94mLoss[0m : 4.74941
[1mStep[0m  [28/42], [94mLoss[0m : 5.02507
[1mStep[0m  [32/42], [94mLoss[0m : 4.98413
[1mStep[0m  [36/42], [94mLoss[0m : 4.49375
[1mStep[0m  [40/42], [94mLoss[0m : 4.47120

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.806, [92mTest[0m: 4.914, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.74161
[1mStep[0m  [4/42], [94mLoss[0m : 5.04496
[1mStep[0m  [8/42], [94mLoss[0m : 4.67390
[1mStep[0m  [12/42], [94mLoss[0m : 4.42726
[1mStep[0m  [16/42], [94mLoss[0m : 4.78830
[1mStep[0m  [20/42], [94mLoss[0m : 4.23940
[1mStep[0m  [24/42], [94mLoss[0m : 4.50001
[1mStep[0m  [28/42], [94mLoss[0m : 4.69512
[1mStep[0m  [32/42], [94mLoss[0m : 4.87239
[1mStep[0m  [36/42], [94mLoss[0m : 4.28041
[1mStep[0m  [40/42], [94mLoss[0m : 4.46701

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.606, [92mTest[0m: 4.705, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.12242
[1mStep[0m  [4/42], [94mLoss[0m : 4.86482
[1mStep[0m  [8/42], [94mLoss[0m : 4.86582
[1mStep[0m  [12/42], [94mLoss[0m : 4.56071
[1mStep[0m  [16/42], [94mLoss[0m : 4.15706
[1mStep[0m  [20/42], [94mLoss[0m : 4.01372
[1mStep[0m  [24/42], [94mLoss[0m : 3.96316
[1mStep[0m  [28/42], [94mLoss[0m : 4.19706
[1mStep[0m  [32/42], [94mLoss[0m : 4.41050
[1mStep[0m  [36/42], [94mLoss[0m : 4.68674
[1mStep[0m  [40/42], [94mLoss[0m : 4.49996

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.422, [92mTest[0m: 4.512, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.26889
[1mStep[0m  [4/42], [94mLoss[0m : 3.98764
[1mStep[0m  [8/42], [94mLoss[0m : 4.27725
[1mStep[0m  [12/42], [94mLoss[0m : 4.15424
[1mStep[0m  [16/42], [94mLoss[0m : 4.45953
[1mStep[0m  [20/42], [94mLoss[0m : 3.87396
[1mStep[0m  [24/42], [94mLoss[0m : 4.18716
[1mStep[0m  [28/42], [94mLoss[0m : 4.12132
[1mStep[0m  [32/42], [94mLoss[0m : 4.06243
[1mStep[0m  [36/42], [94mLoss[0m : 4.33667
[1mStep[0m  [40/42], [94mLoss[0m : 3.66764

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.252, [92mTest[0m: 4.337, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.00432
[1mStep[0m  [4/42], [94mLoss[0m : 4.08120
[1mStep[0m  [8/42], [94mLoss[0m : 4.28324
[1mStep[0m  [12/42], [94mLoss[0m : 3.96444
[1mStep[0m  [16/42], [94mLoss[0m : 4.61003
[1mStep[0m  [20/42], [94mLoss[0m : 4.08961
[1mStep[0m  [24/42], [94mLoss[0m : 3.85598
[1mStep[0m  [28/42], [94mLoss[0m : 3.96430
[1mStep[0m  [32/42], [94mLoss[0m : 4.05553
[1mStep[0m  [36/42], [94mLoss[0m : 3.73401
[1mStep[0m  [40/42], [94mLoss[0m : 3.95923

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.102, [92mTest[0m: 4.175, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.81399
[1mStep[0m  [4/42], [94mLoss[0m : 4.08177
[1mStep[0m  [8/42], [94mLoss[0m : 3.91668
[1mStep[0m  [12/42], [94mLoss[0m : 4.19120
[1mStep[0m  [16/42], [94mLoss[0m : 4.06864
[1mStep[0m  [20/42], [94mLoss[0m : 3.94242
[1mStep[0m  [24/42], [94mLoss[0m : 3.91420
[1mStep[0m  [28/42], [94mLoss[0m : 4.09090
[1mStep[0m  [32/42], [94mLoss[0m : 4.21663
[1mStep[0m  [36/42], [94mLoss[0m : 3.72585
[1mStep[0m  [40/42], [94mLoss[0m : 3.36467

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.958, [92mTest[0m: 4.045, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.78167
[1mStep[0m  [4/42], [94mLoss[0m : 3.67642
[1mStep[0m  [8/42], [94mLoss[0m : 3.87621
[1mStep[0m  [12/42], [94mLoss[0m : 3.73677
[1mStep[0m  [16/42], [94mLoss[0m : 4.03133
[1mStep[0m  [20/42], [94mLoss[0m : 3.62192
[1mStep[0m  [24/42], [94mLoss[0m : 3.96947
[1mStep[0m  [28/42], [94mLoss[0m : 3.51896
[1mStep[0m  [32/42], [94mLoss[0m : 3.88550
[1mStep[0m  [36/42], [94mLoss[0m : 3.52687
[1mStep[0m  [40/42], [94mLoss[0m : 3.57031

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.844, [92mTest[0m: 3.898, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.70475
[1mStep[0m  [4/42], [94mLoss[0m : 3.81304
[1mStep[0m  [8/42], [94mLoss[0m : 3.92990
[1mStep[0m  [12/42], [94mLoss[0m : 3.89342
[1mStep[0m  [16/42], [94mLoss[0m : 3.37381
[1mStep[0m  [20/42], [94mLoss[0m : 3.60207
[1mStep[0m  [24/42], [94mLoss[0m : 3.79110
[1mStep[0m  [28/42], [94mLoss[0m : 3.75155
[1mStep[0m  [32/42], [94mLoss[0m : 3.41787
[1mStep[0m  [36/42], [94mLoss[0m : 3.93102
[1mStep[0m  [40/42], [94mLoss[0m : 3.85108

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.728, [92mTest[0m: 3.790, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.48008
[1mStep[0m  [4/42], [94mLoss[0m : 3.82308
[1mStep[0m  [8/42], [94mLoss[0m : 3.53727
[1mStep[0m  [12/42], [94mLoss[0m : 3.74937
[1mStep[0m  [16/42], [94mLoss[0m : 3.28688
[1mStep[0m  [20/42], [94mLoss[0m : 3.71312
[1mStep[0m  [24/42], [94mLoss[0m : 3.58973
[1mStep[0m  [28/42], [94mLoss[0m : 3.73736
[1mStep[0m  [32/42], [94mLoss[0m : 3.50668
[1mStep[0m  [36/42], [94mLoss[0m : 3.59327
[1mStep[0m  [40/42], [94mLoss[0m : 3.96333

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.630, [92mTest[0m: 3.682, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.44037
[1mStep[0m  [4/42], [94mLoss[0m : 3.65699
[1mStep[0m  [8/42], [94mLoss[0m : 3.53708
[1mStep[0m  [12/42], [94mLoss[0m : 3.55539
[1mStep[0m  [16/42], [94mLoss[0m : 3.52045
[1mStep[0m  [20/42], [94mLoss[0m : 3.05506
[1mStep[0m  [24/42], [94mLoss[0m : 3.57527
[1mStep[0m  [28/42], [94mLoss[0m : 3.21116
[1mStep[0m  [32/42], [94mLoss[0m : 3.38308
[1mStep[0m  [36/42], [94mLoss[0m : 3.55667
[1mStep[0m  [40/42], [94mLoss[0m : 3.43263

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.545, [92mTest[0m: 3.574, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.498
====================================

Phase 1 - Evaluation MAE:  3.497568794659206
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 3.48918
[1mStep[0m  [4/42], [94mLoss[0m : 3.43685
[1mStep[0m  [8/42], [94mLoss[0m : 3.41376
[1mStep[0m  [12/42], [94mLoss[0m : 3.39677
[1mStep[0m  [16/42], [94mLoss[0m : 3.35629
[1mStep[0m  [20/42], [94mLoss[0m : 3.47465
[1mStep[0m  [24/42], [94mLoss[0m : 3.73439
[1mStep[0m  [28/42], [94mLoss[0m : 3.49515
[1mStep[0m  [32/42], [94mLoss[0m : 3.30729
[1mStep[0m  [36/42], [94mLoss[0m : 3.44445
[1mStep[0m  [40/42], [94mLoss[0m : 3.60345

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.450, [92mTest[0m: 3.499, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.47113
[1mStep[0m  [4/42], [94mLoss[0m : 3.35434
[1mStep[0m  [8/42], [94mLoss[0m : 3.47425
[1mStep[0m  [12/42], [94mLoss[0m : 3.58376
[1mStep[0m  [16/42], [94mLoss[0m : 3.59018
[1mStep[0m  [20/42], [94mLoss[0m : 3.42161
[1mStep[0m  [24/42], [94mLoss[0m : 3.28689
[1mStep[0m  [28/42], [94mLoss[0m : 3.53871
[1mStep[0m  [32/42], [94mLoss[0m : 3.39580
[1mStep[0m  [36/42], [94mLoss[0m : 3.31390
[1mStep[0m  [40/42], [94mLoss[0m : 3.42124

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.372, [92mTest[0m: 3.402, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.38877
[1mStep[0m  [4/42], [94mLoss[0m : 3.37836
[1mStep[0m  [8/42], [94mLoss[0m : 3.32166
[1mStep[0m  [12/42], [94mLoss[0m : 3.14262
[1mStep[0m  [16/42], [94mLoss[0m : 2.97143
[1mStep[0m  [20/42], [94mLoss[0m : 2.82128
[1mStep[0m  [24/42], [94mLoss[0m : 3.20039
[1mStep[0m  [28/42], [94mLoss[0m : 3.38902
[1mStep[0m  [32/42], [94mLoss[0m : 3.19506
[1mStep[0m  [36/42], [94mLoss[0m : 3.63502
[1mStep[0m  [40/42], [94mLoss[0m : 3.34761

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.314, [92mTest[0m: 3.316, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.01425
[1mStep[0m  [4/42], [94mLoss[0m : 3.39950
[1mStep[0m  [8/42], [94mLoss[0m : 3.55423
[1mStep[0m  [12/42], [94mLoss[0m : 3.16493
[1mStep[0m  [16/42], [94mLoss[0m : 3.40648
[1mStep[0m  [20/42], [94mLoss[0m : 3.12916
[1mStep[0m  [24/42], [94mLoss[0m : 3.20067
[1mStep[0m  [28/42], [94mLoss[0m : 3.23519
[1mStep[0m  [32/42], [94mLoss[0m : 3.00018
[1mStep[0m  [36/42], [94mLoss[0m : 3.08855
[1mStep[0m  [40/42], [94mLoss[0m : 3.39098

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.224, [92mTest[0m: 3.256, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07502
[1mStep[0m  [4/42], [94mLoss[0m : 3.08859
[1mStep[0m  [8/42], [94mLoss[0m : 3.35590
[1mStep[0m  [12/42], [94mLoss[0m : 3.51158
[1mStep[0m  [16/42], [94mLoss[0m : 3.47881
[1mStep[0m  [20/42], [94mLoss[0m : 3.28655
[1mStep[0m  [24/42], [94mLoss[0m : 2.90773
[1mStep[0m  [28/42], [94mLoss[0m : 2.97025
[1mStep[0m  [32/42], [94mLoss[0m : 3.05980
[1mStep[0m  [36/42], [94mLoss[0m : 3.69281
[1mStep[0m  [40/42], [94mLoss[0m : 3.14034

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.184, [92mTest[0m: 3.193, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.17201
[1mStep[0m  [4/42], [94mLoss[0m : 3.19658
[1mStep[0m  [8/42], [94mLoss[0m : 3.01064
[1mStep[0m  [12/42], [94mLoss[0m : 3.15429
[1mStep[0m  [16/42], [94mLoss[0m : 3.07405
[1mStep[0m  [20/42], [94mLoss[0m : 3.14791
[1mStep[0m  [24/42], [94mLoss[0m : 2.85119
[1mStep[0m  [28/42], [94mLoss[0m : 3.20126
[1mStep[0m  [32/42], [94mLoss[0m : 2.83720
[1mStep[0m  [36/42], [94mLoss[0m : 3.23479
[1mStep[0m  [40/42], [94mLoss[0m : 3.27914

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.128, [92mTest[0m: 3.150, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.35848
[1mStep[0m  [4/42], [94mLoss[0m : 3.15013
[1mStep[0m  [8/42], [94mLoss[0m : 2.93781
[1mStep[0m  [12/42], [94mLoss[0m : 3.27748
[1mStep[0m  [16/42], [94mLoss[0m : 3.08434
[1mStep[0m  [20/42], [94mLoss[0m : 2.91525
[1mStep[0m  [24/42], [94mLoss[0m : 3.09906
[1mStep[0m  [28/42], [94mLoss[0m : 2.86292
[1mStep[0m  [32/42], [94mLoss[0m : 3.07082
[1mStep[0m  [36/42], [94mLoss[0m : 3.30253
[1mStep[0m  [40/42], [94mLoss[0m : 3.09660

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.085, [92mTest[0m: 3.093, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.00997
[1mStep[0m  [4/42], [94mLoss[0m : 2.88908
[1mStep[0m  [8/42], [94mLoss[0m : 3.12373
[1mStep[0m  [12/42], [94mLoss[0m : 2.97712
[1mStep[0m  [16/42], [94mLoss[0m : 2.98648
[1mStep[0m  [20/42], [94mLoss[0m : 3.14685
[1mStep[0m  [24/42], [94mLoss[0m : 2.89811
[1mStep[0m  [28/42], [94mLoss[0m : 3.16253
[1mStep[0m  [32/42], [94mLoss[0m : 3.00770
[1mStep[0m  [36/42], [94mLoss[0m : 2.76159
[1mStep[0m  [40/42], [94mLoss[0m : 3.27856

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.045, [92mTest[0m: 3.037, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.24423
[1mStep[0m  [4/42], [94mLoss[0m : 3.24287
[1mStep[0m  [8/42], [94mLoss[0m : 3.02985
[1mStep[0m  [12/42], [94mLoss[0m : 3.05534
[1mStep[0m  [16/42], [94mLoss[0m : 3.21431
[1mStep[0m  [20/42], [94mLoss[0m : 3.07699
[1mStep[0m  [24/42], [94mLoss[0m : 3.06620
[1mStep[0m  [28/42], [94mLoss[0m : 2.67899
[1mStep[0m  [32/42], [94mLoss[0m : 3.35974
[1mStep[0m  [36/42], [94mLoss[0m : 2.84286
[1mStep[0m  [40/42], [94mLoss[0m : 2.90259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.004, [92mTest[0m: 3.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.08128
[1mStep[0m  [4/42], [94mLoss[0m : 2.86831
[1mStep[0m  [8/42], [94mLoss[0m : 2.87687
[1mStep[0m  [12/42], [94mLoss[0m : 3.15349
[1mStep[0m  [16/42], [94mLoss[0m : 2.95935
[1mStep[0m  [20/42], [94mLoss[0m : 2.91341
[1mStep[0m  [24/42], [94mLoss[0m : 2.81300
[1mStep[0m  [28/42], [94mLoss[0m : 2.89678
[1mStep[0m  [32/42], [94mLoss[0m : 2.83185
[1mStep[0m  [36/42], [94mLoss[0m : 3.22445
[1mStep[0m  [40/42], [94mLoss[0m : 3.04314

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.973, [92mTest[0m: 2.973, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87064
[1mStep[0m  [4/42], [94mLoss[0m : 2.65214
[1mStep[0m  [8/42], [94mLoss[0m : 3.08453
[1mStep[0m  [12/42], [94mLoss[0m : 2.98388
[1mStep[0m  [16/42], [94mLoss[0m : 3.12170
[1mStep[0m  [20/42], [94mLoss[0m : 3.04079
[1mStep[0m  [24/42], [94mLoss[0m : 3.19451
[1mStep[0m  [28/42], [94mLoss[0m : 2.82128
[1mStep[0m  [32/42], [94mLoss[0m : 3.03825
[1mStep[0m  [36/42], [94mLoss[0m : 2.86446
[1mStep[0m  [40/42], [94mLoss[0m : 2.99916

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.948, [92mTest[0m: 2.933, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04608
[1mStep[0m  [4/42], [94mLoss[0m : 3.05786
[1mStep[0m  [8/42], [94mLoss[0m : 2.99091
[1mStep[0m  [12/42], [94mLoss[0m : 3.09486
[1mStep[0m  [16/42], [94mLoss[0m : 2.73540
[1mStep[0m  [20/42], [94mLoss[0m : 2.97659
[1mStep[0m  [24/42], [94mLoss[0m : 2.87250
[1mStep[0m  [28/42], [94mLoss[0m : 2.78102
[1mStep[0m  [32/42], [94mLoss[0m : 3.04398
[1mStep[0m  [36/42], [94mLoss[0m : 2.94261
[1mStep[0m  [40/42], [94mLoss[0m : 2.90970

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.910, [92mTest[0m: 2.903, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74647
[1mStep[0m  [4/42], [94mLoss[0m : 2.66328
[1mStep[0m  [8/42], [94mLoss[0m : 2.93542
[1mStep[0m  [12/42], [94mLoss[0m : 2.63938
[1mStep[0m  [16/42], [94mLoss[0m : 2.95222
[1mStep[0m  [20/42], [94mLoss[0m : 3.23594
[1mStep[0m  [24/42], [94mLoss[0m : 2.84161
[1mStep[0m  [28/42], [94mLoss[0m : 2.94950
[1mStep[0m  [32/42], [94mLoss[0m : 3.01806
[1mStep[0m  [36/42], [94mLoss[0m : 2.81974
[1mStep[0m  [40/42], [94mLoss[0m : 2.69013

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.887, [92mTest[0m: 2.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.12892
[1mStep[0m  [4/42], [94mLoss[0m : 2.66375
[1mStep[0m  [8/42], [94mLoss[0m : 2.75747
[1mStep[0m  [12/42], [94mLoss[0m : 2.71882
[1mStep[0m  [16/42], [94mLoss[0m : 2.73247
[1mStep[0m  [20/42], [94mLoss[0m : 2.91887
[1mStep[0m  [24/42], [94mLoss[0m : 2.84206
[1mStep[0m  [28/42], [94mLoss[0m : 2.73652
[1mStep[0m  [32/42], [94mLoss[0m : 2.89136
[1mStep[0m  [36/42], [94mLoss[0m : 2.64524
[1mStep[0m  [40/42], [94mLoss[0m : 3.12301

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.858, [92mTest[0m: 2.843, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.95252
[1mStep[0m  [4/42], [94mLoss[0m : 2.67511
[1mStep[0m  [8/42], [94mLoss[0m : 2.79775
[1mStep[0m  [12/42], [94mLoss[0m : 2.86044
[1mStep[0m  [16/42], [94mLoss[0m : 2.62232
[1mStep[0m  [20/42], [94mLoss[0m : 2.87220
[1mStep[0m  [24/42], [94mLoss[0m : 2.82227
[1mStep[0m  [28/42], [94mLoss[0m : 3.09517
[1mStep[0m  [32/42], [94mLoss[0m : 2.67844
[1mStep[0m  [36/42], [94mLoss[0m : 2.66863
[1mStep[0m  [40/42], [94mLoss[0m : 2.64577

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.830, [92mTest[0m: 2.823, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.99054
[1mStep[0m  [4/42], [94mLoss[0m : 2.68054
[1mStep[0m  [8/42], [94mLoss[0m : 2.97807
[1mStep[0m  [12/42], [94mLoss[0m : 2.76302
[1mStep[0m  [16/42], [94mLoss[0m : 2.78822
[1mStep[0m  [20/42], [94mLoss[0m : 2.98876
[1mStep[0m  [24/42], [94mLoss[0m : 2.77611
[1mStep[0m  [28/42], [94mLoss[0m : 2.85268
[1mStep[0m  [32/42], [94mLoss[0m : 2.86608
[1mStep[0m  [36/42], [94mLoss[0m : 2.71417
[1mStep[0m  [40/42], [94mLoss[0m : 2.77599

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84041
[1mStep[0m  [4/42], [94mLoss[0m : 2.75355
[1mStep[0m  [8/42], [94mLoss[0m : 2.49378
[1mStep[0m  [12/42], [94mLoss[0m : 2.92452
[1mStep[0m  [16/42], [94mLoss[0m : 2.72465
[1mStep[0m  [20/42], [94mLoss[0m : 2.93611
[1mStep[0m  [24/42], [94mLoss[0m : 2.77305
[1mStep[0m  [28/42], [94mLoss[0m : 2.70978
[1mStep[0m  [32/42], [94mLoss[0m : 2.91905
[1mStep[0m  [36/42], [94mLoss[0m : 2.89776
[1mStep[0m  [40/42], [94mLoss[0m : 2.74834

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49178
[1mStep[0m  [4/42], [94mLoss[0m : 2.87503
[1mStep[0m  [8/42], [94mLoss[0m : 2.95274
[1mStep[0m  [12/42], [94mLoss[0m : 2.73012
[1mStep[0m  [16/42], [94mLoss[0m : 2.82442
[1mStep[0m  [20/42], [94mLoss[0m : 2.74065
[1mStep[0m  [24/42], [94mLoss[0m : 2.81323
[1mStep[0m  [28/42], [94mLoss[0m : 2.72493
[1mStep[0m  [32/42], [94mLoss[0m : 2.89297
[1mStep[0m  [36/42], [94mLoss[0m : 2.84849
[1mStep[0m  [40/42], [94mLoss[0m : 2.98665

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.778, [92mTest[0m: 2.752, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.98703
[1mStep[0m  [4/42], [94mLoss[0m : 2.69712
[1mStep[0m  [8/42], [94mLoss[0m : 2.63388
[1mStep[0m  [12/42], [94mLoss[0m : 2.73492
[1mStep[0m  [16/42], [94mLoss[0m : 2.65152
[1mStep[0m  [20/42], [94mLoss[0m : 2.82902
[1mStep[0m  [24/42], [94mLoss[0m : 2.70474
[1mStep[0m  [28/42], [94mLoss[0m : 2.66093
[1mStep[0m  [32/42], [94mLoss[0m : 2.91104
[1mStep[0m  [36/42], [94mLoss[0m : 2.69562
[1mStep[0m  [40/42], [94mLoss[0m : 2.85141

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92558
[1mStep[0m  [4/42], [94mLoss[0m : 2.75165
[1mStep[0m  [8/42], [94mLoss[0m : 2.33951
[1mStep[0m  [12/42], [94mLoss[0m : 2.60185
[1mStep[0m  [16/42], [94mLoss[0m : 2.69183
[1mStep[0m  [20/42], [94mLoss[0m : 2.45058
[1mStep[0m  [24/42], [94mLoss[0m : 2.79507
[1mStep[0m  [28/42], [94mLoss[0m : 2.78654
[1mStep[0m  [32/42], [94mLoss[0m : 3.06366
[1mStep[0m  [36/42], [94mLoss[0m : 2.76875
[1mStep[0m  [40/42], [94mLoss[0m : 2.80063

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.715, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61773
[1mStep[0m  [4/42], [94mLoss[0m : 2.76971
[1mStep[0m  [8/42], [94mLoss[0m : 2.73635
[1mStep[0m  [12/42], [94mLoss[0m : 2.70982
[1mStep[0m  [16/42], [94mLoss[0m : 2.75463
[1mStep[0m  [20/42], [94mLoss[0m : 2.68292
[1mStep[0m  [24/42], [94mLoss[0m : 2.86222
[1mStep[0m  [28/42], [94mLoss[0m : 2.94121
[1mStep[0m  [32/42], [94mLoss[0m : 2.68778
[1mStep[0m  [36/42], [94mLoss[0m : 2.78712
[1mStep[0m  [40/42], [94mLoss[0m : 2.62779

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46205
[1mStep[0m  [4/42], [94mLoss[0m : 2.83824
[1mStep[0m  [8/42], [94mLoss[0m : 2.59314
[1mStep[0m  [12/42], [94mLoss[0m : 2.84218
[1mStep[0m  [16/42], [94mLoss[0m : 2.81604
[1mStep[0m  [20/42], [94mLoss[0m : 2.64212
[1mStep[0m  [24/42], [94mLoss[0m : 2.76400
[1mStep[0m  [28/42], [94mLoss[0m : 2.64846
[1mStep[0m  [32/42], [94mLoss[0m : 2.70524
[1mStep[0m  [36/42], [94mLoss[0m : 2.65058
[1mStep[0m  [40/42], [94mLoss[0m : 2.96593

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.721, [92mTest[0m: 2.683, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82349
[1mStep[0m  [4/42], [94mLoss[0m : 2.68899
[1mStep[0m  [8/42], [94mLoss[0m : 2.63896
[1mStep[0m  [12/42], [94mLoss[0m : 2.67697
[1mStep[0m  [16/42], [94mLoss[0m : 2.90257
[1mStep[0m  [20/42], [94mLoss[0m : 2.58535
[1mStep[0m  [24/42], [94mLoss[0m : 3.02169
[1mStep[0m  [28/42], [94mLoss[0m : 2.66567
[1mStep[0m  [32/42], [94mLoss[0m : 2.61732
[1mStep[0m  [36/42], [94mLoss[0m : 2.69883
[1mStep[0m  [40/42], [94mLoss[0m : 2.91118

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48598
[1mStep[0m  [4/42], [94mLoss[0m : 2.40697
[1mStep[0m  [8/42], [94mLoss[0m : 2.65474
[1mStep[0m  [12/42], [94mLoss[0m : 2.80526
[1mStep[0m  [16/42], [94mLoss[0m : 2.88204
[1mStep[0m  [20/42], [94mLoss[0m : 2.64439
[1mStep[0m  [24/42], [94mLoss[0m : 2.61166
[1mStep[0m  [28/42], [94mLoss[0m : 2.74171
[1mStep[0m  [32/42], [94mLoss[0m : 2.62161
[1mStep[0m  [36/42], [94mLoss[0m : 2.84492
[1mStep[0m  [40/42], [94mLoss[0m : 2.46001

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.651, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56267
[1mStep[0m  [4/42], [94mLoss[0m : 2.79422
[1mStep[0m  [8/42], [94mLoss[0m : 2.72633
[1mStep[0m  [12/42], [94mLoss[0m : 2.70143
[1mStep[0m  [16/42], [94mLoss[0m : 2.36029
[1mStep[0m  [20/42], [94mLoss[0m : 2.63900
[1mStep[0m  [24/42], [94mLoss[0m : 2.67497
[1mStep[0m  [28/42], [94mLoss[0m : 2.51013
[1mStep[0m  [32/42], [94mLoss[0m : 2.73974
[1mStep[0m  [36/42], [94mLoss[0m : 2.73808
[1mStep[0m  [40/42], [94mLoss[0m : 2.42825

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.636, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73358
[1mStep[0m  [4/42], [94mLoss[0m : 2.61217
[1mStep[0m  [8/42], [94mLoss[0m : 2.84010
[1mStep[0m  [12/42], [94mLoss[0m : 2.68657
[1mStep[0m  [16/42], [94mLoss[0m : 2.73795
[1mStep[0m  [20/42], [94mLoss[0m : 2.39432
[1mStep[0m  [24/42], [94mLoss[0m : 2.63554
[1mStep[0m  [28/42], [94mLoss[0m : 2.65992
[1mStep[0m  [32/42], [94mLoss[0m : 2.66181
[1mStep[0m  [36/42], [94mLoss[0m : 2.67530
[1mStep[0m  [40/42], [94mLoss[0m : 2.74947

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.629, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73722
[1mStep[0m  [4/42], [94mLoss[0m : 2.63813
[1mStep[0m  [8/42], [94mLoss[0m : 2.52830
[1mStep[0m  [12/42], [94mLoss[0m : 2.69876
[1mStep[0m  [16/42], [94mLoss[0m : 2.78875
[1mStep[0m  [20/42], [94mLoss[0m : 2.76042
[1mStep[0m  [24/42], [94mLoss[0m : 2.47186
[1mStep[0m  [28/42], [94mLoss[0m : 2.57510
[1mStep[0m  [32/42], [94mLoss[0m : 2.63050
[1mStep[0m  [36/42], [94mLoss[0m : 2.67672
[1mStep[0m  [40/42], [94mLoss[0m : 2.77422

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.618, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62425
[1mStep[0m  [4/42], [94mLoss[0m : 2.66900
[1mStep[0m  [8/42], [94mLoss[0m : 2.66236
[1mStep[0m  [12/42], [94mLoss[0m : 2.37947
[1mStep[0m  [16/42], [94mLoss[0m : 2.53240
[1mStep[0m  [20/42], [94mLoss[0m : 2.64590
[1mStep[0m  [24/42], [94mLoss[0m : 2.78770
[1mStep[0m  [28/42], [94mLoss[0m : 2.48023
[1mStep[0m  [32/42], [94mLoss[0m : 2.53296
[1mStep[0m  [36/42], [94mLoss[0m : 2.76551
[1mStep[0m  [40/42], [94mLoss[0m : 2.73276

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.607, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51186
[1mStep[0m  [4/42], [94mLoss[0m : 2.82337
[1mStep[0m  [8/42], [94mLoss[0m : 2.73361
[1mStep[0m  [12/42], [94mLoss[0m : 2.77171
[1mStep[0m  [16/42], [94mLoss[0m : 2.53045
[1mStep[0m  [20/42], [94mLoss[0m : 2.60112
[1mStep[0m  [24/42], [94mLoss[0m : 2.36795
[1mStep[0m  [28/42], [94mLoss[0m : 2.70294
[1mStep[0m  [32/42], [94mLoss[0m : 2.79136
[1mStep[0m  [36/42], [94mLoss[0m : 2.75943
[1mStep[0m  [40/42], [94mLoss[0m : 2.58448

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.609, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58866
[1mStep[0m  [4/42], [94mLoss[0m : 2.66848
[1mStep[0m  [8/42], [94mLoss[0m : 2.80578
[1mStep[0m  [12/42], [94mLoss[0m : 2.50085
[1mStep[0m  [16/42], [94mLoss[0m : 2.57796
[1mStep[0m  [20/42], [94mLoss[0m : 2.28289
[1mStep[0m  [24/42], [94mLoss[0m : 2.74767
[1mStep[0m  [28/42], [94mLoss[0m : 2.93674
[1mStep[0m  [32/42], [94mLoss[0m : 2.62704
[1mStep[0m  [36/42], [94mLoss[0m : 2.79020
[1mStep[0m  [40/42], [94mLoss[0m : 2.59643

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.598, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.586
====================================

Phase 2 - Evaluation MAE:  2.585661734853472
MAE score P1      3.497569
MAE score P2      2.585662
loss              2.637636
learning_rate       0.0001
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.59517
[1mStep[0m  [2/21], [94mLoss[0m : 10.81749
[1mStep[0m  [4/21], [94mLoss[0m : 10.21183
[1mStep[0m  [6/21], [94mLoss[0m : 10.69314
[1mStep[0m  [8/21], [94mLoss[0m : 10.70140
[1mStep[0m  [10/21], [94mLoss[0m : 10.97685
[1mStep[0m  [12/21], [94mLoss[0m : 10.32085
[1mStep[0m  [14/21], [94mLoss[0m : 10.62584
[1mStep[0m  [16/21], [94mLoss[0m : 10.60445
[1mStep[0m  [18/21], [94mLoss[0m : 10.37925
[1mStep[0m  [20/21], [94mLoss[0m : 10.28332

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.574, [92mTest[0m: 10.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55212
[1mStep[0m  [2/21], [94mLoss[0m : 10.53790
[1mStep[0m  [4/21], [94mLoss[0m : 10.39313
[1mStep[0m  [6/21], [94mLoss[0m : 10.66472
[1mStep[0m  [8/21], [94mLoss[0m : 10.60775
[1mStep[0m  [10/21], [94mLoss[0m : 10.56701
[1mStep[0m  [12/21], [94mLoss[0m : 10.36657
[1mStep[0m  [14/21], [94mLoss[0m : 10.40085
[1mStep[0m  [16/21], [94mLoss[0m : 10.18409
[1mStep[0m  [18/21], [94mLoss[0m : 10.48361
[1mStep[0m  [20/21], [94mLoss[0m : 10.25605

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.462, [92mTest[0m: 10.529, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.15890
[1mStep[0m  [2/21], [94mLoss[0m : 10.18399
[1mStep[0m  [4/21], [94mLoss[0m : 10.31382
[1mStep[0m  [6/21], [94mLoss[0m : 10.45946
[1mStep[0m  [8/21], [94mLoss[0m : 10.27827
[1mStep[0m  [10/21], [94mLoss[0m : 10.37403
[1mStep[0m  [12/21], [94mLoss[0m : 10.48241
[1mStep[0m  [14/21], [94mLoss[0m : 10.50295
[1mStep[0m  [16/21], [94mLoss[0m : 10.33714
[1mStep[0m  [18/21], [94mLoss[0m : 10.63410
[1mStep[0m  [20/21], [94mLoss[0m : 10.45741

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.363, [92mTest[0m: 10.396, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.27635
[1mStep[0m  [2/21], [94mLoss[0m : 10.36749
[1mStep[0m  [4/21], [94mLoss[0m : 10.36314
[1mStep[0m  [6/21], [94mLoss[0m : 10.30316
[1mStep[0m  [8/21], [94mLoss[0m : 10.55861
[1mStep[0m  [10/21], [94mLoss[0m : 10.01443
[1mStep[0m  [12/21], [94mLoss[0m : 10.16006
[1mStep[0m  [14/21], [94mLoss[0m : 10.00572
[1mStep[0m  [16/21], [94mLoss[0m : 10.30932
[1mStep[0m  [18/21], [94mLoss[0m : 10.10394
[1mStep[0m  [20/21], [94mLoss[0m : 10.17670

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.241, [92mTest[0m: 10.299, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.29034
[1mStep[0m  [2/21], [94mLoss[0m : 10.53065
[1mStep[0m  [4/21], [94mLoss[0m : 10.08359
[1mStep[0m  [6/21], [94mLoss[0m : 10.14995
[1mStep[0m  [8/21], [94mLoss[0m : 9.98161
[1mStep[0m  [10/21], [94mLoss[0m : 10.18581
[1mStep[0m  [12/21], [94mLoss[0m : 10.03810
[1mStep[0m  [14/21], [94mLoss[0m : 10.18983
[1mStep[0m  [16/21], [94mLoss[0m : 10.09130
[1mStep[0m  [18/21], [94mLoss[0m : 10.04177
[1mStep[0m  [20/21], [94mLoss[0m : 9.78260

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.138, [92mTest[0m: 10.189, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.04770
[1mStep[0m  [2/21], [94mLoss[0m : 10.02132
[1mStep[0m  [4/21], [94mLoss[0m : 10.26194
[1mStep[0m  [6/21], [94mLoss[0m : 10.03153
[1mStep[0m  [8/21], [94mLoss[0m : 9.97297
[1mStep[0m  [10/21], [94mLoss[0m : 10.27154
[1mStep[0m  [12/21], [94mLoss[0m : 9.87782
[1mStep[0m  [14/21], [94mLoss[0m : 9.94950
[1mStep[0m  [16/21], [94mLoss[0m : 9.77099
[1mStep[0m  [18/21], [94mLoss[0m : 9.88366
[1mStep[0m  [20/21], [94mLoss[0m : 9.71246

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.027, [92mTest[0m: 10.071, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.72030
[1mStep[0m  [2/21], [94mLoss[0m : 9.89918
[1mStep[0m  [4/21], [94mLoss[0m : 10.14261
[1mStep[0m  [6/21], [94mLoss[0m : 10.19066
[1mStep[0m  [8/21], [94mLoss[0m : 9.90248
[1mStep[0m  [10/21], [94mLoss[0m : 9.75079
[1mStep[0m  [12/21], [94mLoss[0m : 10.07297
[1mStep[0m  [14/21], [94mLoss[0m : 10.00855
[1mStep[0m  [16/21], [94mLoss[0m : 9.95461
[1mStep[0m  [18/21], [94mLoss[0m : 9.82810
[1mStep[0m  [20/21], [94mLoss[0m : 10.18496

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.918, [92mTest[0m: 9.961, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.80772
[1mStep[0m  [2/21], [94mLoss[0m : 9.74502
[1mStep[0m  [4/21], [94mLoss[0m : 9.95270
[1mStep[0m  [6/21], [94mLoss[0m : 9.69278
[1mStep[0m  [8/21], [94mLoss[0m : 9.59233
[1mStep[0m  [10/21], [94mLoss[0m : 10.04279
[1mStep[0m  [12/21], [94mLoss[0m : 10.33031
[1mStep[0m  [14/21], [94mLoss[0m : 9.88356
[1mStep[0m  [16/21], [94mLoss[0m : 9.70615
[1mStep[0m  [18/21], [94mLoss[0m : 9.70111
[1mStep[0m  [20/21], [94mLoss[0m : 9.72566

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.806, [92mTest[0m: 9.861, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.89908
[1mStep[0m  [2/21], [94mLoss[0m : 9.90069
[1mStep[0m  [4/21], [94mLoss[0m : 9.53547
[1mStep[0m  [6/21], [94mLoss[0m : 9.66179
[1mStep[0m  [8/21], [94mLoss[0m : 9.69512
[1mStep[0m  [10/21], [94mLoss[0m : 9.77784
[1mStep[0m  [12/21], [94mLoss[0m : 9.63654
[1mStep[0m  [14/21], [94mLoss[0m : 9.79882
[1mStep[0m  [16/21], [94mLoss[0m : 9.64718
[1mStep[0m  [18/21], [94mLoss[0m : 9.75534
[1mStep[0m  [20/21], [94mLoss[0m : 9.69012

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.703, [92mTest[0m: 9.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70512
[1mStep[0m  [2/21], [94mLoss[0m : 9.85680
[1mStep[0m  [4/21], [94mLoss[0m : 9.67928
[1mStep[0m  [6/21], [94mLoss[0m : 9.49671
[1mStep[0m  [8/21], [94mLoss[0m : 9.77776
[1mStep[0m  [10/21], [94mLoss[0m : 9.64048
[1mStep[0m  [12/21], [94mLoss[0m : 9.50263
[1mStep[0m  [14/21], [94mLoss[0m : 9.63882
[1mStep[0m  [16/21], [94mLoss[0m : 9.49636
[1mStep[0m  [18/21], [94mLoss[0m : 9.74104
[1mStep[0m  [20/21], [94mLoss[0m : 9.39949

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.594, [92mTest[0m: 9.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.52160
[1mStep[0m  [2/21], [94mLoss[0m : 9.39721
[1mStep[0m  [4/21], [94mLoss[0m : 9.68992
[1mStep[0m  [6/21], [94mLoss[0m : 9.43982
[1mStep[0m  [8/21], [94mLoss[0m : 9.55187
[1mStep[0m  [10/21], [94mLoss[0m : 9.75310
[1mStep[0m  [12/21], [94mLoss[0m : 9.40173
[1mStep[0m  [14/21], [94mLoss[0m : 9.54190
[1mStep[0m  [16/21], [94mLoss[0m : 9.38101
[1mStep[0m  [18/21], [94mLoss[0m : 9.46720
[1mStep[0m  [20/21], [94mLoss[0m : 9.56962

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.477, [92mTest[0m: 9.530, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.26412
[1mStep[0m  [2/21], [94mLoss[0m : 9.11392
[1mStep[0m  [4/21], [94mLoss[0m : 9.41847
[1mStep[0m  [6/21], [94mLoss[0m : 9.31742
[1mStep[0m  [8/21], [94mLoss[0m : 9.28730
[1mStep[0m  [10/21], [94mLoss[0m : 9.42338
[1mStep[0m  [12/21], [94mLoss[0m : 9.62256
[1mStep[0m  [14/21], [94mLoss[0m : 9.47715
[1mStep[0m  [16/21], [94mLoss[0m : 9.35921
[1mStep[0m  [18/21], [94mLoss[0m : 9.23950
[1mStep[0m  [20/21], [94mLoss[0m : 9.20765

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.367, [92mTest[0m: 9.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.28609
[1mStep[0m  [2/21], [94mLoss[0m : 9.34480
[1mStep[0m  [4/21], [94mLoss[0m : 9.43804
[1mStep[0m  [6/21], [94mLoss[0m : 9.22324
[1mStep[0m  [8/21], [94mLoss[0m : 9.34286
[1mStep[0m  [10/21], [94mLoss[0m : 9.07831
[1mStep[0m  [12/21], [94mLoss[0m : 9.21287
[1mStep[0m  [14/21], [94mLoss[0m : 9.40150
[1mStep[0m  [16/21], [94mLoss[0m : 9.35341
[1mStep[0m  [18/21], [94mLoss[0m : 9.25953
[1mStep[0m  [20/21], [94mLoss[0m : 9.24362

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.265, [92mTest[0m: 9.320, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.36215
[1mStep[0m  [2/21], [94mLoss[0m : 9.14305
[1mStep[0m  [4/21], [94mLoss[0m : 9.04751
[1mStep[0m  [6/21], [94mLoss[0m : 9.07348
[1mStep[0m  [8/21], [94mLoss[0m : 9.18122
[1mStep[0m  [10/21], [94mLoss[0m : 9.16174
[1mStep[0m  [12/21], [94mLoss[0m : 9.33638
[1mStep[0m  [14/21], [94mLoss[0m : 9.22309
[1mStep[0m  [16/21], [94mLoss[0m : 9.14710
[1mStep[0m  [18/21], [94mLoss[0m : 9.35389
[1mStep[0m  [20/21], [94mLoss[0m : 8.95235

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.149, [92mTest[0m: 9.190, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.89428
[1mStep[0m  [2/21], [94mLoss[0m : 9.04347
[1mStep[0m  [4/21], [94mLoss[0m : 9.21415
[1mStep[0m  [6/21], [94mLoss[0m : 9.00737
[1mStep[0m  [8/21], [94mLoss[0m : 9.09836
[1mStep[0m  [10/21], [94mLoss[0m : 9.20951
[1mStep[0m  [12/21], [94mLoss[0m : 9.18863
[1mStep[0m  [14/21], [94mLoss[0m : 9.01037
[1mStep[0m  [16/21], [94mLoss[0m : 8.73311
[1mStep[0m  [18/21], [94mLoss[0m : 8.84968
[1mStep[0m  [20/21], [94mLoss[0m : 8.96787

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.040, [92mTest[0m: 9.093, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.16770
[1mStep[0m  [2/21], [94mLoss[0m : 8.75091
[1mStep[0m  [4/21], [94mLoss[0m : 9.40902
[1mStep[0m  [6/21], [94mLoss[0m : 8.81372
[1mStep[0m  [8/21], [94mLoss[0m : 8.94967
[1mStep[0m  [10/21], [94mLoss[0m : 9.07086
[1mStep[0m  [12/21], [94mLoss[0m : 9.12566
[1mStep[0m  [14/21], [94mLoss[0m : 9.04427
[1mStep[0m  [16/21], [94mLoss[0m : 8.90591
[1mStep[0m  [18/21], [94mLoss[0m : 8.78452
[1mStep[0m  [20/21], [94mLoss[0m : 8.90136

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.940, [92mTest[0m: 8.970, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.73986
[1mStep[0m  [2/21], [94mLoss[0m : 8.84917
[1mStep[0m  [4/21], [94mLoss[0m : 8.61354
[1mStep[0m  [6/21], [94mLoss[0m : 9.00682
[1mStep[0m  [8/21], [94mLoss[0m : 8.73622
[1mStep[0m  [10/21], [94mLoss[0m : 8.79449
[1mStep[0m  [12/21], [94mLoss[0m : 9.09388
[1mStep[0m  [14/21], [94mLoss[0m : 8.56991
[1mStep[0m  [16/21], [94mLoss[0m : 8.82764
[1mStep[0m  [18/21], [94mLoss[0m : 8.96007
[1mStep[0m  [20/21], [94mLoss[0m : 8.96781

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.821, [92mTest[0m: 8.866, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.78788
[1mStep[0m  [2/21], [94mLoss[0m : 8.58756
[1mStep[0m  [4/21], [94mLoss[0m : 8.66462
[1mStep[0m  [6/21], [94mLoss[0m : 8.93790
[1mStep[0m  [8/21], [94mLoss[0m : 8.64855
[1mStep[0m  [10/21], [94mLoss[0m : 8.74321
[1mStep[0m  [12/21], [94mLoss[0m : 8.77495
[1mStep[0m  [14/21], [94mLoss[0m : 8.81898
[1mStep[0m  [16/21], [94mLoss[0m : 8.61113
[1mStep[0m  [18/21], [94mLoss[0m : 8.50108
[1mStep[0m  [20/21], [94mLoss[0m : 8.67391

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.718, [92mTest[0m: 8.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.90182
[1mStep[0m  [2/21], [94mLoss[0m : 8.74339
[1mStep[0m  [4/21], [94mLoss[0m : 8.48282
[1mStep[0m  [6/21], [94mLoss[0m : 8.33018
[1mStep[0m  [8/21], [94mLoss[0m : 8.71300
[1mStep[0m  [10/21], [94mLoss[0m : 8.77467
[1mStep[0m  [12/21], [94mLoss[0m : 8.53824
[1mStep[0m  [14/21], [94mLoss[0m : 8.60323
[1mStep[0m  [16/21], [94mLoss[0m : 8.64701
[1mStep[0m  [18/21], [94mLoss[0m : 8.62775
[1mStep[0m  [20/21], [94mLoss[0m : 8.61809

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.608, [92mTest[0m: 8.650, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.56516
[1mStep[0m  [2/21], [94mLoss[0m : 8.32115
[1mStep[0m  [4/21], [94mLoss[0m : 8.29231
[1mStep[0m  [6/21], [94mLoss[0m : 8.41233
[1mStep[0m  [8/21], [94mLoss[0m : 8.32767
[1mStep[0m  [10/21], [94mLoss[0m : 8.48980
[1mStep[0m  [12/21], [94mLoss[0m : 8.70352
[1mStep[0m  [14/21], [94mLoss[0m : 8.39299
[1mStep[0m  [16/21], [94mLoss[0m : 8.65026
[1mStep[0m  [18/21], [94mLoss[0m : 8.50672
[1mStep[0m  [20/21], [94mLoss[0m : 8.55920

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.491, [92mTest[0m: 8.540, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.43097
[1mStep[0m  [2/21], [94mLoss[0m : 8.33582
[1mStep[0m  [4/21], [94mLoss[0m : 8.24582
[1mStep[0m  [6/21], [94mLoss[0m : 8.33343
[1mStep[0m  [8/21], [94mLoss[0m : 8.46164
[1mStep[0m  [10/21], [94mLoss[0m : 8.30158
[1mStep[0m  [12/21], [94mLoss[0m : 8.52592
[1mStep[0m  [14/21], [94mLoss[0m : 8.58105
[1mStep[0m  [16/21], [94mLoss[0m : 8.26076
[1mStep[0m  [18/21], [94mLoss[0m : 8.49506
[1mStep[0m  [20/21], [94mLoss[0m : 8.42293

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.388, [92mTest[0m: 8.438, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.16957
[1mStep[0m  [2/21], [94mLoss[0m : 8.29682
[1mStep[0m  [4/21], [94mLoss[0m : 8.72950
[1mStep[0m  [6/21], [94mLoss[0m : 8.53557
[1mStep[0m  [8/21], [94mLoss[0m : 8.16030
[1mStep[0m  [10/21], [94mLoss[0m : 8.16965
[1mStep[0m  [12/21], [94mLoss[0m : 8.32252
[1mStep[0m  [14/21], [94mLoss[0m : 8.58211
[1mStep[0m  [16/21], [94mLoss[0m : 8.29491
[1mStep[0m  [18/21], [94mLoss[0m : 8.26995
[1mStep[0m  [20/21], [94mLoss[0m : 8.38754

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.291, [92mTest[0m: 8.345, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.23122
[1mStep[0m  [2/21], [94mLoss[0m : 8.47458
[1mStep[0m  [4/21], [94mLoss[0m : 8.05261
[1mStep[0m  [6/21], [94mLoss[0m : 8.18301
[1mStep[0m  [8/21], [94mLoss[0m : 8.11039
[1mStep[0m  [10/21], [94mLoss[0m : 8.18331
[1mStep[0m  [12/21], [94mLoss[0m : 8.25472
[1mStep[0m  [14/21], [94mLoss[0m : 8.28106
[1mStep[0m  [16/21], [94mLoss[0m : 8.24076
[1mStep[0m  [18/21], [94mLoss[0m : 8.09859
[1mStep[0m  [20/21], [94mLoss[0m : 8.26780

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.192, [92mTest[0m: 8.235, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.19942
[1mStep[0m  [2/21], [94mLoss[0m : 8.37649
[1mStep[0m  [4/21], [94mLoss[0m : 8.03365
[1mStep[0m  [6/21], [94mLoss[0m : 8.24516
[1mStep[0m  [8/21], [94mLoss[0m : 7.77407
[1mStep[0m  [10/21], [94mLoss[0m : 8.08226
[1mStep[0m  [12/21], [94mLoss[0m : 7.90904
[1mStep[0m  [14/21], [94mLoss[0m : 8.01636
[1mStep[0m  [16/21], [94mLoss[0m : 8.09379
[1mStep[0m  [18/21], [94mLoss[0m : 8.04097
[1mStep[0m  [20/21], [94mLoss[0m : 8.01158

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.094, [92mTest[0m: 8.139, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.04368
[1mStep[0m  [2/21], [94mLoss[0m : 8.14013
[1mStep[0m  [4/21], [94mLoss[0m : 8.00097
[1mStep[0m  [6/21], [94mLoss[0m : 8.24160
[1mStep[0m  [8/21], [94mLoss[0m : 8.01685
[1mStep[0m  [10/21], [94mLoss[0m : 7.83947
[1mStep[0m  [12/21], [94mLoss[0m : 7.68714
[1mStep[0m  [14/21], [94mLoss[0m : 8.04669
[1mStep[0m  [16/21], [94mLoss[0m : 7.92358
[1mStep[0m  [18/21], [94mLoss[0m : 7.75685
[1mStep[0m  [20/21], [94mLoss[0m : 8.24124

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.995, [92mTest[0m: 8.042, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.00410
[1mStep[0m  [2/21], [94mLoss[0m : 7.76638
[1mStep[0m  [4/21], [94mLoss[0m : 7.90449
[1mStep[0m  [6/21], [94mLoss[0m : 7.46562
[1mStep[0m  [8/21], [94mLoss[0m : 8.09278
[1mStep[0m  [10/21], [94mLoss[0m : 7.62150
[1mStep[0m  [12/21], [94mLoss[0m : 8.07261
[1mStep[0m  [14/21], [94mLoss[0m : 8.18056
[1mStep[0m  [16/21], [94mLoss[0m : 7.99669
[1mStep[0m  [18/21], [94mLoss[0m : 7.81162
[1mStep[0m  [20/21], [94mLoss[0m : 7.93278

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.897, [92mTest[0m: 7.942, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.08957
[1mStep[0m  [2/21], [94mLoss[0m : 7.71516
[1mStep[0m  [4/21], [94mLoss[0m : 8.00577
[1mStep[0m  [6/21], [94mLoss[0m : 7.88075
[1mStep[0m  [8/21], [94mLoss[0m : 7.89145
[1mStep[0m  [10/21], [94mLoss[0m : 7.87222
[1mStep[0m  [12/21], [94mLoss[0m : 7.87587
[1mStep[0m  [14/21], [94mLoss[0m : 7.91150
[1mStep[0m  [16/21], [94mLoss[0m : 7.93641
[1mStep[0m  [18/21], [94mLoss[0m : 7.71766
[1mStep[0m  [20/21], [94mLoss[0m : 7.58455

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.802, [92mTest[0m: 7.839, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.62020
[1mStep[0m  [2/21], [94mLoss[0m : 7.80856
[1mStep[0m  [4/21], [94mLoss[0m : 7.65274
[1mStep[0m  [6/21], [94mLoss[0m : 7.79055
[1mStep[0m  [8/21], [94mLoss[0m : 7.95778
[1mStep[0m  [10/21], [94mLoss[0m : 7.75509
[1mStep[0m  [12/21], [94mLoss[0m : 7.73539
[1mStep[0m  [14/21], [94mLoss[0m : 8.14225
[1mStep[0m  [16/21], [94mLoss[0m : 7.97018
[1mStep[0m  [18/21], [94mLoss[0m : 7.38761
[1mStep[0m  [20/21], [94mLoss[0m : 7.68343

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.703, [92mTest[0m: 7.739, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.93436
[1mStep[0m  [2/21], [94mLoss[0m : 7.56248
[1mStep[0m  [4/21], [94mLoss[0m : 7.85049
[1mStep[0m  [6/21], [94mLoss[0m : 7.59969
[1mStep[0m  [8/21], [94mLoss[0m : 7.30906
[1mStep[0m  [10/21], [94mLoss[0m : 7.47918
[1mStep[0m  [12/21], [94mLoss[0m : 7.44605
[1mStep[0m  [14/21], [94mLoss[0m : 7.61710
[1mStep[0m  [16/21], [94mLoss[0m : 7.60861
[1mStep[0m  [18/21], [94mLoss[0m : 7.45167
[1mStep[0m  [20/21], [94mLoss[0m : 7.73018

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.608, [92mTest[0m: 7.643, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.45114
[1mStep[0m  [2/21], [94mLoss[0m : 7.55003
[1mStep[0m  [4/21], [94mLoss[0m : 7.45875
[1mStep[0m  [6/21], [94mLoss[0m : 7.78778
[1mStep[0m  [8/21], [94mLoss[0m : 7.57487
[1mStep[0m  [10/21], [94mLoss[0m : 7.41531
[1mStep[0m  [12/21], [94mLoss[0m : 7.36266
[1mStep[0m  [14/21], [94mLoss[0m : 7.58934
[1mStep[0m  [16/21], [94mLoss[0m : 7.72637
[1mStep[0m  [18/21], [94mLoss[0m : 7.24483
[1mStep[0m  [20/21], [94mLoss[0m : 7.59364

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.508, [92mTest[0m: 7.551, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.440
====================================

Phase 1 - Evaluation MAE:  7.439707006726946
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 7.44554
[1mStep[0m  [2/21], [94mLoss[0m : 7.24439
[1mStep[0m  [4/21], [94mLoss[0m : 7.48974
[1mStep[0m  [6/21], [94mLoss[0m : 7.17430
[1mStep[0m  [8/21], [94mLoss[0m : 7.52817
[1mStep[0m  [10/21], [94mLoss[0m : 7.34857
[1mStep[0m  [12/21], [94mLoss[0m : 7.36713
[1mStep[0m  [14/21], [94mLoss[0m : 7.28087
[1mStep[0m  [16/21], [94mLoss[0m : 7.71085
[1mStep[0m  [18/21], [94mLoss[0m : 7.56731
[1mStep[0m  [20/21], [94mLoss[0m : 7.14148

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.403, [92mTest[0m: 7.443, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.52820
[1mStep[0m  [2/21], [94mLoss[0m : 7.56523
[1mStep[0m  [4/21], [94mLoss[0m : 7.14412
[1mStep[0m  [6/21], [94mLoss[0m : 7.27057
[1mStep[0m  [8/21], [94mLoss[0m : 6.93208
[1mStep[0m  [10/21], [94mLoss[0m : 7.12335
[1mStep[0m  [12/21], [94mLoss[0m : 7.52102
[1mStep[0m  [14/21], [94mLoss[0m : 7.23559
[1mStep[0m  [16/21], [94mLoss[0m : 7.39507
[1mStep[0m  [18/21], [94mLoss[0m : 7.08518
[1mStep[0m  [20/21], [94mLoss[0m : 7.06495

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.299, [92mTest[0m: 7.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.11884
[1mStep[0m  [2/21], [94mLoss[0m : 7.51779
[1mStep[0m  [4/21], [94mLoss[0m : 7.26459
[1mStep[0m  [6/21], [94mLoss[0m : 6.98395
[1mStep[0m  [8/21], [94mLoss[0m : 7.14122
[1mStep[0m  [10/21], [94mLoss[0m : 6.86003
[1mStep[0m  [12/21], [94mLoss[0m : 7.26474
[1mStep[0m  [14/21], [94mLoss[0m : 7.16227
[1mStep[0m  [16/21], [94mLoss[0m : 7.26669
[1mStep[0m  [18/21], [94mLoss[0m : 7.01930
[1mStep[0m  [20/21], [94mLoss[0m : 7.15822

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.186, [92mTest[0m: 7.238, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.30335
[1mStep[0m  [2/21], [94mLoss[0m : 7.31653
[1mStep[0m  [4/21], [94mLoss[0m : 6.93901
[1mStep[0m  [6/21], [94mLoss[0m : 7.07109
[1mStep[0m  [8/21], [94mLoss[0m : 6.88920
[1mStep[0m  [10/21], [94mLoss[0m : 6.55295
[1mStep[0m  [12/21], [94mLoss[0m : 6.95505
[1mStep[0m  [14/21], [94mLoss[0m : 7.16878
[1mStep[0m  [16/21], [94mLoss[0m : 6.90362
[1mStep[0m  [18/21], [94mLoss[0m : 7.04245
[1mStep[0m  [20/21], [94mLoss[0m : 6.92233

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.080, [92mTest[0m: 7.129, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.82398
[1mStep[0m  [2/21], [94mLoss[0m : 6.92839
[1mStep[0m  [4/21], [94mLoss[0m : 6.75356
[1mStep[0m  [6/21], [94mLoss[0m : 7.17633
[1mStep[0m  [8/21], [94mLoss[0m : 7.01139
[1mStep[0m  [10/21], [94mLoss[0m : 7.08701
[1mStep[0m  [12/21], [94mLoss[0m : 6.89366
[1mStep[0m  [14/21], [94mLoss[0m : 6.77318
[1mStep[0m  [16/21], [94mLoss[0m : 6.73346
[1mStep[0m  [18/21], [94mLoss[0m : 6.97284
[1mStep[0m  [20/21], [94mLoss[0m : 7.02387

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.963, [92mTest[0m: 7.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.91521
[1mStep[0m  [2/21], [94mLoss[0m : 6.99977
[1mStep[0m  [4/21], [94mLoss[0m : 6.66054
[1mStep[0m  [6/21], [94mLoss[0m : 6.94781
[1mStep[0m  [8/21], [94mLoss[0m : 7.01956
[1mStep[0m  [10/21], [94mLoss[0m : 7.16023
[1mStep[0m  [12/21], [94mLoss[0m : 6.85733
[1mStep[0m  [14/21], [94mLoss[0m : 6.82181
[1mStep[0m  [16/21], [94mLoss[0m : 6.85900
[1mStep[0m  [18/21], [94mLoss[0m : 6.79982
[1mStep[0m  [20/21], [94mLoss[0m : 6.93698

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.858, [92mTest[0m: 6.899, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.61915
[1mStep[0m  [2/21], [94mLoss[0m : 7.02942
[1mStep[0m  [4/21], [94mLoss[0m : 6.68839
[1mStep[0m  [6/21], [94mLoss[0m : 6.87362
[1mStep[0m  [8/21], [94mLoss[0m : 6.70973
[1mStep[0m  [10/21], [94mLoss[0m : 6.62361
[1mStep[0m  [12/21], [94mLoss[0m : 6.75401
[1mStep[0m  [14/21], [94mLoss[0m : 6.61863
[1mStep[0m  [16/21], [94mLoss[0m : 6.66307
[1mStep[0m  [18/21], [94mLoss[0m : 6.40050
[1mStep[0m  [20/21], [94mLoss[0m : 6.68608

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.745, [92mTest[0m: 6.799, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.56939
[1mStep[0m  [2/21], [94mLoss[0m : 6.65913
[1mStep[0m  [4/21], [94mLoss[0m : 6.47514
[1mStep[0m  [6/21], [94mLoss[0m : 6.54921
[1mStep[0m  [8/21], [94mLoss[0m : 6.79296
[1mStep[0m  [10/21], [94mLoss[0m : 6.71342
[1mStep[0m  [12/21], [94mLoss[0m : 6.76896
[1mStep[0m  [14/21], [94mLoss[0m : 6.48283
[1mStep[0m  [16/21], [94mLoss[0m : 6.60866
[1mStep[0m  [18/21], [94mLoss[0m : 6.58579
[1mStep[0m  [20/21], [94mLoss[0m : 6.49051

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.638, [92mTest[0m: 6.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.63928
[1mStep[0m  [2/21], [94mLoss[0m : 6.65041
[1mStep[0m  [4/21], [94mLoss[0m : 6.43309
[1mStep[0m  [6/21], [94mLoss[0m : 6.74255
[1mStep[0m  [8/21], [94mLoss[0m : 6.35209
[1mStep[0m  [10/21], [94mLoss[0m : 6.30314
[1mStep[0m  [12/21], [94mLoss[0m : 6.47612
[1mStep[0m  [14/21], [94mLoss[0m : 6.40616
[1mStep[0m  [16/21], [94mLoss[0m : 6.58919
[1mStep[0m  [18/21], [94mLoss[0m : 6.64986
[1mStep[0m  [20/21], [94mLoss[0m : 6.51632

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.531, [92mTest[0m: 6.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.63926
[1mStep[0m  [2/21], [94mLoss[0m : 6.29055
[1mStep[0m  [4/21], [94mLoss[0m : 6.45016
[1mStep[0m  [6/21], [94mLoss[0m : 6.50992
[1mStep[0m  [8/21], [94mLoss[0m : 6.59824
[1mStep[0m  [10/21], [94mLoss[0m : 6.25851
[1mStep[0m  [12/21], [94mLoss[0m : 6.26419
[1mStep[0m  [14/21], [94mLoss[0m : 6.27630
[1mStep[0m  [16/21], [94mLoss[0m : 6.47524
[1mStep[0m  [18/21], [94mLoss[0m : 6.30012
[1mStep[0m  [20/21], [94mLoss[0m : 6.81919

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.420, [92mTest[0m: 6.453, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.40097
[1mStep[0m  [2/21], [94mLoss[0m : 6.66199
[1mStep[0m  [4/21], [94mLoss[0m : 6.49112
[1mStep[0m  [6/21], [94mLoss[0m : 6.40216
[1mStep[0m  [8/21], [94mLoss[0m : 6.08063
[1mStep[0m  [10/21], [94mLoss[0m : 6.41168
[1mStep[0m  [12/21], [94mLoss[0m : 5.93119
[1mStep[0m  [14/21], [94mLoss[0m : 6.30098
[1mStep[0m  [16/21], [94mLoss[0m : 6.28113
[1mStep[0m  [18/21], [94mLoss[0m : 6.40531
[1mStep[0m  [20/21], [94mLoss[0m : 6.51449

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.303, [92mTest[0m: 6.344, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.54950
[1mStep[0m  [2/21], [94mLoss[0m : 6.28325
[1mStep[0m  [4/21], [94mLoss[0m : 6.44920
[1mStep[0m  [6/21], [94mLoss[0m : 6.38244
[1mStep[0m  [8/21], [94mLoss[0m : 6.16180
[1mStep[0m  [10/21], [94mLoss[0m : 6.11223
[1mStep[0m  [12/21], [94mLoss[0m : 6.02995
[1mStep[0m  [14/21], [94mLoss[0m : 6.30575
[1mStep[0m  [16/21], [94mLoss[0m : 6.08678
[1mStep[0m  [18/21], [94mLoss[0m : 6.06270
[1mStep[0m  [20/21], [94mLoss[0m : 6.27037

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.196, [92mTest[0m: 6.227, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.00412
[1mStep[0m  [2/21], [94mLoss[0m : 6.30902
[1mStep[0m  [4/21], [94mLoss[0m : 6.21034
[1mStep[0m  [6/21], [94mLoss[0m : 6.11892
[1mStep[0m  [8/21], [94mLoss[0m : 5.94396
[1mStep[0m  [10/21], [94mLoss[0m : 5.94462
[1mStep[0m  [12/21], [94mLoss[0m : 6.08378
[1mStep[0m  [14/21], [94mLoss[0m : 6.00486
[1mStep[0m  [16/21], [94mLoss[0m : 5.95256
[1mStep[0m  [18/21], [94mLoss[0m : 6.25174
[1mStep[0m  [20/21], [94mLoss[0m : 6.13651

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.090, [92mTest[0m: 6.130, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.95805
[1mStep[0m  [2/21], [94mLoss[0m : 5.98234
[1mStep[0m  [4/21], [94mLoss[0m : 6.09423
[1mStep[0m  [6/21], [94mLoss[0m : 6.05922
[1mStep[0m  [8/21], [94mLoss[0m : 6.03381
[1mStep[0m  [10/21], [94mLoss[0m : 5.98240
[1mStep[0m  [12/21], [94mLoss[0m : 5.94921
[1mStep[0m  [14/21], [94mLoss[0m : 5.77785
[1mStep[0m  [16/21], [94mLoss[0m : 5.76892
[1mStep[0m  [18/21], [94mLoss[0m : 5.71109
[1mStep[0m  [20/21], [94mLoss[0m : 5.88823

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.968, [92mTest[0m: 6.022, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.95123
[1mStep[0m  [2/21], [94mLoss[0m : 5.93418
[1mStep[0m  [4/21], [94mLoss[0m : 5.90799
[1mStep[0m  [6/21], [94mLoss[0m : 5.90754
[1mStep[0m  [8/21], [94mLoss[0m : 5.92732
[1mStep[0m  [10/21], [94mLoss[0m : 5.84405
[1mStep[0m  [12/21], [94mLoss[0m : 5.69030
[1mStep[0m  [14/21], [94mLoss[0m : 5.79198
[1mStep[0m  [16/21], [94mLoss[0m : 5.61831
[1mStep[0m  [18/21], [94mLoss[0m : 6.12796
[1mStep[0m  [20/21], [94mLoss[0m : 5.96656

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.873, [92mTest[0m: 5.917, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.60302
[1mStep[0m  [2/21], [94mLoss[0m : 5.87803
[1mStep[0m  [4/21], [94mLoss[0m : 5.89060
[1mStep[0m  [6/21], [94mLoss[0m : 5.75377
[1mStep[0m  [8/21], [94mLoss[0m : 5.76596
[1mStep[0m  [10/21], [94mLoss[0m : 6.02468
[1mStep[0m  [12/21], [94mLoss[0m : 5.81116
[1mStep[0m  [14/21], [94mLoss[0m : 5.50879
[1mStep[0m  [16/21], [94mLoss[0m : 5.81699
[1mStep[0m  [18/21], [94mLoss[0m : 5.74917
[1mStep[0m  [20/21], [94mLoss[0m : 5.55415

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.754, [92mTest[0m: 5.796, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.79628
[1mStep[0m  [2/21], [94mLoss[0m : 5.86418
[1mStep[0m  [4/21], [94mLoss[0m : 5.50756
[1mStep[0m  [6/21], [94mLoss[0m : 5.68952
[1mStep[0m  [8/21], [94mLoss[0m : 5.71385
[1mStep[0m  [10/21], [94mLoss[0m : 5.73413
[1mStep[0m  [12/21], [94mLoss[0m : 5.60427
[1mStep[0m  [14/21], [94mLoss[0m : 5.56243
[1mStep[0m  [16/21], [94mLoss[0m : 5.56643
[1mStep[0m  [18/21], [94mLoss[0m : 5.64980
[1mStep[0m  [20/21], [94mLoss[0m : 5.47277

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.654, [92mTest[0m: 5.700, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.71819
[1mStep[0m  [2/21], [94mLoss[0m : 5.56099
[1mStep[0m  [4/21], [94mLoss[0m : 5.77742
[1mStep[0m  [6/21], [94mLoss[0m : 5.61652
[1mStep[0m  [8/21], [94mLoss[0m : 5.43029
[1mStep[0m  [10/21], [94mLoss[0m : 5.56963
[1mStep[0m  [12/21], [94mLoss[0m : 5.67745
[1mStep[0m  [14/21], [94mLoss[0m : 5.25800
[1mStep[0m  [16/21], [94mLoss[0m : 5.24838
[1mStep[0m  [18/21], [94mLoss[0m : 5.90786
[1mStep[0m  [20/21], [94mLoss[0m : 5.44496

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.541, [92mTest[0m: 5.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.61450
[1mStep[0m  [2/21], [94mLoss[0m : 5.64012
[1mStep[0m  [4/21], [94mLoss[0m : 5.58438
[1mStep[0m  [6/21], [94mLoss[0m : 5.29065
[1mStep[0m  [8/21], [94mLoss[0m : 5.52516
[1mStep[0m  [10/21], [94mLoss[0m : 5.34730
[1mStep[0m  [12/21], [94mLoss[0m : 5.43505
[1mStep[0m  [14/21], [94mLoss[0m : 5.77381
[1mStep[0m  [16/21], [94mLoss[0m : 5.32538
[1mStep[0m  [18/21], [94mLoss[0m : 5.48964
[1mStep[0m  [20/21], [94mLoss[0m : 5.26914

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.447, [92mTest[0m: 5.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.50620
[1mStep[0m  [2/21], [94mLoss[0m : 5.32783
[1mStep[0m  [4/21], [94mLoss[0m : 5.10765
[1mStep[0m  [6/21], [94mLoss[0m : 5.55542
[1mStep[0m  [8/21], [94mLoss[0m : 5.42610
[1mStep[0m  [10/21], [94mLoss[0m : 5.30936
[1mStep[0m  [12/21], [94mLoss[0m : 5.69527
[1mStep[0m  [14/21], [94mLoss[0m : 5.18003
[1mStep[0m  [16/21], [94mLoss[0m : 5.25826
[1mStep[0m  [18/21], [94mLoss[0m : 5.44259
[1mStep[0m  [20/21], [94mLoss[0m : 5.30070

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.341, [92mTest[0m: 5.370, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.10942
[1mStep[0m  [2/21], [94mLoss[0m : 5.15533
[1mStep[0m  [4/21], [94mLoss[0m : 5.22440
[1mStep[0m  [6/21], [94mLoss[0m : 5.34090
[1mStep[0m  [8/21], [94mLoss[0m : 5.22932
[1mStep[0m  [10/21], [94mLoss[0m : 5.29671
[1mStep[0m  [12/21], [94mLoss[0m : 5.35740
[1mStep[0m  [14/21], [94mLoss[0m : 5.26528
[1mStep[0m  [16/21], [94mLoss[0m : 4.97833
[1mStep[0m  [18/21], [94mLoss[0m : 5.57281
[1mStep[0m  [20/21], [94mLoss[0m : 5.19321

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.241, [92mTest[0m: 5.284, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.08785
[1mStep[0m  [2/21], [94mLoss[0m : 5.29561
[1mStep[0m  [4/21], [94mLoss[0m : 5.35484
[1mStep[0m  [6/21], [94mLoss[0m : 4.98923
[1mStep[0m  [8/21], [94mLoss[0m : 5.10458
[1mStep[0m  [10/21], [94mLoss[0m : 5.12426
[1mStep[0m  [12/21], [94mLoss[0m : 5.01758
[1mStep[0m  [14/21], [94mLoss[0m : 5.12166
[1mStep[0m  [16/21], [94mLoss[0m : 5.01618
[1mStep[0m  [18/21], [94mLoss[0m : 5.36888
[1mStep[0m  [20/21], [94mLoss[0m : 5.00040

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.149, [92mTest[0m: 5.187, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.42875
[1mStep[0m  [2/21], [94mLoss[0m : 4.98142
[1mStep[0m  [4/21], [94mLoss[0m : 5.16218
[1mStep[0m  [6/21], [94mLoss[0m : 5.06018
[1mStep[0m  [8/21], [94mLoss[0m : 5.36136
[1mStep[0m  [10/21], [94mLoss[0m : 4.97915
[1mStep[0m  [12/21], [94mLoss[0m : 4.70411
[1mStep[0m  [14/21], [94mLoss[0m : 5.04960
[1mStep[0m  [16/21], [94mLoss[0m : 4.99624
[1mStep[0m  [18/21], [94mLoss[0m : 5.03484
[1mStep[0m  [20/21], [94mLoss[0m : 5.10852

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.065, [92mTest[0m: 5.095, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16454
[1mStep[0m  [2/21], [94mLoss[0m : 5.10422
[1mStep[0m  [4/21], [94mLoss[0m : 5.03140
[1mStep[0m  [6/21], [94mLoss[0m : 5.03147
[1mStep[0m  [8/21], [94mLoss[0m : 4.96292
[1mStep[0m  [10/21], [94mLoss[0m : 4.73377
[1mStep[0m  [12/21], [94mLoss[0m : 5.12604
[1mStep[0m  [14/21], [94mLoss[0m : 4.96971
[1mStep[0m  [16/21], [94mLoss[0m : 4.71426
[1mStep[0m  [18/21], [94mLoss[0m : 4.79815
[1mStep[0m  [20/21], [94mLoss[0m : 4.85057

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.978, [92mTest[0m: 5.013, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.90900
[1mStep[0m  [2/21], [94mLoss[0m : 4.70566
[1mStep[0m  [4/21], [94mLoss[0m : 4.94012
[1mStep[0m  [6/21], [94mLoss[0m : 5.09716
[1mStep[0m  [8/21], [94mLoss[0m : 4.70616
[1mStep[0m  [10/21], [94mLoss[0m : 4.62794
[1mStep[0m  [12/21], [94mLoss[0m : 5.04474
[1mStep[0m  [14/21], [94mLoss[0m : 4.99555
[1mStep[0m  [16/21], [94mLoss[0m : 4.72379
[1mStep[0m  [18/21], [94mLoss[0m : 4.86475
[1mStep[0m  [20/21], [94mLoss[0m : 5.13838

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.896, [92mTest[0m: 4.930, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.06126
[1mStep[0m  [2/21], [94mLoss[0m : 4.64562
[1mStep[0m  [4/21], [94mLoss[0m : 4.84949
[1mStep[0m  [6/21], [94mLoss[0m : 4.98019
[1mStep[0m  [8/21], [94mLoss[0m : 4.69652
[1mStep[0m  [10/21], [94mLoss[0m : 4.86332
[1mStep[0m  [12/21], [94mLoss[0m : 4.69351
[1mStep[0m  [14/21], [94mLoss[0m : 4.73470
[1mStep[0m  [16/21], [94mLoss[0m : 4.73372
[1mStep[0m  [18/21], [94mLoss[0m : 4.78126
[1mStep[0m  [20/21], [94mLoss[0m : 4.88475

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.804, [92mTest[0m: 4.846, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.52104
[1mStep[0m  [2/21], [94mLoss[0m : 4.57474
[1mStep[0m  [4/21], [94mLoss[0m : 4.83343
[1mStep[0m  [6/21], [94mLoss[0m : 4.73215
[1mStep[0m  [8/21], [94mLoss[0m : 4.99269
[1mStep[0m  [10/21], [94mLoss[0m : 4.76975
[1mStep[0m  [12/21], [94mLoss[0m : 4.67284
[1mStep[0m  [14/21], [94mLoss[0m : 4.82419
[1mStep[0m  [16/21], [94mLoss[0m : 4.42353
[1mStep[0m  [18/21], [94mLoss[0m : 4.62873
[1mStep[0m  [20/21], [94mLoss[0m : 4.84728

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.741, [92mTest[0m: 4.775, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.61028
[1mStep[0m  [2/21], [94mLoss[0m : 4.39540
[1mStep[0m  [4/21], [94mLoss[0m : 4.62719
[1mStep[0m  [6/21], [94mLoss[0m : 4.81280
[1mStep[0m  [8/21], [94mLoss[0m : 4.55357
[1mStep[0m  [10/21], [94mLoss[0m : 4.65263
[1mStep[0m  [12/21], [94mLoss[0m : 4.45911
[1mStep[0m  [14/21], [94mLoss[0m : 4.48000
[1mStep[0m  [16/21], [94mLoss[0m : 4.53761
[1mStep[0m  [18/21], [94mLoss[0m : 4.77334
[1mStep[0m  [20/21], [94mLoss[0m : 4.88551

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.639, [92mTest[0m: 4.682, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.22976
[1mStep[0m  [2/21], [94mLoss[0m : 4.57973
[1mStep[0m  [4/21], [94mLoss[0m : 4.46548
[1mStep[0m  [6/21], [94mLoss[0m : 4.49195
[1mStep[0m  [8/21], [94mLoss[0m : 4.69900
[1mStep[0m  [10/21], [94mLoss[0m : 4.68670
[1mStep[0m  [12/21], [94mLoss[0m : 4.49026
[1mStep[0m  [14/21], [94mLoss[0m : 4.89190
[1mStep[0m  [16/21], [94mLoss[0m : 4.72132
[1mStep[0m  [18/21], [94mLoss[0m : 4.74167
[1mStep[0m  [20/21], [94mLoss[0m : 4.79302

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.585, [92mTest[0m: 4.608, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.23465
[1mStep[0m  [2/21], [94mLoss[0m : 4.66867
[1mStep[0m  [4/21], [94mLoss[0m : 4.49360
[1mStep[0m  [6/21], [94mLoss[0m : 4.53608
[1mStep[0m  [8/21], [94mLoss[0m : 4.49454
[1mStep[0m  [10/21], [94mLoss[0m : 4.46868
[1mStep[0m  [12/21], [94mLoss[0m : 4.41632
[1mStep[0m  [14/21], [94mLoss[0m : 4.60780
[1mStep[0m  [16/21], [94mLoss[0m : 4.59994
[1mStep[0m  [18/21], [94mLoss[0m : 4.33720
[1mStep[0m  [20/21], [94mLoss[0m : 4.35547

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.502, [92mTest[0m: 4.545, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.448
====================================

Phase 2 - Evaluation MAE:  4.447725704738072
MAE score P1      7.439707
MAE score P2      4.447726
loss              4.501743
learning_rate       0.0001
batch_size             512
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.02357
[1mStep[0m  [4/42], [94mLoss[0m : 10.45846
[1mStep[0m  [8/42], [94mLoss[0m : 10.89351
[1mStep[0m  [12/42], [94mLoss[0m : 10.88674
[1mStep[0m  [16/42], [94mLoss[0m : 10.72482
[1mStep[0m  [20/42], [94mLoss[0m : 10.98835
[1mStep[0m  [24/42], [94mLoss[0m : 10.76940
[1mStep[0m  [28/42], [94mLoss[0m : 10.85624
[1mStep[0m  [32/42], [94mLoss[0m : 10.80220
[1mStep[0m  [36/42], [94mLoss[0m : 11.22492
[1mStep[0m  [40/42], [94mLoss[0m : 10.88973

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.881, [92mTest[0m: 10.917, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22887
[1mStep[0m  [4/42], [94mLoss[0m : 10.65523
[1mStep[0m  [8/42], [94mLoss[0m : 10.87605
[1mStep[0m  [12/42], [94mLoss[0m : 10.58613
[1mStep[0m  [16/42], [94mLoss[0m : 10.41489
[1mStep[0m  [20/42], [94mLoss[0m : 10.34548
[1mStep[0m  [24/42], [94mLoss[0m : 10.53650
[1mStep[0m  [28/42], [94mLoss[0m : 10.42031
[1mStep[0m  [32/42], [94mLoss[0m : 10.28857
[1mStep[0m  [36/42], [94mLoss[0m : 10.53324
[1mStep[0m  [40/42], [94mLoss[0m : 9.93433

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.540, [92mTest[0m: 10.800, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37961
[1mStep[0m  [4/42], [94mLoss[0m : 10.25735
[1mStep[0m  [8/42], [94mLoss[0m : 10.44121
[1mStep[0m  [12/42], [94mLoss[0m : 10.65314
[1mStep[0m  [16/42], [94mLoss[0m : 10.24998
[1mStep[0m  [20/42], [94mLoss[0m : 9.97650
[1mStep[0m  [24/42], [94mLoss[0m : 10.35750
[1mStep[0m  [28/42], [94mLoss[0m : 10.22481
[1mStep[0m  [32/42], [94mLoss[0m : 10.32423
[1mStep[0m  [36/42], [94mLoss[0m : 10.13073
[1mStep[0m  [40/42], [94mLoss[0m : 9.98815

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.183, [92mTest[0m: 10.562, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.60639
[1mStep[0m  [4/42], [94mLoss[0m : 9.75584
[1mStep[0m  [8/42], [94mLoss[0m : 9.96594
[1mStep[0m  [12/42], [94mLoss[0m : 9.95408
[1mStep[0m  [16/42], [94mLoss[0m : 10.13057
[1mStep[0m  [20/42], [94mLoss[0m : 9.90115
[1mStep[0m  [24/42], [94mLoss[0m : 9.69671
[1mStep[0m  [28/42], [94mLoss[0m : 9.65876
[1mStep[0m  [32/42], [94mLoss[0m : 9.64434
[1mStep[0m  [36/42], [94mLoss[0m : 10.11019
[1mStep[0m  [40/42], [94mLoss[0m : 9.52514

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.814, [92mTest[0m: 10.347, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.85905
[1mStep[0m  [4/42], [94mLoss[0m : 9.63454
[1mStep[0m  [8/42], [94mLoss[0m : 9.46248
[1mStep[0m  [12/42], [94mLoss[0m : 9.55954
[1mStep[0m  [16/42], [94mLoss[0m : 9.23218
[1mStep[0m  [20/42], [94mLoss[0m : 9.66265
[1mStep[0m  [24/42], [94mLoss[0m : 9.38616
[1mStep[0m  [28/42], [94mLoss[0m : 9.63910
[1mStep[0m  [32/42], [94mLoss[0m : 9.28143
[1mStep[0m  [36/42], [94mLoss[0m : 9.23021
[1mStep[0m  [40/42], [94mLoss[0m : 8.79336

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.460, [92mTest[0m: 10.121, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.12159
[1mStep[0m  [4/42], [94mLoss[0m : 8.84424
[1mStep[0m  [8/42], [94mLoss[0m : 9.12849
[1mStep[0m  [12/42], [94mLoss[0m : 9.08946
[1mStep[0m  [16/42], [94mLoss[0m : 9.41495
[1mStep[0m  [20/42], [94mLoss[0m : 8.99930
[1mStep[0m  [24/42], [94mLoss[0m : 9.12306
[1mStep[0m  [28/42], [94mLoss[0m : 9.38598
[1mStep[0m  [32/42], [94mLoss[0m : 8.68843
[1mStep[0m  [36/42], [94mLoss[0m : 8.67005
[1mStep[0m  [40/42], [94mLoss[0m : 8.96920

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.091, [92mTest[0m: 9.871, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.91110
[1mStep[0m  [4/42], [94mLoss[0m : 8.36616
[1mStep[0m  [8/42], [94mLoss[0m : 8.78899
[1mStep[0m  [12/42], [94mLoss[0m : 9.14250
[1mStep[0m  [16/42], [94mLoss[0m : 9.04532
[1mStep[0m  [20/42], [94mLoss[0m : 8.65917
[1mStep[0m  [24/42], [94mLoss[0m : 9.04891
[1mStep[0m  [28/42], [94mLoss[0m : 8.33320
[1mStep[0m  [32/42], [94mLoss[0m : 8.65427
[1mStep[0m  [36/42], [94mLoss[0m : 8.50547
[1mStep[0m  [40/42], [94mLoss[0m : 8.73796

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.729, [92mTest[0m: 9.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.79564
[1mStep[0m  [4/42], [94mLoss[0m : 8.36579
[1mStep[0m  [8/42], [94mLoss[0m : 8.64261
[1mStep[0m  [12/42], [94mLoss[0m : 8.38502
[1mStep[0m  [16/42], [94mLoss[0m : 8.41845
[1mStep[0m  [20/42], [94mLoss[0m : 8.72970
[1mStep[0m  [24/42], [94mLoss[0m : 8.20504
[1mStep[0m  [28/42], [94mLoss[0m : 8.47472
[1mStep[0m  [32/42], [94mLoss[0m : 8.57100
[1mStep[0m  [36/42], [94mLoss[0m : 8.11700
[1mStep[0m  [40/42], [94mLoss[0m : 7.85294

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.342, [92mTest[0m: 9.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.75588
[1mStep[0m  [4/42], [94mLoss[0m : 8.03977
[1mStep[0m  [8/42], [94mLoss[0m : 8.00531
[1mStep[0m  [12/42], [94mLoss[0m : 7.81577
[1mStep[0m  [16/42], [94mLoss[0m : 7.64081
[1mStep[0m  [20/42], [94mLoss[0m : 8.05115
[1mStep[0m  [24/42], [94mLoss[0m : 7.84623
[1mStep[0m  [28/42], [94mLoss[0m : 8.25599
[1mStep[0m  [32/42], [94mLoss[0m : 8.00880
[1mStep[0m  [36/42], [94mLoss[0m : 8.00249
[1mStep[0m  [40/42], [94mLoss[0m : 7.90992

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.961, [92mTest[0m: 9.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.71714
[1mStep[0m  [4/42], [94mLoss[0m : 7.92621
[1mStep[0m  [8/42], [94mLoss[0m : 7.72020
[1mStep[0m  [12/42], [94mLoss[0m : 7.39566
[1mStep[0m  [16/42], [94mLoss[0m : 7.52720
[1mStep[0m  [20/42], [94mLoss[0m : 7.62081
[1mStep[0m  [24/42], [94mLoss[0m : 7.42923
[1mStep[0m  [28/42], [94mLoss[0m : 7.74148
[1mStep[0m  [32/42], [94mLoss[0m : 7.54722
[1mStep[0m  [36/42], [94mLoss[0m : 7.60794
[1mStep[0m  [40/42], [94mLoss[0m : 7.03039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.570, [92mTest[0m: 8.925, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.43006
[1mStep[0m  [4/42], [94mLoss[0m : 7.07905
[1mStep[0m  [8/42], [94mLoss[0m : 7.68699
[1mStep[0m  [12/42], [94mLoss[0m : 7.44547
[1mStep[0m  [16/42], [94mLoss[0m : 7.38844
[1mStep[0m  [20/42], [94mLoss[0m : 6.79352
[1mStep[0m  [24/42], [94mLoss[0m : 6.68663
[1mStep[0m  [28/42], [94mLoss[0m : 7.04630
[1mStep[0m  [32/42], [94mLoss[0m : 6.85564
[1mStep[0m  [36/42], [94mLoss[0m : 7.00653
[1mStep[0m  [40/42], [94mLoss[0m : 6.89516

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.176, [92mTest[0m: 8.648, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.12662
[1mStep[0m  [4/42], [94mLoss[0m : 6.98350
[1mStep[0m  [8/42], [94mLoss[0m : 6.76066
[1mStep[0m  [12/42], [94mLoss[0m : 6.53471
[1mStep[0m  [16/42], [94mLoss[0m : 6.97828
[1mStep[0m  [20/42], [94mLoss[0m : 6.66248
[1mStep[0m  [24/42], [94mLoss[0m : 6.88331
[1mStep[0m  [28/42], [94mLoss[0m : 6.52852
[1mStep[0m  [32/42], [94mLoss[0m : 6.52417
[1mStep[0m  [36/42], [94mLoss[0m : 6.36052
[1mStep[0m  [40/42], [94mLoss[0m : 6.51719

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.782, [92mTest[0m: 8.355, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.48849
[1mStep[0m  [4/42], [94mLoss[0m : 6.63607
[1mStep[0m  [8/42], [94mLoss[0m : 6.84747
[1mStep[0m  [12/42], [94mLoss[0m : 6.44289
[1mStep[0m  [16/42], [94mLoss[0m : 6.19937
[1mStep[0m  [20/42], [94mLoss[0m : 6.09852
[1mStep[0m  [24/42], [94mLoss[0m : 6.81312
[1mStep[0m  [28/42], [94mLoss[0m : 6.26528
[1mStep[0m  [32/42], [94mLoss[0m : 6.04860
[1mStep[0m  [36/42], [94mLoss[0m : 6.60420
[1mStep[0m  [40/42], [94mLoss[0m : 5.90981

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.400, [92mTest[0m: 8.040, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 12 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.737
====================================

Phase 1 - Evaluation MAE:  7.736732278551374
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 6.20117
[1mStep[0m  [4/42], [94mLoss[0m : 6.14179
[1mStep[0m  [8/42], [94mLoss[0m : 5.87397
[1mStep[0m  [12/42], [94mLoss[0m : 6.01840
[1mStep[0m  [16/42], [94mLoss[0m : 5.96454
[1mStep[0m  [20/42], [94mLoss[0m : 6.13379
[1mStep[0m  [24/42], [94mLoss[0m : 6.05244
[1mStep[0m  [28/42], [94mLoss[0m : 6.17521
[1mStep[0m  [32/42], [94mLoss[0m : 5.71290
[1mStep[0m  [36/42], [94mLoss[0m : 5.68884
[1mStep[0m  [40/42], [94mLoss[0m : 6.20133

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.080, [92mTest[0m: 7.734, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.436
====================================

Phase 2 - Evaluation MAE:  7.435987029756818
MAE score P1       7.736732
MAE score P2       7.435987
loss               6.079923
learning_rate        0.0001
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.28051
[1mStep[0m  [4/42], [94mLoss[0m : 10.81010
[1mStep[0m  [8/42], [94mLoss[0m : 11.29729
[1mStep[0m  [12/42], [94mLoss[0m : 10.88285
[1mStep[0m  [16/42], [94mLoss[0m : 10.76284
[1mStep[0m  [20/42], [94mLoss[0m : 10.60859
[1mStep[0m  [24/42], [94mLoss[0m : 10.22848
[1mStep[0m  [28/42], [94mLoss[0m : 10.18137
[1mStep[0m  [32/42], [94mLoss[0m : 10.37414
[1mStep[0m  [36/42], [94mLoss[0m : 9.91081
[1mStep[0m  [40/42], [94mLoss[0m : 9.87815

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.666, [92mTest[0m: 11.021, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08882
[1mStep[0m  [4/42], [94mLoss[0m : 9.86107
[1mStep[0m  [8/42], [94mLoss[0m : 9.57269
[1mStep[0m  [12/42], [94mLoss[0m : 9.26687
[1mStep[0m  [16/42], [94mLoss[0m : 9.45565
[1mStep[0m  [20/42], [94mLoss[0m : 9.38470
[1mStep[0m  [24/42], [94mLoss[0m : 8.94618
[1mStep[0m  [28/42], [94mLoss[0m : 9.06605
[1mStep[0m  [32/42], [94mLoss[0m : 9.00717
[1mStep[0m  [36/42], [94mLoss[0m : 8.70110
[1mStep[0m  [40/42], [94mLoss[0m : 8.54594

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.133, [92mTest[0m: 10.113, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34796
[1mStep[0m  [4/42], [94mLoss[0m : 7.93289
[1mStep[0m  [8/42], [94mLoss[0m : 8.00848
[1mStep[0m  [12/42], [94mLoss[0m : 7.50792
[1mStep[0m  [16/42], [94mLoss[0m : 7.59878
[1mStep[0m  [20/42], [94mLoss[0m : 7.48022
[1mStep[0m  [24/42], [94mLoss[0m : 7.20739
[1mStep[0m  [28/42], [94mLoss[0m : 7.15354
[1mStep[0m  [32/42], [94mLoss[0m : 7.29007
[1mStep[0m  [36/42], [94mLoss[0m : 6.93893
[1mStep[0m  [40/42], [94mLoss[0m : 6.86521

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.525, [92mTest[0m: 8.778, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.49779
[1mStep[0m  [4/42], [94mLoss[0m : 6.57612
[1mStep[0m  [8/42], [94mLoss[0m : 6.69826
[1mStep[0m  [12/42], [94mLoss[0m : 6.46417
[1mStep[0m  [16/42], [94mLoss[0m : 6.41795
[1mStep[0m  [20/42], [94mLoss[0m : 5.93266
[1mStep[0m  [24/42], [94mLoss[0m : 6.05239
[1mStep[0m  [28/42], [94mLoss[0m : 5.65483
[1mStep[0m  [32/42], [94mLoss[0m : 5.99187
[1mStep[0m  [36/42], [94mLoss[0m : 5.90731
[1mStep[0m  [40/42], [94mLoss[0m : 5.60891

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.062, [92mTest[0m: 7.443, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.55942
[1mStep[0m  [4/42], [94mLoss[0m : 4.87009
[1mStep[0m  [8/42], [94mLoss[0m : 4.81433
[1mStep[0m  [12/42], [94mLoss[0m : 5.39942
[1mStep[0m  [16/42], [94mLoss[0m : 4.96476
[1mStep[0m  [20/42], [94mLoss[0m : 4.94679
[1mStep[0m  [24/42], [94mLoss[0m : 5.02570
[1mStep[0m  [28/42], [94mLoss[0m : 4.64526
[1mStep[0m  [32/42], [94mLoss[0m : 4.43928
[1mStep[0m  [36/42], [94mLoss[0m : 4.86889
[1mStep[0m  [40/42], [94mLoss[0m : 4.47561

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.922, [92mTest[0m: 6.262, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.41017
[1mStep[0m  [4/42], [94mLoss[0m : 4.21838
[1mStep[0m  [8/42], [94mLoss[0m : 3.65825
[1mStep[0m  [12/42], [94mLoss[0m : 4.22138
[1mStep[0m  [16/42], [94mLoss[0m : 4.28087
[1mStep[0m  [20/42], [94mLoss[0m : 4.07962
[1mStep[0m  [24/42], [94mLoss[0m : 3.82173
[1mStep[0m  [28/42], [94mLoss[0m : 3.73627
[1mStep[0m  [32/42], [94mLoss[0m : 3.97592
[1mStep[0m  [36/42], [94mLoss[0m : 3.90709
[1mStep[0m  [40/42], [94mLoss[0m : 3.88771

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.015, [92mTest[0m: 5.246, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.27332
[1mStep[0m  [4/42], [94mLoss[0m : 3.61024
[1mStep[0m  [8/42], [94mLoss[0m : 3.41293
[1mStep[0m  [12/42], [94mLoss[0m : 3.36102
[1mStep[0m  [16/42], [94mLoss[0m : 3.40624
[1mStep[0m  [20/42], [94mLoss[0m : 3.39127
[1mStep[0m  [24/42], [94mLoss[0m : 3.37461
[1mStep[0m  [28/42], [94mLoss[0m : 3.61845
[1mStep[0m  [32/42], [94mLoss[0m : 3.07154
[1mStep[0m  [36/42], [94mLoss[0m : 2.92912
[1mStep[0m  [40/42], [94mLoss[0m : 3.03647

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.371, [92mTest[0m: 4.299, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.25741
[1mStep[0m  [4/42], [94mLoss[0m : 3.21881
[1mStep[0m  [8/42], [94mLoss[0m : 2.81304
[1mStep[0m  [12/42], [94mLoss[0m : 3.15079
[1mStep[0m  [16/42], [94mLoss[0m : 2.97568
[1mStep[0m  [20/42], [94mLoss[0m : 2.95631
[1mStep[0m  [24/42], [94mLoss[0m : 3.15232
[1mStep[0m  [28/42], [94mLoss[0m : 2.91318
[1mStep[0m  [32/42], [94mLoss[0m : 2.92107
[1mStep[0m  [36/42], [94mLoss[0m : 2.84047
[1mStep[0m  [40/42], [94mLoss[0m : 2.95137

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.993, [92mTest[0m: 3.609, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.10094
[1mStep[0m  [4/42], [94mLoss[0m : 3.06289
[1mStep[0m  [8/42], [94mLoss[0m : 2.85035
[1mStep[0m  [12/42], [94mLoss[0m : 2.67634
[1mStep[0m  [16/42], [94mLoss[0m : 2.92031
[1mStep[0m  [20/42], [94mLoss[0m : 2.71144
[1mStep[0m  [24/42], [94mLoss[0m : 3.01566
[1mStep[0m  [28/42], [94mLoss[0m : 3.01204
[1mStep[0m  [32/42], [94mLoss[0m : 2.78982
[1mStep[0m  [36/42], [94mLoss[0m : 2.69758
[1mStep[0m  [40/42], [94mLoss[0m : 2.60412

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.807, [92mTest[0m: 3.167, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65426
[1mStep[0m  [4/42], [94mLoss[0m : 2.76311
[1mStep[0m  [8/42], [94mLoss[0m : 2.74386
[1mStep[0m  [12/42], [94mLoss[0m : 2.67234
[1mStep[0m  [16/42], [94mLoss[0m : 2.64223
[1mStep[0m  [20/42], [94mLoss[0m : 2.72270
[1mStep[0m  [24/42], [94mLoss[0m : 2.51374
[1mStep[0m  [28/42], [94mLoss[0m : 2.74635
[1mStep[0m  [32/42], [94mLoss[0m : 2.54324
[1mStep[0m  [36/42], [94mLoss[0m : 2.60925
[1mStep[0m  [40/42], [94mLoss[0m : 2.58303

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71124
[1mStep[0m  [4/42], [94mLoss[0m : 2.58277
[1mStep[0m  [8/42], [94mLoss[0m : 2.57603
[1mStep[0m  [12/42], [94mLoss[0m : 2.67669
[1mStep[0m  [16/42], [94mLoss[0m : 2.55710
[1mStep[0m  [20/42], [94mLoss[0m : 2.76817
[1mStep[0m  [24/42], [94mLoss[0m : 2.62064
[1mStep[0m  [28/42], [94mLoss[0m : 2.49755
[1mStep[0m  [32/42], [94mLoss[0m : 2.55228
[1mStep[0m  [36/42], [94mLoss[0m : 2.50557
[1mStep[0m  [40/42], [94mLoss[0m : 2.74099

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43622
[1mStep[0m  [4/42], [94mLoss[0m : 2.71605
[1mStep[0m  [8/42], [94mLoss[0m : 2.74127
[1mStep[0m  [12/42], [94mLoss[0m : 2.84244
[1mStep[0m  [16/42], [94mLoss[0m : 2.57853
[1mStep[0m  [20/42], [94mLoss[0m : 2.57695
[1mStep[0m  [24/42], [94mLoss[0m : 2.43780
[1mStep[0m  [28/42], [94mLoss[0m : 2.69344
[1mStep[0m  [32/42], [94mLoss[0m : 2.58934
[1mStep[0m  [36/42], [94mLoss[0m : 2.52550
[1mStep[0m  [40/42], [94mLoss[0m : 2.52476

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.753, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51944
[1mStep[0m  [4/42], [94mLoss[0m : 2.61539
[1mStep[0m  [8/42], [94mLoss[0m : 2.73770
[1mStep[0m  [12/42], [94mLoss[0m : 2.34425
[1mStep[0m  [16/42], [94mLoss[0m : 2.67678
[1mStep[0m  [20/42], [94mLoss[0m : 2.51990
[1mStep[0m  [24/42], [94mLoss[0m : 2.89448
[1mStep[0m  [28/42], [94mLoss[0m : 2.38450
[1mStep[0m  [32/42], [94mLoss[0m : 2.54119
[1mStep[0m  [36/42], [94mLoss[0m : 2.81634
[1mStep[0m  [40/42], [94mLoss[0m : 2.56677

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55992
[1mStep[0m  [4/42], [94mLoss[0m : 2.68540
[1mStep[0m  [8/42], [94mLoss[0m : 2.83202
[1mStep[0m  [12/42], [94mLoss[0m : 2.71263
[1mStep[0m  [16/42], [94mLoss[0m : 2.67400
[1mStep[0m  [20/42], [94mLoss[0m : 2.81869
[1mStep[0m  [24/42], [94mLoss[0m : 2.62701
[1mStep[0m  [28/42], [94mLoss[0m : 2.69421
[1mStep[0m  [32/42], [94mLoss[0m : 2.57700
[1mStep[0m  [36/42], [94mLoss[0m : 2.52466
[1mStep[0m  [40/42], [94mLoss[0m : 2.61506

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66957
[1mStep[0m  [4/42], [94mLoss[0m : 2.53972
[1mStep[0m  [8/42], [94mLoss[0m : 2.52592
[1mStep[0m  [12/42], [94mLoss[0m : 2.55102
[1mStep[0m  [16/42], [94mLoss[0m : 2.58718
[1mStep[0m  [20/42], [94mLoss[0m : 2.61537
[1mStep[0m  [24/42], [94mLoss[0m : 2.62315
[1mStep[0m  [28/42], [94mLoss[0m : 2.59950
[1mStep[0m  [32/42], [94mLoss[0m : 2.69506
[1mStep[0m  [36/42], [94mLoss[0m : 2.51140
[1mStep[0m  [40/42], [94mLoss[0m : 2.67295

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.663, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40376
[1mStep[0m  [4/42], [94mLoss[0m : 2.52470
[1mStep[0m  [8/42], [94mLoss[0m : 2.45948
[1mStep[0m  [12/42], [94mLoss[0m : 2.53557
[1mStep[0m  [16/42], [94mLoss[0m : 2.53300
[1mStep[0m  [20/42], [94mLoss[0m : 2.63694
[1mStep[0m  [24/42], [94mLoss[0m : 2.56737
[1mStep[0m  [28/42], [94mLoss[0m : 2.70006
[1mStep[0m  [32/42], [94mLoss[0m : 2.46161
[1mStep[0m  [36/42], [94mLoss[0m : 2.66458
[1mStep[0m  [40/42], [94mLoss[0m : 2.45656

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52216
[1mStep[0m  [4/42], [94mLoss[0m : 2.56719
[1mStep[0m  [8/42], [94mLoss[0m : 2.52330
[1mStep[0m  [12/42], [94mLoss[0m : 2.54314
[1mStep[0m  [16/42], [94mLoss[0m : 2.55516
[1mStep[0m  [20/42], [94mLoss[0m : 2.69331
[1mStep[0m  [24/42], [94mLoss[0m : 2.64500
[1mStep[0m  [28/42], [94mLoss[0m : 2.60752
[1mStep[0m  [32/42], [94mLoss[0m : 2.54777
[1mStep[0m  [36/42], [94mLoss[0m : 2.53105
[1mStep[0m  [40/42], [94mLoss[0m : 2.67168

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58768
[1mStep[0m  [4/42], [94mLoss[0m : 2.61916
[1mStep[0m  [8/42], [94mLoss[0m : 2.63683
[1mStep[0m  [12/42], [94mLoss[0m : 2.60439
[1mStep[0m  [16/42], [94mLoss[0m : 2.60675
[1mStep[0m  [20/42], [94mLoss[0m : 2.80758
[1mStep[0m  [24/42], [94mLoss[0m : 2.54488
[1mStep[0m  [28/42], [94mLoss[0m : 2.55590
[1mStep[0m  [32/42], [94mLoss[0m : 2.46005
[1mStep[0m  [36/42], [94mLoss[0m : 2.51018
[1mStep[0m  [40/42], [94mLoss[0m : 2.69151

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51799
[1mStep[0m  [4/42], [94mLoss[0m : 2.59872
[1mStep[0m  [8/42], [94mLoss[0m : 2.58137
[1mStep[0m  [12/42], [94mLoss[0m : 2.77923
[1mStep[0m  [16/42], [94mLoss[0m : 2.52313
[1mStep[0m  [20/42], [94mLoss[0m : 2.53945
[1mStep[0m  [24/42], [94mLoss[0m : 2.43949
[1mStep[0m  [28/42], [94mLoss[0m : 2.32255
[1mStep[0m  [32/42], [94mLoss[0m : 2.68656
[1mStep[0m  [36/42], [94mLoss[0m : 2.72567
[1mStep[0m  [40/42], [94mLoss[0m : 2.50989

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60669
[1mStep[0m  [4/42], [94mLoss[0m : 2.51773
[1mStep[0m  [8/42], [94mLoss[0m : 2.55137
[1mStep[0m  [12/42], [94mLoss[0m : 2.57606
[1mStep[0m  [16/42], [94mLoss[0m : 2.63364
[1mStep[0m  [20/42], [94mLoss[0m : 2.56874
[1mStep[0m  [24/42], [94mLoss[0m : 2.59482
[1mStep[0m  [28/42], [94mLoss[0m : 2.31072
[1mStep[0m  [32/42], [94mLoss[0m : 2.56872
[1mStep[0m  [36/42], [94mLoss[0m : 2.60467
[1mStep[0m  [40/42], [94mLoss[0m : 2.54374

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.587, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55737
[1mStep[0m  [4/42], [94mLoss[0m : 2.45840
[1mStep[0m  [8/42], [94mLoss[0m : 2.44608
[1mStep[0m  [12/42], [94mLoss[0m : 2.61689
[1mStep[0m  [16/42], [94mLoss[0m : 2.69363
[1mStep[0m  [20/42], [94mLoss[0m : 2.66958
[1mStep[0m  [24/42], [94mLoss[0m : 2.45057
[1mStep[0m  [28/42], [94mLoss[0m : 2.52955
[1mStep[0m  [32/42], [94mLoss[0m : 2.53251
[1mStep[0m  [36/42], [94mLoss[0m : 2.50684
[1mStep[0m  [40/42], [94mLoss[0m : 2.56721

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.592, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47039
[1mStep[0m  [4/42], [94mLoss[0m : 2.54025
[1mStep[0m  [8/42], [94mLoss[0m : 2.28864
[1mStep[0m  [12/42], [94mLoss[0m : 2.58856
[1mStep[0m  [16/42], [94mLoss[0m : 2.50066
[1mStep[0m  [20/42], [94mLoss[0m : 2.72672
[1mStep[0m  [24/42], [94mLoss[0m : 2.62694
[1mStep[0m  [28/42], [94mLoss[0m : 2.48391
[1mStep[0m  [32/42], [94mLoss[0m : 2.69823
[1mStep[0m  [36/42], [94mLoss[0m : 2.49456
[1mStep[0m  [40/42], [94mLoss[0m : 2.59297

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79086
[1mStep[0m  [4/42], [94mLoss[0m : 2.55718
[1mStep[0m  [8/42], [94mLoss[0m : 2.35545
[1mStep[0m  [12/42], [94mLoss[0m : 2.40811
[1mStep[0m  [16/42], [94mLoss[0m : 2.42982
[1mStep[0m  [20/42], [94mLoss[0m : 2.53066
[1mStep[0m  [24/42], [94mLoss[0m : 2.45939
[1mStep[0m  [28/42], [94mLoss[0m : 2.48029
[1mStep[0m  [32/42], [94mLoss[0m : 2.49767
[1mStep[0m  [36/42], [94mLoss[0m : 2.38672
[1mStep[0m  [40/42], [94mLoss[0m : 2.43127

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.569, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46153
[1mStep[0m  [4/42], [94mLoss[0m : 2.69231
[1mStep[0m  [8/42], [94mLoss[0m : 2.58792
[1mStep[0m  [12/42], [94mLoss[0m : 2.27433
[1mStep[0m  [16/42], [94mLoss[0m : 2.64788
[1mStep[0m  [20/42], [94mLoss[0m : 2.70734
[1mStep[0m  [24/42], [94mLoss[0m : 2.45029
[1mStep[0m  [28/42], [94mLoss[0m : 2.61765
[1mStep[0m  [32/42], [94mLoss[0m : 2.63197
[1mStep[0m  [36/42], [94mLoss[0m : 2.58178
[1mStep[0m  [40/42], [94mLoss[0m : 2.56208

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.565, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53084
[1mStep[0m  [4/42], [94mLoss[0m : 2.70474
[1mStep[0m  [8/42], [94mLoss[0m : 2.57338
[1mStep[0m  [12/42], [94mLoss[0m : 2.61288
[1mStep[0m  [16/42], [94mLoss[0m : 2.34045
[1mStep[0m  [20/42], [94mLoss[0m : 2.31539
[1mStep[0m  [24/42], [94mLoss[0m : 2.56175
[1mStep[0m  [28/42], [94mLoss[0m : 2.43408
[1mStep[0m  [32/42], [94mLoss[0m : 2.65838
[1mStep[0m  [36/42], [94mLoss[0m : 2.56968
[1mStep[0m  [40/42], [94mLoss[0m : 2.52231

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77143
[1mStep[0m  [4/42], [94mLoss[0m : 2.51690
[1mStep[0m  [8/42], [94mLoss[0m : 2.50941
[1mStep[0m  [12/42], [94mLoss[0m : 2.44601
[1mStep[0m  [16/42], [94mLoss[0m : 2.35863
[1mStep[0m  [20/42], [94mLoss[0m : 2.31044
[1mStep[0m  [24/42], [94mLoss[0m : 2.57177
[1mStep[0m  [28/42], [94mLoss[0m : 2.80687
[1mStep[0m  [32/42], [94mLoss[0m : 2.37268
[1mStep[0m  [36/42], [94mLoss[0m : 2.47400
[1mStep[0m  [40/42], [94mLoss[0m : 2.45854

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71421
[1mStep[0m  [4/42], [94mLoss[0m : 2.32305
[1mStep[0m  [8/42], [94mLoss[0m : 2.45089
[1mStep[0m  [12/42], [94mLoss[0m : 2.46989
[1mStep[0m  [16/42], [94mLoss[0m : 2.40740
[1mStep[0m  [20/42], [94mLoss[0m : 2.41372
[1mStep[0m  [24/42], [94mLoss[0m : 2.48999
[1mStep[0m  [28/42], [94mLoss[0m : 2.53843
[1mStep[0m  [32/42], [94mLoss[0m : 2.54583
[1mStep[0m  [36/42], [94mLoss[0m : 2.49418
[1mStep[0m  [40/42], [94mLoss[0m : 2.37286

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.570, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43014
[1mStep[0m  [4/42], [94mLoss[0m : 2.50627
[1mStep[0m  [8/42], [94mLoss[0m : 2.56024
[1mStep[0m  [12/42], [94mLoss[0m : 2.58958
[1mStep[0m  [16/42], [94mLoss[0m : 2.47035
[1mStep[0m  [20/42], [94mLoss[0m : 2.35188
[1mStep[0m  [24/42], [94mLoss[0m : 2.59154
[1mStep[0m  [28/42], [94mLoss[0m : 2.35277
[1mStep[0m  [32/42], [94mLoss[0m : 2.37192
[1mStep[0m  [36/42], [94mLoss[0m : 2.35029
[1mStep[0m  [40/42], [94mLoss[0m : 2.57404

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33985
[1mStep[0m  [4/42], [94mLoss[0m : 2.58636
[1mStep[0m  [8/42], [94mLoss[0m : 2.70327
[1mStep[0m  [12/42], [94mLoss[0m : 2.56972
[1mStep[0m  [16/42], [94mLoss[0m : 2.49798
[1mStep[0m  [20/42], [94mLoss[0m : 2.43314
[1mStep[0m  [24/42], [94mLoss[0m : 2.38462
[1mStep[0m  [28/42], [94mLoss[0m : 2.36114
[1mStep[0m  [32/42], [94mLoss[0m : 2.57123
[1mStep[0m  [36/42], [94mLoss[0m : 2.63761
[1mStep[0m  [40/42], [94mLoss[0m : 2.71512

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.552, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43552
[1mStep[0m  [4/42], [94mLoss[0m : 2.51212
[1mStep[0m  [8/42], [94mLoss[0m : 2.42997
[1mStep[0m  [12/42], [94mLoss[0m : 2.38508
[1mStep[0m  [16/42], [94mLoss[0m : 2.26356
[1mStep[0m  [20/42], [94mLoss[0m : 2.57052
[1mStep[0m  [24/42], [94mLoss[0m : 2.69448
[1mStep[0m  [28/42], [94mLoss[0m : 2.65805
[1mStep[0m  [32/42], [94mLoss[0m : 2.75202
[1mStep[0m  [36/42], [94mLoss[0m : 2.37452
[1mStep[0m  [40/42], [94mLoss[0m : 2.54210

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.563
====================================

Phase 1 - Evaluation MAE:  2.5629040002822876
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.66751
[1mStep[0m  [4/42], [94mLoss[0m : 2.43258
[1mStep[0m  [8/42], [94mLoss[0m : 2.56171
[1mStep[0m  [12/42], [94mLoss[0m : 2.80669
[1mStep[0m  [16/42], [94mLoss[0m : 2.36649
[1mStep[0m  [20/42], [94mLoss[0m : 2.56267
[1mStep[0m  [24/42], [94mLoss[0m : 2.84837
[1mStep[0m  [28/42], [94mLoss[0m : 2.50949
[1mStep[0m  [32/42], [94mLoss[0m : 2.46141
[1mStep[0m  [36/42], [94mLoss[0m : 2.46433
[1mStep[0m  [40/42], [94mLoss[0m : 2.47472

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53916
[1mStep[0m  [4/42], [94mLoss[0m : 2.59791
[1mStep[0m  [8/42], [94mLoss[0m : 2.54819
[1mStep[0m  [12/42], [94mLoss[0m : 2.28575
[1mStep[0m  [16/42], [94mLoss[0m : 2.49831
[1mStep[0m  [20/42], [94mLoss[0m : 2.60041
[1mStep[0m  [24/42], [94mLoss[0m : 2.25072
[1mStep[0m  [28/42], [94mLoss[0m : 2.65498
[1mStep[0m  [32/42], [94mLoss[0m : 2.59334
[1mStep[0m  [36/42], [94mLoss[0m : 2.43091
[1mStep[0m  [40/42], [94mLoss[0m : 2.23357

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.555, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53424
[1mStep[0m  [4/42], [94mLoss[0m : 2.33201
[1mStep[0m  [8/42], [94mLoss[0m : 2.44750
[1mStep[0m  [12/42], [94mLoss[0m : 2.45269
[1mStep[0m  [16/42], [94mLoss[0m : 2.41543
[1mStep[0m  [20/42], [94mLoss[0m : 2.40748
[1mStep[0m  [24/42], [94mLoss[0m : 2.22078
[1mStep[0m  [28/42], [94mLoss[0m : 2.61172
[1mStep[0m  [32/42], [94mLoss[0m : 2.58392
[1mStep[0m  [36/42], [94mLoss[0m : 2.46780
[1mStep[0m  [40/42], [94mLoss[0m : 2.59022

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.451, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43746
[1mStep[0m  [4/42], [94mLoss[0m : 2.52481
[1mStep[0m  [8/42], [94mLoss[0m : 2.45353
[1mStep[0m  [12/42], [94mLoss[0m : 2.51792
[1mStep[0m  [16/42], [94mLoss[0m : 2.36394
[1mStep[0m  [20/42], [94mLoss[0m : 2.53456
[1mStep[0m  [24/42], [94mLoss[0m : 2.41915
[1mStep[0m  [28/42], [94mLoss[0m : 2.34720
[1mStep[0m  [32/42], [94mLoss[0m : 2.48776
[1mStep[0m  [36/42], [94mLoss[0m : 2.64361
[1mStep[0m  [40/42], [94mLoss[0m : 2.36009

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64398
[1mStep[0m  [4/42], [94mLoss[0m : 2.54576
[1mStep[0m  [8/42], [94mLoss[0m : 2.42837
[1mStep[0m  [12/42], [94mLoss[0m : 2.42621
[1mStep[0m  [16/42], [94mLoss[0m : 2.49312
[1mStep[0m  [20/42], [94mLoss[0m : 2.36485
[1mStep[0m  [24/42], [94mLoss[0m : 2.41812
[1mStep[0m  [28/42], [94mLoss[0m : 2.48142
[1mStep[0m  [32/42], [94mLoss[0m : 2.57742
[1mStep[0m  [36/42], [94mLoss[0m : 2.57991
[1mStep[0m  [40/42], [94mLoss[0m : 2.59122

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65083
[1mStep[0m  [4/42], [94mLoss[0m : 2.39806
[1mStep[0m  [8/42], [94mLoss[0m : 2.49107
[1mStep[0m  [12/42], [94mLoss[0m : 2.42090
[1mStep[0m  [16/42], [94mLoss[0m : 2.32850
[1mStep[0m  [20/42], [94mLoss[0m : 2.21352
[1mStep[0m  [24/42], [94mLoss[0m : 2.42025
[1mStep[0m  [28/42], [94mLoss[0m : 2.25319
[1mStep[0m  [32/42], [94mLoss[0m : 2.40055
[1mStep[0m  [36/42], [94mLoss[0m : 2.30384
[1mStep[0m  [40/42], [94mLoss[0m : 2.43126

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73972
[1mStep[0m  [4/42], [94mLoss[0m : 2.62963
[1mStep[0m  [8/42], [94mLoss[0m : 2.45064
[1mStep[0m  [12/42], [94mLoss[0m : 2.50972
[1mStep[0m  [16/42], [94mLoss[0m : 2.39067
[1mStep[0m  [20/42], [94mLoss[0m : 2.30758
[1mStep[0m  [24/42], [94mLoss[0m : 2.58047
[1mStep[0m  [28/42], [94mLoss[0m : 2.49773
[1mStep[0m  [32/42], [94mLoss[0m : 2.45499
[1mStep[0m  [36/42], [94mLoss[0m : 2.50762
[1mStep[0m  [40/42], [94mLoss[0m : 2.37319

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34554
[1mStep[0m  [4/42], [94mLoss[0m : 2.42030
[1mStep[0m  [8/42], [94mLoss[0m : 2.32839
[1mStep[0m  [12/42], [94mLoss[0m : 2.38551
[1mStep[0m  [16/42], [94mLoss[0m : 2.39319
[1mStep[0m  [20/42], [94mLoss[0m : 2.30367
[1mStep[0m  [24/42], [94mLoss[0m : 2.24293
[1mStep[0m  [28/42], [94mLoss[0m : 2.60491
[1mStep[0m  [32/42], [94mLoss[0m : 2.35981
[1mStep[0m  [36/42], [94mLoss[0m : 2.52544
[1mStep[0m  [40/42], [94mLoss[0m : 2.14496

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.567, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33820
[1mStep[0m  [4/42], [94mLoss[0m : 2.34209
[1mStep[0m  [8/42], [94mLoss[0m : 2.11028
[1mStep[0m  [12/42], [94mLoss[0m : 2.32094
[1mStep[0m  [16/42], [94mLoss[0m : 2.56990
[1mStep[0m  [20/42], [94mLoss[0m : 2.43226
[1mStep[0m  [24/42], [94mLoss[0m : 2.43116
[1mStep[0m  [28/42], [94mLoss[0m : 2.20501
[1mStep[0m  [32/42], [94mLoss[0m : 2.52185
[1mStep[0m  [36/42], [94mLoss[0m : 2.49075
[1mStep[0m  [40/42], [94mLoss[0m : 2.40906

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.517, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42586
[1mStep[0m  [4/42], [94mLoss[0m : 2.46734
[1mStep[0m  [8/42], [94mLoss[0m : 2.46359
[1mStep[0m  [12/42], [94mLoss[0m : 2.20845
[1mStep[0m  [16/42], [94mLoss[0m : 2.49492
[1mStep[0m  [20/42], [94mLoss[0m : 2.42758
[1mStep[0m  [24/42], [94mLoss[0m : 2.47406
[1mStep[0m  [28/42], [94mLoss[0m : 2.37613
[1mStep[0m  [32/42], [94mLoss[0m : 2.30637
[1mStep[0m  [36/42], [94mLoss[0m : 2.64033
[1mStep[0m  [40/42], [94mLoss[0m : 2.28637

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.633, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41184
[1mStep[0m  [4/42], [94mLoss[0m : 2.31380
[1mStep[0m  [8/42], [94mLoss[0m : 2.35187
[1mStep[0m  [12/42], [94mLoss[0m : 2.45090
[1mStep[0m  [16/42], [94mLoss[0m : 2.64776
[1mStep[0m  [20/42], [94mLoss[0m : 2.37567
[1mStep[0m  [24/42], [94mLoss[0m : 2.31136
[1mStep[0m  [28/42], [94mLoss[0m : 2.34637
[1mStep[0m  [32/42], [94mLoss[0m : 2.36001
[1mStep[0m  [36/42], [94mLoss[0m : 2.47287
[1mStep[0m  [40/42], [94mLoss[0m : 2.87839

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21795
[1mStep[0m  [4/42], [94mLoss[0m : 2.55859
[1mStep[0m  [8/42], [94mLoss[0m : 2.40665
[1mStep[0m  [12/42], [94mLoss[0m : 2.50512
[1mStep[0m  [16/42], [94mLoss[0m : 2.16908
[1mStep[0m  [20/42], [94mLoss[0m : 2.41198
[1mStep[0m  [24/42], [94mLoss[0m : 2.59104
[1mStep[0m  [28/42], [94mLoss[0m : 2.36172
[1mStep[0m  [32/42], [94mLoss[0m : 2.07386
[1mStep[0m  [36/42], [94mLoss[0m : 2.35365
[1mStep[0m  [40/42], [94mLoss[0m : 2.48594

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.523, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50102
[1mStep[0m  [4/42], [94mLoss[0m : 2.20834
[1mStep[0m  [8/42], [94mLoss[0m : 2.47206
[1mStep[0m  [12/42], [94mLoss[0m : 2.43581
[1mStep[0m  [16/42], [94mLoss[0m : 2.32087
[1mStep[0m  [20/42], [94mLoss[0m : 2.44090
[1mStep[0m  [24/42], [94mLoss[0m : 2.27336
[1mStep[0m  [28/42], [94mLoss[0m : 2.17054
[1mStep[0m  [32/42], [94mLoss[0m : 2.55420
[1mStep[0m  [36/42], [94mLoss[0m : 2.40605
[1mStep[0m  [40/42], [94mLoss[0m : 2.28195

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.502, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42806
[1mStep[0m  [4/42], [94mLoss[0m : 2.49691
[1mStep[0m  [8/42], [94mLoss[0m : 2.54050
[1mStep[0m  [12/42], [94mLoss[0m : 2.19840
[1mStep[0m  [16/42], [94mLoss[0m : 2.37360
[1mStep[0m  [20/42], [94mLoss[0m : 2.73717
[1mStep[0m  [24/42], [94mLoss[0m : 2.61504
[1mStep[0m  [28/42], [94mLoss[0m : 2.39457
[1mStep[0m  [32/42], [94mLoss[0m : 2.21534
[1mStep[0m  [36/42], [94mLoss[0m : 2.36115
[1mStep[0m  [40/42], [94mLoss[0m : 2.48726

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46948
[1mStep[0m  [4/42], [94mLoss[0m : 2.26583
[1mStep[0m  [8/42], [94mLoss[0m : 2.08036
[1mStep[0m  [12/42], [94mLoss[0m : 2.40161
[1mStep[0m  [16/42], [94mLoss[0m : 2.38643
[1mStep[0m  [20/42], [94mLoss[0m : 2.25691
[1mStep[0m  [24/42], [94mLoss[0m : 2.35437
[1mStep[0m  [28/42], [94mLoss[0m : 2.13011
[1mStep[0m  [32/42], [94mLoss[0m : 2.42536
[1mStep[0m  [36/42], [94mLoss[0m : 2.43285
[1mStep[0m  [40/42], [94mLoss[0m : 2.34929

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26146
[1mStep[0m  [4/42], [94mLoss[0m : 2.27798
[1mStep[0m  [8/42], [94mLoss[0m : 2.09043
[1mStep[0m  [12/42], [94mLoss[0m : 2.44930
[1mStep[0m  [16/42], [94mLoss[0m : 2.33309
[1mStep[0m  [20/42], [94mLoss[0m : 2.29928
[1mStep[0m  [24/42], [94mLoss[0m : 2.22940
[1mStep[0m  [28/42], [94mLoss[0m : 2.18946
[1mStep[0m  [32/42], [94mLoss[0m : 2.50663
[1mStep[0m  [36/42], [94mLoss[0m : 2.37598
[1mStep[0m  [40/42], [94mLoss[0m : 2.42744

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57870
[1mStep[0m  [4/42], [94mLoss[0m : 2.30045
[1mStep[0m  [8/42], [94mLoss[0m : 2.42479
[1mStep[0m  [12/42], [94mLoss[0m : 2.35445
[1mStep[0m  [16/42], [94mLoss[0m : 2.35660
[1mStep[0m  [20/42], [94mLoss[0m : 2.28571
[1mStep[0m  [24/42], [94mLoss[0m : 2.59403
[1mStep[0m  [28/42], [94mLoss[0m : 2.44553
[1mStep[0m  [32/42], [94mLoss[0m : 2.47092
[1mStep[0m  [36/42], [94mLoss[0m : 2.30909
[1mStep[0m  [40/42], [94mLoss[0m : 2.32888

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.485, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21783
[1mStep[0m  [4/42], [94mLoss[0m : 2.38274
[1mStep[0m  [8/42], [94mLoss[0m : 2.30187
[1mStep[0m  [12/42], [94mLoss[0m : 2.40433
[1mStep[0m  [16/42], [94mLoss[0m : 2.35518
[1mStep[0m  [20/42], [94mLoss[0m : 2.40942
[1mStep[0m  [24/42], [94mLoss[0m : 2.21340
[1mStep[0m  [28/42], [94mLoss[0m : 2.26420
[1mStep[0m  [32/42], [94mLoss[0m : 2.25271
[1mStep[0m  [36/42], [94mLoss[0m : 2.59878
[1mStep[0m  [40/42], [94mLoss[0m : 2.35806

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51680
[1mStep[0m  [4/42], [94mLoss[0m : 2.29193
[1mStep[0m  [8/42], [94mLoss[0m : 2.35082
[1mStep[0m  [12/42], [94mLoss[0m : 2.42543
[1mStep[0m  [16/42], [94mLoss[0m : 2.32986
[1mStep[0m  [20/42], [94mLoss[0m : 2.17879
[1mStep[0m  [24/42], [94mLoss[0m : 2.29777
[1mStep[0m  [28/42], [94mLoss[0m : 2.36061
[1mStep[0m  [32/42], [94mLoss[0m : 2.44588
[1mStep[0m  [36/42], [94mLoss[0m : 2.33401
[1mStep[0m  [40/42], [94mLoss[0m : 2.22541

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34033
[1mStep[0m  [4/42], [94mLoss[0m : 2.43761
[1mStep[0m  [8/42], [94mLoss[0m : 2.16138
[1mStep[0m  [12/42], [94mLoss[0m : 2.36036
[1mStep[0m  [16/42], [94mLoss[0m : 2.26167
[1mStep[0m  [20/42], [94mLoss[0m : 2.32547
[1mStep[0m  [24/42], [94mLoss[0m : 2.26238
[1mStep[0m  [28/42], [94mLoss[0m : 2.40773
[1mStep[0m  [32/42], [94mLoss[0m : 2.39682
[1mStep[0m  [36/42], [94mLoss[0m : 2.56959
[1mStep[0m  [40/42], [94mLoss[0m : 2.25195

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.489, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39595
[1mStep[0m  [4/42], [94mLoss[0m : 2.25119
[1mStep[0m  [8/42], [94mLoss[0m : 2.29747
[1mStep[0m  [12/42], [94mLoss[0m : 2.36847
[1mStep[0m  [16/42], [94mLoss[0m : 2.37046
[1mStep[0m  [20/42], [94mLoss[0m : 2.38394
[1mStep[0m  [24/42], [94mLoss[0m : 2.41375
[1mStep[0m  [28/42], [94mLoss[0m : 2.12364
[1mStep[0m  [32/42], [94mLoss[0m : 2.39473
[1mStep[0m  [36/42], [94mLoss[0m : 2.26445
[1mStep[0m  [40/42], [94mLoss[0m : 2.21860

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.482, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34174
[1mStep[0m  [4/42], [94mLoss[0m : 2.35999
[1mStep[0m  [8/42], [94mLoss[0m : 2.31359
[1mStep[0m  [12/42], [94mLoss[0m : 2.28139
[1mStep[0m  [16/42], [94mLoss[0m : 2.31491
[1mStep[0m  [20/42], [94mLoss[0m : 2.33036
[1mStep[0m  [24/42], [94mLoss[0m : 2.27920
[1mStep[0m  [28/42], [94mLoss[0m : 2.46624
[1mStep[0m  [32/42], [94mLoss[0m : 2.36494
[1mStep[0m  [36/42], [94mLoss[0m : 2.21914
[1mStep[0m  [40/42], [94mLoss[0m : 2.31474

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25032
[1mStep[0m  [4/42], [94mLoss[0m : 2.46000
[1mStep[0m  [8/42], [94mLoss[0m : 2.31263
[1mStep[0m  [12/42], [94mLoss[0m : 2.32636
[1mStep[0m  [16/42], [94mLoss[0m : 2.28425
[1mStep[0m  [20/42], [94mLoss[0m : 2.36737
[1mStep[0m  [24/42], [94mLoss[0m : 2.40721
[1mStep[0m  [28/42], [94mLoss[0m : 2.46590
[1mStep[0m  [32/42], [94mLoss[0m : 2.41841
[1mStep[0m  [36/42], [94mLoss[0m : 2.33938
[1mStep[0m  [40/42], [94mLoss[0m : 2.42542

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39807
[1mStep[0m  [4/42], [94mLoss[0m : 2.40700
[1mStep[0m  [8/42], [94mLoss[0m : 2.48998
[1mStep[0m  [12/42], [94mLoss[0m : 2.52627
[1mStep[0m  [16/42], [94mLoss[0m : 2.24659
[1mStep[0m  [20/42], [94mLoss[0m : 2.39835
[1mStep[0m  [24/42], [94mLoss[0m : 2.11966
[1mStep[0m  [28/42], [94mLoss[0m : 2.38888
[1mStep[0m  [32/42], [94mLoss[0m : 2.35870
[1mStep[0m  [36/42], [94mLoss[0m : 2.32353
[1mStep[0m  [40/42], [94mLoss[0m : 2.38436

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23394
[1mStep[0m  [4/42], [94mLoss[0m : 2.43812
[1mStep[0m  [8/42], [94mLoss[0m : 2.28684
[1mStep[0m  [12/42], [94mLoss[0m : 2.66727
[1mStep[0m  [16/42], [94mLoss[0m : 2.34039
[1mStep[0m  [20/42], [94mLoss[0m : 2.29115
[1mStep[0m  [24/42], [94mLoss[0m : 2.17729
[1mStep[0m  [28/42], [94mLoss[0m : 2.40277
[1mStep[0m  [32/42], [94mLoss[0m : 2.41140
[1mStep[0m  [36/42], [94mLoss[0m : 2.16933
[1mStep[0m  [40/42], [94mLoss[0m : 2.37762

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44021
[1mStep[0m  [4/42], [94mLoss[0m : 2.32999
[1mStep[0m  [8/42], [94mLoss[0m : 2.47378
[1mStep[0m  [12/42], [94mLoss[0m : 2.41937
[1mStep[0m  [16/42], [94mLoss[0m : 2.30492
[1mStep[0m  [20/42], [94mLoss[0m : 2.26629
[1mStep[0m  [24/42], [94mLoss[0m : 2.14990
[1mStep[0m  [28/42], [94mLoss[0m : 2.31951
[1mStep[0m  [32/42], [94mLoss[0m : 2.64888
[1mStep[0m  [36/42], [94mLoss[0m : 2.42203
[1mStep[0m  [40/42], [94mLoss[0m : 2.01563

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17117
[1mStep[0m  [4/42], [94mLoss[0m : 2.22795
[1mStep[0m  [8/42], [94mLoss[0m : 2.46834
[1mStep[0m  [12/42], [94mLoss[0m : 2.35851
[1mStep[0m  [16/42], [94mLoss[0m : 2.41432
[1mStep[0m  [20/42], [94mLoss[0m : 2.36065
[1mStep[0m  [24/42], [94mLoss[0m : 2.28887
[1mStep[0m  [28/42], [94mLoss[0m : 2.36316
[1mStep[0m  [32/42], [94mLoss[0m : 2.21954
[1mStep[0m  [36/42], [94mLoss[0m : 2.40827
[1mStep[0m  [40/42], [94mLoss[0m : 2.25767

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.469, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14584
[1mStep[0m  [4/42], [94mLoss[0m : 2.16367
[1mStep[0m  [8/42], [94mLoss[0m : 2.15780
[1mStep[0m  [12/42], [94mLoss[0m : 2.28175
[1mStep[0m  [16/42], [94mLoss[0m : 2.21447
[1mStep[0m  [20/42], [94mLoss[0m : 2.22861
[1mStep[0m  [24/42], [94mLoss[0m : 2.22470
[1mStep[0m  [28/42], [94mLoss[0m : 2.13185
[1mStep[0m  [32/42], [94mLoss[0m : 2.36592
[1mStep[0m  [36/42], [94mLoss[0m : 2.20930
[1mStep[0m  [40/42], [94mLoss[0m : 2.39352

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.449, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21843
[1mStep[0m  [4/42], [94mLoss[0m : 2.27276
[1mStep[0m  [8/42], [94mLoss[0m : 2.59944
[1mStep[0m  [12/42], [94mLoss[0m : 2.23775
[1mStep[0m  [16/42], [94mLoss[0m : 2.07926
[1mStep[0m  [20/42], [94mLoss[0m : 2.27403
[1mStep[0m  [24/42], [94mLoss[0m : 2.30214
[1mStep[0m  [28/42], [94mLoss[0m : 2.22649
[1mStep[0m  [32/42], [94mLoss[0m : 2.20901
[1mStep[0m  [36/42], [94mLoss[0m : 2.24574
[1mStep[0m  [40/42], [94mLoss[0m : 2.15938

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.451, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13694
[1mStep[0m  [4/42], [94mLoss[0m : 2.18709
[1mStep[0m  [8/42], [94mLoss[0m : 2.39831
[1mStep[0m  [12/42], [94mLoss[0m : 2.02223
[1mStep[0m  [16/42], [94mLoss[0m : 2.30182
[1mStep[0m  [20/42], [94mLoss[0m : 2.34419
[1mStep[0m  [24/42], [94mLoss[0m : 2.16679
[1mStep[0m  [28/42], [94mLoss[0m : 2.29199
[1mStep[0m  [32/42], [94mLoss[0m : 1.90611
[1mStep[0m  [36/42], [94mLoss[0m : 2.33999
[1mStep[0m  [40/42], [94mLoss[0m : 2.45285

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.418
====================================

Phase 2 - Evaluation MAE:  2.417530723980495
MAE score P1      2.562904
MAE score P2      2.417531
loss              2.267877
learning_rate       0.0001
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 8, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.80951
[1mStep[0m  [2/21], [94mLoss[0m : 10.93556
[1mStep[0m  [4/21], [94mLoss[0m : 10.74454
[1mStep[0m  [6/21], [94mLoss[0m : 10.90201
[1mStep[0m  [8/21], [94mLoss[0m : 10.87796
[1mStep[0m  [10/21], [94mLoss[0m : 10.81299
[1mStep[0m  [12/21], [94mLoss[0m : 11.08576
[1mStep[0m  [14/21], [94mLoss[0m : 10.80932
[1mStep[0m  [16/21], [94mLoss[0m : 10.85832
[1mStep[0m  [18/21], [94mLoss[0m : 10.82970
[1mStep[0m  [20/21], [94mLoss[0m : 10.96139

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.903, [92mTest[0m: 10.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79074
[1mStep[0m  [2/21], [94mLoss[0m : 10.85137
[1mStep[0m  [4/21], [94mLoss[0m : 10.83388
[1mStep[0m  [6/21], [94mLoss[0m : 11.00640
[1mStep[0m  [8/21], [94mLoss[0m : 10.61938
[1mStep[0m  [10/21], [94mLoss[0m : 10.81643
[1mStep[0m  [12/21], [94mLoss[0m : 11.11787
[1mStep[0m  [14/21], [94mLoss[0m : 10.76423
[1mStep[0m  [16/21], [94mLoss[0m : 10.92006
[1mStep[0m  [18/21], [94mLoss[0m : 10.85557
[1mStep[0m  [20/21], [94mLoss[0m : 11.12740

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.922, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65120
[1mStep[0m  [2/21], [94mLoss[0m : 11.03664
[1mStep[0m  [4/21], [94mLoss[0m : 11.14472
[1mStep[0m  [6/21], [94mLoss[0m : 11.03809
[1mStep[0m  [8/21], [94mLoss[0m : 10.72875
[1mStep[0m  [10/21], [94mLoss[0m : 10.86415
[1mStep[0m  [12/21], [94mLoss[0m : 11.14936
[1mStep[0m  [14/21], [94mLoss[0m : 10.86957
[1mStep[0m  [16/21], [94mLoss[0m : 10.98267
[1mStep[0m  [18/21], [94mLoss[0m : 10.65443
[1mStep[0m  [20/21], [94mLoss[0m : 10.95384

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.886, [92mTest[0m: 10.908, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93341
[1mStep[0m  [2/21], [94mLoss[0m : 10.91423
[1mStep[0m  [4/21], [94mLoss[0m : 10.67571
[1mStep[0m  [6/21], [94mLoss[0m : 11.11070
[1mStep[0m  [8/21], [94mLoss[0m : 10.88339
[1mStep[0m  [10/21], [94mLoss[0m : 10.87459
[1mStep[0m  [12/21], [94mLoss[0m : 10.86917
[1mStep[0m  [14/21], [94mLoss[0m : 10.81484
[1mStep[0m  [16/21], [94mLoss[0m : 11.06024
[1mStep[0m  [18/21], [94mLoss[0m : 10.69322
[1mStep[0m  [20/21], [94mLoss[0m : 10.58666

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.873, [92mTest[0m: 10.898, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.03311
[1mStep[0m  [2/21], [94mLoss[0m : 10.99310
[1mStep[0m  [4/21], [94mLoss[0m : 10.74670
[1mStep[0m  [6/21], [94mLoss[0m : 11.14080
[1mStep[0m  [8/21], [94mLoss[0m : 10.75738
[1mStep[0m  [10/21], [94mLoss[0m : 10.79356
[1mStep[0m  [12/21], [94mLoss[0m : 10.91016
[1mStep[0m  [14/21], [94mLoss[0m : 10.67193
[1mStep[0m  [16/21], [94mLoss[0m : 10.73530
[1mStep[0m  [18/21], [94mLoss[0m : 10.76888
[1mStep[0m  [20/21], [94mLoss[0m : 10.81094

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.870, [92mTest[0m: 10.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.87527
[1mStep[0m  [2/21], [94mLoss[0m : 10.75453
[1mStep[0m  [4/21], [94mLoss[0m : 10.62049
[1mStep[0m  [6/21], [94mLoss[0m : 10.92424
[1mStep[0m  [8/21], [94mLoss[0m : 10.93293
[1mStep[0m  [10/21], [94mLoss[0m : 10.77154
[1mStep[0m  [12/21], [94mLoss[0m : 10.78378
[1mStep[0m  [14/21], [94mLoss[0m : 11.08602
[1mStep[0m  [16/21], [94mLoss[0m : 10.60857
[1mStep[0m  [18/21], [94mLoss[0m : 10.96676
[1mStep[0m  [20/21], [94mLoss[0m : 10.85204

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96774
[1mStep[0m  [2/21], [94mLoss[0m : 10.68416
[1mStep[0m  [4/21], [94mLoss[0m : 10.87214
[1mStep[0m  [6/21], [94mLoss[0m : 10.81534
[1mStep[0m  [8/21], [94mLoss[0m : 10.57739
[1mStep[0m  [10/21], [94mLoss[0m : 10.86902
[1mStep[0m  [12/21], [94mLoss[0m : 10.93585
[1mStep[0m  [14/21], [94mLoss[0m : 10.99964
[1mStep[0m  [16/21], [94mLoss[0m : 10.78572
[1mStep[0m  [18/21], [94mLoss[0m : 11.16607
[1mStep[0m  [20/21], [94mLoss[0m : 10.82440

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.855, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75355
[1mStep[0m  [2/21], [94mLoss[0m : 11.04253
[1mStep[0m  [4/21], [94mLoss[0m : 10.92780
[1mStep[0m  [6/21], [94mLoss[0m : 10.66062
[1mStep[0m  [8/21], [94mLoss[0m : 10.53903
[1mStep[0m  [10/21], [94mLoss[0m : 10.89272
[1mStep[0m  [12/21], [94mLoss[0m : 10.82638
[1mStep[0m  [14/21], [94mLoss[0m : 10.82084
[1mStep[0m  [16/21], [94mLoss[0m : 10.51553
[1mStep[0m  [18/21], [94mLoss[0m : 10.93546
[1mStep[0m  [20/21], [94mLoss[0m : 10.84105

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.842, [92mTest[0m: 10.850, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90470
[1mStep[0m  [2/21], [94mLoss[0m : 10.72355
[1mStep[0m  [4/21], [94mLoss[0m : 11.06102
[1mStep[0m  [6/21], [94mLoss[0m : 10.97968
[1mStep[0m  [8/21], [94mLoss[0m : 10.61411
[1mStep[0m  [10/21], [94mLoss[0m : 10.73932
[1mStep[0m  [12/21], [94mLoss[0m : 10.92674
[1mStep[0m  [14/21], [94mLoss[0m : 11.11120
[1mStep[0m  [16/21], [94mLoss[0m : 10.58200
[1mStep[0m  [18/21], [94mLoss[0m : 10.78848
[1mStep[0m  [20/21], [94mLoss[0m : 10.96755

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.838, [92mTest[0m: 10.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90626
[1mStep[0m  [2/21], [94mLoss[0m : 10.82613
[1mStep[0m  [4/21], [94mLoss[0m : 10.62813
[1mStep[0m  [6/21], [94mLoss[0m : 10.86747
[1mStep[0m  [8/21], [94mLoss[0m : 10.89399
[1mStep[0m  [10/21], [94mLoss[0m : 10.68798
[1mStep[0m  [12/21], [94mLoss[0m : 10.89074
[1mStep[0m  [14/21], [94mLoss[0m : 10.63421
[1mStep[0m  [16/21], [94mLoss[0m : 11.22266
[1mStep[0m  [18/21], [94mLoss[0m : 10.73687
[1mStep[0m  [20/21], [94mLoss[0m : 10.87039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.838, [92mTest[0m: 10.827, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.91713
[1mStep[0m  [2/21], [94mLoss[0m : 10.77353
[1mStep[0m  [4/21], [94mLoss[0m : 10.95818
[1mStep[0m  [6/21], [94mLoss[0m : 10.80936
[1mStep[0m  [8/21], [94mLoss[0m : 10.82084
[1mStep[0m  [10/21], [94mLoss[0m : 10.99556
[1mStep[0m  [12/21], [94mLoss[0m : 11.02540
[1mStep[0m  [14/21], [94mLoss[0m : 10.93323
[1mStep[0m  [16/21], [94mLoss[0m : 10.81505
[1mStep[0m  [18/21], [94mLoss[0m : 10.93637
[1mStep[0m  [20/21], [94mLoss[0m : 10.55937

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77357
[1mStep[0m  [2/21], [94mLoss[0m : 10.92934
[1mStep[0m  [4/21], [94mLoss[0m : 10.89673
[1mStep[0m  [6/21], [94mLoss[0m : 10.92359
[1mStep[0m  [8/21], [94mLoss[0m : 10.80156
[1mStep[0m  [10/21], [94mLoss[0m : 10.83067
[1mStep[0m  [12/21], [94mLoss[0m : 10.66759
[1mStep[0m  [14/21], [94mLoss[0m : 10.83468
[1mStep[0m  [16/21], [94mLoss[0m : 10.66477
[1mStep[0m  [18/21], [94mLoss[0m : 10.63996
[1mStep[0m  [20/21], [94mLoss[0m : 10.64377

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.813, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55591
[1mStep[0m  [2/21], [94mLoss[0m : 10.83618
[1mStep[0m  [4/21], [94mLoss[0m : 10.87293
[1mStep[0m  [6/21], [94mLoss[0m : 10.82271
[1mStep[0m  [8/21], [94mLoss[0m : 11.05472
[1mStep[0m  [10/21], [94mLoss[0m : 10.81845
[1mStep[0m  [12/21], [94mLoss[0m : 10.90735
[1mStep[0m  [14/21], [94mLoss[0m : 10.76456
[1mStep[0m  [16/21], [94mLoss[0m : 10.78837
[1mStep[0m  [18/21], [94mLoss[0m : 10.85934
[1mStep[0m  [20/21], [94mLoss[0m : 11.00858

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.811, [92mTest[0m: 10.805, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69408
[1mStep[0m  [2/21], [94mLoss[0m : 10.87434
[1mStep[0m  [4/21], [94mLoss[0m : 10.70594
[1mStep[0m  [6/21], [94mLoss[0m : 10.93237
[1mStep[0m  [8/21], [94mLoss[0m : 10.85981
[1mStep[0m  [10/21], [94mLoss[0m : 10.42479
[1mStep[0m  [12/21], [94mLoss[0m : 10.75381
[1mStep[0m  [14/21], [94mLoss[0m : 10.93600
[1mStep[0m  [16/21], [94mLoss[0m : 10.75181
[1mStep[0m  [18/21], [94mLoss[0m : 10.78079
[1mStep[0m  [20/21], [94mLoss[0m : 10.82646

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.804, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58828
[1mStep[0m  [2/21], [94mLoss[0m : 10.85374
[1mStep[0m  [4/21], [94mLoss[0m : 10.60693
[1mStep[0m  [6/21], [94mLoss[0m : 10.82623
[1mStep[0m  [8/21], [94mLoss[0m : 10.75470
[1mStep[0m  [10/21], [94mLoss[0m : 11.04156
[1mStep[0m  [12/21], [94mLoss[0m : 10.99791
[1mStep[0m  [14/21], [94mLoss[0m : 10.80465
[1mStep[0m  [16/21], [94mLoss[0m : 10.80853
[1mStep[0m  [18/21], [94mLoss[0m : 11.03279
[1mStep[0m  [20/21], [94mLoss[0m : 10.69119

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74767
[1mStep[0m  [2/21], [94mLoss[0m : 11.06818
[1mStep[0m  [4/21], [94mLoss[0m : 10.48548
[1mStep[0m  [6/21], [94mLoss[0m : 10.86822
[1mStep[0m  [8/21], [94mLoss[0m : 10.66889
[1mStep[0m  [10/21], [94mLoss[0m : 10.75305
[1mStep[0m  [12/21], [94mLoss[0m : 11.05798
[1mStep[0m  [14/21], [94mLoss[0m : 10.92886
[1mStep[0m  [16/21], [94mLoss[0m : 10.85568
[1mStep[0m  [18/21], [94mLoss[0m : 10.78411
[1mStep[0m  [20/21], [94mLoss[0m : 10.55302

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.777, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86808
[1mStep[0m  [2/21], [94mLoss[0m : 10.65281
[1mStep[0m  [4/21], [94mLoss[0m : 10.54592
[1mStep[0m  [6/21], [94mLoss[0m : 10.87262
[1mStep[0m  [8/21], [94mLoss[0m : 11.03928
[1mStep[0m  [10/21], [94mLoss[0m : 10.82289
[1mStep[0m  [12/21], [94mLoss[0m : 10.68886
[1mStep[0m  [14/21], [94mLoss[0m : 10.63922
[1mStep[0m  [16/21], [94mLoss[0m : 10.82266
[1mStep[0m  [18/21], [94mLoss[0m : 10.91739
[1mStep[0m  [20/21], [94mLoss[0m : 10.84805

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.783, [92mTest[0m: 10.769, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74057
[1mStep[0m  [2/21], [94mLoss[0m : 10.76848
[1mStep[0m  [4/21], [94mLoss[0m : 10.95999
[1mStep[0m  [6/21], [94mLoss[0m : 10.71218
[1mStep[0m  [8/21], [94mLoss[0m : 10.75697
[1mStep[0m  [10/21], [94mLoss[0m : 10.90398
[1mStep[0m  [12/21], [94mLoss[0m : 10.79383
[1mStep[0m  [14/21], [94mLoss[0m : 10.68849
[1mStep[0m  [16/21], [94mLoss[0m : 10.48373
[1mStep[0m  [18/21], [94mLoss[0m : 10.86101
[1mStep[0m  [20/21], [94mLoss[0m : 10.87163

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.780, [92mTest[0m: 10.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75112
[1mStep[0m  [2/21], [94mLoss[0m : 10.76866
[1mStep[0m  [4/21], [94mLoss[0m : 10.95742
[1mStep[0m  [6/21], [94mLoss[0m : 10.62471
[1mStep[0m  [8/21], [94mLoss[0m : 10.84258
[1mStep[0m  [10/21], [94mLoss[0m : 10.66597
[1mStep[0m  [12/21], [94mLoss[0m : 10.52934
[1mStep[0m  [14/21], [94mLoss[0m : 10.68753
[1mStep[0m  [16/21], [94mLoss[0m : 10.69459
[1mStep[0m  [18/21], [94mLoss[0m : 10.71222
[1mStep[0m  [20/21], [94mLoss[0m : 10.79453

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.746, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81669
[1mStep[0m  [2/21], [94mLoss[0m : 10.88218
[1mStep[0m  [4/21], [94mLoss[0m : 10.61536
[1mStep[0m  [6/21], [94mLoss[0m : 10.47771
[1mStep[0m  [8/21], [94mLoss[0m : 10.66098
[1mStep[0m  [10/21], [94mLoss[0m : 10.97038
[1mStep[0m  [12/21], [94mLoss[0m : 10.76425
[1mStep[0m  [14/21], [94mLoss[0m : 10.58396
[1mStep[0m  [16/21], [94mLoss[0m : 11.10900
[1mStep[0m  [18/21], [94mLoss[0m : 10.74361
[1mStep[0m  [20/21], [94mLoss[0m : 10.69842

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.767, [92mTest[0m: 10.737, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67681
[1mStep[0m  [2/21], [94mLoss[0m : 11.17911
[1mStep[0m  [4/21], [94mLoss[0m : 10.79730
[1mStep[0m  [6/21], [94mLoss[0m : 10.71704
[1mStep[0m  [8/21], [94mLoss[0m : 10.51148
[1mStep[0m  [10/21], [94mLoss[0m : 10.72665
[1mStep[0m  [12/21], [94mLoss[0m : 10.82853
[1mStep[0m  [14/21], [94mLoss[0m : 10.52201
[1mStep[0m  [16/21], [94mLoss[0m : 10.74387
[1mStep[0m  [18/21], [94mLoss[0m : 10.41651
[1mStep[0m  [20/21], [94mLoss[0m : 10.58045

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.719, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56328
[1mStep[0m  [2/21], [94mLoss[0m : 10.67247
[1mStep[0m  [4/21], [94mLoss[0m : 10.67606
[1mStep[0m  [6/21], [94mLoss[0m : 10.80175
[1mStep[0m  [8/21], [94mLoss[0m : 10.67872
[1mStep[0m  [10/21], [94mLoss[0m : 10.73508
[1mStep[0m  [12/21], [94mLoss[0m : 10.74883
[1mStep[0m  [14/21], [94mLoss[0m : 10.84823
[1mStep[0m  [16/21], [94mLoss[0m : 10.70359
[1mStep[0m  [18/21], [94mLoss[0m : 11.11427
[1mStep[0m  [20/21], [94mLoss[0m : 10.71480

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.750, [92mTest[0m: 10.708, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69525
[1mStep[0m  [2/21], [94mLoss[0m : 10.68474
[1mStep[0m  [4/21], [94mLoss[0m : 10.72188
[1mStep[0m  [6/21], [94mLoss[0m : 11.02122
[1mStep[0m  [8/21], [94mLoss[0m : 10.57261
[1mStep[0m  [10/21], [94mLoss[0m : 10.74373
[1mStep[0m  [12/21], [94mLoss[0m : 10.80364
[1mStep[0m  [14/21], [94mLoss[0m : 10.61881
[1mStep[0m  [16/21], [94mLoss[0m : 11.10223
[1mStep[0m  [18/21], [94mLoss[0m : 10.84302
[1mStep[0m  [20/21], [94mLoss[0m : 10.77047

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.712, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68057
[1mStep[0m  [2/21], [94mLoss[0m : 11.25726
[1mStep[0m  [4/21], [94mLoss[0m : 10.72197
[1mStep[0m  [6/21], [94mLoss[0m : 10.83076
[1mStep[0m  [8/21], [94mLoss[0m : 10.79052
[1mStep[0m  [10/21], [94mLoss[0m : 10.71570
[1mStep[0m  [12/21], [94mLoss[0m : 10.44091
[1mStep[0m  [14/21], [94mLoss[0m : 10.90951
[1mStep[0m  [16/21], [94mLoss[0m : 10.66742
[1mStep[0m  [18/21], [94mLoss[0m : 10.59260
[1mStep[0m  [20/21], [94mLoss[0m : 10.91739

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.732, [92mTest[0m: 10.709, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73024
[1mStep[0m  [2/21], [94mLoss[0m : 10.79910
[1mStep[0m  [4/21], [94mLoss[0m : 10.65282
[1mStep[0m  [6/21], [94mLoss[0m : 10.75317
[1mStep[0m  [8/21], [94mLoss[0m : 10.68521
[1mStep[0m  [10/21], [94mLoss[0m : 10.69833
[1mStep[0m  [12/21], [94mLoss[0m : 10.62023
[1mStep[0m  [14/21], [94mLoss[0m : 10.50190
[1mStep[0m  [16/21], [94mLoss[0m : 10.82884
[1mStep[0m  [18/21], [94mLoss[0m : 10.70819
[1mStep[0m  [20/21], [94mLoss[0m : 10.69544

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.728, [92mTest[0m: 10.697, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73699
[1mStep[0m  [2/21], [94mLoss[0m : 10.86311
[1mStep[0m  [4/21], [94mLoss[0m : 10.57451
[1mStep[0m  [6/21], [94mLoss[0m : 10.59746
[1mStep[0m  [8/21], [94mLoss[0m : 10.69580
[1mStep[0m  [10/21], [94mLoss[0m : 10.55293
[1mStep[0m  [12/21], [94mLoss[0m : 10.92845
[1mStep[0m  [14/21], [94mLoss[0m : 10.59408
[1mStep[0m  [16/21], [94mLoss[0m : 10.64338
[1mStep[0m  [18/21], [94mLoss[0m : 10.57262
[1mStep[0m  [20/21], [94mLoss[0m : 10.57897

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.678, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81964
[1mStep[0m  [2/21], [94mLoss[0m : 10.72407
[1mStep[0m  [4/21], [94mLoss[0m : 10.71523
[1mStep[0m  [6/21], [94mLoss[0m : 10.71622
[1mStep[0m  [8/21], [94mLoss[0m : 10.76047
[1mStep[0m  [10/21], [94mLoss[0m : 10.98935
[1mStep[0m  [12/21], [94mLoss[0m : 10.81905
[1mStep[0m  [14/21], [94mLoss[0m : 10.92319
[1mStep[0m  [16/21], [94mLoss[0m : 10.49716
[1mStep[0m  [18/21], [94mLoss[0m : 10.89301
[1mStep[0m  [20/21], [94mLoss[0m : 10.74327

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.727, [92mTest[0m: 10.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78971
[1mStep[0m  [2/21], [94mLoss[0m : 10.45995
[1mStep[0m  [4/21], [94mLoss[0m : 10.62553
[1mStep[0m  [6/21], [94mLoss[0m : 10.84349
[1mStep[0m  [8/21], [94mLoss[0m : 10.59153
[1mStep[0m  [10/21], [94mLoss[0m : 10.85365
[1mStep[0m  [12/21], [94mLoss[0m : 10.74578
[1mStep[0m  [14/21], [94mLoss[0m : 10.68283
[1mStep[0m  [16/21], [94mLoss[0m : 10.81286
[1mStep[0m  [18/21], [94mLoss[0m : 10.77304
[1mStep[0m  [20/21], [94mLoss[0m : 10.59371

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.711, [92mTest[0m: 10.652, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67524
[1mStep[0m  [2/21], [94mLoss[0m : 10.65502
[1mStep[0m  [4/21], [94mLoss[0m : 10.63638
[1mStep[0m  [6/21], [94mLoss[0m : 10.77513
[1mStep[0m  [8/21], [94mLoss[0m : 10.91203
[1mStep[0m  [10/21], [94mLoss[0m : 10.81227
[1mStep[0m  [12/21], [94mLoss[0m : 10.68355
[1mStep[0m  [14/21], [94mLoss[0m : 10.68142
[1mStep[0m  [16/21], [94mLoss[0m : 10.69327
[1mStep[0m  [18/21], [94mLoss[0m : 10.70537
[1mStep[0m  [20/21], [94mLoss[0m : 10.82628

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.707, [92mTest[0m: 10.660, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71418
[1mStep[0m  [2/21], [94mLoss[0m : 10.55758
[1mStep[0m  [4/21], [94mLoss[0m : 10.75594
[1mStep[0m  [6/21], [94mLoss[0m : 10.39764
[1mStep[0m  [8/21], [94mLoss[0m : 10.76992
[1mStep[0m  [10/21], [94mLoss[0m : 10.87375
[1mStep[0m  [12/21], [94mLoss[0m : 10.71930
[1mStep[0m  [14/21], [94mLoss[0m : 10.82996
[1mStep[0m  [16/21], [94mLoss[0m : 10.68283
[1mStep[0m  [18/21], [94mLoss[0m : 10.52223
[1mStep[0m  [20/21], [94mLoss[0m : 11.06259

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.706, [92mTest[0m: 10.648, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.636
====================================

Phase 1 - Evaluation MAE:  10.635661806379046
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.81327
[1mStep[0m  [2/21], [94mLoss[0m : 10.92544
[1mStep[0m  [4/21], [94mLoss[0m : 10.53013
[1mStep[0m  [6/21], [94mLoss[0m : 10.77717
[1mStep[0m  [8/21], [94mLoss[0m : 10.83082
[1mStep[0m  [10/21], [94mLoss[0m : 10.58713
[1mStep[0m  [12/21], [94mLoss[0m : 10.73034
[1mStep[0m  [14/21], [94mLoss[0m : 10.76851
[1mStep[0m  [16/21], [94mLoss[0m : 10.54251
[1mStep[0m  [18/21], [94mLoss[0m : 10.81055
[1mStep[0m  [20/21], [94mLoss[0m : 10.64855

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.696, [92mTest[0m: 10.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.76252
[1mStep[0m  [2/21], [94mLoss[0m : 10.65019
[1mStep[0m  [4/21], [94mLoss[0m : 10.56155
[1mStep[0m  [6/21], [94mLoss[0m : 10.75805
[1mStep[0m  [8/21], [94mLoss[0m : 10.65858
[1mStep[0m  [10/21], [94mLoss[0m : 10.61447
[1mStep[0m  [12/21], [94mLoss[0m : 10.55252
[1mStep[0m  [14/21], [94mLoss[0m : 10.91699
[1mStep[0m  [16/21], [94mLoss[0m : 10.59046
[1mStep[0m  [18/21], [94mLoss[0m : 10.77382
[1mStep[0m  [20/21], [94mLoss[0m : 10.48587

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.684, [92mTest[0m: 10.623, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.94647
[1mStep[0m  [2/21], [94mLoss[0m : 10.52740
[1mStep[0m  [4/21], [94mLoss[0m : 10.77537
[1mStep[0m  [6/21], [94mLoss[0m : 10.83864
[1mStep[0m  [8/21], [94mLoss[0m : 10.80533
[1mStep[0m  [10/21], [94mLoss[0m : 10.65294
[1mStep[0m  [12/21], [94mLoss[0m : 10.78097
[1mStep[0m  [14/21], [94mLoss[0m : 10.55079
[1mStep[0m  [16/21], [94mLoss[0m : 10.75548
[1mStep[0m  [18/21], [94mLoss[0m : 10.76398
[1mStep[0m  [20/21], [94mLoss[0m : 10.74551

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.678, [92mTest[0m: 10.605, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42130
[1mStep[0m  [2/21], [94mLoss[0m : 11.00578
[1mStep[0m  [4/21], [94mLoss[0m : 10.58809
[1mStep[0m  [6/21], [94mLoss[0m : 10.85908
[1mStep[0m  [8/21], [94mLoss[0m : 10.48028
[1mStep[0m  [10/21], [94mLoss[0m : 10.81304
[1mStep[0m  [12/21], [94mLoss[0m : 10.80764
[1mStep[0m  [14/21], [94mLoss[0m : 10.58466
[1mStep[0m  [16/21], [94mLoss[0m : 10.56182
[1mStep[0m  [18/21], [94mLoss[0m : 10.50839
[1mStep[0m  [20/21], [94mLoss[0m : 10.64664

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60887
[1mStep[0m  [2/21], [94mLoss[0m : 10.70379
[1mStep[0m  [4/21], [94mLoss[0m : 10.80404
[1mStep[0m  [6/21], [94mLoss[0m : 10.56344
[1mStep[0m  [8/21], [94mLoss[0m : 10.71642
[1mStep[0m  [10/21], [94mLoss[0m : 10.74727
[1mStep[0m  [12/21], [94mLoss[0m : 10.52998
[1mStep[0m  [14/21], [94mLoss[0m : 10.76773
[1mStep[0m  [16/21], [94mLoss[0m : 10.65696
[1mStep[0m  [18/21], [94mLoss[0m : 10.89029
[1mStep[0m  [20/21], [94mLoss[0m : 10.70422

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.650, [92mTest[0m: 10.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70951
[1mStep[0m  [2/21], [94mLoss[0m : 10.64552
[1mStep[0m  [4/21], [94mLoss[0m : 10.51182
[1mStep[0m  [6/21], [94mLoss[0m : 10.50543
[1mStep[0m  [8/21], [94mLoss[0m : 10.65601
[1mStep[0m  [10/21], [94mLoss[0m : 10.53470
[1mStep[0m  [12/21], [94mLoss[0m : 10.75673
[1mStep[0m  [14/21], [94mLoss[0m : 10.68567
[1mStep[0m  [16/21], [94mLoss[0m : 10.44889
[1mStep[0m  [18/21], [94mLoss[0m : 10.51007
[1mStep[0m  [20/21], [94mLoss[0m : 10.80647

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.639, [92mTest[0m: 10.574, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62333
[1mStep[0m  [2/21], [94mLoss[0m : 10.55317
[1mStep[0m  [4/21], [94mLoss[0m : 10.52390
[1mStep[0m  [6/21], [94mLoss[0m : 10.31941
[1mStep[0m  [8/21], [94mLoss[0m : 10.43870
[1mStep[0m  [10/21], [94mLoss[0m : 10.40220
[1mStep[0m  [12/21], [94mLoss[0m : 10.55862
[1mStep[0m  [14/21], [94mLoss[0m : 10.44007
[1mStep[0m  [16/21], [94mLoss[0m : 10.83434
[1mStep[0m  [18/21], [94mLoss[0m : 10.72619
[1mStep[0m  [20/21], [94mLoss[0m : 10.66582

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.629, [92mTest[0m: 10.570, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33881
[1mStep[0m  [2/21], [94mLoss[0m : 10.58292
[1mStep[0m  [4/21], [94mLoss[0m : 10.71591
[1mStep[0m  [6/21], [94mLoss[0m : 10.59572
[1mStep[0m  [8/21], [94mLoss[0m : 10.64261
[1mStep[0m  [10/21], [94mLoss[0m : 10.51186
[1mStep[0m  [12/21], [94mLoss[0m : 10.78474
[1mStep[0m  [14/21], [94mLoss[0m : 10.77605
[1mStep[0m  [16/21], [94mLoss[0m : 10.46244
[1mStep[0m  [18/21], [94mLoss[0m : 10.67815
[1mStep[0m  [20/21], [94mLoss[0m : 10.40675

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.618, [92mTest[0m: 10.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49158
[1mStep[0m  [2/21], [94mLoss[0m : 10.79897
[1mStep[0m  [4/21], [94mLoss[0m : 10.63146
[1mStep[0m  [6/21], [94mLoss[0m : 10.55503
[1mStep[0m  [8/21], [94mLoss[0m : 10.83746
[1mStep[0m  [10/21], [94mLoss[0m : 10.67273
[1mStep[0m  [12/21], [94mLoss[0m : 10.45600
[1mStep[0m  [14/21], [94mLoss[0m : 10.57291
[1mStep[0m  [16/21], [94mLoss[0m : 10.55932
[1mStep[0m  [18/21], [94mLoss[0m : 10.59192
[1mStep[0m  [20/21], [94mLoss[0m : 10.59052

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.537, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60424
[1mStep[0m  [2/21], [94mLoss[0m : 10.86798
[1mStep[0m  [4/21], [94mLoss[0m : 10.59612
[1mStep[0m  [6/21], [94mLoss[0m : 10.64635
[1mStep[0m  [8/21], [94mLoss[0m : 10.75548
[1mStep[0m  [10/21], [94mLoss[0m : 10.59878
[1mStep[0m  [12/21], [94mLoss[0m : 10.65163
[1mStep[0m  [14/21], [94mLoss[0m : 10.35071
[1mStep[0m  [16/21], [94mLoss[0m : 10.56005
[1mStep[0m  [18/21], [94mLoss[0m : 10.73601
[1mStep[0m  [20/21], [94mLoss[0m : 10.62731

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.529, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61363
[1mStep[0m  [2/21], [94mLoss[0m : 10.22424
[1mStep[0m  [4/21], [94mLoss[0m : 10.58983
[1mStep[0m  [6/21], [94mLoss[0m : 10.55442
[1mStep[0m  [8/21], [94mLoss[0m : 10.75613
[1mStep[0m  [10/21], [94mLoss[0m : 11.00962
[1mStep[0m  [12/21], [94mLoss[0m : 10.52314
[1mStep[0m  [14/21], [94mLoss[0m : 10.61815
[1mStep[0m  [16/21], [94mLoss[0m : 10.59412
[1mStep[0m  [18/21], [94mLoss[0m : 10.47034
[1mStep[0m  [20/21], [94mLoss[0m : 10.42639

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.520, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67128
[1mStep[0m  [2/21], [94mLoss[0m : 10.65613
[1mStep[0m  [4/21], [94mLoss[0m : 10.57898
[1mStep[0m  [6/21], [94mLoss[0m : 10.61172
[1mStep[0m  [8/21], [94mLoss[0m : 10.43373
[1mStep[0m  [10/21], [94mLoss[0m : 10.64276
[1mStep[0m  [12/21], [94mLoss[0m : 10.66794
[1mStep[0m  [14/21], [94mLoss[0m : 10.35039
[1mStep[0m  [16/21], [94mLoss[0m : 10.44722
[1mStep[0m  [18/21], [94mLoss[0m : 10.72919
[1mStep[0m  [20/21], [94mLoss[0m : 10.66224

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.583, [92mTest[0m: 10.508, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82462
[1mStep[0m  [2/21], [94mLoss[0m : 10.49450
[1mStep[0m  [4/21], [94mLoss[0m : 10.56461
[1mStep[0m  [6/21], [94mLoss[0m : 10.87383
[1mStep[0m  [8/21], [94mLoss[0m : 10.58204
[1mStep[0m  [10/21], [94mLoss[0m : 10.70415
[1mStep[0m  [12/21], [94mLoss[0m : 10.52411
[1mStep[0m  [14/21], [94mLoss[0m : 10.50239
[1mStep[0m  [16/21], [94mLoss[0m : 10.33730
[1mStep[0m  [18/21], [94mLoss[0m : 10.36741
[1mStep[0m  [20/21], [94mLoss[0m : 10.68241

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.568, [92mTest[0m: 10.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56043
[1mStep[0m  [2/21], [94mLoss[0m : 10.65406
[1mStep[0m  [4/21], [94mLoss[0m : 10.40745
[1mStep[0m  [6/21], [94mLoss[0m : 10.65749
[1mStep[0m  [8/21], [94mLoss[0m : 10.69446
[1mStep[0m  [10/21], [94mLoss[0m : 10.66508
[1mStep[0m  [12/21], [94mLoss[0m : 10.48473
[1mStep[0m  [14/21], [94mLoss[0m : 10.49236
[1mStep[0m  [16/21], [94mLoss[0m : 10.43339
[1mStep[0m  [18/21], [94mLoss[0m : 10.69728
[1mStep[0m  [20/21], [94mLoss[0m : 10.57958

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48717
[1mStep[0m  [2/21], [94mLoss[0m : 10.54762
[1mStep[0m  [4/21], [94mLoss[0m : 10.25619
[1mStep[0m  [6/21], [94mLoss[0m : 10.78664
[1mStep[0m  [8/21], [94mLoss[0m : 10.53410
[1mStep[0m  [10/21], [94mLoss[0m : 10.31826
[1mStep[0m  [12/21], [94mLoss[0m : 10.34406
[1mStep[0m  [14/21], [94mLoss[0m : 10.29912
[1mStep[0m  [16/21], [94mLoss[0m : 10.71794
[1mStep[0m  [18/21], [94mLoss[0m : 10.66057
[1mStep[0m  [20/21], [94mLoss[0m : 10.52278

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.548, [92mTest[0m: 10.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58834
[1mStep[0m  [2/21], [94mLoss[0m : 10.42244
[1mStep[0m  [4/21], [94mLoss[0m : 10.68129
[1mStep[0m  [6/21], [94mLoss[0m : 10.33691
[1mStep[0m  [8/21], [94mLoss[0m : 10.68185
[1mStep[0m  [10/21], [94mLoss[0m : 10.29972
[1mStep[0m  [12/21], [94mLoss[0m : 10.74754
[1mStep[0m  [14/21], [94mLoss[0m : 10.41739
[1mStep[0m  [16/21], [94mLoss[0m : 10.78078
[1mStep[0m  [18/21], [94mLoss[0m : 10.44031
[1mStep[0m  [20/21], [94mLoss[0m : 10.48404

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.532, [92mTest[0m: 10.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.66250
[1mStep[0m  [2/21], [94mLoss[0m : 10.78733
[1mStep[0m  [4/21], [94mLoss[0m : 10.53717
[1mStep[0m  [6/21], [94mLoss[0m : 10.43887
[1mStep[0m  [8/21], [94mLoss[0m : 10.63035
[1mStep[0m  [10/21], [94mLoss[0m : 10.36862
[1mStep[0m  [12/21], [94mLoss[0m : 10.32116
[1mStep[0m  [14/21], [94mLoss[0m : 10.64229
[1mStep[0m  [16/21], [94mLoss[0m : 10.60633
[1mStep[0m  [18/21], [94mLoss[0m : 10.37291
[1mStep[0m  [20/21], [94mLoss[0m : 10.34912

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.520, [92mTest[0m: 10.435, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60711
[1mStep[0m  [2/21], [94mLoss[0m : 10.39066
[1mStep[0m  [4/21], [94mLoss[0m : 10.28428
[1mStep[0m  [6/21], [94mLoss[0m : 10.74907
[1mStep[0m  [8/21], [94mLoss[0m : 10.89783
[1mStep[0m  [10/21], [94mLoss[0m : 10.56085
[1mStep[0m  [12/21], [94mLoss[0m : 10.49645
[1mStep[0m  [14/21], [94mLoss[0m : 10.35151
[1mStep[0m  [16/21], [94mLoss[0m : 10.50899
[1mStep[0m  [18/21], [94mLoss[0m : 10.59032
[1mStep[0m  [20/21], [94mLoss[0m : 10.52913

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.511, [92mTest[0m: 10.410, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40623
[1mStep[0m  [2/21], [94mLoss[0m : 10.49428
[1mStep[0m  [4/21], [94mLoss[0m : 10.30140
[1mStep[0m  [6/21], [94mLoss[0m : 10.47326
[1mStep[0m  [8/21], [94mLoss[0m : 10.23181
[1mStep[0m  [10/21], [94mLoss[0m : 10.48072
[1mStep[0m  [12/21], [94mLoss[0m : 10.68477
[1mStep[0m  [14/21], [94mLoss[0m : 10.47816
[1mStep[0m  [16/21], [94mLoss[0m : 10.57551
[1mStep[0m  [18/21], [94mLoss[0m : 10.34207
[1mStep[0m  [20/21], [94mLoss[0m : 10.68031

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.490, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42410
[1mStep[0m  [2/21], [94mLoss[0m : 10.79102
[1mStep[0m  [4/21], [94mLoss[0m : 10.40124
[1mStep[0m  [6/21], [94mLoss[0m : 10.71422
[1mStep[0m  [8/21], [94mLoss[0m : 10.43494
[1mStep[0m  [10/21], [94mLoss[0m : 10.54127
[1mStep[0m  [12/21], [94mLoss[0m : 10.57376
[1mStep[0m  [14/21], [94mLoss[0m : 10.49597
[1mStep[0m  [16/21], [94mLoss[0m : 10.61936
[1mStep[0m  [18/21], [94mLoss[0m : 10.13388
[1mStep[0m  [20/21], [94mLoss[0m : 10.44269

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.492, [92mTest[0m: 10.391, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49676
[1mStep[0m  [2/21], [94mLoss[0m : 10.71370
[1mStep[0m  [4/21], [94mLoss[0m : 10.28520
[1mStep[0m  [6/21], [94mLoss[0m : 10.51353
[1mStep[0m  [8/21], [94mLoss[0m : 10.71964
[1mStep[0m  [10/21], [94mLoss[0m : 10.25204
[1mStep[0m  [12/21], [94mLoss[0m : 10.56932
[1mStep[0m  [14/21], [94mLoss[0m : 10.43223
[1mStep[0m  [16/21], [94mLoss[0m : 10.36950
[1mStep[0m  [18/21], [94mLoss[0m : 10.59460
[1mStep[0m  [20/21], [94mLoss[0m : 10.52216

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.478, [92mTest[0m: 10.395, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.52301
[1mStep[0m  [2/21], [94mLoss[0m : 10.49149
[1mStep[0m  [4/21], [94mLoss[0m : 10.33638
[1mStep[0m  [6/21], [94mLoss[0m : 10.48010
[1mStep[0m  [8/21], [94mLoss[0m : 10.31124
[1mStep[0m  [10/21], [94mLoss[0m : 10.32468
[1mStep[0m  [12/21], [94mLoss[0m : 10.54401
[1mStep[0m  [14/21], [94mLoss[0m : 10.61207
[1mStep[0m  [16/21], [94mLoss[0m : 10.36007
[1mStep[0m  [18/21], [94mLoss[0m : 10.51596
[1mStep[0m  [20/21], [94mLoss[0m : 10.32715

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.455, [92mTest[0m: 10.382, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47919
[1mStep[0m  [2/21], [94mLoss[0m : 10.45321
[1mStep[0m  [4/21], [94mLoss[0m : 10.44975
[1mStep[0m  [6/21], [94mLoss[0m : 10.57131
[1mStep[0m  [8/21], [94mLoss[0m : 10.37969
[1mStep[0m  [10/21], [94mLoss[0m : 10.42444
[1mStep[0m  [12/21], [94mLoss[0m : 10.20068
[1mStep[0m  [14/21], [94mLoss[0m : 10.54729
[1mStep[0m  [16/21], [94mLoss[0m : 10.31706
[1mStep[0m  [18/21], [94mLoss[0m : 10.57122
[1mStep[0m  [20/21], [94mLoss[0m : 10.47638

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.448, [92mTest[0m: 10.360, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38340
[1mStep[0m  [2/21], [94mLoss[0m : 10.70960
[1mStep[0m  [4/21], [94mLoss[0m : 10.42583
[1mStep[0m  [6/21], [94mLoss[0m : 10.53974
[1mStep[0m  [8/21], [94mLoss[0m : 10.79837
[1mStep[0m  [10/21], [94mLoss[0m : 10.55982
[1mStep[0m  [12/21], [94mLoss[0m : 10.43023
[1mStep[0m  [14/21], [94mLoss[0m : 10.08519
[1mStep[0m  [16/21], [94mLoss[0m : 10.24999
[1mStep[0m  [18/21], [94mLoss[0m : 10.52725
[1mStep[0m  [20/21], [94mLoss[0m : 10.27099

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.431, [92mTest[0m: 10.350, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55233
[1mStep[0m  [2/21], [94mLoss[0m : 10.50313
[1mStep[0m  [4/21], [94mLoss[0m : 10.31658
[1mStep[0m  [6/21], [94mLoss[0m : 10.52268
[1mStep[0m  [8/21], [94mLoss[0m : 10.72844
[1mStep[0m  [10/21], [94mLoss[0m : 10.45467
[1mStep[0m  [12/21], [94mLoss[0m : 10.45401
[1mStep[0m  [14/21], [94mLoss[0m : 10.56960
[1mStep[0m  [16/21], [94mLoss[0m : 10.30170
[1mStep[0m  [18/21], [94mLoss[0m : 10.25502
[1mStep[0m  [20/21], [94mLoss[0m : 10.36080

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.416, [92mTest[0m: 10.336, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.11908
[1mStep[0m  [2/21], [94mLoss[0m : 10.27821
[1mStep[0m  [4/21], [94mLoss[0m : 10.50554
[1mStep[0m  [6/21], [94mLoss[0m : 10.11324
[1mStep[0m  [8/21], [94mLoss[0m : 10.12346
[1mStep[0m  [10/21], [94mLoss[0m : 10.43369
[1mStep[0m  [12/21], [94mLoss[0m : 10.58202
[1mStep[0m  [14/21], [94mLoss[0m : 10.34428
[1mStep[0m  [16/21], [94mLoss[0m : 10.33258
[1mStep[0m  [18/21], [94mLoss[0m : 10.22476
[1mStep[0m  [20/21], [94mLoss[0m : 10.43274

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.401, [92mTest[0m: 10.318, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.28763
[1mStep[0m  [2/21], [94mLoss[0m : 10.55130
[1mStep[0m  [4/21], [94mLoss[0m : 10.45852
[1mStep[0m  [6/21], [94mLoss[0m : 10.79308
[1mStep[0m  [8/21], [94mLoss[0m : 10.38834
[1mStep[0m  [10/21], [94mLoss[0m : 10.56039
[1mStep[0m  [12/21], [94mLoss[0m : 10.11708
[1mStep[0m  [14/21], [94mLoss[0m : 10.19564
[1mStep[0m  [16/21], [94mLoss[0m : 10.26570
[1mStep[0m  [18/21], [94mLoss[0m : 10.38989
[1mStep[0m  [20/21], [94mLoss[0m : 10.38411

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.401, [92mTest[0m: 10.302, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36852
[1mStep[0m  [2/21], [94mLoss[0m : 10.40856
[1mStep[0m  [4/21], [94mLoss[0m : 10.41196
[1mStep[0m  [6/21], [94mLoss[0m : 10.42454
[1mStep[0m  [8/21], [94mLoss[0m : 10.53246
[1mStep[0m  [10/21], [94mLoss[0m : 10.19515
[1mStep[0m  [12/21], [94mLoss[0m : 10.48558
[1mStep[0m  [14/21], [94mLoss[0m : 10.12129
[1mStep[0m  [16/21], [94mLoss[0m : 10.39996
[1mStep[0m  [18/21], [94mLoss[0m : 10.47258
[1mStep[0m  [20/21], [94mLoss[0m : 10.46929

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.392, [92mTest[0m: 10.276, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.24465
[1mStep[0m  [2/21], [94mLoss[0m : 10.06150
[1mStep[0m  [4/21], [94mLoss[0m : 10.56404
[1mStep[0m  [6/21], [94mLoss[0m : 10.47450
[1mStep[0m  [8/21], [94mLoss[0m : 10.28418
[1mStep[0m  [10/21], [94mLoss[0m : 10.30694
[1mStep[0m  [12/21], [94mLoss[0m : 10.44606
[1mStep[0m  [14/21], [94mLoss[0m : 10.47886
[1mStep[0m  [16/21], [94mLoss[0m : 10.22465
[1mStep[0m  [18/21], [94mLoss[0m : 10.27535
[1mStep[0m  [20/21], [94mLoss[0m : 10.23349

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.374, [92mTest[0m: 10.291, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33832
[1mStep[0m  [2/21], [94mLoss[0m : 10.16748
[1mStep[0m  [4/21], [94mLoss[0m : 10.49269
[1mStep[0m  [6/21], [94mLoss[0m : 10.54034
[1mStep[0m  [8/21], [94mLoss[0m : 10.23411
[1mStep[0m  [10/21], [94mLoss[0m : 10.25037
[1mStep[0m  [12/21], [94mLoss[0m : 10.46445
[1mStep[0m  [14/21], [94mLoss[0m : 10.39371
[1mStep[0m  [16/21], [94mLoss[0m : 10.05334
[1mStep[0m  [18/21], [94mLoss[0m : 10.38499
[1mStep[0m  [20/21], [94mLoss[0m : 10.10720

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.365, [92mTest[0m: 10.262, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.241
====================================

Phase 2 - Evaluation MAE:  10.241426740373884
MAE score P1      10.635662
MAE score P2      10.241427
loss              10.365279
learning_rate        0.0001
batch_size              512
hidden_sizes          [250]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.35011
[1mStep[0m  [2/21], [94mLoss[0m : 11.06533
[1mStep[0m  [4/21], [94mLoss[0m : 11.08571
[1mStep[0m  [6/21], [94mLoss[0m : 10.95429
[1mStep[0m  [8/21], [94mLoss[0m : 11.08209
[1mStep[0m  [10/21], [94mLoss[0m : 10.82065
[1mStep[0m  [12/21], [94mLoss[0m : 10.88894
[1mStep[0m  [14/21], [94mLoss[0m : 10.80375
[1mStep[0m  [16/21], [94mLoss[0m : 11.32512
[1mStep[0m  [18/21], [94mLoss[0m : 11.06271
[1mStep[0m  [20/21], [94mLoss[0m : 10.84231

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.039, [92mTest[0m: 10.927, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.23005
[1mStep[0m  [2/21], [94mLoss[0m : 10.86442
[1mStep[0m  [4/21], [94mLoss[0m : 10.84718
[1mStep[0m  [6/21], [94mLoss[0m : 10.95609
[1mStep[0m  [8/21], [94mLoss[0m : 10.81714
[1mStep[0m  [10/21], [94mLoss[0m : 11.20213
[1mStep[0m  [12/21], [94mLoss[0m : 10.95642
[1mStep[0m  [14/21], [94mLoss[0m : 10.80373
[1mStep[0m  [16/21], [94mLoss[0m : 10.72666
[1mStep[0m  [18/21], [94mLoss[0m : 10.69709
[1mStep[0m  [20/21], [94mLoss[0m : 10.53925

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.879, [92mTest[0m: 10.909, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81028
[1mStep[0m  [2/21], [94mLoss[0m : 10.63657
[1mStep[0m  [4/21], [94mLoss[0m : 10.64267
[1mStep[0m  [6/21], [94mLoss[0m : 10.60233
[1mStep[0m  [8/21], [94mLoss[0m : 10.79496
[1mStep[0m  [10/21], [94mLoss[0m : 10.29122
[1mStep[0m  [12/21], [94mLoss[0m : 10.65653
[1mStep[0m  [14/21], [94mLoss[0m : 10.51143
[1mStep[0m  [16/21], [94mLoss[0m : 10.77199
[1mStep[0m  [18/21], [94mLoss[0m : 10.79532
[1mStep[0m  [20/21], [94mLoss[0m : 10.80954

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82140
[1mStep[0m  [2/21], [94mLoss[0m : 10.69899
[1mStep[0m  [4/21], [94mLoss[0m : 10.73545
[1mStep[0m  [6/21], [94mLoss[0m : 10.63125
[1mStep[0m  [8/21], [94mLoss[0m : 10.66965
[1mStep[0m  [10/21], [94mLoss[0m : 10.56234
[1mStep[0m  [12/21], [94mLoss[0m : 10.51609
[1mStep[0m  [14/21], [94mLoss[0m : 10.19706
[1mStep[0m  [16/21], [94mLoss[0m : 10.54766
[1mStep[0m  [18/21], [94mLoss[0m : 10.69938
[1mStep[0m  [20/21], [94mLoss[0m : 10.59796

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.558, [92mTest[0m: 10.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40932
[1mStep[0m  [2/21], [94mLoss[0m : 10.26774
[1mStep[0m  [4/21], [94mLoss[0m : 10.26849
[1mStep[0m  [6/21], [94mLoss[0m : 10.32987
[1mStep[0m  [8/21], [94mLoss[0m : 10.71303
[1mStep[0m  [10/21], [94mLoss[0m : 10.39101
[1mStep[0m  [12/21], [94mLoss[0m : 10.26301
[1mStep[0m  [14/21], [94mLoss[0m : 10.62989
[1mStep[0m  [16/21], [94mLoss[0m : 10.55254
[1mStep[0m  [18/21], [94mLoss[0m : 10.31016
[1mStep[0m  [20/21], [94mLoss[0m : 10.36343

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.395, [92mTest[0m: 10.522, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.37524
[1mStep[0m  [2/21], [94mLoss[0m : 10.11882
[1mStep[0m  [4/21], [94mLoss[0m : 10.48044
[1mStep[0m  [6/21], [94mLoss[0m : 10.29634
[1mStep[0m  [8/21], [94mLoss[0m : 10.28672
[1mStep[0m  [10/21], [94mLoss[0m : 10.30624
[1mStep[0m  [12/21], [94mLoss[0m : 10.23343
[1mStep[0m  [14/21], [94mLoss[0m : 10.15541
[1mStep[0m  [16/21], [94mLoss[0m : 10.23521
[1mStep[0m  [18/21], [94mLoss[0m : 10.20422
[1mStep[0m  [20/21], [94mLoss[0m : 10.03632

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.240, [92mTest[0m: 10.386, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.04565
[1mStep[0m  [2/21], [94mLoss[0m : 10.30433
[1mStep[0m  [4/21], [94mLoss[0m : 10.26234
[1mStep[0m  [6/21], [94mLoss[0m : 10.00717
[1mStep[0m  [8/21], [94mLoss[0m : 9.85941
[1mStep[0m  [10/21], [94mLoss[0m : 9.50138
[1mStep[0m  [12/21], [94mLoss[0m : 10.13944
[1mStep[0m  [14/21], [94mLoss[0m : 10.00039
[1mStep[0m  [16/21], [94mLoss[0m : 10.04803
[1mStep[0m  [18/21], [94mLoss[0m : 9.87129
[1mStep[0m  [20/21], [94mLoss[0m : 10.14697

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.070, [92mTest[0m: 10.265, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.95545
[1mStep[0m  [2/21], [94mLoss[0m : 9.89668
[1mStep[0m  [4/21], [94mLoss[0m : 9.91631
[1mStep[0m  [6/21], [94mLoss[0m : 9.72891
[1mStep[0m  [8/21], [94mLoss[0m : 9.80674
[1mStep[0m  [10/21], [94mLoss[0m : 10.13722
[1mStep[0m  [12/21], [94mLoss[0m : 9.63791
[1mStep[0m  [14/21], [94mLoss[0m : 9.98154
[1mStep[0m  [16/21], [94mLoss[0m : 9.87962
[1mStep[0m  [18/21], [94mLoss[0m : 9.85757
[1mStep[0m  [20/21], [94mLoss[0m : 9.91854

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.919, [92mTest[0m: 10.128, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.82851
[1mStep[0m  [2/21], [94mLoss[0m : 9.69128
[1mStep[0m  [4/21], [94mLoss[0m : 10.14668
[1mStep[0m  [6/21], [94mLoss[0m : 10.09851
[1mStep[0m  [8/21], [94mLoss[0m : 9.51470
[1mStep[0m  [10/21], [94mLoss[0m : 9.75994
[1mStep[0m  [12/21], [94mLoss[0m : 9.92815
[1mStep[0m  [14/21], [94mLoss[0m : 9.61830
[1mStep[0m  [16/21], [94mLoss[0m : 9.45533
[1mStep[0m  [18/21], [94mLoss[0m : 9.82778
[1mStep[0m  [20/21], [94mLoss[0m : 9.87587

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.760, [92mTest[0m: 9.992, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.73132
[1mStep[0m  [2/21], [94mLoss[0m : 9.71390
[1mStep[0m  [4/21], [94mLoss[0m : 9.40299
[1mStep[0m  [6/21], [94mLoss[0m : 9.86124
[1mStep[0m  [8/21], [94mLoss[0m : 9.90615
[1mStep[0m  [10/21], [94mLoss[0m : 9.72746
[1mStep[0m  [12/21], [94mLoss[0m : 9.55199
[1mStep[0m  [14/21], [94mLoss[0m : 9.13125
[1mStep[0m  [16/21], [94mLoss[0m : 9.40835
[1mStep[0m  [18/21], [94mLoss[0m : 9.61272
[1mStep[0m  [20/21], [94mLoss[0m : 9.58827

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.604, [92mTest[0m: 9.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.59309
[1mStep[0m  [2/21], [94mLoss[0m : 9.56520
[1mStep[0m  [4/21], [94mLoss[0m : 9.70928
[1mStep[0m  [6/21], [94mLoss[0m : 9.44267
[1mStep[0m  [8/21], [94mLoss[0m : 9.52147
[1mStep[0m  [10/21], [94mLoss[0m : 9.66940
[1mStep[0m  [12/21], [94mLoss[0m : 9.43831
[1mStep[0m  [14/21], [94mLoss[0m : 9.59914
[1mStep[0m  [16/21], [94mLoss[0m : 9.33569
[1mStep[0m  [18/21], [94mLoss[0m : 9.56584
[1mStep[0m  [20/21], [94mLoss[0m : 9.32834

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.438, [92mTest[0m: 9.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.49846
[1mStep[0m  [2/21], [94mLoss[0m : 9.30077
[1mStep[0m  [4/21], [94mLoss[0m : 9.47335
[1mStep[0m  [6/21], [94mLoss[0m : 9.35958
[1mStep[0m  [8/21], [94mLoss[0m : 9.39192
[1mStep[0m  [10/21], [94mLoss[0m : 9.38153
[1mStep[0m  [12/21], [94mLoss[0m : 9.39768
[1mStep[0m  [14/21], [94mLoss[0m : 9.33550
[1mStep[0m  [16/21], [94mLoss[0m : 9.51840
[1mStep[0m  [18/21], [94mLoss[0m : 9.01818
[1mStep[0m  [20/21], [94mLoss[0m : 9.42972

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.281, [92mTest[0m: 9.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.44462
[1mStep[0m  [2/21], [94mLoss[0m : 9.28552
[1mStep[0m  [4/21], [94mLoss[0m : 9.25668
[1mStep[0m  [6/21], [94mLoss[0m : 8.88689
[1mStep[0m  [8/21], [94mLoss[0m : 9.38512
[1mStep[0m  [10/21], [94mLoss[0m : 9.22329
[1mStep[0m  [12/21], [94mLoss[0m : 8.90545
[1mStep[0m  [14/21], [94mLoss[0m : 8.84372
[1mStep[0m  [16/21], [94mLoss[0m : 9.14101
[1mStep[0m  [18/21], [94mLoss[0m : 9.04293
[1mStep[0m  [20/21], [94mLoss[0m : 8.99095

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.115, [92mTest[0m: 9.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.66934
[1mStep[0m  [2/21], [94mLoss[0m : 8.78593
[1mStep[0m  [4/21], [94mLoss[0m : 8.79217
[1mStep[0m  [6/21], [94mLoss[0m : 8.72691
[1mStep[0m  [8/21], [94mLoss[0m : 9.00784
[1mStep[0m  [10/21], [94mLoss[0m : 8.98776
[1mStep[0m  [12/21], [94mLoss[0m : 9.11893
[1mStep[0m  [14/21], [94mLoss[0m : 9.07639
[1mStep[0m  [16/21], [94mLoss[0m : 9.10497
[1mStep[0m  [18/21], [94mLoss[0m : 9.16127
[1mStep[0m  [20/21], [94mLoss[0m : 9.15938

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.951, [92mTest[0m: 9.342, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.09069
[1mStep[0m  [2/21], [94mLoss[0m : 8.76362
[1mStep[0m  [4/21], [94mLoss[0m : 8.61064
[1mStep[0m  [6/21], [94mLoss[0m : 8.70687
[1mStep[0m  [8/21], [94mLoss[0m : 8.90473
[1mStep[0m  [10/21], [94mLoss[0m : 8.72583
[1mStep[0m  [12/21], [94mLoss[0m : 8.79166
[1mStep[0m  [14/21], [94mLoss[0m : 9.01401
[1mStep[0m  [16/21], [94mLoss[0m : 8.80374
[1mStep[0m  [18/21], [94mLoss[0m : 8.77285
[1mStep[0m  [20/21], [94mLoss[0m : 8.35907

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.797, [92mTest[0m: 9.209, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.64421
[1mStep[0m  [2/21], [94mLoss[0m : 8.68667
[1mStep[0m  [4/21], [94mLoss[0m : 8.62547
[1mStep[0m  [6/21], [94mLoss[0m : 8.77679
[1mStep[0m  [8/21], [94mLoss[0m : 8.60473
[1mStep[0m  [10/21], [94mLoss[0m : 8.58245
[1mStep[0m  [12/21], [94mLoss[0m : 8.54250
[1mStep[0m  [14/21], [94mLoss[0m : 8.73900
[1mStep[0m  [16/21], [94mLoss[0m : 8.53678
[1mStep[0m  [18/21], [94mLoss[0m : 8.38912
[1mStep[0m  [20/21], [94mLoss[0m : 8.49962

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.636, [92mTest[0m: 9.061, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.65360
[1mStep[0m  [2/21], [94mLoss[0m : 8.66719
[1mStep[0m  [4/21], [94mLoss[0m : 8.60554
[1mStep[0m  [6/21], [94mLoss[0m : 8.71031
[1mStep[0m  [8/21], [94mLoss[0m : 8.61956
[1mStep[0m  [10/21], [94mLoss[0m : 8.28281
[1mStep[0m  [12/21], [94mLoss[0m : 8.39454
[1mStep[0m  [14/21], [94mLoss[0m : 8.54566
[1mStep[0m  [16/21], [94mLoss[0m : 8.33964
[1mStep[0m  [18/21], [94mLoss[0m : 8.45216
[1mStep[0m  [20/21], [94mLoss[0m : 8.23951

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.473, [92mTest[0m: 8.947, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.31462
[1mStep[0m  [2/21], [94mLoss[0m : 8.57560
[1mStep[0m  [4/21], [94mLoss[0m : 8.49016
[1mStep[0m  [6/21], [94mLoss[0m : 8.42548
[1mStep[0m  [8/21], [94mLoss[0m : 8.20702
[1mStep[0m  [10/21], [94mLoss[0m : 8.33133
[1mStep[0m  [12/21], [94mLoss[0m : 8.14235
[1mStep[0m  [14/21], [94mLoss[0m : 8.29200
[1mStep[0m  [16/21], [94mLoss[0m : 8.43781
[1mStep[0m  [18/21], [94mLoss[0m : 8.03157
[1mStep[0m  [20/21], [94mLoss[0m : 8.27614

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.326, [92mTest[0m: 8.816, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.14427
[1mStep[0m  [2/21], [94mLoss[0m : 8.16724
[1mStep[0m  [4/21], [94mLoss[0m : 7.99684
[1mStep[0m  [6/21], [94mLoss[0m : 8.19725
[1mStep[0m  [8/21], [94mLoss[0m : 8.21705
[1mStep[0m  [10/21], [94mLoss[0m : 8.30700
[1mStep[0m  [12/21], [94mLoss[0m : 7.91862
[1mStep[0m  [14/21], [94mLoss[0m : 8.10826
[1mStep[0m  [16/21], [94mLoss[0m : 7.84368
[1mStep[0m  [18/21], [94mLoss[0m : 8.04710
[1mStep[0m  [20/21], [94mLoss[0m : 8.18201

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.163, [92mTest[0m: 8.696, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.18994
[1mStep[0m  [2/21], [94mLoss[0m : 8.17402
[1mStep[0m  [4/21], [94mLoss[0m : 8.05915
[1mStep[0m  [6/21], [94mLoss[0m : 7.90108
[1mStep[0m  [8/21], [94mLoss[0m : 8.17779
[1mStep[0m  [10/21], [94mLoss[0m : 7.97307
[1mStep[0m  [12/21], [94mLoss[0m : 7.90327
[1mStep[0m  [14/21], [94mLoss[0m : 8.08846
[1mStep[0m  [16/21], [94mLoss[0m : 7.74176
[1mStep[0m  [18/21], [94mLoss[0m : 7.77763
[1mStep[0m  [20/21], [94mLoss[0m : 8.03951

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.999, [92mTest[0m: 8.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.98240
[1mStep[0m  [2/21], [94mLoss[0m : 7.68421
[1mStep[0m  [4/21], [94mLoss[0m : 7.95861
[1mStep[0m  [6/21], [94mLoss[0m : 8.05140
[1mStep[0m  [8/21], [94mLoss[0m : 8.10603
[1mStep[0m  [10/21], [94mLoss[0m : 7.60042
[1mStep[0m  [12/21], [94mLoss[0m : 7.81080
[1mStep[0m  [14/21], [94mLoss[0m : 7.93461
[1mStep[0m  [16/21], [94mLoss[0m : 8.12367
[1mStep[0m  [18/21], [94mLoss[0m : 7.76897
[1mStep[0m  [20/21], [94mLoss[0m : 7.79494

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.864, [92mTest[0m: 8.431, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.92085
[1mStep[0m  [2/21], [94mLoss[0m : 7.61543
[1mStep[0m  [4/21], [94mLoss[0m : 7.67539
[1mStep[0m  [6/21], [94mLoss[0m : 7.68169
[1mStep[0m  [8/21], [94mLoss[0m : 7.63805
[1mStep[0m  [10/21], [94mLoss[0m : 7.78175
[1mStep[0m  [12/21], [94mLoss[0m : 7.31292
[1mStep[0m  [14/21], [94mLoss[0m : 7.75777
[1mStep[0m  [16/21], [94mLoss[0m : 8.08738
[1mStep[0m  [18/21], [94mLoss[0m : 7.41504
[1mStep[0m  [20/21], [94mLoss[0m : 7.75632

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.721, [92mTest[0m: 8.301, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.65002
[1mStep[0m  [2/21], [94mLoss[0m : 7.65955
[1mStep[0m  [4/21], [94mLoss[0m : 7.45752
[1mStep[0m  [6/21], [94mLoss[0m : 7.79110
[1mStep[0m  [8/21], [94mLoss[0m : 7.82777
[1mStep[0m  [10/21], [94mLoss[0m : 7.49805
[1mStep[0m  [12/21], [94mLoss[0m : 7.70680
[1mStep[0m  [14/21], [94mLoss[0m : 7.41533
[1mStep[0m  [16/21], [94mLoss[0m : 7.69369
[1mStep[0m  [18/21], [94mLoss[0m : 7.62765
[1mStep[0m  [20/21], [94mLoss[0m : 7.47105

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.581, [92mTest[0m: 8.184, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.74594
[1mStep[0m  [2/21], [94mLoss[0m : 7.29664
[1mStep[0m  [4/21], [94mLoss[0m : 7.59335
[1mStep[0m  [6/21], [94mLoss[0m : 7.15506
[1mStep[0m  [8/21], [94mLoss[0m : 7.36390
[1mStep[0m  [10/21], [94mLoss[0m : 7.89466
[1mStep[0m  [12/21], [94mLoss[0m : 7.22122
[1mStep[0m  [14/21], [94mLoss[0m : 7.34773
[1mStep[0m  [16/21], [94mLoss[0m : 7.11106
[1mStep[0m  [18/21], [94mLoss[0m : 7.61706
[1mStep[0m  [20/21], [94mLoss[0m : 7.37936

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.449, [92mTest[0m: 8.074, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.44809
[1mStep[0m  [2/21], [94mLoss[0m : 7.67059
[1mStep[0m  [4/21], [94mLoss[0m : 7.42665
[1mStep[0m  [6/21], [94mLoss[0m : 7.33618
[1mStep[0m  [8/21], [94mLoss[0m : 7.20414
[1mStep[0m  [10/21], [94mLoss[0m : 7.22040
[1mStep[0m  [12/21], [94mLoss[0m : 7.21791
[1mStep[0m  [14/21], [94mLoss[0m : 7.15254
[1mStep[0m  [16/21], [94mLoss[0m : 7.21846
[1mStep[0m  [18/21], [94mLoss[0m : 7.38821
[1mStep[0m  [20/21], [94mLoss[0m : 7.33834

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.314, [92mTest[0m: 7.975, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.00518
[1mStep[0m  [2/21], [94mLoss[0m : 7.32385
[1mStep[0m  [4/21], [94mLoss[0m : 6.96284
[1mStep[0m  [6/21], [94mLoss[0m : 7.22079
[1mStep[0m  [8/21], [94mLoss[0m : 7.01508
[1mStep[0m  [10/21], [94mLoss[0m : 7.06357
[1mStep[0m  [12/21], [94mLoss[0m : 7.43301
[1mStep[0m  [14/21], [94mLoss[0m : 7.27368
[1mStep[0m  [16/21], [94mLoss[0m : 7.15967
[1mStep[0m  [18/21], [94mLoss[0m : 7.05287
[1mStep[0m  [20/21], [94mLoss[0m : 7.33897

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.177, [92mTest[0m: 7.860, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.22972
[1mStep[0m  [2/21], [94mLoss[0m : 6.82760
[1mStep[0m  [4/21], [94mLoss[0m : 7.24709
[1mStep[0m  [6/21], [94mLoss[0m : 7.23001
[1mStep[0m  [8/21], [94mLoss[0m : 7.02401
[1mStep[0m  [10/21], [94mLoss[0m : 7.08261
[1mStep[0m  [12/21], [94mLoss[0m : 7.01304
[1mStep[0m  [14/21], [94mLoss[0m : 7.01537
[1mStep[0m  [16/21], [94mLoss[0m : 7.12141
[1mStep[0m  [18/21], [94mLoss[0m : 6.98714
[1mStep[0m  [20/21], [94mLoss[0m : 6.81612

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.051, [92mTest[0m: 7.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.01248
[1mStep[0m  [2/21], [94mLoss[0m : 7.03000
[1mStep[0m  [4/21], [94mLoss[0m : 6.92345
[1mStep[0m  [6/21], [94mLoss[0m : 7.08068
[1mStep[0m  [8/21], [94mLoss[0m : 6.81825
[1mStep[0m  [10/21], [94mLoss[0m : 6.84282
[1mStep[0m  [12/21], [94mLoss[0m : 6.83327
[1mStep[0m  [14/21], [94mLoss[0m : 6.92623
[1mStep[0m  [16/21], [94mLoss[0m : 6.78542
[1mStep[0m  [18/21], [94mLoss[0m : 6.86016
[1mStep[0m  [20/21], [94mLoss[0m : 6.61322

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.924, [92mTest[0m: 7.628, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.73657
[1mStep[0m  [2/21], [94mLoss[0m : 6.74094
[1mStep[0m  [4/21], [94mLoss[0m : 6.81795
[1mStep[0m  [6/21], [94mLoss[0m : 6.93427
[1mStep[0m  [8/21], [94mLoss[0m : 6.97376
[1mStep[0m  [10/21], [94mLoss[0m : 6.88882
[1mStep[0m  [12/21], [94mLoss[0m : 6.71821
[1mStep[0m  [14/21], [94mLoss[0m : 6.69383
[1mStep[0m  [16/21], [94mLoss[0m : 6.92161
[1mStep[0m  [18/21], [94mLoss[0m : 6.60518
[1mStep[0m  [20/21], [94mLoss[0m : 6.68779

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.805, [92mTest[0m: 7.513, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.93912
[1mStep[0m  [2/21], [94mLoss[0m : 6.79814
[1mStep[0m  [4/21], [94mLoss[0m : 6.74633
[1mStep[0m  [6/21], [94mLoss[0m : 6.81439
[1mStep[0m  [8/21], [94mLoss[0m : 6.69036
[1mStep[0m  [10/21], [94mLoss[0m : 6.73775
[1mStep[0m  [12/21], [94mLoss[0m : 6.50026
[1mStep[0m  [14/21], [94mLoss[0m : 6.59718
[1mStep[0m  [16/21], [94mLoss[0m : 6.40130
[1mStep[0m  [18/21], [94mLoss[0m : 6.50847
[1mStep[0m  [20/21], [94mLoss[0m : 6.48563

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.671, [92mTest[0m: 7.409, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.291
====================================

Phase 1 - Evaluation MAE:  7.290859426770892
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 6.55097
[1mStep[0m  [2/21], [94mLoss[0m : 6.56230
[1mStep[0m  [4/21], [94mLoss[0m : 6.65463
[1mStep[0m  [6/21], [94mLoss[0m : 6.33396
[1mStep[0m  [8/21], [94mLoss[0m : 6.52990
[1mStep[0m  [10/21], [94mLoss[0m : 6.43631
[1mStep[0m  [12/21], [94mLoss[0m : 6.59662
[1mStep[0m  [14/21], [94mLoss[0m : 6.15825
[1mStep[0m  [16/21], [94mLoss[0m : 6.58764
[1mStep[0m  [18/21], [94mLoss[0m : 6.52609
[1mStep[0m  [20/21], [94mLoss[0m : 6.43582

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.547, [92mTest[0m: 7.292, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.18482
[1mStep[0m  [2/21], [94mLoss[0m : 6.22248
[1mStep[0m  [4/21], [94mLoss[0m : 6.65317
[1mStep[0m  [6/21], [94mLoss[0m : 6.59494
[1mStep[0m  [8/21], [94mLoss[0m : 6.24108
[1mStep[0m  [10/21], [94mLoss[0m : 6.31941
[1mStep[0m  [12/21], [94mLoss[0m : 6.29027
[1mStep[0m  [14/21], [94mLoss[0m : 6.52935
[1mStep[0m  [16/21], [94mLoss[0m : 6.08352
[1mStep[0m  [18/21], [94mLoss[0m : 6.32910
[1mStep[0m  [20/21], [94mLoss[0m : 6.23382

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.408, [92mTest[0m: 7.192, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.23687
[1mStep[0m  [2/21], [94mLoss[0m : 6.45706
[1mStep[0m  [4/21], [94mLoss[0m : 6.23275
[1mStep[0m  [6/21], [94mLoss[0m : 6.24076
[1mStep[0m  [8/21], [94mLoss[0m : 6.38982
[1mStep[0m  [10/21], [94mLoss[0m : 6.28757
[1mStep[0m  [12/21], [94mLoss[0m : 6.19948
[1mStep[0m  [14/21], [94mLoss[0m : 6.17863
[1mStep[0m  [16/21], [94mLoss[0m : 6.29699
[1mStep[0m  [18/21], [94mLoss[0m : 6.22171
[1mStep[0m  [20/21], [94mLoss[0m : 6.19226

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.257, [92mTest[0m: 7.060, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.05321
[1mStep[0m  [2/21], [94mLoss[0m : 6.14561
[1mStep[0m  [4/21], [94mLoss[0m : 5.95144
[1mStep[0m  [6/21], [94mLoss[0m : 6.26268
[1mStep[0m  [8/21], [94mLoss[0m : 6.00147
[1mStep[0m  [10/21], [94mLoss[0m : 6.02058
[1mStep[0m  [12/21], [94mLoss[0m : 6.14965
[1mStep[0m  [14/21], [94mLoss[0m : 6.22392
[1mStep[0m  [16/21], [94mLoss[0m : 6.17818
[1mStep[0m  [18/21], [94mLoss[0m : 6.10843
[1mStep[0m  [20/21], [94mLoss[0m : 6.34132

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.118, [92mTest[0m: 6.908, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.73215
[1mStep[0m  [2/21], [94mLoss[0m : 5.94942
[1mStep[0m  [4/21], [94mLoss[0m : 6.51277
[1mStep[0m  [6/21], [94mLoss[0m : 6.20318
[1mStep[0m  [8/21], [94mLoss[0m : 5.97393
[1mStep[0m  [10/21], [94mLoss[0m : 6.08033
[1mStep[0m  [12/21], [94mLoss[0m : 6.20414
[1mStep[0m  [14/21], [94mLoss[0m : 6.16644
[1mStep[0m  [16/21], [94mLoss[0m : 5.93111
[1mStep[0m  [18/21], [94mLoss[0m : 5.97749
[1mStep[0m  [20/21], [94mLoss[0m : 5.84506

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.982, [92mTest[0m: 6.778, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.87276
[1mStep[0m  [2/21], [94mLoss[0m : 6.14286
[1mStep[0m  [4/21], [94mLoss[0m : 5.98541
[1mStep[0m  [6/21], [94mLoss[0m : 6.04989
[1mStep[0m  [8/21], [94mLoss[0m : 5.66187
[1mStep[0m  [10/21], [94mLoss[0m : 5.85278
[1mStep[0m  [12/21], [94mLoss[0m : 5.62414
[1mStep[0m  [14/21], [94mLoss[0m : 5.67272
[1mStep[0m  [16/21], [94mLoss[0m : 5.73797
[1mStep[0m  [18/21], [94mLoss[0m : 5.94171
[1mStep[0m  [20/21], [94mLoss[0m : 5.81679

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.847, [92mTest[0m: 6.653, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.87183
[1mStep[0m  [2/21], [94mLoss[0m : 5.85736
[1mStep[0m  [4/21], [94mLoss[0m : 5.51019
[1mStep[0m  [6/21], [94mLoss[0m : 5.82241
[1mStep[0m  [8/21], [94mLoss[0m : 5.46736
[1mStep[0m  [10/21], [94mLoss[0m : 5.76532
[1mStep[0m  [12/21], [94mLoss[0m : 5.88296
[1mStep[0m  [14/21], [94mLoss[0m : 5.61717
[1mStep[0m  [16/21], [94mLoss[0m : 5.73420
[1mStep[0m  [18/21], [94mLoss[0m : 5.47708
[1mStep[0m  [20/21], [94mLoss[0m : 5.74830

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.717, [92mTest[0m: 6.510, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.82233
[1mStep[0m  [2/21], [94mLoss[0m : 5.78780
[1mStep[0m  [4/21], [94mLoss[0m : 5.51884
[1mStep[0m  [6/21], [94mLoss[0m : 5.68627
[1mStep[0m  [8/21], [94mLoss[0m : 5.47198
[1mStep[0m  [10/21], [94mLoss[0m : 5.35687
[1mStep[0m  [12/21], [94mLoss[0m : 5.46876
[1mStep[0m  [14/21], [94mLoss[0m : 5.80846
[1mStep[0m  [16/21], [94mLoss[0m : 5.65892
[1mStep[0m  [18/21], [94mLoss[0m : 5.44271
[1mStep[0m  [20/21], [94mLoss[0m : 5.56179

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.587, [92mTest[0m: 6.395, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.55862
[1mStep[0m  [2/21], [94mLoss[0m : 5.61921
[1mStep[0m  [4/21], [94mLoss[0m : 5.49405
[1mStep[0m  [6/21], [94mLoss[0m : 5.42343
[1mStep[0m  [8/21], [94mLoss[0m : 5.47727
[1mStep[0m  [10/21], [94mLoss[0m : 5.44472
[1mStep[0m  [12/21], [94mLoss[0m : 5.28394
[1mStep[0m  [14/21], [94mLoss[0m : 5.48285
[1mStep[0m  [16/21], [94mLoss[0m : 5.46023
[1mStep[0m  [18/21], [94mLoss[0m : 5.26961
[1mStep[0m  [20/21], [94mLoss[0m : 5.34788

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.463, [92mTest[0m: 6.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.40410
[1mStep[0m  [2/21], [94mLoss[0m : 5.32806
[1mStep[0m  [4/21], [94mLoss[0m : 5.12760
[1mStep[0m  [6/21], [94mLoss[0m : 5.34004
[1mStep[0m  [8/21], [94mLoss[0m : 5.20617
[1mStep[0m  [10/21], [94mLoss[0m : 5.08342
[1mStep[0m  [12/21], [94mLoss[0m : 5.69001
[1mStep[0m  [14/21], [94mLoss[0m : 5.41805
[1mStep[0m  [16/21], [94mLoss[0m : 5.20663
[1mStep[0m  [18/21], [94mLoss[0m : 5.19064
[1mStep[0m  [20/21], [94mLoss[0m : 5.04775

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.336, [92mTest[0m: 6.108, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16466
[1mStep[0m  [2/21], [94mLoss[0m : 5.29236
[1mStep[0m  [4/21], [94mLoss[0m : 5.16117
[1mStep[0m  [6/21], [94mLoss[0m : 5.26100
[1mStep[0m  [8/21], [94mLoss[0m : 5.15081
[1mStep[0m  [10/21], [94mLoss[0m : 5.18780
[1mStep[0m  [12/21], [94mLoss[0m : 5.21050
[1mStep[0m  [14/21], [94mLoss[0m : 5.13735
[1mStep[0m  [16/21], [94mLoss[0m : 5.27209
[1mStep[0m  [18/21], [94mLoss[0m : 5.00984
[1mStep[0m  [20/21], [94mLoss[0m : 5.11733

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.212, [92mTest[0m: 5.986, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16548
[1mStep[0m  [2/21], [94mLoss[0m : 5.20877
[1mStep[0m  [4/21], [94mLoss[0m : 4.81822
[1mStep[0m  [6/21], [94mLoss[0m : 5.01363
[1mStep[0m  [8/21], [94mLoss[0m : 5.26386
[1mStep[0m  [10/21], [94mLoss[0m : 5.22620
[1mStep[0m  [12/21], [94mLoss[0m : 5.00568
[1mStep[0m  [14/21], [94mLoss[0m : 5.02217
[1mStep[0m  [16/21], [94mLoss[0m : 5.14396
[1mStep[0m  [18/21], [94mLoss[0m : 5.04440
[1mStep[0m  [20/21], [94mLoss[0m : 4.91848

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.082, [92mTest[0m: 5.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.94797
[1mStep[0m  [2/21], [94mLoss[0m : 5.06832
[1mStep[0m  [4/21], [94mLoss[0m : 4.78847
[1mStep[0m  [6/21], [94mLoss[0m : 5.07132
[1mStep[0m  [8/21], [94mLoss[0m : 5.09175
[1mStep[0m  [10/21], [94mLoss[0m : 5.01792
[1mStep[0m  [12/21], [94mLoss[0m : 4.79670
[1mStep[0m  [14/21], [94mLoss[0m : 4.86839
[1mStep[0m  [16/21], [94mLoss[0m : 5.07126
[1mStep[0m  [18/21], [94mLoss[0m : 5.17418
[1mStep[0m  [20/21], [94mLoss[0m : 5.00244

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.980, [92mTest[0m: 5.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.83732
[1mStep[0m  [2/21], [94mLoss[0m : 4.95090
[1mStep[0m  [4/21], [94mLoss[0m : 5.01991
[1mStep[0m  [6/21], [94mLoss[0m : 4.61926
[1mStep[0m  [8/21], [94mLoss[0m : 5.19142
[1mStep[0m  [10/21], [94mLoss[0m : 4.80058
[1mStep[0m  [12/21], [94mLoss[0m : 5.07832
[1mStep[0m  [14/21], [94mLoss[0m : 4.67890
[1mStep[0m  [16/21], [94mLoss[0m : 4.89705
[1mStep[0m  [18/21], [94mLoss[0m : 4.88805
[1mStep[0m  [20/21], [94mLoss[0m : 4.66998

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.855, [92mTest[0m: 5.643, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82985
[1mStep[0m  [2/21], [94mLoss[0m : 5.12720
[1mStep[0m  [4/21], [94mLoss[0m : 4.76003
[1mStep[0m  [6/21], [94mLoss[0m : 4.69048
[1mStep[0m  [8/21], [94mLoss[0m : 4.75243
[1mStep[0m  [10/21], [94mLoss[0m : 4.70217
[1mStep[0m  [12/21], [94mLoss[0m : 4.41649
[1mStep[0m  [14/21], [94mLoss[0m : 4.62634
[1mStep[0m  [16/21], [94mLoss[0m : 4.59978
[1mStep[0m  [18/21], [94mLoss[0m : 4.56823
[1mStep[0m  [20/21], [94mLoss[0m : 4.59142

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.743, [92mTest[0m: 5.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82685
[1mStep[0m  [2/21], [94mLoss[0m : 4.83424
[1mStep[0m  [4/21], [94mLoss[0m : 4.69178
[1mStep[0m  [6/21], [94mLoss[0m : 4.45429
[1mStep[0m  [8/21], [94mLoss[0m : 4.67164
[1mStep[0m  [10/21], [94mLoss[0m : 4.77041
[1mStep[0m  [12/21], [94mLoss[0m : 4.57449
[1mStep[0m  [14/21], [94mLoss[0m : 4.67919
[1mStep[0m  [16/21], [94mLoss[0m : 4.75445
[1mStep[0m  [18/21], [94mLoss[0m : 4.79981
[1mStep[0m  [20/21], [94mLoss[0m : 4.44793

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.632, [92mTest[0m: 5.328, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.40113
[1mStep[0m  [2/21], [94mLoss[0m : 4.61750
[1mStep[0m  [4/21], [94mLoss[0m : 4.47273
[1mStep[0m  [6/21], [94mLoss[0m : 4.54100
[1mStep[0m  [8/21], [94mLoss[0m : 4.34101
[1mStep[0m  [10/21], [94mLoss[0m : 4.65101
[1mStep[0m  [12/21], [94mLoss[0m : 4.42811
[1mStep[0m  [14/21], [94mLoss[0m : 4.42507
[1mStep[0m  [16/21], [94mLoss[0m : 4.33766
[1mStep[0m  [18/21], [94mLoss[0m : 4.74392
[1mStep[0m  [20/21], [94mLoss[0m : 4.71222

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.526, [92mTest[0m: 5.224, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.48469
[1mStep[0m  [2/21], [94mLoss[0m : 4.63275
[1mStep[0m  [4/21], [94mLoss[0m : 4.29698
[1mStep[0m  [6/21], [94mLoss[0m : 4.40771
[1mStep[0m  [8/21], [94mLoss[0m : 4.23969
[1mStep[0m  [10/21], [94mLoss[0m : 4.25820
[1mStep[0m  [12/21], [94mLoss[0m : 4.54414
[1mStep[0m  [14/21], [94mLoss[0m : 4.38565
[1mStep[0m  [16/21], [94mLoss[0m : 4.47573
[1mStep[0m  [18/21], [94mLoss[0m : 4.68355
[1mStep[0m  [20/21], [94mLoss[0m : 4.29593

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.433, [92mTest[0m: 5.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.30957
[1mStep[0m  [2/21], [94mLoss[0m : 4.41984
[1mStep[0m  [4/21], [94mLoss[0m : 4.31127
[1mStep[0m  [6/21], [94mLoss[0m : 4.31754
[1mStep[0m  [8/21], [94mLoss[0m : 4.16348
[1mStep[0m  [10/21], [94mLoss[0m : 4.26643
[1mStep[0m  [12/21], [94mLoss[0m : 4.48040
[1mStep[0m  [14/21], [94mLoss[0m : 4.29862
[1mStep[0m  [16/21], [94mLoss[0m : 4.32668
[1mStep[0m  [18/21], [94mLoss[0m : 4.08191
[1mStep[0m  [20/21], [94mLoss[0m : 4.17551

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.327, [92mTest[0m: 4.978, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.33949
[1mStep[0m  [2/21], [94mLoss[0m : 4.28399
[1mStep[0m  [4/21], [94mLoss[0m : 4.18622
[1mStep[0m  [6/21], [94mLoss[0m : 4.07121
[1mStep[0m  [8/21], [94mLoss[0m : 4.41238
[1mStep[0m  [10/21], [94mLoss[0m : 4.21912
[1mStep[0m  [12/21], [94mLoss[0m : 4.39529
[1mStep[0m  [14/21], [94mLoss[0m : 4.19031
[1mStep[0m  [16/21], [94mLoss[0m : 4.17243
[1mStep[0m  [18/21], [94mLoss[0m : 4.04462
[1mStep[0m  [20/21], [94mLoss[0m : 4.29616

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.224, [92mTest[0m: 4.870, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.94239
[1mStep[0m  [2/21], [94mLoss[0m : 4.40262
[1mStep[0m  [4/21], [94mLoss[0m : 4.37133
[1mStep[0m  [6/21], [94mLoss[0m : 4.27457
[1mStep[0m  [8/21], [94mLoss[0m : 4.05860
[1mStep[0m  [10/21], [94mLoss[0m : 4.19435
[1mStep[0m  [12/21], [94mLoss[0m : 3.95571
[1mStep[0m  [14/21], [94mLoss[0m : 4.14623
[1mStep[0m  [16/21], [94mLoss[0m : 4.15972
[1mStep[0m  [18/21], [94mLoss[0m : 4.04877
[1mStep[0m  [20/21], [94mLoss[0m : 4.37435

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.144, [92mTest[0m: 4.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.31602
[1mStep[0m  [2/21], [94mLoss[0m : 4.11608
[1mStep[0m  [4/21], [94mLoss[0m : 4.23455
[1mStep[0m  [6/21], [94mLoss[0m : 3.82967
[1mStep[0m  [8/21], [94mLoss[0m : 4.16231
[1mStep[0m  [10/21], [94mLoss[0m : 3.86830
[1mStep[0m  [12/21], [94mLoss[0m : 4.08456
[1mStep[0m  [14/21], [94mLoss[0m : 4.02345
[1mStep[0m  [16/21], [94mLoss[0m : 4.05176
[1mStep[0m  [18/21], [94mLoss[0m : 4.12595
[1mStep[0m  [20/21], [94mLoss[0m : 3.78733

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.075, [92mTest[0m: 4.640, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.76225
[1mStep[0m  [2/21], [94mLoss[0m : 3.98210
[1mStep[0m  [4/21], [94mLoss[0m : 4.06840
[1mStep[0m  [6/21], [94mLoss[0m : 4.00011
[1mStep[0m  [8/21], [94mLoss[0m : 4.06998
[1mStep[0m  [10/21], [94mLoss[0m : 4.06776
[1mStep[0m  [12/21], [94mLoss[0m : 3.92391
[1mStep[0m  [14/21], [94mLoss[0m : 4.13348
[1mStep[0m  [16/21], [94mLoss[0m : 4.00203
[1mStep[0m  [18/21], [94mLoss[0m : 4.15863
[1mStep[0m  [20/21], [94mLoss[0m : 4.01668

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.995, [92mTest[0m: 4.528, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.95010
[1mStep[0m  [2/21], [94mLoss[0m : 3.89239
[1mStep[0m  [4/21], [94mLoss[0m : 3.99736
[1mStep[0m  [6/21], [94mLoss[0m : 4.01799
[1mStep[0m  [8/21], [94mLoss[0m : 3.88372
[1mStep[0m  [10/21], [94mLoss[0m : 3.79473
[1mStep[0m  [12/21], [94mLoss[0m : 3.88313
[1mStep[0m  [14/21], [94mLoss[0m : 3.68992
[1mStep[0m  [16/21], [94mLoss[0m : 3.81520
[1mStep[0m  [18/21], [94mLoss[0m : 3.90162
[1mStep[0m  [20/21], [94mLoss[0m : 3.82612

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.923, [92mTest[0m: 4.456, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.85582
[1mStep[0m  [2/21], [94mLoss[0m : 3.58221
[1mStep[0m  [4/21], [94mLoss[0m : 4.00671
[1mStep[0m  [6/21], [94mLoss[0m : 3.95336
[1mStep[0m  [8/21], [94mLoss[0m : 3.72590
[1mStep[0m  [10/21], [94mLoss[0m : 3.75495
[1mStep[0m  [12/21], [94mLoss[0m : 4.09262
[1mStep[0m  [14/21], [94mLoss[0m : 3.69971
[1mStep[0m  [16/21], [94mLoss[0m : 3.65267
[1mStep[0m  [18/21], [94mLoss[0m : 3.93239
[1mStep[0m  [20/21], [94mLoss[0m : 3.81080

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.839, [92mTest[0m: 4.349, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.73611
[1mStep[0m  [2/21], [94mLoss[0m : 3.76626
[1mStep[0m  [4/21], [94mLoss[0m : 3.77570
[1mStep[0m  [6/21], [94mLoss[0m : 3.85795
[1mStep[0m  [8/21], [94mLoss[0m : 3.68493
[1mStep[0m  [10/21], [94mLoss[0m : 3.87170
[1mStep[0m  [12/21], [94mLoss[0m : 3.74400
[1mStep[0m  [14/21], [94mLoss[0m : 3.87423
[1mStep[0m  [16/21], [94mLoss[0m : 3.88911
[1mStep[0m  [18/21], [94mLoss[0m : 3.75373
[1mStep[0m  [20/21], [94mLoss[0m : 3.79485

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.763, [92mTest[0m: 4.277, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.77810
[1mStep[0m  [2/21], [94mLoss[0m : 3.68366
[1mStep[0m  [4/21], [94mLoss[0m : 3.74497
[1mStep[0m  [6/21], [94mLoss[0m : 3.75119
[1mStep[0m  [8/21], [94mLoss[0m : 3.70994
[1mStep[0m  [10/21], [94mLoss[0m : 3.71309
[1mStep[0m  [12/21], [94mLoss[0m : 3.41782
[1mStep[0m  [14/21], [94mLoss[0m : 3.82898
[1mStep[0m  [16/21], [94mLoss[0m : 3.65578
[1mStep[0m  [18/21], [94mLoss[0m : 3.45261
[1mStep[0m  [20/21], [94mLoss[0m : 3.56213

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.706, [92mTest[0m: 4.160, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.62956
[1mStep[0m  [2/21], [94mLoss[0m : 3.70637
[1mStep[0m  [4/21], [94mLoss[0m : 3.39062
[1mStep[0m  [6/21], [94mLoss[0m : 3.67365
[1mStep[0m  [8/21], [94mLoss[0m : 3.66020
[1mStep[0m  [10/21], [94mLoss[0m : 3.72101
[1mStep[0m  [12/21], [94mLoss[0m : 3.49001
[1mStep[0m  [14/21], [94mLoss[0m : 3.54794
[1mStep[0m  [16/21], [94mLoss[0m : 3.45331
[1mStep[0m  [18/21], [94mLoss[0m : 3.53504
[1mStep[0m  [20/21], [94mLoss[0m : 3.62596

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.623, [92mTest[0m: 4.059, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.79229
[1mStep[0m  [2/21], [94mLoss[0m : 3.48561
[1mStep[0m  [4/21], [94mLoss[0m : 3.63886
[1mStep[0m  [6/21], [94mLoss[0m : 3.46424
[1mStep[0m  [8/21], [94mLoss[0m : 3.44169
[1mStep[0m  [10/21], [94mLoss[0m : 3.44541
[1mStep[0m  [12/21], [94mLoss[0m : 3.55481
[1mStep[0m  [14/21], [94mLoss[0m : 3.74727
[1mStep[0m  [16/21], [94mLoss[0m : 3.42887
[1mStep[0m  [18/21], [94mLoss[0m : 3.65716
[1mStep[0m  [20/21], [94mLoss[0m : 3.51653

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.584, [92mTest[0m: 4.000, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55035
[1mStep[0m  [2/21], [94mLoss[0m : 3.56198
[1mStep[0m  [4/21], [94mLoss[0m : 3.41136
[1mStep[0m  [6/21], [94mLoss[0m : 3.59196
[1mStep[0m  [8/21], [94mLoss[0m : 3.67817
[1mStep[0m  [10/21], [94mLoss[0m : 3.40224
[1mStep[0m  [12/21], [94mLoss[0m : 3.53714
[1mStep[0m  [14/21], [94mLoss[0m : 3.64773
[1mStep[0m  [16/21], [94mLoss[0m : 3.25154
[1mStep[0m  [18/21], [94mLoss[0m : 3.46351
[1mStep[0m  [20/21], [94mLoss[0m : 3.34642

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.515, [92mTest[0m: 3.925, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.823
====================================

Phase 2 - Evaluation MAE:  3.8226403849465505
MAE score P1      7.290859
MAE score P2       3.82264
loss              3.514953
learning_rate       0.0001
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.24822
[1mStep[0m  [4/42], [94mLoss[0m : 11.23449
[1mStep[0m  [8/42], [94mLoss[0m : 10.93053
[1mStep[0m  [12/42], [94mLoss[0m : 10.76593
[1mStep[0m  [16/42], [94mLoss[0m : 10.87237
[1mStep[0m  [20/42], [94mLoss[0m : 11.11217
[1mStep[0m  [24/42], [94mLoss[0m : 10.64451
[1mStep[0m  [28/42], [94mLoss[0m : 10.70846
[1mStep[0m  [32/42], [94mLoss[0m : 10.71704
[1mStep[0m  [36/42], [94mLoss[0m : 10.64515
[1mStep[0m  [40/42], [94mLoss[0m : 10.61534

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.812, [92mTest[0m: 11.031, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.39325
[1mStep[0m  [4/42], [94mLoss[0m : 10.83932
[1mStep[0m  [8/42], [94mLoss[0m : 10.34424
[1mStep[0m  [12/42], [94mLoss[0m : 10.72727
[1mStep[0m  [16/42], [94mLoss[0m : 10.33433
[1mStep[0m  [20/42], [94mLoss[0m : 10.32917
[1mStep[0m  [24/42], [94mLoss[0m : 9.89569
[1mStep[0m  [28/42], [94mLoss[0m : 10.35474
[1mStep[0m  [32/42], [94mLoss[0m : 10.29029
[1mStep[0m  [36/42], [94mLoss[0m : 10.09519
[1mStep[0m  [40/42], [94mLoss[0m : 9.84022

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.181, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.66732
[1mStep[0m  [4/42], [94mLoss[0m : 9.89496
[1mStep[0m  [8/42], [94mLoss[0m : 10.03918
[1mStep[0m  [12/42], [94mLoss[0m : 9.85003
[1mStep[0m  [16/42], [94mLoss[0m : 9.71315
[1mStep[0m  [20/42], [94mLoss[0m : 9.42801
[1mStep[0m  [24/42], [94mLoss[0m : 9.52116
[1mStep[0m  [28/42], [94mLoss[0m : 9.40993
[1mStep[0m  [32/42], [94mLoss[0m : 9.57607
[1mStep[0m  [36/42], [94mLoss[0m : 9.18917
[1mStep[0m  [40/42], [94mLoss[0m : 9.55772

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.509, [92mTest[0m: 10.139, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.25941
[1mStep[0m  [4/42], [94mLoss[0m : 9.37904
[1mStep[0m  [8/42], [94mLoss[0m : 8.91560
[1mStep[0m  [12/42], [94mLoss[0m : 9.10219
[1mStep[0m  [16/42], [94mLoss[0m : 8.62568
[1mStep[0m  [20/42], [94mLoss[0m : 8.89935
[1mStep[0m  [24/42], [94mLoss[0m : 8.97171
[1mStep[0m  [28/42], [94mLoss[0m : 8.91676
[1mStep[0m  [32/42], [94mLoss[0m : 8.95330
[1mStep[0m  [36/42], [94mLoss[0m : 8.72508
[1mStep[0m  [40/42], [94mLoss[0m : 8.19646

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.832, [92mTest[0m: 9.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.65917
[1mStep[0m  [4/42], [94mLoss[0m : 8.46058
[1mStep[0m  [8/42], [94mLoss[0m : 8.49019
[1mStep[0m  [12/42], [94mLoss[0m : 8.23981
[1mStep[0m  [16/42], [94mLoss[0m : 8.04579
[1mStep[0m  [20/42], [94mLoss[0m : 8.31405
[1mStep[0m  [24/42], [94mLoss[0m : 7.75259
[1mStep[0m  [28/42], [94mLoss[0m : 7.81436
[1mStep[0m  [32/42], [94mLoss[0m : 8.47936
[1mStep[0m  [36/42], [94mLoss[0m : 7.92880
[1mStep[0m  [40/42], [94mLoss[0m : 7.70944

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.154, [92mTest[0m: 9.153, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.88563
[1mStep[0m  [4/42], [94mLoss[0m : 7.54198
[1mStep[0m  [8/42], [94mLoss[0m : 7.81149
[1mStep[0m  [12/42], [94mLoss[0m : 7.34904
[1mStep[0m  [16/42], [94mLoss[0m : 7.45281
[1mStep[0m  [20/42], [94mLoss[0m : 7.15866
[1mStep[0m  [24/42], [94mLoss[0m : 7.38636
[1mStep[0m  [28/42], [94mLoss[0m : 7.21822
[1mStep[0m  [32/42], [94mLoss[0m : 7.39258
[1mStep[0m  [36/42], [94mLoss[0m : 7.63999
[1mStep[0m  [40/42], [94mLoss[0m : 7.40594

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.506, [92mTest[0m: 8.636, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.65433
[1mStep[0m  [4/42], [94mLoss[0m : 7.03043
[1mStep[0m  [8/42], [94mLoss[0m : 7.03988
[1mStep[0m  [12/42], [94mLoss[0m : 6.92776
[1mStep[0m  [16/42], [94mLoss[0m : 6.87799
[1mStep[0m  [20/42], [94mLoss[0m : 6.65269
[1mStep[0m  [24/42], [94mLoss[0m : 6.55751
[1mStep[0m  [28/42], [94mLoss[0m : 6.54199
[1mStep[0m  [32/42], [94mLoss[0m : 6.61796
[1mStep[0m  [36/42], [94mLoss[0m : 7.13201
[1mStep[0m  [40/42], [94mLoss[0m : 6.39345

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.846, [92mTest[0m: 8.130, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.39769
[1mStep[0m  [4/42], [94mLoss[0m : 6.11662
[1mStep[0m  [8/42], [94mLoss[0m : 6.18777
[1mStep[0m  [12/42], [94mLoss[0m : 6.25606
[1mStep[0m  [16/42], [94mLoss[0m : 6.51258
[1mStep[0m  [20/42], [94mLoss[0m : 5.98248
[1mStep[0m  [24/42], [94mLoss[0m : 5.84859
[1mStep[0m  [28/42], [94mLoss[0m : 5.65494
[1mStep[0m  [32/42], [94mLoss[0m : 6.17525
[1mStep[0m  [36/42], [94mLoss[0m : 6.36295
[1mStep[0m  [40/42], [94mLoss[0m : 5.92503

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.195, [92mTest[0m: 7.592, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.02258
[1mStep[0m  [4/42], [94mLoss[0m : 5.72679
[1mStep[0m  [8/42], [94mLoss[0m : 5.92266
[1mStep[0m  [12/42], [94mLoss[0m : 5.52545
[1mStep[0m  [16/42], [94mLoss[0m : 5.60181
[1mStep[0m  [20/42], [94mLoss[0m : 5.85782
[1mStep[0m  [24/42], [94mLoss[0m : 5.67312
[1mStep[0m  [28/42], [94mLoss[0m : 5.68899
[1mStep[0m  [32/42], [94mLoss[0m : 5.20554
[1mStep[0m  [36/42], [94mLoss[0m : 5.34032
[1mStep[0m  [40/42], [94mLoss[0m : 5.18967

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.595, [92mTest[0m: 7.028, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.83727
[1mStep[0m  [4/42], [94mLoss[0m : 5.24356
[1mStep[0m  [8/42], [94mLoss[0m : 4.85208
[1mStep[0m  [12/42], [94mLoss[0m : 5.37083
[1mStep[0m  [16/42], [94mLoss[0m : 4.73664
[1mStep[0m  [20/42], [94mLoss[0m : 5.09649
[1mStep[0m  [24/42], [94mLoss[0m : 4.88076
[1mStep[0m  [28/42], [94mLoss[0m : 4.43498
[1mStep[0m  [32/42], [94mLoss[0m : 4.87784
[1mStep[0m  [36/42], [94mLoss[0m : 4.69502
[1mStep[0m  [40/42], [94mLoss[0m : 4.15847

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.991, [92mTest[0m: 6.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.30948
[1mStep[0m  [4/42], [94mLoss[0m : 4.43505
[1mStep[0m  [8/42], [94mLoss[0m : 4.57517
[1mStep[0m  [12/42], [94mLoss[0m : 4.64320
[1mStep[0m  [16/42], [94mLoss[0m : 4.39724
[1mStep[0m  [20/42], [94mLoss[0m : 4.15582
[1mStep[0m  [24/42], [94mLoss[0m : 4.56524
[1mStep[0m  [28/42], [94mLoss[0m : 4.39144
[1mStep[0m  [32/42], [94mLoss[0m : 4.54650
[1mStep[0m  [36/42], [94mLoss[0m : 4.11329
[1mStep[0m  [40/42], [94mLoss[0m : 4.30476

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.469, [92mTest[0m: 5.771, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 10 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.197
====================================

Phase 1 - Evaluation MAE:  5.196770599910191
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 4.16178
[1mStep[0m  [4/42], [94mLoss[0m : 4.04808
[1mStep[0m  [8/42], [94mLoss[0m : 4.35074
[1mStep[0m  [12/42], [94mLoss[0m : 4.27646
[1mStep[0m  [16/42], [94mLoss[0m : 4.29305
[1mStep[0m  [20/42], [94mLoss[0m : 4.44783
[1mStep[0m  [24/42], [94mLoss[0m : 3.72783
[1mStep[0m  [28/42], [94mLoss[0m : 3.86409
[1mStep[0m  [32/42], [94mLoss[0m : 4.23716
[1mStep[0m  [36/42], [94mLoss[0m : 4.04577
[1mStep[0m  [40/42], [94mLoss[0m : 3.81029

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.057, [92mTest[0m: 5.205, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.950
====================================

Phase 2 - Evaluation MAE:  4.950381687709263
MAE score P1        5.196771
MAE score P2        4.950382
loss                4.056811
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay            0.01
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.28879
[1mStep[0m  [4/42], [94mLoss[0m : 11.09825
[1mStep[0m  [8/42], [94mLoss[0m : 11.15556
[1mStep[0m  [12/42], [94mLoss[0m : 10.91692
[1mStep[0m  [16/42], [94mLoss[0m : 11.37892
[1mStep[0m  [20/42], [94mLoss[0m : 11.48305
[1mStep[0m  [24/42], [94mLoss[0m : 10.98060
[1mStep[0m  [28/42], [94mLoss[0m : 10.97584
[1mStep[0m  [32/42], [94mLoss[0m : 11.14707
[1mStep[0m  [36/42], [94mLoss[0m : 11.33263
[1mStep[0m  [40/42], [94mLoss[0m : 10.80521

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.224, [92mTest[0m: 11.295, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.33710
[1mStep[0m  [4/42], [94mLoss[0m : 11.76149
[1mStep[0m  [8/42], [94mLoss[0m : 10.78778
[1mStep[0m  [12/42], [94mLoss[0m : 11.23542
[1mStep[0m  [16/42], [94mLoss[0m : 11.44287
[1mStep[0m  [20/42], [94mLoss[0m : 11.07012
[1mStep[0m  [24/42], [94mLoss[0m : 11.25587
[1mStep[0m  [28/42], [94mLoss[0m : 11.45192
[1mStep[0m  [32/42], [94mLoss[0m : 11.44699
[1mStep[0m  [36/42], [94mLoss[0m : 10.90384
[1mStep[0m  [40/42], [94mLoss[0m : 11.18679

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 11.103, [92mTest[0m: 11.162, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95131
[1mStep[0m  [4/42], [94mLoss[0m : 11.17584
[1mStep[0m  [8/42], [94mLoss[0m : 11.23643
[1mStep[0m  [12/42], [94mLoss[0m : 11.13490
[1mStep[0m  [16/42], [94mLoss[0m : 11.16556
[1mStep[0m  [20/42], [94mLoss[0m : 11.05455
[1mStep[0m  [24/42], [94mLoss[0m : 11.06672
[1mStep[0m  [28/42], [94mLoss[0m : 10.50133
[1mStep[0m  [32/42], [94mLoss[0m : 10.87207
[1mStep[0m  [36/42], [94mLoss[0m : 10.93341
[1mStep[0m  [40/42], [94mLoss[0m : 10.98955

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.979, [92mTest[0m: 11.030, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76219
[1mStep[0m  [4/42], [94mLoss[0m : 11.03562
[1mStep[0m  [8/42], [94mLoss[0m : 10.74588
[1mStep[0m  [12/42], [94mLoss[0m : 10.66838
[1mStep[0m  [16/42], [94mLoss[0m : 10.71356
[1mStep[0m  [20/42], [94mLoss[0m : 10.99488
[1mStep[0m  [24/42], [94mLoss[0m : 10.69511
[1mStep[0m  [28/42], [94mLoss[0m : 10.66838
[1mStep[0m  [32/42], [94mLoss[0m : 10.71648
[1mStep[0m  [36/42], [94mLoss[0m : 11.01777
[1mStep[0m  [40/42], [94mLoss[0m : 10.96263

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.860, [92mTest[0m: 10.906, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90045
[1mStep[0m  [4/42], [94mLoss[0m : 10.73523
[1mStep[0m  [8/42], [94mLoss[0m : 11.10631
[1mStep[0m  [12/42], [94mLoss[0m : 10.89223
[1mStep[0m  [16/42], [94mLoss[0m : 10.88356
[1mStep[0m  [20/42], [94mLoss[0m : 10.32967
[1mStep[0m  [24/42], [94mLoss[0m : 10.64989
[1mStep[0m  [28/42], [94mLoss[0m : 10.99368
[1mStep[0m  [32/42], [94mLoss[0m : 10.45860
[1mStep[0m  [36/42], [94mLoss[0m : 11.16089
[1mStep[0m  [40/42], [94mLoss[0m : 10.56518

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71479
[1mStep[0m  [4/42], [94mLoss[0m : 10.74734
[1mStep[0m  [8/42], [94mLoss[0m : 11.04299
[1mStep[0m  [12/42], [94mLoss[0m : 11.12420
[1mStep[0m  [16/42], [94mLoss[0m : 10.35729
[1mStep[0m  [20/42], [94mLoss[0m : 10.43514
[1mStep[0m  [24/42], [94mLoss[0m : 10.47127
[1mStep[0m  [28/42], [94mLoss[0m : 10.39311
[1mStep[0m  [32/42], [94mLoss[0m : 10.74533
[1mStep[0m  [36/42], [94mLoss[0m : 10.41357
[1mStep[0m  [40/42], [94mLoss[0m : 10.92615

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08676
[1mStep[0m  [4/42], [94mLoss[0m : 10.59101
[1mStep[0m  [8/42], [94mLoss[0m : 10.58672
[1mStep[0m  [12/42], [94mLoss[0m : 10.35727
[1mStep[0m  [16/42], [94mLoss[0m : 10.62475
[1mStep[0m  [20/42], [94mLoss[0m : 10.47167
[1mStep[0m  [24/42], [94mLoss[0m : 10.60871
[1mStep[0m  [28/42], [94mLoss[0m : 10.75817
[1mStep[0m  [32/42], [94mLoss[0m : 10.10710
[1mStep[0m  [36/42], [94mLoss[0m : 10.27273
[1mStep[0m  [40/42], [94mLoss[0m : 10.33235

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.497, [92mTest[0m: 10.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.75987
[1mStep[0m  [4/42], [94mLoss[0m : 10.68788
[1mStep[0m  [8/42], [94mLoss[0m : 10.17095
[1mStep[0m  [12/42], [94mLoss[0m : 10.29875
[1mStep[0m  [16/42], [94mLoss[0m : 9.92832
[1mStep[0m  [20/42], [94mLoss[0m : 10.46258
[1mStep[0m  [24/42], [94mLoss[0m : 10.33290
[1mStep[0m  [28/42], [94mLoss[0m : 10.32612
[1mStep[0m  [32/42], [94mLoss[0m : 10.63154
[1mStep[0m  [36/42], [94mLoss[0m : 10.46485
[1mStep[0m  [40/42], [94mLoss[0m : 10.65966

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.388, [92mTest[0m: 10.420, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.68157
[1mStep[0m  [4/42], [94mLoss[0m : 9.94042
[1mStep[0m  [8/42], [94mLoss[0m : 10.62324
[1mStep[0m  [12/42], [94mLoss[0m : 10.52683
[1mStep[0m  [16/42], [94mLoss[0m : 10.27389
[1mStep[0m  [20/42], [94mLoss[0m : 10.27976
[1mStep[0m  [24/42], [94mLoss[0m : 10.10831
[1mStep[0m  [28/42], [94mLoss[0m : 10.26494
[1mStep[0m  [32/42], [94mLoss[0m : 10.17850
[1mStep[0m  [36/42], [94mLoss[0m : 10.23175
[1mStep[0m  [40/42], [94mLoss[0m : 10.50040

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.260, [92mTest[0m: 10.305, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46302
[1mStep[0m  [4/42], [94mLoss[0m : 10.44179
[1mStep[0m  [8/42], [94mLoss[0m : 9.95130
[1mStep[0m  [12/42], [94mLoss[0m : 10.42064
[1mStep[0m  [16/42], [94mLoss[0m : 10.23403
[1mStep[0m  [20/42], [94mLoss[0m : 9.89837
[1mStep[0m  [24/42], [94mLoss[0m : 9.98800
[1mStep[0m  [28/42], [94mLoss[0m : 10.38156
[1mStep[0m  [32/42], [94mLoss[0m : 9.81874
[1mStep[0m  [36/42], [94mLoss[0m : 10.07772
[1mStep[0m  [40/42], [94mLoss[0m : 9.94019

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.137, [92mTest[0m: 10.187, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.96907
[1mStep[0m  [4/42], [94mLoss[0m : 10.09923
[1mStep[0m  [8/42], [94mLoss[0m : 9.61813
[1mStep[0m  [12/42], [94mLoss[0m : 10.01481
[1mStep[0m  [16/42], [94mLoss[0m : 9.86376
[1mStep[0m  [20/42], [94mLoss[0m : 9.92101
[1mStep[0m  [24/42], [94mLoss[0m : 10.05552
[1mStep[0m  [28/42], [94mLoss[0m : 9.79143
[1mStep[0m  [32/42], [94mLoss[0m : 9.69404
[1mStep[0m  [36/42], [94mLoss[0m : 10.04018
[1mStep[0m  [40/42], [94mLoss[0m : 9.33955

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.013, [92mTest[0m: 10.066, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.94431
[1mStep[0m  [4/42], [94mLoss[0m : 10.34047
[1mStep[0m  [8/42], [94mLoss[0m : 9.94998
[1mStep[0m  [12/42], [94mLoss[0m : 9.75353
[1mStep[0m  [16/42], [94mLoss[0m : 9.91836
[1mStep[0m  [20/42], [94mLoss[0m : 10.00914
[1mStep[0m  [24/42], [94mLoss[0m : 10.08763
[1mStep[0m  [28/42], [94mLoss[0m : 9.95679
[1mStep[0m  [32/42], [94mLoss[0m : 9.46627
[1mStep[0m  [36/42], [94mLoss[0m : 9.86956
[1mStep[0m  [40/42], [94mLoss[0m : 10.17171

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.901, [92mTest[0m: 9.928, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.86916
[1mStep[0m  [4/42], [94mLoss[0m : 10.00939
[1mStep[0m  [8/42], [94mLoss[0m : 9.75101
[1mStep[0m  [12/42], [94mLoss[0m : 9.78466
[1mStep[0m  [16/42], [94mLoss[0m : 9.78420
[1mStep[0m  [20/42], [94mLoss[0m : 9.31316
[1mStep[0m  [24/42], [94mLoss[0m : 9.69128
[1mStep[0m  [28/42], [94mLoss[0m : 9.68484
[1mStep[0m  [32/42], [94mLoss[0m : 9.53105
[1mStep[0m  [36/42], [94mLoss[0m : 9.89026
[1mStep[0m  [40/42], [94mLoss[0m : 9.70985

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.772, [92mTest[0m: 9.823, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.43239
[1mStep[0m  [4/42], [94mLoss[0m : 9.56601
[1mStep[0m  [8/42], [94mLoss[0m : 9.65451
[1mStep[0m  [12/42], [94mLoss[0m : 9.92629
[1mStep[0m  [16/42], [94mLoss[0m : 9.58600
[1mStep[0m  [20/42], [94mLoss[0m : 9.53678
[1mStep[0m  [24/42], [94mLoss[0m : 9.51517
[1mStep[0m  [28/42], [94mLoss[0m : 9.60109
[1mStep[0m  [32/42], [94mLoss[0m : 9.60812
[1mStep[0m  [36/42], [94mLoss[0m : 9.73871
[1mStep[0m  [40/42], [94mLoss[0m : 9.94319

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.655, [92mTest[0m: 9.699, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.31967
[1mStep[0m  [4/42], [94mLoss[0m : 9.76442
[1mStep[0m  [8/42], [94mLoss[0m : 9.70078
[1mStep[0m  [12/42], [94mLoss[0m : 9.88828
[1mStep[0m  [16/42], [94mLoss[0m : 9.88031
[1mStep[0m  [20/42], [94mLoss[0m : 9.52050
[1mStep[0m  [24/42], [94mLoss[0m : 9.58004
[1mStep[0m  [28/42], [94mLoss[0m : 9.65993
[1mStep[0m  [32/42], [94mLoss[0m : 9.68257
[1mStep[0m  [36/42], [94mLoss[0m : 9.43637
[1mStep[0m  [40/42], [94mLoss[0m : 9.46020

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.524, [92mTest[0m: 9.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.57475
[1mStep[0m  [4/42], [94mLoss[0m : 9.42543
[1mStep[0m  [8/42], [94mLoss[0m : 9.69330
[1mStep[0m  [12/42], [94mLoss[0m : 9.54479
[1mStep[0m  [16/42], [94mLoss[0m : 9.39841
[1mStep[0m  [20/42], [94mLoss[0m : 9.54315
[1mStep[0m  [24/42], [94mLoss[0m : 9.15313
[1mStep[0m  [28/42], [94mLoss[0m : 9.27029
[1mStep[0m  [32/42], [94mLoss[0m : 9.20476
[1mStep[0m  [36/42], [94mLoss[0m : 9.36521
[1mStep[0m  [40/42], [94mLoss[0m : 9.39998

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.400, [92mTest[0m: 9.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.51788
[1mStep[0m  [4/42], [94mLoss[0m : 8.90271
[1mStep[0m  [8/42], [94mLoss[0m : 9.61795
[1mStep[0m  [12/42], [94mLoss[0m : 9.29478
[1mStep[0m  [16/42], [94mLoss[0m : 8.94802
[1mStep[0m  [20/42], [94mLoss[0m : 9.08947
[1mStep[0m  [24/42], [94mLoss[0m : 9.46414
[1mStep[0m  [28/42], [94mLoss[0m : 9.05792
[1mStep[0m  [32/42], [94mLoss[0m : 9.30423
[1mStep[0m  [36/42], [94mLoss[0m : 8.81155
[1mStep[0m  [40/42], [94mLoss[0m : 9.40957

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.282, [92mTest[0m: 9.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.56494
[1mStep[0m  [4/42], [94mLoss[0m : 9.30448
[1mStep[0m  [8/42], [94mLoss[0m : 9.04306
[1mStep[0m  [12/42], [94mLoss[0m : 9.20003
[1mStep[0m  [16/42], [94mLoss[0m : 9.21222
[1mStep[0m  [20/42], [94mLoss[0m : 9.20328
[1mStep[0m  [24/42], [94mLoss[0m : 9.33101
[1mStep[0m  [28/42], [94mLoss[0m : 8.89925
[1mStep[0m  [32/42], [94mLoss[0m : 9.34220
[1mStep[0m  [36/42], [94mLoss[0m : 9.01673
[1mStep[0m  [40/42], [94mLoss[0m : 9.33431

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.155, [92mTest[0m: 9.210, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.37081
[1mStep[0m  [4/42], [94mLoss[0m : 9.13114
[1mStep[0m  [8/42], [94mLoss[0m : 8.74992
[1mStep[0m  [12/42], [94mLoss[0m : 9.04172
[1mStep[0m  [16/42], [94mLoss[0m : 9.06216
[1mStep[0m  [20/42], [94mLoss[0m : 8.64512
[1mStep[0m  [24/42], [94mLoss[0m : 8.76840
[1mStep[0m  [28/42], [94mLoss[0m : 8.84179
[1mStep[0m  [32/42], [94mLoss[0m : 8.98900
[1mStep[0m  [36/42], [94mLoss[0m : 9.12704
[1mStep[0m  [40/42], [94mLoss[0m : 8.59087

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.039, [92mTest[0m: 9.078, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.95735
[1mStep[0m  [4/42], [94mLoss[0m : 8.65215
[1mStep[0m  [8/42], [94mLoss[0m : 9.12733
[1mStep[0m  [12/42], [94mLoss[0m : 8.71334
[1mStep[0m  [16/42], [94mLoss[0m : 8.50074
[1mStep[0m  [20/42], [94mLoss[0m : 9.01631
[1mStep[0m  [24/42], [94mLoss[0m : 8.64325
[1mStep[0m  [28/42], [94mLoss[0m : 8.39250
[1mStep[0m  [32/42], [94mLoss[0m : 8.78183
[1mStep[0m  [36/42], [94mLoss[0m : 8.57412
[1mStep[0m  [40/42], [94mLoss[0m : 8.97537

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.928, [92mTest[0m: 8.981, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.39536
[1mStep[0m  [4/42], [94mLoss[0m : 8.94844
[1mStep[0m  [8/42], [94mLoss[0m : 8.73669
[1mStep[0m  [12/42], [94mLoss[0m : 8.56857
[1mStep[0m  [16/42], [94mLoss[0m : 9.13333
[1mStep[0m  [20/42], [94mLoss[0m : 8.64479
[1mStep[0m  [24/42], [94mLoss[0m : 9.19085
[1mStep[0m  [28/42], [94mLoss[0m : 8.55785
[1mStep[0m  [32/42], [94mLoss[0m : 8.91683
[1mStep[0m  [36/42], [94mLoss[0m : 8.49734
[1mStep[0m  [40/42], [94mLoss[0m : 8.80442

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.805, [92mTest[0m: 8.841, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34331
[1mStep[0m  [4/42], [94mLoss[0m : 8.99819
[1mStep[0m  [8/42], [94mLoss[0m : 8.39975
[1mStep[0m  [12/42], [94mLoss[0m : 8.45094
[1mStep[0m  [16/42], [94mLoss[0m : 8.77671
[1mStep[0m  [20/42], [94mLoss[0m : 8.97552
[1mStep[0m  [24/42], [94mLoss[0m : 8.60193
[1mStep[0m  [28/42], [94mLoss[0m : 8.60983
[1mStep[0m  [32/42], [94mLoss[0m : 8.98598
[1mStep[0m  [36/42], [94mLoss[0m : 8.34996
[1mStep[0m  [40/42], [94mLoss[0m : 8.64640

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.698, [92mTest[0m: 8.739, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.37454
[1mStep[0m  [4/42], [94mLoss[0m : 8.46626
[1mStep[0m  [8/42], [94mLoss[0m : 9.09061
[1mStep[0m  [12/42], [94mLoss[0m : 8.45261
[1mStep[0m  [16/42], [94mLoss[0m : 9.06264
[1mStep[0m  [20/42], [94mLoss[0m : 8.59814
[1mStep[0m  [24/42], [94mLoss[0m : 8.60049
[1mStep[0m  [28/42], [94mLoss[0m : 8.30712
[1mStep[0m  [32/42], [94mLoss[0m : 8.84610
[1mStep[0m  [36/42], [94mLoss[0m : 8.73695
[1mStep[0m  [40/42], [94mLoss[0m : 8.59880

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.591, [92mTest[0m: 8.624, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.78475
[1mStep[0m  [4/42], [94mLoss[0m : 8.59712
[1mStep[0m  [8/42], [94mLoss[0m : 8.65653
[1mStep[0m  [12/42], [94mLoss[0m : 8.48574
[1mStep[0m  [16/42], [94mLoss[0m : 8.51142
[1mStep[0m  [20/42], [94mLoss[0m : 8.77799
[1mStep[0m  [24/42], [94mLoss[0m : 8.34138
[1mStep[0m  [28/42], [94mLoss[0m : 8.27431
[1mStep[0m  [32/42], [94mLoss[0m : 8.47197
[1mStep[0m  [36/42], [94mLoss[0m : 8.83921
[1mStep[0m  [40/42], [94mLoss[0m : 8.11711

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.474, [92mTest[0m: 8.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.86041
[1mStep[0m  [4/42], [94mLoss[0m : 7.88774
[1mStep[0m  [8/42], [94mLoss[0m : 8.03723
[1mStep[0m  [12/42], [94mLoss[0m : 8.10876
[1mStep[0m  [16/42], [94mLoss[0m : 8.42024
[1mStep[0m  [20/42], [94mLoss[0m : 8.76060
[1mStep[0m  [24/42], [94mLoss[0m : 8.51639
[1mStep[0m  [28/42], [94mLoss[0m : 8.56439
[1mStep[0m  [32/42], [94mLoss[0m : 8.25731
[1mStep[0m  [36/42], [94mLoss[0m : 8.23820
[1mStep[0m  [40/42], [94mLoss[0m : 8.41995

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.367, [92mTest[0m: 8.410, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27485
[1mStep[0m  [4/42], [94mLoss[0m : 8.08866
[1mStep[0m  [8/42], [94mLoss[0m : 8.23229
[1mStep[0m  [12/42], [94mLoss[0m : 7.87774
[1mStep[0m  [16/42], [94mLoss[0m : 8.69068
[1mStep[0m  [20/42], [94mLoss[0m : 8.33871
[1mStep[0m  [24/42], [94mLoss[0m : 8.03515
[1mStep[0m  [28/42], [94mLoss[0m : 8.19614
[1mStep[0m  [32/42], [94mLoss[0m : 8.36408
[1mStep[0m  [36/42], [94mLoss[0m : 8.37081
[1mStep[0m  [40/42], [94mLoss[0m : 8.34678

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.254, [92mTest[0m: 8.313, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.28846
[1mStep[0m  [4/42], [94mLoss[0m : 8.05380
[1mStep[0m  [8/42], [94mLoss[0m : 8.22441
[1mStep[0m  [12/42], [94mLoss[0m : 8.24147
[1mStep[0m  [16/42], [94mLoss[0m : 8.21960
[1mStep[0m  [20/42], [94mLoss[0m : 8.29711
[1mStep[0m  [24/42], [94mLoss[0m : 7.82119
[1mStep[0m  [28/42], [94mLoss[0m : 8.48285
[1mStep[0m  [32/42], [94mLoss[0m : 8.31525
[1mStep[0m  [36/42], [94mLoss[0m : 8.03558
[1mStep[0m  [40/42], [94mLoss[0m : 8.40582

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.150, [92mTest[0m: 8.189, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.31390
[1mStep[0m  [4/42], [94mLoss[0m : 8.05174
[1mStep[0m  [8/42], [94mLoss[0m : 8.24907
[1mStep[0m  [12/42], [94mLoss[0m : 7.83172
[1mStep[0m  [16/42], [94mLoss[0m : 8.46442
[1mStep[0m  [20/42], [94mLoss[0m : 8.11625
[1mStep[0m  [24/42], [94mLoss[0m : 8.07263
[1mStep[0m  [28/42], [94mLoss[0m : 7.94191
[1mStep[0m  [32/42], [94mLoss[0m : 8.36525
[1mStep[0m  [36/42], [94mLoss[0m : 8.09687
[1mStep[0m  [40/42], [94mLoss[0m : 8.14820

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.048, [92mTest[0m: 8.070, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.38992
[1mStep[0m  [4/42], [94mLoss[0m : 8.43274
[1mStep[0m  [8/42], [94mLoss[0m : 8.09335
[1mStep[0m  [12/42], [94mLoss[0m : 7.79906
[1mStep[0m  [16/42], [94mLoss[0m : 8.10292
[1mStep[0m  [20/42], [94mLoss[0m : 7.70729
[1mStep[0m  [24/42], [94mLoss[0m : 7.86660
[1mStep[0m  [28/42], [94mLoss[0m : 7.83245
[1mStep[0m  [32/42], [94mLoss[0m : 7.68142
[1mStep[0m  [36/42], [94mLoss[0m : 7.75261
[1mStep[0m  [40/42], [94mLoss[0m : 7.39391

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.928, [92mTest[0m: 7.974, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.07578
[1mStep[0m  [4/42], [94mLoss[0m : 7.87126
[1mStep[0m  [8/42], [94mLoss[0m : 7.81929
[1mStep[0m  [12/42], [94mLoss[0m : 7.46850
[1mStep[0m  [16/42], [94mLoss[0m : 8.19508
[1mStep[0m  [20/42], [94mLoss[0m : 8.00574
[1mStep[0m  [24/42], [94mLoss[0m : 7.60878
[1mStep[0m  [28/42], [94mLoss[0m : 7.96434
[1mStep[0m  [32/42], [94mLoss[0m : 7.55853
[1mStep[0m  [36/42], [94mLoss[0m : 8.24625
[1mStep[0m  [40/42], [94mLoss[0m : 7.49670

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.832, [92mTest[0m: 7.862, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.745
====================================

Phase 1 - Evaluation MAE:  7.74477618081229
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 7.77888
[1mStep[0m  [4/42], [94mLoss[0m : 7.49893
[1mStep[0m  [8/42], [94mLoss[0m : 7.96199
[1mStep[0m  [12/42], [94mLoss[0m : 7.21044
[1mStep[0m  [16/42], [94mLoss[0m : 7.43627
[1mStep[0m  [20/42], [94mLoss[0m : 7.47313
[1mStep[0m  [24/42], [94mLoss[0m : 7.79624
[1mStep[0m  [28/42], [94mLoss[0m : 7.47820
[1mStep[0m  [32/42], [94mLoss[0m : 8.03212
[1mStep[0m  [36/42], [94mLoss[0m : 7.81640
[1mStep[0m  [40/42], [94mLoss[0m : 7.94809

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.707, [92mTest[0m: 7.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.74302
[1mStep[0m  [4/42], [94mLoss[0m : 7.33313
[1mStep[0m  [8/42], [94mLoss[0m : 7.94837
[1mStep[0m  [12/42], [94mLoss[0m : 7.33908
[1mStep[0m  [16/42], [94mLoss[0m : 7.68921
[1mStep[0m  [20/42], [94mLoss[0m : 7.71065
[1mStep[0m  [24/42], [94mLoss[0m : 7.69096
[1mStep[0m  [28/42], [94mLoss[0m : 7.38698
[1mStep[0m  [32/42], [94mLoss[0m : 7.43406
[1mStep[0m  [36/42], [94mLoss[0m : 7.99544
[1mStep[0m  [40/42], [94mLoss[0m : 7.45653

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.577, [92mTest[0m: 7.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.39764
[1mStep[0m  [4/42], [94mLoss[0m : 7.30537
[1mStep[0m  [8/42], [94mLoss[0m : 7.60489
[1mStep[0m  [12/42], [94mLoss[0m : 7.63866
[1mStep[0m  [16/42], [94mLoss[0m : 7.41147
[1mStep[0m  [20/42], [94mLoss[0m : 7.46749
[1mStep[0m  [24/42], [94mLoss[0m : 7.68523
[1mStep[0m  [28/42], [94mLoss[0m : 7.21514
[1mStep[0m  [32/42], [94mLoss[0m : 7.53517
[1mStep[0m  [36/42], [94mLoss[0m : 7.13694
[1mStep[0m  [40/42], [94mLoss[0m : 7.42246

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.458, [92mTest[0m: 7.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.41847
[1mStep[0m  [4/42], [94mLoss[0m : 7.66584
[1mStep[0m  [8/42], [94mLoss[0m : 7.19660
[1mStep[0m  [12/42], [94mLoss[0m : 7.25208
[1mStep[0m  [16/42], [94mLoss[0m : 7.31139
[1mStep[0m  [20/42], [94mLoss[0m : 7.33836
[1mStep[0m  [24/42], [94mLoss[0m : 6.99508
[1mStep[0m  [28/42], [94mLoss[0m : 7.38085
[1mStep[0m  [32/42], [94mLoss[0m : 6.83107
[1mStep[0m  [36/42], [94mLoss[0m : 6.98119
[1mStep[0m  [40/42], [94mLoss[0m : 6.92331

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.337, [92mTest[0m: 7.377, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.20413
[1mStep[0m  [4/42], [94mLoss[0m : 7.52447
[1mStep[0m  [8/42], [94mLoss[0m : 7.26821
[1mStep[0m  [12/42], [94mLoss[0m : 7.10891
[1mStep[0m  [16/42], [94mLoss[0m : 7.10070
[1mStep[0m  [20/42], [94mLoss[0m : 6.92208
[1mStep[0m  [24/42], [94mLoss[0m : 7.18810
[1mStep[0m  [28/42], [94mLoss[0m : 7.19704
[1mStep[0m  [32/42], [94mLoss[0m : 7.04988
[1mStep[0m  [36/42], [94mLoss[0m : 7.41301
[1mStep[0m  [40/42], [94mLoss[0m : 6.76999

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.222, [92mTest[0m: 7.258, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.21186
[1mStep[0m  [4/42], [94mLoss[0m : 7.27356
[1mStep[0m  [8/42], [94mLoss[0m : 6.80169
[1mStep[0m  [12/42], [94mLoss[0m : 7.13772
[1mStep[0m  [16/42], [94mLoss[0m : 7.09179
[1mStep[0m  [20/42], [94mLoss[0m : 7.31878
[1mStep[0m  [24/42], [94mLoss[0m : 7.01596
[1mStep[0m  [28/42], [94mLoss[0m : 6.87090
[1mStep[0m  [32/42], [94mLoss[0m : 6.92436
[1mStep[0m  [36/42], [94mLoss[0m : 7.33233
[1mStep[0m  [40/42], [94mLoss[0m : 7.47874

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.092, [92mTest[0m: 7.137, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.93945
[1mStep[0m  [4/42], [94mLoss[0m : 7.40678
[1mStep[0m  [8/42], [94mLoss[0m : 6.92493
[1mStep[0m  [12/42], [94mLoss[0m : 6.91513
[1mStep[0m  [16/42], [94mLoss[0m : 6.77532
[1mStep[0m  [20/42], [94mLoss[0m : 6.84537
[1mStep[0m  [24/42], [94mLoss[0m : 6.77272
[1mStep[0m  [28/42], [94mLoss[0m : 6.77333
[1mStep[0m  [32/42], [94mLoss[0m : 7.04890
[1mStep[0m  [36/42], [94mLoss[0m : 6.71327
[1mStep[0m  [40/42], [94mLoss[0m : 6.71955

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.977, [92mTest[0m: 7.009, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.75023
[1mStep[0m  [4/42], [94mLoss[0m : 6.91892
[1mStep[0m  [8/42], [94mLoss[0m : 7.04727
[1mStep[0m  [12/42], [94mLoss[0m : 6.61374
[1mStep[0m  [16/42], [94mLoss[0m : 6.80196
[1mStep[0m  [20/42], [94mLoss[0m : 7.05622
[1mStep[0m  [24/42], [94mLoss[0m : 6.95993
[1mStep[0m  [28/42], [94mLoss[0m : 6.83840
[1mStep[0m  [32/42], [94mLoss[0m : 7.12620
[1mStep[0m  [36/42], [94mLoss[0m : 7.05217
[1mStep[0m  [40/42], [94mLoss[0m : 6.55956

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.853, [92mTest[0m: 6.882, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.65039
[1mStep[0m  [4/42], [94mLoss[0m : 6.94209
[1mStep[0m  [8/42], [94mLoss[0m : 6.58810
[1mStep[0m  [12/42], [94mLoss[0m : 6.93163
[1mStep[0m  [16/42], [94mLoss[0m : 7.17776
[1mStep[0m  [20/42], [94mLoss[0m : 6.72934
[1mStep[0m  [24/42], [94mLoss[0m : 6.48592
[1mStep[0m  [28/42], [94mLoss[0m : 6.30481
[1mStep[0m  [32/42], [94mLoss[0m : 6.40999
[1mStep[0m  [36/42], [94mLoss[0m : 6.72740
[1mStep[0m  [40/42], [94mLoss[0m : 6.60304

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.731, [92mTest[0m: 6.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.57677
[1mStep[0m  [4/42], [94mLoss[0m : 6.30613
[1mStep[0m  [8/42], [94mLoss[0m : 6.93446
[1mStep[0m  [12/42], [94mLoss[0m : 6.37457
[1mStep[0m  [16/42], [94mLoss[0m : 6.12887
[1mStep[0m  [20/42], [94mLoss[0m : 6.42251
[1mStep[0m  [24/42], [94mLoss[0m : 6.65115
[1mStep[0m  [28/42], [94mLoss[0m : 6.89764
[1mStep[0m  [32/42], [94mLoss[0m : 6.71634
[1mStep[0m  [36/42], [94mLoss[0m : 6.48784
[1mStep[0m  [40/42], [94mLoss[0m : 6.66744

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.601, [92mTest[0m: 6.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.43842
[1mStep[0m  [4/42], [94mLoss[0m : 6.35770
[1mStep[0m  [8/42], [94mLoss[0m : 6.33699
[1mStep[0m  [12/42], [94mLoss[0m : 6.83364
[1mStep[0m  [16/42], [94mLoss[0m : 6.26228
[1mStep[0m  [20/42], [94mLoss[0m : 6.93362
[1mStep[0m  [24/42], [94mLoss[0m : 6.26725
[1mStep[0m  [28/42], [94mLoss[0m : 6.43579
[1mStep[0m  [32/42], [94mLoss[0m : 6.35181
[1mStep[0m  [36/42], [94mLoss[0m : 6.14433
[1mStep[0m  [40/42], [94mLoss[0m : 6.70701

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.488, [92mTest[0m: 6.524, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.42100
[1mStep[0m  [4/42], [94mLoss[0m : 6.35576
[1mStep[0m  [8/42], [94mLoss[0m : 6.32865
[1mStep[0m  [12/42], [94mLoss[0m : 6.43411
[1mStep[0m  [16/42], [94mLoss[0m : 6.38478
[1mStep[0m  [20/42], [94mLoss[0m : 6.39722
[1mStep[0m  [24/42], [94mLoss[0m : 6.45138
[1mStep[0m  [28/42], [94mLoss[0m : 6.11422
[1mStep[0m  [32/42], [94mLoss[0m : 6.42284
[1mStep[0m  [36/42], [94mLoss[0m : 6.41327
[1mStep[0m  [40/42], [94mLoss[0m : 6.04429

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.372, [92mTest[0m: 6.384, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.18956
[1mStep[0m  [4/42], [94mLoss[0m : 6.33041
[1mStep[0m  [8/42], [94mLoss[0m : 6.37845
[1mStep[0m  [12/42], [94mLoss[0m : 6.57620
[1mStep[0m  [16/42], [94mLoss[0m : 6.65073
[1mStep[0m  [20/42], [94mLoss[0m : 6.40846
[1mStep[0m  [24/42], [94mLoss[0m : 5.84101
[1mStep[0m  [28/42], [94mLoss[0m : 6.08581
[1mStep[0m  [32/42], [94mLoss[0m : 5.89695
[1mStep[0m  [36/42], [94mLoss[0m : 6.23918
[1mStep[0m  [40/42], [94mLoss[0m : 6.16968

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.245, [92mTest[0m: 6.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.92997
[1mStep[0m  [4/42], [94mLoss[0m : 6.14262
[1mStep[0m  [8/42], [94mLoss[0m : 6.50653
[1mStep[0m  [12/42], [94mLoss[0m : 5.78920
[1mStep[0m  [16/42], [94mLoss[0m : 6.10457
[1mStep[0m  [20/42], [94mLoss[0m : 6.11740
[1mStep[0m  [24/42], [94mLoss[0m : 5.82627
[1mStep[0m  [28/42], [94mLoss[0m : 6.20356
[1mStep[0m  [32/42], [94mLoss[0m : 6.11696
[1mStep[0m  [36/42], [94mLoss[0m : 6.05594
[1mStep[0m  [40/42], [94mLoss[0m : 6.45811

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.122, [92mTest[0m: 6.164, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.35155
[1mStep[0m  [4/42], [94mLoss[0m : 5.99898
[1mStep[0m  [8/42], [94mLoss[0m : 6.24592
[1mStep[0m  [12/42], [94mLoss[0m : 5.80681
[1mStep[0m  [16/42], [94mLoss[0m : 6.10402
[1mStep[0m  [20/42], [94mLoss[0m : 6.05370
[1mStep[0m  [24/42], [94mLoss[0m : 6.28448
[1mStep[0m  [28/42], [94mLoss[0m : 6.17120
[1mStep[0m  [32/42], [94mLoss[0m : 5.46051
[1mStep[0m  [36/42], [94mLoss[0m : 5.60859
[1mStep[0m  [40/42], [94mLoss[0m : 5.95833

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.003, [92mTest[0m: 6.035, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.10666
[1mStep[0m  [4/42], [94mLoss[0m : 5.87949
[1mStep[0m  [8/42], [94mLoss[0m : 5.75718
[1mStep[0m  [12/42], [94mLoss[0m : 5.79713
[1mStep[0m  [16/42], [94mLoss[0m : 5.83613
[1mStep[0m  [20/42], [94mLoss[0m : 5.85898
[1mStep[0m  [24/42], [94mLoss[0m : 5.55247
[1mStep[0m  [28/42], [94mLoss[0m : 5.69751
[1mStep[0m  [32/42], [94mLoss[0m : 5.59434
[1mStep[0m  [36/42], [94mLoss[0m : 5.92937
[1mStep[0m  [40/42], [94mLoss[0m : 5.72093

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.881, [92mTest[0m: 5.898, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.57904
[1mStep[0m  [4/42], [94mLoss[0m : 5.51235
[1mStep[0m  [8/42], [94mLoss[0m : 5.84684
[1mStep[0m  [12/42], [94mLoss[0m : 5.64145
[1mStep[0m  [16/42], [94mLoss[0m : 5.67674
[1mStep[0m  [20/42], [94mLoss[0m : 5.66739
[1mStep[0m  [24/42], [94mLoss[0m : 5.73027
[1mStep[0m  [28/42], [94mLoss[0m : 5.98617
[1mStep[0m  [32/42], [94mLoss[0m : 5.75690
[1mStep[0m  [36/42], [94mLoss[0m : 5.74903
[1mStep[0m  [40/42], [94mLoss[0m : 5.66014

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.763, [92mTest[0m: 5.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.67426
[1mStep[0m  [4/42], [94mLoss[0m : 5.82272
[1mStep[0m  [8/42], [94mLoss[0m : 5.92062
[1mStep[0m  [12/42], [94mLoss[0m : 5.55849
[1mStep[0m  [16/42], [94mLoss[0m : 5.76679
[1mStep[0m  [20/42], [94mLoss[0m : 5.44578
[1mStep[0m  [24/42], [94mLoss[0m : 5.09216
[1mStep[0m  [28/42], [94mLoss[0m : 5.44980
[1mStep[0m  [32/42], [94mLoss[0m : 5.17510
[1mStep[0m  [36/42], [94mLoss[0m : 5.84405
[1mStep[0m  [40/42], [94mLoss[0m : 5.65821

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.635, [92mTest[0m: 5.677, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.50830
[1mStep[0m  [4/42], [94mLoss[0m : 5.33408
[1mStep[0m  [8/42], [94mLoss[0m : 5.36235
[1mStep[0m  [12/42], [94mLoss[0m : 5.81863
[1mStep[0m  [16/42], [94mLoss[0m : 5.80760
[1mStep[0m  [20/42], [94mLoss[0m : 5.50792
[1mStep[0m  [24/42], [94mLoss[0m : 5.35571
[1mStep[0m  [28/42], [94mLoss[0m : 5.15607
[1mStep[0m  [32/42], [94mLoss[0m : 5.79230
[1mStep[0m  [36/42], [94mLoss[0m : 5.41688
[1mStep[0m  [40/42], [94mLoss[0m : 5.62943

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.532, [92mTest[0m: 5.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.86889
[1mStep[0m  [4/42], [94mLoss[0m : 5.23909
[1mStep[0m  [8/42], [94mLoss[0m : 5.17914
[1mStep[0m  [12/42], [94mLoss[0m : 5.47325
[1mStep[0m  [16/42], [94mLoss[0m : 5.48039
[1mStep[0m  [20/42], [94mLoss[0m : 5.16820
[1mStep[0m  [24/42], [94mLoss[0m : 5.39934
[1mStep[0m  [28/42], [94mLoss[0m : 5.33776
[1mStep[0m  [32/42], [94mLoss[0m : 5.18654
[1mStep[0m  [36/42], [94mLoss[0m : 5.85244
[1mStep[0m  [40/42], [94mLoss[0m : 5.16542

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.412, [92mTest[0m: 5.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.25797
[1mStep[0m  [4/42], [94mLoss[0m : 5.72163
[1mStep[0m  [8/42], [94mLoss[0m : 5.20628
[1mStep[0m  [12/42], [94mLoss[0m : 5.30803
[1mStep[0m  [16/42], [94mLoss[0m : 5.28207
[1mStep[0m  [20/42], [94mLoss[0m : 5.48064
[1mStep[0m  [24/42], [94mLoss[0m : 4.93644
[1mStep[0m  [28/42], [94mLoss[0m : 4.97555
[1mStep[0m  [32/42], [94mLoss[0m : 5.35958
[1mStep[0m  [36/42], [94mLoss[0m : 5.29660
[1mStep[0m  [40/42], [94mLoss[0m : 5.11149

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.293, [92mTest[0m: 5.330, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.59471
[1mStep[0m  [4/42], [94mLoss[0m : 5.39352
[1mStep[0m  [8/42], [94mLoss[0m : 4.95301
[1mStep[0m  [12/42], [94mLoss[0m : 5.39402
[1mStep[0m  [16/42], [94mLoss[0m : 5.28330
[1mStep[0m  [20/42], [94mLoss[0m : 5.14032
[1mStep[0m  [24/42], [94mLoss[0m : 5.31171
[1mStep[0m  [28/42], [94mLoss[0m : 5.38612
[1mStep[0m  [32/42], [94mLoss[0m : 5.46418
[1mStep[0m  [36/42], [94mLoss[0m : 5.40072
[1mStep[0m  [40/42], [94mLoss[0m : 5.28624

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.208, [92mTest[0m: 5.241, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.46276
[1mStep[0m  [4/42], [94mLoss[0m : 4.83843
[1mStep[0m  [8/42], [94mLoss[0m : 4.95299
[1mStep[0m  [12/42], [94mLoss[0m : 4.94245
[1mStep[0m  [16/42], [94mLoss[0m : 5.40775
[1mStep[0m  [20/42], [94mLoss[0m : 4.81161
[1mStep[0m  [24/42], [94mLoss[0m : 5.20730
[1mStep[0m  [28/42], [94mLoss[0m : 5.11592
[1mStep[0m  [32/42], [94mLoss[0m : 5.32959
[1mStep[0m  [36/42], [94mLoss[0m : 5.20250
[1mStep[0m  [40/42], [94mLoss[0m : 5.09812

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.108, [92mTest[0m: 5.134, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15485
[1mStep[0m  [4/42], [94mLoss[0m : 5.04234
[1mStep[0m  [8/42], [94mLoss[0m : 5.21403
[1mStep[0m  [12/42], [94mLoss[0m : 5.00744
[1mStep[0m  [16/42], [94mLoss[0m : 4.82132
[1mStep[0m  [20/42], [94mLoss[0m : 5.25731
[1mStep[0m  [24/42], [94mLoss[0m : 5.02260
[1mStep[0m  [28/42], [94mLoss[0m : 5.07469
[1mStep[0m  [32/42], [94mLoss[0m : 4.73636
[1mStep[0m  [36/42], [94mLoss[0m : 5.00210
[1mStep[0m  [40/42], [94mLoss[0m : 5.01074

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.027, [92mTest[0m: 5.034, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.22202
[1mStep[0m  [4/42], [94mLoss[0m : 4.90576
[1mStep[0m  [8/42], [94mLoss[0m : 5.06189
[1mStep[0m  [12/42], [94mLoss[0m : 4.69128
[1mStep[0m  [16/42], [94mLoss[0m : 5.02691
[1mStep[0m  [20/42], [94mLoss[0m : 4.45046
[1mStep[0m  [24/42], [94mLoss[0m : 4.77771
[1mStep[0m  [28/42], [94mLoss[0m : 4.83459
[1mStep[0m  [32/42], [94mLoss[0m : 4.90649
[1mStep[0m  [36/42], [94mLoss[0m : 4.66920
[1mStep[0m  [40/42], [94mLoss[0m : 5.01969

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.918, [92mTest[0m: 4.936, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.84930
[1mStep[0m  [4/42], [94mLoss[0m : 4.94376
[1mStep[0m  [8/42], [94mLoss[0m : 5.14349
[1mStep[0m  [12/42], [94mLoss[0m : 4.92918
[1mStep[0m  [16/42], [94mLoss[0m : 4.48671
[1mStep[0m  [20/42], [94mLoss[0m : 4.78279
[1mStep[0m  [24/42], [94mLoss[0m : 4.61782
[1mStep[0m  [28/42], [94mLoss[0m : 4.69957
[1mStep[0m  [32/42], [94mLoss[0m : 4.82607
[1mStep[0m  [36/42], [94mLoss[0m : 4.72534
[1mStep[0m  [40/42], [94mLoss[0m : 4.75384

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.844, [92mTest[0m: 4.853, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.02995
[1mStep[0m  [4/42], [94mLoss[0m : 4.49782
[1mStep[0m  [8/42], [94mLoss[0m : 4.72108
[1mStep[0m  [12/42], [94mLoss[0m : 4.85090
[1mStep[0m  [16/42], [94mLoss[0m : 4.85111
[1mStep[0m  [20/42], [94mLoss[0m : 4.67209
[1mStep[0m  [24/42], [94mLoss[0m : 4.88960
[1mStep[0m  [28/42], [94mLoss[0m : 4.64688
[1mStep[0m  [32/42], [94mLoss[0m : 5.03835
[1mStep[0m  [36/42], [94mLoss[0m : 4.63236
[1mStep[0m  [40/42], [94mLoss[0m : 4.39350

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.747, [92mTest[0m: 4.768, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.66433
[1mStep[0m  [4/42], [94mLoss[0m : 4.70659
[1mStep[0m  [8/42], [94mLoss[0m : 4.94027
[1mStep[0m  [12/42], [94mLoss[0m : 4.77393
[1mStep[0m  [16/42], [94mLoss[0m : 4.90259
[1mStep[0m  [20/42], [94mLoss[0m : 4.62321
[1mStep[0m  [24/42], [94mLoss[0m : 5.00501
[1mStep[0m  [28/42], [94mLoss[0m : 4.84655
[1mStep[0m  [32/42], [94mLoss[0m : 4.84931
[1mStep[0m  [36/42], [94mLoss[0m : 4.72994
[1mStep[0m  [40/42], [94mLoss[0m : 4.34295

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.665, [92mTest[0m: 4.688, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.36635
[1mStep[0m  [4/42], [94mLoss[0m : 4.19763
[1mStep[0m  [8/42], [94mLoss[0m : 4.25990
[1mStep[0m  [12/42], [94mLoss[0m : 4.67426
[1mStep[0m  [16/42], [94mLoss[0m : 4.25718
[1mStep[0m  [20/42], [94mLoss[0m : 4.21802
[1mStep[0m  [24/42], [94mLoss[0m : 4.58754
[1mStep[0m  [28/42], [94mLoss[0m : 4.98534
[1mStep[0m  [32/42], [94mLoss[0m : 4.61383
[1mStep[0m  [36/42], [94mLoss[0m : 4.43494
[1mStep[0m  [40/42], [94mLoss[0m : 4.62556

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.568, [92mTest[0m: 4.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.43587
[1mStep[0m  [4/42], [94mLoss[0m : 4.68886
[1mStep[0m  [8/42], [94mLoss[0m : 4.20148
[1mStep[0m  [12/42], [94mLoss[0m : 4.57120
[1mStep[0m  [16/42], [94mLoss[0m : 4.40061
[1mStep[0m  [20/42], [94mLoss[0m : 4.63577
[1mStep[0m  [24/42], [94mLoss[0m : 4.37813
[1mStep[0m  [28/42], [94mLoss[0m : 4.50591
[1mStep[0m  [32/42], [94mLoss[0m : 4.71224
[1mStep[0m  [36/42], [94mLoss[0m : 4.18307
[1mStep[0m  [40/42], [94mLoss[0m : 4.38860

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.511, [92mTest[0m: 4.507, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.440
====================================

Phase 2 - Evaluation MAE:  4.439740589686802
MAE score P1      7.744776
MAE score P2      4.439741
loss              4.511404
learning_rate       0.0001
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 12, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.70362
[1mStep[0m  [4/42], [94mLoss[0m : 10.86732
[1mStep[0m  [8/42], [94mLoss[0m : 10.66448
[1mStep[0m  [12/42], [94mLoss[0m : 10.64651
[1mStep[0m  [16/42], [94mLoss[0m : 11.09313
[1mStep[0m  [20/42], [94mLoss[0m : 10.74553
[1mStep[0m  [24/42], [94mLoss[0m : 10.46167
[1mStep[0m  [28/42], [94mLoss[0m : 10.62943
[1mStep[0m  [32/42], [94mLoss[0m : 10.85359
[1mStep[0m  [36/42], [94mLoss[0m : 10.19735
[1mStep[0m  [40/42], [94mLoss[0m : 10.30696

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.635, [92mTest[0m: 10.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25686
[1mStep[0m  [4/42], [94mLoss[0m : 10.23633
[1mStep[0m  [8/42], [94mLoss[0m : 10.06115
[1mStep[0m  [12/42], [94mLoss[0m : 10.67063
[1mStep[0m  [16/42], [94mLoss[0m : 10.61853
[1mStep[0m  [20/42], [94mLoss[0m : 10.37175
[1mStep[0m  [24/42], [94mLoss[0m : 10.36659
[1mStep[0m  [28/42], [94mLoss[0m : 9.84236
[1mStep[0m  [32/42], [94mLoss[0m : 10.41260
[1mStep[0m  [36/42], [94mLoss[0m : 9.97459
[1mStep[0m  [40/42], [94mLoss[0m : 10.47750

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.411, [92mTest[0m: 10.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19131
[1mStep[0m  [4/42], [94mLoss[0m : 10.02021
[1mStep[0m  [8/42], [94mLoss[0m : 10.43954
[1mStep[0m  [12/42], [94mLoss[0m : 10.26334
[1mStep[0m  [16/42], [94mLoss[0m : 9.83780
[1mStep[0m  [20/42], [94mLoss[0m : 10.19111
[1mStep[0m  [24/42], [94mLoss[0m : 10.16857
[1mStep[0m  [28/42], [94mLoss[0m : 10.05395
[1mStep[0m  [32/42], [94mLoss[0m : 10.58484
[1mStep[0m  [36/42], [94mLoss[0m : 9.98507
[1mStep[0m  [40/42], [94mLoss[0m : 10.16729

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.203, [92mTest[0m: 10.291, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15004
[1mStep[0m  [4/42], [94mLoss[0m : 9.95427
[1mStep[0m  [8/42], [94mLoss[0m : 9.97785
[1mStep[0m  [12/42], [94mLoss[0m : 9.86463
[1mStep[0m  [16/42], [94mLoss[0m : 10.15403
[1mStep[0m  [20/42], [94mLoss[0m : 10.02506
[1mStep[0m  [24/42], [94mLoss[0m : 9.89878
[1mStep[0m  [28/42], [94mLoss[0m : 9.84333
[1mStep[0m  [32/42], [94mLoss[0m : 10.26064
[1mStep[0m  [36/42], [94mLoss[0m : 10.01307
[1mStep[0m  [40/42], [94mLoss[0m : 9.77278

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.975, [92mTest[0m: 10.074, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.84024
[1mStep[0m  [4/42], [94mLoss[0m : 10.25835
[1mStep[0m  [8/42], [94mLoss[0m : 9.29584
[1mStep[0m  [12/42], [94mLoss[0m : 9.90820
[1mStep[0m  [16/42], [94mLoss[0m : 9.45738
[1mStep[0m  [20/42], [94mLoss[0m : 10.01615
[1mStep[0m  [24/42], [94mLoss[0m : 9.72988
[1mStep[0m  [28/42], [94mLoss[0m : 9.61229
[1mStep[0m  [32/42], [94mLoss[0m : 9.81342
[1mStep[0m  [36/42], [94mLoss[0m : 9.48398
[1mStep[0m  [40/42], [94mLoss[0m : 9.31665

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.766, [92mTest[0m: 9.863, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.94671
[1mStep[0m  [4/42], [94mLoss[0m : 9.50440
[1mStep[0m  [8/42], [94mLoss[0m : 9.87722
[1mStep[0m  [12/42], [94mLoss[0m : 9.69206
[1mStep[0m  [16/42], [94mLoss[0m : 9.48883
[1mStep[0m  [20/42], [94mLoss[0m : 9.78360
[1mStep[0m  [24/42], [94mLoss[0m : 9.07018
[1mStep[0m  [28/42], [94mLoss[0m : 9.51166
[1mStep[0m  [32/42], [94mLoss[0m : 9.25992
[1mStep[0m  [36/42], [94mLoss[0m : 9.62219
[1mStep[0m  [40/42], [94mLoss[0m : 9.42220

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.534, [92mTest[0m: 9.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.12436
[1mStep[0m  [4/42], [94mLoss[0m : 9.50463
[1mStep[0m  [8/42], [94mLoss[0m : 9.42703
[1mStep[0m  [12/42], [94mLoss[0m : 9.49959
[1mStep[0m  [16/42], [94mLoss[0m : 9.65842
[1mStep[0m  [20/42], [94mLoss[0m : 9.38778
[1mStep[0m  [24/42], [94mLoss[0m : 9.46527
[1mStep[0m  [28/42], [94mLoss[0m : 9.11146
[1mStep[0m  [32/42], [94mLoss[0m : 9.48073
[1mStep[0m  [36/42], [94mLoss[0m : 8.83499
[1mStep[0m  [40/42], [94mLoss[0m : 9.00376

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.324, [92mTest[0m: 9.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.10738
[1mStep[0m  [4/42], [94mLoss[0m : 9.03067
[1mStep[0m  [8/42], [94mLoss[0m : 9.31515
[1mStep[0m  [12/42], [94mLoss[0m : 9.24522
[1mStep[0m  [16/42], [94mLoss[0m : 9.20654
[1mStep[0m  [20/42], [94mLoss[0m : 9.20635
[1mStep[0m  [24/42], [94mLoss[0m : 9.08099
[1mStep[0m  [28/42], [94mLoss[0m : 9.33452
[1mStep[0m  [32/42], [94mLoss[0m : 9.10979
[1mStep[0m  [36/42], [94mLoss[0m : 8.83201
[1mStep[0m  [40/42], [94mLoss[0m : 8.47313

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.102, [92mTest[0m: 9.194, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.75226
[1mStep[0m  [4/42], [94mLoss[0m : 8.87842
[1mStep[0m  [8/42], [94mLoss[0m : 9.40976
[1mStep[0m  [12/42], [94mLoss[0m : 8.80069
[1mStep[0m  [16/42], [94mLoss[0m : 9.07357
[1mStep[0m  [20/42], [94mLoss[0m : 8.72492
[1mStep[0m  [24/42], [94mLoss[0m : 9.11963
[1mStep[0m  [28/42], [94mLoss[0m : 8.87007
[1mStep[0m  [32/42], [94mLoss[0m : 8.98282
[1mStep[0m  [36/42], [94mLoss[0m : 8.67534
[1mStep[0m  [40/42], [94mLoss[0m : 8.92816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.881, [92mTest[0m: 8.976, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.01840
[1mStep[0m  [4/42], [94mLoss[0m : 8.61977
[1mStep[0m  [8/42], [94mLoss[0m : 8.90911
[1mStep[0m  [12/42], [94mLoss[0m : 8.71046
[1mStep[0m  [16/42], [94mLoss[0m : 8.46084
[1mStep[0m  [20/42], [94mLoss[0m : 8.54516
[1mStep[0m  [24/42], [94mLoss[0m : 8.38768
[1mStep[0m  [28/42], [94mLoss[0m : 8.32253
[1mStep[0m  [32/42], [94mLoss[0m : 8.37301
[1mStep[0m  [36/42], [94mLoss[0m : 8.33900
[1mStep[0m  [40/42], [94mLoss[0m : 8.45210

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.668, [92mTest[0m: 8.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.67859
[1mStep[0m  [4/42], [94mLoss[0m : 8.42848
[1mStep[0m  [8/42], [94mLoss[0m : 8.58262
[1mStep[0m  [12/42], [94mLoss[0m : 8.68397
[1mStep[0m  [16/42], [94mLoss[0m : 8.82903
[1mStep[0m  [20/42], [94mLoss[0m : 8.09338
[1mStep[0m  [24/42], [94mLoss[0m : 8.37369
[1mStep[0m  [28/42], [94mLoss[0m : 8.52837
[1mStep[0m  [32/42], [94mLoss[0m : 8.39743
[1mStep[0m  [36/42], [94mLoss[0m : 8.32854
[1mStep[0m  [40/42], [94mLoss[0m : 8.11949

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.449, [92mTest[0m: 8.545, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.17332
[1mStep[0m  [4/42], [94mLoss[0m : 8.10131
[1mStep[0m  [8/42], [94mLoss[0m : 8.38413
[1mStep[0m  [12/42], [94mLoss[0m : 7.97705
[1mStep[0m  [16/42], [94mLoss[0m : 8.01221
[1mStep[0m  [20/42], [94mLoss[0m : 8.07137
[1mStep[0m  [24/42], [94mLoss[0m : 8.55279
[1mStep[0m  [28/42], [94mLoss[0m : 8.68739
[1mStep[0m  [32/42], [94mLoss[0m : 8.51985
[1mStep[0m  [36/42], [94mLoss[0m : 8.47406
[1mStep[0m  [40/42], [94mLoss[0m : 8.32639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.231, [92mTest[0m: 8.340, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.04363
[1mStep[0m  [4/42], [94mLoss[0m : 8.29091
[1mStep[0m  [8/42], [94mLoss[0m : 8.31560
[1mStep[0m  [12/42], [94mLoss[0m : 7.98669
[1mStep[0m  [16/42], [94mLoss[0m : 7.83116
[1mStep[0m  [20/42], [94mLoss[0m : 7.54006
[1mStep[0m  [24/42], [94mLoss[0m : 8.45023
[1mStep[0m  [28/42], [94mLoss[0m : 7.76711
[1mStep[0m  [32/42], [94mLoss[0m : 7.87157
[1mStep[0m  [36/42], [94mLoss[0m : 8.18002
[1mStep[0m  [40/42], [94mLoss[0m : 7.69064

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.011, [92mTest[0m: 8.119, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.43261
[1mStep[0m  [4/42], [94mLoss[0m : 7.88990
[1mStep[0m  [8/42], [94mLoss[0m : 8.08521
[1mStep[0m  [12/42], [94mLoss[0m : 7.85171
[1mStep[0m  [16/42], [94mLoss[0m : 7.61696
[1mStep[0m  [20/42], [94mLoss[0m : 8.16706
[1mStep[0m  [24/42], [94mLoss[0m : 7.61313
[1mStep[0m  [28/42], [94mLoss[0m : 7.85370
[1mStep[0m  [32/42], [94mLoss[0m : 7.59499
[1mStep[0m  [36/42], [94mLoss[0m : 7.40084
[1mStep[0m  [40/42], [94mLoss[0m : 8.13714

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.787, [92mTest[0m: 7.891, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.50374
[1mStep[0m  [4/42], [94mLoss[0m : 7.76480
[1mStep[0m  [8/42], [94mLoss[0m : 7.80451
[1mStep[0m  [12/42], [94mLoss[0m : 6.96393
[1mStep[0m  [16/42], [94mLoss[0m : 7.45796
[1mStep[0m  [20/42], [94mLoss[0m : 7.41721
[1mStep[0m  [24/42], [94mLoss[0m : 7.79983
[1mStep[0m  [28/42], [94mLoss[0m : 7.37640
[1mStep[0m  [32/42], [94mLoss[0m : 7.42061
[1mStep[0m  [36/42], [94mLoss[0m : 7.88688
[1mStep[0m  [40/42], [94mLoss[0m : 7.42282

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.563, [92mTest[0m: 7.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.56736
[1mStep[0m  [4/42], [94mLoss[0m : 7.12462
[1mStep[0m  [8/42], [94mLoss[0m : 7.31727
[1mStep[0m  [12/42], [94mLoss[0m : 7.25973
[1mStep[0m  [16/42], [94mLoss[0m : 7.43093
[1mStep[0m  [20/42], [94mLoss[0m : 7.13840
[1mStep[0m  [24/42], [94mLoss[0m : 7.23663
[1mStep[0m  [28/42], [94mLoss[0m : 7.35194
[1mStep[0m  [32/42], [94mLoss[0m : 7.61882
[1mStep[0m  [36/42], [94mLoss[0m : 7.02460
[1mStep[0m  [40/42], [94mLoss[0m : 7.38482

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.348, [92mTest[0m: 7.460, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.31990
[1mStep[0m  [4/42], [94mLoss[0m : 7.30251
[1mStep[0m  [8/42], [94mLoss[0m : 7.22565
[1mStep[0m  [12/42], [94mLoss[0m : 7.22118
[1mStep[0m  [16/42], [94mLoss[0m : 7.13806
[1mStep[0m  [20/42], [94mLoss[0m : 7.26551
[1mStep[0m  [24/42], [94mLoss[0m : 7.12142
[1mStep[0m  [28/42], [94mLoss[0m : 6.92406
[1mStep[0m  [32/42], [94mLoss[0m : 6.93804
[1mStep[0m  [36/42], [94mLoss[0m : 7.14753
[1mStep[0m  [40/42], [94mLoss[0m : 7.05601

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.132, [92mTest[0m: 7.244, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.76835
[1mStep[0m  [4/42], [94mLoss[0m : 7.13658
[1mStep[0m  [8/42], [94mLoss[0m : 6.84480
[1mStep[0m  [12/42], [94mLoss[0m : 7.28168
[1mStep[0m  [16/42], [94mLoss[0m : 6.66107
[1mStep[0m  [20/42], [94mLoss[0m : 6.85631
[1mStep[0m  [24/42], [94mLoss[0m : 6.66068
[1mStep[0m  [28/42], [94mLoss[0m : 6.96982
[1mStep[0m  [32/42], [94mLoss[0m : 6.57901
[1mStep[0m  [36/42], [94mLoss[0m : 6.73151
[1mStep[0m  [40/42], [94mLoss[0m : 6.48959

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.918, [92mTest[0m: 7.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.66531
[1mStep[0m  [4/42], [94mLoss[0m : 6.56410
[1mStep[0m  [8/42], [94mLoss[0m : 6.61040
[1mStep[0m  [12/42], [94mLoss[0m : 6.65008
[1mStep[0m  [16/42], [94mLoss[0m : 6.83866
[1mStep[0m  [20/42], [94mLoss[0m : 7.10472
[1mStep[0m  [24/42], [94mLoss[0m : 6.51264
[1mStep[0m  [28/42], [94mLoss[0m : 6.85506
[1mStep[0m  [32/42], [94mLoss[0m : 6.59197
[1mStep[0m  [36/42], [94mLoss[0m : 5.84276
[1mStep[0m  [40/42], [94mLoss[0m : 6.77633

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.701, [92mTest[0m: 6.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.63329
[1mStep[0m  [4/42], [94mLoss[0m : 6.65799
[1mStep[0m  [8/42], [94mLoss[0m : 6.19953
[1mStep[0m  [12/42], [94mLoss[0m : 7.02629
[1mStep[0m  [16/42], [94mLoss[0m : 6.40185
[1mStep[0m  [20/42], [94mLoss[0m : 6.71563
[1mStep[0m  [24/42], [94mLoss[0m : 6.71319
[1mStep[0m  [28/42], [94mLoss[0m : 6.20570
[1mStep[0m  [32/42], [94mLoss[0m : 6.26954
[1mStep[0m  [36/42], [94mLoss[0m : 6.00666
[1mStep[0m  [40/42], [94mLoss[0m : 6.23896

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.482, [92mTest[0m: 6.580, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.66614
[1mStep[0m  [4/42], [94mLoss[0m : 6.33619
[1mStep[0m  [8/42], [94mLoss[0m : 6.14886
[1mStep[0m  [12/42], [94mLoss[0m : 6.28075
[1mStep[0m  [16/42], [94mLoss[0m : 6.45684
[1mStep[0m  [20/42], [94mLoss[0m : 6.14123
[1mStep[0m  [24/42], [94mLoss[0m : 6.20492
[1mStep[0m  [28/42], [94mLoss[0m : 6.66956
[1mStep[0m  [32/42], [94mLoss[0m : 6.27515
[1mStep[0m  [36/42], [94mLoss[0m : 6.16465
[1mStep[0m  [40/42], [94mLoss[0m : 6.03034

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.273, [92mTest[0m: 6.354, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.06388
[1mStep[0m  [4/42], [94mLoss[0m : 6.52797
[1mStep[0m  [8/42], [94mLoss[0m : 6.11139
[1mStep[0m  [12/42], [94mLoss[0m : 6.10047
[1mStep[0m  [16/42], [94mLoss[0m : 6.02293
[1mStep[0m  [20/42], [94mLoss[0m : 6.26110
[1mStep[0m  [24/42], [94mLoss[0m : 6.17252
[1mStep[0m  [28/42], [94mLoss[0m : 6.07816
[1mStep[0m  [32/42], [94mLoss[0m : 6.34923
[1mStep[0m  [36/42], [94mLoss[0m : 5.53020
[1mStep[0m  [40/42], [94mLoss[0m : 6.17503

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.077, [92mTest[0m: 6.161, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.17305
[1mStep[0m  [4/42], [94mLoss[0m : 5.74908
[1mStep[0m  [8/42], [94mLoss[0m : 5.81557
[1mStep[0m  [12/42], [94mLoss[0m : 6.00117
[1mStep[0m  [16/42], [94mLoss[0m : 6.19804
[1mStep[0m  [20/42], [94mLoss[0m : 5.32129
[1mStep[0m  [24/42], [94mLoss[0m : 5.47913
[1mStep[0m  [28/42], [94mLoss[0m : 5.81647
[1mStep[0m  [32/42], [94mLoss[0m : 5.95315
[1mStep[0m  [36/42], [94mLoss[0m : 5.69998
[1mStep[0m  [40/42], [94mLoss[0m : 5.65844

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.881, [92mTest[0m: 5.974, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.87368
[1mStep[0m  [4/42], [94mLoss[0m : 5.63003
[1mStep[0m  [8/42], [94mLoss[0m : 5.29434
[1mStep[0m  [12/42], [94mLoss[0m : 5.54585
[1mStep[0m  [16/42], [94mLoss[0m : 5.70635
[1mStep[0m  [20/42], [94mLoss[0m : 5.58037
[1mStep[0m  [24/42], [94mLoss[0m : 5.55183
[1mStep[0m  [28/42], [94mLoss[0m : 5.13039
[1mStep[0m  [32/42], [94mLoss[0m : 5.62985
[1mStep[0m  [36/42], [94mLoss[0m : 5.57617
[1mStep[0m  [40/42], [94mLoss[0m : 5.47650

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.679, [92mTest[0m: 5.773, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.36067
[1mStep[0m  [4/42], [94mLoss[0m : 5.21662
[1mStep[0m  [8/42], [94mLoss[0m : 5.53838
[1mStep[0m  [12/42], [94mLoss[0m : 5.82199
[1mStep[0m  [16/42], [94mLoss[0m : 5.42260
[1mStep[0m  [20/42], [94mLoss[0m : 5.46631
[1mStep[0m  [24/42], [94mLoss[0m : 5.45807
[1mStep[0m  [28/42], [94mLoss[0m : 5.64414
[1mStep[0m  [32/42], [94mLoss[0m : 5.39781
[1mStep[0m  [36/42], [94mLoss[0m : 5.31334
[1mStep[0m  [40/42], [94mLoss[0m : 5.32360

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.499, [92mTest[0m: 5.595, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.54659
[1mStep[0m  [4/42], [94mLoss[0m : 5.42952
[1mStep[0m  [8/42], [94mLoss[0m : 5.14554
[1mStep[0m  [12/42], [94mLoss[0m : 5.58127
[1mStep[0m  [16/42], [94mLoss[0m : 5.62921
[1mStep[0m  [20/42], [94mLoss[0m : 5.21841
[1mStep[0m  [24/42], [94mLoss[0m : 5.47160
[1mStep[0m  [28/42], [94mLoss[0m : 5.24013
[1mStep[0m  [32/42], [94mLoss[0m : 4.90059
[1mStep[0m  [36/42], [94mLoss[0m : 5.50850
[1mStep[0m  [40/42], [94mLoss[0m : 5.20306

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.324, [92mTest[0m: 5.412, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.30016
[1mStep[0m  [4/42], [94mLoss[0m : 5.37068
[1mStep[0m  [8/42], [94mLoss[0m : 5.61805
[1mStep[0m  [12/42], [94mLoss[0m : 5.40181
[1mStep[0m  [16/42], [94mLoss[0m : 5.50414
[1mStep[0m  [20/42], [94mLoss[0m : 5.19455
[1mStep[0m  [24/42], [94mLoss[0m : 5.21759
[1mStep[0m  [28/42], [94mLoss[0m : 5.66514
[1mStep[0m  [32/42], [94mLoss[0m : 4.93073
[1mStep[0m  [36/42], [94mLoss[0m : 5.08808
[1mStep[0m  [40/42], [94mLoss[0m : 5.28050

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.153, [92mTest[0m: 5.245, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.92894
[1mStep[0m  [4/42], [94mLoss[0m : 5.23966
[1mStep[0m  [8/42], [94mLoss[0m : 5.05436
[1mStep[0m  [12/42], [94mLoss[0m : 5.15728
[1mStep[0m  [16/42], [94mLoss[0m : 5.39687
[1mStep[0m  [20/42], [94mLoss[0m : 4.86774
[1mStep[0m  [24/42], [94mLoss[0m : 4.89309
[1mStep[0m  [28/42], [94mLoss[0m : 5.13133
[1mStep[0m  [32/42], [94mLoss[0m : 5.15284
[1mStep[0m  [36/42], [94mLoss[0m : 5.40718
[1mStep[0m  [40/42], [94mLoss[0m : 4.75520

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.986, [92mTest[0m: 5.061, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.80874
[1mStep[0m  [4/42], [94mLoss[0m : 5.30954
[1mStep[0m  [8/42], [94mLoss[0m : 4.76361
[1mStep[0m  [12/42], [94mLoss[0m : 4.88327
[1mStep[0m  [16/42], [94mLoss[0m : 4.84109
[1mStep[0m  [20/42], [94mLoss[0m : 4.69034
[1mStep[0m  [24/42], [94mLoss[0m : 4.33017
[1mStep[0m  [28/42], [94mLoss[0m : 4.82877
[1mStep[0m  [32/42], [94mLoss[0m : 4.98952
[1mStep[0m  [36/42], [94mLoss[0m : 5.02237
[1mStep[0m  [40/42], [94mLoss[0m : 5.10006

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.832, [92mTest[0m: 4.900, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.75579
[1mStep[0m  [4/42], [94mLoss[0m : 4.88306
[1mStep[0m  [8/42], [94mLoss[0m : 4.83162
[1mStep[0m  [12/42], [94mLoss[0m : 4.14521
[1mStep[0m  [16/42], [94mLoss[0m : 4.99705
[1mStep[0m  [20/42], [94mLoss[0m : 4.57194
[1mStep[0m  [24/42], [94mLoss[0m : 4.72344
[1mStep[0m  [28/42], [94mLoss[0m : 4.46562
[1mStep[0m  [32/42], [94mLoss[0m : 4.67234
[1mStep[0m  [36/42], [94mLoss[0m : 4.65865
[1mStep[0m  [40/42], [94mLoss[0m : 4.87292

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.691, [92mTest[0m: 4.755, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.616
====================================

Phase 1 - Evaluation MAE:  4.615614175796509
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 4.33007
[1mStep[0m  [4/42], [94mLoss[0m : 4.29354
[1mStep[0m  [8/42], [94mLoss[0m : 4.58637
[1mStep[0m  [12/42], [94mLoss[0m : 4.76349
[1mStep[0m  [16/42], [94mLoss[0m : 4.51473
[1mStep[0m  [20/42], [94mLoss[0m : 4.48942
[1mStep[0m  [24/42], [94mLoss[0m : 4.65418
[1mStep[0m  [28/42], [94mLoss[0m : 4.86385
[1mStep[0m  [32/42], [94mLoss[0m : 4.70357
[1mStep[0m  [36/42], [94mLoss[0m : 4.77822
[1mStep[0m  [40/42], [94mLoss[0m : 4.72770

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.541, [92mTest[0m: 4.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.31123
[1mStep[0m  [4/42], [94mLoss[0m : 4.71013
[1mStep[0m  [8/42], [94mLoss[0m : 4.22466
[1mStep[0m  [12/42], [94mLoss[0m : 4.43863
[1mStep[0m  [16/42], [94mLoss[0m : 4.56569
[1mStep[0m  [20/42], [94mLoss[0m : 4.02501
[1mStep[0m  [24/42], [94mLoss[0m : 4.40024
[1mStep[0m  [28/42], [94mLoss[0m : 4.37454
[1mStep[0m  [32/42], [94mLoss[0m : 4.48778
[1mStep[0m  [36/42], [94mLoss[0m : 4.82434
[1mStep[0m  [40/42], [94mLoss[0m : 4.60943

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.396, [92mTest[0m: 4.451, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.11563
[1mStep[0m  [4/42], [94mLoss[0m : 4.57123
[1mStep[0m  [8/42], [94mLoss[0m : 3.90586
[1mStep[0m  [12/42], [94mLoss[0m : 4.41328
[1mStep[0m  [16/42], [94mLoss[0m : 4.27581
[1mStep[0m  [20/42], [94mLoss[0m : 4.50012
[1mStep[0m  [24/42], [94mLoss[0m : 4.19761
[1mStep[0m  [28/42], [94mLoss[0m : 4.29121
[1mStep[0m  [32/42], [94mLoss[0m : 4.18896
[1mStep[0m  [36/42], [94mLoss[0m : 4.18411
[1mStep[0m  [40/42], [94mLoss[0m : 4.20475

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.252, [92mTest[0m: 4.312, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.51532
[1mStep[0m  [4/42], [94mLoss[0m : 4.01817
[1mStep[0m  [8/42], [94mLoss[0m : 4.48580
[1mStep[0m  [12/42], [94mLoss[0m : 4.41179
[1mStep[0m  [16/42], [94mLoss[0m : 3.74740
[1mStep[0m  [20/42], [94mLoss[0m : 4.13620
[1mStep[0m  [24/42], [94mLoss[0m : 3.74449
[1mStep[0m  [28/42], [94mLoss[0m : 3.79031
[1mStep[0m  [32/42], [94mLoss[0m : 4.27842
[1mStep[0m  [36/42], [94mLoss[0m : 4.11057
[1mStep[0m  [40/42], [94mLoss[0m : 4.02767

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.138, [92mTest[0m: 4.197, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.96635
[1mStep[0m  [4/42], [94mLoss[0m : 4.23044
[1mStep[0m  [8/42], [94mLoss[0m : 4.33983
[1mStep[0m  [12/42], [94mLoss[0m : 4.34893
[1mStep[0m  [16/42], [94mLoss[0m : 3.99521
[1mStep[0m  [20/42], [94mLoss[0m : 4.06332
[1mStep[0m  [24/42], [94mLoss[0m : 4.45846
[1mStep[0m  [28/42], [94mLoss[0m : 3.90825
[1mStep[0m  [32/42], [94mLoss[0m : 4.27297
[1mStep[0m  [36/42], [94mLoss[0m : 3.58758
[1mStep[0m  [40/42], [94mLoss[0m : 4.29236

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.025, [92mTest[0m: 4.062, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.67365
[1mStep[0m  [4/42], [94mLoss[0m : 4.01567
[1mStep[0m  [8/42], [94mLoss[0m : 3.86731
[1mStep[0m  [12/42], [94mLoss[0m : 3.85002
[1mStep[0m  [16/42], [94mLoss[0m : 4.36828
[1mStep[0m  [20/42], [94mLoss[0m : 4.15565
[1mStep[0m  [24/42], [94mLoss[0m : 4.09792
[1mStep[0m  [28/42], [94mLoss[0m : 3.92913
[1mStep[0m  [32/42], [94mLoss[0m : 3.85679
[1mStep[0m  [36/42], [94mLoss[0m : 4.37113
[1mStep[0m  [40/42], [94mLoss[0m : 4.11860

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.914, [92mTest[0m: 3.964, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.01563
[1mStep[0m  [4/42], [94mLoss[0m : 3.98086
[1mStep[0m  [8/42], [94mLoss[0m : 3.64002
[1mStep[0m  [12/42], [94mLoss[0m : 3.70547
[1mStep[0m  [16/42], [94mLoss[0m : 3.42791
[1mStep[0m  [20/42], [94mLoss[0m : 3.96874
[1mStep[0m  [24/42], [94mLoss[0m : 3.33121
[1mStep[0m  [28/42], [94mLoss[0m : 3.89656
[1mStep[0m  [32/42], [94mLoss[0m : 3.94337
[1mStep[0m  [36/42], [94mLoss[0m : 3.96391
[1mStep[0m  [40/42], [94mLoss[0m : 3.97374

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.819, [92mTest[0m: 3.849, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.64192
[1mStep[0m  [4/42], [94mLoss[0m : 3.83278
[1mStep[0m  [8/42], [94mLoss[0m : 4.03281
[1mStep[0m  [12/42], [94mLoss[0m : 3.33572
[1mStep[0m  [16/42], [94mLoss[0m : 3.94826
[1mStep[0m  [20/42], [94mLoss[0m : 3.85216
[1mStep[0m  [24/42], [94mLoss[0m : 3.64690
[1mStep[0m  [28/42], [94mLoss[0m : 3.88829
[1mStep[0m  [32/42], [94mLoss[0m : 3.67589
[1mStep[0m  [36/42], [94mLoss[0m : 3.95804
[1mStep[0m  [40/42], [94mLoss[0m : 3.65016

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.736, [92mTest[0m: 3.736, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.64292
[1mStep[0m  [4/42], [94mLoss[0m : 3.68984
[1mStep[0m  [8/42], [94mLoss[0m : 3.81869
[1mStep[0m  [12/42], [94mLoss[0m : 3.63866
[1mStep[0m  [16/42], [94mLoss[0m : 3.66346
[1mStep[0m  [20/42], [94mLoss[0m : 3.69724
[1mStep[0m  [24/42], [94mLoss[0m : 3.64867
[1mStep[0m  [28/42], [94mLoss[0m : 3.79195
[1mStep[0m  [32/42], [94mLoss[0m : 3.86275
[1mStep[0m  [36/42], [94mLoss[0m : 3.71312
[1mStep[0m  [40/42], [94mLoss[0m : 3.11558

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.658, [92mTest[0m: 3.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.81476
[1mStep[0m  [4/42], [94mLoss[0m : 3.74063
[1mStep[0m  [8/42], [94mLoss[0m : 3.46462
[1mStep[0m  [12/42], [94mLoss[0m : 3.49040
[1mStep[0m  [16/42], [94mLoss[0m : 3.70769
[1mStep[0m  [20/42], [94mLoss[0m : 3.69224
[1mStep[0m  [24/42], [94mLoss[0m : 3.59657
[1mStep[0m  [28/42], [94mLoss[0m : 3.86365
[1mStep[0m  [32/42], [94mLoss[0m : 3.35957
[1mStep[0m  [36/42], [94mLoss[0m : 3.91737
[1mStep[0m  [40/42], [94mLoss[0m : 3.48762

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.566, [92mTest[0m: 3.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15866
[1mStep[0m  [4/42], [94mLoss[0m : 3.53286
[1mStep[0m  [8/42], [94mLoss[0m : 3.66917
[1mStep[0m  [12/42], [94mLoss[0m : 3.17168
[1mStep[0m  [16/42], [94mLoss[0m : 3.36024
[1mStep[0m  [20/42], [94mLoss[0m : 3.37617
[1mStep[0m  [24/42], [94mLoss[0m : 3.25895
[1mStep[0m  [28/42], [94mLoss[0m : 3.36761
[1mStep[0m  [32/42], [94mLoss[0m : 3.66008
[1mStep[0m  [36/42], [94mLoss[0m : 3.49059
[1mStep[0m  [40/42], [94mLoss[0m : 3.72471

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.520, [92mTest[0m: 3.496, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.61088
[1mStep[0m  [4/42], [94mLoss[0m : 3.68159
[1mStep[0m  [8/42], [94mLoss[0m : 3.47858
[1mStep[0m  [12/42], [94mLoss[0m : 3.16702
[1mStep[0m  [16/42], [94mLoss[0m : 3.11212
[1mStep[0m  [20/42], [94mLoss[0m : 3.63498
[1mStep[0m  [24/42], [94mLoss[0m : 3.61102
[1mStep[0m  [28/42], [94mLoss[0m : 3.67332
[1mStep[0m  [32/42], [94mLoss[0m : 3.53464
[1mStep[0m  [36/42], [94mLoss[0m : 3.45432
[1mStep[0m  [40/42], [94mLoss[0m : 3.45987

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.441, [92mTest[0m: 3.419, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.49359
[1mStep[0m  [4/42], [94mLoss[0m : 3.16873
[1mStep[0m  [8/42], [94mLoss[0m : 3.31172
[1mStep[0m  [12/42], [94mLoss[0m : 3.66566
[1mStep[0m  [16/42], [94mLoss[0m : 3.27140
[1mStep[0m  [20/42], [94mLoss[0m : 3.23422
[1mStep[0m  [24/42], [94mLoss[0m : 3.46392
[1mStep[0m  [28/42], [94mLoss[0m : 3.29971
[1mStep[0m  [32/42], [94mLoss[0m : 3.58135
[1mStep[0m  [36/42], [94mLoss[0m : 3.33636
[1mStep[0m  [40/42], [94mLoss[0m : 3.56463

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.376, [92mTest[0m: 3.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.10437
[1mStep[0m  [4/42], [94mLoss[0m : 3.53002
[1mStep[0m  [8/42], [94mLoss[0m : 3.17278
[1mStep[0m  [12/42], [94mLoss[0m : 3.48783
[1mStep[0m  [16/42], [94mLoss[0m : 3.33403
[1mStep[0m  [20/42], [94mLoss[0m : 3.29416
[1mStep[0m  [24/42], [94mLoss[0m : 3.23128
[1mStep[0m  [28/42], [94mLoss[0m : 3.19336
[1mStep[0m  [32/42], [94mLoss[0m : 3.16525
[1mStep[0m  [36/42], [94mLoss[0m : 3.40393
[1mStep[0m  [40/42], [94mLoss[0m : 3.11525

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.332, [92mTest[0m: 3.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.02630
[1mStep[0m  [4/42], [94mLoss[0m : 3.43749
[1mStep[0m  [8/42], [94mLoss[0m : 3.49867
[1mStep[0m  [12/42], [94mLoss[0m : 3.49411
[1mStep[0m  [16/42], [94mLoss[0m : 3.18731
[1mStep[0m  [20/42], [94mLoss[0m : 3.50883
[1mStep[0m  [24/42], [94mLoss[0m : 3.19264
[1mStep[0m  [28/42], [94mLoss[0m : 3.21823
[1mStep[0m  [32/42], [94mLoss[0m : 3.24881
[1mStep[0m  [36/42], [94mLoss[0m : 3.03982
[1mStep[0m  [40/42], [94mLoss[0m : 3.23311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.283, [92mTest[0m: 3.242, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.16632
[1mStep[0m  [4/42], [94mLoss[0m : 3.35585
[1mStep[0m  [8/42], [94mLoss[0m : 3.17924
[1mStep[0m  [12/42], [94mLoss[0m : 3.42538
[1mStep[0m  [16/42], [94mLoss[0m : 3.20655
[1mStep[0m  [20/42], [94mLoss[0m : 3.27022
[1mStep[0m  [24/42], [94mLoss[0m : 3.13848
[1mStep[0m  [28/42], [94mLoss[0m : 3.18769
[1mStep[0m  [32/42], [94mLoss[0m : 3.27223
[1mStep[0m  [36/42], [94mLoss[0m : 3.26021
[1mStep[0m  [40/42], [94mLoss[0m : 3.23056

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.234, [92mTest[0m: 3.182, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92002
[1mStep[0m  [4/42], [94mLoss[0m : 3.01994
[1mStep[0m  [8/42], [94mLoss[0m : 3.28060
[1mStep[0m  [12/42], [94mLoss[0m : 3.20373
[1mStep[0m  [16/42], [94mLoss[0m : 3.01231
[1mStep[0m  [20/42], [94mLoss[0m : 3.17491
[1mStep[0m  [24/42], [94mLoss[0m : 3.12771
[1mStep[0m  [28/42], [94mLoss[0m : 3.19437
[1mStep[0m  [32/42], [94mLoss[0m : 3.26576
[1mStep[0m  [36/42], [94mLoss[0m : 3.18651
[1mStep[0m  [40/42], [94mLoss[0m : 3.27431

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.194, [92mTest[0m: 3.129, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.26227
[1mStep[0m  [4/42], [94mLoss[0m : 3.04704
[1mStep[0m  [8/42], [94mLoss[0m : 3.05962
[1mStep[0m  [12/42], [94mLoss[0m : 3.26606
[1mStep[0m  [16/42], [94mLoss[0m : 3.22041
[1mStep[0m  [20/42], [94mLoss[0m : 3.14066
[1mStep[0m  [24/42], [94mLoss[0m : 3.29852
[1mStep[0m  [28/42], [94mLoss[0m : 2.96770
[1mStep[0m  [32/42], [94mLoss[0m : 2.91155
[1mStep[0m  [36/42], [94mLoss[0m : 3.18521
[1mStep[0m  [40/42], [94mLoss[0m : 3.30856

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.157, [92mTest[0m: 3.094, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.13269
[1mStep[0m  [4/42], [94mLoss[0m : 3.47358
[1mStep[0m  [8/42], [94mLoss[0m : 3.07410
[1mStep[0m  [12/42], [94mLoss[0m : 3.12040
[1mStep[0m  [16/42], [94mLoss[0m : 2.86115
[1mStep[0m  [20/42], [94mLoss[0m : 2.93242
[1mStep[0m  [24/42], [94mLoss[0m : 2.96142
[1mStep[0m  [28/42], [94mLoss[0m : 2.80624
[1mStep[0m  [32/42], [94mLoss[0m : 2.99203
[1mStep[0m  [36/42], [94mLoss[0m : 3.02024
[1mStep[0m  [40/42], [94mLoss[0m : 2.91995

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.104, [92mTest[0m: 3.048, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.34878
[1mStep[0m  [4/42], [94mLoss[0m : 2.86768
[1mStep[0m  [8/42], [94mLoss[0m : 2.92435
[1mStep[0m  [12/42], [94mLoss[0m : 2.89811
[1mStep[0m  [16/42], [94mLoss[0m : 2.91912
[1mStep[0m  [20/42], [94mLoss[0m : 3.14923
[1mStep[0m  [24/42], [94mLoss[0m : 3.14569
[1mStep[0m  [28/42], [94mLoss[0m : 2.86633
[1mStep[0m  [32/42], [94mLoss[0m : 3.08114
[1mStep[0m  [36/42], [94mLoss[0m : 3.20674
[1mStep[0m  [40/42], [94mLoss[0m : 3.17651

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.090, [92mTest[0m: 3.015, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87376
[1mStep[0m  [4/42], [94mLoss[0m : 3.09783
[1mStep[0m  [8/42], [94mLoss[0m : 3.27339
[1mStep[0m  [12/42], [94mLoss[0m : 3.01943
[1mStep[0m  [16/42], [94mLoss[0m : 2.81009
[1mStep[0m  [20/42], [94mLoss[0m : 3.08415
[1mStep[0m  [24/42], [94mLoss[0m : 2.86473
[1mStep[0m  [28/42], [94mLoss[0m : 3.14228
[1mStep[0m  [32/42], [94mLoss[0m : 3.09245
[1mStep[0m  [36/42], [94mLoss[0m : 2.91481
[1mStep[0m  [40/42], [94mLoss[0m : 3.14743

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.050, [92mTest[0m: 2.975, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.08271
[1mStep[0m  [4/42], [94mLoss[0m : 2.87845
[1mStep[0m  [8/42], [94mLoss[0m : 2.96591
[1mStep[0m  [12/42], [94mLoss[0m : 3.29221
[1mStep[0m  [16/42], [94mLoss[0m : 2.88668
[1mStep[0m  [20/42], [94mLoss[0m : 2.94721
[1mStep[0m  [24/42], [94mLoss[0m : 3.06855
[1mStep[0m  [28/42], [94mLoss[0m : 3.13793
[1mStep[0m  [32/42], [94mLoss[0m : 3.19055
[1mStep[0m  [36/42], [94mLoss[0m : 2.86811
[1mStep[0m  [40/42], [94mLoss[0m : 2.97463

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.028, [92mTest[0m: 2.939, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.16134
[1mStep[0m  [4/42], [94mLoss[0m : 3.13986
[1mStep[0m  [8/42], [94mLoss[0m : 3.19883
[1mStep[0m  [12/42], [94mLoss[0m : 2.76886
[1mStep[0m  [16/42], [94mLoss[0m : 3.06567
[1mStep[0m  [20/42], [94mLoss[0m : 3.30198
[1mStep[0m  [24/42], [94mLoss[0m : 2.80696
[1mStep[0m  [28/42], [94mLoss[0m : 3.05364
[1mStep[0m  [32/42], [94mLoss[0m : 3.01937
[1mStep[0m  [36/42], [94mLoss[0m : 3.00763
[1mStep[0m  [40/42], [94mLoss[0m : 2.95204

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.000, [92mTest[0m: 2.920, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07224
[1mStep[0m  [4/42], [94mLoss[0m : 2.95309
[1mStep[0m  [8/42], [94mLoss[0m : 3.36837
[1mStep[0m  [12/42], [94mLoss[0m : 3.04578
[1mStep[0m  [16/42], [94mLoss[0m : 2.95649
[1mStep[0m  [20/42], [94mLoss[0m : 2.93575
[1mStep[0m  [24/42], [94mLoss[0m : 2.94918
[1mStep[0m  [28/42], [94mLoss[0m : 2.88073
[1mStep[0m  [32/42], [94mLoss[0m : 3.10988
[1mStep[0m  [36/42], [94mLoss[0m : 2.89431
[1mStep[0m  [40/42], [94mLoss[0m : 3.00593

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.975, [92mTest[0m: 2.894, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04812
[1mStep[0m  [4/42], [94mLoss[0m : 2.65341
[1mStep[0m  [8/42], [94mLoss[0m : 3.18031
[1mStep[0m  [12/42], [94mLoss[0m : 2.85977
[1mStep[0m  [16/42], [94mLoss[0m : 2.94256
[1mStep[0m  [20/42], [94mLoss[0m : 2.58847
[1mStep[0m  [24/42], [94mLoss[0m : 3.00751
[1mStep[0m  [28/42], [94mLoss[0m : 3.04031
[1mStep[0m  [32/42], [94mLoss[0m : 2.86060
[1mStep[0m  [36/42], [94mLoss[0m : 3.02251
[1mStep[0m  [40/42], [94mLoss[0m : 2.90159

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.952, [92mTest[0m: 2.865, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80959
[1mStep[0m  [4/42], [94mLoss[0m : 2.92148
[1mStep[0m  [8/42], [94mLoss[0m : 2.84546
[1mStep[0m  [12/42], [94mLoss[0m : 2.75127
[1mStep[0m  [16/42], [94mLoss[0m : 2.55870
[1mStep[0m  [20/42], [94mLoss[0m : 3.33902
[1mStep[0m  [24/42], [94mLoss[0m : 3.14221
[1mStep[0m  [28/42], [94mLoss[0m : 3.04870
[1mStep[0m  [32/42], [94mLoss[0m : 2.73842
[1mStep[0m  [36/42], [94mLoss[0m : 2.95862
[1mStep[0m  [40/42], [94mLoss[0m : 2.92202

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.946, [92mTest[0m: 2.842, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.01398
[1mStep[0m  [4/42], [94mLoss[0m : 2.91329
[1mStep[0m  [8/42], [94mLoss[0m : 2.75607
[1mStep[0m  [12/42], [94mLoss[0m : 2.88554
[1mStep[0m  [16/42], [94mLoss[0m : 3.21250
[1mStep[0m  [20/42], [94mLoss[0m : 2.70839
[1mStep[0m  [24/42], [94mLoss[0m : 3.06338
[1mStep[0m  [28/42], [94mLoss[0m : 2.95978
[1mStep[0m  [32/42], [94mLoss[0m : 2.68410
[1mStep[0m  [36/42], [94mLoss[0m : 2.87885
[1mStep[0m  [40/42], [94mLoss[0m : 2.83804

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.907, [92mTest[0m: 2.809, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.99406
[1mStep[0m  [4/42], [94mLoss[0m : 2.94546
[1mStep[0m  [8/42], [94mLoss[0m : 2.99254
[1mStep[0m  [12/42], [94mLoss[0m : 2.63891
[1mStep[0m  [16/42], [94mLoss[0m : 2.76847
[1mStep[0m  [20/42], [94mLoss[0m : 2.96002
[1mStep[0m  [24/42], [94mLoss[0m : 2.80428
[1mStep[0m  [28/42], [94mLoss[0m : 3.02450
[1mStep[0m  [32/42], [94mLoss[0m : 2.95308
[1mStep[0m  [36/42], [94mLoss[0m : 2.96051
[1mStep[0m  [40/42], [94mLoss[0m : 2.98858

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.903, [92mTest[0m: 2.795, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77244
[1mStep[0m  [4/42], [94mLoss[0m : 2.76023
[1mStep[0m  [8/42], [94mLoss[0m : 2.85656
[1mStep[0m  [12/42], [94mLoss[0m : 2.98901
[1mStep[0m  [16/42], [94mLoss[0m : 2.89535
[1mStep[0m  [20/42], [94mLoss[0m : 3.24535
[1mStep[0m  [24/42], [94mLoss[0m : 3.02544
[1mStep[0m  [28/42], [94mLoss[0m : 2.92274
[1mStep[0m  [32/42], [94mLoss[0m : 2.68073
[1mStep[0m  [36/42], [94mLoss[0m : 2.79354
[1mStep[0m  [40/42], [94mLoss[0m : 2.93898

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.876, [92mTest[0m: 2.774, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62398
[1mStep[0m  [4/42], [94mLoss[0m : 2.78656
[1mStep[0m  [8/42], [94mLoss[0m : 2.70627
[1mStep[0m  [12/42], [94mLoss[0m : 2.80267
[1mStep[0m  [16/42], [94mLoss[0m : 2.68236
[1mStep[0m  [20/42], [94mLoss[0m : 3.11784
[1mStep[0m  [24/42], [94mLoss[0m : 2.90614
[1mStep[0m  [28/42], [94mLoss[0m : 2.92902
[1mStep[0m  [32/42], [94mLoss[0m : 3.23723
[1mStep[0m  [36/42], [94mLoss[0m : 2.90574
[1mStep[0m  [40/42], [94mLoss[0m : 2.84384

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.868, [92mTest[0m: 2.751, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.743
====================================

Phase 2 - Evaluation MAE:  2.742701836994716
MAE score P1        4.615614
MAE score P2        2.742702
loss                2.867533
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay           0.001
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.56816
[1mStep[0m  [4/42], [94mLoss[0m : 10.39998
[1mStep[0m  [8/42], [94mLoss[0m : 10.47944
[1mStep[0m  [12/42], [94mLoss[0m : 10.42676
[1mStep[0m  [16/42], [94mLoss[0m : 10.20291
[1mStep[0m  [20/42], [94mLoss[0m : 9.87511
[1mStep[0m  [24/42], [94mLoss[0m : 10.29548
[1mStep[0m  [28/42], [94mLoss[0m : 10.19458
[1mStep[0m  [32/42], [94mLoss[0m : 10.18162
[1mStep[0m  [36/42], [94mLoss[0m : 10.05368
[1mStep[0m  [40/42], [94mLoss[0m : 9.84033

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.294, [92mTest[0m: 10.749, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.83234
[1mStep[0m  [4/42], [94mLoss[0m : 10.15542
[1mStep[0m  [8/42], [94mLoss[0m : 10.25509
[1mStep[0m  [12/42], [94mLoss[0m : 10.15494
[1mStep[0m  [16/42], [94mLoss[0m : 9.71119
[1mStep[0m  [20/42], [94mLoss[0m : 9.52642
[1mStep[0m  [24/42], [94mLoss[0m : 9.90891
[1mStep[0m  [28/42], [94mLoss[0m : 9.86667
[1mStep[0m  [32/42], [94mLoss[0m : 9.49306
[1mStep[0m  [36/42], [94mLoss[0m : 9.04323
[1mStep[0m  [40/42], [94mLoss[0m : 9.61171

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.653, [92mTest[0m: 10.172, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.45146
[1mStep[0m  [4/42], [94mLoss[0m : 9.25727
[1mStep[0m  [8/42], [94mLoss[0m : 9.18858
[1mStep[0m  [12/42], [94mLoss[0m : 9.55755
[1mStep[0m  [16/42], [94mLoss[0m : 8.72910
[1mStep[0m  [20/42], [94mLoss[0m : 8.43402
[1mStep[0m  [24/42], [94mLoss[0m : 9.01711
[1mStep[0m  [28/42], [94mLoss[0m : 8.64256
[1mStep[0m  [32/42], [94mLoss[0m : 8.97622
[1mStep[0m  [36/42], [94mLoss[0m : 8.65331
[1mStep[0m  [40/42], [94mLoss[0m : 8.62033

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.976, [92mTest[0m: 9.578, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.85994
[1mStep[0m  [4/42], [94mLoss[0m : 8.52642
[1mStep[0m  [8/42], [94mLoss[0m : 8.79590
[1mStep[0m  [12/42], [94mLoss[0m : 8.72627
[1mStep[0m  [16/42], [94mLoss[0m : 8.11474
[1mStep[0m  [20/42], [94mLoss[0m : 8.28425
[1mStep[0m  [24/42], [94mLoss[0m : 7.94570
[1mStep[0m  [28/42], [94mLoss[0m : 8.16951
[1mStep[0m  [32/42], [94mLoss[0m : 7.87439
[1mStep[0m  [36/42], [94mLoss[0m : 7.96982
[1mStep[0m  [40/42], [94mLoss[0m : 7.82954

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.300, [92mTest[0m: 8.997, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.02946
[1mStep[0m  [4/42], [94mLoss[0m : 7.67231
[1mStep[0m  [8/42], [94mLoss[0m : 8.23562
[1mStep[0m  [12/42], [94mLoss[0m : 7.85166
[1mStep[0m  [16/42], [94mLoss[0m : 7.81015
[1mStep[0m  [20/42], [94mLoss[0m : 7.41782
[1mStep[0m  [24/42], [94mLoss[0m : 7.29157
[1mStep[0m  [28/42], [94mLoss[0m : 7.43635
[1mStep[0m  [32/42], [94mLoss[0m : 7.55527
[1mStep[0m  [36/42], [94mLoss[0m : 7.67083
[1mStep[0m  [40/42], [94mLoss[0m : 7.34175

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.628, [92mTest[0m: 8.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.67683
[1mStep[0m  [4/42], [94mLoss[0m : 7.36246
[1mStep[0m  [8/42], [94mLoss[0m : 6.91813
[1mStep[0m  [12/42], [94mLoss[0m : 7.14390
[1mStep[0m  [16/42], [94mLoss[0m : 7.32771
[1mStep[0m  [20/42], [94mLoss[0m : 6.94410
[1mStep[0m  [24/42], [94mLoss[0m : 6.87517
[1mStep[0m  [28/42], [94mLoss[0m : 7.22048
[1mStep[0m  [32/42], [94mLoss[0m : 7.03547
[1mStep[0m  [36/42], [94mLoss[0m : 6.77564
[1mStep[0m  [40/42], [94mLoss[0m : 6.32562

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.987, [92mTest[0m: 7.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.76553
[1mStep[0m  [4/42], [94mLoss[0m : 6.40219
[1mStep[0m  [8/42], [94mLoss[0m : 6.58663
[1mStep[0m  [12/42], [94mLoss[0m : 6.68720
[1mStep[0m  [16/42], [94mLoss[0m : 6.76577
[1mStep[0m  [20/42], [94mLoss[0m : 6.67616
[1mStep[0m  [24/42], [94mLoss[0m : 6.54189
[1mStep[0m  [28/42], [94mLoss[0m : 6.12559
[1mStep[0m  [32/42], [94mLoss[0m : 6.30609
[1mStep[0m  [36/42], [94mLoss[0m : 6.08793
[1mStep[0m  [40/42], [94mLoss[0m : 6.30917

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.364, [92mTest[0m: 7.346, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.99256
[1mStep[0m  [4/42], [94mLoss[0m : 5.55879
[1mStep[0m  [8/42], [94mLoss[0m : 5.68817
[1mStep[0m  [12/42], [94mLoss[0m : 6.02687
[1mStep[0m  [16/42], [94mLoss[0m : 5.86212
[1mStep[0m  [20/42], [94mLoss[0m : 5.73078
[1mStep[0m  [24/42], [94mLoss[0m : 5.90101
[1mStep[0m  [28/42], [94mLoss[0m : 5.44499
[1mStep[0m  [32/42], [94mLoss[0m : 5.67821
[1mStep[0m  [36/42], [94mLoss[0m : 5.71276
[1mStep[0m  [40/42], [94mLoss[0m : 5.73418

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.750, [92mTest[0m: 6.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.51069
[1mStep[0m  [4/42], [94mLoss[0m : 5.65993
[1mStep[0m  [8/42], [94mLoss[0m : 5.09151
[1mStep[0m  [12/42], [94mLoss[0m : 4.68765
[1mStep[0m  [16/42], [94mLoss[0m : 5.05797
[1mStep[0m  [20/42], [94mLoss[0m : 5.42710
[1mStep[0m  [24/42], [94mLoss[0m : 4.85163
[1mStep[0m  [28/42], [94mLoss[0m : 5.21714
[1mStep[0m  [32/42], [94mLoss[0m : 4.86925
[1mStep[0m  [36/42], [94mLoss[0m : 4.41560
[1mStep[0m  [40/42], [94mLoss[0m : 4.84158

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.182, [92mTest[0m: 6.179, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.04345
[1mStep[0m  [4/42], [94mLoss[0m : 4.76578
[1mStep[0m  [8/42], [94mLoss[0m : 4.49067
[1mStep[0m  [12/42], [94mLoss[0m : 4.92525
[1mStep[0m  [16/42], [94mLoss[0m : 4.72903
[1mStep[0m  [20/42], [94mLoss[0m : 4.62562
[1mStep[0m  [24/42], [94mLoss[0m : 4.43454
[1mStep[0m  [28/42], [94mLoss[0m : 4.62221
[1mStep[0m  [32/42], [94mLoss[0m : 4.62608
[1mStep[0m  [36/42], [94mLoss[0m : 4.93681
[1mStep[0m  [40/42], [94mLoss[0m : 4.37514

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.637, [92mTest[0m: 5.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.27717
[1mStep[0m  [4/42], [94mLoss[0m : 4.21014
[1mStep[0m  [8/42], [94mLoss[0m : 4.26628
[1mStep[0m  [12/42], [94mLoss[0m : 4.58918
[1mStep[0m  [16/42], [94mLoss[0m : 4.30239
[1mStep[0m  [20/42], [94mLoss[0m : 4.21076
[1mStep[0m  [24/42], [94mLoss[0m : 4.33997
[1mStep[0m  [28/42], [94mLoss[0m : 4.37834
[1mStep[0m  [32/42], [94mLoss[0m : 4.02004
[1mStep[0m  [36/42], [94mLoss[0m : 3.71505
[1mStep[0m  [40/42], [94mLoss[0m : 3.87022

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.164, [92mTest[0m: 4.951, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.64829
[1mStep[0m  [4/42], [94mLoss[0m : 3.70253
[1mStep[0m  [8/42], [94mLoss[0m : 3.96558
[1mStep[0m  [12/42], [94mLoss[0m : 3.85462
[1mStep[0m  [16/42], [94mLoss[0m : 3.80546
[1mStep[0m  [20/42], [94mLoss[0m : 3.78858
[1mStep[0m  [24/42], [94mLoss[0m : 3.92972
[1mStep[0m  [28/42], [94mLoss[0m : 3.63843
[1mStep[0m  [32/42], [94mLoss[0m : 3.71149
[1mStep[0m  [36/42], [94mLoss[0m : 3.41572
[1mStep[0m  [40/42], [94mLoss[0m : 3.42661

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.780, [92mTest[0m: 4.408, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.50743
[1mStep[0m  [4/42], [94mLoss[0m : 3.44499
[1mStep[0m  [8/42], [94mLoss[0m : 3.28011
[1mStep[0m  [12/42], [94mLoss[0m : 3.49500
[1mStep[0m  [16/42], [94mLoss[0m : 3.36184
[1mStep[0m  [20/42], [94mLoss[0m : 3.61654
[1mStep[0m  [24/42], [94mLoss[0m : 3.71813
[1mStep[0m  [28/42], [94mLoss[0m : 3.55415
[1mStep[0m  [32/42], [94mLoss[0m : 3.36922
[1mStep[0m  [36/42], [94mLoss[0m : 3.31462
[1mStep[0m  [40/42], [94mLoss[0m : 3.26072

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.462, [92mTest[0m: 3.982, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.19853
[1mStep[0m  [4/42], [94mLoss[0m : 3.28938
[1mStep[0m  [8/42], [94mLoss[0m : 3.16790
[1mStep[0m  [12/42], [94mLoss[0m : 3.05679
[1mStep[0m  [16/42], [94mLoss[0m : 3.01321
[1mStep[0m  [20/42], [94mLoss[0m : 3.20295
[1mStep[0m  [24/42], [94mLoss[0m : 3.29441
[1mStep[0m  [28/42], [94mLoss[0m : 3.34772
[1mStep[0m  [32/42], [94mLoss[0m : 2.94125
[1mStep[0m  [36/42], [94mLoss[0m : 3.14740
[1mStep[0m  [40/42], [94mLoss[0m : 3.18122

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.216, [92mTest[0m: 3.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15290
[1mStep[0m  [4/42], [94mLoss[0m : 3.26250
[1mStep[0m  [8/42], [94mLoss[0m : 3.10596
[1mStep[0m  [12/42], [94mLoss[0m : 3.07992
[1mStep[0m  [16/42], [94mLoss[0m : 3.18528
[1mStep[0m  [20/42], [94mLoss[0m : 2.79366
[1mStep[0m  [24/42], [94mLoss[0m : 3.04403
[1mStep[0m  [28/42], [94mLoss[0m : 3.13938
[1mStep[0m  [32/42], [94mLoss[0m : 3.25631
[1mStep[0m  [36/42], [94mLoss[0m : 3.19932
[1mStep[0m  [40/42], [94mLoss[0m : 2.97335

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.094, [92mTest[0m: 3.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.02087
[1mStep[0m  [4/42], [94mLoss[0m : 2.92049
[1mStep[0m  [8/42], [94mLoss[0m : 2.79917
[1mStep[0m  [12/42], [94mLoss[0m : 2.85266
[1mStep[0m  [16/42], [94mLoss[0m : 2.95714
[1mStep[0m  [20/42], [94mLoss[0m : 2.77600
[1mStep[0m  [24/42], [94mLoss[0m : 2.98955
[1mStep[0m  [28/42], [94mLoss[0m : 2.93721
[1mStep[0m  [32/42], [94mLoss[0m : 3.03951
[1mStep[0m  [36/42], [94mLoss[0m : 2.97835
[1mStep[0m  [40/42], [94mLoss[0m : 2.70912

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.963, [92mTest[0m: 3.207, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04879
[1mStep[0m  [4/42], [94mLoss[0m : 2.88458
[1mStep[0m  [8/42], [94mLoss[0m : 2.90479
[1mStep[0m  [12/42], [94mLoss[0m : 2.87574
[1mStep[0m  [16/42], [94mLoss[0m : 2.83315
[1mStep[0m  [20/42], [94mLoss[0m : 3.07717
[1mStep[0m  [24/42], [94mLoss[0m : 2.72346
[1mStep[0m  [28/42], [94mLoss[0m : 2.72673
[1mStep[0m  [32/42], [94mLoss[0m : 2.56012
[1mStep[0m  [36/42], [94mLoss[0m : 2.97231
[1mStep[0m  [40/42], [94mLoss[0m : 2.78618

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.869, [92mTest[0m: 3.054, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53695
[1mStep[0m  [4/42], [94mLoss[0m : 2.93670
[1mStep[0m  [8/42], [94mLoss[0m : 2.82144
[1mStep[0m  [12/42], [94mLoss[0m : 2.91661
[1mStep[0m  [16/42], [94mLoss[0m : 2.63080
[1mStep[0m  [20/42], [94mLoss[0m : 2.76811
[1mStep[0m  [24/42], [94mLoss[0m : 2.63233
[1mStep[0m  [28/42], [94mLoss[0m : 2.79941
[1mStep[0m  [32/42], [94mLoss[0m : 2.58518
[1mStep[0m  [36/42], [94mLoss[0m : 2.82011
[1mStep[0m  [40/42], [94mLoss[0m : 2.81007

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.803, [92mTest[0m: 2.941, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.90429
[1mStep[0m  [4/42], [94mLoss[0m : 2.75785
[1mStep[0m  [8/42], [94mLoss[0m : 2.68843
[1mStep[0m  [12/42], [94mLoss[0m : 2.56626
[1mStep[0m  [16/42], [94mLoss[0m : 2.93507
[1mStep[0m  [20/42], [94mLoss[0m : 2.73518
[1mStep[0m  [24/42], [94mLoss[0m : 2.80531
[1mStep[0m  [28/42], [94mLoss[0m : 2.62424
[1mStep[0m  [32/42], [94mLoss[0m : 2.80180
[1mStep[0m  [36/42], [94mLoss[0m : 2.63068
[1mStep[0m  [40/42], [94mLoss[0m : 2.63417

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76818
[1mStep[0m  [4/42], [94mLoss[0m : 2.87656
[1mStep[0m  [8/42], [94mLoss[0m : 2.66422
[1mStep[0m  [12/42], [94mLoss[0m : 2.75888
[1mStep[0m  [16/42], [94mLoss[0m : 2.82468
[1mStep[0m  [20/42], [94mLoss[0m : 2.76227
[1mStep[0m  [24/42], [94mLoss[0m : 2.74218
[1mStep[0m  [28/42], [94mLoss[0m : 2.49679
[1mStep[0m  [32/42], [94mLoss[0m : 2.54916
[1mStep[0m  [36/42], [94mLoss[0m : 2.80748
[1mStep[0m  [40/42], [94mLoss[0m : 2.70135

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80341
[1mStep[0m  [4/42], [94mLoss[0m : 2.46599
[1mStep[0m  [8/42], [94mLoss[0m : 2.80060
[1mStep[0m  [12/42], [94mLoss[0m : 2.64656
[1mStep[0m  [16/42], [94mLoss[0m : 3.00132
[1mStep[0m  [20/42], [94mLoss[0m : 2.77767
[1mStep[0m  [24/42], [94mLoss[0m : 2.76474
[1mStep[0m  [28/42], [94mLoss[0m : 2.46553
[1mStep[0m  [32/42], [94mLoss[0m : 2.82421
[1mStep[0m  [36/42], [94mLoss[0m : 2.76527
[1mStep[0m  [40/42], [94mLoss[0m : 2.57304

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.691, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72578
[1mStep[0m  [4/42], [94mLoss[0m : 2.65021
[1mStep[0m  [8/42], [94mLoss[0m : 2.88624
[1mStep[0m  [12/42], [94mLoss[0m : 2.92011
[1mStep[0m  [16/42], [94mLoss[0m : 2.65637
[1mStep[0m  [20/42], [94mLoss[0m : 2.52449
[1mStep[0m  [24/42], [94mLoss[0m : 2.91899
[1mStep[0m  [28/42], [94mLoss[0m : 2.65144
[1mStep[0m  [32/42], [94mLoss[0m : 2.73918
[1mStep[0m  [36/42], [94mLoss[0m : 2.71206
[1mStep[0m  [40/42], [94mLoss[0m : 2.86653

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57161
[1mStep[0m  [4/42], [94mLoss[0m : 2.69807
[1mStep[0m  [8/42], [94mLoss[0m : 2.75940
[1mStep[0m  [12/42], [94mLoss[0m : 2.38347
[1mStep[0m  [16/42], [94mLoss[0m : 2.68943
[1mStep[0m  [20/42], [94mLoss[0m : 2.62311
[1mStep[0m  [24/42], [94mLoss[0m : 2.85849
[1mStep[0m  [28/42], [94mLoss[0m : 2.63796
[1mStep[0m  [32/42], [94mLoss[0m : 2.65824
[1mStep[0m  [36/42], [94mLoss[0m : 2.55726
[1mStep[0m  [40/42], [94mLoss[0m : 2.83770

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.650, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74767
[1mStep[0m  [4/42], [94mLoss[0m : 2.47809
[1mStep[0m  [8/42], [94mLoss[0m : 2.60208
[1mStep[0m  [12/42], [94mLoss[0m : 2.57164
[1mStep[0m  [16/42], [94mLoss[0m : 2.66738
[1mStep[0m  [20/42], [94mLoss[0m : 2.57742
[1mStep[0m  [24/42], [94mLoss[0m : 2.63468
[1mStep[0m  [28/42], [94mLoss[0m : 2.53127
[1mStep[0m  [32/42], [94mLoss[0m : 2.44089
[1mStep[0m  [36/42], [94mLoss[0m : 2.65191
[1mStep[0m  [40/42], [94mLoss[0m : 2.73963

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.642, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44853
[1mStep[0m  [4/42], [94mLoss[0m : 2.61392
[1mStep[0m  [8/42], [94mLoss[0m : 2.67444
[1mStep[0m  [12/42], [94mLoss[0m : 2.79716
[1mStep[0m  [16/42], [94mLoss[0m : 2.54012
[1mStep[0m  [20/42], [94mLoss[0m : 2.43982
[1mStep[0m  [24/42], [94mLoss[0m : 2.66820
[1mStep[0m  [28/42], [94mLoss[0m : 2.86747
[1mStep[0m  [32/42], [94mLoss[0m : 3.03148
[1mStep[0m  [36/42], [94mLoss[0m : 2.58110
[1mStep[0m  [40/42], [94mLoss[0m : 2.79180

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.623, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58639
[1mStep[0m  [4/42], [94mLoss[0m : 2.64141
[1mStep[0m  [8/42], [94mLoss[0m : 2.55825
[1mStep[0m  [12/42], [94mLoss[0m : 2.59274
[1mStep[0m  [16/42], [94mLoss[0m : 2.95223
[1mStep[0m  [20/42], [94mLoss[0m : 2.58400
[1mStep[0m  [24/42], [94mLoss[0m : 2.44532
[1mStep[0m  [28/42], [94mLoss[0m : 2.71184
[1mStep[0m  [32/42], [94mLoss[0m : 2.69190
[1mStep[0m  [36/42], [94mLoss[0m : 2.53508
[1mStep[0m  [40/42], [94mLoss[0m : 2.42426

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61157
[1mStep[0m  [4/42], [94mLoss[0m : 2.62752
[1mStep[0m  [8/42], [94mLoss[0m : 2.89292
[1mStep[0m  [12/42], [94mLoss[0m : 2.57027
[1mStep[0m  [16/42], [94mLoss[0m : 2.66352
[1mStep[0m  [20/42], [94mLoss[0m : 2.72374
[1mStep[0m  [24/42], [94mLoss[0m : 2.70036
[1mStep[0m  [28/42], [94mLoss[0m : 2.73563
[1mStep[0m  [32/42], [94mLoss[0m : 2.87370
[1mStep[0m  [36/42], [94mLoss[0m : 2.47503
[1mStep[0m  [40/42], [94mLoss[0m : 2.67552

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79495
[1mStep[0m  [4/42], [94mLoss[0m : 2.87047
[1mStep[0m  [8/42], [94mLoss[0m : 2.53679
[1mStep[0m  [12/42], [94mLoss[0m : 2.51597
[1mStep[0m  [16/42], [94mLoss[0m : 2.63310
[1mStep[0m  [20/42], [94mLoss[0m : 2.32588
[1mStep[0m  [24/42], [94mLoss[0m : 2.51583
[1mStep[0m  [28/42], [94mLoss[0m : 2.48104
[1mStep[0m  [32/42], [94mLoss[0m : 2.78899
[1mStep[0m  [36/42], [94mLoss[0m : 2.75954
[1mStep[0m  [40/42], [94mLoss[0m : 2.78181

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.564, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66540
[1mStep[0m  [4/42], [94mLoss[0m : 2.62349
[1mStep[0m  [8/42], [94mLoss[0m : 2.52682
[1mStep[0m  [12/42], [94mLoss[0m : 2.40777
[1mStep[0m  [16/42], [94mLoss[0m : 2.52287
[1mStep[0m  [20/42], [94mLoss[0m : 2.67645
[1mStep[0m  [24/42], [94mLoss[0m : 2.65673
[1mStep[0m  [28/42], [94mLoss[0m : 2.58164
[1mStep[0m  [32/42], [94mLoss[0m : 2.52547
[1mStep[0m  [36/42], [94mLoss[0m : 2.66878
[1mStep[0m  [40/42], [94mLoss[0m : 2.67960

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.566, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54747
[1mStep[0m  [4/42], [94mLoss[0m : 2.51204
[1mStep[0m  [8/42], [94mLoss[0m : 2.91783
[1mStep[0m  [12/42], [94mLoss[0m : 2.61647
[1mStep[0m  [16/42], [94mLoss[0m : 2.55361
[1mStep[0m  [20/42], [94mLoss[0m : 2.72319
[1mStep[0m  [24/42], [94mLoss[0m : 2.63106
[1mStep[0m  [28/42], [94mLoss[0m : 2.58432
[1mStep[0m  [32/42], [94mLoss[0m : 2.77612
[1mStep[0m  [36/42], [94mLoss[0m : 2.59145
[1mStep[0m  [40/42], [94mLoss[0m : 2.57677

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.550, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.536
====================================

Phase 1 - Evaluation MAE:  2.5358061620167325
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.74739
[1mStep[0m  [4/42], [94mLoss[0m : 2.43110
[1mStep[0m  [8/42], [94mLoss[0m : 2.63012
[1mStep[0m  [12/42], [94mLoss[0m : 2.76505
[1mStep[0m  [16/42], [94mLoss[0m : 2.89484
[1mStep[0m  [20/42], [94mLoss[0m : 2.50774
[1mStep[0m  [24/42], [94mLoss[0m : 2.61225
[1mStep[0m  [28/42], [94mLoss[0m : 2.60076
[1mStep[0m  [32/42], [94mLoss[0m : 2.50642
[1mStep[0m  [36/42], [94mLoss[0m : 2.50056
[1mStep[0m  [40/42], [94mLoss[0m : 2.56126

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55437
[1mStep[0m  [4/42], [94mLoss[0m : 2.75257
[1mStep[0m  [8/42], [94mLoss[0m : 2.67058
[1mStep[0m  [12/42], [94mLoss[0m : 2.91879
[1mStep[0m  [16/42], [94mLoss[0m : 2.53510
[1mStep[0m  [20/42], [94mLoss[0m : 2.52268
[1mStep[0m  [24/42], [94mLoss[0m : 2.68779
[1mStep[0m  [28/42], [94mLoss[0m : 2.53679
[1mStep[0m  [32/42], [94mLoss[0m : 2.70607
[1mStep[0m  [36/42], [94mLoss[0m : 2.58549
[1mStep[0m  [40/42], [94mLoss[0m : 2.66155

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40461
[1mStep[0m  [4/42], [94mLoss[0m : 2.63597
[1mStep[0m  [8/42], [94mLoss[0m : 2.77439
[1mStep[0m  [12/42], [94mLoss[0m : 2.58885
[1mStep[0m  [16/42], [94mLoss[0m : 2.59369
[1mStep[0m  [20/42], [94mLoss[0m : 2.84899
[1mStep[0m  [24/42], [94mLoss[0m : 2.49559
[1mStep[0m  [28/42], [94mLoss[0m : 2.45954
[1mStep[0m  [32/42], [94mLoss[0m : 2.52051
[1mStep[0m  [36/42], [94mLoss[0m : 2.59989
[1mStep[0m  [40/42], [94mLoss[0m : 2.50744

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64853
[1mStep[0m  [4/42], [94mLoss[0m : 2.59109
[1mStep[0m  [8/42], [94mLoss[0m : 2.71352
[1mStep[0m  [12/42], [94mLoss[0m : 2.60158
[1mStep[0m  [16/42], [94mLoss[0m : 2.46487
[1mStep[0m  [20/42], [94mLoss[0m : 2.52507
[1mStep[0m  [24/42], [94mLoss[0m : 2.42822
[1mStep[0m  [28/42], [94mLoss[0m : 2.76341
[1mStep[0m  [32/42], [94mLoss[0m : 2.33270
[1mStep[0m  [36/42], [94mLoss[0m : 2.61473
[1mStep[0m  [40/42], [94mLoss[0m : 2.63752

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.507, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68153
[1mStep[0m  [4/42], [94mLoss[0m : 2.40591
[1mStep[0m  [8/42], [94mLoss[0m : 2.70966
[1mStep[0m  [12/42], [94mLoss[0m : 2.45705
[1mStep[0m  [16/42], [94mLoss[0m : 2.71128
[1mStep[0m  [20/42], [94mLoss[0m : 2.65377
[1mStep[0m  [24/42], [94mLoss[0m : 2.83697
[1mStep[0m  [28/42], [94mLoss[0m : 2.47000
[1mStep[0m  [32/42], [94mLoss[0m : 2.43753
[1mStep[0m  [36/42], [94mLoss[0m : 2.64069
[1mStep[0m  [40/42], [94mLoss[0m : 2.37466

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57493
[1mStep[0m  [4/42], [94mLoss[0m : 2.44429
[1mStep[0m  [8/42], [94mLoss[0m : 2.46487
[1mStep[0m  [12/42], [94mLoss[0m : 2.67103
[1mStep[0m  [16/42], [94mLoss[0m : 2.68176
[1mStep[0m  [20/42], [94mLoss[0m : 2.53581
[1mStep[0m  [24/42], [94mLoss[0m : 2.62688
[1mStep[0m  [28/42], [94mLoss[0m : 2.62100
[1mStep[0m  [32/42], [94mLoss[0m : 2.58154
[1mStep[0m  [36/42], [94mLoss[0m : 2.61854
[1mStep[0m  [40/42], [94mLoss[0m : 2.73375

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43815
[1mStep[0m  [4/42], [94mLoss[0m : 2.46799
[1mStep[0m  [8/42], [94mLoss[0m : 2.78672
[1mStep[0m  [12/42], [94mLoss[0m : 2.58228
[1mStep[0m  [16/42], [94mLoss[0m : 2.78918
[1mStep[0m  [20/42], [94mLoss[0m : 2.79594
[1mStep[0m  [24/42], [94mLoss[0m : 2.52501
[1mStep[0m  [28/42], [94mLoss[0m : 2.46846
[1mStep[0m  [32/42], [94mLoss[0m : 2.50903
[1mStep[0m  [36/42], [94mLoss[0m : 2.47149
[1mStep[0m  [40/42], [94mLoss[0m : 2.73512

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43001
[1mStep[0m  [4/42], [94mLoss[0m : 2.48308
[1mStep[0m  [8/42], [94mLoss[0m : 2.46703
[1mStep[0m  [12/42], [94mLoss[0m : 2.43138
[1mStep[0m  [16/42], [94mLoss[0m : 2.67567
[1mStep[0m  [20/42], [94mLoss[0m : 2.54007
[1mStep[0m  [24/42], [94mLoss[0m : 2.51324
[1mStep[0m  [28/42], [94mLoss[0m : 2.64747
[1mStep[0m  [32/42], [94mLoss[0m : 2.75407
[1mStep[0m  [36/42], [94mLoss[0m : 2.47735
[1mStep[0m  [40/42], [94mLoss[0m : 2.61912

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69807
[1mStep[0m  [4/42], [94mLoss[0m : 2.69570
[1mStep[0m  [8/42], [94mLoss[0m : 2.46854
[1mStep[0m  [12/42], [94mLoss[0m : 2.53085
[1mStep[0m  [16/42], [94mLoss[0m : 2.36063
[1mStep[0m  [20/42], [94mLoss[0m : 2.59422
[1mStep[0m  [24/42], [94mLoss[0m : 2.72168
[1mStep[0m  [28/42], [94mLoss[0m : 2.53027
[1mStep[0m  [32/42], [94mLoss[0m : 2.73161
[1mStep[0m  [36/42], [94mLoss[0m : 2.40035
[1mStep[0m  [40/42], [94mLoss[0m : 2.64768

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59199
[1mStep[0m  [4/42], [94mLoss[0m : 2.37384
[1mStep[0m  [8/42], [94mLoss[0m : 2.55229
[1mStep[0m  [12/42], [94mLoss[0m : 2.43295
[1mStep[0m  [16/42], [94mLoss[0m : 2.40181
[1mStep[0m  [20/42], [94mLoss[0m : 2.46908
[1mStep[0m  [24/42], [94mLoss[0m : 2.43625
[1mStep[0m  [28/42], [94mLoss[0m : 2.75601
[1mStep[0m  [32/42], [94mLoss[0m : 2.45038
[1mStep[0m  [36/42], [94mLoss[0m : 2.36295
[1mStep[0m  [40/42], [94mLoss[0m : 2.57360

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39587
[1mStep[0m  [4/42], [94mLoss[0m : 2.58870
[1mStep[0m  [8/42], [94mLoss[0m : 2.37682
[1mStep[0m  [12/42], [94mLoss[0m : 2.46009
[1mStep[0m  [16/42], [94mLoss[0m : 2.62266
[1mStep[0m  [20/42], [94mLoss[0m : 2.71005
[1mStep[0m  [24/42], [94mLoss[0m : 2.74370
[1mStep[0m  [28/42], [94mLoss[0m : 2.55133
[1mStep[0m  [32/42], [94mLoss[0m : 2.44884
[1mStep[0m  [36/42], [94mLoss[0m : 2.78915
[1mStep[0m  [40/42], [94mLoss[0m : 2.71396

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56319
[1mStep[0m  [4/42], [94mLoss[0m : 2.91169
[1mStep[0m  [8/42], [94mLoss[0m : 2.43781
[1mStep[0m  [12/42], [94mLoss[0m : 2.24337
[1mStep[0m  [16/42], [94mLoss[0m : 2.46411
[1mStep[0m  [20/42], [94mLoss[0m : 2.48355
[1mStep[0m  [24/42], [94mLoss[0m : 2.69640
[1mStep[0m  [28/42], [94mLoss[0m : 2.39335
[1mStep[0m  [32/42], [94mLoss[0m : 2.49321
[1mStep[0m  [36/42], [94mLoss[0m : 2.51607
[1mStep[0m  [40/42], [94mLoss[0m : 2.74461

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.692, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74564
[1mStep[0m  [4/42], [94mLoss[0m : 2.59914
[1mStep[0m  [8/42], [94mLoss[0m : 2.48415
[1mStep[0m  [12/42], [94mLoss[0m : 2.48407
[1mStep[0m  [16/42], [94mLoss[0m : 2.53424
[1mStep[0m  [20/42], [94mLoss[0m : 2.67214
[1mStep[0m  [24/42], [94mLoss[0m : 2.43896
[1mStep[0m  [28/42], [94mLoss[0m : 2.79619
[1mStep[0m  [32/42], [94mLoss[0m : 2.52333
[1mStep[0m  [36/42], [94mLoss[0m : 2.41772
[1mStep[0m  [40/42], [94mLoss[0m : 2.80974

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.811, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65208
[1mStep[0m  [4/42], [94mLoss[0m : 2.36199
[1mStep[0m  [8/42], [94mLoss[0m : 2.59461
[1mStep[0m  [12/42], [94mLoss[0m : 2.70728
[1mStep[0m  [16/42], [94mLoss[0m : 2.40179
[1mStep[0m  [20/42], [94mLoss[0m : 2.41667
[1mStep[0m  [24/42], [94mLoss[0m : 2.37879
[1mStep[0m  [28/42], [94mLoss[0m : 2.65441
[1mStep[0m  [32/42], [94mLoss[0m : 2.66797
[1mStep[0m  [36/42], [94mLoss[0m : 2.58053
[1mStep[0m  [40/42], [94mLoss[0m : 2.75163

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58078
[1mStep[0m  [4/42], [94mLoss[0m : 2.36039
[1mStep[0m  [8/42], [94mLoss[0m : 2.37404
[1mStep[0m  [12/42], [94mLoss[0m : 2.45078
[1mStep[0m  [16/42], [94mLoss[0m : 2.57866
[1mStep[0m  [20/42], [94mLoss[0m : 2.44531
[1mStep[0m  [24/42], [94mLoss[0m : 2.38529
[1mStep[0m  [28/42], [94mLoss[0m : 2.49623
[1mStep[0m  [32/42], [94mLoss[0m : 2.57103
[1mStep[0m  [36/42], [94mLoss[0m : 2.55752
[1mStep[0m  [40/42], [94mLoss[0m : 2.46532

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.722, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41889
[1mStep[0m  [4/42], [94mLoss[0m : 2.52969
[1mStep[0m  [8/42], [94mLoss[0m : 2.70231
[1mStep[0m  [12/42], [94mLoss[0m : 2.30422
[1mStep[0m  [16/42], [94mLoss[0m : 2.51996
[1mStep[0m  [20/42], [94mLoss[0m : 2.52478
[1mStep[0m  [24/42], [94mLoss[0m : 2.65860
[1mStep[0m  [28/42], [94mLoss[0m : 2.48092
[1mStep[0m  [32/42], [94mLoss[0m : 2.32834
[1mStep[0m  [36/42], [94mLoss[0m : 2.72172
[1mStep[0m  [40/42], [94mLoss[0m : 2.53907

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44566
[1mStep[0m  [4/42], [94mLoss[0m : 2.44787
[1mStep[0m  [8/42], [94mLoss[0m : 2.58524
[1mStep[0m  [12/42], [94mLoss[0m : 2.64798
[1mStep[0m  [16/42], [94mLoss[0m : 2.48128
[1mStep[0m  [20/42], [94mLoss[0m : 2.29348
[1mStep[0m  [24/42], [94mLoss[0m : 2.64611
[1mStep[0m  [28/42], [94mLoss[0m : 2.38057
[1mStep[0m  [32/42], [94mLoss[0m : 2.45667
[1mStep[0m  [36/42], [94mLoss[0m : 2.44501
[1mStep[0m  [40/42], [94mLoss[0m : 2.43649

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.766, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51232
[1mStep[0m  [4/42], [94mLoss[0m : 2.46283
[1mStep[0m  [8/42], [94mLoss[0m : 2.37818
[1mStep[0m  [12/42], [94mLoss[0m : 2.44392
[1mStep[0m  [16/42], [94mLoss[0m : 2.35907
[1mStep[0m  [20/42], [94mLoss[0m : 2.37182
[1mStep[0m  [24/42], [94mLoss[0m : 2.39024
[1mStep[0m  [28/42], [94mLoss[0m : 2.36893
[1mStep[0m  [32/42], [94mLoss[0m : 2.42553
[1mStep[0m  [36/42], [94mLoss[0m : 2.83098
[1mStep[0m  [40/42], [94mLoss[0m : 2.24687

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.691, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43548
[1mStep[0m  [4/42], [94mLoss[0m : 2.63782
[1mStep[0m  [8/42], [94mLoss[0m : 2.51719
[1mStep[0m  [12/42], [94mLoss[0m : 2.47577
[1mStep[0m  [16/42], [94mLoss[0m : 2.53935
[1mStep[0m  [20/42], [94mLoss[0m : 2.42517
[1mStep[0m  [24/42], [94mLoss[0m : 2.43617
[1mStep[0m  [28/42], [94mLoss[0m : 2.56151
[1mStep[0m  [32/42], [94mLoss[0m : 2.44391
[1mStep[0m  [36/42], [94mLoss[0m : 2.64478
[1mStep[0m  [40/42], [94mLoss[0m : 2.41642

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.741, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38079
[1mStep[0m  [4/42], [94mLoss[0m : 2.61886
[1mStep[0m  [8/42], [94mLoss[0m : 2.44253
[1mStep[0m  [12/42], [94mLoss[0m : 2.60561
[1mStep[0m  [16/42], [94mLoss[0m : 2.31078
[1mStep[0m  [20/42], [94mLoss[0m : 2.36664
[1mStep[0m  [24/42], [94mLoss[0m : 2.50348
[1mStep[0m  [28/42], [94mLoss[0m : 2.72753
[1mStep[0m  [32/42], [94mLoss[0m : 2.37548
[1mStep[0m  [36/42], [94mLoss[0m : 2.43533
[1mStep[0m  [40/42], [94mLoss[0m : 2.47261

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.776, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43505
[1mStep[0m  [4/42], [94mLoss[0m : 2.37975
[1mStep[0m  [8/42], [94mLoss[0m : 2.25645
[1mStep[0m  [12/42], [94mLoss[0m : 2.35974
[1mStep[0m  [16/42], [94mLoss[0m : 2.36671
[1mStep[0m  [20/42], [94mLoss[0m : 2.59929
[1mStep[0m  [24/42], [94mLoss[0m : 2.30602
[1mStep[0m  [28/42], [94mLoss[0m : 2.37764
[1mStep[0m  [32/42], [94mLoss[0m : 2.46950
[1mStep[0m  [36/42], [94mLoss[0m : 2.39265
[1mStep[0m  [40/42], [94mLoss[0m : 2.61351

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.674, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55379
[1mStep[0m  [4/42], [94mLoss[0m : 2.45880
[1mStep[0m  [8/42], [94mLoss[0m : 2.34667
[1mStep[0m  [12/42], [94mLoss[0m : 2.49646
[1mStep[0m  [16/42], [94mLoss[0m : 2.42292
[1mStep[0m  [20/42], [94mLoss[0m : 2.45780
[1mStep[0m  [24/42], [94mLoss[0m : 2.27963
[1mStep[0m  [28/42], [94mLoss[0m : 2.49560
[1mStep[0m  [32/42], [94mLoss[0m : 2.59608
[1mStep[0m  [36/42], [94mLoss[0m : 2.44171
[1mStep[0m  [40/42], [94mLoss[0m : 2.44016

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.670, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59233
[1mStep[0m  [4/42], [94mLoss[0m : 2.53016
[1mStep[0m  [8/42], [94mLoss[0m : 2.74356
[1mStep[0m  [12/42], [94mLoss[0m : 2.33892
[1mStep[0m  [16/42], [94mLoss[0m : 2.51854
[1mStep[0m  [20/42], [94mLoss[0m : 2.52600
[1mStep[0m  [24/42], [94mLoss[0m : 2.42673
[1mStep[0m  [28/42], [94mLoss[0m : 2.34960
[1mStep[0m  [32/42], [94mLoss[0m : 2.68715
[1mStep[0m  [36/42], [94mLoss[0m : 2.36921
[1mStep[0m  [40/42], [94mLoss[0m : 2.30780

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.635, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27409
[1mStep[0m  [4/42], [94mLoss[0m : 2.33069
[1mStep[0m  [8/42], [94mLoss[0m : 2.41377
[1mStep[0m  [12/42], [94mLoss[0m : 2.29399
[1mStep[0m  [16/42], [94mLoss[0m : 2.29712
[1mStep[0m  [20/42], [94mLoss[0m : 2.34059
[1mStep[0m  [24/42], [94mLoss[0m : 2.41296
[1mStep[0m  [28/42], [94mLoss[0m : 2.50313
[1mStep[0m  [32/42], [94mLoss[0m : 2.53967
[1mStep[0m  [36/42], [94mLoss[0m : 2.34043
[1mStep[0m  [40/42], [94mLoss[0m : 2.31830

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.641, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21462
[1mStep[0m  [4/42], [94mLoss[0m : 2.46211
[1mStep[0m  [8/42], [94mLoss[0m : 2.17678
[1mStep[0m  [12/42], [94mLoss[0m : 2.44525
[1mStep[0m  [16/42], [94mLoss[0m : 2.48100
[1mStep[0m  [20/42], [94mLoss[0m : 2.70601
[1mStep[0m  [24/42], [94mLoss[0m : 2.41130
[1mStep[0m  [28/42], [94mLoss[0m : 2.60756
[1mStep[0m  [32/42], [94mLoss[0m : 2.59166
[1mStep[0m  [36/42], [94mLoss[0m : 2.30965
[1mStep[0m  [40/42], [94mLoss[0m : 2.44880

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.645, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40606
[1mStep[0m  [4/42], [94mLoss[0m : 2.41567
[1mStep[0m  [8/42], [94mLoss[0m : 2.36990
[1mStep[0m  [12/42], [94mLoss[0m : 2.51727
[1mStep[0m  [16/42], [94mLoss[0m : 2.42121
[1mStep[0m  [20/42], [94mLoss[0m : 2.35136
[1mStep[0m  [24/42], [94mLoss[0m : 2.41327
[1mStep[0m  [28/42], [94mLoss[0m : 2.48021
[1mStep[0m  [32/42], [94mLoss[0m : 2.52046
[1mStep[0m  [36/42], [94mLoss[0m : 2.58057
[1mStep[0m  [40/42], [94mLoss[0m : 2.34276

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.610, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50299
[1mStep[0m  [4/42], [94mLoss[0m : 2.41890
[1mStep[0m  [8/42], [94mLoss[0m : 2.54793
[1mStep[0m  [12/42], [94mLoss[0m : 2.32123
[1mStep[0m  [16/42], [94mLoss[0m : 2.40569
[1mStep[0m  [20/42], [94mLoss[0m : 2.53991
[1mStep[0m  [24/42], [94mLoss[0m : 2.38433
[1mStep[0m  [28/42], [94mLoss[0m : 2.67933
[1mStep[0m  [32/42], [94mLoss[0m : 2.49455
[1mStep[0m  [36/42], [94mLoss[0m : 2.41324
[1mStep[0m  [40/42], [94mLoss[0m : 2.42525

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.637, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52124
[1mStep[0m  [4/42], [94mLoss[0m : 2.40561
[1mStep[0m  [8/42], [94mLoss[0m : 2.25894
[1mStep[0m  [12/42], [94mLoss[0m : 2.36665
[1mStep[0m  [16/42], [94mLoss[0m : 2.27295
[1mStep[0m  [20/42], [94mLoss[0m : 2.39980
[1mStep[0m  [24/42], [94mLoss[0m : 2.49261
[1mStep[0m  [28/42], [94mLoss[0m : 2.57645
[1mStep[0m  [32/42], [94mLoss[0m : 2.37595
[1mStep[0m  [36/42], [94mLoss[0m : 2.47114
[1mStep[0m  [40/42], [94mLoss[0m : 2.41269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.617, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46304
[1mStep[0m  [4/42], [94mLoss[0m : 2.48434
[1mStep[0m  [8/42], [94mLoss[0m : 2.49653
[1mStep[0m  [12/42], [94mLoss[0m : 2.57913
[1mStep[0m  [16/42], [94mLoss[0m : 2.50177
[1mStep[0m  [20/42], [94mLoss[0m : 2.20784
[1mStep[0m  [24/42], [94mLoss[0m : 2.50627
[1mStep[0m  [28/42], [94mLoss[0m : 2.52675
[1mStep[0m  [32/42], [94mLoss[0m : 2.53001
[1mStep[0m  [36/42], [94mLoss[0m : 2.46612
[1mStep[0m  [40/42], [94mLoss[0m : 2.46632

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40391
[1mStep[0m  [4/42], [94mLoss[0m : 2.29246
[1mStep[0m  [8/42], [94mLoss[0m : 2.35211
[1mStep[0m  [12/42], [94mLoss[0m : 2.32725
[1mStep[0m  [16/42], [94mLoss[0m : 2.46183
[1mStep[0m  [20/42], [94mLoss[0m : 2.53944
[1mStep[0m  [24/42], [94mLoss[0m : 2.52948
[1mStep[0m  [28/42], [94mLoss[0m : 2.35197
[1mStep[0m  [32/42], [94mLoss[0m : 2.38769
[1mStep[0m  [36/42], [94mLoss[0m : 2.40667
[1mStep[0m  [40/42], [94mLoss[0m : 2.78786

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.589, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.547
====================================

Phase 2 - Evaluation MAE:  2.5467017889022827
MAE score P1      2.535806
MAE score P2      2.546702
loss              2.417291
learning_rate       0.0001
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.85544
[1mStep[0m  [2/21], [94mLoss[0m : 10.89965
[1mStep[0m  [4/21], [94mLoss[0m : 10.75648
[1mStep[0m  [6/21], [94mLoss[0m : 11.14683
[1mStep[0m  [8/21], [94mLoss[0m : 10.97500
[1mStep[0m  [10/21], [94mLoss[0m : 10.83747
[1mStep[0m  [12/21], [94mLoss[0m : 10.93578
[1mStep[0m  [14/21], [94mLoss[0m : 10.52176
[1mStep[0m  [16/21], [94mLoss[0m : 10.86113
[1mStep[0m  [18/21], [94mLoss[0m : 10.81116
[1mStep[0m  [20/21], [94mLoss[0m : 11.05013

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.855, [92mTest[0m: 10.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.85848
[1mStep[0m  [2/21], [94mLoss[0m : 10.67495
[1mStep[0m  [4/21], [94mLoss[0m : 10.76275
[1mStep[0m  [6/21], [94mLoss[0m : 10.86387
[1mStep[0m  [8/21], [94mLoss[0m : 10.76686
[1mStep[0m  [10/21], [94mLoss[0m : 10.92394
[1mStep[0m  [12/21], [94mLoss[0m : 10.90442
[1mStep[0m  [14/21], [94mLoss[0m : 10.89301
[1mStep[0m  [16/21], [94mLoss[0m : 10.98084
[1mStep[0m  [18/21], [94mLoss[0m : 11.07993
[1mStep[0m  [20/21], [94mLoss[0m : 10.88850

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.832, [92mTest[0m: 10.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.64978
[1mStep[0m  [2/21], [94mLoss[0m : 10.96657
[1mStep[0m  [4/21], [94mLoss[0m : 11.00862
[1mStep[0m  [6/21], [94mLoss[0m : 10.59585
[1mStep[0m  [8/21], [94mLoss[0m : 10.79265
[1mStep[0m  [10/21], [94mLoss[0m : 10.54349
[1mStep[0m  [12/21], [94mLoss[0m : 10.72475
[1mStep[0m  [14/21], [94mLoss[0m : 10.82049
[1mStep[0m  [16/21], [94mLoss[0m : 10.85636
[1mStep[0m  [18/21], [94mLoss[0m : 10.81533
[1mStep[0m  [20/21], [94mLoss[0m : 10.74302

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.812, [92mTest[0m: 10.892, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54132
[1mStep[0m  [2/21], [94mLoss[0m : 10.86402
[1mStep[0m  [4/21], [94mLoss[0m : 11.04194
[1mStep[0m  [6/21], [94mLoss[0m : 10.72883
[1mStep[0m  [8/21], [94mLoss[0m : 10.55737
[1mStep[0m  [10/21], [94mLoss[0m : 10.60499
[1mStep[0m  [12/21], [94mLoss[0m : 10.86200
[1mStep[0m  [14/21], [94mLoss[0m : 10.92399
[1mStep[0m  [16/21], [94mLoss[0m : 10.79087
[1mStep[0m  [18/21], [94mLoss[0m : 10.58334
[1mStep[0m  [20/21], [94mLoss[0m : 10.74765

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.802, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.80774
[1mStep[0m  [2/21], [94mLoss[0m : 10.80641
[1mStep[0m  [4/21], [94mLoss[0m : 10.65358
[1mStep[0m  [6/21], [94mLoss[0m : 10.68970
[1mStep[0m  [8/21], [94mLoss[0m : 10.45819
[1mStep[0m  [10/21], [94mLoss[0m : 10.95783
[1mStep[0m  [12/21], [94mLoss[0m : 10.92844
[1mStep[0m  [14/21], [94mLoss[0m : 10.76422
[1mStep[0m  [16/21], [94mLoss[0m : 10.79946
[1mStep[0m  [18/21], [94mLoss[0m : 10.94223
[1mStep[0m  [20/21], [94mLoss[0m : 10.49584

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.844, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39816
[1mStep[0m  [2/21], [94mLoss[0m : 10.97129
[1mStep[0m  [4/21], [94mLoss[0m : 10.75654
[1mStep[0m  [6/21], [94mLoss[0m : 10.46083
[1mStep[0m  [8/21], [94mLoss[0m : 10.76433
[1mStep[0m  [10/21], [94mLoss[0m : 10.62590
[1mStep[0m  [12/21], [94mLoss[0m : 10.55483
[1mStep[0m  [14/21], [94mLoss[0m : 10.82626
[1mStep[0m  [16/21], [94mLoss[0m : 10.92934
[1mStep[0m  [18/21], [94mLoss[0m : 10.75776
[1mStep[0m  [20/21], [94mLoss[0m : 10.62229

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.753, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93972
[1mStep[0m  [2/21], [94mLoss[0m : 10.95043
[1mStep[0m  [4/21], [94mLoss[0m : 10.47096
[1mStep[0m  [6/21], [94mLoss[0m : 10.76678
[1mStep[0m  [8/21], [94mLoss[0m : 10.87965
[1mStep[0m  [10/21], [94mLoss[0m : 10.84215
[1mStep[0m  [12/21], [94mLoss[0m : 10.60732
[1mStep[0m  [14/21], [94mLoss[0m : 10.73348
[1mStep[0m  [16/21], [94mLoss[0m : 10.82686
[1mStep[0m  [18/21], [94mLoss[0m : 10.57159
[1mStep[0m  [20/21], [94mLoss[0m : 10.52267

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.735, [92mTest[0m: 10.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71515
[1mStep[0m  [2/21], [94mLoss[0m : 10.69548
[1mStep[0m  [4/21], [94mLoss[0m : 10.58628
[1mStep[0m  [6/21], [94mLoss[0m : 10.59438
[1mStep[0m  [8/21], [94mLoss[0m : 11.01738
[1mStep[0m  [10/21], [94mLoss[0m : 10.57810
[1mStep[0m  [12/21], [94mLoss[0m : 10.58070
[1mStep[0m  [14/21], [94mLoss[0m : 10.83578
[1mStep[0m  [16/21], [94mLoss[0m : 10.86449
[1mStep[0m  [18/21], [94mLoss[0m : 10.92255
[1mStep[0m  [20/21], [94mLoss[0m : 10.93494

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.793, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70551
[1mStep[0m  [2/21], [94mLoss[0m : 10.73878
[1mStep[0m  [4/21], [94mLoss[0m : 10.34917
[1mStep[0m  [6/21], [94mLoss[0m : 10.64568
[1mStep[0m  [8/21], [94mLoss[0m : 10.86450
[1mStep[0m  [10/21], [94mLoss[0m : 10.55376
[1mStep[0m  [12/21], [94mLoss[0m : 10.92057
[1mStep[0m  [14/21], [94mLoss[0m : 10.79699
[1mStep[0m  [16/21], [94mLoss[0m : 10.55411
[1mStep[0m  [18/21], [94mLoss[0m : 10.83851
[1mStep[0m  [20/21], [94mLoss[0m : 10.42586

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.698, [92mTest[0m: 10.786, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55182
[1mStep[0m  [2/21], [94mLoss[0m : 10.46454
[1mStep[0m  [4/21], [94mLoss[0m : 10.81554
[1mStep[0m  [6/21], [94mLoss[0m : 10.74278
[1mStep[0m  [8/21], [94mLoss[0m : 10.64682
[1mStep[0m  [10/21], [94mLoss[0m : 10.68478
[1mStep[0m  [12/21], [94mLoss[0m : 10.68045
[1mStep[0m  [14/21], [94mLoss[0m : 10.71207
[1mStep[0m  [16/21], [94mLoss[0m : 10.41633
[1mStep[0m  [18/21], [94mLoss[0m : 10.85242
[1mStep[0m  [20/21], [94mLoss[0m : 10.57144

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.673, [92mTest[0m: 10.761, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82528
[1mStep[0m  [2/21], [94mLoss[0m : 10.59256
[1mStep[0m  [4/21], [94mLoss[0m : 10.74257
[1mStep[0m  [6/21], [94mLoss[0m : 11.03725
[1mStep[0m  [8/21], [94mLoss[0m : 10.66896
[1mStep[0m  [10/21], [94mLoss[0m : 10.49287
[1mStep[0m  [12/21], [94mLoss[0m : 10.85730
[1mStep[0m  [14/21], [94mLoss[0m : 10.83051
[1mStep[0m  [16/21], [94mLoss[0m : 10.43808
[1mStep[0m  [18/21], [94mLoss[0m : 10.43940
[1mStep[0m  [20/21], [94mLoss[0m : 10.57701

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.755, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73235
[1mStep[0m  [2/21], [94mLoss[0m : 10.30604
[1mStep[0m  [4/21], [94mLoss[0m : 10.43199
[1mStep[0m  [6/21], [94mLoss[0m : 10.51940
[1mStep[0m  [8/21], [94mLoss[0m : 10.59189
[1mStep[0m  [10/21], [94mLoss[0m : 10.46794
[1mStep[0m  [12/21], [94mLoss[0m : 10.56672
[1mStep[0m  [14/21], [94mLoss[0m : 10.68963
[1mStep[0m  [16/21], [94mLoss[0m : 10.74582
[1mStep[0m  [18/21], [94mLoss[0m : 10.40387
[1mStep[0m  [20/21], [94mLoss[0m : 10.78166

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.738, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48474
[1mStep[0m  [2/21], [94mLoss[0m : 10.47478
[1mStep[0m  [4/21], [94mLoss[0m : 10.60023
[1mStep[0m  [6/21], [94mLoss[0m : 10.73029
[1mStep[0m  [8/21], [94mLoss[0m : 10.62360
[1mStep[0m  [10/21], [94mLoss[0m : 10.55408
[1mStep[0m  [12/21], [94mLoss[0m : 10.61871
[1mStep[0m  [14/21], [94mLoss[0m : 10.72342
[1mStep[0m  [16/21], [94mLoss[0m : 10.41940
[1mStep[0m  [18/21], [94mLoss[0m : 10.62200
[1mStep[0m  [20/21], [94mLoss[0m : 10.61493

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50092
[1mStep[0m  [2/21], [94mLoss[0m : 10.91008
[1mStep[0m  [4/21], [94mLoss[0m : 10.77180
[1mStep[0m  [6/21], [94mLoss[0m : 10.53192
[1mStep[0m  [8/21], [94mLoss[0m : 10.82699
[1mStep[0m  [10/21], [94mLoss[0m : 10.52970
[1mStep[0m  [12/21], [94mLoss[0m : 10.54352
[1mStep[0m  [14/21], [94mLoss[0m : 10.65102
[1mStep[0m  [16/21], [94mLoss[0m : 10.64213
[1mStep[0m  [18/21], [94mLoss[0m : 10.65086
[1mStep[0m  [20/21], [94mLoss[0m : 10.38007

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.592, [92mTest[0m: 10.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.66166
[1mStep[0m  [2/21], [94mLoss[0m : 10.38841
[1mStep[0m  [4/21], [94mLoss[0m : 10.78922
[1mStep[0m  [6/21], [94mLoss[0m : 10.40712
[1mStep[0m  [8/21], [94mLoss[0m : 10.56256
[1mStep[0m  [10/21], [94mLoss[0m : 10.45007
[1mStep[0m  [12/21], [94mLoss[0m : 10.95123
[1mStep[0m  [14/21], [94mLoss[0m : 10.53412
[1mStep[0m  [16/21], [94mLoss[0m : 10.87176
[1mStep[0m  [18/21], [94mLoss[0m : 10.52324
[1mStep[0m  [20/21], [94mLoss[0m : 10.46303

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.581, [92mTest[0m: 10.695, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69370
[1mStep[0m  [2/21], [94mLoss[0m : 10.54767
[1mStep[0m  [4/21], [94mLoss[0m : 10.47130
[1mStep[0m  [6/21], [94mLoss[0m : 10.47352
[1mStep[0m  [8/21], [94mLoss[0m : 10.75421
[1mStep[0m  [10/21], [94mLoss[0m : 10.49882
[1mStep[0m  [12/21], [94mLoss[0m : 10.56250
[1mStep[0m  [14/21], [94mLoss[0m : 10.37965
[1mStep[0m  [16/21], [94mLoss[0m : 10.72769
[1mStep[0m  [18/21], [94mLoss[0m : 10.77907
[1mStep[0m  [20/21], [94mLoss[0m : 10.36188

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.554, [92mTest[0m: 10.660, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59480
[1mStep[0m  [2/21], [94mLoss[0m : 10.31695
[1mStep[0m  [4/21], [94mLoss[0m : 10.68211
[1mStep[0m  [6/21], [94mLoss[0m : 10.43965
[1mStep[0m  [8/21], [94mLoss[0m : 10.69794
[1mStep[0m  [10/21], [94mLoss[0m : 10.75627
[1mStep[0m  [12/21], [94mLoss[0m : 10.66943
[1mStep[0m  [14/21], [94mLoss[0m : 10.32557
[1mStep[0m  [16/21], [94mLoss[0m : 10.29261
[1mStep[0m  [18/21], [94mLoss[0m : 10.62471
[1mStep[0m  [20/21], [94mLoss[0m : 10.32849

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.533, [92mTest[0m: 10.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49579
[1mStep[0m  [2/21], [94mLoss[0m : 10.57960
[1mStep[0m  [4/21], [94mLoss[0m : 10.56224
[1mStep[0m  [6/21], [94mLoss[0m : 10.66199
[1mStep[0m  [8/21], [94mLoss[0m : 10.39522
[1mStep[0m  [10/21], [94mLoss[0m : 10.72431
[1mStep[0m  [12/21], [94mLoss[0m : 10.64129
[1mStep[0m  [14/21], [94mLoss[0m : 10.80486
[1mStep[0m  [16/21], [94mLoss[0m : 10.22214
[1mStep[0m  [18/21], [94mLoss[0m : 10.71819
[1mStep[0m  [20/21], [94mLoss[0m : 10.58242

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.513, [92mTest[0m: 10.656, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48923
[1mStep[0m  [2/21], [94mLoss[0m : 10.59478
[1mStep[0m  [4/21], [94mLoss[0m : 10.59631
[1mStep[0m  [6/21], [94mLoss[0m : 10.39208
[1mStep[0m  [8/21], [94mLoss[0m : 10.50369
[1mStep[0m  [10/21], [94mLoss[0m : 10.40506
[1mStep[0m  [12/21], [94mLoss[0m : 10.55291
[1mStep[0m  [14/21], [94mLoss[0m : 10.35687
[1mStep[0m  [16/21], [94mLoss[0m : 10.33974
[1mStep[0m  [18/21], [94mLoss[0m : 10.43942
[1mStep[0m  [20/21], [94mLoss[0m : 10.32582

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.499, [92mTest[0m: 10.652, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62692
[1mStep[0m  [2/21], [94mLoss[0m : 10.45264
[1mStep[0m  [4/21], [94mLoss[0m : 10.18066
[1mStep[0m  [6/21], [94mLoss[0m : 10.39669
[1mStep[0m  [8/21], [94mLoss[0m : 10.54427
[1mStep[0m  [10/21], [94mLoss[0m : 10.51108
[1mStep[0m  [12/21], [94mLoss[0m : 10.39409
[1mStep[0m  [14/21], [94mLoss[0m : 10.37351
[1mStep[0m  [16/21], [94mLoss[0m : 10.59687
[1mStep[0m  [18/21], [94mLoss[0m : 10.69572
[1mStep[0m  [20/21], [94mLoss[0m : 10.68839

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.469, [92mTest[0m: 10.621, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51322
[1mStep[0m  [2/21], [94mLoss[0m : 10.54573
[1mStep[0m  [4/21], [94mLoss[0m : 10.57826
[1mStep[0m  [6/21], [94mLoss[0m : 10.27151
[1mStep[0m  [8/21], [94mLoss[0m : 10.64860
[1mStep[0m  [10/21], [94mLoss[0m : 10.34833
[1mStep[0m  [12/21], [94mLoss[0m : 11.06629
[1mStep[0m  [14/21], [94mLoss[0m : 10.46014
[1mStep[0m  [16/21], [94mLoss[0m : 10.38088
[1mStep[0m  [18/21], [94mLoss[0m : 10.31063
[1mStep[0m  [20/21], [94mLoss[0m : 10.49669

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.444, [92mTest[0m: 10.590, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38783
[1mStep[0m  [2/21], [94mLoss[0m : 10.28684
[1mStep[0m  [4/21], [94mLoss[0m : 10.27497
[1mStep[0m  [6/21], [94mLoss[0m : 10.59175
[1mStep[0m  [8/21], [94mLoss[0m : 10.54082
[1mStep[0m  [10/21], [94mLoss[0m : 10.63913
[1mStep[0m  [12/21], [94mLoss[0m : 10.38257
[1mStep[0m  [14/21], [94mLoss[0m : 10.38475
[1mStep[0m  [16/21], [94mLoss[0m : 10.59344
[1mStep[0m  [18/21], [94mLoss[0m : 10.57801
[1mStep[0m  [20/21], [94mLoss[0m : 10.48452

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.596, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42562
[1mStep[0m  [2/21], [94mLoss[0m : 10.31047
[1mStep[0m  [4/21], [94mLoss[0m : 10.84826
[1mStep[0m  [6/21], [94mLoss[0m : 10.44541
[1mStep[0m  [8/21], [94mLoss[0m : 10.20294
[1mStep[0m  [10/21], [94mLoss[0m : 10.20078
[1mStep[0m  [12/21], [94mLoss[0m : 10.31229
[1mStep[0m  [14/21], [94mLoss[0m : 10.49130
[1mStep[0m  [16/21], [94mLoss[0m : 10.33540
[1mStep[0m  [18/21], [94mLoss[0m : 10.63206
[1mStep[0m  [20/21], [94mLoss[0m : 10.77738

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.590, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49462
[1mStep[0m  [2/21], [94mLoss[0m : 10.42900
[1mStep[0m  [4/21], [94mLoss[0m : 10.40412
[1mStep[0m  [6/21], [94mLoss[0m : 10.36729
[1mStep[0m  [8/21], [94mLoss[0m : 10.36216
[1mStep[0m  [10/21], [94mLoss[0m : 10.39376
[1mStep[0m  [12/21], [94mLoss[0m : 10.37601
[1mStep[0m  [14/21], [94mLoss[0m : 10.34785
[1mStep[0m  [16/21], [94mLoss[0m : 10.36774
[1mStep[0m  [18/21], [94mLoss[0m : 10.06550
[1mStep[0m  [20/21], [94mLoss[0m : 10.48301

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.399, [92mTest[0m: 10.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38498
[1mStep[0m  [2/21], [94mLoss[0m : 10.41707
[1mStep[0m  [4/21], [94mLoss[0m : 10.67795
[1mStep[0m  [6/21], [94mLoss[0m : 10.42849
[1mStep[0m  [8/21], [94mLoss[0m : 10.71074
[1mStep[0m  [10/21], [94mLoss[0m : 10.41712
[1mStep[0m  [12/21], [94mLoss[0m : 10.31316
[1mStep[0m  [14/21], [94mLoss[0m : 10.52948
[1mStep[0m  [16/21], [94mLoss[0m : 10.19959
[1mStep[0m  [18/21], [94mLoss[0m : 10.26410
[1mStep[0m  [20/21], [94mLoss[0m : 10.27156

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42859
[1mStep[0m  [2/21], [94mLoss[0m : 10.41304
[1mStep[0m  [4/21], [94mLoss[0m : 9.90639
[1mStep[0m  [6/21], [94mLoss[0m : 10.30190
[1mStep[0m  [8/21], [94mLoss[0m : 10.25812
[1mStep[0m  [10/21], [94mLoss[0m : 10.35748
[1mStep[0m  [12/21], [94mLoss[0m : 10.17997
[1mStep[0m  [14/21], [94mLoss[0m : 10.65349
[1mStep[0m  [16/21], [94mLoss[0m : 10.61293
[1mStep[0m  [18/21], [94mLoss[0m : 10.20587
[1mStep[0m  [20/21], [94mLoss[0m : 9.99867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.365, [92mTest[0m: 10.539, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.26189
[1mStep[0m  [2/21], [94mLoss[0m : 10.50271
[1mStep[0m  [4/21], [94mLoss[0m : 10.39446
[1mStep[0m  [6/21], [94mLoss[0m : 10.31089
[1mStep[0m  [8/21], [94mLoss[0m : 10.60842
[1mStep[0m  [10/21], [94mLoss[0m : 10.60892
[1mStep[0m  [12/21], [94mLoss[0m : 10.62379
[1mStep[0m  [14/21], [94mLoss[0m : 10.41314
[1mStep[0m  [16/21], [94mLoss[0m : 10.09791
[1mStep[0m  [18/21], [94mLoss[0m : 10.45565
[1mStep[0m  [20/21], [94mLoss[0m : 10.18859

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.349, [92mTest[0m: 10.539, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.53070
[1mStep[0m  [2/21], [94mLoss[0m : 10.04481
[1mStep[0m  [4/21], [94mLoss[0m : 10.36427
[1mStep[0m  [6/21], [94mLoss[0m : 10.50115
[1mStep[0m  [8/21], [94mLoss[0m : 10.35854
[1mStep[0m  [10/21], [94mLoss[0m : 10.06889
[1mStep[0m  [12/21], [94mLoss[0m : 10.14069
[1mStep[0m  [14/21], [94mLoss[0m : 10.37562
[1mStep[0m  [16/21], [94mLoss[0m : 10.41853
[1mStep[0m  [18/21], [94mLoss[0m : 10.31366
[1mStep[0m  [20/21], [94mLoss[0m : 10.51269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.335, [92mTest[0m: 10.501, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56322
[1mStep[0m  [2/21], [94mLoss[0m : 10.29260
[1mStep[0m  [4/21], [94mLoss[0m : 10.35502
[1mStep[0m  [6/21], [94mLoss[0m : 10.06586
[1mStep[0m  [8/21], [94mLoss[0m : 10.33996
[1mStep[0m  [10/21], [94mLoss[0m : 10.20729
[1mStep[0m  [12/21], [94mLoss[0m : 10.37592
[1mStep[0m  [14/21], [94mLoss[0m : 10.05480
[1mStep[0m  [16/21], [94mLoss[0m : 10.30108
[1mStep[0m  [18/21], [94mLoss[0m : 10.51686
[1mStep[0m  [20/21], [94mLoss[0m : 10.39019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.311, [92mTest[0m: 10.502, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33798
[1mStep[0m  [2/21], [94mLoss[0m : 10.15321
[1mStep[0m  [4/21], [94mLoss[0m : 10.33039
[1mStep[0m  [6/21], [94mLoss[0m : 10.17432
[1mStep[0m  [8/21], [94mLoss[0m : 10.18946
[1mStep[0m  [10/21], [94mLoss[0m : 10.43377
[1mStep[0m  [12/21], [94mLoss[0m : 10.16735
[1mStep[0m  [14/21], [94mLoss[0m : 10.36543
[1mStep[0m  [16/21], [94mLoss[0m : 10.40635
[1mStep[0m  [18/21], [94mLoss[0m : 10.20692
[1mStep[0m  [20/21], [94mLoss[0m : 10.21187

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.291, [92mTest[0m: 10.497, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.484
====================================

Phase 1 - Evaluation MAE:  10.484128679547991
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.51862
[1mStep[0m  [2/21], [94mLoss[0m : 10.35561
[1mStep[0m  [4/21], [94mLoss[0m : 10.50525
[1mStep[0m  [6/21], [94mLoss[0m : 10.23973
[1mStep[0m  [8/21], [94mLoss[0m : 10.23597
[1mStep[0m  [10/21], [94mLoss[0m : 10.10783
[1mStep[0m  [12/21], [94mLoss[0m : 10.41771
[1mStep[0m  [14/21], [94mLoss[0m : 10.21881
[1mStep[0m  [16/21], [94mLoss[0m : 10.14207
[1mStep[0m  [18/21], [94mLoss[0m : 9.99031
[1mStep[0m  [20/21], [94mLoss[0m : 9.89666

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.274, [92mTest[0m: 10.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.34413
[1mStep[0m  [2/21], [94mLoss[0m : 10.31664
[1mStep[0m  [4/21], [94mLoss[0m : 10.27763
[1mStep[0m  [6/21], [94mLoss[0m : 10.28880
[1mStep[0m  [8/21], [94mLoss[0m : 10.37864
[1mStep[0m  [10/21], [94mLoss[0m : 10.31184
[1mStep[0m  [12/21], [94mLoss[0m : 10.33079
[1mStep[0m  [14/21], [94mLoss[0m : 10.31239
[1mStep[0m  [16/21], [94mLoss[0m : 10.23516
[1mStep[0m  [18/21], [94mLoss[0m : 10.15929
[1mStep[0m  [20/21], [94mLoss[0m : 10.13230

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.262, [92mTest[0m: 10.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69764
[1mStep[0m  [2/21], [94mLoss[0m : 10.00450
[1mStep[0m  [4/21], [94mLoss[0m : 9.96648
[1mStep[0m  [6/21], [94mLoss[0m : 10.18555
[1mStep[0m  [8/21], [94mLoss[0m : 10.28917
[1mStep[0m  [10/21], [94mLoss[0m : 10.14511
[1mStep[0m  [12/21], [94mLoss[0m : 10.15244
[1mStep[0m  [14/21], [94mLoss[0m : 10.35598
[1mStep[0m  [16/21], [94mLoss[0m : 10.10102
[1mStep[0m  [18/21], [94mLoss[0m : 10.45381
[1mStep[0m  [20/21], [94mLoss[0m : 9.93094

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.231, [92mTest[0m: 10.436, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.03257
[1mStep[0m  [2/21], [94mLoss[0m : 10.32124
[1mStep[0m  [4/21], [94mLoss[0m : 10.08149
[1mStep[0m  [6/21], [94mLoss[0m : 10.45226
[1mStep[0m  [8/21], [94mLoss[0m : 10.21230
[1mStep[0m  [10/21], [94mLoss[0m : 10.18862
[1mStep[0m  [12/21], [94mLoss[0m : 10.22368
[1mStep[0m  [14/21], [94mLoss[0m : 10.08957
[1mStep[0m  [16/21], [94mLoss[0m : 10.05465
[1mStep[0m  [18/21], [94mLoss[0m : 10.34771
[1mStep[0m  [20/21], [94mLoss[0m : 10.23351

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.212, [92mTest[0m: 10.436, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.20733
[1mStep[0m  [2/21], [94mLoss[0m : 10.21807
[1mStep[0m  [4/21], [94mLoss[0m : 9.95527
[1mStep[0m  [6/21], [94mLoss[0m : 10.18001
[1mStep[0m  [8/21], [94mLoss[0m : 10.51976
[1mStep[0m  [10/21], [94mLoss[0m : 10.02597
[1mStep[0m  [12/21], [94mLoss[0m : 10.24502
[1mStep[0m  [14/21], [94mLoss[0m : 9.96314
[1mStep[0m  [16/21], [94mLoss[0m : 10.07714
[1mStep[0m  [18/21], [94mLoss[0m : 10.17740
[1mStep[0m  [20/21], [94mLoss[0m : 10.26953

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.194, [92mTest[0m: 10.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.21840
[1mStep[0m  [2/21], [94mLoss[0m : 10.40943
[1mStep[0m  [4/21], [94mLoss[0m : 10.14690
[1mStep[0m  [6/21], [94mLoss[0m : 10.10087
[1mStep[0m  [8/21], [94mLoss[0m : 10.19443
[1mStep[0m  [10/21], [94mLoss[0m : 10.09405
[1mStep[0m  [12/21], [94mLoss[0m : 9.75712
[1mStep[0m  [14/21], [94mLoss[0m : 10.17078
[1mStep[0m  [16/21], [94mLoss[0m : 10.09254
[1mStep[0m  [18/21], [94mLoss[0m : 10.23010
[1mStep[0m  [20/21], [94mLoss[0m : 9.98110

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.173, [92mTest[0m: 10.398, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.18776
[1mStep[0m  [2/21], [94mLoss[0m : 10.03475
[1mStep[0m  [4/21], [94mLoss[0m : 10.29275
[1mStep[0m  [6/21], [94mLoss[0m : 10.05281
[1mStep[0m  [8/21], [94mLoss[0m : 10.45752
[1mStep[0m  [10/21], [94mLoss[0m : 10.33295
[1mStep[0m  [12/21], [94mLoss[0m : 10.09518
[1mStep[0m  [14/21], [94mLoss[0m : 10.05115
[1mStep[0m  [16/21], [94mLoss[0m : 9.95526
[1mStep[0m  [18/21], [94mLoss[0m : 10.06658
[1mStep[0m  [20/21], [94mLoss[0m : 9.93629

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.151, [92mTest[0m: 10.387, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.16416
[1mStep[0m  [2/21], [94mLoss[0m : 10.00666
[1mStep[0m  [4/21], [94mLoss[0m : 10.23780
[1mStep[0m  [6/21], [94mLoss[0m : 10.06770
[1mStep[0m  [8/21], [94mLoss[0m : 9.87214
[1mStep[0m  [10/21], [94mLoss[0m : 10.29024
[1mStep[0m  [12/21], [94mLoss[0m : 10.22297
[1mStep[0m  [14/21], [94mLoss[0m : 10.10420
[1mStep[0m  [16/21], [94mLoss[0m : 10.15871
[1mStep[0m  [18/21], [94mLoss[0m : 10.16887
[1mStep[0m  [20/21], [94mLoss[0m : 10.19495

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.131, [92mTest[0m: 10.356, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.11240
[1mStep[0m  [2/21], [94mLoss[0m : 10.15940
[1mStep[0m  [4/21], [94mLoss[0m : 10.19822
[1mStep[0m  [6/21], [94mLoss[0m : 9.82258
[1mStep[0m  [8/21], [94mLoss[0m : 10.32815
[1mStep[0m  [10/21], [94mLoss[0m : 10.19612
[1mStep[0m  [12/21], [94mLoss[0m : 9.96198
[1mStep[0m  [14/21], [94mLoss[0m : 10.24499
[1mStep[0m  [16/21], [94mLoss[0m : 9.99381
[1mStep[0m  [18/21], [94mLoss[0m : 10.35766
[1mStep[0m  [20/21], [94mLoss[0m : 10.20655

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.114, [92mTest[0m: 10.348, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12078
[1mStep[0m  [2/21], [94mLoss[0m : 10.40697
[1mStep[0m  [4/21], [94mLoss[0m : 10.35920
[1mStep[0m  [6/21], [94mLoss[0m : 9.89353
[1mStep[0m  [8/21], [94mLoss[0m : 10.13623
[1mStep[0m  [10/21], [94mLoss[0m : 10.17216
[1mStep[0m  [12/21], [94mLoss[0m : 10.28064
[1mStep[0m  [14/21], [94mLoss[0m : 10.34121
[1mStep[0m  [16/21], [94mLoss[0m : 10.04530
[1mStep[0m  [18/21], [94mLoss[0m : 9.86022
[1mStep[0m  [20/21], [94mLoss[0m : 10.26941

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.095, [92mTest[0m: 10.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.02774
[1mStep[0m  [2/21], [94mLoss[0m : 9.99087
[1mStep[0m  [4/21], [94mLoss[0m : 10.17303
[1mStep[0m  [6/21], [94mLoss[0m : 10.14613
[1mStep[0m  [8/21], [94mLoss[0m : 9.89874
[1mStep[0m  [10/21], [94mLoss[0m : 10.05516
[1mStep[0m  [12/21], [94mLoss[0m : 10.21515
[1mStep[0m  [14/21], [94mLoss[0m : 10.49723
[1mStep[0m  [16/21], [94mLoss[0m : 9.92536
[1mStep[0m  [18/21], [94mLoss[0m : 10.04713
[1mStep[0m  [20/21], [94mLoss[0m : 10.09179

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.071, [92mTest[0m: 10.322, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12411
[1mStep[0m  [2/21], [94mLoss[0m : 9.96479
[1mStep[0m  [4/21], [94mLoss[0m : 10.13324
[1mStep[0m  [6/21], [94mLoss[0m : 9.93301
[1mStep[0m  [8/21], [94mLoss[0m : 10.21125
[1mStep[0m  [10/21], [94mLoss[0m : 10.30569
[1mStep[0m  [12/21], [94mLoss[0m : 9.97315
[1mStep[0m  [14/21], [94mLoss[0m : 9.94816
[1mStep[0m  [16/21], [94mLoss[0m : 10.26649
[1mStep[0m  [18/21], [94mLoss[0m : 10.04194
[1mStep[0m  [20/21], [94mLoss[0m : 10.28251

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.050, [92mTest[0m: 10.309, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12804
[1mStep[0m  [2/21], [94mLoss[0m : 9.89732
[1mStep[0m  [4/21], [94mLoss[0m : 9.73784
[1mStep[0m  [6/21], [94mLoss[0m : 10.37294
[1mStep[0m  [8/21], [94mLoss[0m : 9.67552
[1mStep[0m  [10/21], [94mLoss[0m : 9.85082
[1mStep[0m  [12/21], [94mLoss[0m : 10.04905
[1mStep[0m  [14/21], [94mLoss[0m : 10.15752
[1mStep[0m  [16/21], [94mLoss[0m : 10.22475
[1mStep[0m  [18/21], [94mLoss[0m : 10.06612
[1mStep[0m  [20/21], [94mLoss[0m : 10.15559

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.035, [92mTest[0m: 10.284, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.03966
[1mStep[0m  [2/21], [94mLoss[0m : 9.84909
[1mStep[0m  [4/21], [94mLoss[0m : 9.91828
[1mStep[0m  [6/21], [94mLoss[0m : 10.11730
[1mStep[0m  [8/21], [94mLoss[0m : 10.01319
[1mStep[0m  [10/21], [94mLoss[0m : 10.10531
[1mStep[0m  [12/21], [94mLoss[0m : 10.07945
[1mStep[0m  [14/21], [94mLoss[0m : 10.01849
[1mStep[0m  [16/21], [94mLoss[0m : 9.78046
[1mStep[0m  [18/21], [94mLoss[0m : 10.12303
[1mStep[0m  [20/21], [94mLoss[0m : 9.95276

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.011, [92mTest[0m: 10.264, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.99259
[1mStep[0m  [2/21], [94mLoss[0m : 10.20959
[1mStep[0m  [4/21], [94mLoss[0m : 9.87197
[1mStep[0m  [6/21], [94mLoss[0m : 10.09596
[1mStep[0m  [8/21], [94mLoss[0m : 10.03543
[1mStep[0m  [10/21], [94mLoss[0m : 10.15690
[1mStep[0m  [12/21], [94mLoss[0m : 10.16527
[1mStep[0m  [14/21], [94mLoss[0m : 9.81290
[1mStep[0m  [16/21], [94mLoss[0m : 9.88510
[1mStep[0m  [18/21], [94mLoss[0m : 9.92446
[1mStep[0m  [20/21], [94mLoss[0m : 9.90981

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.995, [92mTest[0m: 10.249, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.98458
[1mStep[0m  [2/21], [94mLoss[0m : 9.74164
[1mStep[0m  [4/21], [94mLoss[0m : 9.79714
[1mStep[0m  [6/21], [94mLoss[0m : 9.99935
[1mStep[0m  [8/21], [94mLoss[0m : 9.89127
[1mStep[0m  [10/21], [94mLoss[0m : 10.03776
[1mStep[0m  [12/21], [94mLoss[0m : 10.17017
[1mStep[0m  [14/21], [94mLoss[0m : 9.83726
[1mStep[0m  [16/21], [94mLoss[0m : 10.32340
[1mStep[0m  [18/21], [94mLoss[0m : 10.22105
[1mStep[0m  [20/21], [94mLoss[0m : 10.10445

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.968, [92mTest[0m: 10.252, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.92882
[1mStep[0m  [2/21], [94mLoss[0m : 9.76065
[1mStep[0m  [4/21], [94mLoss[0m : 10.09717
[1mStep[0m  [6/21], [94mLoss[0m : 9.69195
[1mStep[0m  [8/21], [94mLoss[0m : 10.05680
[1mStep[0m  [10/21], [94mLoss[0m : 10.03452
[1mStep[0m  [12/21], [94mLoss[0m : 9.93849
[1mStep[0m  [14/21], [94mLoss[0m : 9.67106
[1mStep[0m  [16/21], [94mLoss[0m : 10.06363
[1mStep[0m  [18/21], [94mLoss[0m : 10.08353
[1mStep[0m  [20/21], [94mLoss[0m : 10.12657

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.950, [92mTest[0m: 10.223, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.01344
[1mStep[0m  [2/21], [94mLoss[0m : 10.07987
[1mStep[0m  [4/21], [94mLoss[0m : 9.90352
[1mStep[0m  [6/21], [94mLoss[0m : 9.78833
[1mStep[0m  [8/21], [94mLoss[0m : 9.74317
[1mStep[0m  [10/21], [94mLoss[0m : 9.99133
[1mStep[0m  [12/21], [94mLoss[0m : 10.01841
[1mStep[0m  [14/21], [94mLoss[0m : 9.87989
[1mStep[0m  [16/21], [94mLoss[0m : 10.02960
[1mStep[0m  [18/21], [94mLoss[0m : 9.85040
[1mStep[0m  [20/21], [94mLoss[0m : 9.57262

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.928, [92mTest[0m: 10.222, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.96532
[1mStep[0m  [2/21], [94mLoss[0m : 10.06691
[1mStep[0m  [4/21], [94mLoss[0m : 9.98177
[1mStep[0m  [6/21], [94mLoss[0m : 9.85611
[1mStep[0m  [8/21], [94mLoss[0m : 10.02791
[1mStep[0m  [10/21], [94mLoss[0m : 9.95880
[1mStep[0m  [12/21], [94mLoss[0m : 9.72265
[1mStep[0m  [14/21], [94mLoss[0m : 9.90154
[1mStep[0m  [16/21], [94mLoss[0m : 9.82573
[1mStep[0m  [18/21], [94mLoss[0m : 9.83211
[1mStep[0m  [20/21], [94mLoss[0m : 10.04576

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.915, [92mTest[0m: 10.190, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.91779
[1mStep[0m  [2/21], [94mLoss[0m : 10.16265
[1mStep[0m  [4/21], [94mLoss[0m : 9.88124
[1mStep[0m  [6/21], [94mLoss[0m : 9.80579
[1mStep[0m  [8/21], [94mLoss[0m : 9.96708
[1mStep[0m  [10/21], [94mLoss[0m : 9.91689
[1mStep[0m  [12/21], [94mLoss[0m : 10.15569
[1mStep[0m  [14/21], [94mLoss[0m : 9.67599
[1mStep[0m  [16/21], [94mLoss[0m : 9.72613
[1mStep[0m  [18/21], [94mLoss[0m : 9.99832
[1mStep[0m  [20/21], [94mLoss[0m : 9.73817

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.888, [92mTest[0m: 10.185, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.56433
[1mStep[0m  [2/21], [94mLoss[0m : 10.18258
[1mStep[0m  [4/21], [94mLoss[0m : 10.06995
[1mStep[0m  [6/21], [94mLoss[0m : 9.78465
[1mStep[0m  [8/21], [94mLoss[0m : 9.66938
[1mStep[0m  [10/21], [94mLoss[0m : 9.90461
[1mStep[0m  [12/21], [94mLoss[0m : 9.85769
[1mStep[0m  [14/21], [94mLoss[0m : 9.90511
[1mStep[0m  [16/21], [94mLoss[0m : 9.87954
[1mStep[0m  [18/21], [94mLoss[0m : 9.62869
[1mStep[0m  [20/21], [94mLoss[0m : 9.52113

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.870, [92mTest[0m: 10.171, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.92937
[1mStep[0m  [2/21], [94mLoss[0m : 9.76524
[1mStep[0m  [4/21], [94mLoss[0m : 9.53053
[1mStep[0m  [6/21], [94mLoss[0m : 9.57029
[1mStep[0m  [8/21], [94mLoss[0m : 9.94769
[1mStep[0m  [10/21], [94mLoss[0m : 10.06766
[1mStep[0m  [12/21], [94mLoss[0m : 9.84838
[1mStep[0m  [14/21], [94mLoss[0m : 9.71771
[1mStep[0m  [16/21], [94mLoss[0m : 9.68285
[1mStep[0m  [18/21], [94mLoss[0m : 9.90341
[1mStep[0m  [20/21], [94mLoss[0m : 10.08840

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.847, [92mTest[0m: 10.151, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.76495
[1mStep[0m  [2/21], [94mLoss[0m : 9.98415
[1mStep[0m  [4/21], [94mLoss[0m : 9.81076
[1mStep[0m  [6/21], [94mLoss[0m : 10.05202
[1mStep[0m  [8/21], [94mLoss[0m : 9.85138
[1mStep[0m  [10/21], [94mLoss[0m : 9.85977
[1mStep[0m  [12/21], [94mLoss[0m : 9.73576
[1mStep[0m  [14/21], [94mLoss[0m : 10.11618
[1mStep[0m  [16/21], [94mLoss[0m : 9.94437
[1mStep[0m  [18/21], [94mLoss[0m : 9.81057
[1mStep[0m  [20/21], [94mLoss[0m : 9.65360

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.834, [92mTest[0m: 10.129, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12944
[1mStep[0m  [2/21], [94mLoss[0m : 9.91129
[1mStep[0m  [4/21], [94mLoss[0m : 9.75453
[1mStep[0m  [6/21], [94mLoss[0m : 9.84231
[1mStep[0m  [8/21], [94mLoss[0m : 9.60901
[1mStep[0m  [10/21], [94mLoss[0m : 9.86445
[1mStep[0m  [12/21], [94mLoss[0m : 9.70645
[1mStep[0m  [14/21], [94mLoss[0m : 9.84614
[1mStep[0m  [16/21], [94mLoss[0m : 9.61516
[1mStep[0m  [18/21], [94mLoss[0m : 9.94663
[1mStep[0m  [20/21], [94mLoss[0m : 9.81259

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.815, [92mTest[0m: 10.125, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.36321
[1mStep[0m  [2/21], [94mLoss[0m : 9.75195
[1mStep[0m  [4/21], [94mLoss[0m : 9.73168
[1mStep[0m  [6/21], [94mLoss[0m : 9.84631
[1mStep[0m  [8/21], [94mLoss[0m : 9.83886
[1mStep[0m  [10/21], [94mLoss[0m : 9.89054
[1mStep[0m  [12/21], [94mLoss[0m : 9.63541
[1mStep[0m  [14/21], [94mLoss[0m : 9.83723
[1mStep[0m  [16/21], [94mLoss[0m : 9.88861
[1mStep[0m  [18/21], [94mLoss[0m : 9.99054
[1mStep[0m  [20/21], [94mLoss[0m : 9.85331

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.789, [92mTest[0m: 10.101, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.57275
[1mStep[0m  [2/21], [94mLoss[0m : 9.89156
[1mStep[0m  [4/21], [94mLoss[0m : 9.83262
[1mStep[0m  [6/21], [94mLoss[0m : 9.76880
[1mStep[0m  [8/21], [94mLoss[0m : 9.76790
[1mStep[0m  [10/21], [94mLoss[0m : 9.87058
[1mStep[0m  [12/21], [94mLoss[0m : 9.89684
[1mStep[0m  [14/21], [94mLoss[0m : 9.87017
[1mStep[0m  [16/21], [94mLoss[0m : 9.86117
[1mStep[0m  [18/21], [94mLoss[0m : 9.75804
[1mStep[0m  [20/21], [94mLoss[0m : 9.76042

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.783, [92mTest[0m: 10.090, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.86124
[1mStep[0m  [2/21], [94mLoss[0m : 9.76402
[1mStep[0m  [4/21], [94mLoss[0m : 9.87136
[1mStep[0m  [6/21], [94mLoss[0m : 9.69795
[1mStep[0m  [8/21], [94mLoss[0m : 9.57903
[1mStep[0m  [10/21], [94mLoss[0m : 9.75540
[1mStep[0m  [12/21], [94mLoss[0m : 9.76082
[1mStep[0m  [14/21], [94mLoss[0m : 9.90822
[1mStep[0m  [16/21], [94mLoss[0m : 10.06294
[1mStep[0m  [18/21], [94mLoss[0m : 9.26036
[1mStep[0m  [20/21], [94mLoss[0m : 9.94359

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.756, [92mTest[0m: 10.074, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.64848
[1mStep[0m  [2/21], [94mLoss[0m : 9.78836
[1mStep[0m  [4/21], [94mLoss[0m : 9.76572
[1mStep[0m  [6/21], [94mLoss[0m : 9.91491
[1mStep[0m  [8/21], [94mLoss[0m : 9.71219
[1mStep[0m  [10/21], [94mLoss[0m : 9.64843
[1mStep[0m  [12/21], [94mLoss[0m : 9.56553
[1mStep[0m  [14/21], [94mLoss[0m : 9.65429
[1mStep[0m  [16/21], [94mLoss[0m : 9.62719
[1mStep[0m  [18/21], [94mLoss[0m : 9.96106
[1mStep[0m  [20/21], [94mLoss[0m : 9.83337

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.739, [92mTest[0m: 10.062, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.72950
[1mStep[0m  [2/21], [94mLoss[0m : 9.57134
[1mStep[0m  [4/21], [94mLoss[0m : 9.85593
[1mStep[0m  [6/21], [94mLoss[0m : 10.09433
[1mStep[0m  [8/21], [94mLoss[0m : 9.80365
[1mStep[0m  [10/21], [94mLoss[0m : 9.64398
[1mStep[0m  [12/21], [94mLoss[0m : 9.82166
[1mStep[0m  [14/21], [94mLoss[0m : 9.71068
[1mStep[0m  [16/21], [94mLoss[0m : 9.61265
[1mStep[0m  [18/21], [94mLoss[0m : 9.53710
[1mStep[0m  [20/21], [94mLoss[0m : 9.61491

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.730, [92mTest[0m: 10.050, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.64469
[1mStep[0m  [2/21], [94mLoss[0m : 9.55217
[1mStep[0m  [4/21], [94mLoss[0m : 9.58945
[1mStep[0m  [6/21], [94mLoss[0m : 9.78346
[1mStep[0m  [8/21], [94mLoss[0m : 9.66952
[1mStep[0m  [10/21], [94mLoss[0m : 9.74938
[1mStep[0m  [12/21], [94mLoss[0m : 9.47065
[1mStep[0m  [14/21], [94mLoss[0m : 9.62626
[1mStep[0m  [16/21], [94mLoss[0m : 9.89289
[1mStep[0m  [18/21], [94mLoss[0m : 9.71708
[1mStep[0m  [20/21], [94mLoss[0m : 9.81400

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.704, [92mTest[0m: 10.033, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.028
====================================

Phase 2 - Evaluation MAE:  10.028155735560826
MAE score P1      10.484129
MAE score P2      10.028156
loss               9.703972
learning_rate        0.0001
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.09812
[1mStep[0m  [4/42], [94mLoss[0m : 10.83007
[1mStep[0m  [8/42], [94mLoss[0m : 10.59650
[1mStep[0m  [12/42], [94mLoss[0m : 10.63613
[1mStep[0m  [16/42], [94mLoss[0m : 10.91133
[1mStep[0m  [20/42], [94mLoss[0m : 10.66819
[1mStep[0m  [24/42], [94mLoss[0m : 11.11251
[1mStep[0m  [28/42], [94mLoss[0m : 11.00615
[1mStep[0m  [32/42], [94mLoss[0m : 10.81480
[1mStep[0m  [36/42], [94mLoss[0m : 10.98125
[1mStep[0m  [40/42], [94mLoss[0m : 11.00685

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.797, [92mTest[0m: 10.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.96878
[1mStep[0m  [4/42], [94mLoss[0m : 10.64500
[1mStep[0m  [8/42], [94mLoss[0m : 10.86745
[1mStep[0m  [12/42], [94mLoss[0m : 10.15519
[1mStep[0m  [16/42], [94mLoss[0m : 11.15183
[1mStep[0m  [20/42], [94mLoss[0m : 11.09213
[1mStep[0m  [24/42], [94mLoss[0m : 10.81284
[1mStep[0m  [28/42], [94mLoss[0m : 10.90266
[1mStep[0m  [32/42], [94mLoss[0m : 11.32300
[1mStep[0m  [36/42], [94mLoss[0m : 10.30333
[1mStep[0m  [40/42], [94mLoss[0m : 10.67817

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.782, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68336
[1mStep[0m  [4/42], [94mLoss[0m : 10.91947
[1mStep[0m  [8/42], [94mLoss[0m : 10.59996
[1mStep[0m  [12/42], [94mLoss[0m : 10.83839
[1mStep[0m  [16/42], [94mLoss[0m : 10.88067
[1mStep[0m  [20/42], [94mLoss[0m : 10.58953
[1mStep[0m  [24/42], [94mLoss[0m : 11.19920
[1mStep[0m  [28/42], [94mLoss[0m : 10.71207
[1mStep[0m  [32/42], [94mLoss[0m : 10.50085
[1mStep[0m  [36/42], [94mLoss[0m : 10.80603
[1mStep[0m  [40/42], [94mLoss[0m : 11.11483

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.783, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87232
[1mStep[0m  [4/42], [94mLoss[0m : 10.62156
[1mStep[0m  [8/42], [94mLoss[0m : 11.24263
[1mStep[0m  [12/42], [94mLoss[0m : 10.83134
[1mStep[0m  [16/42], [94mLoss[0m : 11.00046
[1mStep[0m  [20/42], [94mLoss[0m : 10.55009
[1mStep[0m  [24/42], [94mLoss[0m : 10.53691
[1mStep[0m  [28/42], [94mLoss[0m : 10.73249
[1mStep[0m  [32/42], [94mLoss[0m : 10.53654
[1mStep[0m  [36/42], [94mLoss[0m : 10.41748
[1mStep[0m  [40/42], [94mLoss[0m : 10.97970

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.769, [92mTest[0m: 10.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.26293
[1mStep[0m  [4/42], [94mLoss[0m : 10.62080
[1mStep[0m  [8/42], [94mLoss[0m : 10.70439
[1mStep[0m  [12/42], [94mLoss[0m : 10.71064
[1mStep[0m  [16/42], [94mLoss[0m : 10.73553
[1mStep[0m  [20/42], [94mLoss[0m : 10.47544
[1mStep[0m  [24/42], [94mLoss[0m : 10.89674
[1mStep[0m  [28/42], [94mLoss[0m : 10.98372
[1mStep[0m  [32/42], [94mLoss[0m : 10.88946
[1mStep[0m  [36/42], [94mLoss[0m : 10.67675
[1mStep[0m  [40/42], [94mLoss[0m : 10.95351

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.746, [92mTest[0m: 10.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.86738
[1mStep[0m  [4/42], [94mLoss[0m : 10.78749
[1mStep[0m  [8/42], [94mLoss[0m : 10.42562
[1mStep[0m  [12/42], [94mLoss[0m : 10.75017
[1mStep[0m  [16/42], [94mLoss[0m : 11.02794
[1mStep[0m  [20/42], [94mLoss[0m : 10.95021
[1mStep[0m  [24/42], [94mLoss[0m : 10.63666
[1mStep[0m  [28/42], [94mLoss[0m : 10.30179
[1mStep[0m  [32/42], [94mLoss[0m : 10.16071
[1mStep[0m  [36/42], [94mLoss[0m : 10.81349
[1mStep[0m  [40/42], [94mLoss[0m : 10.68455

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.747, [92mTest[0m: 10.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93324
[1mStep[0m  [4/42], [94mLoss[0m : 10.68566
[1mStep[0m  [8/42], [94mLoss[0m : 10.77045
[1mStep[0m  [12/42], [94mLoss[0m : 10.92581
[1mStep[0m  [16/42], [94mLoss[0m : 10.76992
[1mStep[0m  [20/42], [94mLoss[0m : 10.62531
[1mStep[0m  [24/42], [94mLoss[0m : 10.83173
[1mStep[0m  [28/42], [94mLoss[0m : 11.03581
[1mStep[0m  [32/42], [94mLoss[0m : 10.13155
[1mStep[0m  [36/42], [94mLoss[0m : 10.66688
[1mStep[0m  [40/42], [94mLoss[0m : 10.75683

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.731, [92mTest[0m: 10.723, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71594
[1mStep[0m  [4/42], [94mLoss[0m : 10.61028
[1mStep[0m  [8/42], [94mLoss[0m : 10.75141
[1mStep[0m  [12/42], [94mLoss[0m : 10.81775
[1mStep[0m  [16/42], [94mLoss[0m : 10.88198
[1mStep[0m  [20/42], [94mLoss[0m : 10.72916
[1mStep[0m  [24/42], [94mLoss[0m : 10.40277
[1mStep[0m  [28/42], [94mLoss[0m : 10.86230
[1mStep[0m  [32/42], [94mLoss[0m : 10.46609
[1mStep[0m  [36/42], [94mLoss[0m : 11.14707
[1mStep[0m  [40/42], [94mLoss[0m : 11.22623

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.727, [92mTest[0m: 10.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.78326
[1mStep[0m  [4/42], [94mLoss[0m : 10.70382
[1mStep[0m  [8/42], [94mLoss[0m : 11.03102
[1mStep[0m  [12/42], [94mLoss[0m : 10.73313
[1mStep[0m  [16/42], [94mLoss[0m : 10.50530
[1mStep[0m  [20/42], [94mLoss[0m : 10.53727
[1mStep[0m  [24/42], [94mLoss[0m : 10.64206
[1mStep[0m  [28/42], [94mLoss[0m : 10.96455
[1mStep[0m  [32/42], [94mLoss[0m : 10.45689
[1mStep[0m  [36/42], [94mLoss[0m : 10.80702
[1mStep[0m  [40/42], [94mLoss[0m : 10.54140

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.715, [92mTest[0m: 10.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.45989
[1mStep[0m  [4/42], [94mLoss[0m : 11.01884
[1mStep[0m  [8/42], [94mLoss[0m : 10.76535
[1mStep[0m  [12/42], [94mLoss[0m : 10.55476
[1mStep[0m  [16/42], [94mLoss[0m : 10.72657
[1mStep[0m  [20/42], [94mLoss[0m : 10.60746
[1mStep[0m  [24/42], [94mLoss[0m : 10.65313
[1mStep[0m  [28/42], [94mLoss[0m : 10.77123
[1mStep[0m  [32/42], [94mLoss[0m : 11.04042
[1mStep[0m  [36/42], [94mLoss[0m : 10.55627
[1mStep[0m  [40/42], [94mLoss[0m : 10.45248

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.706, [92mTest[0m: 10.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.19740
[1mStep[0m  [4/42], [94mLoss[0m : 10.53821
[1mStep[0m  [8/42], [94mLoss[0m : 10.50403
[1mStep[0m  [12/42], [94mLoss[0m : 10.79042
[1mStep[0m  [16/42], [94mLoss[0m : 10.74036
[1mStep[0m  [20/42], [94mLoss[0m : 10.54931
[1mStep[0m  [24/42], [94mLoss[0m : 11.03810
[1mStep[0m  [28/42], [94mLoss[0m : 10.47702
[1mStep[0m  [32/42], [94mLoss[0m : 10.34385
[1mStep[0m  [36/42], [94mLoss[0m : 10.61027
[1mStep[0m  [40/42], [94mLoss[0m : 10.84114

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.698, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87382
[1mStep[0m  [4/42], [94mLoss[0m : 11.05296
[1mStep[0m  [8/42], [94mLoss[0m : 10.84857
[1mStep[0m  [12/42], [94mLoss[0m : 10.49312
[1mStep[0m  [16/42], [94mLoss[0m : 10.48460
[1mStep[0m  [20/42], [94mLoss[0m : 10.51826
[1mStep[0m  [24/42], [94mLoss[0m : 10.76179
[1mStep[0m  [28/42], [94mLoss[0m : 10.68988
[1mStep[0m  [32/42], [94mLoss[0m : 10.89005
[1mStep[0m  [36/42], [94mLoss[0m : 10.59086
[1mStep[0m  [40/42], [94mLoss[0m : 10.64384

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.682, [92mTest[0m: 10.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.27008
[1mStep[0m  [4/42], [94mLoss[0m : 10.65969
[1mStep[0m  [8/42], [94mLoss[0m : 10.60461
[1mStep[0m  [12/42], [94mLoss[0m : 10.75570
[1mStep[0m  [16/42], [94mLoss[0m : 10.70611
[1mStep[0m  [20/42], [94mLoss[0m : 10.61771
[1mStep[0m  [24/42], [94mLoss[0m : 10.83909
[1mStep[0m  [28/42], [94mLoss[0m : 10.26931
[1mStep[0m  [32/42], [94mLoss[0m : 10.97088
[1mStep[0m  [36/42], [94mLoss[0m : 11.00021
[1mStep[0m  [40/42], [94mLoss[0m : 10.71772

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.02755
[1mStep[0m  [4/42], [94mLoss[0m : 10.77289
[1mStep[0m  [8/42], [94mLoss[0m : 10.76126
[1mStep[0m  [12/42], [94mLoss[0m : 10.98487
[1mStep[0m  [16/42], [94mLoss[0m : 10.80556
[1mStep[0m  [20/42], [94mLoss[0m : 10.30028
[1mStep[0m  [24/42], [94mLoss[0m : 10.27805
[1mStep[0m  [28/42], [94mLoss[0m : 10.04267
[1mStep[0m  [32/42], [94mLoss[0m : 10.87477
[1mStep[0m  [36/42], [94mLoss[0m : 11.04045
[1mStep[0m  [40/42], [94mLoss[0m : 10.59484

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.35808
[1mStep[0m  [4/42], [94mLoss[0m : 10.31478
[1mStep[0m  [8/42], [94mLoss[0m : 10.70710
[1mStep[0m  [12/42], [94mLoss[0m : 10.73037
[1mStep[0m  [16/42], [94mLoss[0m : 10.79380
[1mStep[0m  [20/42], [94mLoss[0m : 10.65142
[1mStep[0m  [24/42], [94mLoss[0m : 10.87093
[1mStep[0m  [28/42], [94mLoss[0m : 10.47126
[1mStep[0m  [32/42], [94mLoss[0m : 10.79951
[1mStep[0m  [36/42], [94mLoss[0m : 10.73876
[1mStep[0m  [40/42], [94mLoss[0m : 10.53805

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.652, [92mTest[0m: 10.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.40650
[1mStep[0m  [4/42], [94mLoss[0m : 10.67480
[1mStep[0m  [8/42], [94mLoss[0m : 10.37489
[1mStep[0m  [12/42], [94mLoss[0m : 10.69351
[1mStep[0m  [16/42], [94mLoss[0m : 10.62838
[1mStep[0m  [20/42], [94mLoss[0m : 10.81192
[1mStep[0m  [24/42], [94mLoss[0m : 11.01495
[1mStep[0m  [28/42], [94mLoss[0m : 10.51709
[1mStep[0m  [32/42], [94mLoss[0m : 10.48916
[1mStep[0m  [36/42], [94mLoss[0m : 11.09043
[1mStep[0m  [40/42], [94mLoss[0m : 11.17511

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.649, [92mTest[0m: 10.613, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.59919
[1mStep[0m  [4/42], [94mLoss[0m : 10.59896
[1mStep[0m  [8/42], [94mLoss[0m : 10.29855
[1mStep[0m  [12/42], [94mLoss[0m : 10.63521
[1mStep[0m  [16/42], [94mLoss[0m : 10.60605
[1mStep[0m  [20/42], [94mLoss[0m : 10.56716
[1mStep[0m  [24/42], [94mLoss[0m : 10.68092
[1mStep[0m  [28/42], [94mLoss[0m : 10.42003
[1mStep[0m  [32/42], [94mLoss[0m : 10.77576
[1mStep[0m  [36/42], [94mLoss[0m : 10.83388
[1mStep[0m  [40/42], [94mLoss[0m : 10.43462

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.624, [92mTest[0m: 10.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.78377
[1mStep[0m  [4/42], [94mLoss[0m : 10.76747
[1mStep[0m  [8/42], [94mLoss[0m : 10.90254
[1mStep[0m  [12/42], [94mLoss[0m : 10.09419
[1mStep[0m  [16/42], [94mLoss[0m : 10.31381
[1mStep[0m  [20/42], [94mLoss[0m : 10.89168
[1mStep[0m  [24/42], [94mLoss[0m : 10.90259
[1mStep[0m  [28/42], [94mLoss[0m : 10.57897
[1mStep[0m  [32/42], [94mLoss[0m : 10.65681
[1mStep[0m  [36/42], [94mLoss[0m : 10.86053
[1mStep[0m  [40/42], [94mLoss[0m : 10.60820

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.625, [92mTest[0m: 10.592, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.62084
[1mStep[0m  [4/42], [94mLoss[0m : 10.50706
[1mStep[0m  [8/42], [94mLoss[0m : 10.59724
[1mStep[0m  [12/42], [94mLoss[0m : 10.65635
[1mStep[0m  [16/42], [94mLoss[0m : 10.91965
[1mStep[0m  [20/42], [94mLoss[0m : 10.56200
[1mStep[0m  [24/42], [94mLoss[0m : 10.67013
[1mStep[0m  [28/42], [94mLoss[0m : 10.58110
[1mStep[0m  [32/42], [94mLoss[0m : 10.50840
[1mStep[0m  [36/42], [94mLoss[0m : 10.94072
[1mStep[0m  [40/42], [94mLoss[0m : 10.47951

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.50483
[1mStep[0m  [4/42], [94mLoss[0m : 10.57801
[1mStep[0m  [8/42], [94mLoss[0m : 10.55875
[1mStep[0m  [12/42], [94mLoss[0m : 10.68531
[1mStep[0m  [16/42], [94mLoss[0m : 10.65511
[1mStep[0m  [20/42], [94mLoss[0m : 10.31361
[1mStep[0m  [24/42], [94mLoss[0m : 10.80280
[1mStep[0m  [28/42], [94mLoss[0m : 10.55790
[1mStep[0m  [32/42], [94mLoss[0m : 10.33385
[1mStep[0m  [36/42], [94mLoss[0m : 10.65796
[1mStep[0m  [40/42], [94mLoss[0m : 10.80729

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.602, [92mTest[0m: 10.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.55321
[1mStep[0m  [4/42], [94mLoss[0m : 10.58512
[1mStep[0m  [8/42], [94mLoss[0m : 10.48604
[1mStep[0m  [12/42], [94mLoss[0m : 10.71070
[1mStep[0m  [16/42], [94mLoss[0m : 10.82338
[1mStep[0m  [20/42], [94mLoss[0m : 10.27906
[1mStep[0m  [24/42], [94mLoss[0m : 10.63499
[1mStep[0m  [28/42], [94mLoss[0m : 10.20333
[1mStep[0m  [32/42], [94mLoss[0m : 10.69474
[1mStep[0m  [36/42], [94mLoss[0m : 10.74452
[1mStep[0m  [40/42], [94mLoss[0m : 10.92777

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.585, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26087
[1mStep[0m  [4/42], [94mLoss[0m : 10.80974
[1mStep[0m  [8/42], [94mLoss[0m : 10.60258
[1mStep[0m  [12/42], [94mLoss[0m : 10.33277
[1mStep[0m  [16/42], [94mLoss[0m : 10.61181
[1mStep[0m  [20/42], [94mLoss[0m : 10.65877
[1mStep[0m  [24/42], [94mLoss[0m : 10.40141
[1mStep[0m  [28/42], [94mLoss[0m : 10.37055
[1mStep[0m  [32/42], [94mLoss[0m : 10.82607
[1mStep[0m  [36/42], [94mLoss[0m : 10.85595
[1mStep[0m  [40/42], [94mLoss[0m : 10.56708

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.45955
[1mStep[0m  [4/42], [94mLoss[0m : 10.38024
[1mStep[0m  [8/42], [94mLoss[0m : 10.51328
[1mStep[0m  [12/42], [94mLoss[0m : 10.40187
[1mStep[0m  [16/42], [94mLoss[0m : 10.93972
[1mStep[0m  [20/42], [94mLoss[0m : 10.25049
[1mStep[0m  [24/42], [94mLoss[0m : 10.86192
[1mStep[0m  [28/42], [94mLoss[0m : 11.05920
[1mStep[0m  [32/42], [94mLoss[0m : 10.62309
[1mStep[0m  [36/42], [94mLoss[0m : 10.57272
[1mStep[0m  [40/42], [94mLoss[0m : 10.84162

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.580, [92mTest[0m: 10.531, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76592
[1mStep[0m  [4/42], [94mLoss[0m : 11.12722
[1mStep[0m  [8/42], [94mLoss[0m : 10.58664
[1mStep[0m  [12/42], [94mLoss[0m : 10.81048
[1mStep[0m  [16/42], [94mLoss[0m : 10.88314
[1mStep[0m  [20/42], [94mLoss[0m : 10.68482
[1mStep[0m  [24/42], [94mLoss[0m : 10.49254
[1mStep[0m  [28/42], [94mLoss[0m : 10.62770
[1mStep[0m  [32/42], [94mLoss[0m : 10.32243
[1mStep[0m  [36/42], [94mLoss[0m : 10.43943
[1mStep[0m  [40/42], [94mLoss[0m : 10.30111

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.559, [92mTest[0m: 10.498, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53713
[1mStep[0m  [4/42], [94mLoss[0m : 10.52101
[1mStep[0m  [8/42], [94mLoss[0m : 10.50755
[1mStep[0m  [12/42], [94mLoss[0m : 10.69008
[1mStep[0m  [16/42], [94mLoss[0m : 10.54617
[1mStep[0m  [20/42], [94mLoss[0m : 10.41992
[1mStep[0m  [24/42], [94mLoss[0m : 10.58759
[1mStep[0m  [28/42], [94mLoss[0m : 10.73346
[1mStep[0m  [32/42], [94mLoss[0m : 10.61013
[1mStep[0m  [36/42], [94mLoss[0m : 10.32809
[1mStep[0m  [40/42], [94mLoss[0m : 10.29924

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.506, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51276
[1mStep[0m  [4/42], [94mLoss[0m : 11.06312
[1mStep[0m  [8/42], [94mLoss[0m : 10.36140
[1mStep[0m  [12/42], [94mLoss[0m : 10.34530
[1mStep[0m  [16/42], [94mLoss[0m : 10.92306
[1mStep[0m  [20/42], [94mLoss[0m : 10.56403
[1mStep[0m  [24/42], [94mLoss[0m : 10.51594
[1mStep[0m  [28/42], [94mLoss[0m : 10.63792
[1mStep[0m  [32/42], [94mLoss[0m : 10.36569
[1mStep[0m  [36/42], [94mLoss[0m : 10.67861
[1mStep[0m  [40/42], [94mLoss[0m : 11.02169

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.542, [92mTest[0m: 10.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54086
[1mStep[0m  [4/42], [94mLoss[0m : 10.60825
[1mStep[0m  [8/42], [94mLoss[0m : 10.69214
[1mStep[0m  [12/42], [94mLoss[0m : 10.09037
[1mStep[0m  [16/42], [94mLoss[0m : 10.54634
[1mStep[0m  [20/42], [94mLoss[0m : 10.20034
[1mStep[0m  [24/42], [94mLoss[0m : 10.56488
[1mStep[0m  [28/42], [94mLoss[0m : 10.41771
[1mStep[0m  [32/42], [94mLoss[0m : 10.79374
[1mStep[0m  [36/42], [94mLoss[0m : 10.61919
[1mStep[0m  [40/42], [94mLoss[0m : 10.54751

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.537, [92mTest[0m: 10.485, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53942
[1mStep[0m  [4/42], [94mLoss[0m : 10.68014
[1mStep[0m  [8/42], [94mLoss[0m : 10.23712
[1mStep[0m  [12/42], [94mLoss[0m : 10.50492
[1mStep[0m  [16/42], [94mLoss[0m : 10.40967
[1mStep[0m  [20/42], [94mLoss[0m : 10.73245
[1mStep[0m  [24/42], [94mLoss[0m : 10.77305
[1mStep[0m  [28/42], [94mLoss[0m : 10.28434
[1mStep[0m  [32/42], [94mLoss[0m : 10.59093
[1mStep[0m  [36/42], [94mLoss[0m : 10.41841
[1mStep[0m  [40/42], [94mLoss[0m : 10.65109

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.530, [92mTest[0m: 10.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.78720
[1mStep[0m  [4/42], [94mLoss[0m : 10.53325
[1mStep[0m  [8/42], [94mLoss[0m : 10.38339
[1mStep[0m  [12/42], [94mLoss[0m : 10.54523
[1mStep[0m  [16/42], [94mLoss[0m : 10.62259
[1mStep[0m  [20/42], [94mLoss[0m : 10.30175
[1mStep[0m  [24/42], [94mLoss[0m : 10.16471
[1mStep[0m  [28/42], [94mLoss[0m : 10.55606
[1mStep[0m  [32/42], [94mLoss[0m : 10.81153
[1mStep[0m  [36/42], [94mLoss[0m : 10.64352
[1mStep[0m  [40/42], [94mLoss[0m : 10.53837

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68322
[1mStep[0m  [4/42], [94mLoss[0m : 10.81984
[1mStep[0m  [8/42], [94mLoss[0m : 10.41878
[1mStep[0m  [12/42], [94mLoss[0m : 10.63510
[1mStep[0m  [16/42], [94mLoss[0m : 10.25074
[1mStep[0m  [20/42], [94mLoss[0m : 10.53626
[1mStep[0m  [24/42], [94mLoss[0m : 10.66845
[1mStep[0m  [28/42], [94mLoss[0m : 10.60297
[1mStep[0m  [32/42], [94mLoss[0m : 10.04259
[1mStep[0m  [36/42], [94mLoss[0m : 10.46575
[1mStep[0m  [40/42], [94mLoss[0m : 10.33464

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.510, [92mTest[0m: 10.435, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.416
====================================

Phase 1 - Evaluation MAE:  10.416478565761022
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.32186
[1mStep[0m  [4/42], [94mLoss[0m : 10.39234
[1mStep[0m  [8/42], [94mLoss[0m : 10.35279
[1mStep[0m  [12/42], [94mLoss[0m : 10.94282
[1mStep[0m  [16/42], [94mLoss[0m : 10.46167
[1mStep[0m  [20/42], [94mLoss[0m : 10.91210
[1mStep[0m  [24/42], [94mLoss[0m : 10.69596
[1mStep[0m  [28/42], [94mLoss[0m : 10.68242
[1mStep[0m  [32/42], [94mLoss[0m : 10.75980
[1mStep[0m  [36/42], [94mLoss[0m : 10.47122
[1mStep[0m  [40/42], [94mLoss[0m : 10.70047

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.491, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32129
[1mStep[0m  [4/42], [94mLoss[0m : 10.62697
[1mStep[0m  [8/42], [94mLoss[0m : 10.45027
[1mStep[0m  [12/42], [94mLoss[0m : 10.57724
[1mStep[0m  [16/42], [94mLoss[0m : 10.43890
[1mStep[0m  [20/42], [94mLoss[0m : 10.25801
[1mStep[0m  [24/42], [94mLoss[0m : 10.56516
[1mStep[0m  [28/42], [94mLoss[0m : 10.64999
[1mStep[0m  [32/42], [94mLoss[0m : 10.54061
[1mStep[0m  [36/42], [94mLoss[0m : 10.17347
[1mStep[0m  [40/42], [94mLoss[0m : 10.74884

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.482, [92mTest[0m: 10.414, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.50060
[1mStep[0m  [4/42], [94mLoss[0m : 10.26891
[1mStep[0m  [8/42], [94mLoss[0m : 10.70855
[1mStep[0m  [12/42], [94mLoss[0m : 10.41664
[1mStep[0m  [16/42], [94mLoss[0m : 10.14138
[1mStep[0m  [20/42], [94mLoss[0m : 10.78862
[1mStep[0m  [24/42], [94mLoss[0m : 10.62770
[1mStep[0m  [28/42], [94mLoss[0m : 10.43362
[1mStep[0m  [32/42], [94mLoss[0m : 10.47282
[1mStep[0m  [36/42], [94mLoss[0m : 10.38351
[1mStep[0m  [40/42], [94mLoss[0m : 10.62647

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.470, [92mTest[0m: 10.387, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.33502
[1mStep[0m  [4/42], [94mLoss[0m : 10.36697
[1mStep[0m  [8/42], [94mLoss[0m : 10.02602
[1mStep[0m  [12/42], [94mLoss[0m : 10.58808
[1mStep[0m  [16/42], [94mLoss[0m : 10.47253
[1mStep[0m  [20/42], [94mLoss[0m : 10.48158
[1mStep[0m  [24/42], [94mLoss[0m : 10.62977
[1mStep[0m  [28/42], [94mLoss[0m : 10.46420
[1mStep[0m  [32/42], [94mLoss[0m : 10.23407
[1mStep[0m  [36/42], [94mLoss[0m : 10.65271
[1mStep[0m  [40/42], [94mLoss[0m : 10.50205

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.461, [92mTest[0m: 10.386, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.60275
[1mStep[0m  [4/42], [94mLoss[0m : 10.54091
[1mStep[0m  [8/42], [94mLoss[0m : 10.51871
[1mStep[0m  [12/42], [94mLoss[0m : 10.28699
[1mStep[0m  [16/42], [94mLoss[0m : 10.51801
[1mStep[0m  [20/42], [94mLoss[0m : 10.26040
[1mStep[0m  [24/42], [94mLoss[0m : 10.34641
[1mStep[0m  [28/42], [94mLoss[0m : 10.59252
[1mStep[0m  [32/42], [94mLoss[0m : 10.25178
[1mStep[0m  [36/42], [94mLoss[0m : 10.52970
[1mStep[0m  [40/42], [94mLoss[0m : 10.50277

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.455, [92mTest[0m: 10.366, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44060
[1mStep[0m  [4/42], [94mLoss[0m : 10.19614
[1mStep[0m  [8/42], [94mLoss[0m : 10.65936
[1mStep[0m  [12/42], [94mLoss[0m : 11.04409
[1mStep[0m  [16/42], [94mLoss[0m : 10.38680
[1mStep[0m  [20/42], [94mLoss[0m : 10.50044
[1mStep[0m  [24/42], [94mLoss[0m : 10.29980
[1mStep[0m  [28/42], [94mLoss[0m : 10.67122
[1mStep[0m  [32/42], [94mLoss[0m : 10.50617
[1mStep[0m  [36/42], [94mLoss[0m : 10.50998
[1mStep[0m  [40/42], [94mLoss[0m : 10.14611

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.352, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23618
[1mStep[0m  [4/42], [94mLoss[0m : 10.60585
[1mStep[0m  [8/42], [94mLoss[0m : 10.09158
[1mStep[0m  [12/42], [94mLoss[0m : 10.58584
[1mStep[0m  [16/42], [94mLoss[0m : 10.03947
[1mStep[0m  [20/42], [94mLoss[0m : 10.57828
[1mStep[0m  [24/42], [94mLoss[0m : 10.12802
[1mStep[0m  [28/42], [94mLoss[0m : 10.29488
[1mStep[0m  [32/42], [94mLoss[0m : 10.85885
[1mStep[0m  [36/42], [94mLoss[0m : 10.27637
[1mStep[0m  [40/42], [94mLoss[0m : 10.48935

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.333, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.18075
[1mStep[0m  [4/42], [94mLoss[0m : 10.25756
[1mStep[0m  [8/42], [94mLoss[0m : 10.23331
[1mStep[0m  [12/42], [94mLoss[0m : 10.60614
[1mStep[0m  [16/42], [94mLoss[0m : 10.47236
[1mStep[0m  [20/42], [94mLoss[0m : 10.63133
[1mStep[0m  [24/42], [94mLoss[0m : 10.39124
[1mStep[0m  [28/42], [94mLoss[0m : 10.65074
[1mStep[0m  [32/42], [94mLoss[0m : 10.64332
[1mStep[0m  [36/42], [94mLoss[0m : 10.34192
[1mStep[0m  [40/42], [94mLoss[0m : 10.35588

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.416, [92mTest[0m: 10.339, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72208
[1mStep[0m  [4/42], [94mLoss[0m : 10.09691
[1mStep[0m  [8/42], [94mLoss[0m : 10.74749
[1mStep[0m  [12/42], [94mLoss[0m : 9.95729
[1mStep[0m  [16/42], [94mLoss[0m : 10.60144
[1mStep[0m  [20/42], [94mLoss[0m : 10.20190
[1mStep[0m  [24/42], [94mLoss[0m : 10.14824
[1mStep[0m  [28/42], [94mLoss[0m : 10.06950
[1mStep[0m  [32/42], [94mLoss[0m : 10.36540
[1mStep[0m  [36/42], [94mLoss[0m : 10.33282
[1mStep[0m  [40/42], [94mLoss[0m : 10.87592

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.311, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26308
[1mStep[0m  [4/42], [94mLoss[0m : 10.76197
[1mStep[0m  [8/42], [94mLoss[0m : 10.37526
[1mStep[0m  [12/42], [94mLoss[0m : 10.55948
[1mStep[0m  [16/42], [94mLoss[0m : 10.35204
[1mStep[0m  [20/42], [94mLoss[0m : 10.31616
[1mStep[0m  [24/42], [94mLoss[0m : 10.72841
[1mStep[0m  [28/42], [94mLoss[0m : 10.29913
[1mStep[0m  [32/42], [94mLoss[0m : 10.20950
[1mStep[0m  [36/42], [94mLoss[0m : 10.35212
[1mStep[0m  [40/42], [94mLoss[0m : 10.24658

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.402, [92mTest[0m: 10.283, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52088
[1mStep[0m  [4/42], [94mLoss[0m : 10.46571
[1mStep[0m  [8/42], [94mLoss[0m : 10.72827
[1mStep[0m  [12/42], [94mLoss[0m : 10.54677
[1mStep[0m  [16/42], [94mLoss[0m : 10.00608
[1mStep[0m  [20/42], [94mLoss[0m : 10.37028
[1mStep[0m  [24/42], [94mLoss[0m : 10.23429
[1mStep[0m  [28/42], [94mLoss[0m : 10.33249
[1mStep[0m  [32/42], [94mLoss[0m : 10.30194
[1mStep[0m  [36/42], [94mLoss[0m : 10.21098
[1mStep[0m  [40/42], [94mLoss[0m : 10.21460

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.375, [92mTest[0m: 10.272, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.47192
[1mStep[0m  [4/42], [94mLoss[0m : 10.66390
[1mStep[0m  [8/42], [94mLoss[0m : 10.40899
[1mStep[0m  [12/42], [94mLoss[0m : 10.58085
[1mStep[0m  [16/42], [94mLoss[0m : 10.50282
[1mStep[0m  [20/42], [94mLoss[0m : 9.95573
[1mStep[0m  [24/42], [94mLoss[0m : 10.04165
[1mStep[0m  [28/42], [94mLoss[0m : 10.05645
[1mStep[0m  [32/42], [94mLoss[0m : 10.11499
[1mStep[0m  [36/42], [94mLoss[0m : 10.50805
[1mStep[0m  [40/42], [94mLoss[0m : 10.37583

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.369, [92mTest[0m: 10.251, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.03617
[1mStep[0m  [4/42], [94mLoss[0m : 10.55145
[1mStep[0m  [8/42], [94mLoss[0m : 10.44444
[1mStep[0m  [12/42], [94mLoss[0m : 9.79952
[1mStep[0m  [16/42], [94mLoss[0m : 10.38081
[1mStep[0m  [20/42], [94mLoss[0m : 10.02297
[1mStep[0m  [24/42], [94mLoss[0m : 10.19317
[1mStep[0m  [28/42], [94mLoss[0m : 10.07585
[1mStep[0m  [32/42], [94mLoss[0m : 10.13400
[1mStep[0m  [36/42], [94mLoss[0m : 10.37474
[1mStep[0m  [40/42], [94mLoss[0m : 10.47138

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.356, [92mTest[0m: 10.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.90647
[1mStep[0m  [4/42], [94mLoss[0m : 10.56133
[1mStep[0m  [8/42], [94mLoss[0m : 10.15433
[1mStep[0m  [12/42], [94mLoss[0m : 10.50497
[1mStep[0m  [16/42], [94mLoss[0m : 10.66882
[1mStep[0m  [20/42], [94mLoss[0m : 10.60251
[1mStep[0m  [24/42], [94mLoss[0m : 10.19534
[1mStep[0m  [28/42], [94mLoss[0m : 10.04385
[1mStep[0m  [32/42], [94mLoss[0m : 10.38054
[1mStep[0m  [36/42], [94mLoss[0m : 10.47593
[1mStep[0m  [40/42], [94mLoss[0m : 10.53429

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.350, [92mTest[0m: 10.260, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.29333
[1mStep[0m  [4/42], [94mLoss[0m : 10.32112
[1mStep[0m  [8/42], [94mLoss[0m : 10.26755
[1mStep[0m  [12/42], [94mLoss[0m : 10.91789
[1mStep[0m  [16/42], [94mLoss[0m : 10.51614
[1mStep[0m  [20/42], [94mLoss[0m : 10.46633
[1mStep[0m  [24/42], [94mLoss[0m : 10.72862
[1mStep[0m  [28/42], [94mLoss[0m : 10.47908
[1mStep[0m  [32/42], [94mLoss[0m : 10.81773
[1mStep[0m  [36/42], [94mLoss[0m : 10.47575
[1mStep[0m  [40/42], [94mLoss[0m : 10.51167

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.334, [92mTest[0m: 10.214, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.56762
[1mStep[0m  [4/42], [94mLoss[0m : 10.20051
[1mStep[0m  [8/42], [94mLoss[0m : 10.23773
[1mStep[0m  [12/42], [94mLoss[0m : 10.18060
[1mStep[0m  [16/42], [94mLoss[0m : 9.98652
[1mStep[0m  [20/42], [94mLoss[0m : 10.51243
[1mStep[0m  [24/42], [94mLoss[0m : 10.32921
[1mStep[0m  [28/42], [94mLoss[0m : 10.37014
[1mStep[0m  [32/42], [94mLoss[0m : 10.42705
[1mStep[0m  [36/42], [94mLoss[0m : 9.91902
[1mStep[0m  [40/42], [94mLoss[0m : 10.36784

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.311, [92mTest[0m: 10.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52915
[1mStep[0m  [4/42], [94mLoss[0m : 10.90599
[1mStep[0m  [8/42], [94mLoss[0m : 10.03590
[1mStep[0m  [12/42], [94mLoss[0m : 10.39411
[1mStep[0m  [16/42], [94mLoss[0m : 10.21405
[1mStep[0m  [20/42], [94mLoss[0m : 9.86092
[1mStep[0m  [24/42], [94mLoss[0m : 10.33828
[1mStep[0m  [28/42], [94mLoss[0m : 10.41451
[1mStep[0m  [32/42], [94mLoss[0m : 10.17211
[1mStep[0m  [36/42], [94mLoss[0m : 9.85989
[1mStep[0m  [40/42], [94mLoss[0m : 10.45239

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.312, [92mTest[0m: 10.185, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25411
[1mStep[0m  [4/42], [94mLoss[0m : 10.34533
[1mStep[0m  [8/42], [94mLoss[0m : 10.67190
[1mStep[0m  [12/42], [94mLoss[0m : 10.50831
[1mStep[0m  [16/42], [94mLoss[0m : 10.35017
[1mStep[0m  [20/42], [94mLoss[0m : 10.12328
[1mStep[0m  [24/42], [94mLoss[0m : 10.39362
[1mStep[0m  [28/42], [94mLoss[0m : 10.12912
[1mStep[0m  [32/42], [94mLoss[0m : 10.03566
[1mStep[0m  [36/42], [94mLoss[0m : 10.31927
[1mStep[0m  [40/42], [94mLoss[0m : 10.52480

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.299, [92mTest[0m: 10.176, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51074
[1mStep[0m  [4/42], [94mLoss[0m : 10.46497
[1mStep[0m  [8/42], [94mLoss[0m : 10.20963
[1mStep[0m  [12/42], [94mLoss[0m : 10.16787
[1mStep[0m  [16/42], [94mLoss[0m : 9.98561
[1mStep[0m  [20/42], [94mLoss[0m : 10.16008
[1mStep[0m  [24/42], [94mLoss[0m : 10.58963
[1mStep[0m  [28/42], [94mLoss[0m : 10.25636
[1mStep[0m  [32/42], [94mLoss[0m : 10.14011
[1mStep[0m  [36/42], [94mLoss[0m : 10.42018
[1mStep[0m  [40/42], [94mLoss[0m : 10.12192

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.294, [92mTest[0m: 10.157, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.24453
[1mStep[0m  [4/42], [94mLoss[0m : 10.35809
[1mStep[0m  [8/42], [94mLoss[0m : 10.41756
[1mStep[0m  [12/42], [94mLoss[0m : 10.17158
[1mStep[0m  [16/42], [94mLoss[0m : 10.33631
[1mStep[0m  [20/42], [94mLoss[0m : 10.52489
[1mStep[0m  [24/42], [94mLoss[0m : 10.48748
[1mStep[0m  [28/42], [94mLoss[0m : 10.14543
[1mStep[0m  [32/42], [94mLoss[0m : 9.89186
[1mStep[0m  [36/42], [94mLoss[0m : 10.05890
[1mStep[0m  [40/42], [94mLoss[0m : 10.47692

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.274, [92mTest[0m: 10.158, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.10482
[1mStep[0m  [4/42], [94mLoss[0m : 10.59855
[1mStep[0m  [8/42], [94mLoss[0m : 9.63974
[1mStep[0m  [12/42], [94mLoss[0m : 10.53816
[1mStep[0m  [16/42], [94mLoss[0m : 10.44421
[1mStep[0m  [20/42], [94mLoss[0m : 10.39806
[1mStep[0m  [24/42], [94mLoss[0m : 10.72526
[1mStep[0m  [28/42], [94mLoss[0m : 10.01512
[1mStep[0m  [32/42], [94mLoss[0m : 10.19071
[1mStep[0m  [36/42], [94mLoss[0m : 10.30682
[1mStep[0m  [40/42], [94mLoss[0m : 9.98238

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.262, [92mTest[0m: 10.140, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25933
[1mStep[0m  [4/42], [94mLoss[0m : 10.40361
[1mStep[0m  [8/42], [94mLoss[0m : 10.23064
[1mStep[0m  [12/42], [94mLoss[0m : 10.45112
[1mStep[0m  [16/42], [94mLoss[0m : 9.83556
[1mStep[0m  [20/42], [94mLoss[0m : 10.05390
[1mStep[0m  [24/42], [94mLoss[0m : 10.44331
[1mStep[0m  [28/42], [94mLoss[0m : 9.74726
[1mStep[0m  [32/42], [94mLoss[0m : 10.35190
[1mStep[0m  [36/42], [94mLoss[0m : 10.57375
[1mStep[0m  [40/42], [94mLoss[0m : 10.26432

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.257, [92mTest[0m: 10.111, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15551
[1mStep[0m  [4/42], [94mLoss[0m : 10.09189
[1mStep[0m  [8/42], [94mLoss[0m : 10.32470
[1mStep[0m  [12/42], [94mLoss[0m : 10.00650
[1mStep[0m  [16/42], [94mLoss[0m : 10.42591
[1mStep[0m  [20/42], [94mLoss[0m : 10.53958
[1mStep[0m  [24/42], [94mLoss[0m : 10.26190
[1mStep[0m  [28/42], [94mLoss[0m : 10.39121
[1mStep[0m  [32/42], [94mLoss[0m : 10.30620
[1mStep[0m  [36/42], [94mLoss[0m : 10.35291
[1mStep[0m  [40/42], [94mLoss[0m : 10.08566

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.241, [92mTest[0m: 10.130, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.03860
[1mStep[0m  [4/42], [94mLoss[0m : 10.08008
[1mStep[0m  [8/42], [94mLoss[0m : 10.27286
[1mStep[0m  [12/42], [94mLoss[0m : 10.22398
[1mStep[0m  [16/42], [94mLoss[0m : 10.32411
[1mStep[0m  [20/42], [94mLoss[0m : 10.01376
[1mStep[0m  [24/42], [94mLoss[0m : 10.37833
[1mStep[0m  [28/42], [94mLoss[0m : 10.17131
[1mStep[0m  [32/42], [94mLoss[0m : 10.18044
[1mStep[0m  [36/42], [94mLoss[0m : 9.65768
[1mStep[0m  [40/42], [94mLoss[0m : 10.21639

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.228, [92mTest[0m: 10.109, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.13267
[1mStep[0m  [4/42], [94mLoss[0m : 10.03499
[1mStep[0m  [8/42], [94mLoss[0m : 10.31574
[1mStep[0m  [12/42], [94mLoss[0m : 10.21547
[1mStep[0m  [16/42], [94mLoss[0m : 10.16060
[1mStep[0m  [20/42], [94mLoss[0m : 10.11870
[1mStep[0m  [24/42], [94mLoss[0m : 10.08535
[1mStep[0m  [28/42], [94mLoss[0m : 10.70481
[1mStep[0m  [32/42], [94mLoss[0m : 10.32756
[1mStep[0m  [36/42], [94mLoss[0m : 10.01128
[1mStep[0m  [40/42], [94mLoss[0m : 10.30338

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.215, [92mTest[0m: 10.073, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19579
[1mStep[0m  [4/42], [94mLoss[0m : 10.33120
[1mStep[0m  [8/42], [94mLoss[0m : 10.22693
[1mStep[0m  [12/42], [94mLoss[0m : 10.21353
[1mStep[0m  [16/42], [94mLoss[0m : 10.24292
[1mStep[0m  [20/42], [94mLoss[0m : 10.05459
[1mStep[0m  [24/42], [94mLoss[0m : 10.05330
[1mStep[0m  [28/42], [94mLoss[0m : 10.28141
[1mStep[0m  [32/42], [94mLoss[0m : 10.44089
[1mStep[0m  [36/42], [94mLoss[0m : 9.98679
[1mStep[0m  [40/42], [94mLoss[0m : 10.46867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.204, [92mTest[0m: 10.073, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22525
[1mStep[0m  [4/42], [94mLoss[0m : 9.92825
[1mStep[0m  [8/42], [94mLoss[0m : 10.12118
[1mStep[0m  [12/42], [94mLoss[0m : 10.19251
[1mStep[0m  [16/42], [94mLoss[0m : 10.47531
[1mStep[0m  [20/42], [94mLoss[0m : 10.32152
[1mStep[0m  [24/42], [94mLoss[0m : 10.35197
[1mStep[0m  [28/42], [94mLoss[0m : 10.12390
[1mStep[0m  [32/42], [94mLoss[0m : 10.28720
[1mStep[0m  [36/42], [94mLoss[0m : 9.97341
