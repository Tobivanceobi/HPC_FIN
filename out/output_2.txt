no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  2
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.06001
[1mStep[0m  [5/53], [94mLoss[0m : 10.81750
[1mStep[0m  [10/53], [94mLoss[0m : 11.07603
[1mStep[0m  [15/53], [94mLoss[0m : 11.12480
[1mStep[0m  [20/53], [94mLoss[0m : 11.27729
[1mStep[0m  [25/53], [94mLoss[0m : 10.97519
[1mStep[0m  [30/53], [94mLoss[0m : 10.83125
[1mStep[0m  [35/53], [94mLoss[0m : 10.76329
[1mStep[0m  [40/53], [94mLoss[0m : 10.99446
[1mStep[0m  [45/53], [94mLoss[0m : 11.09793
[1mStep[0m  [50/53], [94mLoss[0m : 10.54601

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.891, [92mTest[0m: 10.876, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.08984
[1mStep[0m  [5/53], [94mLoss[0m : 10.67704
[1mStep[0m  [10/53], [94mLoss[0m : 10.94725
[1mStep[0m  [15/53], [94mLoss[0m : 11.06863
[1mStep[0m  [20/53], [94mLoss[0m : 10.76165
[1mStep[0m  [25/53], [94mLoss[0m : 11.02406
[1mStep[0m  [30/53], [94mLoss[0m : 10.94955
[1mStep[0m  [35/53], [94mLoss[0m : 10.71572
[1mStep[0m  [40/53], [94mLoss[0m : 10.70611
[1mStep[0m  [45/53], [94mLoss[0m : 11.16115
[1mStep[0m  [50/53], [94mLoss[0m : 10.95605

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.888, [92mTest[0m: 10.908, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.08273
[1mStep[0m  [5/53], [94mLoss[0m : 10.99315
[1mStep[0m  [10/53], [94mLoss[0m : 10.76586
[1mStep[0m  [15/53], [94mLoss[0m : 10.86830
[1mStep[0m  [20/53], [94mLoss[0m : 10.87914
[1mStep[0m  [25/53], [94mLoss[0m : 10.76972
[1mStep[0m  [30/53], [94mLoss[0m : 10.68450
[1mStep[0m  [35/53], [94mLoss[0m : 10.88134
[1mStep[0m  [40/53], [94mLoss[0m : 10.52293
[1mStep[0m  [45/53], [94mLoss[0m : 11.05134
[1mStep[0m  [50/53], [94mLoss[0m : 10.92415

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.880, [92mTest[0m: 10.912, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.02661
[1mStep[0m  [5/53], [94mLoss[0m : 10.82957
[1mStep[0m  [10/53], [94mLoss[0m : 10.76884
[1mStep[0m  [15/53], [94mLoss[0m : 11.02004
[1mStep[0m  [20/53], [94mLoss[0m : 10.61521
[1mStep[0m  [25/53], [94mLoss[0m : 10.73004
[1mStep[0m  [30/53], [94mLoss[0m : 11.10762
[1mStep[0m  [35/53], [94mLoss[0m : 11.06186
[1mStep[0m  [40/53], [94mLoss[0m : 10.81934
[1mStep[0m  [45/53], [94mLoss[0m : 10.83527
[1mStep[0m  [50/53], [94mLoss[0m : 10.91766

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.869, [92mTest[0m: 10.893, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.97270
[1mStep[0m  [5/53], [94mLoss[0m : 10.86944
[1mStep[0m  [10/53], [94mLoss[0m : 10.70056
[1mStep[0m  [15/53], [94mLoss[0m : 10.92778
[1mStep[0m  [20/53], [94mLoss[0m : 10.86808
[1mStep[0m  [25/53], [94mLoss[0m : 10.87868
[1mStep[0m  [30/53], [94mLoss[0m : 10.78220
[1mStep[0m  [35/53], [94mLoss[0m : 10.92527
[1mStep[0m  [40/53], [94mLoss[0m : 10.97697
[1mStep[0m  [45/53], [94mLoss[0m : 10.83140
[1mStep[0m  [50/53], [94mLoss[0m : 10.34945

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.879, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.75491
[1mStep[0m  [5/53], [94mLoss[0m : 10.54200
[1mStep[0m  [10/53], [94mLoss[0m : 10.92134
[1mStep[0m  [15/53], [94mLoss[0m : 10.55511
[1mStep[0m  [20/53], [94mLoss[0m : 10.81808
[1mStep[0m  [25/53], [94mLoss[0m : 11.18294
[1mStep[0m  [30/53], [94mLoss[0m : 10.57078
[1mStep[0m  [35/53], [94mLoss[0m : 11.13475
[1mStep[0m  [40/53], [94mLoss[0m : 10.72496
[1mStep[0m  [45/53], [94mLoss[0m : 10.78098
[1mStep[0m  [50/53], [94mLoss[0m : 10.83245

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.856, [92mTest[0m: 10.871, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.60304
[1mStep[0m  [5/53], [94mLoss[0m : 10.65821
[1mStep[0m  [10/53], [94mLoss[0m : 10.71389
[1mStep[0m  [15/53], [94mLoss[0m : 10.87121
[1mStep[0m  [20/53], [94mLoss[0m : 10.65686
[1mStep[0m  [25/53], [94mLoss[0m : 11.20145
[1mStep[0m  [30/53], [94mLoss[0m : 11.14575
[1mStep[0m  [35/53], [94mLoss[0m : 10.64161
[1mStep[0m  [40/53], [94mLoss[0m : 10.78805
[1mStep[0m  [45/53], [94mLoss[0m : 11.01913
[1mStep[0m  [50/53], [94mLoss[0m : 11.19711

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.844, [92mTest[0m: 10.856, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.04084
[1mStep[0m  [5/53], [94mLoss[0m : 10.83306
[1mStep[0m  [10/53], [94mLoss[0m : 10.67236
[1mStep[0m  [15/53], [94mLoss[0m : 11.02551
[1mStep[0m  [20/53], [94mLoss[0m : 10.60246
[1mStep[0m  [25/53], [94mLoss[0m : 10.84139
[1mStep[0m  [30/53], [94mLoss[0m : 10.95738
[1mStep[0m  [35/53], [94mLoss[0m : 11.02558
[1mStep[0m  [40/53], [94mLoss[0m : 10.85518
[1mStep[0m  [45/53], [94mLoss[0m : 10.58944
[1mStep[0m  [50/53], [94mLoss[0m : 10.88776

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.840, [92mTest[0m: 10.863, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.99170
[1mStep[0m  [5/53], [94mLoss[0m : 10.65768
[1mStep[0m  [10/53], [94mLoss[0m : 10.98153
[1mStep[0m  [15/53], [94mLoss[0m : 11.04345
[1mStep[0m  [20/53], [94mLoss[0m : 11.38888
[1mStep[0m  [25/53], [94mLoss[0m : 10.81710
[1mStep[0m  [30/53], [94mLoss[0m : 10.99196
[1mStep[0m  [35/53], [94mLoss[0m : 10.58031
[1mStep[0m  [40/53], [94mLoss[0m : 10.63026
[1mStep[0m  [45/53], [94mLoss[0m : 11.08494
[1mStep[0m  [50/53], [94mLoss[0m : 10.56680

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.834, [92mTest[0m: 10.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.45197
[1mStep[0m  [5/53], [94mLoss[0m : 10.65885
[1mStep[0m  [10/53], [94mLoss[0m : 11.03337
[1mStep[0m  [15/53], [94mLoss[0m : 10.81150
[1mStep[0m  [20/53], [94mLoss[0m : 10.48985
[1mStep[0m  [25/53], [94mLoss[0m : 10.75575
[1mStep[0m  [30/53], [94mLoss[0m : 11.04181
[1mStep[0m  [35/53], [94mLoss[0m : 10.65255
[1mStep[0m  [40/53], [94mLoss[0m : 10.59168
[1mStep[0m  [45/53], [94mLoss[0m : 11.15614
[1mStep[0m  [50/53], [94mLoss[0m : 10.29068

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.826, [92mTest[0m: 10.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.83731
[1mStep[0m  [5/53], [94mLoss[0m : 11.16386
[1mStep[0m  [10/53], [94mLoss[0m : 10.74668
[1mStep[0m  [15/53], [94mLoss[0m : 10.98367
[1mStep[0m  [20/53], [94mLoss[0m : 10.93410
[1mStep[0m  [25/53], [94mLoss[0m : 10.83441
[1mStep[0m  [30/53], [94mLoss[0m : 10.69162
[1mStep[0m  [35/53], [94mLoss[0m : 10.98213
[1mStep[0m  [40/53], [94mLoss[0m : 10.81176
[1mStep[0m  [45/53], [94mLoss[0m : 10.89232
[1mStep[0m  [50/53], [94mLoss[0m : 10.50810

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.815, [92mTest[0m: 10.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.02735
[1mStep[0m  [5/53], [94mLoss[0m : 10.70533
[1mStep[0m  [10/53], [94mLoss[0m : 10.92206
[1mStep[0m  [15/53], [94mLoss[0m : 10.52803
[1mStep[0m  [20/53], [94mLoss[0m : 10.78790
[1mStep[0m  [25/53], [94mLoss[0m : 10.80209
[1mStep[0m  [30/53], [94mLoss[0m : 11.07032
[1mStep[0m  [35/53], [94mLoss[0m : 10.43187
[1mStep[0m  [40/53], [94mLoss[0m : 10.73929
[1mStep[0m  [45/53], [94mLoss[0m : 10.86863
[1mStep[0m  [50/53], [94mLoss[0m : 10.70401

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.811, [92mTest[0m: 10.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.32721
[1mStep[0m  [5/53], [94mLoss[0m : 10.95636
[1mStep[0m  [10/53], [94mLoss[0m : 10.72329
[1mStep[0m  [15/53], [94mLoss[0m : 10.89797
[1mStep[0m  [20/53], [94mLoss[0m : 10.82646
[1mStep[0m  [25/53], [94mLoss[0m : 10.78580
[1mStep[0m  [30/53], [94mLoss[0m : 10.87854
[1mStep[0m  [35/53], [94mLoss[0m : 10.64752
[1mStep[0m  [40/53], [94mLoss[0m : 10.75928
[1mStep[0m  [45/53], [94mLoss[0m : 11.05380
[1mStep[0m  [50/53], [94mLoss[0m : 10.88782

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.796, [92mTest[0m: 10.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.64538
[1mStep[0m  [5/53], [94mLoss[0m : 10.93630
[1mStep[0m  [10/53], [94mLoss[0m : 10.78947
[1mStep[0m  [15/53], [94mLoss[0m : 10.37086
[1mStep[0m  [20/53], [94mLoss[0m : 10.59765
[1mStep[0m  [25/53], [94mLoss[0m : 10.67705
[1mStep[0m  [30/53], [94mLoss[0m : 10.99817
[1mStep[0m  [35/53], [94mLoss[0m : 10.72014
[1mStep[0m  [40/53], [94mLoss[0m : 11.27438
[1mStep[0m  [45/53], [94mLoss[0m : 11.02094
[1mStep[0m  [50/53], [94mLoss[0m : 10.89701

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.775, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.53238
[1mStep[0m  [5/53], [94mLoss[0m : 10.84059
[1mStep[0m  [10/53], [94mLoss[0m : 11.01127
[1mStep[0m  [15/53], [94mLoss[0m : 10.23214
[1mStep[0m  [20/53], [94mLoss[0m : 10.63890
[1mStep[0m  [25/53], [94mLoss[0m : 11.35389
[1mStep[0m  [30/53], [94mLoss[0m : 10.77510
[1mStep[0m  [35/53], [94mLoss[0m : 11.05811
[1mStep[0m  [40/53], [94mLoss[0m : 11.09081
[1mStep[0m  [45/53], [94mLoss[0m : 10.96799
[1mStep[0m  [50/53], [94mLoss[0m : 11.17230

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.785, [92mTest[0m: 10.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.72768
[1mStep[0m  [5/53], [94mLoss[0m : 10.75527
[1mStep[0m  [10/53], [94mLoss[0m : 10.73335
[1mStep[0m  [15/53], [94mLoss[0m : 10.98493
[1mStep[0m  [20/53], [94mLoss[0m : 10.74375
[1mStep[0m  [25/53], [94mLoss[0m : 10.70102
[1mStep[0m  [30/53], [94mLoss[0m : 10.55873
[1mStep[0m  [35/53], [94mLoss[0m : 11.08973
[1mStep[0m  [40/53], [94mLoss[0m : 10.92268
[1mStep[0m  [45/53], [94mLoss[0m : 10.84642
[1mStep[0m  [50/53], [94mLoss[0m : 10.87817

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.781, [92mTest[0m: 10.759, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.75104
[1mStep[0m  [5/53], [94mLoss[0m : 10.81598
[1mStep[0m  [10/53], [94mLoss[0m : 10.96842
[1mStep[0m  [15/53], [94mLoss[0m : 10.76020
[1mStep[0m  [20/53], [94mLoss[0m : 10.81599
[1mStep[0m  [25/53], [94mLoss[0m : 10.52253
[1mStep[0m  [30/53], [94mLoss[0m : 10.78250
[1mStep[0m  [35/53], [94mLoss[0m : 10.19091
[1mStep[0m  [40/53], [94mLoss[0m : 11.18529
[1mStep[0m  [45/53], [94mLoss[0m : 10.65261
[1mStep[0m  [50/53], [94mLoss[0m : 10.53890

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.769, [92mTest[0m: 10.752, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.74386
[1mStep[0m  [5/53], [94mLoss[0m : 11.00608
[1mStep[0m  [10/53], [94mLoss[0m : 10.25470
[1mStep[0m  [15/53], [94mLoss[0m : 10.77229
[1mStep[0m  [20/53], [94mLoss[0m : 10.81324
[1mStep[0m  [25/53], [94mLoss[0m : 10.64886
[1mStep[0m  [30/53], [94mLoss[0m : 10.43456
[1mStep[0m  [35/53], [94mLoss[0m : 10.86264
[1mStep[0m  [40/53], [94mLoss[0m : 11.05342
[1mStep[0m  [45/53], [94mLoss[0m : 11.10101
[1mStep[0m  [50/53], [94mLoss[0m : 10.98448

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.763, [92mTest[0m: 10.736, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.64657
[1mStep[0m  [5/53], [94mLoss[0m : 10.80363
[1mStep[0m  [10/53], [94mLoss[0m : 10.67928
[1mStep[0m  [15/53], [94mLoss[0m : 10.76661
[1mStep[0m  [20/53], [94mLoss[0m : 10.94126
[1mStep[0m  [25/53], [94mLoss[0m : 11.03395
[1mStep[0m  [30/53], [94mLoss[0m : 11.06689
[1mStep[0m  [35/53], [94mLoss[0m : 10.67867
[1mStep[0m  [40/53], [94mLoss[0m : 10.77745
[1mStep[0m  [45/53], [94mLoss[0m : 10.70152
[1mStep[0m  [50/53], [94mLoss[0m : 10.79086

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.754, [92mTest[0m: 10.722, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.91975
[1mStep[0m  [5/53], [94mLoss[0m : 10.44687
[1mStep[0m  [10/53], [94mLoss[0m : 10.85609
[1mStep[0m  [15/53], [94mLoss[0m : 10.61574
[1mStep[0m  [20/53], [94mLoss[0m : 10.58301
[1mStep[0m  [25/53], [94mLoss[0m : 10.94801
[1mStep[0m  [30/53], [94mLoss[0m : 10.72299
[1mStep[0m  [35/53], [94mLoss[0m : 10.44780
[1mStep[0m  [40/53], [94mLoss[0m : 10.86997
[1mStep[0m  [45/53], [94mLoss[0m : 10.28643
[1mStep[0m  [50/53], [94mLoss[0m : 10.80781

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.723, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.70302
[1mStep[0m  [5/53], [94mLoss[0m : 11.04823
[1mStep[0m  [10/53], [94mLoss[0m : 10.51894
[1mStep[0m  [15/53], [94mLoss[0m : 10.60621
[1mStep[0m  [20/53], [94mLoss[0m : 10.41061
[1mStep[0m  [25/53], [94mLoss[0m : 10.90207
[1mStep[0m  [30/53], [94mLoss[0m : 10.67606
[1mStep[0m  [35/53], [94mLoss[0m : 10.74122
[1mStep[0m  [40/53], [94mLoss[0m : 10.88642
[1mStep[0m  [45/53], [94mLoss[0m : 10.86175
[1mStep[0m  [50/53], [94mLoss[0m : 10.56031

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.747, [92mTest[0m: 10.723, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.32812
[1mStep[0m  [5/53], [94mLoss[0m : 10.70296
[1mStep[0m  [10/53], [94mLoss[0m : 10.54838
[1mStep[0m  [15/53], [94mLoss[0m : 10.99532
[1mStep[0m  [20/53], [94mLoss[0m : 10.39010
[1mStep[0m  [25/53], [94mLoss[0m : 10.68440
[1mStep[0m  [30/53], [94mLoss[0m : 10.92765
[1mStep[0m  [35/53], [94mLoss[0m : 10.48125
[1mStep[0m  [40/53], [94mLoss[0m : 10.77900
[1mStep[0m  [45/53], [94mLoss[0m : 10.58449
[1mStep[0m  [50/53], [94mLoss[0m : 10.99470

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.737, [92mTest[0m: 10.681, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.68160
[1mStep[0m  [5/53], [94mLoss[0m : 10.78904
[1mStep[0m  [10/53], [94mLoss[0m : 10.53956
[1mStep[0m  [15/53], [94mLoss[0m : 11.15665
[1mStep[0m  [20/53], [94mLoss[0m : 10.55019
[1mStep[0m  [25/53], [94mLoss[0m : 10.59571
[1mStep[0m  [30/53], [94mLoss[0m : 10.75477
[1mStep[0m  [35/53], [94mLoss[0m : 10.78946
[1mStep[0m  [40/53], [94mLoss[0m : 10.25519
[1mStep[0m  [45/53], [94mLoss[0m : 11.10423
[1mStep[0m  [50/53], [94mLoss[0m : 10.89963

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.693, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.57950
[1mStep[0m  [5/53], [94mLoss[0m : 10.94103
[1mStep[0m  [10/53], [94mLoss[0m : 10.71277
[1mStep[0m  [15/53], [94mLoss[0m : 10.64320
[1mStep[0m  [20/53], [94mLoss[0m : 11.03363
[1mStep[0m  [25/53], [94mLoss[0m : 10.93946
[1mStep[0m  [30/53], [94mLoss[0m : 10.62171
[1mStep[0m  [35/53], [94mLoss[0m : 10.82320
[1mStep[0m  [40/53], [94mLoss[0m : 10.64313
[1mStep[0m  [45/53], [94mLoss[0m : 10.57350
[1mStep[0m  [50/53], [94mLoss[0m : 10.64000

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.722, [92mTest[0m: 10.683, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.80767
[1mStep[0m  [5/53], [94mLoss[0m : 11.04107
[1mStep[0m  [10/53], [94mLoss[0m : 10.28708
[1mStep[0m  [15/53], [94mLoss[0m : 10.79055
[1mStep[0m  [20/53], [94mLoss[0m : 10.81568
[1mStep[0m  [25/53], [94mLoss[0m : 10.64521
[1mStep[0m  [30/53], [94mLoss[0m : 10.76651
[1mStep[0m  [35/53], [94mLoss[0m : 10.83432
[1mStep[0m  [40/53], [94mLoss[0m : 10.71194
[1mStep[0m  [45/53], [94mLoss[0m : 10.58931
[1mStep[0m  [50/53], [94mLoss[0m : 10.60459

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.676, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.63910
[1mStep[0m  [5/53], [94mLoss[0m : 10.37100
[1mStep[0m  [10/53], [94mLoss[0m : 10.58733
[1mStep[0m  [15/53], [94mLoss[0m : 10.47660
[1mStep[0m  [20/53], [94mLoss[0m : 11.26221
[1mStep[0m  [25/53], [94mLoss[0m : 10.56670
[1mStep[0m  [30/53], [94mLoss[0m : 10.50814
[1mStep[0m  [35/53], [94mLoss[0m : 10.63040
[1mStep[0m  [40/53], [94mLoss[0m : 10.61600
[1mStep[0m  [45/53], [94mLoss[0m : 11.01132
[1mStep[0m  [50/53], [94mLoss[0m : 10.51190

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.706, [92mTest[0m: 10.655, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.95286
[1mStep[0m  [5/53], [94mLoss[0m : 10.64614
[1mStep[0m  [10/53], [94mLoss[0m : 10.79578
[1mStep[0m  [15/53], [94mLoss[0m : 10.85570
[1mStep[0m  [20/53], [94mLoss[0m : 10.76306
[1mStep[0m  [25/53], [94mLoss[0m : 10.78618
[1mStep[0m  [30/53], [94mLoss[0m : 10.63039
[1mStep[0m  [35/53], [94mLoss[0m : 10.65158
[1mStep[0m  [40/53], [94mLoss[0m : 10.79886
[1mStep[0m  [45/53], [94mLoss[0m : 10.69171
[1mStep[0m  [50/53], [94mLoss[0m : 10.90474

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.699, [92mTest[0m: 10.651, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.40860
[1mStep[0m  [5/53], [94mLoss[0m : 10.47316
[1mStep[0m  [10/53], [94mLoss[0m : 10.53089
[1mStep[0m  [15/53], [94mLoss[0m : 10.55258
[1mStep[0m  [20/53], [94mLoss[0m : 11.02282
[1mStep[0m  [25/53], [94mLoss[0m : 10.49837
[1mStep[0m  [30/53], [94mLoss[0m : 10.76452
[1mStep[0m  [35/53], [94mLoss[0m : 10.54318
[1mStep[0m  [40/53], [94mLoss[0m : 10.56418
[1mStep[0m  [45/53], [94mLoss[0m : 10.57608
[1mStep[0m  [50/53], [94mLoss[0m : 10.57602

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.694, [92mTest[0m: 10.648, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.78561
[1mStep[0m  [5/53], [94mLoss[0m : 10.09622
[1mStep[0m  [10/53], [94mLoss[0m : 10.69636
[1mStep[0m  [15/53], [94mLoss[0m : 10.84824
[1mStep[0m  [20/53], [94mLoss[0m : 10.42821
[1mStep[0m  [25/53], [94mLoss[0m : 10.64165
[1mStep[0m  [30/53], [94mLoss[0m : 10.74284
[1mStep[0m  [35/53], [94mLoss[0m : 10.61105
[1mStep[0m  [40/53], [94mLoss[0m : 11.15872
[1mStep[0m  [45/53], [94mLoss[0m : 10.42690
[1mStep[0m  [50/53], [94mLoss[0m : 10.75904

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.684, [92mTest[0m: 10.630, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.54904
[1mStep[0m  [5/53], [94mLoss[0m : 10.54587
[1mStep[0m  [10/53], [94mLoss[0m : 11.15915
[1mStep[0m  [15/53], [94mLoss[0m : 10.54075
[1mStep[0m  [20/53], [94mLoss[0m : 11.09388
[1mStep[0m  [25/53], [94mLoss[0m : 10.61212
[1mStep[0m  [30/53], [94mLoss[0m : 10.25747
[1mStep[0m  [35/53], [94mLoss[0m : 10.43256
[1mStep[0m  [40/53], [94mLoss[0m : 10.99109
[1mStep[0m  [45/53], [94mLoss[0m : 10.57442
[1mStep[0m  [50/53], [94mLoss[0m : 10.44568

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.681, [92mTest[0m: 10.627, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.624
====================================

Phase 1 - Evaluation MAE:  10.623603784121
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.13393
[1mStep[0m  [5/53], [94mLoss[0m : 10.71774
[1mStep[0m  [10/53], [94mLoss[0m : 10.84977
[1mStep[0m  [15/53], [94mLoss[0m : 11.04170
[1mStep[0m  [20/53], [94mLoss[0m : 10.63012
[1mStep[0m  [25/53], [94mLoss[0m : 10.63278
[1mStep[0m  [30/53], [94mLoss[0m : 10.73219
[1mStep[0m  [35/53], [94mLoss[0m : 10.39221
[1mStep[0m  [40/53], [94mLoss[0m : 10.76583
[1mStep[0m  [45/53], [94mLoss[0m : 10.83616
[1mStep[0m  [50/53], [94mLoss[0m : 10.71927

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.671, [92mTest[0m: 10.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.37846
[1mStep[0m  [5/53], [94mLoss[0m : 10.86520
[1mStep[0m  [10/53], [94mLoss[0m : 10.68053
[1mStep[0m  [15/53], [94mLoss[0m : 10.89165
[1mStep[0m  [20/53], [94mLoss[0m : 10.60033
[1mStep[0m  [25/53], [94mLoss[0m : 10.36562
[1mStep[0m  [30/53], [94mLoss[0m : 10.97921
[1mStep[0m  [35/53], [94mLoss[0m : 10.65552
[1mStep[0m  [40/53], [94mLoss[0m : 10.46175
[1mStep[0m  [45/53], [94mLoss[0m : 10.50091
[1mStep[0m  [50/53], [94mLoss[0m : 11.00767

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.659, [92mTest[0m: 10.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.78977
[1mStep[0m  [5/53], [94mLoss[0m : 10.85154
[1mStep[0m  [10/53], [94mLoss[0m : 10.30492
[1mStep[0m  [15/53], [94mLoss[0m : 10.58973
[1mStep[0m  [20/53], [94mLoss[0m : 10.75480
[1mStep[0m  [25/53], [94mLoss[0m : 10.49871
[1mStep[0m  [30/53], [94mLoss[0m : 10.63950
[1mStep[0m  [35/53], [94mLoss[0m : 10.30935
[1mStep[0m  [40/53], [94mLoss[0m : 10.14657
[1mStep[0m  [45/53], [94mLoss[0m : 10.66604
[1mStep[0m  [50/53], [94mLoss[0m : 10.77499

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.653, [92mTest[0m: 10.595, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.54560
[1mStep[0m  [5/53], [94mLoss[0m : 10.65962
[1mStep[0m  [10/53], [94mLoss[0m : 10.66195
[1mStep[0m  [15/53], [94mLoss[0m : 10.77272
[1mStep[0m  [20/53], [94mLoss[0m : 10.59080
[1mStep[0m  [25/53], [94mLoss[0m : 11.31873
[1mStep[0m  [30/53], [94mLoss[0m : 10.69392
[1mStep[0m  [35/53], [94mLoss[0m : 10.49162
[1mStep[0m  [40/53], [94mLoss[0m : 10.64932
[1mStep[0m  [45/53], [94mLoss[0m : 10.88917
[1mStep[0m  [50/53], [94mLoss[0m : 10.51673

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.648, [92mTest[0m: 10.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.92497
[1mStep[0m  [5/53], [94mLoss[0m : 10.61887
[1mStep[0m  [10/53], [94mLoss[0m : 10.56505
[1mStep[0m  [15/53], [94mLoss[0m : 10.41948
[1mStep[0m  [20/53], [94mLoss[0m : 10.74724
[1mStep[0m  [25/53], [94mLoss[0m : 10.94044
[1mStep[0m  [30/53], [94mLoss[0m : 10.79511
[1mStep[0m  [35/53], [94mLoss[0m : 10.61110
[1mStep[0m  [40/53], [94mLoss[0m : 10.20011
[1mStep[0m  [45/53], [94mLoss[0m : 10.96286
[1mStep[0m  [50/53], [94mLoss[0m : 10.49269

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.43933
[1mStep[0m  [5/53], [94mLoss[0m : 10.50677
[1mStep[0m  [10/53], [94mLoss[0m : 10.81721
[1mStep[0m  [15/53], [94mLoss[0m : 10.18444
[1mStep[0m  [20/53], [94mLoss[0m : 10.65782
[1mStep[0m  [25/53], [94mLoss[0m : 10.78221
[1mStep[0m  [30/53], [94mLoss[0m : 10.76811
[1mStep[0m  [35/53], [94mLoss[0m : 10.26246
[1mStep[0m  [40/53], [94mLoss[0m : 10.87186
[1mStep[0m  [45/53], [94mLoss[0m : 10.67293
[1mStep[0m  [50/53], [94mLoss[0m : 10.91497

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.625, [92mTest[0m: 10.544, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.66919
[1mStep[0m  [5/53], [94mLoss[0m : 10.84613
[1mStep[0m  [10/53], [94mLoss[0m : 10.78114
[1mStep[0m  [15/53], [94mLoss[0m : 10.59970
[1mStep[0m  [20/53], [94mLoss[0m : 10.51609
[1mStep[0m  [25/53], [94mLoss[0m : 10.38741
[1mStep[0m  [30/53], [94mLoss[0m : 10.72847
[1mStep[0m  [35/53], [94mLoss[0m : 10.63089
[1mStep[0m  [40/53], [94mLoss[0m : 10.97501
[1mStep[0m  [45/53], [94mLoss[0m : 10.50856
[1mStep[0m  [50/53], [94mLoss[0m : 10.59405

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.623, [92mTest[0m: 10.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.71217
[1mStep[0m  [5/53], [94mLoss[0m : 10.65901
[1mStep[0m  [10/53], [94mLoss[0m : 10.58352
[1mStep[0m  [15/53], [94mLoss[0m : 10.63776
[1mStep[0m  [20/53], [94mLoss[0m : 10.29798
[1mStep[0m  [25/53], [94mLoss[0m : 10.77475
[1mStep[0m  [30/53], [94mLoss[0m : 10.39435
[1mStep[0m  [35/53], [94mLoss[0m : 11.01145
[1mStep[0m  [40/53], [94mLoss[0m : 10.65059
[1mStep[0m  [45/53], [94mLoss[0m : 10.57015
[1mStep[0m  [50/53], [94mLoss[0m : 10.75723

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.615, [92mTest[0m: 10.539, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.56077
[1mStep[0m  [5/53], [94mLoss[0m : 10.73976
[1mStep[0m  [10/53], [94mLoss[0m : 10.32508
[1mStep[0m  [15/53], [94mLoss[0m : 10.83041
[1mStep[0m  [20/53], [94mLoss[0m : 10.60056
[1mStep[0m  [25/53], [94mLoss[0m : 10.46792
[1mStep[0m  [30/53], [94mLoss[0m : 10.32801
[1mStep[0m  [35/53], [94mLoss[0m : 10.71185
[1mStep[0m  [40/53], [94mLoss[0m : 10.34385
[1mStep[0m  [45/53], [94mLoss[0m : 10.88438
[1mStep[0m  [50/53], [94mLoss[0m : 10.58559

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.597, [92mTest[0m: 10.517, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.75964
[1mStep[0m  [5/53], [94mLoss[0m : 10.37696
[1mStep[0m  [10/53], [94mLoss[0m : 10.40453
[1mStep[0m  [15/53], [94mLoss[0m : 10.52309
[1mStep[0m  [20/53], [94mLoss[0m : 10.64554
[1mStep[0m  [25/53], [94mLoss[0m : 10.37915
[1mStep[0m  [30/53], [94mLoss[0m : 10.80211
[1mStep[0m  [35/53], [94mLoss[0m : 10.63818
[1mStep[0m  [40/53], [94mLoss[0m : 10.67239
[1mStep[0m  [45/53], [94mLoss[0m : 10.73199
[1mStep[0m  [50/53], [94mLoss[0m : 10.99717

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.595, [92mTest[0m: 10.520, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.84377
[1mStep[0m  [5/53], [94mLoss[0m : 10.68693
[1mStep[0m  [10/53], [94mLoss[0m : 10.46154
[1mStep[0m  [15/53], [94mLoss[0m : 11.12692
[1mStep[0m  [20/53], [94mLoss[0m : 10.63808
[1mStep[0m  [25/53], [94mLoss[0m : 10.30749
[1mStep[0m  [30/53], [94mLoss[0m : 10.54742
[1mStep[0m  [35/53], [94mLoss[0m : 10.48053
[1mStep[0m  [40/53], [94mLoss[0m : 10.51989
[1mStep[0m  [45/53], [94mLoss[0m : 10.51184
[1mStep[0m  [50/53], [94mLoss[0m : 10.55361

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.586, [92mTest[0m: 10.521, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.64938
[1mStep[0m  [5/53], [94mLoss[0m : 11.04065
[1mStep[0m  [10/53], [94mLoss[0m : 10.45953
[1mStep[0m  [15/53], [94mLoss[0m : 10.35422
[1mStep[0m  [20/53], [94mLoss[0m : 10.81386
[1mStep[0m  [25/53], [94mLoss[0m : 10.45238
[1mStep[0m  [30/53], [94mLoss[0m : 10.41654
[1mStep[0m  [35/53], [94mLoss[0m : 10.70264
[1mStep[0m  [40/53], [94mLoss[0m : 10.57306
[1mStep[0m  [45/53], [94mLoss[0m : 10.58773
[1mStep[0m  [50/53], [94mLoss[0m : 10.59985

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.581, [92mTest[0m: 10.488, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.15737
[1mStep[0m  [5/53], [94mLoss[0m : 10.68231
[1mStep[0m  [10/53], [94mLoss[0m : 10.56281
[1mStep[0m  [15/53], [94mLoss[0m : 10.35813
[1mStep[0m  [20/53], [94mLoss[0m : 10.38438
[1mStep[0m  [25/53], [94mLoss[0m : 10.54894
[1mStep[0m  [30/53], [94mLoss[0m : 10.76300
[1mStep[0m  [35/53], [94mLoss[0m : 10.07079
[1mStep[0m  [40/53], [94mLoss[0m : 10.23853
[1mStep[0m  [45/53], [94mLoss[0m : 10.73484
[1mStep[0m  [50/53], [94mLoss[0m : 10.49973

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.566, [92mTest[0m: 10.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.56079
[1mStep[0m  [5/53], [94mLoss[0m : 10.62865
[1mStep[0m  [10/53], [94mLoss[0m : 10.42590
[1mStep[0m  [15/53], [94mLoss[0m : 10.52986
[1mStep[0m  [20/53], [94mLoss[0m : 10.56806
[1mStep[0m  [25/53], [94mLoss[0m : 10.66456
[1mStep[0m  [30/53], [94mLoss[0m : 10.74316
[1mStep[0m  [35/53], [94mLoss[0m : 10.66911
[1mStep[0m  [40/53], [94mLoss[0m : 10.84624
[1mStep[0m  [45/53], [94mLoss[0m : 10.62677
[1mStep[0m  [50/53], [94mLoss[0m : 10.56663

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.554, [92mTest[0m: 10.463, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.48170
[1mStep[0m  [5/53], [94mLoss[0m : 10.29070
[1mStep[0m  [10/53], [94mLoss[0m : 10.41694
[1mStep[0m  [15/53], [94mLoss[0m : 10.57173
[1mStep[0m  [20/53], [94mLoss[0m : 10.27233
[1mStep[0m  [25/53], [94mLoss[0m : 10.59081
[1mStep[0m  [30/53], [94mLoss[0m : 10.13972
[1mStep[0m  [35/53], [94mLoss[0m : 10.61363
[1mStep[0m  [40/53], [94mLoss[0m : 10.20101
[1mStep[0m  [45/53], [94mLoss[0m : 10.71730
[1mStep[0m  [50/53], [94mLoss[0m : 11.13989

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.555, [92mTest[0m: 10.459, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.52767
[1mStep[0m  [5/53], [94mLoss[0m : 10.15204
[1mStep[0m  [10/53], [94mLoss[0m : 10.68255
[1mStep[0m  [15/53], [94mLoss[0m : 10.48485
[1mStep[0m  [20/53], [94mLoss[0m : 10.62670
[1mStep[0m  [25/53], [94mLoss[0m : 10.70462
[1mStep[0m  [30/53], [94mLoss[0m : 10.70548
[1mStep[0m  [35/53], [94mLoss[0m : 10.39486
[1mStep[0m  [40/53], [94mLoss[0m : 10.66372
[1mStep[0m  [45/53], [94mLoss[0m : 10.52192
[1mStep[0m  [50/53], [94mLoss[0m : 10.43505

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.539, [92mTest[0m: 10.462, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.75250
[1mStep[0m  [5/53], [94mLoss[0m : 10.34464
[1mStep[0m  [10/53], [94mLoss[0m : 10.54029
[1mStep[0m  [15/53], [94mLoss[0m : 10.47913
[1mStep[0m  [20/53], [94mLoss[0m : 10.57982
[1mStep[0m  [25/53], [94mLoss[0m : 10.69005
[1mStep[0m  [30/53], [94mLoss[0m : 10.42372
[1mStep[0m  [35/53], [94mLoss[0m : 10.61726
[1mStep[0m  [40/53], [94mLoss[0m : 10.62638
[1mStep[0m  [45/53], [94mLoss[0m : 10.38856
[1mStep[0m  [50/53], [94mLoss[0m : 10.28075

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.532, [92mTest[0m: 10.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.34493
[1mStep[0m  [5/53], [94mLoss[0m : 10.73895
[1mStep[0m  [10/53], [94mLoss[0m : 10.56848
[1mStep[0m  [15/53], [94mLoss[0m : 10.29005
[1mStep[0m  [20/53], [94mLoss[0m : 10.89379
[1mStep[0m  [25/53], [94mLoss[0m : 10.18940
[1mStep[0m  [30/53], [94mLoss[0m : 10.84916
[1mStep[0m  [35/53], [94mLoss[0m : 10.45566
[1mStep[0m  [40/53], [94mLoss[0m : 10.60406
[1mStep[0m  [45/53], [94mLoss[0m : 10.58172
[1mStep[0m  [50/53], [94mLoss[0m : 10.47121

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.528, [92mTest[0m: 10.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.57843
[1mStep[0m  [5/53], [94mLoss[0m : 10.35896
[1mStep[0m  [10/53], [94mLoss[0m : 10.77556
[1mStep[0m  [15/53], [94mLoss[0m : 10.39719
[1mStep[0m  [20/53], [94mLoss[0m : 10.55679
[1mStep[0m  [25/53], [94mLoss[0m : 10.30590
[1mStep[0m  [30/53], [94mLoss[0m : 10.34870
[1mStep[0m  [35/53], [94mLoss[0m : 10.40879
[1mStep[0m  [40/53], [94mLoss[0m : 10.59177
[1mStep[0m  [45/53], [94mLoss[0m : 10.07700
[1mStep[0m  [50/53], [94mLoss[0m : 10.55442

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.521, [92mTest[0m: 10.408, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.34126
[1mStep[0m  [5/53], [94mLoss[0m : 10.36873
[1mStep[0m  [10/53], [94mLoss[0m : 10.50602
[1mStep[0m  [15/53], [94mLoss[0m : 10.43324
[1mStep[0m  [20/53], [94mLoss[0m : 10.73374
[1mStep[0m  [25/53], [94mLoss[0m : 10.23528
[1mStep[0m  [30/53], [94mLoss[0m : 10.39564
[1mStep[0m  [35/53], [94mLoss[0m : 10.27744
[1mStep[0m  [40/53], [94mLoss[0m : 10.66399
[1mStep[0m  [45/53], [94mLoss[0m : 10.47130
[1mStep[0m  [50/53], [94mLoss[0m : 10.65992

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.507, [92mTest[0m: 10.403, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.48589
[1mStep[0m  [5/53], [94mLoss[0m : 10.58727
[1mStep[0m  [10/53], [94mLoss[0m : 10.68313
[1mStep[0m  [15/53], [94mLoss[0m : 10.19161
[1mStep[0m  [20/53], [94mLoss[0m : 10.42490
[1mStep[0m  [25/53], [94mLoss[0m : 10.57789
[1mStep[0m  [30/53], [94mLoss[0m : 10.18592
[1mStep[0m  [35/53], [94mLoss[0m : 10.60302
[1mStep[0m  [40/53], [94mLoss[0m : 10.29798
[1mStep[0m  [45/53], [94mLoss[0m : 10.61938
[1mStep[0m  [50/53], [94mLoss[0m : 10.53626

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.498, [92mTest[0m: 10.404, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.77151
[1mStep[0m  [5/53], [94mLoss[0m : 10.37145
[1mStep[0m  [10/53], [94mLoss[0m : 10.11123
[1mStep[0m  [15/53], [94mLoss[0m : 10.59459
[1mStep[0m  [20/53], [94mLoss[0m : 10.16264
[1mStep[0m  [25/53], [94mLoss[0m : 10.57105
[1mStep[0m  [30/53], [94mLoss[0m : 10.42991
[1mStep[0m  [35/53], [94mLoss[0m : 10.77933
[1mStep[0m  [40/53], [94mLoss[0m : 10.51248
[1mStep[0m  [45/53], [94mLoss[0m : 10.59804
[1mStep[0m  [50/53], [94mLoss[0m : 10.38365

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.493, [92mTest[0m: 10.400, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.76571
[1mStep[0m  [5/53], [94mLoss[0m : 10.68388
[1mStep[0m  [10/53], [94mLoss[0m : 10.33798
[1mStep[0m  [15/53], [94mLoss[0m : 10.89308
[1mStep[0m  [20/53], [94mLoss[0m : 10.12247
[1mStep[0m  [25/53], [94mLoss[0m : 10.58934
[1mStep[0m  [30/53], [94mLoss[0m : 10.58142
[1mStep[0m  [35/53], [94mLoss[0m : 10.34669
[1mStep[0m  [40/53], [94mLoss[0m : 10.52549
[1mStep[0m  [45/53], [94mLoss[0m : 11.00518
[1mStep[0m  [50/53], [94mLoss[0m : 10.03904

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.487, [92mTest[0m: 10.371, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.73799
[1mStep[0m  [5/53], [94mLoss[0m : 10.28529
[1mStep[0m  [10/53], [94mLoss[0m : 10.35476
[1mStep[0m  [15/53], [94mLoss[0m : 10.37732
[1mStep[0m  [20/53], [94mLoss[0m : 10.52446
[1mStep[0m  [25/53], [94mLoss[0m : 10.31288
[1mStep[0m  [30/53], [94mLoss[0m : 11.07649
[1mStep[0m  [35/53], [94mLoss[0m : 10.56059
[1mStep[0m  [40/53], [94mLoss[0m : 10.54334
[1mStep[0m  [45/53], [94mLoss[0m : 10.26238
[1mStep[0m  [50/53], [94mLoss[0m : 10.50330

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.481, [92mTest[0m: 10.365, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.52826
[1mStep[0m  [5/53], [94mLoss[0m : 10.53045
[1mStep[0m  [10/53], [94mLoss[0m : 10.86848
[1mStep[0m  [15/53], [94mLoss[0m : 10.36802
[1mStep[0m  [20/53], [94mLoss[0m : 10.49854
[1mStep[0m  [25/53], [94mLoss[0m : 10.37127
[1mStep[0m  [30/53], [94mLoss[0m : 10.60188
[1mStep[0m  [35/53], [94mLoss[0m : 10.33737
[1mStep[0m  [40/53], [94mLoss[0m : 10.42255
[1mStep[0m  [45/53], [94mLoss[0m : 10.22116
[1mStep[0m  [50/53], [94mLoss[0m : 10.55343

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.468, [92mTest[0m: 10.350, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.45844
[1mStep[0m  [5/53], [94mLoss[0m : 10.51998
[1mStep[0m  [10/53], [94mLoss[0m : 10.60232
[1mStep[0m  [15/53], [94mLoss[0m : 10.47298
[1mStep[0m  [20/53], [94mLoss[0m : 10.52737
[1mStep[0m  [25/53], [94mLoss[0m : 10.68887
[1mStep[0m  [30/53], [94mLoss[0m : 10.32338
[1mStep[0m  [35/53], [94mLoss[0m : 10.66147
[1mStep[0m  [40/53], [94mLoss[0m : 10.27226
[1mStep[0m  [45/53], [94mLoss[0m : 10.37395
[1mStep[0m  [50/53], [94mLoss[0m : 10.27500

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.465, [92mTest[0m: 10.353, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.33519
[1mStep[0m  [5/53], [94mLoss[0m : 10.24644
[1mStep[0m  [10/53], [94mLoss[0m : 10.50151
[1mStep[0m  [15/53], [94mLoss[0m : 10.58512
[1mStep[0m  [20/53], [94mLoss[0m : 10.34723
[1mStep[0m  [25/53], [94mLoss[0m : 10.18122
[1mStep[0m  [30/53], [94mLoss[0m : 10.62234
[1mStep[0m  [35/53], [94mLoss[0m : 10.65487
[1mStep[0m  [40/53], [94mLoss[0m : 10.41086
[1mStep[0m  [45/53], [94mLoss[0m : 10.40493
[1mStep[0m  [50/53], [94mLoss[0m : 10.94160

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.458, [92mTest[0m: 10.339, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.07156
[1mStep[0m  [5/53], [94mLoss[0m : 10.71956
[1mStep[0m  [10/53], [94mLoss[0m : 10.46905
[1mStep[0m  [15/53], [94mLoss[0m : 10.47632
[1mStep[0m  [20/53], [94mLoss[0m : 10.30015
[1mStep[0m  [25/53], [94mLoss[0m : 10.75824
[1mStep[0m  [30/53], [94mLoss[0m : 10.68597
[1mStep[0m  [35/53], [94mLoss[0m : 10.32058
[1mStep[0m  [40/53], [94mLoss[0m : 10.52402
[1mStep[0m  [45/53], [94mLoss[0m : 10.42962
[1mStep[0m  [50/53], [94mLoss[0m : 10.57789

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.447, [92mTest[0m: 10.351, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.76044
[1mStep[0m  [5/53], [94mLoss[0m : 10.45604
[1mStep[0m  [10/53], [94mLoss[0m : 10.61787
[1mStep[0m  [15/53], [94mLoss[0m : 10.40288
[1mStep[0m  [20/53], [94mLoss[0m : 10.65647
[1mStep[0m  [25/53], [94mLoss[0m : 10.16224
[1mStep[0m  [30/53], [94mLoss[0m : 10.27443
[1mStep[0m  [35/53], [94mLoss[0m : 10.21479
[1mStep[0m  [40/53], [94mLoss[0m : 10.41793
[1mStep[0m  [45/53], [94mLoss[0m : 10.65172
[1mStep[0m  [50/53], [94mLoss[0m : 10.49126

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.436, [92mTest[0m: 10.344, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.32824
[1mStep[0m  [5/53], [94mLoss[0m : 10.16511
[1mStep[0m  [10/53], [94mLoss[0m : 10.27919
[1mStep[0m  [15/53], [94mLoss[0m : 10.43315
[1mStep[0m  [20/53], [94mLoss[0m : 10.58969
[1mStep[0m  [25/53], [94mLoss[0m : 10.64969
[1mStep[0m  [30/53], [94mLoss[0m : 10.14556
[1mStep[0m  [35/53], [94mLoss[0m : 10.29750
[1mStep[0m  [40/53], [94mLoss[0m : 10.69735
[1mStep[0m  [45/53], [94mLoss[0m : 10.84090
[1mStep[0m  [50/53], [94mLoss[0m : 10.63667

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.433, [92mTest[0m: 10.340, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.323
====================================

Phase 2 - Evaluation MAE:  10.322991444514347
MAE score P1       10.623604
MAE score P2       10.322991
loss               10.433352
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay            0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.70576
[1mStep[0m  [2/26], [94mLoss[0m : 10.81539
[1mStep[0m  [4/26], [94mLoss[0m : 10.61624
[1mStep[0m  [6/26], [94mLoss[0m : 10.78748
[1mStep[0m  [8/26], [94mLoss[0m : 11.00075
[1mStep[0m  [10/26], [94mLoss[0m : 10.56890
[1mStep[0m  [12/26], [94mLoss[0m : 11.09790
[1mStep[0m  [14/26], [94mLoss[0m : 10.69759
[1mStep[0m  [16/26], [94mLoss[0m : 10.95264
[1mStep[0m  [18/26], [94mLoss[0m : 10.87266
[1mStep[0m  [20/26], [94mLoss[0m : 10.42075
[1mStep[0m  [22/26], [94mLoss[0m : 10.64644
[1mStep[0m  [24/26], [94mLoss[0m : 10.68849

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.764, [92mTest[0m: 10.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.00841
[1mStep[0m  [2/26], [94mLoss[0m : 10.39765
[1mStep[0m  [4/26], [94mLoss[0m : 10.80176
[1mStep[0m  [6/26], [94mLoss[0m : 10.64848
[1mStep[0m  [8/26], [94mLoss[0m : 10.39524
[1mStep[0m  [10/26], [94mLoss[0m : 10.41504
[1mStep[0m  [12/26], [94mLoss[0m : 10.72891
[1mStep[0m  [14/26], [94mLoss[0m : 10.59650
[1mStep[0m  [16/26], [94mLoss[0m : 10.45986
[1mStep[0m  [18/26], [94mLoss[0m : 10.79110
[1mStep[0m  [20/26], [94mLoss[0m : 10.55287
[1mStep[0m  [22/26], [94mLoss[0m : 10.83885
[1mStep[0m  [24/26], [94mLoss[0m : 10.63970

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.630, [92mTest[0m: 10.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.55794
[1mStep[0m  [2/26], [94mLoss[0m : 10.60591
[1mStep[0m  [4/26], [94mLoss[0m : 10.51226
[1mStep[0m  [6/26], [94mLoss[0m : 10.49290
[1mStep[0m  [8/26], [94mLoss[0m : 10.89296
[1mStep[0m  [10/26], [94mLoss[0m : 10.23128
[1mStep[0m  [12/26], [94mLoss[0m : 10.49585
[1mStep[0m  [14/26], [94mLoss[0m : 10.37873
[1mStep[0m  [16/26], [94mLoss[0m : 10.67063
[1mStep[0m  [18/26], [94mLoss[0m : 10.39705
[1mStep[0m  [20/26], [94mLoss[0m : 10.33350
[1mStep[0m  [22/26], [94mLoss[0m : 10.53148
[1mStep[0m  [24/26], [94mLoss[0m : 10.62166

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.487, [92mTest[0m: 10.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.47309
[1mStep[0m  [2/26], [94mLoss[0m : 10.54837
[1mStep[0m  [4/26], [94mLoss[0m : 10.49163
[1mStep[0m  [6/26], [94mLoss[0m : 10.20204
[1mStep[0m  [8/26], [94mLoss[0m : 10.36379
[1mStep[0m  [10/26], [94mLoss[0m : 10.43366
[1mStep[0m  [12/26], [94mLoss[0m : 10.66741
[1mStep[0m  [14/26], [94mLoss[0m : 10.26611
[1mStep[0m  [16/26], [94mLoss[0m : 10.38865
[1mStep[0m  [18/26], [94mLoss[0m : 10.40039
[1mStep[0m  [20/26], [94mLoss[0m : 10.29159
[1mStep[0m  [22/26], [94mLoss[0m : 10.34799
[1mStep[0m  [24/26], [94mLoss[0m : 10.31709

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.351, [92mTest[0m: 10.411, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.27394
[1mStep[0m  [2/26], [94mLoss[0m : 10.25163
[1mStep[0m  [4/26], [94mLoss[0m : 10.32922
[1mStep[0m  [6/26], [94mLoss[0m : 9.92155
[1mStep[0m  [8/26], [94mLoss[0m : 10.50535
[1mStep[0m  [10/26], [94mLoss[0m : 10.23368
[1mStep[0m  [12/26], [94mLoss[0m : 10.23517
[1mStep[0m  [14/26], [94mLoss[0m : 10.23289
[1mStep[0m  [16/26], [94mLoss[0m : 10.18620
[1mStep[0m  [18/26], [94mLoss[0m : 10.22584
[1mStep[0m  [20/26], [94mLoss[0m : 10.17846
[1mStep[0m  [22/26], [94mLoss[0m : 10.23645
[1mStep[0m  [24/26], [94mLoss[0m : 10.28513

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.215, [92mTest[0m: 10.277, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.00314
[1mStep[0m  [2/26], [94mLoss[0m : 10.18092
[1mStep[0m  [4/26], [94mLoss[0m : 10.07878
[1mStep[0m  [6/26], [94mLoss[0m : 10.00946
[1mStep[0m  [8/26], [94mLoss[0m : 9.85145
[1mStep[0m  [10/26], [94mLoss[0m : 9.99272
[1mStep[0m  [12/26], [94mLoss[0m : 10.03105
[1mStep[0m  [14/26], [94mLoss[0m : 10.48580
[1mStep[0m  [16/26], [94mLoss[0m : 10.08359
[1mStep[0m  [18/26], [94mLoss[0m : 10.03443
[1mStep[0m  [20/26], [94mLoss[0m : 10.04445
[1mStep[0m  [22/26], [94mLoss[0m : 10.06120
[1mStep[0m  [24/26], [94mLoss[0m : 10.01558

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.082, [92mTest[0m: 10.155, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.07820
[1mStep[0m  [2/26], [94mLoss[0m : 10.07241
[1mStep[0m  [4/26], [94mLoss[0m : 10.10219
[1mStep[0m  [6/26], [94mLoss[0m : 10.22798
[1mStep[0m  [8/26], [94mLoss[0m : 9.67494
[1mStep[0m  [10/26], [94mLoss[0m : 9.86147
[1mStep[0m  [12/26], [94mLoss[0m : 9.70819
[1mStep[0m  [14/26], [94mLoss[0m : 9.90550
[1mStep[0m  [16/26], [94mLoss[0m : 10.19641
[1mStep[0m  [18/26], [94mLoss[0m : 10.05196
[1mStep[0m  [20/26], [94mLoss[0m : 9.77459
[1mStep[0m  [22/26], [94mLoss[0m : 9.82004
[1mStep[0m  [24/26], [94mLoss[0m : 9.85358

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.944, [92mTest[0m: 10.018, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.15505
[1mStep[0m  [2/26], [94mLoss[0m : 9.93703
[1mStep[0m  [4/26], [94mLoss[0m : 9.91018
[1mStep[0m  [6/26], [94mLoss[0m : 9.80881
[1mStep[0m  [8/26], [94mLoss[0m : 9.64555
[1mStep[0m  [10/26], [94mLoss[0m : 9.74015
[1mStep[0m  [12/26], [94mLoss[0m : 9.84015
[1mStep[0m  [14/26], [94mLoss[0m : 9.97438
[1mStep[0m  [16/26], [94mLoss[0m : 9.77005
[1mStep[0m  [18/26], [94mLoss[0m : 9.88442
[1mStep[0m  [20/26], [94mLoss[0m : 9.90728
[1mStep[0m  [22/26], [94mLoss[0m : 9.81040
[1mStep[0m  [24/26], [94mLoss[0m : 10.05587

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.812, [92mTest[0m: 9.877, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.72623
[1mStep[0m  [2/26], [94mLoss[0m : 9.73240
[1mStep[0m  [4/26], [94mLoss[0m : 9.93298
[1mStep[0m  [6/26], [94mLoss[0m : 9.64035
[1mStep[0m  [8/26], [94mLoss[0m : 9.62472
[1mStep[0m  [10/26], [94mLoss[0m : 9.53871
[1mStep[0m  [12/26], [94mLoss[0m : 9.83067
[1mStep[0m  [14/26], [94mLoss[0m : 9.58490
[1mStep[0m  [16/26], [94mLoss[0m : 9.58812
[1mStep[0m  [18/26], [94mLoss[0m : 9.67575
[1mStep[0m  [20/26], [94mLoss[0m : 9.83993
[1mStep[0m  [22/26], [94mLoss[0m : 9.61324
[1mStep[0m  [24/26], [94mLoss[0m : 9.63244

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.688, [92mTest[0m: 9.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.50591
[1mStep[0m  [2/26], [94mLoss[0m : 9.67769
[1mStep[0m  [4/26], [94mLoss[0m : 9.62201
[1mStep[0m  [6/26], [94mLoss[0m : 9.41946
[1mStep[0m  [8/26], [94mLoss[0m : 9.57159
[1mStep[0m  [10/26], [94mLoss[0m : 9.63013
[1mStep[0m  [12/26], [94mLoss[0m : 9.67868
[1mStep[0m  [14/26], [94mLoss[0m : 9.51864
[1mStep[0m  [16/26], [94mLoss[0m : 9.43900
[1mStep[0m  [18/26], [94mLoss[0m : 9.47014
[1mStep[0m  [20/26], [94mLoss[0m : 9.65122
[1mStep[0m  [22/26], [94mLoss[0m : 9.28281
[1mStep[0m  [24/26], [94mLoss[0m : 9.50035

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.537, [92mTest[0m: 9.584, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.38371
[1mStep[0m  [2/26], [94mLoss[0m : 9.22981
[1mStep[0m  [4/26], [94mLoss[0m : 9.60739
[1mStep[0m  [6/26], [94mLoss[0m : 9.17548
[1mStep[0m  [8/26], [94mLoss[0m : 9.39779
[1mStep[0m  [10/26], [94mLoss[0m : 9.33706
[1mStep[0m  [12/26], [94mLoss[0m : 9.34181
[1mStep[0m  [14/26], [94mLoss[0m : 9.53339
[1mStep[0m  [16/26], [94mLoss[0m : 9.30469
[1mStep[0m  [18/26], [94mLoss[0m : 9.47385
[1mStep[0m  [20/26], [94mLoss[0m : 9.70789
[1mStep[0m  [22/26], [94mLoss[0m : 9.23781
[1mStep[0m  [24/26], [94mLoss[0m : 9.21404

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.406, [92mTest[0m: 9.471, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.25139
[1mStep[0m  [2/26], [94mLoss[0m : 9.45463
[1mStep[0m  [4/26], [94mLoss[0m : 9.05323
[1mStep[0m  [6/26], [94mLoss[0m : 9.19414
[1mStep[0m  [8/26], [94mLoss[0m : 9.16471
[1mStep[0m  [10/26], [94mLoss[0m : 9.36331
[1mStep[0m  [12/26], [94mLoss[0m : 9.23243
[1mStep[0m  [14/26], [94mLoss[0m : 9.39224
[1mStep[0m  [16/26], [94mLoss[0m : 9.37617
[1mStep[0m  [18/26], [94mLoss[0m : 9.40622
[1mStep[0m  [20/26], [94mLoss[0m : 9.21135
[1mStep[0m  [22/26], [94mLoss[0m : 9.30941
[1mStep[0m  [24/26], [94mLoss[0m : 9.07780

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.276, [92mTest[0m: 9.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.40886
[1mStep[0m  [2/26], [94mLoss[0m : 9.30681
[1mStep[0m  [4/26], [94mLoss[0m : 9.10983
[1mStep[0m  [6/26], [94mLoss[0m : 9.16402
[1mStep[0m  [8/26], [94mLoss[0m : 9.20598
[1mStep[0m  [10/26], [94mLoss[0m : 9.21552
[1mStep[0m  [12/26], [94mLoss[0m : 9.19281
[1mStep[0m  [14/26], [94mLoss[0m : 9.09162
[1mStep[0m  [16/26], [94mLoss[0m : 8.98486
[1mStep[0m  [18/26], [94mLoss[0m : 8.97734
[1mStep[0m  [20/26], [94mLoss[0m : 9.02340
[1mStep[0m  [22/26], [94mLoss[0m : 9.21959
[1mStep[0m  [24/26], [94mLoss[0m : 9.00245

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.132, [92mTest[0m: 9.184, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.96054
[1mStep[0m  [2/26], [94mLoss[0m : 8.84186
[1mStep[0m  [4/26], [94mLoss[0m : 9.04499
[1mStep[0m  [6/26], [94mLoss[0m : 9.14053
[1mStep[0m  [8/26], [94mLoss[0m : 9.09706
[1mStep[0m  [10/26], [94mLoss[0m : 9.34552
[1mStep[0m  [12/26], [94mLoss[0m : 8.88615
[1mStep[0m  [14/26], [94mLoss[0m : 9.15722
[1mStep[0m  [16/26], [94mLoss[0m : 9.12505
[1mStep[0m  [18/26], [94mLoss[0m : 9.02352
[1mStep[0m  [20/26], [94mLoss[0m : 9.26048
[1mStep[0m  [22/26], [94mLoss[0m : 9.08287
[1mStep[0m  [24/26], [94mLoss[0m : 8.66377

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.005, [92mTest[0m: 9.058, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.10275
[1mStep[0m  [2/26], [94mLoss[0m : 8.76621
[1mStep[0m  [4/26], [94mLoss[0m : 8.92316
[1mStep[0m  [6/26], [94mLoss[0m : 8.83128
[1mStep[0m  [8/26], [94mLoss[0m : 8.79194
[1mStep[0m  [10/26], [94mLoss[0m : 8.72076
[1mStep[0m  [12/26], [94mLoss[0m : 8.94727
[1mStep[0m  [14/26], [94mLoss[0m : 8.87304
[1mStep[0m  [16/26], [94mLoss[0m : 8.86370
[1mStep[0m  [18/26], [94mLoss[0m : 8.84053
[1mStep[0m  [20/26], [94mLoss[0m : 8.76594
[1mStep[0m  [22/26], [94mLoss[0m : 8.53683
[1mStep[0m  [24/26], [94mLoss[0m : 8.98923

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.852, [92mTest[0m: 8.922, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.85311
[1mStep[0m  [2/26], [94mLoss[0m : 8.80828
[1mStep[0m  [4/26], [94mLoss[0m : 8.59568
[1mStep[0m  [6/26], [94mLoss[0m : 8.79392
[1mStep[0m  [8/26], [94mLoss[0m : 8.81002
[1mStep[0m  [10/26], [94mLoss[0m : 8.84364
[1mStep[0m  [12/26], [94mLoss[0m : 8.73310
[1mStep[0m  [14/26], [94mLoss[0m : 9.00668
[1mStep[0m  [16/26], [94mLoss[0m : 8.91017
[1mStep[0m  [18/26], [94mLoss[0m : 8.68150
[1mStep[0m  [20/26], [94mLoss[0m : 8.85676
[1mStep[0m  [22/26], [94mLoss[0m : 8.42640
[1mStep[0m  [24/26], [94mLoss[0m : 8.52642

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.724, [92mTest[0m: 8.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.56756
[1mStep[0m  [2/26], [94mLoss[0m : 8.45088
[1mStep[0m  [4/26], [94mLoss[0m : 8.82037
[1mStep[0m  [6/26], [94mLoss[0m : 8.48223
[1mStep[0m  [8/26], [94mLoss[0m : 8.68862
[1mStep[0m  [10/26], [94mLoss[0m : 8.65257
[1mStep[0m  [12/26], [94mLoss[0m : 8.59748
[1mStep[0m  [14/26], [94mLoss[0m : 8.60751
[1mStep[0m  [16/26], [94mLoss[0m : 8.65934
[1mStep[0m  [18/26], [94mLoss[0m : 8.46140
[1mStep[0m  [20/26], [94mLoss[0m : 8.44467
[1mStep[0m  [22/26], [94mLoss[0m : 8.41704
[1mStep[0m  [24/26], [94mLoss[0m : 8.80796

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.587, [92mTest[0m: 8.641, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.68303
[1mStep[0m  [2/26], [94mLoss[0m : 8.34227
[1mStep[0m  [4/26], [94mLoss[0m : 8.72877
[1mStep[0m  [6/26], [94mLoss[0m : 8.59160
[1mStep[0m  [8/26], [94mLoss[0m : 8.56639
[1mStep[0m  [10/26], [94mLoss[0m : 8.86881
[1mStep[0m  [12/26], [94mLoss[0m : 8.56082
[1mStep[0m  [14/26], [94mLoss[0m : 8.52361
[1mStep[0m  [16/26], [94mLoss[0m : 8.37182
[1mStep[0m  [18/26], [94mLoss[0m : 8.30301
[1mStep[0m  [20/26], [94mLoss[0m : 8.25007
[1mStep[0m  [22/26], [94mLoss[0m : 8.44860
[1mStep[0m  [24/26], [94mLoss[0m : 8.32314

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.463, [92mTest[0m: 8.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.32323
[1mStep[0m  [2/26], [94mLoss[0m : 8.43508
[1mStep[0m  [4/26], [94mLoss[0m : 8.40932
[1mStep[0m  [6/26], [94mLoss[0m : 8.26036
[1mStep[0m  [8/26], [94mLoss[0m : 8.07689
[1mStep[0m  [10/26], [94mLoss[0m : 8.53060
[1mStep[0m  [12/26], [94mLoss[0m : 8.07235
[1mStep[0m  [14/26], [94mLoss[0m : 8.13480
[1mStep[0m  [16/26], [94mLoss[0m : 8.29745
[1mStep[0m  [18/26], [94mLoss[0m : 8.42361
[1mStep[0m  [20/26], [94mLoss[0m : 8.43273
[1mStep[0m  [22/26], [94mLoss[0m : 8.46436
[1mStep[0m  [24/26], [94mLoss[0m : 8.21369

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.323, [92mTest[0m: 8.371, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.38375
[1mStep[0m  [2/26], [94mLoss[0m : 8.32186
[1mStep[0m  [4/26], [94mLoss[0m : 8.10107
[1mStep[0m  [6/26], [94mLoss[0m : 8.27518
[1mStep[0m  [8/26], [94mLoss[0m : 8.24858
[1mStep[0m  [10/26], [94mLoss[0m : 8.34958
[1mStep[0m  [12/26], [94mLoss[0m : 8.13364
[1mStep[0m  [14/26], [94mLoss[0m : 8.18842
[1mStep[0m  [16/26], [94mLoss[0m : 8.14152
[1mStep[0m  [18/26], [94mLoss[0m : 8.06420
[1mStep[0m  [20/26], [94mLoss[0m : 8.46298
[1mStep[0m  [22/26], [94mLoss[0m : 7.99770
[1mStep[0m  [24/26], [94mLoss[0m : 8.07553

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.194, [92mTest[0m: 8.239, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.79090
[1mStep[0m  [2/26], [94mLoss[0m : 7.75978
[1mStep[0m  [4/26], [94mLoss[0m : 8.18009
[1mStep[0m  [6/26], [94mLoss[0m : 8.02697
[1mStep[0m  [8/26], [94mLoss[0m : 8.13132
[1mStep[0m  [10/26], [94mLoss[0m : 7.90309
[1mStep[0m  [12/26], [94mLoss[0m : 8.10571
[1mStep[0m  [14/26], [94mLoss[0m : 8.22929
[1mStep[0m  [16/26], [94mLoss[0m : 8.19874
[1mStep[0m  [18/26], [94mLoss[0m : 8.11312
[1mStep[0m  [20/26], [94mLoss[0m : 7.94489
[1mStep[0m  [22/26], [94mLoss[0m : 7.95952
[1mStep[0m  [24/26], [94mLoss[0m : 8.03625

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.066, [92mTest[0m: 8.108, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.80827
[1mStep[0m  [2/26], [94mLoss[0m : 7.92302
[1mStep[0m  [4/26], [94mLoss[0m : 8.12415
[1mStep[0m  [6/26], [94mLoss[0m : 7.70467
[1mStep[0m  [8/26], [94mLoss[0m : 8.16179
[1mStep[0m  [10/26], [94mLoss[0m : 8.04158
[1mStep[0m  [12/26], [94mLoss[0m : 7.44976
[1mStep[0m  [14/26], [94mLoss[0m : 7.94008
[1mStep[0m  [16/26], [94mLoss[0m : 7.95536
[1mStep[0m  [18/26], [94mLoss[0m : 7.92582
[1mStep[0m  [20/26], [94mLoss[0m : 7.83074
[1mStep[0m  [22/26], [94mLoss[0m : 7.92562
[1mStep[0m  [24/26], [94mLoss[0m : 7.80539

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.937, [92mTest[0m: 7.995, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.91479
[1mStep[0m  [2/26], [94mLoss[0m : 7.64620
[1mStep[0m  [4/26], [94mLoss[0m : 8.18258
[1mStep[0m  [6/26], [94mLoss[0m : 7.60097
[1mStep[0m  [8/26], [94mLoss[0m : 7.95003
[1mStep[0m  [10/26], [94mLoss[0m : 8.01069
[1mStep[0m  [12/26], [94mLoss[0m : 7.82759
[1mStep[0m  [14/26], [94mLoss[0m : 7.80308
[1mStep[0m  [16/26], [94mLoss[0m : 7.64072
[1mStep[0m  [18/26], [94mLoss[0m : 7.94300
[1mStep[0m  [20/26], [94mLoss[0m : 7.70966
[1mStep[0m  [22/26], [94mLoss[0m : 7.73178
[1mStep[0m  [24/26], [94mLoss[0m : 7.80159

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.813, [92mTest[0m: 7.864, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.53722
[1mStep[0m  [2/26], [94mLoss[0m : 7.67443
[1mStep[0m  [4/26], [94mLoss[0m : 7.81787
[1mStep[0m  [6/26], [94mLoss[0m : 7.55722
[1mStep[0m  [8/26], [94mLoss[0m : 7.80719
[1mStep[0m  [10/26], [94mLoss[0m : 7.48372
[1mStep[0m  [12/26], [94mLoss[0m : 7.68958
[1mStep[0m  [14/26], [94mLoss[0m : 7.76161
[1mStep[0m  [16/26], [94mLoss[0m : 7.65025
[1mStep[0m  [18/26], [94mLoss[0m : 7.75760
[1mStep[0m  [20/26], [94mLoss[0m : 7.75042
[1mStep[0m  [22/26], [94mLoss[0m : 7.76128
[1mStep[0m  [24/26], [94mLoss[0m : 7.60359

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.694, [92mTest[0m: 7.739, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.88959
[1mStep[0m  [2/26], [94mLoss[0m : 7.82387
[1mStep[0m  [4/26], [94mLoss[0m : 7.31813
[1mStep[0m  [6/26], [94mLoss[0m : 7.67730
[1mStep[0m  [8/26], [94mLoss[0m : 7.55615
[1mStep[0m  [10/26], [94mLoss[0m : 7.58506
[1mStep[0m  [12/26], [94mLoss[0m : 7.67422
[1mStep[0m  [14/26], [94mLoss[0m : 7.69398
[1mStep[0m  [16/26], [94mLoss[0m : 7.70236
[1mStep[0m  [18/26], [94mLoss[0m : 7.47602
[1mStep[0m  [20/26], [94mLoss[0m : 7.70162
[1mStep[0m  [22/26], [94mLoss[0m : 7.54044
[1mStep[0m  [24/26], [94mLoss[0m : 7.59918

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.578, [92mTest[0m: 7.617, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.47627
[1mStep[0m  [2/26], [94mLoss[0m : 7.42852
[1mStep[0m  [4/26], [94mLoss[0m : 7.73033
[1mStep[0m  [6/26], [94mLoss[0m : 7.48697
[1mStep[0m  [8/26], [94mLoss[0m : 7.40012
[1mStep[0m  [10/26], [94mLoss[0m : 7.02322
[1mStep[0m  [12/26], [94mLoss[0m : 7.49888
[1mStep[0m  [14/26], [94mLoss[0m : 6.91856
[1mStep[0m  [16/26], [94mLoss[0m : 7.26984
[1mStep[0m  [18/26], [94mLoss[0m : 7.39704
[1mStep[0m  [20/26], [94mLoss[0m : 7.60395
[1mStep[0m  [22/26], [94mLoss[0m : 7.45654
[1mStep[0m  [24/26], [94mLoss[0m : 7.57842

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.435, [92mTest[0m: 7.509, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.25393
[1mStep[0m  [2/26], [94mLoss[0m : 7.37309
[1mStep[0m  [4/26], [94mLoss[0m : 7.35433
[1mStep[0m  [6/26], [94mLoss[0m : 7.33549
[1mStep[0m  [8/26], [94mLoss[0m : 7.23909
[1mStep[0m  [10/26], [94mLoss[0m : 7.39905
[1mStep[0m  [12/26], [94mLoss[0m : 7.45467
[1mStep[0m  [14/26], [94mLoss[0m : 7.77592
[1mStep[0m  [16/26], [94mLoss[0m : 7.50353
[1mStep[0m  [18/26], [94mLoss[0m : 7.22070
[1mStep[0m  [20/26], [94mLoss[0m : 7.30701
[1mStep[0m  [22/26], [94mLoss[0m : 6.95910
[1mStep[0m  [24/26], [94mLoss[0m : 7.61241

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.324, [92mTest[0m: 7.372, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.35747
[1mStep[0m  [2/26], [94mLoss[0m : 7.24537
[1mStep[0m  [4/26], [94mLoss[0m : 7.10066
[1mStep[0m  [6/26], [94mLoss[0m : 7.33361
[1mStep[0m  [8/26], [94mLoss[0m : 7.08651
[1mStep[0m  [10/26], [94mLoss[0m : 7.06646
[1mStep[0m  [12/26], [94mLoss[0m : 7.29555
[1mStep[0m  [14/26], [94mLoss[0m : 7.20479
[1mStep[0m  [16/26], [94mLoss[0m : 7.21738
[1mStep[0m  [18/26], [94mLoss[0m : 7.26635
[1mStep[0m  [20/26], [94mLoss[0m : 7.37422
[1mStep[0m  [22/26], [94mLoss[0m : 7.47075
[1mStep[0m  [24/26], [94mLoss[0m : 6.99687

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.205, [92mTest[0m: 7.248, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.79989
[1mStep[0m  [2/26], [94mLoss[0m : 7.16171
[1mStep[0m  [4/26], [94mLoss[0m : 6.92865
[1mStep[0m  [6/26], [94mLoss[0m : 7.20153
[1mStep[0m  [8/26], [94mLoss[0m : 7.15241
[1mStep[0m  [10/26], [94mLoss[0m : 7.19358
[1mStep[0m  [12/26], [94mLoss[0m : 6.72748
[1mStep[0m  [14/26], [94mLoss[0m : 7.25080
[1mStep[0m  [16/26], [94mLoss[0m : 6.90835
[1mStep[0m  [18/26], [94mLoss[0m : 7.11344
[1mStep[0m  [20/26], [94mLoss[0m : 7.20957
[1mStep[0m  [22/26], [94mLoss[0m : 6.85555
[1mStep[0m  [24/26], [94mLoss[0m : 6.67210

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.079, [92mTest[0m: 7.138, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.16540
[1mStep[0m  [2/26], [94mLoss[0m : 6.99288
[1mStep[0m  [4/26], [94mLoss[0m : 7.02824
[1mStep[0m  [6/26], [94mLoss[0m : 6.96999
[1mStep[0m  [8/26], [94mLoss[0m : 6.98630
[1mStep[0m  [10/26], [94mLoss[0m : 7.03565
[1mStep[0m  [12/26], [94mLoss[0m : 6.90966
[1mStep[0m  [14/26], [94mLoss[0m : 6.81288
[1mStep[0m  [16/26], [94mLoss[0m : 7.00266
[1mStep[0m  [18/26], [94mLoss[0m : 7.02124
[1mStep[0m  [20/26], [94mLoss[0m : 7.27273
[1mStep[0m  [22/26], [94mLoss[0m : 6.85008
[1mStep[0m  [24/26], [94mLoss[0m : 6.72169

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.972, [92mTest[0m: 7.015, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.879
====================================

Phase 1 - Evaluation MAE:  6.878607236422026
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 6.88557
[1mStep[0m  [2/26], [94mLoss[0m : 7.02218
[1mStep[0m  [4/26], [94mLoss[0m : 6.71987
[1mStep[0m  [6/26], [94mLoss[0m : 7.06224
[1mStep[0m  [8/26], [94mLoss[0m : 6.86551
[1mStep[0m  [10/26], [94mLoss[0m : 6.60644
[1mStep[0m  [12/26], [94mLoss[0m : 6.91589
[1mStep[0m  [14/26], [94mLoss[0m : 6.69281
[1mStep[0m  [16/26], [94mLoss[0m : 6.85966
[1mStep[0m  [18/26], [94mLoss[0m : 6.79503
[1mStep[0m  [20/26], [94mLoss[0m : 6.95833
[1mStep[0m  [22/26], [94mLoss[0m : 6.58758
[1mStep[0m  [24/26], [94mLoss[0m : 6.64250

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.839, [92mTest[0m: 6.896, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.98318
[1mStep[0m  [2/26], [94mLoss[0m : 6.53172
[1mStep[0m  [4/26], [94mLoss[0m : 6.64599
[1mStep[0m  [6/26], [94mLoss[0m : 6.71759
[1mStep[0m  [8/26], [94mLoss[0m : 6.84531
[1mStep[0m  [10/26], [94mLoss[0m : 6.62516
[1mStep[0m  [12/26], [94mLoss[0m : 6.89883
[1mStep[0m  [14/26], [94mLoss[0m : 6.63517
[1mStep[0m  [16/26], [94mLoss[0m : 6.76864
[1mStep[0m  [18/26], [94mLoss[0m : 6.70227
[1mStep[0m  [20/26], [94mLoss[0m : 6.45483
[1mStep[0m  [22/26], [94mLoss[0m : 6.55023
[1mStep[0m  [24/26], [94mLoss[0m : 6.38998

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.692, [92mTest[0m: 6.763, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.52886
[1mStep[0m  [2/26], [94mLoss[0m : 6.70037
[1mStep[0m  [4/26], [94mLoss[0m : 6.49539
[1mStep[0m  [6/26], [94mLoss[0m : 6.63158
[1mStep[0m  [8/26], [94mLoss[0m : 6.70687
[1mStep[0m  [10/26], [94mLoss[0m : 6.58640
[1mStep[0m  [12/26], [94mLoss[0m : 6.60844
[1mStep[0m  [14/26], [94mLoss[0m : 6.46108
[1mStep[0m  [16/26], [94mLoss[0m : 6.61660
[1mStep[0m  [18/26], [94mLoss[0m : 6.44996
[1mStep[0m  [20/26], [94mLoss[0m : 6.48294
[1mStep[0m  [22/26], [94mLoss[0m : 6.52672
[1mStep[0m  [24/26], [94mLoss[0m : 6.56679

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.567, [92mTest[0m: 6.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.39208
[1mStep[0m  [2/26], [94mLoss[0m : 6.45655
[1mStep[0m  [4/26], [94mLoss[0m : 6.49701
[1mStep[0m  [6/26], [94mLoss[0m : 6.72267
[1mStep[0m  [8/26], [94mLoss[0m : 6.35369
[1mStep[0m  [10/26], [94mLoss[0m : 6.38346
[1mStep[0m  [12/26], [94mLoss[0m : 6.32197
[1mStep[0m  [14/26], [94mLoss[0m : 6.49992
[1mStep[0m  [16/26], [94mLoss[0m : 6.41906
[1mStep[0m  [18/26], [94mLoss[0m : 6.44828
[1mStep[0m  [20/26], [94mLoss[0m : 6.38945
[1mStep[0m  [22/26], [94mLoss[0m : 6.36088
[1mStep[0m  [24/26], [94mLoss[0m : 6.49650

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.430, [92mTest[0m: 6.484, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.65663
[1mStep[0m  [2/26], [94mLoss[0m : 6.32361
[1mStep[0m  [4/26], [94mLoss[0m : 6.37594
[1mStep[0m  [6/26], [94mLoss[0m : 6.19939
[1mStep[0m  [8/26], [94mLoss[0m : 6.37579
[1mStep[0m  [10/26], [94mLoss[0m : 6.44398
[1mStep[0m  [12/26], [94mLoss[0m : 6.14433
[1mStep[0m  [14/26], [94mLoss[0m : 6.33246
[1mStep[0m  [16/26], [94mLoss[0m : 6.18655
[1mStep[0m  [18/26], [94mLoss[0m : 6.28146
[1mStep[0m  [20/26], [94mLoss[0m : 6.42615
[1mStep[0m  [22/26], [94mLoss[0m : 6.27916
[1mStep[0m  [24/26], [94mLoss[0m : 6.22176

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.287, [92mTest[0m: 6.342, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.37821
[1mStep[0m  [2/26], [94mLoss[0m : 6.21667
[1mStep[0m  [4/26], [94mLoss[0m : 6.02636
[1mStep[0m  [6/26], [94mLoss[0m : 5.95095
[1mStep[0m  [8/26], [94mLoss[0m : 6.23737
[1mStep[0m  [10/26], [94mLoss[0m : 6.13480
[1mStep[0m  [12/26], [94mLoss[0m : 6.43437
[1mStep[0m  [14/26], [94mLoss[0m : 6.14304
[1mStep[0m  [16/26], [94mLoss[0m : 6.08568
[1mStep[0m  [18/26], [94mLoss[0m : 6.03373
[1mStep[0m  [20/26], [94mLoss[0m : 6.42288
[1mStep[0m  [22/26], [94mLoss[0m : 6.13453
[1mStep[0m  [24/26], [94mLoss[0m : 6.35820

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.159, [92mTest[0m: 6.219, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.31340
[1mStep[0m  [2/26], [94mLoss[0m : 6.27156
[1mStep[0m  [4/26], [94mLoss[0m : 6.06350
[1mStep[0m  [6/26], [94mLoss[0m : 5.91210
[1mStep[0m  [8/26], [94mLoss[0m : 5.83955
[1mStep[0m  [10/26], [94mLoss[0m : 6.16086
[1mStep[0m  [12/26], [94mLoss[0m : 5.83879
[1mStep[0m  [14/26], [94mLoss[0m : 5.87106
[1mStep[0m  [16/26], [94mLoss[0m : 6.13601
[1mStep[0m  [18/26], [94mLoss[0m : 5.82844
[1mStep[0m  [20/26], [94mLoss[0m : 6.06580
[1mStep[0m  [22/26], [94mLoss[0m : 6.30125
[1mStep[0m  [24/26], [94mLoss[0m : 5.96968

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.029, [92mTest[0m: 6.084, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.70874
[1mStep[0m  [2/26], [94mLoss[0m : 5.96754
[1mStep[0m  [4/26], [94mLoss[0m : 6.09983
[1mStep[0m  [6/26], [94mLoss[0m : 5.66686
[1mStep[0m  [8/26], [94mLoss[0m : 6.13040
[1mStep[0m  [10/26], [94mLoss[0m : 5.89296
[1mStep[0m  [12/26], [94mLoss[0m : 5.73494
[1mStep[0m  [14/26], [94mLoss[0m : 5.78199
[1mStep[0m  [16/26], [94mLoss[0m : 5.89722
[1mStep[0m  [18/26], [94mLoss[0m : 5.94703
[1mStep[0m  [20/26], [94mLoss[0m : 5.85771
[1mStep[0m  [22/26], [94mLoss[0m : 5.70513
[1mStep[0m  [24/26], [94mLoss[0m : 5.70423

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.881, [92mTest[0m: 5.948, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.99111
[1mStep[0m  [2/26], [94mLoss[0m : 5.75032
[1mStep[0m  [4/26], [94mLoss[0m : 5.90606
[1mStep[0m  [6/26], [94mLoss[0m : 5.80006
[1mStep[0m  [8/26], [94mLoss[0m : 5.88330
[1mStep[0m  [10/26], [94mLoss[0m : 5.68683
[1mStep[0m  [12/26], [94mLoss[0m : 6.10066
[1mStep[0m  [14/26], [94mLoss[0m : 5.76646
[1mStep[0m  [16/26], [94mLoss[0m : 5.91095
[1mStep[0m  [18/26], [94mLoss[0m : 5.54207
[1mStep[0m  [20/26], [94mLoss[0m : 5.67082
[1mStep[0m  [22/26], [94mLoss[0m : 5.75171
[1mStep[0m  [24/26], [94mLoss[0m : 5.48255

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.762, [92mTest[0m: 5.801, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.54881
[1mStep[0m  [2/26], [94mLoss[0m : 5.58272
[1mStep[0m  [4/26], [94mLoss[0m : 5.69925
[1mStep[0m  [6/26], [94mLoss[0m : 5.58495
[1mStep[0m  [8/26], [94mLoss[0m : 5.62291
[1mStep[0m  [10/26], [94mLoss[0m : 5.53647
[1mStep[0m  [12/26], [94mLoss[0m : 5.72128
[1mStep[0m  [14/26], [94mLoss[0m : 5.68402
[1mStep[0m  [16/26], [94mLoss[0m : 5.89804
[1mStep[0m  [18/26], [94mLoss[0m : 5.65078
[1mStep[0m  [20/26], [94mLoss[0m : 5.36214
[1mStep[0m  [22/26], [94mLoss[0m : 5.42269
[1mStep[0m  [24/26], [94mLoss[0m : 5.92748

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.639, [92mTest[0m: 5.674, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.56139
[1mStep[0m  [2/26], [94mLoss[0m : 5.44893
[1mStep[0m  [4/26], [94mLoss[0m : 5.63713
[1mStep[0m  [6/26], [94mLoss[0m : 5.49103
[1mStep[0m  [8/26], [94mLoss[0m : 5.36776
[1mStep[0m  [10/26], [94mLoss[0m : 5.72353
[1mStep[0m  [12/26], [94mLoss[0m : 5.55747
[1mStep[0m  [14/26], [94mLoss[0m : 5.49130
[1mStep[0m  [16/26], [94mLoss[0m : 5.64924
[1mStep[0m  [18/26], [94mLoss[0m : 5.33212
[1mStep[0m  [20/26], [94mLoss[0m : 5.25894
[1mStep[0m  [22/26], [94mLoss[0m : 5.40047
[1mStep[0m  [24/26], [94mLoss[0m : 5.74559

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.509, [92mTest[0m: 5.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.32060
[1mStep[0m  [2/26], [94mLoss[0m : 5.40565
[1mStep[0m  [4/26], [94mLoss[0m : 5.45934
[1mStep[0m  [6/26], [94mLoss[0m : 4.99812
[1mStep[0m  [8/26], [94mLoss[0m : 5.26853
[1mStep[0m  [10/26], [94mLoss[0m : 5.26782
[1mStep[0m  [12/26], [94mLoss[0m : 5.30858
[1mStep[0m  [14/26], [94mLoss[0m : 5.32027
[1mStep[0m  [16/26], [94mLoss[0m : 5.65163
[1mStep[0m  [18/26], [94mLoss[0m : 5.16925
[1mStep[0m  [20/26], [94mLoss[0m : 5.35061
[1mStep[0m  [22/26], [94mLoss[0m : 5.26142
[1mStep[0m  [24/26], [94mLoss[0m : 5.56300

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.375, [92mTest[0m: 5.419, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.47838
[1mStep[0m  [2/26], [94mLoss[0m : 5.27194
[1mStep[0m  [4/26], [94mLoss[0m : 5.23847
[1mStep[0m  [6/26], [94mLoss[0m : 5.30402
[1mStep[0m  [8/26], [94mLoss[0m : 5.47747
[1mStep[0m  [10/26], [94mLoss[0m : 5.20919
[1mStep[0m  [12/26], [94mLoss[0m : 4.99331
[1mStep[0m  [14/26], [94mLoss[0m : 5.16643
[1mStep[0m  [16/26], [94mLoss[0m : 5.35636
[1mStep[0m  [18/26], [94mLoss[0m : 5.27274
[1mStep[0m  [20/26], [94mLoss[0m : 5.20762
[1mStep[0m  [22/26], [94mLoss[0m : 5.18224
[1mStep[0m  [24/26], [94mLoss[0m : 5.01902

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.249, [92mTest[0m: 5.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.18467
[1mStep[0m  [2/26], [94mLoss[0m : 5.26511
[1mStep[0m  [4/26], [94mLoss[0m : 5.25793
[1mStep[0m  [6/26], [94mLoss[0m : 5.12620
[1mStep[0m  [8/26], [94mLoss[0m : 5.18640
[1mStep[0m  [10/26], [94mLoss[0m : 5.21561
[1mStep[0m  [12/26], [94mLoss[0m : 5.18463
[1mStep[0m  [14/26], [94mLoss[0m : 5.01897
[1mStep[0m  [16/26], [94mLoss[0m : 5.12828
[1mStep[0m  [18/26], [94mLoss[0m : 5.24539
[1mStep[0m  [20/26], [94mLoss[0m : 5.05048
[1mStep[0m  [22/26], [94mLoss[0m : 5.05506
[1mStep[0m  [24/26], [94mLoss[0m : 5.06073

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.146, [92mTest[0m: 5.167, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.13088
[1mStep[0m  [2/26], [94mLoss[0m : 5.01738
[1mStep[0m  [4/26], [94mLoss[0m : 5.04774
[1mStep[0m  [6/26], [94mLoss[0m : 5.01456
[1mStep[0m  [8/26], [94mLoss[0m : 5.02221
[1mStep[0m  [10/26], [94mLoss[0m : 5.00593
[1mStep[0m  [12/26], [94mLoss[0m : 5.08757
[1mStep[0m  [14/26], [94mLoss[0m : 5.20967
[1mStep[0m  [16/26], [94mLoss[0m : 4.97586
[1mStep[0m  [18/26], [94mLoss[0m : 4.59845
[1mStep[0m  [20/26], [94mLoss[0m : 5.16321
[1mStep[0m  [22/26], [94mLoss[0m : 4.92077
[1mStep[0m  [24/26], [94mLoss[0m : 5.19999

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.024, [92mTest[0m: 5.054, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.98270
[1mStep[0m  [2/26], [94mLoss[0m : 4.90325
[1mStep[0m  [4/26], [94mLoss[0m : 4.94640
[1mStep[0m  [6/26], [94mLoss[0m : 4.74648
[1mStep[0m  [8/26], [94mLoss[0m : 5.04607
[1mStep[0m  [10/26], [94mLoss[0m : 4.82439
[1mStep[0m  [12/26], [94mLoss[0m : 5.34485
[1mStep[0m  [14/26], [94mLoss[0m : 4.94951
[1mStep[0m  [16/26], [94mLoss[0m : 4.73874
[1mStep[0m  [18/26], [94mLoss[0m : 4.98126
[1mStep[0m  [20/26], [94mLoss[0m : 4.47509
[1mStep[0m  [22/26], [94mLoss[0m : 4.77217
[1mStep[0m  [24/26], [94mLoss[0m : 4.64095

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.905, [92mTest[0m: 4.933, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.89008
[1mStep[0m  [2/26], [94mLoss[0m : 4.90191
[1mStep[0m  [4/26], [94mLoss[0m : 4.88685
[1mStep[0m  [6/26], [94mLoss[0m : 4.97100
[1mStep[0m  [8/26], [94mLoss[0m : 4.61427
[1mStep[0m  [10/26], [94mLoss[0m : 4.59144
[1mStep[0m  [12/26], [94mLoss[0m : 4.83763
[1mStep[0m  [14/26], [94mLoss[0m : 4.99430
[1mStep[0m  [16/26], [94mLoss[0m : 4.74246
[1mStep[0m  [18/26], [94mLoss[0m : 4.82704
[1mStep[0m  [20/26], [94mLoss[0m : 4.75699
[1mStep[0m  [22/26], [94mLoss[0m : 4.69697
[1mStep[0m  [24/26], [94mLoss[0m : 4.67387

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.799, [92mTest[0m: 4.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.52272
[1mStep[0m  [2/26], [94mLoss[0m : 4.80048
[1mStep[0m  [4/26], [94mLoss[0m : 4.78969
[1mStep[0m  [6/26], [94mLoss[0m : 4.56811
[1mStep[0m  [8/26], [94mLoss[0m : 4.76333
[1mStep[0m  [10/26], [94mLoss[0m : 4.77908
[1mStep[0m  [12/26], [94mLoss[0m : 4.58128
[1mStep[0m  [14/26], [94mLoss[0m : 4.94193
[1mStep[0m  [16/26], [94mLoss[0m : 4.79369
[1mStep[0m  [18/26], [94mLoss[0m : 4.54638
[1mStep[0m  [20/26], [94mLoss[0m : 4.55586
[1mStep[0m  [22/26], [94mLoss[0m : 4.81041
[1mStep[0m  [24/26], [94mLoss[0m : 4.40321

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.694, [92mTest[0m: 4.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.39266
[1mStep[0m  [2/26], [94mLoss[0m : 4.67177
[1mStep[0m  [4/26], [94mLoss[0m : 4.45110
[1mStep[0m  [6/26], [94mLoss[0m : 4.66488
[1mStep[0m  [8/26], [94mLoss[0m : 4.66998
[1mStep[0m  [10/26], [94mLoss[0m : 4.70064
[1mStep[0m  [12/26], [94mLoss[0m : 4.64405
[1mStep[0m  [14/26], [94mLoss[0m : 4.41190
[1mStep[0m  [16/26], [94mLoss[0m : 4.60812
[1mStep[0m  [18/26], [94mLoss[0m : 4.51394
[1mStep[0m  [20/26], [94mLoss[0m : 4.52527
[1mStep[0m  [22/26], [94mLoss[0m : 4.66819
[1mStep[0m  [24/26], [94mLoss[0m : 4.52464

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.594, [92mTest[0m: 4.604, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.41242
[1mStep[0m  [2/26], [94mLoss[0m : 4.52876
[1mStep[0m  [4/26], [94mLoss[0m : 4.72187
[1mStep[0m  [6/26], [94mLoss[0m : 4.33496
[1mStep[0m  [8/26], [94mLoss[0m : 4.53409
[1mStep[0m  [10/26], [94mLoss[0m : 4.55115
[1mStep[0m  [12/26], [94mLoss[0m : 4.29534
[1mStep[0m  [14/26], [94mLoss[0m : 4.47947
[1mStep[0m  [16/26], [94mLoss[0m : 4.43148
[1mStep[0m  [18/26], [94mLoss[0m : 4.38741
[1mStep[0m  [20/26], [94mLoss[0m : 4.44787
[1mStep[0m  [22/26], [94mLoss[0m : 4.27815
[1mStep[0m  [24/26], [94mLoss[0m : 4.44118

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.488, [92mTest[0m: 4.523, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.71198
[1mStep[0m  [2/26], [94mLoss[0m : 4.45915
[1mStep[0m  [4/26], [94mLoss[0m : 4.43623
[1mStep[0m  [6/26], [94mLoss[0m : 4.52417
[1mStep[0m  [8/26], [94mLoss[0m : 4.34528
[1mStep[0m  [10/26], [94mLoss[0m : 4.33912
[1mStep[0m  [12/26], [94mLoss[0m : 4.42200
[1mStep[0m  [14/26], [94mLoss[0m : 4.44709
[1mStep[0m  [16/26], [94mLoss[0m : 4.46208
[1mStep[0m  [18/26], [94mLoss[0m : 4.34586
[1mStep[0m  [20/26], [94mLoss[0m : 4.62924
[1mStep[0m  [22/26], [94mLoss[0m : 4.29134
[1mStep[0m  [24/26], [94mLoss[0m : 4.22233

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.407, [92mTest[0m: 4.404, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.21286
[1mStep[0m  [2/26], [94mLoss[0m : 4.38179
[1mStep[0m  [4/26], [94mLoss[0m : 4.23106
[1mStep[0m  [6/26], [94mLoss[0m : 4.43267
[1mStep[0m  [8/26], [94mLoss[0m : 4.50802
[1mStep[0m  [10/26], [94mLoss[0m : 4.46186
[1mStep[0m  [12/26], [94mLoss[0m : 4.47406
[1mStep[0m  [14/26], [94mLoss[0m : 4.16252
[1mStep[0m  [16/26], [94mLoss[0m : 4.51307
[1mStep[0m  [18/26], [94mLoss[0m : 4.48497
[1mStep[0m  [20/26], [94mLoss[0m : 4.58585
[1mStep[0m  [22/26], [94mLoss[0m : 4.40949
[1mStep[0m  [24/26], [94mLoss[0m : 4.12767

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.336, [92mTest[0m: 4.333, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.42728
[1mStep[0m  [2/26], [94mLoss[0m : 4.42102
[1mStep[0m  [4/26], [94mLoss[0m : 4.11225
[1mStep[0m  [6/26], [94mLoss[0m : 4.00863
[1mStep[0m  [8/26], [94mLoss[0m : 4.38135
[1mStep[0m  [10/26], [94mLoss[0m : 4.00600
[1mStep[0m  [12/26], [94mLoss[0m : 4.08208
[1mStep[0m  [14/26], [94mLoss[0m : 4.44210
[1mStep[0m  [16/26], [94mLoss[0m : 4.17289
[1mStep[0m  [18/26], [94mLoss[0m : 4.07827
[1mStep[0m  [20/26], [94mLoss[0m : 4.44544
[1mStep[0m  [22/26], [94mLoss[0m : 4.16496
[1mStep[0m  [24/26], [94mLoss[0m : 4.22965

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.251, [92mTest[0m: 4.250, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.19329
[1mStep[0m  [2/26], [94mLoss[0m : 4.34183
[1mStep[0m  [4/26], [94mLoss[0m : 3.92619
[1mStep[0m  [6/26], [94mLoss[0m : 4.02936
[1mStep[0m  [8/26], [94mLoss[0m : 4.14238
[1mStep[0m  [10/26], [94mLoss[0m : 4.14582
[1mStep[0m  [12/26], [94mLoss[0m : 4.28213
[1mStep[0m  [14/26], [94mLoss[0m : 4.32939
[1mStep[0m  [16/26], [94mLoss[0m : 4.04791
[1mStep[0m  [18/26], [94mLoss[0m : 4.14573
[1mStep[0m  [20/26], [94mLoss[0m : 4.12271
[1mStep[0m  [22/26], [94mLoss[0m : 4.17250
[1mStep[0m  [24/26], [94mLoss[0m : 4.05452

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.177, [92mTest[0m: 4.165, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.50456
[1mStep[0m  [2/26], [94mLoss[0m : 4.21815
[1mStep[0m  [4/26], [94mLoss[0m : 4.20430
[1mStep[0m  [6/26], [94mLoss[0m : 4.20458
[1mStep[0m  [8/26], [94mLoss[0m : 3.95388
[1mStep[0m  [10/26], [94mLoss[0m : 4.08470
[1mStep[0m  [12/26], [94mLoss[0m : 4.04906
[1mStep[0m  [14/26], [94mLoss[0m : 3.79492
[1mStep[0m  [16/26], [94mLoss[0m : 4.20565
[1mStep[0m  [18/26], [94mLoss[0m : 4.05141
[1mStep[0m  [20/26], [94mLoss[0m : 4.11423
[1mStep[0m  [22/26], [94mLoss[0m : 3.97375
[1mStep[0m  [24/26], [94mLoss[0m : 3.74996

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.100, [92mTest[0m: 4.103, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.10944
[1mStep[0m  [2/26], [94mLoss[0m : 3.95116
[1mStep[0m  [4/26], [94mLoss[0m : 4.10553
[1mStep[0m  [6/26], [94mLoss[0m : 3.98324
[1mStep[0m  [8/26], [94mLoss[0m : 4.28977
[1mStep[0m  [10/26], [94mLoss[0m : 4.18697
[1mStep[0m  [12/26], [94mLoss[0m : 3.90889
[1mStep[0m  [14/26], [94mLoss[0m : 4.01357
[1mStep[0m  [16/26], [94mLoss[0m : 4.20843
[1mStep[0m  [18/26], [94mLoss[0m : 4.18270
[1mStep[0m  [20/26], [94mLoss[0m : 3.85701
[1mStep[0m  [22/26], [94mLoss[0m : 3.90414
[1mStep[0m  [24/26], [94mLoss[0m : 3.95169

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.037, [92mTest[0m: 4.036, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.76249
[1mStep[0m  [2/26], [94mLoss[0m : 3.79415
[1mStep[0m  [4/26], [94mLoss[0m : 3.85210
[1mStep[0m  [6/26], [94mLoss[0m : 3.80429
[1mStep[0m  [8/26], [94mLoss[0m : 4.09340
[1mStep[0m  [10/26], [94mLoss[0m : 3.89848
[1mStep[0m  [12/26], [94mLoss[0m : 3.95908
[1mStep[0m  [14/26], [94mLoss[0m : 4.03836
[1mStep[0m  [16/26], [94mLoss[0m : 3.69311
[1mStep[0m  [18/26], [94mLoss[0m : 3.83370
[1mStep[0m  [20/26], [94mLoss[0m : 4.02508
[1mStep[0m  [22/26], [94mLoss[0m : 4.22350
[1mStep[0m  [24/26], [94mLoss[0m : 4.13240

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.967, [92mTest[0m: 3.945, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.11485
[1mStep[0m  [2/26], [94mLoss[0m : 3.98242
[1mStep[0m  [4/26], [94mLoss[0m : 3.85528
[1mStep[0m  [6/26], [94mLoss[0m : 3.99334
[1mStep[0m  [8/26], [94mLoss[0m : 3.78382
[1mStep[0m  [10/26], [94mLoss[0m : 4.10750
[1mStep[0m  [12/26], [94mLoss[0m : 3.89084
[1mStep[0m  [14/26], [94mLoss[0m : 3.88892
[1mStep[0m  [16/26], [94mLoss[0m : 3.78583
[1mStep[0m  [18/26], [94mLoss[0m : 3.97640
[1mStep[0m  [20/26], [94mLoss[0m : 4.13846
[1mStep[0m  [22/26], [94mLoss[0m : 3.76254
[1mStep[0m  [24/26], [94mLoss[0m : 3.69849

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.923, [92mTest[0m: 3.902, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.68149
[1mStep[0m  [2/26], [94mLoss[0m : 3.64848
[1mStep[0m  [4/26], [94mLoss[0m : 4.15319
[1mStep[0m  [6/26], [94mLoss[0m : 4.06849
[1mStep[0m  [8/26], [94mLoss[0m : 3.80005
[1mStep[0m  [10/26], [94mLoss[0m : 3.76496
[1mStep[0m  [12/26], [94mLoss[0m : 3.71879
[1mStep[0m  [14/26], [94mLoss[0m : 3.86304
[1mStep[0m  [16/26], [94mLoss[0m : 4.19320
[1mStep[0m  [18/26], [94mLoss[0m : 3.71690
[1mStep[0m  [20/26], [94mLoss[0m : 3.80123
[1mStep[0m  [22/26], [94mLoss[0m : 3.79466
[1mStep[0m  [24/26], [94mLoss[0m : 3.82061

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.843, [92mTest[0m: 3.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.11594
[1mStep[0m  [2/26], [94mLoss[0m : 3.98742
[1mStep[0m  [4/26], [94mLoss[0m : 3.53564
[1mStep[0m  [6/26], [94mLoss[0m : 3.93863
[1mStep[0m  [8/26], [94mLoss[0m : 3.81337
[1mStep[0m  [10/26], [94mLoss[0m : 3.73011
[1mStep[0m  [12/26], [94mLoss[0m : 3.96468
[1mStep[0m  [14/26], [94mLoss[0m : 3.87032
[1mStep[0m  [16/26], [94mLoss[0m : 3.87370
[1mStep[0m  [18/26], [94mLoss[0m : 3.74125
[1mStep[0m  [20/26], [94mLoss[0m : 3.79150
[1mStep[0m  [22/26], [94mLoss[0m : 3.50364
[1mStep[0m  [24/26], [94mLoss[0m : 3.52823

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.801, [92mTest[0m: 3.766, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.724
====================================

Phase 2 - Evaluation MAE:  3.7242255210876465
MAE score P1        6.878607
MAE score P2        3.724226
loss                3.801212
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.5
weight_decay          0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.85761
[1mStep[0m  [5/53], [94mLoss[0m : 10.92532
[1mStep[0m  [10/53], [94mLoss[0m : 10.77575
[1mStep[0m  [15/53], [94mLoss[0m : 10.61189
[1mStep[0m  [20/53], [94mLoss[0m : 10.49584
[1mStep[0m  [25/53], [94mLoss[0m : 10.75556
[1mStep[0m  [30/53], [94mLoss[0m : 10.56982
[1mStep[0m  [35/53], [94mLoss[0m : 10.24437
[1mStep[0m  [40/53], [94mLoss[0m : 10.71108
[1mStep[0m  [45/53], [94mLoss[0m : 11.14053
[1mStep[0m  [50/53], [94mLoss[0m : 11.13602

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.89394
[1mStep[0m  [5/53], [94mLoss[0m : 10.75274
[1mStep[0m  [10/53], [94mLoss[0m : 10.55190
[1mStep[0m  [15/53], [94mLoss[0m : 10.85755
[1mStep[0m  [20/53], [94mLoss[0m : 10.41512
[1mStep[0m  [25/53], [94mLoss[0m : 10.48495
[1mStep[0m  [30/53], [94mLoss[0m : 10.78982
[1mStep[0m  [35/53], [94mLoss[0m : 10.78228
[1mStep[0m  [40/53], [94mLoss[0m : 10.75414
[1mStep[0m  [45/53], [94mLoss[0m : 10.39390
[1mStep[0m  [50/53], [94mLoss[0m : 10.16781

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.631, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.79615
[1mStep[0m  [5/53], [94mLoss[0m : 10.31723
[1mStep[0m  [10/53], [94mLoss[0m : 10.61922
[1mStep[0m  [15/53], [94mLoss[0m : 10.81532
[1mStep[0m  [20/53], [94mLoss[0m : 10.17469
[1mStep[0m  [25/53], [94mLoss[0m : 10.37686
[1mStep[0m  [30/53], [94mLoss[0m : 10.78796
[1mStep[0m  [35/53], [94mLoss[0m : 10.57995
[1mStep[0m  [40/53], [94mLoss[0m : 10.36876
[1mStep[0m  [45/53], [94mLoss[0m : 10.50537
[1mStep[0m  [50/53], [94mLoss[0m : 10.59641

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.530, [92mTest[0m: 10.661, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.93277
[1mStep[0m  [5/53], [94mLoss[0m : 10.76153
[1mStep[0m  [10/53], [94mLoss[0m : 10.39304
[1mStep[0m  [15/53], [94mLoss[0m : 10.65859
[1mStep[0m  [20/53], [94mLoss[0m : 10.68749
[1mStep[0m  [25/53], [94mLoss[0m : 10.55109
[1mStep[0m  [30/53], [94mLoss[0m : 10.60685
[1mStep[0m  [35/53], [94mLoss[0m : 10.49080
[1mStep[0m  [40/53], [94mLoss[0m : 10.36311
[1mStep[0m  [45/53], [94mLoss[0m : 10.02028
[1mStep[0m  [50/53], [94mLoss[0m : 10.08068

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.448, [92mTest[0m: 10.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.48267
[1mStep[0m  [5/53], [94mLoss[0m : 10.37646
[1mStep[0m  [10/53], [94mLoss[0m : 10.99967
[1mStep[0m  [15/53], [94mLoss[0m : 10.09087
[1mStep[0m  [20/53], [94mLoss[0m : 10.86094
[1mStep[0m  [25/53], [94mLoss[0m : 10.76740
[1mStep[0m  [30/53], [94mLoss[0m : 10.36458
[1mStep[0m  [35/53], [94mLoss[0m : 10.45976
[1mStep[0m  [40/53], [94mLoss[0m : 10.31482
[1mStep[0m  [45/53], [94mLoss[0m : 10.30539
[1mStep[0m  [50/53], [94mLoss[0m : 10.09538

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.355, [92mTest[0m: 10.541, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.32170
[1mStep[0m  [5/53], [94mLoss[0m : 10.37256
[1mStep[0m  [10/53], [94mLoss[0m : 10.29495
[1mStep[0m  [15/53], [94mLoss[0m : 9.94870
[1mStep[0m  [20/53], [94mLoss[0m : 10.11117
[1mStep[0m  [25/53], [94mLoss[0m : 10.59313
[1mStep[0m  [30/53], [94mLoss[0m : 10.16238
[1mStep[0m  [35/53], [94mLoss[0m : 10.74682
[1mStep[0m  [40/53], [94mLoss[0m : 10.29545
[1mStep[0m  [45/53], [94mLoss[0m : 10.23455
[1mStep[0m  [50/53], [94mLoss[0m : 10.05441

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.269, [92mTest[0m: 10.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.32603
[1mStep[0m  [5/53], [94mLoss[0m : 10.29712
[1mStep[0m  [10/53], [94mLoss[0m : 10.28299
[1mStep[0m  [15/53], [94mLoss[0m : 10.30375
[1mStep[0m  [20/53], [94mLoss[0m : 10.04401
[1mStep[0m  [25/53], [94mLoss[0m : 10.49205
[1mStep[0m  [30/53], [94mLoss[0m : 10.07733
[1mStep[0m  [35/53], [94mLoss[0m : 9.92398
[1mStep[0m  [40/53], [94mLoss[0m : 10.38292
[1mStep[0m  [45/53], [94mLoss[0m : 10.32584
[1mStep[0m  [50/53], [94mLoss[0m : 9.98339

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.171, [92mTest[0m: 10.434, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.07947
[1mStep[0m  [5/53], [94mLoss[0m : 10.22758
[1mStep[0m  [10/53], [94mLoss[0m : 10.58023
[1mStep[0m  [15/53], [94mLoss[0m : 10.09305
[1mStep[0m  [20/53], [94mLoss[0m : 9.76219
[1mStep[0m  [25/53], [94mLoss[0m : 10.09369
[1mStep[0m  [30/53], [94mLoss[0m : 10.03493
[1mStep[0m  [35/53], [94mLoss[0m : 9.97268
[1mStep[0m  [40/53], [94mLoss[0m : 10.49801
[1mStep[0m  [45/53], [94mLoss[0m : 10.06851
[1mStep[0m  [50/53], [94mLoss[0m : 10.19330

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.081, [92mTest[0m: 10.356, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.89844
[1mStep[0m  [5/53], [94mLoss[0m : 9.79367
[1mStep[0m  [10/53], [94mLoss[0m : 10.16156
[1mStep[0m  [15/53], [94mLoss[0m : 9.63423
[1mStep[0m  [20/53], [94mLoss[0m : 9.80439
[1mStep[0m  [25/53], [94mLoss[0m : 10.03834
[1mStep[0m  [30/53], [94mLoss[0m : 10.13809
[1mStep[0m  [35/53], [94mLoss[0m : 10.08434
[1mStep[0m  [40/53], [94mLoss[0m : 9.59953
[1mStep[0m  [45/53], [94mLoss[0m : 10.51542
[1mStep[0m  [50/53], [94mLoss[0m : 10.00519

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.991, [92mTest[0m: 10.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.15018
[1mStep[0m  [5/53], [94mLoss[0m : 9.83287
[1mStep[0m  [10/53], [94mLoss[0m : 10.03879
[1mStep[0m  [15/53], [94mLoss[0m : 9.68709
[1mStep[0m  [20/53], [94mLoss[0m : 9.94476
[1mStep[0m  [25/53], [94mLoss[0m : 9.81897
[1mStep[0m  [30/53], [94mLoss[0m : 9.61673
[1mStep[0m  [35/53], [94mLoss[0m : 10.16380
[1mStep[0m  [40/53], [94mLoss[0m : 10.25717
[1mStep[0m  [45/53], [94mLoss[0m : 9.83531
[1mStep[0m  [50/53], [94mLoss[0m : 9.90111

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.906, [92mTest[0m: 10.243, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.06024
[1mStep[0m  [5/53], [94mLoss[0m : 9.98808
[1mStep[0m  [10/53], [94mLoss[0m : 9.58263
[1mStep[0m  [15/53], [94mLoss[0m : 9.63082
[1mStep[0m  [20/53], [94mLoss[0m : 9.61533
[1mStep[0m  [25/53], [94mLoss[0m : 10.02128
[1mStep[0m  [30/53], [94mLoss[0m : 9.75479
[1mStep[0m  [35/53], [94mLoss[0m : 9.91914
[1mStep[0m  [40/53], [94mLoss[0m : 9.78957
[1mStep[0m  [45/53], [94mLoss[0m : 9.72942
[1mStep[0m  [50/53], [94mLoss[0m : 9.76137

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.817, [92mTest[0m: 10.179, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.90269
[1mStep[0m  [5/53], [94mLoss[0m : 9.95275
[1mStep[0m  [10/53], [94mLoss[0m : 9.60532
[1mStep[0m  [15/53], [94mLoss[0m : 9.87210
[1mStep[0m  [20/53], [94mLoss[0m : 9.76694
[1mStep[0m  [25/53], [94mLoss[0m : 9.96728
[1mStep[0m  [30/53], [94mLoss[0m : 10.11742
[1mStep[0m  [35/53], [94mLoss[0m : 9.52406
[1mStep[0m  [40/53], [94mLoss[0m : 9.82998
[1mStep[0m  [45/53], [94mLoss[0m : 9.51980
[1mStep[0m  [50/53], [94mLoss[0m : 9.90567

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.729, [92mTest[0m: 10.131, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.70119
[1mStep[0m  [5/53], [94mLoss[0m : 9.75359
[1mStep[0m  [10/53], [94mLoss[0m : 9.62295
[1mStep[0m  [15/53], [94mLoss[0m : 9.37675
[1mStep[0m  [20/53], [94mLoss[0m : 9.49810
[1mStep[0m  [25/53], [94mLoss[0m : 9.50447
[1mStep[0m  [30/53], [94mLoss[0m : 9.69472
[1mStep[0m  [35/53], [94mLoss[0m : 9.74069
[1mStep[0m  [40/53], [94mLoss[0m : 9.37679
[1mStep[0m  [45/53], [94mLoss[0m : 9.42526
[1mStep[0m  [50/53], [94mLoss[0m : 9.78110

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.632, [92mTest[0m: 10.074, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.40701
[1mStep[0m  [5/53], [94mLoss[0m : 9.69914
[1mStep[0m  [10/53], [94mLoss[0m : 9.33393
[1mStep[0m  [15/53], [94mLoss[0m : 9.08952
[1mStep[0m  [20/53], [94mLoss[0m : 9.79620
[1mStep[0m  [25/53], [94mLoss[0m : 9.88332
[1mStep[0m  [30/53], [94mLoss[0m : 9.65258
[1mStep[0m  [35/53], [94mLoss[0m : 10.08368
[1mStep[0m  [40/53], [94mLoss[0m : 9.17732
[1mStep[0m  [45/53], [94mLoss[0m : 9.72735
[1mStep[0m  [50/53], [94mLoss[0m : 9.59563

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.538, [92mTest[0m: 10.007, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.08097
[1mStep[0m  [5/53], [94mLoss[0m : 9.77541
[1mStep[0m  [10/53], [94mLoss[0m : 9.63798
[1mStep[0m  [15/53], [94mLoss[0m : 9.32827
[1mStep[0m  [20/53], [94mLoss[0m : 9.26654
[1mStep[0m  [25/53], [94mLoss[0m : 9.33222
[1mStep[0m  [30/53], [94mLoss[0m : 9.56952
[1mStep[0m  [35/53], [94mLoss[0m : 9.50019
[1mStep[0m  [40/53], [94mLoss[0m : 9.50772
[1mStep[0m  [45/53], [94mLoss[0m : 9.21663
[1mStep[0m  [50/53], [94mLoss[0m : 9.64748

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.446, [92mTest[0m: 9.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.65798
[1mStep[0m  [5/53], [94mLoss[0m : 9.66906
[1mStep[0m  [10/53], [94mLoss[0m : 9.64799
[1mStep[0m  [15/53], [94mLoss[0m : 9.47886
[1mStep[0m  [20/53], [94mLoss[0m : 9.07140
[1mStep[0m  [25/53], [94mLoss[0m : 8.98250
[1mStep[0m  [30/53], [94mLoss[0m : 9.32065
[1mStep[0m  [35/53], [94mLoss[0m : 9.58058
[1mStep[0m  [40/53], [94mLoss[0m : 9.55010
[1mStep[0m  [45/53], [94mLoss[0m : 9.08023
[1mStep[0m  [50/53], [94mLoss[0m : 9.49727

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.366, [92mTest[0m: 9.895, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.96371
[1mStep[0m  [5/53], [94mLoss[0m : 9.85248
[1mStep[0m  [10/53], [94mLoss[0m : 9.42626
[1mStep[0m  [15/53], [94mLoss[0m : 9.31757
[1mStep[0m  [20/53], [94mLoss[0m : 9.52860
[1mStep[0m  [25/53], [94mLoss[0m : 9.45276
[1mStep[0m  [30/53], [94mLoss[0m : 9.18511
[1mStep[0m  [35/53], [94mLoss[0m : 9.04291
[1mStep[0m  [40/53], [94mLoss[0m : 9.48333
[1mStep[0m  [45/53], [94mLoss[0m : 9.20823
[1mStep[0m  [50/53], [94mLoss[0m : 9.21807

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.270, [92mTest[0m: 9.823, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.16934
[1mStep[0m  [5/53], [94mLoss[0m : 9.34409
[1mStep[0m  [10/53], [94mLoss[0m : 9.45502
[1mStep[0m  [15/53], [94mLoss[0m : 9.19042
[1mStep[0m  [20/53], [94mLoss[0m : 9.14284
[1mStep[0m  [25/53], [94mLoss[0m : 9.20620
[1mStep[0m  [30/53], [94mLoss[0m : 9.18355
[1mStep[0m  [35/53], [94mLoss[0m : 8.88252
[1mStep[0m  [40/53], [94mLoss[0m : 9.44736
[1mStep[0m  [45/53], [94mLoss[0m : 9.01395
[1mStep[0m  [50/53], [94mLoss[0m : 9.20094

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.165, [92mTest[0m: 9.751, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.14869
[1mStep[0m  [5/53], [94mLoss[0m : 9.09183
[1mStep[0m  [10/53], [94mLoss[0m : 8.72609
[1mStep[0m  [15/53], [94mLoss[0m : 9.08829
[1mStep[0m  [20/53], [94mLoss[0m : 8.93613
[1mStep[0m  [25/53], [94mLoss[0m : 9.14106
[1mStep[0m  [30/53], [94mLoss[0m : 9.11281
[1mStep[0m  [35/53], [94mLoss[0m : 8.86152
[1mStep[0m  [40/53], [94mLoss[0m : 8.75456
[1mStep[0m  [45/53], [94mLoss[0m : 8.94257
[1mStep[0m  [50/53], [94mLoss[0m : 9.12171

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.080, [92mTest[0m: 9.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.29893
[1mStep[0m  [5/53], [94mLoss[0m : 8.91465
[1mStep[0m  [10/53], [94mLoss[0m : 8.73606
[1mStep[0m  [15/53], [94mLoss[0m : 9.04607
[1mStep[0m  [20/53], [94mLoss[0m : 8.93633
[1mStep[0m  [25/53], [94mLoss[0m : 8.58775
[1mStep[0m  [30/53], [94mLoss[0m : 8.65616
[1mStep[0m  [35/53], [94mLoss[0m : 9.05200
[1mStep[0m  [40/53], [94mLoss[0m : 9.23031
[1mStep[0m  [45/53], [94mLoss[0m : 9.17338
[1mStep[0m  [50/53], [94mLoss[0m : 9.21329

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.983, [92mTest[0m: 9.647, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.72053
[1mStep[0m  [5/53], [94mLoss[0m : 8.70559
[1mStep[0m  [10/53], [94mLoss[0m : 8.93077
[1mStep[0m  [15/53], [94mLoss[0m : 9.20876
[1mStep[0m  [20/53], [94mLoss[0m : 8.79216
[1mStep[0m  [25/53], [94mLoss[0m : 9.00747
[1mStep[0m  [30/53], [94mLoss[0m : 8.85168
[1mStep[0m  [35/53], [94mLoss[0m : 9.04745
[1mStep[0m  [40/53], [94mLoss[0m : 8.93359
[1mStep[0m  [45/53], [94mLoss[0m : 9.13936
[1mStep[0m  [50/53], [94mLoss[0m : 8.98725

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.894, [92mTest[0m: 9.591, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.89182
[1mStep[0m  [5/53], [94mLoss[0m : 8.57247
[1mStep[0m  [10/53], [94mLoss[0m : 8.44360
[1mStep[0m  [15/53], [94mLoss[0m : 9.04431
[1mStep[0m  [20/53], [94mLoss[0m : 8.26079
[1mStep[0m  [25/53], [94mLoss[0m : 8.58157
[1mStep[0m  [30/53], [94mLoss[0m : 8.73678
[1mStep[0m  [35/53], [94mLoss[0m : 8.32562
[1mStep[0m  [40/53], [94mLoss[0m : 9.07391
[1mStep[0m  [45/53], [94mLoss[0m : 8.78821
[1mStep[0m  [50/53], [94mLoss[0m : 8.98485

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.801, [92mTest[0m: 9.526, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.60624
[1mStep[0m  [5/53], [94mLoss[0m : 9.16114
[1mStep[0m  [10/53], [94mLoss[0m : 8.62815
[1mStep[0m  [15/53], [94mLoss[0m : 8.68382
[1mStep[0m  [20/53], [94mLoss[0m : 8.98448
[1mStep[0m  [25/53], [94mLoss[0m : 8.61818
[1mStep[0m  [30/53], [94mLoss[0m : 8.63306
[1mStep[0m  [35/53], [94mLoss[0m : 8.99218
[1mStep[0m  [40/53], [94mLoss[0m : 8.65743
[1mStep[0m  [45/53], [94mLoss[0m : 8.61147
[1mStep[0m  [50/53], [94mLoss[0m : 8.51101

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.724, [92mTest[0m: 9.464, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.33428
[1mStep[0m  [5/53], [94mLoss[0m : 8.94274
[1mStep[0m  [10/53], [94mLoss[0m : 8.34666
[1mStep[0m  [15/53], [94mLoss[0m : 8.49694
[1mStep[0m  [20/53], [94mLoss[0m : 9.19403
[1mStep[0m  [25/53], [94mLoss[0m : 8.75100
[1mStep[0m  [30/53], [94mLoss[0m : 8.87197
[1mStep[0m  [35/53], [94mLoss[0m : 8.23681
[1mStep[0m  [40/53], [94mLoss[0m : 8.69513
[1mStep[0m  [45/53], [94mLoss[0m : 8.34338
[1mStep[0m  [50/53], [94mLoss[0m : 8.68417

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.632, [92mTest[0m: 9.400, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.35206
[1mStep[0m  [5/53], [94mLoss[0m : 8.93532
[1mStep[0m  [10/53], [94mLoss[0m : 8.84349
[1mStep[0m  [15/53], [94mLoss[0m : 8.35217
[1mStep[0m  [20/53], [94mLoss[0m : 8.42140
[1mStep[0m  [25/53], [94mLoss[0m : 8.46909
[1mStep[0m  [30/53], [94mLoss[0m : 8.56515
[1mStep[0m  [35/53], [94mLoss[0m : 8.69291
[1mStep[0m  [40/53], [94mLoss[0m : 8.56207
[1mStep[0m  [45/53], [94mLoss[0m : 8.37863
[1mStep[0m  [50/53], [94mLoss[0m : 8.85238

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.546, [92mTest[0m: 9.334, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.54687
[1mStep[0m  [5/53], [94mLoss[0m : 8.68872
[1mStep[0m  [10/53], [94mLoss[0m : 8.77280
[1mStep[0m  [15/53], [94mLoss[0m : 8.51304
[1mStep[0m  [20/53], [94mLoss[0m : 8.69132
[1mStep[0m  [25/53], [94mLoss[0m : 8.14213
[1mStep[0m  [30/53], [94mLoss[0m : 8.37698
[1mStep[0m  [35/53], [94mLoss[0m : 8.49129
[1mStep[0m  [40/53], [94mLoss[0m : 8.18046
[1mStep[0m  [45/53], [94mLoss[0m : 8.28369
[1mStep[0m  [50/53], [94mLoss[0m : 8.39014

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.472, [92mTest[0m: 9.289, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.46138
[1mStep[0m  [5/53], [94mLoss[0m : 8.48875
[1mStep[0m  [10/53], [94mLoss[0m : 8.18814
[1mStep[0m  [15/53], [94mLoss[0m : 8.59998
[1mStep[0m  [20/53], [94mLoss[0m : 8.04955
[1mStep[0m  [25/53], [94mLoss[0m : 8.24642
[1mStep[0m  [30/53], [94mLoss[0m : 8.44460
[1mStep[0m  [35/53], [94mLoss[0m : 8.62672
[1mStep[0m  [40/53], [94mLoss[0m : 8.25845
[1mStep[0m  [45/53], [94mLoss[0m : 8.49186
[1mStep[0m  [50/53], [94mLoss[0m : 8.12603

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.385, [92mTest[0m: 9.225, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.38768
[1mStep[0m  [5/53], [94mLoss[0m : 8.09722
[1mStep[0m  [10/53], [94mLoss[0m : 8.39732
[1mStep[0m  [15/53], [94mLoss[0m : 8.78154
[1mStep[0m  [20/53], [94mLoss[0m : 8.22481
[1mStep[0m  [25/53], [94mLoss[0m : 8.17100
[1mStep[0m  [30/53], [94mLoss[0m : 8.38204
[1mStep[0m  [35/53], [94mLoss[0m : 8.51736
[1mStep[0m  [40/53], [94mLoss[0m : 8.37373
[1mStep[0m  [45/53], [94mLoss[0m : 8.39535
[1mStep[0m  [50/53], [94mLoss[0m : 8.00346

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.287, [92mTest[0m: 9.189, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.03888
[1mStep[0m  [5/53], [94mLoss[0m : 8.20924
[1mStep[0m  [10/53], [94mLoss[0m : 8.24575
[1mStep[0m  [15/53], [94mLoss[0m : 8.04459
[1mStep[0m  [20/53], [94mLoss[0m : 8.65652
[1mStep[0m  [25/53], [94mLoss[0m : 8.31820
[1mStep[0m  [30/53], [94mLoss[0m : 8.40689
[1mStep[0m  [35/53], [94mLoss[0m : 8.39247
[1mStep[0m  [40/53], [94mLoss[0m : 8.04533
[1mStep[0m  [45/53], [94mLoss[0m : 8.45370
[1mStep[0m  [50/53], [94mLoss[0m : 7.98366

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.206, [92mTest[0m: 9.124, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.29063
[1mStep[0m  [5/53], [94mLoss[0m : 8.03129
[1mStep[0m  [10/53], [94mLoss[0m : 8.00261
[1mStep[0m  [15/53], [94mLoss[0m : 8.25222
[1mStep[0m  [20/53], [94mLoss[0m : 8.12142
[1mStep[0m  [25/53], [94mLoss[0m : 8.11235
[1mStep[0m  [30/53], [94mLoss[0m : 8.19215
[1mStep[0m  [35/53], [94mLoss[0m : 7.92230
[1mStep[0m  [40/53], [94mLoss[0m : 8.21212
[1mStep[0m  [45/53], [94mLoss[0m : 7.61762
[1mStep[0m  [50/53], [94mLoss[0m : 8.16157

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.117, [92mTest[0m: 9.041, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.997
====================================

Phase 1 - Evaluation MAE:  8.997216518108662
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 8.06741
[1mStep[0m  [5/53], [94mLoss[0m : 8.17769
[1mStep[0m  [10/53], [94mLoss[0m : 7.66142
[1mStep[0m  [15/53], [94mLoss[0m : 8.12010
[1mStep[0m  [20/53], [94mLoss[0m : 7.82974
[1mStep[0m  [25/53], [94mLoss[0m : 8.09483
[1mStep[0m  [30/53], [94mLoss[0m : 7.87539
[1mStep[0m  [35/53], [94mLoss[0m : 8.05313
[1mStep[0m  [40/53], [94mLoss[0m : 8.37858
[1mStep[0m  [45/53], [94mLoss[0m : 7.98665
[1mStep[0m  [50/53], [94mLoss[0m : 7.89566

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.029, [92mTest[0m: 8.979, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.97288
[1mStep[0m  [5/53], [94mLoss[0m : 8.07285
[1mStep[0m  [10/53], [94mLoss[0m : 8.02024
[1mStep[0m  [15/53], [94mLoss[0m : 7.83605
[1mStep[0m  [20/53], [94mLoss[0m : 7.58759
[1mStep[0m  [25/53], [94mLoss[0m : 7.96041
[1mStep[0m  [30/53], [94mLoss[0m : 7.87018
[1mStep[0m  [35/53], [94mLoss[0m : 7.82764
[1mStep[0m  [40/53], [94mLoss[0m : 7.97717
[1mStep[0m  [45/53], [94mLoss[0m : 7.78833
[1mStep[0m  [50/53], [94mLoss[0m : 7.69249

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.916, [92mTest[0m: 8.902, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.32564
[1mStep[0m  [5/53], [94mLoss[0m : 7.86814
[1mStep[0m  [10/53], [94mLoss[0m : 7.86057
[1mStep[0m  [15/53], [94mLoss[0m : 8.15070
[1mStep[0m  [20/53], [94mLoss[0m : 7.98571
[1mStep[0m  [25/53], [94mLoss[0m : 7.76083
[1mStep[0m  [30/53], [94mLoss[0m : 8.06155
[1mStep[0m  [35/53], [94mLoss[0m : 7.73105
[1mStep[0m  [40/53], [94mLoss[0m : 8.10308
[1mStep[0m  [45/53], [94mLoss[0m : 7.67926
[1mStep[0m  [50/53], [94mLoss[0m : 8.17348

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.830, [92mTest[0m: 8.831, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.92791
[1mStep[0m  [5/53], [94mLoss[0m : 8.25848
[1mStep[0m  [10/53], [94mLoss[0m : 7.64955
[1mStep[0m  [15/53], [94mLoss[0m : 7.83222
[1mStep[0m  [20/53], [94mLoss[0m : 7.82329
[1mStep[0m  [25/53], [94mLoss[0m : 7.77500
[1mStep[0m  [30/53], [94mLoss[0m : 7.59969
[1mStep[0m  [35/53], [94mLoss[0m : 7.24722
[1mStep[0m  [40/53], [94mLoss[0m : 7.75405
[1mStep[0m  [45/53], [94mLoss[0m : 7.49305
[1mStep[0m  [50/53], [94mLoss[0m : 7.44411

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.712, [92mTest[0m: 8.760, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.71051
[1mStep[0m  [5/53], [94mLoss[0m : 7.87057
[1mStep[0m  [10/53], [94mLoss[0m : 7.84939
[1mStep[0m  [15/53], [94mLoss[0m : 7.62119
[1mStep[0m  [20/53], [94mLoss[0m : 7.60873
[1mStep[0m  [25/53], [94mLoss[0m : 7.76091
[1mStep[0m  [30/53], [94mLoss[0m : 7.48547
[1mStep[0m  [35/53], [94mLoss[0m : 7.24104
[1mStep[0m  [40/53], [94mLoss[0m : 7.59883
[1mStep[0m  [45/53], [94mLoss[0m : 7.43097
[1mStep[0m  [50/53], [94mLoss[0m : 7.58377

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.628, [92mTest[0m: 8.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.69292
[1mStep[0m  [5/53], [94mLoss[0m : 7.73975
[1mStep[0m  [10/53], [94mLoss[0m : 7.66355
[1mStep[0m  [15/53], [94mLoss[0m : 7.50317
[1mStep[0m  [20/53], [94mLoss[0m : 7.36134
[1mStep[0m  [25/53], [94mLoss[0m : 7.33022
[1mStep[0m  [30/53], [94mLoss[0m : 7.63300
[1mStep[0m  [35/53], [94mLoss[0m : 7.68501
[1mStep[0m  [40/53], [94mLoss[0m : 7.72316
[1mStep[0m  [45/53], [94mLoss[0m : 7.47668
[1mStep[0m  [50/53], [94mLoss[0m : 7.66101

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.523, [92mTest[0m: 8.625, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.18700
[1mStep[0m  [5/53], [94mLoss[0m : 7.81808
[1mStep[0m  [10/53], [94mLoss[0m : 7.76710
[1mStep[0m  [15/53], [94mLoss[0m : 7.43311
[1mStep[0m  [20/53], [94mLoss[0m : 7.49952
[1mStep[0m  [25/53], [94mLoss[0m : 7.47589
[1mStep[0m  [30/53], [94mLoss[0m : 7.85627
[1mStep[0m  [35/53], [94mLoss[0m : 7.18667
[1mStep[0m  [40/53], [94mLoss[0m : 7.31139
[1mStep[0m  [45/53], [94mLoss[0m : 7.59857
[1mStep[0m  [50/53], [94mLoss[0m : 7.36765

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.421, [92mTest[0m: 8.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.85685
[1mStep[0m  [5/53], [94mLoss[0m : 7.32102
[1mStep[0m  [10/53], [94mLoss[0m : 7.34686
[1mStep[0m  [15/53], [94mLoss[0m : 7.21184
[1mStep[0m  [20/53], [94mLoss[0m : 7.24016
[1mStep[0m  [25/53], [94mLoss[0m : 7.04621
[1mStep[0m  [30/53], [94mLoss[0m : 7.60616
[1mStep[0m  [35/53], [94mLoss[0m : 7.56618
[1mStep[0m  [40/53], [94mLoss[0m : 7.37303
[1mStep[0m  [45/53], [94mLoss[0m : 7.22904
[1mStep[0m  [50/53], [94mLoss[0m : 7.12836

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.320, [92mTest[0m: 8.465, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 7 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.357
====================================

Phase 2 - Evaluation MAE:  8.357229104408852
MAE score P1       8.997217
MAE score P2       8.357229
loss               7.320491
learning_rate        0.0001
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.76156
[1mStep[0m  [2/26], [94mLoss[0m : 10.77514
[1mStep[0m  [4/26], [94mLoss[0m : 10.83483
[1mStep[0m  [6/26], [94mLoss[0m : 10.78539
[1mStep[0m  [8/26], [94mLoss[0m : 10.69099
[1mStep[0m  [10/26], [94mLoss[0m : 10.85842
[1mStep[0m  [12/26], [94mLoss[0m : 10.53179
[1mStep[0m  [14/26], [94mLoss[0m : 10.47163
[1mStep[0m  [16/26], [94mLoss[0m : 10.51467
[1mStep[0m  [18/26], [94mLoss[0m : 10.77637
[1mStep[0m  [20/26], [94mLoss[0m : 10.16829
[1mStep[0m  [22/26], [94mLoss[0m : 10.28667
[1mStep[0m  [24/26], [94mLoss[0m : 10.56432

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.623, [92mTest[0m: 10.819, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.27483
[1mStep[0m  [2/26], [94mLoss[0m : 10.20532
[1mStep[0m  [4/26], [94mLoss[0m : 10.59894
[1mStep[0m  [6/26], [94mLoss[0m : 10.43631
[1mStep[0m  [8/26], [94mLoss[0m : 10.31032
[1mStep[0m  [10/26], [94mLoss[0m : 10.43707
[1mStep[0m  [12/26], [94mLoss[0m : 10.27906
[1mStep[0m  [14/26], [94mLoss[0m : 10.38664
[1mStep[0m  [16/26], [94mLoss[0m : 10.37577
[1mStep[0m  [18/26], [94mLoss[0m : 9.97285
[1mStep[0m  [20/26], [94mLoss[0m : 10.22514
[1mStep[0m  [22/26], [94mLoss[0m : 10.16489
[1mStep[0m  [24/26], [94mLoss[0m : 10.02107

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.261, [92mTest[0m: 10.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.20142
[1mStep[0m  [2/26], [94mLoss[0m : 9.95531
[1mStep[0m  [4/26], [94mLoss[0m : 9.81520
[1mStep[0m  [6/26], [94mLoss[0m : 10.05152
[1mStep[0m  [8/26], [94mLoss[0m : 9.94621
[1mStep[0m  [10/26], [94mLoss[0m : 10.02855
[1mStep[0m  [12/26], [94mLoss[0m : 9.80547
[1mStep[0m  [14/26], [94mLoss[0m : 9.88873
[1mStep[0m  [16/26], [94mLoss[0m : 9.70074
[1mStep[0m  [18/26], [94mLoss[0m : 9.56355
[1mStep[0m  [20/26], [94mLoss[0m : 9.81963
[1mStep[0m  [22/26], [94mLoss[0m : 9.91624
[1mStep[0m  [24/26], [94mLoss[0m : 9.71796

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.848, [92mTest[0m: 10.221, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.60382
[1mStep[0m  [2/26], [94mLoss[0m : 9.79610
[1mStep[0m  [4/26], [94mLoss[0m : 9.59746
[1mStep[0m  [6/26], [94mLoss[0m : 9.60441
[1mStep[0m  [8/26], [94mLoss[0m : 9.39375
[1mStep[0m  [10/26], [94mLoss[0m : 9.41911
[1mStep[0m  [12/26], [94mLoss[0m : 9.79507
[1mStep[0m  [14/26], [94mLoss[0m : 9.35528
[1mStep[0m  [16/26], [94mLoss[0m : 9.35053
[1mStep[0m  [18/26], [94mLoss[0m : 9.42761
[1mStep[0m  [20/26], [94mLoss[0m : 9.06537
[1mStep[0m  [22/26], [94mLoss[0m : 9.18214
[1mStep[0m  [24/26], [94mLoss[0m : 9.13600

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.431, [92mTest[0m: 9.916, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.31985
[1mStep[0m  [2/26], [94mLoss[0m : 9.24347
[1mStep[0m  [4/26], [94mLoss[0m : 9.06380
[1mStep[0m  [6/26], [94mLoss[0m : 9.16044
[1mStep[0m  [8/26], [94mLoss[0m : 8.95416
[1mStep[0m  [10/26], [94mLoss[0m : 9.05708
[1mStep[0m  [12/26], [94mLoss[0m : 9.08490
[1mStep[0m  [14/26], [94mLoss[0m : 9.09651
[1mStep[0m  [16/26], [94mLoss[0m : 8.97093
[1mStep[0m  [18/26], [94mLoss[0m : 8.79832
[1mStep[0m  [20/26], [94mLoss[0m : 8.96147
[1mStep[0m  [22/26], [94mLoss[0m : 8.94752
[1mStep[0m  [24/26], [94mLoss[0m : 8.87981

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.034, [92mTest[0m: 9.597, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.49666
[1mStep[0m  [2/26], [94mLoss[0m : 8.78212
[1mStep[0m  [4/26], [94mLoss[0m : 8.61861
[1mStep[0m  [6/26], [94mLoss[0m : 8.54975
[1mStep[0m  [8/26], [94mLoss[0m : 8.58134
[1mStep[0m  [10/26], [94mLoss[0m : 8.82759
[1mStep[0m  [12/26], [94mLoss[0m : 8.78089
[1mStep[0m  [14/26], [94mLoss[0m : 8.52801
[1mStep[0m  [16/26], [94mLoss[0m : 8.50982
[1mStep[0m  [18/26], [94mLoss[0m : 8.42639
[1mStep[0m  [20/26], [94mLoss[0m : 8.45797
[1mStep[0m  [22/26], [94mLoss[0m : 8.69478
[1mStep[0m  [24/26], [94mLoss[0m : 8.60454

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.620, [92mTest[0m: 9.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.33739
[1mStep[0m  [2/26], [94mLoss[0m : 8.18927
[1mStep[0m  [4/26], [94mLoss[0m : 8.50173
[1mStep[0m  [6/26], [94mLoss[0m : 8.32035
[1mStep[0m  [8/26], [94mLoss[0m : 8.14017
[1mStep[0m  [10/26], [94mLoss[0m : 8.10920
[1mStep[0m  [12/26], [94mLoss[0m : 7.98028
[1mStep[0m  [14/26], [94mLoss[0m : 8.15155
[1mStep[0m  [16/26], [94mLoss[0m : 8.35274
[1mStep[0m  [18/26], [94mLoss[0m : 7.99914
[1mStep[0m  [20/26], [94mLoss[0m : 8.18714
[1mStep[0m  [22/26], [94mLoss[0m : 8.15048
[1mStep[0m  [24/26], [94mLoss[0m : 8.18024

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.196, [92mTest[0m: 8.988, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.92793
[1mStep[0m  [2/26], [94mLoss[0m : 8.03838
[1mStep[0m  [4/26], [94mLoss[0m : 7.81037
[1mStep[0m  [6/26], [94mLoss[0m : 7.83672
[1mStep[0m  [8/26], [94mLoss[0m : 7.85524
[1mStep[0m  [10/26], [94mLoss[0m : 7.68601
[1mStep[0m  [12/26], [94mLoss[0m : 7.69773
[1mStep[0m  [14/26], [94mLoss[0m : 7.62123
[1mStep[0m  [16/26], [94mLoss[0m : 7.65063
[1mStep[0m  [18/26], [94mLoss[0m : 7.59672
[1mStep[0m  [20/26], [94mLoss[0m : 7.55122
[1mStep[0m  [22/26], [94mLoss[0m : 7.71122
[1mStep[0m  [24/26], [94mLoss[0m : 7.42137

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.784, [92mTest[0m: 8.674, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.58150
[1mStep[0m  [2/26], [94mLoss[0m : 7.53684
[1mStep[0m  [4/26], [94mLoss[0m : 7.66796
[1mStep[0m  [6/26], [94mLoss[0m : 7.69100
[1mStep[0m  [8/26], [94mLoss[0m : 7.27016
[1mStep[0m  [10/26], [94mLoss[0m : 7.56268
[1mStep[0m  [12/26], [94mLoss[0m : 7.39697
[1mStep[0m  [14/26], [94mLoss[0m : 7.39898
[1mStep[0m  [16/26], [94mLoss[0m : 7.25448
[1mStep[0m  [18/26], [94mLoss[0m : 7.31097
[1mStep[0m  [20/26], [94mLoss[0m : 7.39577
[1mStep[0m  [22/26], [94mLoss[0m : 7.07737
[1mStep[0m  [24/26], [94mLoss[0m : 6.98561

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.377, [92mTest[0m: 8.352, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.02996
[1mStep[0m  [2/26], [94mLoss[0m : 7.34953
[1mStep[0m  [4/26], [94mLoss[0m : 7.30268
[1mStep[0m  [6/26], [94mLoss[0m : 6.91581
[1mStep[0m  [8/26], [94mLoss[0m : 6.83943
[1mStep[0m  [10/26], [94mLoss[0m : 7.17552
[1mStep[0m  [12/26], [94mLoss[0m : 7.01911
[1mStep[0m  [14/26], [94mLoss[0m : 6.94967
[1mStep[0m  [16/26], [94mLoss[0m : 7.20287
[1mStep[0m  [18/26], [94mLoss[0m : 6.81654
[1mStep[0m  [20/26], [94mLoss[0m : 6.54058
[1mStep[0m  [22/26], [94mLoss[0m : 6.75160
[1mStep[0m  [24/26], [94mLoss[0m : 7.02011

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.964, [92mTest[0m: 8.034, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.61097
[1mStep[0m  [2/26], [94mLoss[0m : 6.63495
[1mStep[0m  [4/26], [94mLoss[0m : 6.61572
[1mStep[0m  [6/26], [94mLoss[0m : 6.71855
[1mStep[0m  [8/26], [94mLoss[0m : 6.53109
[1mStep[0m  [10/26], [94mLoss[0m : 6.24251
[1mStep[0m  [12/26], [94mLoss[0m : 6.71209
[1mStep[0m  [14/26], [94mLoss[0m : 6.44322
[1mStep[0m  [16/26], [94mLoss[0m : 6.39311
[1mStep[0m  [18/26], [94mLoss[0m : 6.50123
[1mStep[0m  [20/26], [94mLoss[0m : 6.22158
[1mStep[0m  [22/26], [94mLoss[0m : 6.50528
[1mStep[0m  [24/26], [94mLoss[0m : 6.52553

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.552, [92mTest[0m: 7.687, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.19639
[1mStep[0m  [2/26], [94mLoss[0m : 6.35611
[1mStep[0m  [4/26], [94mLoss[0m : 6.23921
[1mStep[0m  [6/26], [94mLoss[0m : 6.34475
[1mStep[0m  [8/26], [94mLoss[0m : 6.17326
[1mStep[0m  [10/26], [94mLoss[0m : 5.95050
[1mStep[0m  [12/26], [94mLoss[0m : 6.26250
[1mStep[0m  [14/26], [94mLoss[0m : 6.33765
[1mStep[0m  [16/26], [94mLoss[0m : 6.10215
[1mStep[0m  [18/26], [94mLoss[0m : 6.16166
[1mStep[0m  [20/26], [94mLoss[0m : 6.10854
[1mStep[0m  [22/26], [94mLoss[0m : 5.77131
[1mStep[0m  [24/26], [94mLoss[0m : 5.99338

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.165, [92mTest[0m: 7.357, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.86817
[1mStep[0m  [2/26], [94mLoss[0m : 5.96442
[1mStep[0m  [4/26], [94mLoss[0m : 5.69545
[1mStep[0m  [6/26], [94mLoss[0m : 5.92693
[1mStep[0m  [8/26], [94mLoss[0m : 5.44292
[1mStep[0m  [10/26], [94mLoss[0m : 5.87713
[1mStep[0m  [12/26], [94mLoss[0m : 5.73358
[1mStep[0m  [14/26], [94mLoss[0m : 5.82162
[1mStep[0m  [16/26], [94mLoss[0m : 5.77360
[1mStep[0m  [18/26], [94mLoss[0m : 5.87082
[1mStep[0m  [20/26], [94mLoss[0m : 5.70644
[1mStep[0m  [22/26], [94mLoss[0m : 5.88346
[1mStep[0m  [24/26], [94mLoss[0m : 5.40441

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.765, [92mTest[0m: 7.016, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.32009
[1mStep[0m  [2/26], [94mLoss[0m : 5.16303
[1mStep[0m  [4/26], [94mLoss[0m : 5.55334
[1mStep[0m  [6/26], [94mLoss[0m : 5.26007
[1mStep[0m  [8/26], [94mLoss[0m : 5.35995
[1mStep[0m  [10/26], [94mLoss[0m : 5.52624
[1mStep[0m  [12/26], [94mLoss[0m : 5.11658
[1mStep[0m  [14/26], [94mLoss[0m : 5.29591
[1mStep[0m  [16/26], [94mLoss[0m : 5.62254
[1mStep[0m  [18/26], [94mLoss[0m : 5.14516
[1mStep[0m  [20/26], [94mLoss[0m : 5.39383
[1mStep[0m  [22/26], [94mLoss[0m : 5.20002
[1mStep[0m  [24/26], [94mLoss[0m : 5.37544

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.381, [92mTest[0m: 6.636, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.18614
[1mStep[0m  [2/26], [94mLoss[0m : 4.92526
[1mStep[0m  [4/26], [94mLoss[0m : 5.38464
[1mStep[0m  [6/26], [94mLoss[0m : 5.02649
[1mStep[0m  [8/26], [94mLoss[0m : 5.29214
[1mStep[0m  [10/26], [94mLoss[0m : 4.93944
[1mStep[0m  [12/26], [94mLoss[0m : 5.04032
[1mStep[0m  [14/26], [94mLoss[0m : 5.03211
[1mStep[0m  [16/26], [94mLoss[0m : 4.83363
[1mStep[0m  [18/26], [94mLoss[0m : 5.17724
[1mStep[0m  [20/26], [94mLoss[0m : 4.95157
[1mStep[0m  [22/26], [94mLoss[0m : 5.08090
[1mStep[0m  [24/26], [94mLoss[0m : 5.04815

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.014, [92mTest[0m: 6.259, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 14 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.852
====================================

Phase 1 - Evaluation MAE:  5.85247567983774
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 4.87862
[1mStep[0m  [2/26], [94mLoss[0m : 4.69059
[1mStep[0m  [4/26], [94mLoss[0m : 4.96690
[1mStep[0m  [6/26], [94mLoss[0m : 4.56860
[1mStep[0m  [8/26], [94mLoss[0m : 5.04712
[1mStep[0m  [10/26], [94mLoss[0m : 4.53240
[1mStep[0m  [12/26], [94mLoss[0m : 4.88393
[1mStep[0m  [14/26], [94mLoss[0m : 4.75296
[1mStep[0m  [16/26], [94mLoss[0m : 4.75678
[1mStep[0m  [18/26], [94mLoss[0m : 4.49398
[1mStep[0m  [20/26], [94mLoss[0m : 4.49838
[1mStep[0m  [22/26], [94mLoss[0m : 4.48716
[1mStep[0m  [24/26], [94mLoss[0m : 4.49550

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.724, [92mTest[0m: 5.852, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.733
====================================

Phase 2 - Evaluation MAE:  5.733128951146052
MAE score P1        5.852476
MAE score P2        5.733129
loss                4.723648
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay          0.0001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 11.07289
[1mStep[0m  [5/53], [94mLoss[0m : 10.88437
[1mStep[0m  [10/53], [94mLoss[0m : 10.75175
[1mStep[0m  [15/53], [94mLoss[0m : 10.67741
[1mStep[0m  [20/53], [94mLoss[0m : 10.51349
[1mStep[0m  [25/53], [94mLoss[0m : 10.63228
[1mStep[0m  [30/53], [94mLoss[0m : 10.71863
[1mStep[0m  [35/53], [94mLoss[0m : 10.84068
[1mStep[0m  [40/53], [94mLoss[0m : 10.12896
[1mStep[0m  [45/53], [94mLoss[0m : 10.30949
[1mStep[0m  [50/53], [94mLoss[0m : 10.32218

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.650, [92mTest[0m: 10.872, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.16615
[1mStep[0m  [5/53], [94mLoss[0m : 10.74550
[1mStep[0m  [10/53], [94mLoss[0m : 9.69462
[1mStep[0m  [15/53], [94mLoss[0m : 10.35065
[1mStep[0m  [20/53], [94mLoss[0m : 10.40762
[1mStep[0m  [25/53], [94mLoss[0m : 10.16889
[1mStep[0m  [30/53], [94mLoss[0m : 9.85600
[1mStep[0m  [35/53], [94mLoss[0m : 9.43865
[1mStep[0m  [40/53], [94mLoss[0m : 9.64055
[1mStep[0m  [45/53], [94mLoss[0m : 9.20724
[1mStep[0m  [50/53], [94mLoss[0m : 9.10936

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.849, [92mTest[0m: 10.460, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.43889
[1mStep[0m  [5/53], [94mLoss[0m : 9.42987
[1mStep[0m  [10/53], [94mLoss[0m : 9.14872
[1mStep[0m  [15/53], [94mLoss[0m : 9.13216
[1mStep[0m  [20/53], [94mLoss[0m : 8.79104
[1mStep[0m  [25/53], [94mLoss[0m : 8.89100
[1mStep[0m  [30/53], [94mLoss[0m : 9.09441
[1mStep[0m  [35/53], [94mLoss[0m : 8.66669
[1mStep[0m  [40/53], [94mLoss[0m : 9.22706
[1mStep[0m  [45/53], [94mLoss[0m : 8.58175
[1mStep[0m  [50/53], [94mLoss[0m : 8.39646

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.013, [92mTest[0m: 9.930, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.04071
[1mStep[0m  [5/53], [94mLoss[0m : 8.54113
[1mStep[0m  [10/53], [94mLoss[0m : 8.51726
[1mStep[0m  [15/53], [94mLoss[0m : 8.50900
[1mStep[0m  [20/53], [94mLoss[0m : 7.90310
[1mStep[0m  [25/53], [94mLoss[0m : 8.36681
[1mStep[0m  [30/53], [94mLoss[0m : 8.11625
[1mStep[0m  [35/53], [94mLoss[0m : 8.16465
[1mStep[0m  [40/53], [94mLoss[0m : 7.91449
[1mStep[0m  [45/53], [94mLoss[0m : 7.71495
[1mStep[0m  [50/53], [94mLoss[0m : 7.62018

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.163, [92mTest[0m: 9.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.70656
[1mStep[0m  [5/53], [94mLoss[0m : 7.60823
[1mStep[0m  [10/53], [94mLoss[0m : 7.46799
[1mStep[0m  [15/53], [94mLoss[0m : 7.40942
[1mStep[0m  [20/53], [94mLoss[0m : 6.82204
[1mStep[0m  [25/53], [94mLoss[0m : 7.42275
[1mStep[0m  [30/53], [94mLoss[0m : 7.07522
[1mStep[0m  [35/53], [94mLoss[0m : 7.09988
[1mStep[0m  [40/53], [94mLoss[0m : 7.13480
[1mStep[0m  [45/53], [94mLoss[0m : 7.22734
[1mStep[0m  [50/53], [94mLoss[0m : 6.92867

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.336, [92mTest[0m: 8.855, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.10573
[1mStep[0m  [5/53], [94mLoss[0m : 7.04801
[1mStep[0m  [10/53], [94mLoss[0m : 6.74068
[1mStep[0m  [15/53], [94mLoss[0m : 6.88139
[1mStep[0m  [20/53], [94mLoss[0m : 6.66763
[1mStep[0m  [25/53], [94mLoss[0m : 6.70481
[1mStep[0m  [30/53], [94mLoss[0m : 6.28535
[1mStep[0m  [35/53], [94mLoss[0m : 6.13083
[1mStep[0m  [40/53], [94mLoss[0m : 6.35774
[1mStep[0m  [45/53], [94mLoss[0m : 6.04063
[1mStep[0m  [50/53], [94mLoss[0m : 5.68733

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.541, [92mTest[0m: 8.311, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.21450
[1mStep[0m  [5/53], [94mLoss[0m : 5.93680
[1mStep[0m  [10/53], [94mLoss[0m : 5.51410
[1mStep[0m  [15/53], [94mLoss[0m : 5.75468
[1mStep[0m  [20/53], [94mLoss[0m : 5.68229
[1mStep[0m  [25/53], [94mLoss[0m : 5.70223
[1mStep[0m  [30/53], [94mLoss[0m : 5.61270
[1mStep[0m  [35/53], [94mLoss[0m : 5.71377
[1mStep[0m  [40/53], [94mLoss[0m : 5.29806
[1mStep[0m  [45/53], [94mLoss[0m : 5.38464
[1mStep[0m  [50/53], [94mLoss[0m : 5.64788

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.755, [92mTest[0m: 7.703, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.27185
[1mStep[0m  [5/53], [94mLoss[0m : 4.85607
[1mStep[0m  [10/53], [94mLoss[0m : 5.42051
[1mStep[0m  [15/53], [94mLoss[0m : 5.34233
[1mStep[0m  [20/53], [94mLoss[0m : 4.80970
[1mStep[0m  [25/53], [94mLoss[0m : 4.87635
[1mStep[0m  [30/53], [94mLoss[0m : 4.94718
[1mStep[0m  [35/53], [94mLoss[0m : 4.76589
[1mStep[0m  [40/53], [94mLoss[0m : 4.99309
[1mStep[0m  [45/53], [94mLoss[0m : 4.30561
[1mStep[0m  [50/53], [94mLoss[0m : 4.51151

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.035, [92mTest[0m: 7.079, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.65274
[1mStep[0m  [5/53], [94mLoss[0m : 4.99834
[1mStep[0m  [10/53], [94mLoss[0m : 4.46861
[1mStep[0m  [15/53], [94mLoss[0m : 4.78880
[1mStep[0m  [20/53], [94mLoss[0m : 4.32920
[1mStep[0m  [25/53], [94mLoss[0m : 4.12395
[1mStep[0m  [30/53], [94mLoss[0m : 4.08866
[1mStep[0m  [35/53], [94mLoss[0m : 4.10593
[1mStep[0m  [40/53], [94mLoss[0m : 4.11661
[1mStep[0m  [45/53], [94mLoss[0m : 4.43345
[1mStep[0m  [50/53], [94mLoss[0m : 4.53654

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.424, [92mTest[0m: 6.422, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 8 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.808
====================================

Phase 1 - Evaluation MAE:  5.808111300835242
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 4.03376
[1mStep[0m  [5/53], [94mLoss[0m : 4.31480
[1mStep[0m  [10/53], [94mLoss[0m : 4.13645
[1mStep[0m  [15/53], [94mLoss[0m : 4.10117
[1mStep[0m  [20/53], [94mLoss[0m : 4.10401
[1mStep[0m  [25/53], [94mLoss[0m : 3.83747
[1mStep[0m  [30/53], [94mLoss[0m : 3.93901
[1mStep[0m  [35/53], [94mLoss[0m : 3.69733
[1mStep[0m  [40/53], [94mLoss[0m : 4.12227
[1mStep[0m  [45/53], [94mLoss[0m : 4.20336
[1mStep[0m  [50/53], [94mLoss[0m : 3.60685

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.967, [92mTest[0m: 5.801, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.067
====================================

Phase 2 - Evaluation MAE:  5.066872926858755
MAE score P1        5.808111
MAE score P2        5.066873
loss                3.966883
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.9
weight_decay           0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.95522
[1mStep[0m  [2/26], [94mLoss[0m : 10.98120
[1mStep[0m  [4/26], [94mLoss[0m : 10.91834
[1mStep[0m  [6/26], [94mLoss[0m : 10.85965
[1mStep[0m  [8/26], [94mLoss[0m : 10.77266
[1mStep[0m  [10/26], [94mLoss[0m : 10.65403
[1mStep[0m  [12/26], [94mLoss[0m : 10.27221
[1mStep[0m  [14/26], [94mLoss[0m : 10.43432
[1mStep[0m  [16/26], [94mLoss[0m : 9.99733
[1mStep[0m  [18/26], [94mLoss[0m : 10.36568
[1mStep[0m  [20/26], [94mLoss[0m : 10.12485
[1mStep[0m  [22/26], [94mLoss[0m : 9.83001
[1mStep[0m  [24/26], [94mLoss[0m : 9.96085

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.460, [92mTest[0m: 10.974, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.82629
[1mStep[0m  [2/26], [94mLoss[0m : 9.51074
[1mStep[0m  [4/26], [94mLoss[0m : 9.13129
[1mStep[0m  [6/26], [94mLoss[0m : 9.04160
[1mStep[0m  [8/26], [94mLoss[0m : 9.18052
[1mStep[0m  [10/26], [94mLoss[0m : 8.77658
[1mStep[0m  [12/26], [94mLoss[0m : 8.80315
[1mStep[0m  [14/26], [94mLoss[0m : 8.65624
[1mStep[0m  [16/26], [94mLoss[0m : 8.72080
[1mStep[0m  [18/26], [94mLoss[0m : 8.50555
[1mStep[0m  [20/26], [94mLoss[0m : 8.09415
[1mStep[0m  [22/26], [94mLoss[0m : 7.88289
[1mStep[0m  [24/26], [94mLoss[0m : 7.84010

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.718, [92mTest[0m: 9.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.86393
[1mStep[0m  [2/26], [94mLoss[0m : 7.45563
[1mStep[0m  [4/26], [94mLoss[0m : 7.37296
[1mStep[0m  [6/26], [94mLoss[0m : 7.09591
[1mStep[0m  [8/26], [94mLoss[0m : 7.08478
[1mStep[0m  [10/26], [94mLoss[0m : 6.56374
[1mStep[0m  [12/26], [94mLoss[0m : 6.94471
[1mStep[0m  [14/26], [94mLoss[0m : 6.97812
[1mStep[0m  [16/26], [94mLoss[0m : 6.37417
[1mStep[0m  [18/26], [94mLoss[0m : 6.51421
[1mStep[0m  [20/26], [94mLoss[0m : 6.35108
[1mStep[0m  [22/26], [94mLoss[0m : 5.98741
[1mStep[0m  [24/26], [94mLoss[0m : 6.00491

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.747, [92mTest[0m: 7.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.83980
[1mStep[0m  [2/26], [94mLoss[0m : 5.46336
[1mStep[0m  [4/26], [94mLoss[0m : 5.30019
[1mStep[0m  [6/26], [94mLoss[0m : 5.55590
[1mStep[0m  [8/26], [94mLoss[0m : 4.90062
[1mStep[0m  [10/26], [94mLoss[0m : 4.89096
[1mStep[0m  [12/26], [94mLoss[0m : 5.06078
[1mStep[0m  [14/26], [94mLoss[0m : 4.78236
[1mStep[0m  [16/26], [94mLoss[0m : 4.76516
[1mStep[0m  [18/26], [94mLoss[0m : 4.29782
[1mStep[0m  [20/26], [94mLoss[0m : 4.42652
[1mStep[0m  [22/26], [94mLoss[0m : 4.12018
[1mStep[0m  [24/26], [94mLoss[0m : 4.05882

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.891, [92mTest[0m: 5.732, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.18231
[1mStep[0m  [2/26], [94mLoss[0m : 3.97410
[1mStep[0m  [4/26], [94mLoss[0m : 3.97120
[1mStep[0m  [6/26], [94mLoss[0m : 3.86554
[1mStep[0m  [8/26], [94mLoss[0m : 3.83613
[1mStep[0m  [10/26], [94mLoss[0m : 3.62955
[1mStep[0m  [12/26], [94mLoss[0m : 3.65861
[1mStep[0m  [14/26], [94mLoss[0m : 3.45631
[1mStep[0m  [16/26], [94mLoss[0m : 3.47398
[1mStep[0m  [18/26], [94mLoss[0m : 3.56348
[1mStep[0m  [20/26], [94mLoss[0m : 3.21467
[1mStep[0m  [22/26], [94mLoss[0m : 3.67299
[1mStep[0m  [24/26], [94mLoss[0m : 3.38208

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.686, [92mTest[0m: 4.107, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.26962
[1mStep[0m  [2/26], [94mLoss[0m : 3.61561
[1mStep[0m  [4/26], [94mLoss[0m : 3.25589
[1mStep[0m  [6/26], [94mLoss[0m : 3.17923
[1mStep[0m  [8/26], [94mLoss[0m : 3.30766
[1mStep[0m  [10/26], [94mLoss[0m : 3.30157
[1mStep[0m  [12/26], [94mLoss[0m : 3.30722
[1mStep[0m  [14/26], [94mLoss[0m : 3.24582
[1mStep[0m  [16/26], [94mLoss[0m : 3.16768
[1mStep[0m  [18/26], [94mLoss[0m : 2.99607
[1mStep[0m  [20/26], [94mLoss[0m : 2.91339
[1mStep[0m  [22/26], [94mLoss[0m : 3.06001
[1mStep[0m  [24/26], [94mLoss[0m : 2.77375

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.156, [92mTest[0m: 3.278, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.06356
[1mStep[0m  [2/26], [94mLoss[0m : 2.87991
[1mStep[0m  [4/26], [94mLoss[0m : 3.06903
[1mStep[0m  [6/26], [94mLoss[0m : 2.87522
[1mStep[0m  [8/26], [94mLoss[0m : 2.98033
[1mStep[0m  [10/26], [94mLoss[0m : 3.03691
[1mStep[0m  [12/26], [94mLoss[0m : 3.01636
[1mStep[0m  [14/26], [94mLoss[0m : 2.84294
[1mStep[0m  [16/26], [94mLoss[0m : 2.93594
[1mStep[0m  [18/26], [94mLoss[0m : 2.79124
[1mStep[0m  [20/26], [94mLoss[0m : 2.94999
[1mStep[0m  [22/26], [94mLoss[0m : 2.72287
[1mStep[0m  [24/26], [94mLoss[0m : 2.78637

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.933, [92mTest[0m: 2.977, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85648
[1mStep[0m  [2/26], [94mLoss[0m : 2.94385
[1mStep[0m  [4/26], [94mLoss[0m : 2.82453
[1mStep[0m  [6/26], [94mLoss[0m : 2.82289
[1mStep[0m  [8/26], [94mLoss[0m : 2.93113
[1mStep[0m  [10/26], [94mLoss[0m : 2.91277
[1mStep[0m  [12/26], [94mLoss[0m : 2.78441
[1mStep[0m  [14/26], [94mLoss[0m : 2.64349
[1mStep[0m  [16/26], [94mLoss[0m : 3.03734
[1mStep[0m  [18/26], [94mLoss[0m : 2.70805
[1mStep[0m  [20/26], [94mLoss[0m : 2.92564
[1mStep[0m  [22/26], [94mLoss[0m : 2.97189
[1mStep[0m  [24/26], [94mLoss[0m : 2.82046

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.857, [92mTest[0m: 2.850, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.95007
[1mStep[0m  [2/26], [94mLoss[0m : 2.89333
[1mStep[0m  [4/26], [94mLoss[0m : 2.83369
[1mStep[0m  [6/26], [94mLoss[0m : 2.82236
[1mStep[0m  [8/26], [94mLoss[0m : 2.98159
[1mStep[0m  [10/26], [94mLoss[0m : 2.75220
[1mStep[0m  [12/26], [94mLoss[0m : 2.91490
[1mStep[0m  [14/26], [94mLoss[0m : 2.82379
[1mStep[0m  [16/26], [94mLoss[0m : 2.79441
[1mStep[0m  [18/26], [94mLoss[0m : 2.55348
[1mStep[0m  [20/26], [94mLoss[0m : 2.70098
[1mStep[0m  [22/26], [94mLoss[0m : 2.58597
[1mStep[0m  [24/26], [94mLoss[0m : 2.70109

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.802, [92mTest[0m: 2.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81591
[1mStep[0m  [2/26], [94mLoss[0m : 2.67399
[1mStep[0m  [4/26], [94mLoss[0m : 2.88446
[1mStep[0m  [6/26], [94mLoss[0m : 2.80512
[1mStep[0m  [8/26], [94mLoss[0m : 2.77350
[1mStep[0m  [10/26], [94mLoss[0m : 2.63994
[1mStep[0m  [12/26], [94mLoss[0m : 2.87122
[1mStep[0m  [14/26], [94mLoss[0m : 2.81552
[1mStep[0m  [16/26], [94mLoss[0m : 2.76678
[1mStep[0m  [18/26], [94mLoss[0m : 2.90574
[1mStep[0m  [20/26], [94mLoss[0m : 2.64184
[1mStep[0m  [22/26], [94mLoss[0m : 2.55877
[1mStep[0m  [24/26], [94mLoss[0m : 2.83488

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.732, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72542
[1mStep[0m  [2/26], [94mLoss[0m : 2.86904
[1mStep[0m  [4/26], [94mLoss[0m : 2.66837
[1mStep[0m  [6/26], [94mLoss[0m : 2.54184
[1mStep[0m  [8/26], [94mLoss[0m : 2.86020
[1mStep[0m  [10/26], [94mLoss[0m : 2.71954
[1mStep[0m  [12/26], [94mLoss[0m : 2.88397
[1mStep[0m  [14/26], [94mLoss[0m : 2.68833
[1mStep[0m  [16/26], [94mLoss[0m : 2.68304
[1mStep[0m  [18/26], [94mLoss[0m : 2.59535
[1mStep[0m  [20/26], [94mLoss[0m : 2.63944
[1mStep[0m  [22/26], [94mLoss[0m : 2.53912
[1mStep[0m  [24/26], [94mLoss[0m : 2.88486

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.727, [92mTest[0m: 2.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.87513
[1mStep[0m  [2/26], [94mLoss[0m : 2.86213
[1mStep[0m  [4/26], [94mLoss[0m : 2.73430
[1mStep[0m  [6/26], [94mLoss[0m : 2.75048
[1mStep[0m  [8/26], [94mLoss[0m : 2.66141
[1mStep[0m  [10/26], [94mLoss[0m : 2.66323
[1mStep[0m  [12/26], [94mLoss[0m : 2.77352
[1mStep[0m  [14/26], [94mLoss[0m : 2.55387
[1mStep[0m  [16/26], [94mLoss[0m : 2.73313
[1mStep[0m  [18/26], [94mLoss[0m : 2.64385
[1mStep[0m  [20/26], [94mLoss[0m : 2.75143
[1mStep[0m  [22/26], [94mLoss[0m : 2.63238
[1mStep[0m  [24/26], [94mLoss[0m : 2.68000

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.711, [92mTest[0m: 2.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66693
[1mStep[0m  [2/26], [94mLoss[0m : 2.68957
[1mStep[0m  [4/26], [94mLoss[0m : 2.73690
[1mStep[0m  [6/26], [94mLoss[0m : 2.53562
[1mStep[0m  [8/26], [94mLoss[0m : 2.58301
[1mStep[0m  [10/26], [94mLoss[0m : 2.68236
[1mStep[0m  [12/26], [94mLoss[0m : 2.62098
[1mStep[0m  [14/26], [94mLoss[0m : 2.62131
[1mStep[0m  [16/26], [94mLoss[0m : 2.55013
[1mStep[0m  [18/26], [94mLoss[0m : 2.71162
[1mStep[0m  [20/26], [94mLoss[0m : 2.64219
[1mStep[0m  [22/26], [94mLoss[0m : 2.78776
[1mStep[0m  [24/26], [94mLoss[0m : 2.75660

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.644, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81680
[1mStep[0m  [2/26], [94mLoss[0m : 2.52099
[1mStep[0m  [4/26], [94mLoss[0m : 2.70902
[1mStep[0m  [6/26], [94mLoss[0m : 2.70089
[1mStep[0m  [8/26], [94mLoss[0m : 2.54851
[1mStep[0m  [10/26], [94mLoss[0m : 2.59470
[1mStep[0m  [12/26], [94mLoss[0m : 2.77496
[1mStep[0m  [14/26], [94mLoss[0m : 2.58690
[1mStep[0m  [16/26], [94mLoss[0m : 2.72085
[1mStep[0m  [18/26], [94mLoss[0m : 2.65503
[1mStep[0m  [20/26], [94mLoss[0m : 2.90040
[1mStep[0m  [22/26], [94mLoss[0m : 2.63305
[1mStep[0m  [24/26], [94mLoss[0m : 2.63500

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.623, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.62521
[1mStep[0m  [2/26], [94mLoss[0m : 2.67154
[1mStep[0m  [4/26], [94mLoss[0m : 2.73114
[1mStep[0m  [6/26], [94mLoss[0m : 2.57272
[1mStep[0m  [8/26], [94mLoss[0m : 2.57954
[1mStep[0m  [10/26], [94mLoss[0m : 2.75855
[1mStep[0m  [12/26], [94mLoss[0m : 2.68308
[1mStep[0m  [14/26], [94mLoss[0m : 2.69494
[1mStep[0m  [16/26], [94mLoss[0m : 2.64341
[1mStep[0m  [18/26], [94mLoss[0m : 2.69922
[1mStep[0m  [20/26], [94mLoss[0m : 2.65837
[1mStep[0m  [22/26], [94mLoss[0m : 2.83574
[1mStep[0m  [24/26], [94mLoss[0m : 2.59779

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.608, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69027
[1mStep[0m  [2/26], [94mLoss[0m : 2.74225
[1mStep[0m  [4/26], [94mLoss[0m : 2.73680
[1mStep[0m  [6/26], [94mLoss[0m : 2.65980
[1mStep[0m  [8/26], [94mLoss[0m : 2.51908
[1mStep[0m  [10/26], [94mLoss[0m : 2.62707
[1mStep[0m  [12/26], [94mLoss[0m : 2.67895
[1mStep[0m  [14/26], [94mLoss[0m : 2.71797
[1mStep[0m  [16/26], [94mLoss[0m : 2.54904
[1mStep[0m  [18/26], [94mLoss[0m : 2.73078
[1mStep[0m  [20/26], [94mLoss[0m : 2.45718
[1mStep[0m  [22/26], [94mLoss[0m : 2.54966
[1mStep[0m  [24/26], [94mLoss[0m : 2.70228

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.598, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71319
[1mStep[0m  [2/26], [94mLoss[0m : 2.56387
[1mStep[0m  [4/26], [94mLoss[0m : 2.60503
[1mStep[0m  [6/26], [94mLoss[0m : 2.65723
[1mStep[0m  [8/26], [94mLoss[0m : 2.51909
[1mStep[0m  [10/26], [94mLoss[0m : 2.56905
[1mStep[0m  [12/26], [94mLoss[0m : 2.79328
[1mStep[0m  [14/26], [94mLoss[0m : 2.56541
[1mStep[0m  [16/26], [94mLoss[0m : 2.62179
[1mStep[0m  [18/26], [94mLoss[0m : 2.76216
[1mStep[0m  [20/26], [94mLoss[0m : 2.57224
[1mStep[0m  [22/26], [94mLoss[0m : 2.46989
[1mStep[0m  [24/26], [94mLoss[0m : 2.75578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.67744
[1mStep[0m  [2/26], [94mLoss[0m : 2.56573
[1mStep[0m  [4/26], [94mLoss[0m : 2.55271
[1mStep[0m  [6/26], [94mLoss[0m : 2.80489
[1mStep[0m  [8/26], [94mLoss[0m : 2.62990
[1mStep[0m  [10/26], [94mLoss[0m : 2.58147
[1mStep[0m  [12/26], [94mLoss[0m : 2.54345
[1mStep[0m  [14/26], [94mLoss[0m : 2.73014
[1mStep[0m  [16/26], [94mLoss[0m : 2.65672
[1mStep[0m  [18/26], [94mLoss[0m : 2.56823
[1mStep[0m  [20/26], [94mLoss[0m : 2.60075
[1mStep[0m  [22/26], [94mLoss[0m : 2.61251
[1mStep[0m  [24/26], [94mLoss[0m : 2.65943

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61508
[1mStep[0m  [2/26], [94mLoss[0m : 2.60824
[1mStep[0m  [4/26], [94mLoss[0m : 2.66806
[1mStep[0m  [6/26], [94mLoss[0m : 2.58132
[1mStep[0m  [8/26], [94mLoss[0m : 2.65674
[1mStep[0m  [10/26], [94mLoss[0m : 2.69359
[1mStep[0m  [12/26], [94mLoss[0m : 2.62305
[1mStep[0m  [14/26], [94mLoss[0m : 2.48287
[1mStep[0m  [16/26], [94mLoss[0m : 2.53938
[1mStep[0m  [18/26], [94mLoss[0m : 2.51650
[1mStep[0m  [20/26], [94mLoss[0m : 2.59009
[1mStep[0m  [22/26], [94mLoss[0m : 2.68067
[1mStep[0m  [24/26], [94mLoss[0m : 2.65406

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.558, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.43561
[1mStep[0m  [2/26], [94mLoss[0m : 2.65904
[1mStep[0m  [4/26], [94mLoss[0m : 2.72393
[1mStep[0m  [6/26], [94mLoss[0m : 2.56525
[1mStep[0m  [8/26], [94mLoss[0m : 2.64481
[1mStep[0m  [10/26], [94mLoss[0m : 2.66544
[1mStep[0m  [12/26], [94mLoss[0m : 2.85463
[1mStep[0m  [14/26], [94mLoss[0m : 2.52043
[1mStep[0m  [16/26], [94mLoss[0m : 2.64424
[1mStep[0m  [18/26], [94mLoss[0m : 2.46456
[1mStep[0m  [20/26], [94mLoss[0m : 2.71376
[1mStep[0m  [22/26], [94mLoss[0m : 2.58612
[1mStep[0m  [24/26], [94mLoss[0m : 2.61436

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.559, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57316
[1mStep[0m  [2/26], [94mLoss[0m : 2.69047
[1mStep[0m  [4/26], [94mLoss[0m : 2.49155
[1mStep[0m  [6/26], [94mLoss[0m : 2.67771
[1mStep[0m  [8/26], [94mLoss[0m : 2.50923
[1mStep[0m  [10/26], [94mLoss[0m : 2.62794
[1mStep[0m  [12/26], [94mLoss[0m : 2.45784
[1mStep[0m  [14/26], [94mLoss[0m : 2.63989
[1mStep[0m  [16/26], [94mLoss[0m : 2.49177
[1mStep[0m  [18/26], [94mLoss[0m : 2.53749
[1mStep[0m  [20/26], [94mLoss[0m : 2.59888
[1mStep[0m  [22/26], [94mLoss[0m : 2.55474
[1mStep[0m  [24/26], [94mLoss[0m : 2.64747

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.548, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.64201
[1mStep[0m  [2/26], [94mLoss[0m : 2.58754
[1mStep[0m  [4/26], [94mLoss[0m : 2.38990
[1mStep[0m  [6/26], [94mLoss[0m : 2.53336
[1mStep[0m  [8/26], [94mLoss[0m : 2.53133
[1mStep[0m  [10/26], [94mLoss[0m : 2.54015
[1mStep[0m  [12/26], [94mLoss[0m : 2.47921
[1mStep[0m  [14/26], [94mLoss[0m : 2.68481
[1mStep[0m  [16/26], [94mLoss[0m : 2.61983
[1mStep[0m  [18/26], [94mLoss[0m : 2.66549
[1mStep[0m  [20/26], [94mLoss[0m : 2.62444
[1mStep[0m  [22/26], [94mLoss[0m : 2.67764
[1mStep[0m  [24/26], [94mLoss[0m : 2.69219

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.542, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55164
[1mStep[0m  [2/26], [94mLoss[0m : 2.65909
[1mStep[0m  [4/26], [94mLoss[0m : 2.63608
[1mStep[0m  [6/26], [94mLoss[0m : 2.63063
[1mStep[0m  [8/26], [94mLoss[0m : 2.54307
[1mStep[0m  [10/26], [94mLoss[0m : 2.61480
[1mStep[0m  [12/26], [94mLoss[0m : 2.52609
[1mStep[0m  [14/26], [94mLoss[0m : 2.61727
[1mStep[0m  [16/26], [94mLoss[0m : 2.53181
[1mStep[0m  [18/26], [94mLoss[0m : 2.38425
[1mStep[0m  [20/26], [94mLoss[0m : 2.63630
[1mStep[0m  [22/26], [94mLoss[0m : 2.56935
[1mStep[0m  [24/26], [94mLoss[0m : 2.69163

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.538, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.55220
[1mStep[0m  [2/26], [94mLoss[0m : 2.65565
[1mStep[0m  [4/26], [94mLoss[0m : 2.45793
[1mStep[0m  [6/26], [94mLoss[0m : 2.50228
[1mStep[0m  [8/26], [94mLoss[0m : 2.59049
[1mStep[0m  [10/26], [94mLoss[0m : 2.56241
[1mStep[0m  [12/26], [94mLoss[0m : 2.47265
[1mStep[0m  [14/26], [94mLoss[0m : 2.56810
[1mStep[0m  [16/26], [94mLoss[0m : 2.60272
[1mStep[0m  [18/26], [94mLoss[0m : 2.66927
[1mStep[0m  [20/26], [94mLoss[0m : 2.57035
[1mStep[0m  [22/26], [94mLoss[0m : 2.57876
[1mStep[0m  [24/26], [94mLoss[0m : 2.65524

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.537, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.65315
[1mStep[0m  [2/26], [94mLoss[0m : 2.53611
[1mStep[0m  [4/26], [94mLoss[0m : 2.60666
[1mStep[0m  [6/26], [94mLoss[0m : 2.57359
[1mStep[0m  [8/26], [94mLoss[0m : 2.50497
[1mStep[0m  [10/26], [94mLoss[0m : 2.59040
[1mStep[0m  [12/26], [94mLoss[0m : 2.59802
[1mStep[0m  [14/26], [94mLoss[0m : 2.53616
[1mStep[0m  [16/26], [94mLoss[0m : 2.57041
[1mStep[0m  [18/26], [94mLoss[0m : 2.57890
[1mStep[0m  [20/26], [94mLoss[0m : 2.55140
[1mStep[0m  [22/26], [94mLoss[0m : 2.57750
[1mStep[0m  [24/26], [94mLoss[0m : 2.53580

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.530, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.63522
[1mStep[0m  [2/26], [94mLoss[0m : 2.47254
[1mStep[0m  [4/26], [94mLoss[0m : 2.60039
[1mStep[0m  [6/26], [94mLoss[0m : 2.57569
[1mStep[0m  [8/26], [94mLoss[0m : 2.44196
[1mStep[0m  [10/26], [94mLoss[0m : 2.56554
[1mStep[0m  [12/26], [94mLoss[0m : 2.55782
[1mStep[0m  [14/26], [94mLoss[0m : 2.53328
[1mStep[0m  [16/26], [94mLoss[0m : 2.55097
[1mStep[0m  [18/26], [94mLoss[0m : 2.52478
[1mStep[0m  [20/26], [94mLoss[0m : 2.36845
[1mStep[0m  [22/26], [94mLoss[0m : 2.65815
[1mStep[0m  [24/26], [94mLoss[0m : 2.40064

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.39796
[1mStep[0m  [2/26], [94mLoss[0m : 2.45663
[1mStep[0m  [4/26], [94mLoss[0m : 2.62333
[1mStep[0m  [6/26], [94mLoss[0m : 2.70076
[1mStep[0m  [8/26], [94mLoss[0m : 2.71004
[1mStep[0m  [10/26], [94mLoss[0m : 2.63208
[1mStep[0m  [12/26], [94mLoss[0m : 2.53555
[1mStep[0m  [14/26], [94mLoss[0m : 2.49027
[1mStep[0m  [16/26], [94mLoss[0m : 2.41318
[1mStep[0m  [18/26], [94mLoss[0m : 2.53031
[1mStep[0m  [20/26], [94mLoss[0m : 2.60160
[1mStep[0m  [22/26], [94mLoss[0m : 2.62512
[1mStep[0m  [24/26], [94mLoss[0m : 2.39940

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.521, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61751
[1mStep[0m  [2/26], [94mLoss[0m : 2.47935
[1mStep[0m  [4/26], [94mLoss[0m : 2.61379
[1mStep[0m  [6/26], [94mLoss[0m : 2.58940
[1mStep[0m  [8/26], [94mLoss[0m : 2.52278
[1mStep[0m  [10/26], [94mLoss[0m : 2.60987
[1mStep[0m  [12/26], [94mLoss[0m : 2.46258
[1mStep[0m  [14/26], [94mLoss[0m : 2.47984
[1mStep[0m  [16/26], [94mLoss[0m : 2.73645
[1mStep[0m  [18/26], [94mLoss[0m : 2.52759
[1mStep[0m  [20/26], [94mLoss[0m : 2.67808
[1mStep[0m  [22/26], [94mLoss[0m : 2.56285
[1mStep[0m  [24/26], [94mLoss[0m : 2.41490

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.514, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.59338
[1mStep[0m  [2/26], [94mLoss[0m : 2.57812
[1mStep[0m  [4/26], [94mLoss[0m : 2.42723
[1mStep[0m  [6/26], [94mLoss[0m : 2.67432
[1mStep[0m  [8/26], [94mLoss[0m : 2.57717
[1mStep[0m  [10/26], [94mLoss[0m : 2.51859
[1mStep[0m  [12/26], [94mLoss[0m : 2.60637
[1mStep[0m  [14/26], [94mLoss[0m : 2.65698
[1mStep[0m  [16/26], [94mLoss[0m : 2.60525
[1mStep[0m  [18/26], [94mLoss[0m : 2.58638
[1mStep[0m  [20/26], [94mLoss[0m : 2.50195
[1mStep[0m  [22/26], [94mLoss[0m : 2.62313
[1mStep[0m  [24/26], [94mLoss[0m : 2.56218

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.518, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.60149
[1mStep[0m  [2/26], [94mLoss[0m : 2.56134
[1mStep[0m  [4/26], [94mLoss[0m : 2.49072
[1mStep[0m  [6/26], [94mLoss[0m : 2.40184
[1mStep[0m  [8/26], [94mLoss[0m : 2.66015
[1mStep[0m  [10/26], [94mLoss[0m : 2.62610
[1mStep[0m  [12/26], [94mLoss[0m : 2.68171
[1mStep[0m  [14/26], [94mLoss[0m : 2.60749
[1mStep[0m  [16/26], [94mLoss[0m : 2.61595
[1mStep[0m  [18/26], [94mLoss[0m : 2.63762
[1mStep[0m  [20/26], [94mLoss[0m : 2.56959
[1mStep[0m  [22/26], [94mLoss[0m : 2.53883
[1mStep[0m  [24/26], [94mLoss[0m : 2.56871

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.516, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.513
====================================

Phase 1 - Evaluation MAE:  2.512801463787372
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 2.66671
[1mStep[0m  [2/26], [94mLoss[0m : 2.54307
[1mStep[0m  [4/26], [94mLoss[0m : 2.51536
[1mStep[0m  [6/26], [94mLoss[0m : 2.49662
[1mStep[0m  [8/26], [94mLoss[0m : 2.61848
[1mStep[0m  [10/26], [94mLoss[0m : 2.60014
[1mStep[0m  [12/26], [94mLoss[0m : 2.52962
[1mStep[0m  [14/26], [94mLoss[0m : 2.51464
[1mStep[0m  [16/26], [94mLoss[0m : 2.58942
[1mStep[0m  [18/26], [94mLoss[0m : 2.58448
[1mStep[0m  [20/26], [94mLoss[0m : 2.62199
[1mStep[0m  [22/26], [94mLoss[0m : 2.37887
[1mStep[0m  [24/26], [94mLoss[0m : 2.44608

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.509, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.37701
[1mStep[0m  [2/26], [94mLoss[0m : 2.58064
[1mStep[0m  [4/26], [94mLoss[0m : 2.63302
[1mStep[0m  [6/26], [94mLoss[0m : 2.48676
[1mStep[0m  [8/26], [94mLoss[0m : 2.63459
[1mStep[0m  [10/26], [94mLoss[0m : 2.57037
[1mStep[0m  [12/26], [94mLoss[0m : 2.53451
[1mStep[0m  [14/26], [94mLoss[0m : 2.51112
[1mStep[0m  [16/26], [94mLoss[0m : 2.57542
[1mStep[0m  [18/26], [94mLoss[0m : 2.58224
[1mStep[0m  [20/26], [94mLoss[0m : 2.42724
[1mStep[0m  [22/26], [94mLoss[0m : 2.57528
[1mStep[0m  [24/26], [94mLoss[0m : 2.30519

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.508, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.53557
[1mStep[0m  [2/26], [94mLoss[0m : 2.57852
[1mStep[0m  [4/26], [94mLoss[0m : 2.47237
[1mStep[0m  [6/26], [94mLoss[0m : 2.57005
[1mStep[0m  [8/26], [94mLoss[0m : 2.59847
[1mStep[0m  [10/26], [94mLoss[0m : 2.44266
[1mStep[0m  [12/26], [94mLoss[0m : 2.36845
[1mStep[0m  [14/26], [94mLoss[0m : 2.49980
[1mStep[0m  [16/26], [94mLoss[0m : 2.56959
[1mStep[0m  [18/26], [94mLoss[0m : 2.39824
[1mStep[0m  [20/26], [94mLoss[0m : 2.52958
[1mStep[0m  [22/26], [94mLoss[0m : 2.43985
[1mStep[0m  [24/26], [94mLoss[0m : 2.53501

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.501, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.72032
[1mStep[0m  [2/26], [94mLoss[0m : 2.55295
[1mStep[0m  [4/26], [94mLoss[0m : 2.49200
[1mStep[0m  [6/26], [94mLoss[0m : 2.58531
[1mStep[0m  [8/26], [94mLoss[0m : 2.68303
[1mStep[0m  [10/26], [94mLoss[0m : 2.50859
[1mStep[0m  [12/26], [94mLoss[0m : 2.42413
[1mStep[0m  [14/26], [94mLoss[0m : 2.46941
[1mStep[0m  [16/26], [94mLoss[0m : 2.41536
[1mStep[0m  [18/26], [94mLoss[0m : 2.49912
[1mStep[0m  [20/26], [94mLoss[0m : 2.59008
[1mStep[0m  [22/26], [94mLoss[0m : 2.49158
[1mStep[0m  [24/26], [94mLoss[0m : 2.38633

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.488, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61366
[1mStep[0m  [2/26], [94mLoss[0m : 2.52619
[1mStep[0m  [4/26], [94mLoss[0m : 2.44308
[1mStep[0m  [6/26], [94mLoss[0m : 2.52792
[1mStep[0m  [8/26], [94mLoss[0m : 2.56304
[1mStep[0m  [10/26], [94mLoss[0m : 2.54900
[1mStep[0m  [12/26], [94mLoss[0m : 2.47433
[1mStep[0m  [14/26], [94mLoss[0m : 2.56626
[1mStep[0m  [16/26], [94mLoss[0m : 2.56234
[1mStep[0m  [18/26], [94mLoss[0m : 2.67985
[1mStep[0m  [20/26], [94mLoss[0m : 2.41217
[1mStep[0m  [22/26], [94mLoss[0m : 2.65589
[1mStep[0m  [24/26], [94mLoss[0m : 2.44166

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52405
[1mStep[0m  [2/26], [94mLoss[0m : 2.38933
[1mStep[0m  [4/26], [94mLoss[0m : 2.50722
[1mStep[0m  [6/26], [94mLoss[0m : 2.57153
[1mStep[0m  [8/26], [94mLoss[0m : 2.43841
[1mStep[0m  [10/26], [94mLoss[0m : 2.58803
[1mStep[0m  [12/26], [94mLoss[0m : 2.57084
[1mStep[0m  [14/26], [94mLoss[0m : 2.36955
[1mStep[0m  [16/26], [94mLoss[0m : 2.64225
[1mStep[0m  [18/26], [94mLoss[0m : 2.55333
[1mStep[0m  [20/26], [94mLoss[0m : 2.62518
[1mStep[0m  [22/26], [94mLoss[0m : 2.47863
[1mStep[0m  [24/26], [94mLoss[0m : 2.54911

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.483, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.57442
[1mStep[0m  [2/26], [94mLoss[0m : 2.35385
[1mStep[0m  [4/26], [94mLoss[0m : 2.52514
[1mStep[0m  [6/26], [94mLoss[0m : 2.61669
[1mStep[0m  [8/26], [94mLoss[0m : 2.50623
[1mStep[0m  [10/26], [94mLoss[0m : 2.45634
[1mStep[0m  [12/26], [94mLoss[0m : 2.64305
[1mStep[0m  [14/26], [94mLoss[0m : 2.51689
[1mStep[0m  [16/26], [94mLoss[0m : 2.64677
[1mStep[0m  [18/26], [94mLoss[0m : 2.61778
[1mStep[0m  [20/26], [94mLoss[0m : 2.53176
[1mStep[0m  [22/26], [94mLoss[0m : 2.48323
[1mStep[0m  [24/26], [94mLoss[0m : 2.40438

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.480, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66244
[1mStep[0m  [2/26], [94mLoss[0m : 2.52547
[1mStep[0m  [4/26], [94mLoss[0m : 2.60291
[1mStep[0m  [6/26], [94mLoss[0m : 2.56875
[1mStep[0m  [8/26], [94mLoss[0m : 2.55067
[1mStep[0m  [10/26], [94mLoss[0m : 2.57308
[1mStep[0m  [12/26], [94mLoss[0m : 2.44185
[1mStep[0m  [14/26], [94mLoss[0m : 2.42885
[1mStep[0m  [16/26], [94mLoss[0m : 2.34942
[1mStep[0m  [18/26], [94mLoss[0m : 2.40655
[1mStep[0m  [20/26], [94mLoss[0m : 2.58290
[1mStep[0m  [22/26], [94mLoss[0m : 2.51246
[1mStep[0m  [24/26], [94mLoss[0m : 2.53474

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.476, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56402
[1mStep[0m  [2/26], [94mLoss[0m : 2.55064
[1mStep[0m  [4/26], [94mLoss[0m : 2.48378
[1mStep[0m  [6/26], [94mLoss[0m : 2.53900
[1mStep[0m  [8/26], [94mLoss[0m : 2.47302
[1mStep[0m  [10/26], [94mLoss[0m : 2.55268
[1mStep[0m  [12/26], [94mLoss[0m : 2.58959
[1mStep[0m  [14/26], [94mLoss[0m : 2.61479
[1mStep[0m  [16/26], [94mLoss[0m : 2.70745
[1mStep[0m  [18/26], [94mLoss[0m : 2.58348
[1mStep[0m  [20/26], [94mLoss[0m : 2.38309
[1mStep[0m  [22/26], [94mLoss[0m : 2.40051
[1mStep[0m  [24/26], [94mLoss[0m : 2.59058

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.473, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36986
[1mStep[0m  [2/26], [94mLoss[0m : 2.60526
[1mStep[0m  [4/26], [94mLoss[0m : 2.53290
[1mStep[0m  [6/26], [94mLoss[0m : 2.57936
[1mStep[0m  [8/26], [94mLoss[0m : 2.37558
[1mStep[0m  [10/26], [94mLoss[0m : 2.41285
[1mStep[0m  [12/26], [94mLoss[0m : 2.59158
[1mStep[0m  [14/26], [94mLoss[0m : 2.47429
[1mStep[0m  [16/26], [94mLoss[0m : 2.46326
[1mStep[0m  [18/26], [94mLoss[0m : 2.70234
[1mStep[0m  [20/26], [94mLoss[0m : 2.44437
[1mStep[0m  [22/26], [94mLoss[0m : 2.51696
[1mStep[0m  [24/26], [94mLoss[0m : 2.47839

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.477, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66211
[1mStep[0m  [2/26], [94mLoss[0m : 2.41677
[1mStep[0m  [4/26], [94mLoss[0m : 2.50824
[1mStep[0m  [6/26], [94mLoss[0m : 2.48218
[1mStep[0m  [8/26], [94mLoss[0m : 2.34528
[1mStep[0m  [10/26], [94mLoss[0m : 2.45160
[1mStep[0m  [12/26], [94mLoss[0m : 2.53008
[1mStep[0m  [14/26], [94mLoss[0m : 2.49117
[1mStep[0m  [16/26], [94mLoss[0m : 2.41643
[1mStep[0m  [18/26], [94mLoss[0m : 2.55432
[1mStep[0m  [20/26], [94mLoss[0m : 2.41179
[1mStep[0m  [22/26], [94mLoss[0m : 2.42573
[1mStep[0m  [24/26], [94mLoss[0m : 2.34825

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42290
[1mStep[0m  [2/26], [94mLoss[0m : 2.31124
[1mStep[0m  [4/26], [94mLoss[0m : 2.52140
[1mStep[0m  [6/26], [94mLoss[0m : 2.36791
[1mStep[0m  [8/26], [94mLoss[0m : 2.53770
[1mStep[0m  [10/26], [94mLoss[0m : 2.36436
[1mStep[0m  [12/26], [94mLoss[0m : 2.24602
[1mStep[0m  [14/26], [94mLoss[0m : 2.46403
[1mStep[0m  [16/26], [94mLoss[0m : 2.54250
[1mStep[0m  [18/26], [94mLoss[0m : 2.47614
[1mStep[0m  [20/26], [94mLoss[0m : 2.65252
[1mStep[0m  [22/26], [94mLoss[0m : 2.52357
[1mStep[0m  [24/26], [94mLoss[0m : 2.52044

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.467, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.46789
[1mStep[0m  [2/26], [94mLoss[0m : 2.65458
[1mStep[0m  [4/26], [94mLoss[0m : 2.45213
[1mStep[0m  [6/26], [94mLoss[0m : 2.29585
[1mStep[0m  [8/26], [94mLoss[0m : 2.38937
[1mStep[0m  [10/26], [94mLoss[0m : 2.56074
[1mStep[0m  [12/26], [94mLoss[0m : 2.62013
[1mStep[0m  [14/26], [94mLoss[0m : 2.47314
[1mStep[0m  [16/26], [94mLoss[0m : 2.47179
[1mStep[0m  [18/26], [94mLoss[0m : 2.31067
[1mStep[0m  [20/26], [94mLoss[0m : 2.53789
[1mStep[0m  [22/26], [94mLoss[0m : 2.61268
[1mStep[0m  [24/26], [94mLoss[0m : 2.54583

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47864
[1mStep[0m  [2/26], [94mLoss[0m : 2.49524
[1mStep[0m  [4/26], [94mLoss[0m : 2.48749
[1mStep[0m  [6/26], [94mLoss[0m : 2.27966
[1mStep[0m  [8/26], [94mLoss[0m : 2.46831
[1mStep[0m  [10/26], [94mLoss[0m : 2.55231
[1mStep[0m  [12/26], [94mLoss[0m : 2.48819
[1mStep[0m  [14/26], [94mLoss[0m : 2.51796
[1mStep[0m  [16/26], [94mLoss[0m : 2.50378
[1mStep[0m  [18/26], [94mLoss[0m : 2.39756
[1mStep[0m  [20/26], [94mLoss[0m : 2.42807
[1mStep[0m  [22/26], [94mLoss[0m : 2.54964
[1mStep[0m  [24/26], [94mLoss[0m : 2.44120

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.471, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51313
[1mStep[0m  [2/26], [94mLoss[0m : 2.54728
[1mStep[0m  [4/26], [94mLoss[0m : 2.47523
[1mStep[0m  [6/26], [94mLoss[0m : 2.56971
[1mStep[0m  [8/26], [94mLoss[0m : 2.51608
[1mStep[0m  [10/26], [94mLoss[0m : 2.33165
[1mStep[0m  [12/26], [94mLoss[0m : 2.34965
[1mStep[0m  [14/26], [94mLoss[0m : 2.41167
[1mStep[0m  [16/26], [94mLoss[0m : 2.44399
[1mStep[0m  [18/26], [94mLoss[0m : 2.55459
[1mStep[0m  [20/26], [94mLoss[0m : 2.45143
[1mStep[0m  [22/26], [94mLoss[0m : 2.52653
[1mStep[0m  [24/26], [94mLoss[0m : 2.44808

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.461, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.42531
[1mStep[0m  [2/26], [94mLoss[0m : 2.37914
[1mStep[0m  [4/26], [94mLoss[0m : 2.44413
[1mStep[0m  [6/26], [94mLoss[0m : 2.47469
[1mStep[0m  [8/26], [94mLoss[0m : 2.43750
[1mStep[0m  [10/26], [94mLoss[0m : 2.38308
[1mStep[0m  [12/26], [94mLoss[0m : 2.49362
[1mStep[0m  [14/26], [94mLoss[0m : 2.51659
[1mStep[0m  [16/26], [94mLoss[0m : 2.51711
[1mStep[0m  [18/26], [94mLoss[0m : 2.59482
[1mStep[0m  [20/26], [94mLoss[0m : 2.45895
[1mStep[0m  [22/26], [94mLoss[0m : 2.42449
[1mStep[0m  [24/26], [94mLoss[0m : 2.50511

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.458, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52225
[1mStep[0m  [2/26], [94mLoss[0m : 2.42211
[1mStep[0m  [4/26], [94mLoss[0m : 2.42928
[1mStep[0m  [6/26], [94mLoss[0m : 2.50550
[1mStep[0m  [8/26], [94mLoss[0m : 2.47010
[1mStep[0m  [10/26], [94mLoss[0m : 2.56988
[1mStep[0m  [12/26], [94mLoss[0m : 2.43213
[1mStep[0m  [14/26], [94mLoss[0m : 2.35117
[1mStep[0m  [16/26], [94mLoss[0m : 2.29636
[1mStep[0m  [18/26], [94mLoss[0m : 2.60702
[1mStep[0m  [20/26], [94mLoss[0m : 2.43129
[1mStep[0m  [22/26], [94mLoss[0m : 2.49348
[1mStep[0m  [24/26], [94mLoss[0m : 2.48069

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.457, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41919
[1mStep[0m  [2/26], [94mLoss[0m : 2.42429
[1mStep[0m  [4/26], [94mLoss[0m : 2.36953
[1mStep[0m  [6/26], [94mLoss[0m : 2.32320
[1mStep[0m  [8/26], [94mLoss[0m : 2.40895
[1mStep[0m  [10/26], [94mLoss[0m : 2.56057
[1mStep[0m  [12/26], [94mLoss[0m : 2.45350
[1mStep[0m  [14/26], [94mLoss[0m : 2.32590
[1mStep[0m  [16/26], [94mLoss[0m : 2.54222
[1mStep[0m  [18/26], [94mLoss[0m : 2.54150
[1mStep[0m  [20/26], [94mLoss[0m : 2.46968
[1mStep[0m  [22/26], [94mLoss[0m : 2.37167
[1mStep[0m  [24/26], [94mLoss[0m : 2.55368

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.47104
[1mStep[0m  [2/26], [94mLoss[0m : 2.40274
[1mStep[0m  [4/26], [94mLoss[0m : 2.43518
[1mStep[0m  [6/26], [94mLoss[0m : 2.55940
[1mStep[0m  [8/26], [94mLoss[0m : 2.56782
[1mStep[0m  [10/26], [94mLoss[0m : 2.46837
[1mStep[0m  [12/26], [94mLoss[0m : 2.54132
[1mStep[0m  [14/26], [94mLoss[0m : 2.48794
[1mStep[0m  [16/26], [94mLoss[0m : 2.55029
[1mStep[0m  [18/26], [94mLoss[0m : 2.32346
[1mStep[0m  [20/26], [94mLoss[0m : 2.47560
[1mStep[0m  [22/26], [94mLoss[0m : 2.38967
[1mStep[0m  [24/26], [94mLoss[0m : 2.47465

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.448, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52800
[1mStep[0m  [2/26], [94mLoss[0m : 2.43839
[1mStep[0m  [4/26], [94mLoss[0m : 2.44027
[1mStep[0m  [6/26], [94mLoss[0m : 2.52502
[1mStep[0m  [8/26], [94mLoss[0m : 2.32602
[1mStep[0m  [10/26], [94mLoss[0m : 2.49325
[1mStep[0m  [12/26], [94mLoss[0m : 2.52842
[1mStep[0m  [14/26], [94mLoss[0m : 2.47238
[1mStep[0m  [16/26], [94mLoss[0m : 2.40821
[1mStep[0m  [18/26], [94mLoss[0m : 2.48874
[1mStep[0m  [20/26], [94mLoss[0m : 2.44522
[1mStep[0m  [22/26], [94mLoss[0m : 2.31438
[1mStep[0m  [24/26], [94mLoss[0m : 2.43198

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.58982
[1mStep[0m  [2/26], [94mLoss[0m : 2.31887
[1mStep[0m  [4/26], [94mLoss[0m : 2.44362
[1mStep[0m  [6/26], [94mLoss[0m : 2.37941
[1mStep[0m  [8/26], [94mLoss[0m : 2.55842
[1mStep[0m  [10/26], [94mLoss[0m : 2.45097
[1mStep[0m  [12/26], [94mLoss[0m : 2.58836
[1mStep[0m  [14/26], [94mLoss[0m : 2.52320
[1mStep[0m  [16/26], [94mLoss[0m : 2.39842
[1mStep[0m  [18/26], [94mLoss[0m : 2.54371
[1mStep[0m  [20/26], [94mLoss[0m : 2.46904
[1mStep[0m  [22/26], [94mLoss[0m : 2.57817
[1mStep[0m  [24/26], [94mLoss[0m : 2.40834

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.449, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52829
[1mStep[0m  [2/26], [94mLoss[0m : 2.50873
[1mStep[0m  [4/26], [94mLoss[0m : 2.34171
[1mStep[0m  [6/26], [94mLoss[0m : 2.31937
[1mStep[0m  [8/26], [94mLoss[0m : 2.60568
[1mStep[0m  [10/26], [94mLoss[0m : 2.43058
[1mStep[0m  [12/26], [94mLoss[0m : 2.61643
[1mStep[0m  [14/26], [94mLoss[0m : 2.37437
[1mStep[0m  [16/26], [94mLoss[0m : 2.50635
[1mStep[0m  [18/26], [94mLoss[0m : 2.42444
[1mStep[0m  [20/26], [94mLoss[0m : 2.35285
[1mStep[0m  [22/26], [94mLoss[0m : 2.41140
[1mStep[0m  [24/26], [94mLoss[0m : 2.58713

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.35118
[1mStep[0m  [2/26], [94mLoss[0m : 2.44184
[1mStep[0m  [4/26], [94mLoss[0m : 2.43587
[1mStep[0m  [6/26], [94mLoss[0m : 2.33902
[1mStep[0m  [8/26], [94mLoss[0m : 2.45057
[1mStep[0m  [10/26], [94mLoss[0m : 2.59537
[1mStep[0m  [12/26], [94mLoss[0m : 2.55628
[1mStep[0m  [14/26], [94mLoss[0m : 2.33781
[1mStep[0m  [16/26], [94mLoss[0m : 2.40440
[1mStep[0m  [18/26], [94mLoss[0m : 2.42739
[1mStep[0m  [20/26], [94mLoss[0m : 2.52087
[1mStep[0m  [22/26], [94mLoss[0m : 2.35828
[1mStep[0m  [24/26], [94mLoss[0m : 2.49731

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.56096
[1mStep[0m  [2/26], [94mLoss[0m : 2.49831
[1mStep[0m  [4/26], [94mLoss[0m : 2.53879
[1mStep[0m  [6/26], [94mLoss[0m : 2.48596
[1mStep[0m  [8/26], [94mLoss[0m : 2.55339
[1mStep[0m  [10/26], [94mLoss[0m : 2.37606
[1mStep[0m  [12/26], [94mLoss[0m : 2.59619
[1mStep[0m  [14/26], [94mLoss[0m : 2.40242
[1mStep[0m  [16/26], [94mLoss[0m : 2.39407
[1mStep[0m  [18/26], [94mLoss[0m : 2.40547
[1mStep[0m  [20/26], [94mLoss[0m : 2.39327
[1mStep[0m  [22/26], [94mLoss[0m : 2.40052
[1mStep[0m  [24/26], [94mLoss[0m : 2.37914

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.52533
[1mStep[0m  [2/26], [94mLoss[0m : 2.70642
[1mStep[0m  [4/26], [94mLoss[0m : 2.44765
[1mStep[0m  [6/26], [94mLoss[0m : 2.44343
[1mStep[0m  [8/26], [94mLoss[0m : 2.39171
[1mStep[0m  [10/26], [94mLoss[0m : 2.45956
[1mStep[0m  [12/26], [94mLoss[0m : 2.34006
[1mStep[0m  [14/26], [94mLoss[0m : 2.37274
[1mStep[0m  [16/26], [94mLoss[0m : 2.56283
[1mStep[0m  [18/26], [94mLoss[0m : 2.45574
[1mStep[0m  [20/26], [94mLoss[0m : 2.37578
[1mStep[0m  [22/26], [94mLoss[0m : 2.36886
[1mStep[0m  [24/26], [94mLoss[0m : 2.52135

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.448, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.51368
[1mStep[0m  [2/26], [94mLoss[0m : 2.47935
[1mStep[0m  [4/26], [94mLoss[0m : 2.47192
[1mStep[0m  [6/26], [94mLoss[0m : 2.43978
[1mStep[0m  [8/26], [94mLoss[0m : 2.42573
[1mStep[0m  [10/26], [94mLoss[0m : 2.47971
[1mStep[0m  [12/26], [94mLoss[0m : 2.34501
[1mStep[0m  [14/26], [94mLoss[0m : 2.45497
[1mStep[0m  [16/26], [94mLoss[0m : 2.40768
[1mStep[0m  [18/26], [94mLoss[0m : 2.39162
[1mStep[0m  [20/26], [94mLoss[0m : 2.48382
[1mStep[0m  [22/26], [94mLoss[0m : 2.42165
[1mStep[0m  [24/26], [94mLoss[0m : 2.49349

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.435, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.49843
[1mStep[0m  [2/26], [94mLoss[0m : 2.33157
[1mStep[0m  [4/26], [94mLoss[0m : 2.44696
[1mStep[0m  [6/26], [94mLoss[0m : 2.33248
[1mStep[0m  [8/26], [94mLoss[0m : 2.36325
[1mStep[0m  [10/26], [94mLoss[0m : 2.42930
[1mStep[0m  [12/26], [94mLoss[0m : 2.43062
[1mStep[0m  [14/26], [94mLoss[0m : 2.39896
[1mStep[0m  [16/26], [94mLoss[0m : 2.52961
[1mStep[0m  [18/26], [94mLoss[0m : 2.47889
[1mStep[0m  [20/26], [94mLoss[0m : 2.53127
[1mStep[0m  [22/26], [94mLoss[0m : 2.52246
[1mStep[0m  [24/26], [94mLoss[0m : 2.44108

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.445, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.36834
[1mStep[0m  [2/26], [94mLoss[0m : 2.35913
[1mStep[0m  [4/26], [94mLoss[0m : 2.44511
[1mStep[0m  [6/26], [94mLoss[0m : 2.41575
[1mStep[0m  [8/26], [94mLoss[0m : 2.50825
[1mStep[0m  [10/26], [94mLoss[0m : 2.50761
[1mStep[0m  [12/26], [94mLoss[0m : 2.52913
[1mStep[0m  [14/26], [94mLoss[0m : 2.39383
[1mStep[0m  [16/26], [94mLoss[0m : 2.32228
[1mStep[0m  [18/26], [94mLoss[0m : 2.46471
[1mStep[0m  [20/26], [94mLoss[0m : 2.34746
[1mStep[0m  [22/26], [94mLoss[0m : 2.37014
[1mStep[0m  [24/26], [94mLoss[0m : 2.54497

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.440, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.61192
[1mStep[0m  [2/26], [94mLoss[0m : 2.44516
[1mStep[0m  [4/26], [94mLoss[0m : 2.27089
[1mStep[0m  [6/26], [94mLoss[0m : 2.55029
[1mStep[0m  [8/26], [94mLoss[0m : 2.53620
[1mStep[0m  [10/26], [94mLoss[0m : 2.33254
[1mStep[0m  [12/26], [94mLoss[0m : 2.50767
[1mStep[0m  [14/26], [94mLoss[0m : 2.42393
[1mStep[0m  [16/26], [94mLoss[0m : 2.42943
[1mStep[0m  [18/26], [94mLoss[0m : 2.48873
[1mStep[0m  [20/26], [94mLoss[0m : 2.47317
[1mStep[0m  [22/26], [94mLoss[0m : 2.52710
[1mStep[0m  [24/26], [94mLoss[0m : 2.51459

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.449, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.50238
[1mStep[0m  [2/26], [94mLoss[0m : 2.42393
[1mStep[0m  [4/26], [94mLoss[0m : 2.36742
[1mStep[0m  [6/26], [94mLoss[0m : 2.40760
[1mStep[0m  [8/26], [94mLoss[0m : 2.46694
[1mStep[0m  [10/26], [94mLoss[0m : 2.42991
[1mStep[0m  [12/26], [94mLoss[0m : 2.57154
[1mStep[0m  [14/26], [94mLoss[0m : 2.44675
[1mStep[0m  [16/26], [94mLoss[0m : 2.68008
[1mStep[0m  [18/26], [94mLoss[0m : 2.43757
[1mStep[0m  [20/26], [94mLoss[0m : 2.44095
[1mStep[0m  [22/26], [94mLoss[0m : 2.50991
[1mStep[0m  [24/26], [94mLoss[0m : 2.35199

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.438
====================================

Phase 2 - Evaluation MAE:  2.43822849713839
MAE score P1      2.512801
MAE score P2      2.438228
loss              2.436433
learning_rate       0.0001
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.9
weight_decay          0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.64044
[1mStep[0m  [5/53], [94mLoss[0m : 10.75507
[1mStep[0m  [10/53], [94mLoss[0m : 11.00320
[1mStep[0m  [15/53], [94mLoss[0m : 10.73993
[1mStep[0m  [20/53], [94mLoss[0m : 10.91310
[1mStep[0m  [25/53], [94mLoss[0m : 11.11252
[1mStep[0m  [30/53], [94mLoss[0m : 10.64483
[1mStep[0m  [35/53], [94mLoss[0m : 10.71023
[1mStep[0m  [40/53], [94mLoss[0m : 10.92226
[1mStep[0m  [45/53], [94mLoss[0m : 10.92531
[1mStep[0m  [50/53], [94mLoss[0m : 10.62344

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.887, [92mTest[0m: 11.151, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.86648
[1mStep[0m  [5/53], [94mLoss[0m : 10.37683
[1mStep[0m  [10/53], [94mLoss[0m : 11.22301
[1mStep[0m  [15/53], [94mLoss[0m : 10.78519
[1mStep[0m  [20/53], [94mLoss[0m : 10.21054
[1mStep[0m  [25/53], [94mLoss[0m : 10.59275
[1mStep[0m  [30/53], [94mLoss[0m : 9.67610
[1mStep[0m  [35/53], [94mLoss[0m : 10.19664
[1mStep[0m  [40/53], [94mLoss[0m : 10.38132
[1mStep[0m  [45/53], [94mLoss[0m : 10.24957
[1mStep[0m  [50/53], [94mLoss[0m : 9.55539

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.673, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.14890
[1mStep[0m  [5/53], [94mLoss[0m : 10.02029
[1mStep[0m  [10/53], [94mLoss[0m : 10.74087
[1mStep[0m  [15/53], [94mLoss[0m : 10.10728
[1mStep[0m  [20/53], [94mLoss[0m : 9.89761
[1mStep[0m  [25/53], [94mLoss[0m : 10.12726
[1mStep[0m  [30/53], [94mLoss[0m : 9.83164
[1mStep[0m  [35/53], [94mLoss[0m : 9.66178
[1mStep[0m  [40/53], [94mLoss[0m : 9.72592
[1mStep[0m  [45/53], [94mLoss[0m : 9.15950
[1mStep[0m  [50/53], [94mLoss[0m : 10.14174

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.993, [92mTest[0m: 10.209, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.55004
[1mStep[0m  [5/53], [94mLoss[0m : 9.83241
[1mStep[0m  [10/53], [94mLoss[0m : 9.93346
[1mStep[0m  [15/53], [94mLoss[0m : 9.55291
[1mStep[0m  [20/53], [94mLoss[0m : 9.27092
[1mStep[0m  [25/53], [94mLoss[0m : 9.75315
[1mStep[0m  [30/53], [94mLoss[0m : 9.99605
[1mStep[0m  [35/53], [94mLoss[0m : 9.50033
[1mStep[0m  [40/53], [94mLoss[0m : 9.07563
[1mStep[0m  [45/53], [94mLoss[0m : 9.19933
[1mStep[0m  [50/53], [94mLoss[0m : 9.59474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.545, [92mTest[0m: 9.755, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.48289
[1mStep[0m  [5/53], [94mLoss[0m : 9.18325
[1mStep[0m  [10/53], [94mLoss[0m : 8.95678
[1mStep[0m  [15/53], [94mLoss[0m : 9.21414
[1mStep[0m  [20/53], [94mLoss[0m : 9.11215
[1mStep[0m  [25/53], [94mLoss[0m : 9.01113
[1mStep[0m  [30/53], [94mLoss[0m : 9.35584
[1mStep[0m  [35/53], [94mLoss[0m : 8.85600
[1mStep[0m  [40/53], [94mLoss[0m : 8.97925
[1mStep[0m  [45/53], [94mLoss[0m : 8.68824
[1mStep[0m  [50/53], [94mLoss[0m : 9.11519

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.110, [92mTest[0m: 9.312, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.09010
[1mStep[0m  [5/53], [94mLoss[0m : 8.79854
[1mStep[0m  [10/53], [94mLoss[0m : 9.02421
[1mStep[0m  [15/53], [94mLoss[0m : 8.78035
[1mStep[0m  [20/53], [94mLoss[0m : 8.54936
[1mStep[0m  [25/53], [94mLoss[0m : 8.79346
[1mStep[0m  [30/53], [94mLoss[0m : 8.63845
[1mStep[0m  [35/53], [94mLoss[0m : 8.07410
[1mStep[0m  [40/53], [94mLoss[0m : 8.68064
[1mStep[0m  [45/53], [94mLoss[0m : 8.30789
[1mStep[0m  [50/53], [94mLoss[0m : 8.10814

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.653, [92mTest[0m: 8.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.40291
[1mStep[0m  [5/53], [94mLoss[0m : 8.39762
[1mStep[0m  [10/53], [94mLoss[0m : 8.27906
[1mStep[0m  [15/53], [94mLoss[0m : 8.46390
[1mStep[0m  [20/53], [94mLoss[0m : 8.42968
[1mStep[0m  [25/53], [94mLoss[0m : 7.91801
[1mStep[0m  [30/53], [94mLoss[0m : 8.75163
[1mStep[0m  [35/53], [94mLoss[0m : 8.03600
[1mStep[0m  [40/53], [94mLoss[0m : 8.02105
[1mStep[0m  [45/53], [94mLoss[0m : 8.14388
[1mStep[0m  [50/53], [94mLoss[0m : 7.69544

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.207, [92mTest[0m: 8.428, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.10945
[1mStep[0m  [5/53], [94mLoss[0m : 7.70564
[1mStep[0m  [10/53], [94mLoss[0m : 7.76213
[1mStep[0m  [15/53], [94mLoss[0m : 7.74811
[1mStep[0m  [20/53], [94mLoss[0m : 7.70741
[1mStep[0m  [25/53], [94mLoss[0m : 7.69886
[1mStep[0m  [30/53], [94mLoss[0m : 7.81778
[1mStep[0m  [35/53], [94mLoss[0m : 7.73906
[1mStep[0m  [40/53], [94mLoss[0m : 7.40614
[1mStep[0m  [45/53], [94mLoss[0m : 7.80902
[1mStep[0m  [50/53], [94mLoss[0m : 7.62230

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.758, [92mTest[0m: 7.991, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.87847
[1mStep[0m  [5/53], [94mLoss[0m : 7.48871
[1mStep[0m  [10/53], [94mLoss[0m : 7.28711
[1mStep[0m  [15/53], [94mLoss[0m : 7.10633
[1mStep[0m  [20/53], [94mLoss[0m : 7.04693
[1mStep[0m  [25/53], [94mLoss[0m : 7.09780
[1mStep[0m  [30/53], [94mLoss[0m : 6.99540
[1mStep[0m  [35/53], [94mLoss[0m : 6.86422
[1mStep[0m  [40/53], [94mLoss[0m : 7.42548
[1mStep[0m  [45/53], [94mLoss[0m : 7.20969
[1mStep[0m  [50/53], [94mLoss[0m : 6.76410

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.312, [92mTest[0m: 7.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.86030
[1mStep[0m  [5/53], [94mLoss[0m : 7.27568
[1mStep[0m  [10/53], [94mLoss[0m : 6.87808
[1mStep[0m  [15/53], [94mLoss[0m : 7.15421
[1mStep[0m  [20/53], [94mLoss[0m : 7.27133
[1mStep[0m  [25/53], [94mLoss[0m : 7.05366
[1mStep[0m  [30/53], [94mLoss[0m : 7.10158
[1mStep[0m  [35/53], [94mLoss[0m : 6.79147
[1mStep[0m  [40/53], [94mLoss[0m : 6.87278
[1mStep[0m  [45/53], [94mLoss[0m : 6.69023
[1mStep[0m  [50/53], [94mLoss[0m : 6.73766

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.862, [92mTest[0m: 7.082, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.66005
[1mStep[0m  [5/53], [94mLoss[0m : 6.84221
[1mStep[0m  [10/53], [94mLoss[0m : 6.25151
[1mStep[0m  [15/53], [94mLoss[0m : 6.37686
[1mStep[0m  [20/53], [94mLoss[0m : 6.52167
[1mStep[0m  [25/53], [94mLoss[0m : 6.41438
[1mStep[0m  [30/53], [94mLoss[0m : 6.03712
[1mStep[0m  [35/53], [94mLoss[0m : 6.52003
[1mStep[0m  [40/53], [94mLoss[0m : 6.02044
[1mStep[0m  [45/53], [94mLoss[0m : 6.21426
[1mStep[0m  [50/53], [94mLoss[0m : 6.25916

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.414, [92mTest[0m: 6.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.23004
[1mStep[0m  [5/53], [94mLoss[0m : 6.00589
[1mStep[0m  [10/53], [94mLoss[0m : 6.19048
[1mStep[0m  [15/53], [94mLoss[0m : 5.96305
[1mStep[0m  [20/53], [94mLoss[0m : 5.75764
[1mStep[0m  [25/53], [94mLoss[0m : 6.02316
[1mStep[0m  [30/53], [94mLoss[0m : 6.02815
[1mStep[0m  [35/53], [94mLoss[0m : 6.35026
[1mStep[0m  [40/53], [94mLoss[0m : 5.42571
[1mStep[0m  [45/53], [94mLoss[0m : 5.99988
[1mStep[0m  [50/53], [94mLoss[0m : 5.96338

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.977, [92mTest[0m: 6.200, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.08711
[1mStep[0m  [5/53], [94mLoss[0m : 5.72073
[1mStep[0m  [10/53], [94mLoss[0m : 5.81157
[1mStep[0m  [15/53], [94mLoss[0m : 5.52646
[1mStep[0m  [20/53], [94mLoss[0m : 5.35124
[1mStep[0m  [25/53], [94mLoss[0m : 5.55756
[1mStep[0m  [30/53], [94mLoss[0m : 5.06948
[1mStep[0m  [35/53], [94mLoss[0m : 5.23493
[1mStep[0m  [40/53], [94mLoss[0m : 5.41481
[1mStep[0m  [45/53], [94mLoss[0m : 5.47086
[1mStep[0m  [50/53], [94mLoss[0m : 5.54398

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.553, [92mTest[0m: 5.752, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.47772
[1mStep[0m  [5/53], [94mLoss[0m : 5.62282
[1mStep[0m  [10/53], [94mLoss[0m : 5.36828
[1mStep[0m  [15/53], [94mLoss[0m : 5.27762
[1mStep[0m  [20/53], [94mLoss[0m : 5.23906
[1mStep[0m  [25/53], [94mLoss[0m : 5.29437
[1mStep[0m  [30/53], [94mLoss[0m : 5.24284
[1mStep[0m  [35/53], [94mLoss[0m : 4.73081
[1mStep[0m  [40/53], [94mLoss[0m : 5.30696
[1mStep[0m  [45/53], [94mLoss[0m : 4.99179
[1mStep[0m  [50/53], [94mLoss[0m : 5.06795

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.151, [92mTest[0m: 5.342, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.99901
[1mStep[0m  [5/53], [94mLoss[0m : 5.06495
[1mStep[0m  [10/53], [94mLoss[0m : 4.46801
[1mStep[0m  [15/53], [94mLoss[0m : 5.12305
[1mStep[0m  [20/53], [94mLoss[0m : 4.93392
[1mStep[0m  [25/53], [94mLoss[0m : 4.98552
[1mStep[0m  [30/53], [94mLoss[0m : 4.38574
[1mStep[0m  [35/53], [94mLoss[0m : 4.52289
[1mStep[0m  [40/53], [94mLoss[0m : 4.95189
[1mStep[0m  [45/53], [94mLoss[0m : 4.59605
[1mStep[0m  [50/53], [94mLoss[0m : 4.54875

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.790, [92mTest[0m: 4.951, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.50671
[1mStep[0m  [5/53], [94mLoss[0m : 4.52522
[1mStep[0m  [10/53], [94mLoss[0m : 4.79719
[1mStep[0m  [15/53], [94mLoss[0m : 4.54192
[1mStep[0m  [20/53], [94mLoss[0m : 4.12158
[1mStep[0m  [25/53], [94mLoss[0m : 4.58293
[1mStep[0m  [30/53], [94mLoss[0m : 4.56233
[1mStep[0m  [35/53], [94mLoss[0m : 4.14960
[1mStep[0m  [40/53], [94mLoss[0m : 4.34720
[1mStep[0m  [45/53], [94mLoss[0m : 4.16103
[1mStep[0m  [50/53], [94mLoss[0m : 4.50470

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.485, [92mTest[0m: 4.625, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.22709
[1mStep[0m  [5/53], [94mLoss[0m : 4.17169
[1mStep[0m  [10/53], [94mLoss[0m : 4.49810
[1mStep[0m  [15/53], [94mLoss[0m : 4.28413
[1mStep[0m  [20/53], [94mLoss[0m : 4.08995
[1mStep[0m  [25/53], [94mLoss[0m : 4.08189
[1mStep[0m  [30/53], [94mLoss[0m : 3.99970
[1mStep[0m  [35/53], [94mLoss[0m : 4.60673
[1mStep[0m  [40/53], [94mLoss[0m : 4.01033
[1mStep[0m  [45/53], [94mLoss[0m : 4.47662
[1mStep[0m  [50/53], [94mLoss[0m : 4.26855

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.219, [92mTest[0m: 4.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.33944
[1mStep[0m  [5/53], [94mLoss[0m : 4.24229
[1mStep[0m  [10/53], [94mLoss[0m : 4.14035
[1mStep[0m  [15/53], [94mLoss[0m : 4.26461
[1mStep[0m  [20/53], [94mLoss[0m : 3.95073
[1mStep[0m  [25/53], [94mLoss[0m : 4.25180
[1mStep[0m  [30/53], [94mLoss[0m : 4.08480
[1mStep[0m  [35/53], [94mLoss[0m : 4.10182
[1mStep[0m  [40/53], [94mLoss[0m : 4.02993
[1mStep[0m  [45/53], [94mLoss[0m : 3.62826
[1mStep[0m  [50/53], [94mLoss[0m : 3.76867

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.995, [92mTest[0m: 4.083, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.45000
[1mStep[0m  [5/53], [94mLoss[0m : 3.69018
[1mStep[0m  [10/53], [94mLoss[0m : 3.92346
[1mStep[0m  [15/53], [94mLoss[0m : 3.78170
[1mStep[0m  [20/53], [94mLoss[0m : 3.97421
[1mStep[0m  [25/53], [94mLoss[0m : 3.58440
[1mStep[0m  [30/53], [94mLoss[0m : 3.60411
[1mStep[0m  [35/53], [94mLoss[0m : 4.24232
[1mStep[0m  [40/53], [94mLoss[0m : 3.89477
[1mStep[0m  [45/53], [94mLoss[0m : 3.82949
[1mStep[0m  [50/53], [94mLoss[0m : 3.82420

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.806, [92mTest[0m: 3.866, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.84911
[1mStep[0m  [5/53], [94mLoss[0m : 3.95099
[1mStep[0m  [10/53], [94mLoss[0m : 3.30884
[1mStep[0m  [15/53], [94mLoss[0m : 3.84470
[1mStep[0m  [20/53], [94mLoss[0m : 3.39430
[1mStep[0m  [25/53], [94mLoss[0m : 3.73371
[1mStep[0m  [30/53], [94mLoss[0m : 3.29138
[1mStep[0m  [35/53], [94mLoss[0m : 3.62411
[1mStep[0m  [40/53], [94mLoss[0m : 3.65220
[1mStep[0m  [45/53], [94mLoss[0m : 3.59951
[1mStep[0m  [50/53], [94mLoss[0m : 3.58229

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.645, [92mTest[0m: 3.697, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.42460
[1mStep[0m  [5/53], [94mLoss[0m : 3.22801
[1mStep[0m  [10/53], [94mLoss[0m : 3.50616
[1mStep[0m  [15/53], [94mLoss[0m : 3.62771
[1mStep[0m  [20/53], [94mLoss[0m : 3.49204
[1mStep[0m  [25/53], [94mLoss[0m : 3.65302
[1mStep[0m  [30/53], [94mLoss[0m : 3.53823
[1mStep[0m  [35/53], [94mLoss[0m : 3.52904
[1mStep[0m  [40/53], [94mLoss[0m : 3.28082
[1mStep[0m  [45/53], [94mLoss[0m : 3.79059
[1mStep[0m  [50/53], [94mLoss[0m : 3.49767

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.521, [92mTest[0m: 3.538, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.81628
[1mStep[0m  [5/53], [94mLoss[0m : 3.73981
[1mStep[0m  [10/53], [94mLoss[0m : 3.47128
[1mStep[0m  [15/53], [94mLoss[0m : 3.46273
[1mStep[0m  [20/53], [94mLoss[0m : 3.25237
[1mStep[0m  [25/53], [94mLoss[0m : 3.12495
[1mStep[0m  [30/53], [94mLoss[0m : 3.20832
[1mStep[0m  [35/53], [94mLoss[0m : 3.30708
[1mStep[0m  [40/53], [94mLoss[0m : 3.25093
[1mStep[0m  [45/53], [94mLoss[0m : 3.20167
[1mStep[0m  [50/53], [94mLoss[0m : 3.51039

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.413, [92mTest[0m: 3.442, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.31483
[1mStep[0m  [5/53], [94mLoss[0m : 3.14939
[1mStep[0m  [10/53], [94mLoss[0m : 2.93975
[1mStep[0m  [15/53], [94mLoss[0m : 3.10534
[1mStep[0m  [20/53], [94mLoss[0m : 3.48378
[1mStep[0m  [25/53], [94mLoss[0m : 3.51148
[1mStep[0m  [30/53], [94mLoss[0m : 3.48196
[1mStep[0m  [35/53], [94mLoss[0m : 3.38235
[1mStep[0m  [40/53], [94mLoss[0m : 3.29037
[1mStep[0m  [45/53], [94mLoss[0m : 3.10798
[1mStep[0m  [50/53], [94mLoss[0m : 3.25445

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.340, [92mTest[0m: 3.340, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.35576
[1mStep[0m  [5/53], [94mLoss[0m : 3.22774
[1mStep[0m  [10/53], [94mLoss[0m : 3.42786
[1mStep[0m  [15/53], [94mLoss[0m : 3.09268
[1mStep[0m  [20/53], [94mLoss[0m : 3.40006
[1mStep[0m  [25/53], [94mLoss[0m : 3.28831
[1mStep[0m  [30/53], [94mLoss[0m : 2.91927
[1mStep[0m  [35/53], [94mLoss[0m : 3.38332
[1mStep[0m  [40/53], [94mLoss[0m : 3.03514
[1mStep[0m  [45/53], [94mLoss[0m : 3.38376
[1mStep[0m  [50/53], [94mLoss[0m : 3.42985

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.262, [92mTest[0m: 3.267, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.24602
[1mStep[0m  [5/53], [94mLoss[0m : 3.19885
[1mStep[0m  [10/53], [94mLoss[0m : 3.13045
[1mStep[0m  [15/53], [94mLoss[0m : 3.18612
[1mStep[0m  [20/53], [94mLoss[0m : 3.33898
[1mStep[0m  [25/53], [94mLoss[0m : 3.05572
[1mStep[0m  [30/53], [94mLoss[0m : 3.17916
[1mStep[0m  [35/53], [94mLoss[0m : 2.92651
[1mStep[0m  [40/53], [94mLoss[0m : 2.97374
[1mStep[0m  [45/53], [94mLoss[0m : 3.15841
[1mStep[0m  [50/53], [94mLoss[0m : 3.12474

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.212, [92mTest[0m: 3.211, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.03118
[1mStep[0m  [5/53], [94mLoss[0m : 3.18344
[1mStep[0m  [10/53], [94mLoss[0m : 3.22858
[1mStep[0m  [15/53], [94mLoss[0m : 2.92300
[1mStep[0m  [20/53], [94mLoss[0m : 2.98569
[1mStep[0m  [25/53], [94mLoss[0m : 3.03677
[1mStep[0m  [30/53], [94mLoss[0m : 3.14663
[1mStep[0m  [35/53], [94mLoss[0m : 3.24207
[1mStep[0m  [40/53], [94mLoss[0m : 3.25775
[1mStep[0m  [45/53], [94mLoss[0m : 3.43422
[1mStep[0m  [50/53], [94mLoss[0m : 3.23085

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.154, [92mTest[0m: 3.152, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.14230
[1mStep[0m  [5/53], [94mLoss[0m : 2.90931
[1mStep[0m  [10/53], [94mLoss[0m : 2.94863
[1mStep[0m  [15/53], [94mLoss[0m : 3.34336
[1mStep[0m  [20/53], [94mLoss[0m : 3.32042
[1mStep[0m  [25/53], [94mLoss[0m : 3.16931
[1mStep[0m  [30/53], [94mLoss[0m : 2.99808
[1mStep[0m  [35/53], [94mLoss[0m : 3.26577
[1mStep[0m  [40/53], [94mLoss[0m : 3.04079
[1mStep[0m  [45/53], [94mLoss[0m : 2.86827
[1mStep[0m  [50/53], [94mLoss[0m : 3.14166

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.118, [92mTest[0m: 3.104, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.27344
[1mStep[0m  [5/53], [94mLoss[0m : 3.10218
[1mStep[0m  [10/53], [94mLoss[0m : 3.41607
[1mStep[0m  [15/53], [94mLoss[0m : 3.18079
[1mStep[0m  [20/53], [94mLoss[0m : 3.03562
[1mStep[0m  [25/53], [94mLoss[0m : 3.03735
[1mStep[0m  [30/53], [94mLoss[0m : 3.11010
[1mStep[0m  [35/53], [94mLoss[0m : 2.98860
[1mStep[0m  [40/53], [94mLoss[0m : 3.14497
[1mStep[0m  [45/53], [94mLoss[0m : 2.79693
[1mStep[0m  [50/53], [94mLoss[0m : 3.20634

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.077, [92mTest[0m: 3.058, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86008
[1mStep[0m  [5/53], [94mLoss[0m : 3.10971
[1mStep[0m  [10/53], [94mLoss[0m : 2.97344
[1mStep[0m  [15/53], [94mLoss[0m : 3.28774
[1mStep[0m  [20/53], [94mLoss[0m : 3.01524
[1mStep[0m  [25/53], [94mLoss[0m : 2.77004
[1mStep[0m  [30/53], [94mLoss[0m : 2.98761
[1mStep[0m  [35/53], [94mLoss[0m : 3.33177
[1mStep[0m  [40/53], [94mLoss[0m : 2.62636
[1mStep[0m  [45/53], [94mLoss[0m : 3.18377
[1mStep[0m  [50/53], [94mLoss[0m : 2.92685

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.044, [92mTest[0m: 3.023, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.11944
[1mStep[0m  [5/53], [94mLoss[0m : 3.13247
[1mStep[0m  [10/53], [94mLoss[0m : 2.98885
[1mStep[0m  [15/53], [94mLoss[0m : 3.05051
[1mStep[0m  [20/53], [94mLoss[0m : 2.91747
[1mStep[0m  [25/53], [94mLoss[0m : 2.82751
[1mStep[0m  [30/53], [94mLoss[0m : 2.90746
[1mStep[0m  [35/53], [94mLoss[0m : 3.15185
[1mStep[0m  [40/53], [94mLoss[0m : 3.14240
[1mStep[0m  [45/53], [94mLoss[0m : 3.05610
[1mStep[0m  [50/53], [94mLoss[0m : 3.11526

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.009, [92mTest[0m: 3.002, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.972
====================================

Phase 1 - Evaluation MAE:  2.9717847200540395
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 3.05161
[1mStep[0m  [5/53], [94mLoss[0m : 3.07406
[1mStep[0m  [10/53], [94mLoss[0m : 3.12555
[1mStep[0m  [15/53], [94mLoss[0m : 3.08734
[1mStep[0m  [20/53], [94mLoss[0m : 2.99760
[1mStep[0m  [25/53], [94mLoss[0m : 3.08712
[1mStep[0m  [30/53], [94mLoss[0m : 3.02838
[1mStep[0m  [35/53], [94mLoss[0m : 3.06183
[1mStep[0m  [40/53], [94mLoss[0m : 3.03476
[1mStep[0m  [45/53], [94mLoss[0m : 2.93522
[1mStep[0m  [50/53], [94mLoss[0m : 2.96853

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.986, [92mTest[0m: 2.968, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.03417
[1mStep[0m  [5/53], [94mLoss[0m : 2.96152
[1mStep[0m  [10/53], [94mLoss[0m : 2.78366
[1mStep[0m  [15/53], [94mLoss[0m : 2.98781
[1mStep[0m  [20/53], [94mLoss[0m : 2.98639
[1mStep[0m  [25/53], [94mLoss[0m : 3.01420
[1mStep[0m  [30/53], [94mLoss[0m : 3.26858
[1mStep[0m  [35/53], [94mLoss[0m : 3.12611
[1mStep[0m  [40/53], [94mLoss[0m : 2.81189
[1mStep[0m  [45/53], [94mLoss[0m : 2.99217
[1mStep[0m  [50/53], [94mLoss[0m : 2.78960

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.947, [92mTest[0m: 2.930, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.95787
[1mStep[0m  [5/53], [94mLoss[0m : 2.95931
[1mStep[0m  [10/53], [94mLoss[0m : 3.06322
[1mStep[0m  [15/53], [94mLoss[0m : 3.13545
[1mStep[0m  [20/53], [94mLoss[0m : 3.04988
[1mStep[0m  [25/53], [94mLoss[0m : 2.85550
[1mStep[0m  [30/53], [94mLoss[0m : 2.81565
[1mStep[0m  [35/53], [94mLoss[0m : 2.76061
[1mStep[0m  [40/53], [94mLoss[0m : 2.88313
[1mStep[0m  [45/53], [94mLoss[0m : 2.62111
[1mStep[0m  [50/53], [94mLoss[0m : 3.03149

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.916, [92mTest[0m: 2.891, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.03587
[1mStep[0m  [5/53], [94mLoss[0m : 2.63832
[1mStep[0m  [10/53], [94mLoss[0m : 3.19004
[1mStep[0m  [15/53], [94mLoss[0m : 2.84071
[1mStep[0m  [20/53], [94mLoss[0m : 3.01878
[1mStep[0m  [25/53], [94mLoss[0m : 2.66931
[1mStep[0m  [30/53], [94mLoss[0m : 3.16776
[1mStep[0m  [35/53], [94mLoss[0m : 3.08340
[1mStep[0m  [40/53], [94mLoss[0m : 2.91663
[1mStep[0m  [45/53], [94mLoss[0m : 3.12217
[1mStep[0m  [50/53], [94mLoss[0m : 2.93155

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.892, [92mTest[0m: 2.866, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.94558
[1mStep[0m  [5/53], [94mLoss[0m : 3.10616
[1mStep[0m  [10/53], [94mLoss[0m : 2.93825
[1mStep[0m  [15/53], [94mLoss[0m : 2.74855
[1mStep[0m  [20/53], [94mLoss[0m : 2.80516
[1mStep[0m  [25/53], [94mLoss[0m : 2.81546
[1mStep[0m  [30/53], [94mLoss[0m : 2.68299
[1mStep[0m  [35/53], [94mLoss[0m : 2.86341
[1mStep[0m  [40/53], [94mLoss[0m : 2.89730
[1mStep[0m  [45/53], [94mLoss[0m : 2.80535
[1mStep[0m  [50/53], [94mLoss[0m : 2.89546

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.858, [92mTest[0m: 2.834, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.13509
[1mStep[0m  [5/53], [94mLoss[0m : 3.02636
[1mStep[0m  [10/53], [94mLoss[0m : 2.95783
[1mStep[0m  [15/53], [94mLoss[0m : 3.00174
[1mStep[0m  [20/53], [94mLoss[0m : 2.88472
[1mStep[0m  [25/53], [94mLoss[0m : 2.92937
[1mStep[0m  [30/53], [94mLoss[0m : 2.81921
[1mStep[0m  [35/53], [94mLoss[0m : 2.74715
[1mStep[0m  [40/53], [94mLoss[0m : 2.87376
[1mStep[0m  [45/53], [94mLoss[0m : 2.70890
[1mStep[0m  [50/53], [94mLoss[0m : 2.97420

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.833, [92mTest[0m: 2.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86885
[1mStep[0m  [5/53], [94mLoss[0m : 2.72130
[1mStep[0m  [10/53], [94mLoss[0m : 3.21389
[1mStep[0m  [15/53], [94mLoss[0m : 3.03299
[1mStep[0m  [20/53], [94mLoss[0m : 3.00717
[1mStep[0m  [25/53], [94mLoss[0m : 2.86058
[1mStep[0m  [30/53], [94mLoss[0m : 2.85563
[1mStep[0m  [35/53], [94mLoss[0m : 2.56860
[1mStep[0m  [40/53], [94mLoss[0m : 2.80815
[1mStep[0m  [45/53], [94mLoss[0m : 2.56797
[1mStep[0m  [50/53], [94mLoss[0m : 3.00771

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.810, [92mTest[0m: 2.793, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60094
[1mStep[0m  [5/53], [94mLoss[0m : 3.02951
[1mStep[0m  [10/53], [94mLoss[0m : 2.71056
[1mStep[0m  [15/53], [94mLoss[0m : 2.74753
[1mStep[0m  [20/53], [94mLoss[0m : 2.85206
[1mStep[0m  [25/53], [94mLoss[0m : 2.77804
[1mStep[0m  [30/53], [94mLoss[0m : 2.74033
[1mStep[0m  [35/53], [94mLoss[0m : 2.63524
[1mStep[0m  [40/53], [94mLoss[0m : 2.85847
[1mStep[0m  [45/53], [94mLoss[0m : 2.89313
[1mStep[0m  [50/53], [94mLoss[0m : 2.74119

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.793, [92mTest[0m: 2.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65578
[1mStep[0m  [5/53], [94mLoss[0m : 2.98543
[1mStep[0m  [10/53], [94mLoss[0m : 2.96195
[1mStep[0m  [15/53], [94mLoss[0m : 2.78731
[1mStep[0m  [20/53], [94mLoss[0m : 2.69894
[1mStep[0m  [25/53], [94mLoss[0m : 2.72926
[1mStep[0m  [30/53], [94mLoss[0m : 2.95288
[1mStep[0m  [35/53], [94mLoss[0m : 2.65463
[1mStep[0m  [40/53], [94mLoss[0m : 2.74875
[1mStep[0m  [45/53], [94mLoss[0m : 2.62660
[1mStep[0m  [50/53], [94mLoss[0m : 2.59988

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.768, [92mTest[0m: 2.740, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.94782
[1mStep[0m  [5/53], [94mLoss[0m : 2.78274
[1mStep[0m  [10/53], [94mLoss[0m : 2.70692
[1mStep[0m  [15/53], [94mLoss[0m : 2.94007
[1mStep[0m  [20/53], [94mLoss[0m : 3.02160
[1mStep[0m  [25/53], [94mLoss[0m : 2.69248
[1mStep[0m  [30/53], [94mLoss[0m : 2.60738
[1mStep[0m  [35/53], [94mLoss[0m : 2.84329
[1mStep[0m  [40/53], [94mLoss[0m : 2.72985
[1mStep[0m  [45/53], [94mLoss[0m : 2.62518
[1mStep[0m  [50/53], [94mLoss[0m : 2.69769

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.755, [92mTest[0m: 2.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.78352
[1mStep[0m  [5/53], [94mLoss[0m : 2.62905
[1mStep[0m  [10/53], [94mLoss[0m : 2.60854
[1mStep[0m  [15/53], [94mLoss[0m : 2.77502
[1mStep[0m  [20/53], [94mLoss[0m : 2.60376
[1mStep[0m  [25/53], [94mLoss[0m : 2.82438
[1mStep[0m  [30/53], [94mLoss[0m : 2.92259
[1mStep[0m  [35/53], [94mLoss[0m : 2.68504
[1mStep[0m  [40/53], [94mLoss[0m : 2.66522
[1mStep[0m  [45/53], [94mLoss[0m : 2.76728
[1mStep[0m  [50/53], [94mLoss[0m : 2.57009

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.749, [92mTest[0m: 2.711, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72690
[1mStep[0m  [5/53], [94mLoss[0m : 2.67332
[1mStep[0m  [10/53], [94mLoss[0m : 2.71270
[1mStep[0m  [15/53], [94mLoss[0m : 2.86501
[1mStep[0m  [20/53], [94mLoss[0m : 2.65757
[1mStep[0m  [25/53], [94mLoss[0m : 2.48658
[1mStep[0m  [30/53], [94mLoss[0m : 2.79365
[1mStep[0m  [35/53], [94mLoss[0m : 2.87960
[1mStep[0m  [40/53], [94mLoss[0m : 2.89774
[1mStep[0m  [45/53], [94mLoss[0m : 2.51619
[1mStep[0m  [50/53], [94mLoss[0m : 2.56903

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.694, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75656
[1mStep[0m  [5/53], [94mLoss[0m : 2.52433
[1mStep[0m  [10/53], [94mLoss[0m : 2.94963
[1mStep[0m  [15/53], [94mLoss[0m : 2.88853
[1mStep[0m  [20/53], [94mLoss[0m : 2.68965
[1mStep[0m  [25/53], [94mLoss[0m : 2.79413
[1mStep[0m  [30/53], [94mLoss[0m : 2.70588
[1mStep[0m  [35/53], [94mLoss[0m : 2.58231
[1mStep[0m  [40/53], [94mLoss[0m : 2.83297
[1mStep[0m  [45/53], [94mLoss[0m : 2.71184
[1mStep[0m  [50/53], [94mLoss[0m : 2.62749

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.679, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.77724
[1mStep[0m  [5/53], [94mLoss[0m : 2.73941
[1mStep[0m  [10/53], [94mLoss[0m : 2.76531
[1mStep[0m  [15/53], [94mLoss[0m : 2.60914
[1mStep[0m  [20/53], [94mLoss[0m : 2.88513
[1mStep[0m  [25/53], [94mLoss[0m : 2.81561
[1mStep[0m  [30/53], [94mLoss[0m : 2.66164
[1mStep[0m  [35/53], [94mLoss[0m : 2.48272
[1mStep[0m  [40/53], [94mLoss[0m : 2.80656
[1mStep[0m  [45/53], [94mLoss[0m : 2.68157
[1mStep[0m  [50/53], [94mLoss[0m : 2.74512

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63996
[1mStep[0m  [5/53], [94mLoss[0m : 2.54976
[1mStep[0m  [10/53], [94mLoss[0m : 2.63246
[1mStep[0m  [15/53], [94mLoss[0m : 2.65593
[1mStep[0m  [20/53], [94mLoss[0m : 2.89938
[1mStep[0m  [25/53], [94mLoss[0m : 2.61196
[1mStep[0m  [30/53], [94mLoss[0m : 2.57163
[1mStep[0m  [35/53], [94mLoss[0m : 2.93130
[1mStep[0m  [40/53], [94mLoss[0m : 2.67880
[1mStep[0m  [45/53], [94mLoss[0m : 2.82931
[1mStep[0m  [50/53], [94mLoss[0m : 2.75000

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.658, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.38799
[1mStep[0m  [5/53], [94mLoss[0m : 2.70203
[1mStep[0m  [10/53], [94mLoss[0m : 2.57024
[1mStep[0m  [15/53], [94mLoss[0m : 2.74091
[1mStep[0m  [20/53], [94mLoss[0m : 2.56153
[1mStep[0m  [25/53], [94mLoss[0m : 2.64197
[1mStep[0m  [30/53], [94mLoss[0m : 2.66721
[1mStep[0m  [35/53], [94mLoss[0m : 2.57999
[1mStep[0m  [40/53], [94mLoss[0m : 2.62230
[1mStep[0m  [45/53], [94mLoss[0m : 2.72250
[1mStep[0m  [50/53], [94mLoss[0m : 2.53271

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.675, [92mTest[0m: 2.656, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.65431
[1mStep[0m  [5/53], [94mLoss[0m : 2.81933
[1mStep[0m  [10/53], [94mLoss[0m : 2.54364
[1mStep[0m  [15/53], [94mLoss[0m : 2.61135
[1mStep[0m  [20/53], [94mLoss[0m : 2.78337
[1mStep[0m  [25/53], [94mLoss[0m : 2.68998
[1mStep[0m  [30/53], [94mLoss[0m : 2.66284
[1mStep[0m  [35/53], [94mLoss[0m : 2.59807
[1mStep[0m  [40/53], [94mLoss[0m : 2.52899
[1mStep[0m  [45/53], [94mLoss[0m : 2.35748
[1mStep[0m  [50/53], [94mLoss[0m : 2.82807

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61837
[1mStep[0m  [5/53], [94mLoss[0m : 2.58443
[1mStep[0m  [10/53], [94mLoss[0m : 2.74599
[1mStep[0m  [15/53], [94mLoss[0m : 2.51167
[1mStep[0m  [20/53], [94mLoss[0m : 2.79567
[1mStep[0m  [25/53], [94mLoss[0m : 2.34410
[1mStep[0m  [30/53], [94mLoss[0m : 2.79139
[1mStep[0m  [35/53], [94mLoss[0m : 2.51524
[1mStep[0m  [40/53], [94mLoss[0m : 2.42613
[1mStep[0m  [45/53], [94mLoss[0m : 2.91738
[1mStep[0m  [50/53], [94mLoss[0m : 2.67597

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.57951
[1mStep[0m  [5/53], [94mLoss[0m : 2.77569
[1mStep[0m  [10/53], [94mLoss[0m : 2.91017
[1mStep[0m  [15/53], [94mLoss[0m : 2.67872
[1mStep[0m  [20/53], [94mLoss[0m : 2.69964
[1mStep[0m  [25/53], [94mLoss[0m : 2.64653
[1mStep[0m  [30/53], [94mLoss[0m : 2.60949
[1mStep[0m  [35/53], [94mLoss[0m : 2.65499
[1mStep[0m  [40/53], [94mLoss[0m : 2.81773
[1mStep[0m  [45/53], [94mLoss[0m : 2.90509
[1mStep[0m  [50/53], [94mLoss[0m : 2.55336

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.653, [92mTest[0m: 2.616, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63732
[1mStep[0m  [5/53], [94mLoss[0m : 2.85481
[1mStep[0m  [10/53], [94mLoss[0m : 2.68229
[1mStep[0m  [15/53], [94mLoss[0m : 2.88907
[1mStep[0m  [20/53], [94mLoss[0m : 2.56557
[1mStep[0m  [25/53], [94mLoss[0m : 2.70409
[1mStep[0m  [30/53], [94mLoss[0m : 2.64429
[1mStep[0m  [35/53], [94mLoss[0m : 2.60568
[1mStep[0m  [40/53], [94mLoss[0m : 2.75656
[1mStep[0m  [45/53], [94mLoss[0m : 2.70607
[1mStep[0m  [50/53], [94mLoss[0m : 2.73004

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.619, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.64198
[1mStep[0m  [5/53], [94mLoss[0m : 2.67885
[1mStep[0m  [10/53], [94mLoss[0m : 2.57578
[1mStep[0m  [15/53], [94mLoss[0m : 2.63504
[1mStep[0m  [20/53], [94mLoss[0m : 2.64198
[1mStep[0m  [25/53], [94mLoss[0m : 2.79156
[1mStep[0m  [30/53], [94mLoss[0m : 2.57327
[1mStep[0m  [35/53], [94mLoss[0m : 2.57154
[1mStep[0m  [40/53], [94mLoss[0m : 2.55935
[1mStep[0m  [45/53], [94mLoss[0m : 2.49120
[1mStep[0m  [50/53], [94mLoss[0m : 2.71039

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.605, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70596
[1mStep[0m  [5/53], [94mLoss[0m : 2.84063
[1mStep[0m  [10/53], [94mLoss[0m : 2.58314
[1mStep[0m  [15/53], [94mLoss[0m : 2.68266
[1mStep[0m  [20/53], [94mLoss[0m : 2.55972
[1mStep[0m  [25/53], [94mLoss[0m : 2.49187
[1mStep[0m  [30/53], [94mLoss[0m : 2.90202
[1mStep[0m  [35/53], [94mLoss[0m : 2.69069
[1mStep[0m  [40/53], [94mLoss[0m : 2.57781
[1mStep[0m  [45/53], [94mLoss[0m : 2.64027
[1mStep[0m  [50/53], [94mLoss[0m : 2.58615

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.604, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.69827
[1mStep[0m  [5/53], [94mLoss[0m : 2.68258
[1mStep[0m  [10/53], [94mLoss[0m : 2.80923
[1mStep[0m  [15/53], [94mLoss[0m : 2.43693
[1mStep[0m  [20/53], [94mLoss[0m : 2.59276
[1mStep[0m  [25/53], [94mLoss[0m : 2.91946
[1mStep[0m  [30/53], [94mLoss[0m : 2.61491
[1mStep[0m  [35/53], [94mLoss[0m : 2.74108
[1mStep[0m  [40/53], [94mLoss[0m : 2.66298
[1mStep[0m  [45/53], [94mLoss[0m : 2.70188
[1mStep[0m  [50/53], [94mLoss[0m : 2.59428

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.627, [92mTest[0m: 2.592, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58546
[1mStep[0m  [5/53], [94mLoss[0m : 2.79352
[1mStep[0m  [10/53], [94mLoss[0m : 2.61145
[1mStep[0m  [15/53], [94mLoss[0m : 2.64988
[1mStep[0m  [20/53], [94mLoss[0m : 2.51294
[1mStep[0m  [25/53], [94mLoss[0m : 2.64901
[1mStep[0m  [30/53], [94mLoss[0m : 2.67665
[1mStep[0m  [35/53], [94mLoss[0m : 2.42544
[1mStep[0m  [40/53], [94mLoss[0m : 2.53537
[1mStep[0m  [45/53], [94mLoss[0m : 2.79522
[1mStep[0m  [50/53], [94mLoss[0m : 2.47037

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.589, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63581
[1mStep[0m  [5/53], [94mLoss[0m : 2.61279
[1mStep[0m  [10/53], [94mLoss[0m : 2.52486
[1mStep[0m  [15/53], [94mLoss[0m : 2.68047
[1mStep[0m  [20/53], [94mLoss[0m : 2.69529
[1mStep[0m  [25/53], [94mLoss[0m : 2.65084
[1mStep[0m  [30/53], [94mLoss[0m : 2.50127
[1mStep[0m  [35/53], [94mLoss[0m : 2.77852
[1mStep[0m  [40/53], [94mLoss[0m : 2.57050
[1mStep[0m  [45/53], [94mLoss[0m : 2.55167
[1mStep[0m  [50/53], [94mLoss[0m : 2.55715

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.588, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.52212
[1mStep[0m  [5/53], [94mLoss[0m : 2.52649
[1mStep[0m  [10/53], [94mLoss[0m : 2.49204
[1mStep[0m  [15/53], [94mLoss[0m : 2.46619
[1mStep[0m  [20/53], [94mLoss[0m : 2.54306
[1mStep[0m  [25/53], [94mLoss[0m : 2.74736
[1mStep[0m  [30/53], [94mLoss[0m : 2.35638
[1mStep[0m  [35/53], [94mLoss[0m : 2.52416
[1mStep[0m  [40/53], [94mLoss[0m : 2.81523
[1mStep[0m  [45/53], [94mLoss[0m : 2.66144
[1mStep[0m  [50/53], [94mLoss[0m : 2.64686

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.577, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.81370
[1mStep[0m  [5/53], [94mLoss[0m : 2.64615
[1mStep[0m  [10/53], [94mLoss[0m : 2.57991
[1mStep[0m  [15/53], [94mLoss[0m : 2.55590
[1mStep[0m  [20/53], [94mLoss[0m : 2.46231
[1mStep[0m  [25/53], [94mLoss[0m : 2.45101
[1mStep[0m  [30/53], [94mLoss[0m : 2.56563
[1mStep[0m  [35/53], [94mLoss[0m : 2.53537
[1mStep[0m  [40/53], [94mLoss[0m : 2.87430
[1mStep[0m  [45/53], [94mLoss[0m : 2.60159
[1mStep[0m  [50/53], [94mLoss[0m : 2.72012

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.72364
[1mStep[0m  [5/53], [94mLoss[0m : 2.75954
[1mStep[0m  [10/53], [94mLoss[0m : 2.65669
[1mStep[0m  [15/53], [94mLoss[0m : 2.33367
[1mStep[0m  [20/53], [94mLoss[0m : 2.60417
[1mStep[0m  [25/53], [94mLoss[0m : 2.57480
[1mStep[0m  [30/53], [94mLoss[0m : 2.63778
[1mStep[0m  [35/53], [94mLoss[0m : 2.73419
[1mStep[0m  [40/53], [94mLoss[0m : 2.82778
[1mStep[0m  [45/53], [94mLoss[0m : 2.41280
[1mStep[0m  [50/53], [94mLoss[0m : 2.81894

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61335
[1mStep[0m  [5/53], [94mLoss[0m : 2.45085
[1mStep[0m  [10/53], [94mLoss[0m : 2.70971
[1mStep[0m  [15/53], [94mLoss[0m : 2.74306
[1mStep[0m  [20/53], [94mLoss[0m : 2.68732
[1mStep[0m  [25/53], [94mLoss[0m : 2.63424
[1mStep[0m  [30/53], [94mLoss[0m : 2.47411
[1mStep[0m  [35/53], [94mLoss[0m : 2.78939
[1mStep[0m  [40/53], [94mLoss[0m : 2.43009
[1mStep[0m  [45/53], [94mLoss[0m : 2.66002
[1mStep[0m  [50/53], [94mLoss[0m : 2.62805

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43109
[1mStep[0m  [5/53], [94mLoss[0m : 2.48673
[1mStep[0m  [10/53], [94mLoss[0m : 2.52024
[1mStep[0m  [15/53], [94mLoss[0m : 2.65741
[1mStep[0m  [20/53], [94mLoss[0m : 2.63013
[1mStep[0m  [25/53], [94mLoss[0m : 2.81404
[1mStep[0m  [30/53], [94mLoss[0m : 2.54041
[1mStep[0m  [35/53], [94mLoss[0m : 2.86629
[1mStep[0m  [40/53], [94mLoss[0m : 2.55020
[1mStep[0m  [45/53], [94mLoss[0m : 2.61274
[1mStep[0m  [50/53], [94mLoss[0m : 2.40643

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.576, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.567
====================================

Phase 2 - Evaluation MAE:  2.567443746786851
MAE score P1      2.971785
MAE score P2      2.567444
loss              2.593805
learning_rate       0.0001
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.96408
[1mStep[0m  [2/26], [94mLoss[0m : 10.85694
[1mStep[0m  [4/26], [94mLoss[0m : 10.58493
[1mStep[0m  [6/26], [94mLoss[0m : 10.71878
[1mStep[0m  [8/26], [94mLoss[0m : 10.95107
[1mStep[0m  [10/26], [94mLoss[0m : 11.03545
[1mStep[0m  [12/26], [94mLoss[0m : 11.28965
[1mStep[0m  [14/26], [94mLoss[0m : 10.49912
[1mStep[0m  [16/26], [94mLoss[0m : 10.86526
[1mStep[0m  [18/26], [94mLoss[0m : 11.03200
[1mStep[0m  [20/26], [94mLoss[0m : 11.06222
[1mStep[0m  [22/26], [94mLoss[0m : 11.03093
[1mStep[0m  [24/26], [94mLoss[0m : 10.88688

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.876, [92mTest[0m: 10.986, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.86188
[1mStep[0m  [2/26], [94mLoss[0m : 10.99210
[1mStep[0m  [4/26], [94mLoss[0m : 10.71244
[1mStep[0m  [6/26], [94mLoss[0m : 10.94018
[1mStep[0m  [8/26], [94mLoss[0m : 10.80784
[1mStep[0m  [10/26], [94mLoss[0m : 10.96093
[1mStep[0m  [12/26], [94mLoss[0m : 10.94017
[1mStep[0m  [14/26], [94mLoss[0m : 10.73616
[1mStep[0m  [16/26], [94mLoss[0m : 10.72896
[1mStep[0m  [18/26], [94mLoss[0m : 10.87899
[1mStep[0m  [20/26], [94mLoss[0m : 10.94194
[1mStep[0m  [22/26], [94mLoss[0m : 11.07033
[1mStep[0m  [24/26], [94mLoss[0m : 10.84018

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.869, [92mTest[0m: 10.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.80294
[1mStep[0m  [2/26], [94mLoss[0m : 10.84184
[1mStep[0m  [4/26], [94mLoss[0m : 10.83850
[1mStep[0m  [6/26], [94mLoss[0m : 10.82014
[1mStep[0m  [8/26], [94mLoss[0m : 10.97028
[1mStep[0m  [10/26], [94mLoss[0m : 10.87868
[1mStep[0m  [12/26], [94mLoss[0m : 10.85586
[1mStep[0m  [14/26], [94mLoss[0m : 10.91916
[1mStep[0m  [16/26], [94mLoss[0m : 10.81080
[1mStep[0m  [18/26], [94mLoss[0m : 10.69241
[1mStep[0m  [20/26], [94mLoss[0m : 10.76766
[1mStep[0m  [22/26], [94mLoss[0m : 11.07461
[1mStep[0m  [24/26], [94mLoss[0m : 11.01146

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.875, [92mTest[0m: 10.931, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.60972
[1mStep[0m  [2/26], [94mLoss[0m : 11.15404
[1mStep[0m  [4/26], [94mLoss[0m : 11.06054
[1mStep[0m  [6/26], [94mLoss[0m : 10.76179
[1mStep[0m  [8/26], [94mLoss[0m : 10.84914
[1mStep[0m  [10/26], [94mLoss[0m : 10.82559
[1mStep[0m  [12/26], [94mLoss[0m : 10.48900
[1mStep[0m  [14/26], [94mLoss[0m : 11.07109
[1mStep[0m  [16/26], [94mLoss[0m : 10.92286
[1mStep[0m  [18/26], [94mLoss[0m : 10.79770
[1mStep[0m  [20/26], [94mLoss[0m : 10.64160
[1mStep[0m  [22/26], [94mLoss[0m : 11.19810
[1mStep[0m  [24/26], [94mLoss[0m : 10.82934

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.861, [92mTest[0m: 10.928, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.71783
[1mStep[0m  [2/26], [94mLoss[0m : 10.60058
[1mStep[0m  [4/26], [94mLoss[0m : 10.52789
[1mStep[0m  [6/26], [94mLoss[0m : 11.48002
[1mStep[0m  [8/26], [94mLoss[0m : 10.93819
[1mStep[0m  [10/26], [94mLoss[0m : 10.88993
[1mStep[0m  [12/26], [94mLoss[0m : 10.85851
[1mStep[0m  [14/26], [94mLoss[0m : 10.91398
[1mStep[0m  [16/26], [94mLoss[0m : 10.89260
[1mStep[0m  [18/26], [94mLoss[0m : 10.93197
[1mStep[0m  [20/26], [94mLoss[0m : 10.70668
[1mStep[0m  [22/26], [94mLoss[0m : 10.91382
[1mStep[0m  [24/26], [94mLoss[0m : 10.94340

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.864, [92mTest[0m: 10.913, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.73405
[1mStep[0m  [2/26], [94mLoss[0m : 10.83375
[1mStep[0m  [4/26], [94mLoss[0m : 10.92727
[1mStep[0m  [6/26], [94mLoss[0m : 10.81124
[1mStep[0m  [8/26], [94mLoss[0m : 11.21791
[1mStep[0m  [10/26], [94mLoss[0m : 10.72977
[1mStep[0m  [12/26], [94mLoss[0m : 10.72275
[1mStep[0m  [14/26], [94mLoss[0m : 10.85496
[1mStep[0m  [16/26], [94mLoss[0m : 10.79338
[1mStep[0m  [18/26], [94mLoss[0m : 10.90789
[1mStep[0m  [20/26], [94mLoss[0m : 10.89245
[1mStep[0m  [22/26], [94mLoss[0m : 11.06103
[1mStep[0m  [24/26], [94mLoss[0m : 10.90808

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.856, [92mTest[0m: 10.910, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.88428
[1mStep[0m  [2/26], [94mLoss[0m : 10.46956
[1mStep[0m  [4/26], [94mLoss[0m : 10.41031
[1mStep[0m  [6/26], [94mLoss[0m : 10.67292
[1mStep[0m  [8/26], [94mLoss[0m : 10.98100
[1mStep[0m  [10/26], [94mLoss[0m : 11.01850
[1mStep[0m  [12/26], [94mLoss[0m : 10.96424
[1mStep[0m  [14/26], [94mLoss[0m : 11.05517
[1mStep[0m  [16/26], [94mLoss[0m : 10.92904
[1mStep[0m  [18/26], [94mLoss[0m : 10.94075
[1mStep[0m  [20/26], [94mLoss[0m : 11.07443
[1mStep[0m  [22/26], [94mLoss[0m : 10.94564
[1mStep[0m  [24/26], [94mLoss[0m : 10.69760

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.852, [92mTest[0m: 10.909, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.96979
[1mStep[0m  [2/26], [94mLoss[0m : 10.86831
[1mStep[0m  [4/26], [94mLoss[0m : 11.09594
[1mStep[0m  [6/26], [94mLoss[0m : 10.92657
[1mStep[0m  [8/26], [94mLoss[0m : 10.88318
[1mStep[0m  [10/26], [94mLoss[0m : 10.75948
[1mStep[0m  [12/26], [94mLoss[0m : 10.85227
[1mStep[0m  [14/26], [94mLoss[0m : 10.96859
[1mStep[0m  [16/26], [94mLoss[0m : 10.84877
[1mStep[0m  [18/26], [94mLoss[0m : 10.76668
[1mStep[0m  [20/26], [94mLoss[0m : 10.66721
[1mStep[0m  [22/26], [94mLoss[0m : 10.71515
[1mStep[0m  [24/26], [94mLoss[0m : 10.76405

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.844, [92mTest[0m: 10.900, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.81032
[1mStep[0m  [2/26], [94mLoss[0m : 10.68114
[1mStep[0m  [4/26], [94mLoss[0m : 10.82660
[1mStep[0m  [6/26], [94mLoss[0m : 10.74567
[1mStep[0m  [8/26], [94mLoss[0m : 10.86629
[1mStep[0m  [10/26], [94mLoss[0m : 10.88488
[1mStep[0m  [12/26], [94mLoss[0m : 10.76270
[1mStep[0m  [14/26], [94mLoss[0m : 11.00901
[1mStep[0m  [16/26], [94mLoss[0m : 10.83377
[1mStep[0m  [18/26], [94mLoss[0m : 10.81954
[1mStep[0m  [20/26], [94mLoss[0m : 10.93544
[1mStep[0m  [22/26], [94mLoss[0m : 10.91170
[1mStep[0m  [24/26], [94mLoss[0m : 11.07312

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.843, [92mTest[0m: 10.918, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.02426
[1mStep[0m  [2/26], [94mLoss[0m : 10.59180
[1mStep[0m  [4/26], [94mLoss[0m : 10.68804
[1mStep[0m  [6/26], [94mLoss[0m : 10.92072
[1mStep[0m  [8/26], [94mLoss[0m : 10.88353
[1mStep[0m  [10/26], [94mLoss[0m : 10.96497
[1mStep[0m  [12/26], [94mLoss[0m : 10.84868
[1mStep[0m  [14/26], [94mLoss[0m : 10.87815
[1mStep[0m  [16/26], [94mLoss[0m : 10.58105
[1mStep[0m  [18/26], [94mLoss[0m : 10.81803
[1mStep[0m  [20/26], [94mLoss[0m : 10.89395
[1mStep[0m  [22/26], [94mLoss[0m : 10.94617
[1mStep[0m  [24/26], [94mLoss[0m : 10.61736

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.843, [92mTest[0m: 10.895, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.68693
[1mStep[0m  [2/26], [94mLoss[0m : 10.88929
[1mStep[0m  [4/26], [94mLoss[0m : 10.83935
[1mStep[0m  [6/26], [94mLoss[0m : 10.80355
[1mStep[0m  [8/26], [94mLoss[0m : 11.11838
[1mStep[0m  [10/26], [94mLoss[0m : 10.79239
[1mStep[0m  [12/26], [94mLoss[0m : 10.64351
[1mStep[0m  [14/26], [94mLoss[0m : 10.63557
[1mStep[0m  [16/26], [94mLoss[0m : 10.97242
[1mStep[0m  [18/26], [94mLoss[0m : 10.89422
[1mStep[0m  [20/26], [94mLoss[0m : 10.83097
[1mStep[0m  [22/26], [94mLoss[0m : 10.81883
[1mStep[0m  [24/26], [94mLoss[0m : 10.77394

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.883, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.84344
[1mStep[0m  [2/26], [94mLoss[0m : 10.89005
[1mStep[0m  [4/26], [94mLoss[0m : 10.84120
[1mStep[0m  [6/26], [94mLoss[0m : 10.80575
[1mStep[0m  [8/26], [94mLoss[0m : 11.06914
[1mStep[0m  [10/26], [94mLoss[0m : 10.71690
[1mStep[0m  [12/26], [94mLoss[0m : 10.76091
[1mStep[0m  [14/26], [94mLoss[0m : 10.71023
[1mStep[0m  [16/26], [94mLoss[0m : 10.98806
[1mStep[0m  [18/26], [94mLoss[0m : 10.91919
[1mStep[0m  [20/26], [94mLoss[0m : 10.89199
[1mStep[0m  [22/26], [94mLoss[0m : 10.66465
[1mStep[0m  [24/26], [94mLoss[0m : 10.70262

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.823, [92mTest[0m: 10.880, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.74292
[1mStep[0m  [2/26], [94mLoss[0m : 10.98266
[1mStep[0m  [4/26], [94mLoss[0m : 10.85695
[1mStep[0m  [6/26], [94mLoss[0m : 10.69660
[1mStep[0m  [8/26], [94mLoss[0m : 10.89970
[1mStep[0m  [10/26], [94mLoss[0m : 11.23419
[1mStep[0m  [12/26], [94mLoss[0m : 11.00142
[1mStep[0m  [14/26], [94mLoss[0m : 10.70470
[1mStep[0m  [16/26], [94mLoss[0m : 10.51591
[1mStep[0m  [18/26], [94mLoss[0m : 10.64681
[1mStep[0m  [20/26], [94mLoss[0m : 10.99693
[1mStep[0m  [22/26], [94mLoss[0m : 10.77694
[1mStep[0m  [24/26], [94mLoss[0m : 11.00873

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.815, [92mTest[0m: 10.877, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.77276
[1mStep[0m  [2/26], [94mLoss[0m : 11.13644
[1mStep[0m  [4/26], [94mLoss[0m : 11.03283
[1mStep[0m  [6/26], [94mLoss[0m : 10.78968
[1mStep[0m  [8/26], [94mLoss[0m : 11.07205
[1mStep[0m  [10/26], [94mLoss[0m : 10.70335
[1mStep[0m  [12/26], [94mLoss[0m : 10.81270
[1mStep[0m  [14/26], [94mLoss[0m : 10.62226
[1mStep[0m  [16/26], [94mLoss[0m : 10.80548
[1mStep[0m  [18/26], [94mLoss[0m : 10.82121
[1mStep[0m  [20/26], [94mLoss[0m : 10.97092
[1mStep[0m  [22/26], [94mLoss[0m : 10.43182
[1mStep[0m  [24/26], [94mLoss[0m : 10.56632

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.826, [92mTest[0m: 10.863, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.83652
[1mStep[0m  [2/26], [94mLoss[0m : 10.94811
[1mStep[0m  [4/26], [94mLoss[0m : 11.00601
[1mStep[0m  [6/26], [94mLoss[0m : 10.99166
[1mStep[0m  [8/26], [94mLoss[0m : 11.02363
[1mStep[0m  [10/26], [94mLoss[0m : 10.54965
[1mStep[0m  [12/26], [94mLoss[0m : 10.98789
[1mStep[0m  [14/26], [94mLoss[0m : 10.95146
[1mStep[0m  [16/26], [94mLoss[0m : 10.75288
[1mStep[0m  [18/26], [94mLoss[0m : 10.59325
[1mStep[0m  [20/26], [94mLoss[0m : 10.94030
[1mStep[0m  [22/26], [94mLoss[0m : 10.67737
[1mStep[0m  [24/26], [94mLoss[0m : 10.84471

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.810, [92mTest[0m: 10.860, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.08953
[1mStep[0m  [2/26], [94mLoss[0m : 10.88767
[1mStep[0m  [4/26], [94mLoss[0m : 10.85785
[1mStep[0m  [6/26], [94mLoss[0m : 10.67823
[1mStep[0m  [8/26], [94mLoss[0m : 10.87540
[1mStep[0m  [10/26], [94mLoss[0m : 10.85398
[1mStep[0m  [12/26], [94mLoss[0m : 11.12595
[1mStep[0m  [14/26], [94mLoss[0m : 10.59499
[1mStep[0m  [16/26], [94mLoss[0m : 10.95653
[1mStep[0m  [18/26], [94mLoss[0m : 10.74777
[1mStep[0m  [20/26], [94mLoss[0m : 10.58698
[1mStep[0m  [22/26], [94mLoss[0m : 10.88797
[1mStep[0m  [24/26], [94mLoss[0m : 10.79861

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.816, [92mTest[0m: 10.843, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.91566
[1mStep[0m  [2/26], [94mLoss[0m : 10.79943
[1mStep[0m  [4/26], [94mLoss[0m : 10.74578
[1mStep[0m  [6/26], [94mLoss[0m : 10.51890
[1mStep[0m  [8/26], [94mLoss[0m : 11.15600
[1mStep[0m  [10/26], [94mLoss[0m : 10.74203
[1mStep[0m  [12/26], [94mLoss[0m : 10.67308
[1mStep[0m  [14/26], [94mLoss[0m : 10.51410
[1mStep[0m  [16/26], [94mLoss[0m : 10.80473
[1mStep[0m  [18/26], [94mLoss[0m : 10.72911
[1mStep[0m  [20/26], [94mLoss[0m : 10.77238
[1mStep[0m  [22/26], [94mLoss[0m : 10.79631
[1mStep[0m  [24/26], [94mLoss[0m : 10.83014

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.803, [92mTest[0m: 10.856, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.16043
[1mStep[0m  [2/26], [94mLoss[0m : 10.59084
[1mStep[0m  [4/26], [94mLoss[0m : 10.84001
[1mStep[0m  [6/26], [94mLoss[0m : 10.68359
[1mStep[0m  [8/26], [94mLoss[0m : 10.82601
[1mStep[0m  [10/26], [94mLoss[0m : 10.65985
[1mStep[0m  [12/26], [94mLoss[0m : 10.63559
[1mStep[0m  [14/26], [94mLoss[0m : 10.88319
[1mStep[0m  [16/26], [94mLoss[0m : 10.60034
[1mStep[0m  [18/26], [94mLoss[0m : 10.79605
[1mStep[0m  [20/26], [94mLoss[0m : 10.82489
[1mStep[0m  [22/26], [94mLoss[0m : 10.99037
[1mStep[0m  [24/26], [94mLoss[0m : 10.69629

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.854, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.82430
[1mStep[0m  [2/26], [94mLoss[0m : 10.84790
[1mStep[0m  [4/26], [94mLoss[0m : 10.72981
[1mStep[0m  [6/26], [94mLoss[0m : 10.65469
[1mStep[0m  [8/26], [94mLoss[0m : 10.66447
[1mStep[0m  [10/26], [94mLoss[0m : 10.78664
[1mStep[0m  [12/26], [94mLoss[0m : 10.82359
[1mStep[0m  [14/26], [94mLoss[0m : 10.77016
[1mStep[0m  [16/26], [94mLoss[0m : 10.73720
[1mStep[0m  [18/26], [94mLoss[0m : 10.69939
[1mStep[0m  [20/26], [94mLoss[0m : 10.59001
[1mStep[0m  [22/26], [94mLoss[0m : 10.70681
[1mStep[0m  [24/26], [94mLoss[0m : 10.91992

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.791, [92mTest[0m: 10.852, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.67003
[1mStep[0m  [2/26], [94mLoss[0m : 10.64472
[1mStep[0m  [4/26], [94mLoss[0m : 10.72422
[1mStep[0m  [6/26], [94mLoss[0m : 10.84355
[1mStep[0m  [8/26], [94mLoss[0m : 10.54881
[1mStep[0m  [10/26], [94mLoss[0m : 10.74633
[1mStep[0m  [12/26], [94mLoss[0m : 10.88745
[1mStep[0m  [14/26], [94mLoss[0m : 11.11308
[1mStep[0m  [16/26], [94mLoss[0m : 10.83739
[1mStep[0m  [18/26], [94mLoss[0m : 11.05797
[1mStep[0m  [20/26], [94mLoss[0m : 10.87919
[1mStep[0m  [22/26], [94mLoss[0m : 10.92963
[1mStep[0m  [24/26], [94mLoss[0m : 10.81944

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.793, [92mTest[0m: 10.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.58946
[1mStep[0m  [2/26], [94mLoss[0m : 10.95268
[1mStep[0m  [4/26], [94mLoss[0m : 10.83124
[1mStep[0m  [6/26], [94mLoss[0m : 10.61806
[1mStep[0m  [8/26], [94mLoss[0m : 10.70174
[1mStep[0m  [10/26], [94mLoss[0m : 10.74210
[1mStep[0m  [12/26], [94mLoss[0m : 10.56783
[1mStep[0m  [14/26], [94mLoss[0m : 10.69187
[1mStep[0m  [16/26], [94mLoss[0m : 10.89954
[1mStep[0m  [18/26], [94mLoss[0m : 10.82600
[1mStep[0m  [20/26], [94mLoss[0m : 11.03220
[1mStep[0m  [22/26], [94mLoss[0m : 10.77235
[1mStep[0m  [24/26], [94mLoss[0m : 11.00945

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.787, [92mTest[0m: 10.834, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.60288
[1mStep[0m  [2/26], [94mLoss[0m : 10.63041
[1mStep[0m  [4/26], [94mLoss[0m : 10.72276
[1mStep[0m  [6/26], [94mLoss[0m : 10.64831
[1mStep[0m  [8/26], [94mLoss[0m : 10.64400
[1mStep[0m  [10/26], [94mLoss[0m : 10.66164
[1mStep[0m  [12/26], [94mLoss[0m : 11.04024
[1mStep[0m  [14/26], [94mLoss[0m : 10.81674
[1mStep[0m  [16/26], [94mLoss[0m : 10.80801
[1mStep[0m  [18/26], [94mLoss[0m : 10.81809
[1mStep[0m  [20/26], [94mLoss[0m : 10.76245
[1mStep[0m  [22/26], [94mLoss[0m : 10.75771
[1mStep[0m  [24/26], [94mLoss[0m : 10.85729

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.796, [92mTest[0m: 10.816, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.87652
[1mStep[0m  [2/26], [94mLoss[0m : 10.65236
[1mStep[0m  [4/26], [94mLoss[0m : 11.13633
[1mStep[0m  [6/26], [94mLoss[0m : 10.72692
[1mStep[0m  [8/26], [94mLoss[0m : 10.97817
[1mStep[0m  [10/26], [94mLoss[0m : 10.77984
[1mStep[0m  [12/26], [94mLoss[0m : 10.46798
[1mStep[0m  [14/26], [94mLoss[0m : 10.75310
[1mStep[0m  [16/26], [94mLoss[0m : 10.85673
[1mStep[0m  [18/26], [94mLoss[0m : 10.83337
[1mStep[0m  [20/26], [94mLoss[0m : 10.68483
[1mStep[0m  [22/26], [94mLoss[0m : 10.82089
[1mStep[0m  [24/26], [94mLoss[0m : 10.94625

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.789, [92mTest[0m: 10.804, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.97934
[1mStep[0m  [2/26], [94mLoss[0m : 10.51773
[1mStep[0m  [4/26], [94mLoss[0m : 10.79970
[1mStep[0m  [6/26], [94mLoss[0m : 10.94237
[1mStep[0m  [8/26], [94mLoss[0m : 10.50202
[1mStep[0m  [10/26], [94mLoss[0m : 10.98384
[1mStep[0m  [12/26], [94mLoss[0m : 10.89763
[1mStep[0m  [14/26], [94mLoss[0m : 10.87321
[1mStep[0m  [16/26], [94mLoss[0m : 10.66669
[1mStep[0m  [18/26], [94mLoss[0m : 10.64878
[1mStep[0m  [20/26], [94mLoss[0m : 10.72836
[1mStep[0m  [22/26], [94mLoss[0m : 10.88059
[1mStep[0m  [24/26], [94mLoss[0m : 10.55782

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.771, [92mTest[0m: 10.802, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.10677
[1mStep[0m  [2/26], [94mLoss[0m : 10.78710
[1mStep[0m  [4/26], [94mLoss[0m : 10.64869
[1mStep[0m  [6/26], [94mLoss[0m : 10.74632
[1mStep[0m  [8/26], [94mLoss[0m : 10.88841
[1mStep[0m  [10/26], [94mLoss[0m : 10.51333
[1mStep[0m  [12/26], [94mLoss[0m : 10.80309
[1mStep[0m  [14/26], [94mLoss[0m : 10.75266
[1mStep[0m  [16/26], [94mLoss[0m : 10.89909
[1mStep[0m  [18/26], [94mLoss[0m : 10.70442
[1mStep[0m  [20/26], [94mLoss[0m : 10.64764
[1mStep[0m  [22/26], [94mLoss[0m : 10.91422
[1mStep[0m  [24/26], [94mLoss[0m : 10.72577

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.762, [92mTest[0m: 10.799, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.82512
[1mStep[0m  [2/26], [94mLoss[0m : 10.66979
[1mStep[0m  [4/26], [94mLoss[0m : 10.58490
[1mStep[0m  [6/26], [94mLoss[0m : 10.72957
[1mStep[0m  [8/26], [94mLoss[0m : 10.73636
[1mStep[0m  [10/26], [94mLoss[0m : 10.98640
[1mStep[0m  [12/26], [94mLoss[0m : 10.76017
[1mStep[0m  [14/26], [94mLoss[0m : 11.17103
[1mStep[0m  [16/26], [94mLoss[0m : 10.89760
[1mStep[0m  [18/26], [94mLoss[0m : 10.68685
[1mStep[0m  [20/26], [94mLoss[0m : 10.75636
[1mStep[0m  [22/26], [94mLoss[0m : 10.67653
[1mStep[0m  [24/26], [94mLoss[0m : 10.95926

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.764, [92mTest[0m: 10.798, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.85795
[1mStep[0m  [2/26], [94mLoss[0m : 10.51355
[1mStep[0m  [4/26], [94mLoss[0m : 10.89519
[1mStep[0m  [6/26], [94mLoss[0m : 10.82551
[1mStep[0m  [8/26], [94mLoss[0m : 10.75638
[1mStep[0m  [10/26], [94mLoss[0m : 10.75253
[1mStep[0m  [12/26], [94mLoss[0m : 10.81420
[1mStep[0m  [14/26], [94mLoss[0m : 10.57883
[1mStep[0m  [16/26], [94mLoss[0m : 10.65975
[1mStep[0m  [18/26], [94mLoss[0m : 11.13331
[1mStep[0m  [20/26], [94mLoss[0m : 10.54931
[1mStep[0m  [22/26], [94mLoss[0m : 10.80621
[1mStep[0m  [24/26], [94mLoss[0m : 11.03062

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.761, [92mTest[0m: 10.793, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.71901
[1mStep[0m  [2/26], [94mLoss[0m : 10.95523
[1mStep[0m  [4/26], [94mLoss[0m : 10.72184
[1mStep[0m  [6/26], [94mLoss[0m : 10.86508
[1mStep[0m  [8/26], [94mLoss[0m : 10.77115
[1mStep[0m  [10/26], [94mLoss[0m : 10.75500
[1mStep[0m  [12/26], [94mLoss[0m : 10.62104
[1mStep[0m  [14/26], [94mLoss[0m : 10.72483
[1mStep[0m  [16/26], [94mLoss[0m : 10.56715
[1mStep[0m  [18/26], [94mLoss[0m : 10.84693
[1mStep[0m  [20/26], [94mLoss[0m : 10.73982
[1mStep[0m  [22/26], [94mLoss[0m : 10.78381
[1mStep[0m  [24/26], [94mLoss[0m : 11.11462

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.752, [92mTest[0m: 10.790, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.55747
[1mStep[0m  [2/26], [94mLoss[0m : 10.62450
[1mStep[0m  [4/26], [94mLoss[0m : 10.51322
[1mStep[0m  [6/26], [94mLoss[0m : 10.54664
[1mStep[0m  [8/26], [94mLoss[0m : 10.62051
[1mStep[0m  [10/26], [94mLoss[0m : 10.74051
[1mStep[0m  [12/26], [94mLoss[0m : 10.70415
[1mStep[0m  [14/26], [94mLoss[0m : 10.93555
[1mStep[0m  [16/26], [94mLoss[0m : 10.77944
[1mStep[0m  [18/26], [94mLoss[0m : 11.01979
[1mStep[0m  [20/26], [94mLoss[0m : 10.72483
[1mStep[0m  [22/26], [94mLoss[0m : 10.71987
[1mStep[0m  [24/26], [94mLoss[0m : 10.60438

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.750, [92mTest[0m: 10.790, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.57257
[1mStep[0m  [2/26], [94mLoss[0m : 10.66786
[1mStep[0m  [4/26], [94mLoss[0m : 10.89224
[1mStep[0m  [6/26], [94mLoss[0m : 10.86242
[1mStep[0m  [8/26], [94mLoss[0m : 10.53118
[1mStep[0m  [10/26], [94mLoss[0m : 10.74544
[1mStep[0m  [12/26], [94mLoss[0m : 10.82979
[1mStep[0m  [14/26], [94mLoss[0m : 10.86769
[1mStep[0m  [16/26], [94mLoss[0m : 10.80276
[1mStep[0m  [18/26], [94mLoss[0m : 10.86915
[1mStep[0m  [20/26], [94mLoss[0m : 10.66279
[1mStep[0m  [22/26], [94mLoss[0m : 10.75372
[1mStep[0m  [24/26], [94mLoss[0m : 10.85810

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.747, [92mTest[0m: 10.766, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.753
====================================

Phase 1 - Evaluation MAE:  10.752828671382023
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/26], [94mLoss[0m : 10.65742
[1mStep[0m  [2/26], [94mLoss[0m : 10.61943
[1mStep[0m  [4/26], [94mLoss[0m : 10.78270
[1mStep[0m  [6/26], [94mLoss[0m : 10.93110
[1mStep[0m  [8/26], [94mLoss[0m : 10.52002
[1mStep[0m  [10/26], [94mLoss[0m : 10.56186
[1mStep[0m  [12/26], [94mLoss[0m : 10.74544
[1mStep[0m  [14/26], [94mLoss[0m : 10.85424
[1mStep[0m  [16/26], [94mLoss[0m : 10.95121
[1mStep[0m  [18/26], [94mLoss[0m : 10.82941
[1mStep[0m  [20/26], [94mLoss[0m : 10.67926
[1mStep[0m  [22/26], [94mLoss[0m : 10.64323
[1mStep[0m  [24/26], [94mLoss[0m : 10.83122

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.744, [92mTest[0m: 10.766, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.46308
[1mStep[0m  [2/26], [94mLoss[0m : 10.55697
[1mStep[0m  [4/26], [94mLoss[0m : 10.49419
[1mStep[0m  [6/26], [94mLoss[0m : 10.80878
[1mStep[0m  [8/26], [94mLoss[0m : 10.76619
[1mStep[0m  [10/26], [94mLoss[0m : 10.59466
[1mStep[0m  [12/26], [94mLoss[0m : 10.64578
[1mStep[0m  [14/26], [94mLoss[0m : 10.75890
[1mStep[0m  [16/26], [94mLoss[0m : 10.55066
[1mStep[0m  [18/26], [94mLoss[0m : 10.93043
[1mStep[0m  [20/26], [94mLoss[0m : 10.75168
[1mStep[0m  [22/26], [94mLoss[0m : 10.77503
[1mStep[0m  [24/26], [94mLoss[0m : 10.92542

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.738, [92mTest[0m: 10.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.95311
[1mStep[0m  [2/26], [94mLoss[0m : 10.72217
[1mStep[0m  [4/26], [94mLoss[0m : 10.62389
[1mStep[0m  [6/26], [94mLoss[0m : 11.15196
[1mStep[0m  [8/26], [94mLoss[0m : 10.67797
[1mStep[0m  [10/26], [94mLoss[0m : 10.76962
[1mStep[0m  [12/26], [94mLoss[0m : 10.68040
[1mStep[0m  [14/26], [94mLoss[0m : 10.68789
[1mStep[0m  [16/26], [94mLoss[0m : 10.72859
[1mStep[0m  [18/26], [94mLoss[0m : 10.51704
[1mStep[0m  [20/26], [94mLoss[0m : 10.87201
[1mStep[0m  [22/26], [94mLoss[0m : 10.77525
[1mStep[0m  [24/26], [94mLoss[0m : 10.48662

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.734, [92mTest[0m: 10.761, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.64356
[1mStep[0m  [2/26], [94mLoss[0m : 10.77362
[1mStep[0m  [4/26], [94mLoss[0m : 10.50692
[1mStep[0m  [6/26], [94mLoss[0m : 10.85258
[1mStep[0m  [8/26], [94mLoss[0m : 10.81398
[1mStep[0m  [10/26], [94mLoss[0m : 10.59039
[1mStep[0m  [12/26], [94mLoss[0m : 10.80725
[1mStep[0m  [14/26], [94mLoss[0m : 10.68615
[1mStep[0m  [16/26], [94mLoss[0m : 10.71021
[1mStep[0m  [18/26], [94mLoss[0m : 10.67636
[1mStep[0m  [20/26], [94mLoss[0m : 10.75510
[1mStep[0m  [22/26], [94mLoss[0m : 10.79090
[1mStep[0m  [24/26], [94mLoss[0m : 10.81167

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.755, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.62960
[1mStep[0m  [2/26], [94mLoss[0m : 10.70853
[1mStep[0m  [4/26], [94mLoss[0m : 10.85526
[1mStep[0m  [6/26], [94mLoss[0m : 10.42207
[1mStep[0m  [8/26], [94mLoss[0m : 10.85972
[1mStep[0m  [10/26], [94mLoss[0m : 10.77287
[1mStep[0m  [12/26], [94mLoss[0m : 10.72659
[1mStep[0m  [14/26], [94mLoss[0m : 10.85439
[1mStep[0m  [16/26], [94mLoss[0m : 10.89454
[1mStep[0m  [18/26], [94mLoss[0m : 10.73198
[1mStep[0m  [20/26], [94mLoss[0m : 10.78131
[1mStep[0m  [22/26], [94mLoss[0m : 10.57914
[1mStep[0m  [24/26], [94mLoss[0m : 10.49200

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.747, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.69030
[1mStep[0m  [2/26], [94mLoss[0m : 10.70772
[1mStep[0m  [4/26], [94mLoss[0m : 10.66659
[1mStep[0m  [6/26], [94mLoss[0m : 10.62417
[1mStep[0m  [8/26], [94mLoss[0m : 10.95885
[1mStep[0m  [10/26], [94mLoss[0m : 10.68200
[1mStep[0m  [12/26], [94mLoss[0m : 10.59474
[1mStep[0m  [14/26], [94mLoss[0m : 10.77185
[1mStep[0m  [16/26], [94mLoss[0m : 10.77224
[1mStep[0m  [18/26], [94mLoss[0m : 10.35004
[1mStep[0m  [20/26], [94mLoss[0m : 10.83090
[1mStep[0m  [22/26], [94mLoss[0m : 10.66006
[1mStep[0m  [24/26], [94mLoss[0m : 10.73787

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.82375
[1mStep[0m  [2/26], [94mLoss[0m : 10.66180
[1mStep[0m  [4/26], [94mLoss[0m : 10.93576
[1mStep[0m  [6/26], [94mLoss[0m : 10.38797
[1mStep[0m  [8/26], [94mLoss[0m : 10.58971
[1mStep[0m  [10/26], [94mLoss[0m : 10.80541
[1mStep[0m  [12/26], [94mLoss[0m : 10.61796
[1mStep[0m  [14/26], [94mLoss[0m : 11.03743
[1mStep[0m  [16/26], [94mLoss[0m : 10.93456
[1mStep[0m  [18/26], [94mLoss[0m : 10.99004
[1mStep[0m  [20/26], [94mLoss[0m : 10.62217
[1mStep[0m  [22/26], [94mLoss[0m : 10.61871
[1mStep[0m  [24/26], [94mLoss[0m : 10.60990

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.714, [92mTest[0m: 10.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.00819
[1mStep[0m  [2/26], [94mLoss[0m : 10.47126
[1mStep[0m  [4/26], [94mLoss[0m : 10.96811
[1mStep[0m  [6/26], [94mLoss[0m : 10.93214
[1mStep[0m  [8/26], [94mLoss[0m : 10.72407
[1mStep[0m  [10/26], [94mLoss[0m : 10.53933
[1mStep[0m  [12/26], [94mLoss[0m : 10.53629
[1mStep[0m  [14/26], [94mLoss[0m : 10.97746
[1mStep[0m  [16/26], [94mLoss[0m : 10.87743
[1mStep[0m  [18/26], [94mLoss[0m : 10.85446
[1mStep[0m  [20/26], [94mLoss[0m : 10.51616
[1mStep[0m  [22/26], [94mLoss[0m : 10.63349
[1mStep[0m  [24/26], [94mLoss[0m : 10.65051

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.709, [92mTest[0m: 10.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.77781
[1mStep[0m  [2/26], [94mLoss[0m : 10.68336
[1mStep[0m  [4/26], [94mLoss[0m : 10.55562
[1mStep[0m  [6/26], [94mLoss[0m : 10.69539
[1mStep[0m  [8/26], [94mLoss[0m : 10.52501
[1mStep[0m  [10/26], [94mLoss[0m : 10.55465
[1mStep[0m  [12/26], [94mLoss[0m : 10.62027
[1mStep[0m  [14/26], [94mLoss[0m : 10.62315
[1mStep[0m  [16/26], [94mLoss[0m : 10.52993
[1mStep[0m  [18/26], [94mLoss[0m : 10.86645
[1mStep[0m  [20/26], [94mLoss[0m : 10.68799
[1mStep[0m  [22/26], [94mLoss[0m : 10.83640
[1mStep[0m  [24/26], [94mLoss[0m : 10.84348

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.701, [92mTest[0m: 10.700, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.95201
[1mStep[0m  [2/26], [94mLoss[0m : 10.44399
[1mStep[0m  [4/26], [94mLoss[0m : 10.85250
[1mStep[0m  [6/26], [94mLoss[0m : 10.66712
[1mStep[0m  [8/26], [94mLoss[0m : 10.58478
[1mStep[0m  [10/26], [94mLoss[0m : 10.66709
[1mStep[0m  [12/26], [94mLoss[0m : 10.70187
[1mStep[0m  [14/26], [94mLoss[0m : 10.62439
[1mStep[0m  [16/26], [94mLoss[0m : 10.75459
[1mStep[0m  [18/26], [94mLoss[0m : 10.53342
[1mStep[0m  [20/26], [94mLoss[0m : 10.77380
[1mStep[0m  [22/26], [94mLoss[0m : 10.73980
[1mStep[0m  [24/26], [94mLoss[0m : 10.89285

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.688, [92mTest[0m: 10.709, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.39370
[1mStep[0m  [2/26], [94mLoss[0m : 10.56460
[1mStep[0m  [4/26], [94mLoss[0m : 10.93767
[1mStep[0m  [6/26], [94mLoss[0m : 10.41081
[1mStep[0m  [8/26], [94mLoss[0m : 10.76949
[1mStep[0m  [10/26], [94mLoss[0m : 10.86400
[1mStep[0m  [12/26], [94mLoss[0m : 10.59954
[1mStep[0m  [14/26], [94mLoss[0m : 10.78117
[1mStep[0m  [16/26], [94mLoss[0m : 10.67011
[1mStep[0m  [18/26], [94mLoss[0m : 10.46096
[1mStep[0m  [20/26], [94mLoss[0m : 10.82624
[1mStep[0m  [22/26], [94mLoss[0m : 10.59721
[1mStep[0m  [24/26], [94mLoss[0m : 10.60489

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.686, [92mTest[0m: 10.701, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.97831
[1mStep[0m  [2/26], [94mLoss[0m : 10.74510
[1mStep[0m  [4/26], [94mLoss[0m : 10.44291
[1mStep[0m  [6/26], [94mLoss[0m : 10.74336
[1mStep[0m  [8/26], [94mLoss[0m : 10.63067
[1mStep[0m  [10/26], [94mLoss[0m : 10.82927
[1mStep[0m  [12/26], [94mLoss[0m : 10.70283
[1mStep[0m  [14/26], [94mLoss[0m : 10.39954
[1mStep[0m  [16/26], [94mLoss[0m : 10.67142
[1mStep[0m  [18/26], [94mLoss[0m : 10.76367
[1mStep[0m  [20/26], [94mLoss[0m : 10.89288
[1mStep[0m  [22/26], [94mLoss[0m : 10.52918
[1mStep[0m  [24/26], [94mLoss[0m : 10.41392

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.672, [92mTest[0m: 10.684, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.81760
[1mStep[0m  [2/26], [94mLoss[0m : 10.67373
[1mStep[0m  [4/26], [94mLoss[0m : 10.84695
[1mStep[0m  [6/26], [94mLoss[0m : 10.57038
[1mStep[0m  [8/26], [94mLoss[0m : 10.34195
[1mStep[0m  [10/26], [94mLoss[0m : 10.60347
[1mStep[0m  [12/26], [94mLoss[0m : 10.54106
[1mStep[0m  [14/26], [94mLoss[0m : 10.56034
[1mStep[0m  [16/26], [94mLoss[0m : 10.64010
[1mStep[0m  [18/26], [94mLoss[0m : 10.86954
[1mStep[0m  [20/26], [94mLoss[0m : 10.57430
[1mStep[0m  [22/26], [94mLoss[0m : 10.75385
[1mStep[0m  [24/26], [94mLoss[0m : 10.76231

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.674, [92mTest[0m: 10.687, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.84701
[1mStep[0m  [2/26], [94mLoss[0m : 10.71964
[1mStep[0m  [4/26], [94mLoss[0m : 10.68809
[1mStep[0m  [6/26], [94mLoss[0m : 10.64942
[1mStep[0m  [8/26], [94mLoss[0m : 10.68210
[1mStep[0m  [10/26], [94mLoss[0m : 10.70371
[1mStep[0m  [12/26], [94mLoss[0m : 10.73022
[1mStep[0m  [14/26], [94mLoss[0m : 10.74688
[1mStep[0m  [16/26], [94mLoss[0m : 10.77763
[1mStep[0m  [18/26], [94mLoss[0m : 10.70994
[1mStep[0m  [20/26], [94mLoss[0m : 10.63534
[1mStep[0m  [22/26], [94mLoss[0m : 10.53272
[1mStep[0m  [24/26], [94mLoss[0m : 10.62630

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.670, [92mTest[0m: 10.662, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.71043
[1mStep[0m  [2/26], [94mLoss[0m : 10.89278
[1mStep[0m  [4/26], [94mLoss[0m : 10.50502
[1mStep[0m  [6/26], [94mLoss[0m : 10.14129
[1mStep[0m  [8/26], [94mLoss[0m : 10.64738
[1mStep[0m  [10/26], [94mLoss[0m : 10.36656
[1mStep[0m  [12/26], [94mLoss[0m : 10.73172
[1mStep[0m  [14/26], [94mLoss[0m : 10.34485
[1mStep[0m  [16/26], [94mLoss[0m : 10.67098
[1mStep[0m  [18/26], [94mLoss[0m : 10.87260
[1mStep[0m  [20/26], [94mLoss[0m : 10.47465
[1mStep[0m  [22/26], [94mLoss[0m : 10.89794
[1mStep[0m  [24/26], [94mLoss[0m : 10.82781

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.661, [92mTest[0m: 10.653, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.66889
[1mStep[0m  [2/26], [94mLoss[0m : 10.53514
[1mStep[0m  [4/26], [94mLoss[0m : 10.48730
[1mStep[0m  [6/26], [94mLoss[0m : 10.65409
[1mStep[0m  [8/26], [94mLoss[0m : 10.44672
[1mStep[0m  [10/26], [94mLoss[0m : 10.53896
[1mStep[0m  [12/26], [94mLoss[0m : 10.56210
[1mStep[0m  [14/26], [94mLoss[0m : 11.00147
[1mStep[0m  [16/26], [94mLoss[0m : 10.32924
[1mStep[0m  [18/26], [94mLoss[0m : 10.53286
[1mStep[0m  [20/26], [94mLoss[0m : 10.60778
[1mStep[0m  [22/26], [94mLoss[0m : 10.72659
[1mStep[0m  [24/26], [94mLoss[0m : 10.66966

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.637, [92mTest[0m: 10.651, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.15619
[1mStep[0m  [2/26], [94mLoss[0m : 10.42640
[1mStep[0m  [4/26], [94mLoss[0m : 10.75265
[1mStep[0m  [6/26], [94mLoss[0m : 10.53169
[1mStep[0m  [8/26], [94mLoss[0m : 10.56347
[1mStep[0m  [10/26], [94mLoss[0m : 10.87634
[1mStep[0m  [12/26], [94mLoss[0m : 10.67975
[1mStep[0m  [14/26], [94mLoss[0m : 10.73330
[1mStep[0m  [16/26], [94mLoss[0m : 10.33898
[1mStep[0m  [18/26], [94mLoss[0m : 10.64163
[1mStep[0m  [20/26], [94mLoss[0m : 10.71064
[1mStep[0m  [22/26], [94mLoss[0m : 10.69897
[1mStep[0m  [24/26], [94mLoss[0m : 10.58169

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.650, [92mTest[0m: 10.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.44665
[1mStep[0m  [2/26], [94mLoss[0m : 10.77103
[1mStep[0m  [4/26], [94mLoss[0m : 10.38630
[1mStep[0m  [6/26], [94mLoss[0m : 10.75654
[1mStep[0m  [8/26], [94mLoss[0m : 10.58998
[1mStep[0m  [10/26], [94mLoss[0m : 10.62757
[1mStep[0m  [12/26], [94mLoss[0m : 10.56786
[1mStep[0m  [14/26], [94mLoss[0m : 10.78821
[1mStep[0m  [16/26], [94mLoss[0m : 10.64465
[1mStep[0m  [18/26], [94mLoss[0m : 10.40156
[1mStep[0m  [20/26], [94mLoss[0m : 10.68627
[1mStep[0m  [22/26], [94mLoss[0m : 10.52980
[1mStep[0m  [24/26], [94mLoss[0m : 10.67509

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.646, [92mTest[0m: 10.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.72806
[1mStep[0m  [2/26], [94mLoss[0m : 10.60797
[1mStep[0m  [4/26], [94mLoss[0m : 10.46531
[1mStep[0m  [6/26], [94mLoss[0m : 10.64228
[1mStep[0m  [8/26], [94mLoss[0m : 10.68938
[1mStep[0m  [10/26], [94mLoss[0m : 10.54987
[1mStep[0m  [12/26], [94mLoss[0m : 10.65549
[1mStep[0m  [14/26], [94mLoss[0m : 10.50923
[1mStep[0m  [16/26], [94mLoss[0m : 10.52879
[1mStep[0m  [18/26], [94mLoss[0m : 10.76298
[1mStep[0m  [20/26], [94mLoss[0m : 10.43913
[1mStep[0m  [22/26], [94mLoss[0m : 10.77524
[1mStep[0m  [24/26], [94mLoss[0m : 10.71651

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.628, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.69134
[1mStep[0m  [2/26], [94mLoss[0m : 10.70011
[1mStep[0m  [4/26], [94mLoss[0m : 10.50651
[1mStep[0m  [6/26], [94mLoss[0m : 11.09971
[1mStep[0m  [8/26], [94mLoss[0m : 10.49470
[1mStep[0m  [10/26], [94mLoss[0m : 10.82821
[1mStep[0m  [12/26], [94mLoss[0m : 10.68375
[1mStep[0m  [14/26], [94mLoss[0m : 10.70320
[1mStep[0m  [16/26], [94mLoss[0m : 10.63932
[1mStep[0m  [18/26], [94mLoss[0m : 10.41531
[1mStep[0m  [20/26], [94mLoss[0m : 10.61483
[1mStep[0m  [22/26], [94mLoss[0m : 10.67837
[1mStep[0m  [24/26], [94mLoss[0m : 10.70873

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.621, [92mTest[0m: 10.619, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.60561
[1mStep[0m  [2/26], [94mLoss[0m : 10.45912
[1mStep[0m  [4/26], [94mLoss[0m : 10.72642
[1mStep[0m  [6/26], [94mLoss[0m : 10.56984
[1mStep[0m  [8/26], [94mLoss[0m : 10.90975
[1mStep[0m  [10/26], [94mLoss[0m : 10.51503
[1mStep[0m  [12/26], [94mLoss[0m : 10.84131
[1mStep[0m  [14/26], [94mLoss[0m : 10.58555
[1mStep[0m  [16/26], [94mLoss[0m : 10.56403
[1mStep[0m  [18/26], [94mLoss[0m : 10.66625
[1mStep[0m  [20/26], [94mLoss[0m : 10.68446
[1mStep[0m  [22/26], [94mLoss[0m : 10.67180
[1mStep[0m  [24/26], [94mLoss[0m : 10.25174

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.620, [92mTest[0m: 10.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.46369
[1mStep[0m  [2/26], [94mLoss[0m : 10.70098
[1mStep[0m  [4/26], [94mLoss[0m : 10.48184
[1mStep[0m  [6/26], [94mLoss[0m : 10.65970
[1mStep[0m  [8/26], [94mLoss[0m : 10.67120
[1mStep[0m  [10/26], [94mLoss[0m : 10.35391
[1mStep[0m  [12/26], [94mLoss[0m : 10.97091
[1mStep[0m  [14/26], [94mLoss[0m : 10.48057
[1mStep[0m  [16/26], [94mLoss[0m : 10.50316
[1mStep[0m  [18/26], [94mLoss[0m : 10.78736
[1mStep[0m  [20/26], [94mLoss[0m : 10.42007
[1mStep[0m  [22/26], [94mLoss[0m : 10.61112
[1mStep[0m  [24/26], [94mLoss[0m : 10.56918

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.615, [92mTest[0m: 10.607, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.57310
[1mStep[0m  [2/26], [94mLoss[0m : 10.83617
[1mStep[0m  [4/26], [94mLoss[0m : 10.71532
[1mStep[0m  [6/26], [94mLoss[0m : 10.65115
[1mStep[0m  [8/26], [94mLoss[0m : 10.40607
[1mStep[0m  [10/26], [94mLoss[0m : 10.84338
[1mStep[0m  [12/26], [94mLoss[0m : 10.42192
[1mStep[0m  [14/26], [94mLoss[0m : 10.73426
[1mStep[0m  [16/26], [94mLoss[0m : 10.73572
[1mStep[0m  [18/26], [94mLoss[0m : 10.54352
[1mStep[0m  [20/26], [94mLoss[0m : 10.63206
[1mStep[0m  [22/26], [94mLoss[0m : 10.88081
[1mStep[0m  [24/26], [94mLoss[0m : 10.62926

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.615, [92mTest[0m: 10.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.49678
[1mStep[0m  [2/26], [94mLoss[0m : 10.35331
[1mStep[0m  [4/26], [94mLoss[0m : 10.82052
[1mStep[0m  [6/26], [94mLoss[0m : 10.57570
[1mStep[0m  [8/26], [94mLoss[0m : 10.66466
[1mStep[0m  [10/26], [94mLoss[0m : 10.50288
[1mStep[0m  [12/26], [94mLoss[0m : 10.47907
[1mStep[0m  [14/26], [94mLoss[0m : 10.76639
[1mStep[0m  [16/26], [94mLoss[0m : 10.77030
[1mStep[0m  [18/26], [94mLoss[0m : 10.79265
[1mStep[0m  [20/26], [94mLoss[0m : 10.47722
[1mStep[0m  [22/26], [94mLoss[0m : 10.70311
[1mStep[0m  [24/26], [94mLoss[0m : 10.81374

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.610, [92mTest[0m: 10.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.67681
[1mStep[0m  [2/26], [94mLoss[0m : 10.49686
[1mStep[0m  [4/26], [94mLoss[0m : 10.56743
[1mStep[0m  [6/26], [94mLoss[0m : 10.66601
[1mStep[0m  [8/26], [94mLoss[0m : 10.67608
[1mStep[0m  [10/26], [94mLoss[0m : 10.48524
[1mStep[0m  [12/26], [94mLoss[0m : 10.68037
[1mStep[0m  [14/26], [94mLoss[0m : 10.58912
[1mStep[0m  [16/26], [94mLoss[0m : 10.54872
[1mStep[0m  [18/26], [94mLoss[0m : 10.53544
[1mStep[0m  [20/26], [94mLoss[0m : 10.76598
[1mStep[0m  [22/26], [94mLoss[0m : 10.70097
[1mStep[0m  [24/26], [94mLoss[0m : 10.58663

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.601, [92mTest[0m: 10.580, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.46033
[1mStep[0m  [2/26], [94mLoss[0m : 10.49045
[1mStep[0m  [4/26], [94mLoss[0m : 10.64078
[1mStep[0m  [6/26], [94mLoss[0m : 10.56100
[1mStep[0m  [8/26], [94mLoss[0m : 10.61588
[1mStep[0m  [10/26], [94mLoss[0m : 10.59745
[1mStep[0m  [12/26], [94mLoss[0m : 10.69678
[1mStep[0m  [14/26], [94mLoss[0m : 10.43793
[1mStep[0m  [16/26], [94mLoss[0m : 10.37121
[1mStep[0m  [18/26], [94mLoss[0m : 10.52869
[1mStep[0m  [20/26], [94mLoss[0m : 10.23518
[1mStep[0m  [22/26], [94mLoss[0m : 10.54303
[1mStep[0m  [24/26], [94mLoss[0m : 10.87867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.585, [92mTest[0m: 10.559, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.41281
[1mStep[0m  [2/26], [94mLoss[0m : 10.50008
[1mStep[0m  [4/26], [94mLoss[0m : 10.47296
[1mStep[0m  [6/26], [94mLoss[0m : 10.69550
[1mStep[0m  [8/26], [94mLoss[0m : 10.45892
[1mStep[0m  [10/26], [94mLoss[0m : 10.69374
[1mStep[0m  [12/26], [94mLoss[0m : 10.75118
[1mStep[0m  [14/26], [94mLoss[0m : 10.87075
[1mStep[0m  [16/26], [94mLoss[0m : 10.30352
[1mStep[0m  [18/26], [94mLoss[0m : 10.64608
[1mStep[0m  [20/26], [94mLoss[0m : 10.51174
[1mStep[0m  [22/26], [94mLoss[0m : 10.40292
[1mStep[0m  [24/26], [94mLoss[0m : 10.45727

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.584, [92mTest[0m: 10.568, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.30643
[1mStep[0m  [2/26], [94mLoss[0m : 10.48704
[1mStep[0m  [4/26], [94mLoss[0m : 10.47303
[1mStep[0m  [6/26], [94mLoss[0m : 10.45587
[1mStep[0m  [8/26], [94mLoss[0m : 10.63885
[1mStep[0m  [10/26], [94mLoss[0m : 10.77704
[1mStep[0m  [12/26], [94mLoss[0m : 10.90685
[1mStep[0m  [14/26], [94mLoss[0m : 10.63552
[1mStep[0m  [16/26], [94mLoss[0m : 10.52635
[1mStep[0m  [18/26], [94mLoss[0m : 10.52422
[1mStep[0m  [20/26], [94mLoss[0m : 10.63192
[1mStep[0m  [22/26], [94mLoss[0m : 10.60888
[1mStep[0m  [24/26], [94mLoss[0m : 10.55284

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.577, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.56405
[1mStep[0m  [2/26], [94mLoss[0m : 10.98395
[1mStep[0m  [4/26], [94mLoss[0m : 10.57287
[1mStep[0m  [6/26], [94mLoss[0m : 10.76150
[1mStep[0m  [8/26], [94mLoss[0m : 10.70930
[1mStep[0m  [10/26], [94mLoss[0m : 10.66924
[1mStep[0m  [12/26], [94mLoss[0m : 10.48155
[1mStep[0m  [14/26], [94mLoss[0m : 10.62562
[1mStep[0m  [16/26], [94mLoss[0m : 10.51199
[1mStep[0m  [18/26], [94mLoss[0m : 10.55492
[1mStep[0m  [20/26], [94mLoss[0m : 10.44624
[1mStep[0m  [22/26], [94mLoss[0m : 10.52917
[1mStep[0m  [24/26], [94mLoss[0m : 10.61192

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.586, [92mTest[0m: 10.546, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.46835
[1mStep[0m  [2/26], [94mLoss[0m : 10.47246
[1mStep[0m  [4/26], [94mLoss[0m : 10.55229
[1mStep[0m  [6/26], [94mLoss[0m : 10.70891
[1mStep[0m  [8/26], [94mLoss[0m : 10.66335
[1mStep[0m  [10/26], [94mLoss[0m : 10.43621
[1mStep[0m  [12/26], [94mLoss[0m : 10.69725
[1mStep[0m  [14/26], [94mLoss[0m : 10.57153
[1mStep[0m  [16/26], [94mLoss[0m : 10.91253
[1mStep[0m  [18/26], [94mLoss[0m : 10.84669
[1mStep[0m  [20/26], [94mLoss[0m : 10.37022
[1mStep[0m  [22/26], [94mLoss[0m : 10.49852
[1mStep[0m  [24/26], [94mLoss[0m : 10.68973

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.573, [92mTest[0m: 10.536, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.530
====================================

Phase 2 - Evaluation MAE:  10.529908326955942
MAE score P1      10.752829
MAE score P2      10.529908
loss              10.572716
learning_rate        0.0001
batch_size              512
hidden_sizes          [250]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 10.81345
[1mStep[0m  [2/26], [94mLoss[0m : 10.84504
[1mStep[0m  [4/26], [94mLoss[0m : 11.05605
[1mStep[0m  [6/26], [94mLoss[0m : 11.08071
[1mStep[0m  [8/26], [94mLoss[0m : 10.75767
[1mStep[0m  [10/26], [94mLoss[0m : 10.95698
[1mStep[0m  [12/26], [94mLoss[0m : 10.91331
[1mStep[0m  [14/26], [94mLoss[0m : 10.89100
[1mStep[0m  [16/26], [94mLoss[0m : 10.81042
[1mStep[0m  [18/26], [94mLoss[0m : 10.58521
[1mStep[0m  [20/26], [94mLoss[0m : 10.85109
[1mStep[0m  [22/26], [94mLoss[0m : 10.73073
[1mStep[0m  [24/26], [94mLoss[0m : 11.07123

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.822, [92mTest[0m: 10.853, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.87121
[1mStep[0m  [2/26], [94mLoss[0m : 11.06877
[1mStep[0m  [4/26], [94mLoss[0m : 10.93474
[1mStep[0m  [6/26], [94mLoss[0m : 10.92208
[1mStep[0m  [8/26], [94mLoss[0m : 11.10294
[1mStep[0m  [10/26], [94mLoss[0m : 10.89886
[1mStep[0m  [12/26], [94mLoss[0m : 10.56984
[1mStep[0m  [14/26], [94mLoss[0m : 10.90261
[1mStep[0m  [16/26], [94mLoss[0m : 10.75138
[1mStep[0m  [18/26], [94mLoss[0m : 10.98176
[1mStep[0m  [20/26], [94mLoss[0m : 11.05871
[1mStep[0m  [22/26], [94mLoss[0m : 10.49607
[1mStep[0m  [24/26], [94mLoss[0m : 11.01960

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.805, [92mTest[0m: 10.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.75693
[1mStep[0m  [2/26], [94mLoss[0m : 10.59049
[1mStep[0m  [4/26], [94mLoss[0m : 10.53546
[1mStep[0m  [6/26], [94mLoss[0m : 10.79290
[1mStep[0m  [8/26], [94mLoss[0m : 10.71213
[1mStep[0m  [10/26], [94mLoss[0m : 10.73228
[1mStep[0m  [12/26], [94mLoss[0m : 11.08714
[1mStep[0m  [14/26], [94mLoss[0m : 10.84905
[1mStep[0m  [16/26], [94mLoss[0m : 10.85809
[1mStep[0m  [18/26], [94mLoss[0m : 10.99918
[1mStep[0m  [20/26], [94mLoss[0m : 10.40259
[1mStep[0m  [22/26], [94mLoss[0m : 10.32451
[1mStep[0m  [24/26], [94mLoss[0m : 11.19622

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.768, [92mTest[0m: 10.773, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.80341
[1mStep[0m  [2/26], [94mLoss[0m : 10.67572
[1mStep[0m  [4/26], [94mLoss[0m : 10.92751
[1mStep[0m  [6/26], [94mLoss[0m : 10.59761
[1mStep[0m  [8/26], [94mLoss[0m : 10.50676
[1mStep[0m  [10/26], [94mLoss[0m : 10.84987
[1mStep[0m  [12/26], [94mLoss[0m : 11.02591
[1mStep[0m  [14/26], [94mLoss[0m : 10.74034
[1mStep[0m  [16/26], [94mLoss[0m : 10.86781
[1mStep[0m  [18/26], [94mLoss[0m : 10.49953
[1mStep[0m  [20/26], [94mLoss[0m : 10.58075
[1mStep[0m  [22/26], [94mLoss[0m : 10.78042
[1mStep[0m  [24/26], [94mLoss[0m : 10.54436

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.730, [92mTest[0m: 10.730, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.45547
[1mStep[0m  [2/26], [94mLoss[0m : 10.74463
[1mStep[0m  [4/26], [94mLoss[0m : 11.04769
[1mStep[0m  [6/26], [94mLoss[0m : 10.69568
[1mStep[0m  [8/26], [94mLoss[0m : 10.49442
[1mStep[0m  [10/26], [94mLoss[0m : 11.07998
[1mStep[0m  [12/26], [94mLoss[0m : 10.91515
[1mStep[0m  [14/26], [94mLoss[0m : 10.78183
[1mStep[0m  [16/26], [94mLoss[0m : 10.83884
[1mStep[0m  [18/26], [94mLoss[0m : 10.44063
[1mStep[0m  [20/26], [94mLoss[0m : 10.58757
[1mStep[0m  [22/26], [94mLoss[0m : 10.83472
[1mStep[0m  [24/26], [94mLoss[0m : 10.35299

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.697, [92mTest[0m: 10.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.90528
[1mStep[0m  [2/26], [94mLoss[0m : 10.63741
[1mStep[0m  [4/26], [94mLoss[0m : 10.78241
[1mStep[0m  [6/26], [94mLoss[0m : 10.74335
[1mStep[0m  [8/26], [94mLoss[0m : 10.98250
[1mStep[0m  [10/26], [94mLoss[0m : 10.76251
[1mStep[0m  [12/26], [94mLoss[0m : 10.76127
[1mStep[0m  [14/26], [94mLoss[0m : 10.53716
[1mStep[0m  [16/26], [94mLoss[0m : 10.43109
[1mStep[0m  [18/26], [94mLoss[0m : 10.28194
[1mStep[0m  [20/26], [94mLoss[0m : 10.43406
[1mStep[0m  [22/26], [94mLoss[0m : 10.87877
[1mStep[0m  [24/26], [94mLoss[0m : 10.62890

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.666, [92mTest[0m: 10.654, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.58986
[1mStep[0m  [2/26], [94mLoss[0m : 10.53190
[1mStep[0m  [4/26], [94mLoss[0m : 10.75886
[1mStep[0m  [6/26], [94mLoss[0m : 10.31703
[1mStep[0m  [8/26], [94mLoss[0m : 10.77227
[1mStep[0m  [10/26], [94mLoss[0m : 10.49973
[1mStep[0m  [12/26], [94mLoss[0m : 10.32521
[1mStep[0m  [14/26], [94mLoss[0m : 10.66659
[1mStep[0m  [16/26], [94mLoss[0m : 10.75846
[1mStep[0m  [18/26], [94mLoss[0m : 10.88165
[1mStep[0m  [20/26], [94mLoss[0m : 10.44545
[1mStep[0m  [22/26], [94mLoss[0m : 10.64564
[1mStep[0m  [24/26], [94mLoss[0m : 10.72862

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.627, [92mTest[0m: 10.610, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.62271
[1mStep[0m  [2/26], [94mLoss[0m : 10.57355
[1mStep[0m  [4/26], [94mLoss[0m : 10.63509
[1mStep[0m  [6/26], [94mLoss[0m : 10.69385
[1mStep[0m  [8/26], [94mLoss[0m : 10.67826
[1mStep[0m  [10/26], [94mLoss[0m : 10.61996
[1mStep[0m  [12/26], [94mLoss[0m : 10.39854
[1mStep[0m  [14/26], [94mLoss[0m : 10.52587
[1mStep[0m  [16/26], [94mLoss[0m : 10.64256
[1mStep[0m  [18/26], [94mLoss[0m : 10.80478
[1mStep[0m  [20/26], [94mLoss[0m : 10.29876
[1mStep[0m  [22/26], [94mLoss[0m : 10.39238
[1mStep[0m  [24/26], [94mLoss[0m : 10.66920

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.44977
[1mStep[0m  [2/26], [94mLoss[0m : 10.31091
[1mStep[0m  [4/26], [94mLoss[0m : 10.43830
[1mStep[0m  [6/26], [94mLoss[0m : 10.41916
[1mStep[0m  [8/26], [94mLoss[0m : 10.53304
[1mStep[0m  [10/26], [94mLoss[0m : 10.51723
[1mStep[0m  [12/26], [94mLoss[0m : 10.40119
[1mStep[0m  [14/26], [94mLoss[0m : 10.32698
[1mStep[0m  [16/26], [94mLoss[0m : 10.63124
[1mStep[0m  [18/26], [94mLoss[0m : 10.46287
[1mStep[0m  [20/26], [94mLoss[0m : 10.83107
[1mStep[0m  [22/26], [94mLoss[0m : 10.46799
[1mStep[0m  [24/26], [94mLoss[0m : 10.59073

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.566, [92mTest[0m: 10.514, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.66423
[1mStep[0m  [2/26], [94mLoss[0m : 10.17737
[1mStep[0m  [4/26], [94mLoss[0m : 10.71845
[1mStep[0m  [6/26], [94mLoss[0m : 10.52868
[1mStep[0m  [8/26], [94mLoss[0m : 10.36877
[1mStep[0m  [10/26], [94mLoss[0m : 10.41179
[1mStep[0m  [12/26], [94mLoss[0m : 10.68561
[1mStep[0m  [14/26], [94mLoss[0m : 10.55416
[1mStep[0m  [16/26], [94mLoss[0m : 10.43142
[1mStep[0m  [18/26], [94mLoss[0m : 10.74083
[1mStep[0m  [20/26], [94mLoss[0m : 10.95018
[1mStep[0m  [22/26], [94mLoss[0m : 10.48085
[1mStep[0m  [24/26], [94mLoss[0m : 10.51947

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.542, [92mTest[0m: 10.468, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.57977
[1mStep[0m  [2/26], [94mLoss[0m : 10.61831
[1mStep[0m  [4/26], [94mLoss[0m : 10.53920
[1mStep[0m  [6/26], [94mLoss[0m : 10.70873
[1mStep[0m  [8/26], [94mLoss[0m : 10.74075
[1mStep[0m  [10/26], [94mLoss[0m : 10.24433
[1mStep[0m  [12/26], [94mLoss[0m : 10.48855
[1mStep[0m  [14/26], [94mLoss[0m : 10.56977
[1mStep[0m  [16/26], [94mLoss[0m : 10.43378
[1mStep[0m  [18/26], [94mLoss[0m : 10.19004
[1mStep[0m  [20/26], [94mLoss[0m : 10.53117
[1mStep[0m  [22/26], [94mLoss[0m : 10.53800
[1mStep[0m  [24/26], [94mLoss[0m : 10.65880

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.508, [92mTest[0m: 10.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.71756
[1mStep[0m  [2/26], [94mLoss[0m : 10.30981
[1mStep[0m  [4/26], [94mLoss[0m : 10.46778
[1mStep[0m  [6/26], [94mLoss[0m : 10.39221
[1mStep[0m  [8/26], [94mLoss[0m : 10.77389
[1mStep[0m  [10/26], [94mLoss[0m : 10.48963
[1mStep[0m  [12/26], [94mLoss[0m : 10.49907
[1mStep[0m  [14/26], [94mLoss[0m : 10.25426
[1mStep[0m  [16/26], [94mLoss[0m : 10.48691
[1mStep[0m  [18/26], [94mLoss[0m : 10.29588
[1mStep[0m  [20/26], [94mLoss[0m : 10.52950
[1mStep[0m  [22/26], [94mLoss[0m : 10.50839
[1mStep[0m  [24/26], [94mLoss[0m : 10.50291

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.466, [92mTest[0m: 10.386, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.46778
[1mStep[0m  [2/26], [94mLoss[0m : 10.57255
[1mStep[0m  [4/26], [94mLoss[0m : 10.59753
[1mStep[0m  [6/26], [94mLoss[0m : 10.50834
[1mStep[0m  [8/26], [94mLoss[0m : 10.78038
[1mStep[0m  [10/26], [94mLoss[0m : 10.61882
[1mStep[0m  [12/26], [94mLoss[0m : 10.46436
[1mStep[0m  [14/26], [94mLoss[0m : 10.28247
[1mStep[0m  [16/26], [94mLoss[0m : 10.26419
[1mStep[0m  [18/26], [94mLoss[0m : 10.59660
[1mStep[0m  [20/26], [94mLoss[0m : 10.38464
[1mStep[0m  [22/26], [94mLoss[0m : 10.34516
[1mStep[0m  [24/26], [94mLoss[0m : 10.40347

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.446, [92mTest[0m: 10.327, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.62423
[1mStep[0m  [2/26], [94mLoss[0m : 10.48450
[1mStep[0m  [4/26], [94mLoss[0m : 10.37617
[1mStep[0m  [6/26], [94mLoss[0m : 10.39649
[1mStep[0m  [8/26], [94mLoss[0m : 10.53525
[1mStep[0m  [10/26], [94mLoss[0m : 10.35196
[1mStep[0m  [12/26], [94mLoss[0m : 10.33836
[1mStep[0m  [14/26], [94mLoss[0m : 10.56190
[1mStep[0m  [16/26], [94mLoss[0m : 10.24358
[1mStep[0m  [18/26], [94mLoss[0m : 10.31463
[1mStep[0m  [20/26], [94mLoss[0m : 10.49139
[1mStep[0m  [22/26], [94mLoss[0m : 10.55807
[1mStep[0m  [24/26], [94mLoss[0m : 10.50044

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.399, [92mTest[0m: 10.290, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.28550
[1mStep[0m  [2/26], [94mLoss[0m : 10.43744
[1mStep[0m  [4/26], [94mLoss[0m : 10.66262
[1mStep[0m  [6/26], [94mLoss[0m : 10.54973
[1mStep[0m  [8/26], [94mLoss[0m : 10.44343
[1mStep[0m  [10/26], [94mLoss[0m : 10.28820
[1mStep[0m  [12/26], [94mLoss[0m : 10.40884
[1mStep[0m  [14/26], [94mLoss[0m : 10.35181
[1mStep[0m  [16/26], [94mLoss[0m : 10.29653
[1mStep[0m  [18/26], [94mLoss[0m : 10.40207
[1mStep[0m  [20/26], [94mLoss[0m : 10.59309
[1mStep[0m  [22/26], [94mLoss[0m : 10.57482
[1mStep[0m  [24/26], [94mLoss[0m : 10.25729

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.364, [92mTest[0m: 10.254, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.63723
[1mStep[0m  [2/26], [94mLoss[0m : 10.05786
[1mStep[0m  [4/26], [94mLoss[0m : 10.28462
[1mStep[0m  [6/26], [94mLoss[0m : 10.36321
[1mStep[0m  [8/26], [94mLoss[0m : 10.30228
[1mStep[0m  [10/26], [94mLoss[0m : 10.35369
[1mStep[0m  [12/26], [94mLoss[0m : 10.64026
[1mStep[0m  [14/26], [94mLoss[0m : 10.17356
[1mStep[0m  [16/26], [94mLoss[0m : 10.63673
[1mStep[0m  [18/26], [94mLoss[0m : 10.61587
[1mStep[0m  [20/26], [94mLoss[0m : 10.10802
[1mStep[0m  [22/26], [94mLoss[0m : 10.01064
[1mStep[0m  [24/26], [94mLoss[0m : 10.24827

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.343, [92mTest[0m: 10.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.42889
[1mStep[0m  [2/26], [94mLoss[0m : 9.98381
[1mStep[0m  [4/26], [94mLoss[0m : 10.43036
[1mStep[0m  [6/26], [94mLoss[0m : 10.53810
[1mStep[0m  [8/26], [94mLoss[0m : 10.28694
[1mStep[0m  [10/26], [94mLoss[0m : 10.17447
[1mStep[0m  [12/26], [94mLoss[0m : 10.09332
[1mStep[0m  [14/26], [94mLoss[0m : 10.01126
[1mStep[0m  [16/26], [94mLoss[0m : 9.97236
[1mStep[0m  [18/26], [94mLoss[0m : 10.40795
[1mStep[0m  [20/26], [94mLoss[0m : 10.28145
[1mStep[0m  [22/26], [94mLoss[0m : 10.22624
[1mStep[0m  [24/26], [94mLoss[0m : 10.24232

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.301, [92mTest[0m: 10.162, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.43884
[1mStep[0m  [2/26], [94mLoss[0m : 10.40688
[1mStep[0m  [4/26], [94mLoss[0m : 10.43782
[1mStep[0m  [6/26], [94mLoss[0m : 10.35074
[1mStep[0m  [8/26], [94mLoss[0m : 10.33702
[1mStep[0m  [10/26], [94mLoss[0m : 9.88489
[1mStep[0m  [12/26], [94mLoss[0m : 10.24836
[1mStep[0m  [14/26], [94mLoss[0m : 9.99080
[1mStep[0m  [16/26], [94mLoss[0m : 10.48224
[1mStep[0m  [18/26], [94mLoss[0m : 10.36618
[1mStep[0m  [20/26], [94mLoss[0m : 10.14973
[1mStep[0m  [22/26], [94mLoss[0m : 10.20170
[1mStep[0m  [24/26], [94mLoss[0m : 10.12175

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.267, [92mTest[0m: 10.122, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.24754
[1mStep[0m  [2/26], [94mLoss[0m : 10.41797
[1mStep[0m  [4/26], [94mLoss[0m : 10.14018
[1mStep[0m  [6/26], [94mLoss[0m : 10.35256
[1mStep[0m  [8/26], [94mLoss[0m : 10.19255
[1mStep[0m  [10/26], [94mLoss[0m : 10.13434
[1mStep[0m  [12/26], [94mLoss[0m : 10.35938
[1mStep[0m  [14/26], [94mLoss[0m : 10.06524
[1mStep[0m  [16/26], [94mLoss[0m : 10.41803
[1mStep[0m  [18/26], [94mLoss[0m : 10.02771
[1mStep[0m  [20/26], [94mLoss[0m : 10.07542
[1mStep[0m  [22/26], [94mLoss[0m : 10.41976
[1mStep[0m  [24/26], [94mLoss[0m : 10.29107

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.232, [92mTest[0m: 10.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.22971
[1mStep[0m  [2/26], [94mLoss[0m : 10.15455
[1mStep[0m  [4/26], [94mLoss[0m : 10.20844
[1mStep[0m  [6/26], [94mLoss[0m : 10.19204
[1mStep[0m  [8/26], [94mLoss[0m : 10.37271
[1mStep[0m  [10/26], [94mLoss[0m : 10.12785
[1mStep[0m  [12/26], [94mLoss[0m : 10.11502
[1mStep[0m  [14/26], [94mLoss[0m : 9.84097
[1mStep[0m  [16/26], [94mLoss[0m : 10.19068
[1mStep[0m  [18/26], [94mLoss[0m : 10.11104
[1mStep[0m  [20/26], [94mLoss[0m : 10.53152
[1mStep[0m  [22/26], [94mLoss[0m : 9.91860
[1mStep[0m  [24/26], [94mLoss[0m : 10.40037

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.198, [92mTest[0m: 10.021, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.11475
[1mStep[0m  [2/26], [94mLoss[0m : 10.21888
[1mStep[0m  [4/26], [94mLoss[0m : 10.06841
[1mStep[0m  [6/26], [94mLoss[0m : 10.34462
[1mStep[0m  [8/26], [94mLoss[0m : 10.45801
[1mStep[0m  [10/26], [94mLoss[0m : 10.07703
[1mStep[0m  [12/26], [94mLoss[0m : 10.35380
[1mStep[0m  [14/26], [94mLoss[0m : 10.43991
[1mStep[0m  [16/26], [94mLoss[0m : 10.07049
[1mStep[0m  [18/26], [94mLoss[0m : 9.96622
[1mStep[0m  [20/26], [94mLoss[0m : 9.95049
[1mStep[0m  [22/26], [94mLoss[0m : 10.05844
[1mStep[0m  [24/26], [94mLoss[0m : 10.28200

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.170, [92mTest[0m: 9.988, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.93184
[1mStep[0m  [2/26], [94mLoss[0m : 10.05795
[1mStep[0m  [4/26], [94mLoss[0m : 10.40114
[1mStep[0m  [6/26], [94mLoss[0m : 9.97670
[1mStep[0m  [8/26], [94mLoss[0m : 10.02279
[1mStep[0m  [10/26], [94mLoss[0m : 10.25711
[1mStep[0m  [12/26], [94mLoss[0m : 10.22412
[1mStep[0m  [14/26], [94mLoss[0m : 10.30915
[1mStep[0m  [16/26], [94mLoss[0m : 10.15317
[1mStep[0m  [18/26], [94mLoss[0m : 10.05523
[1mStep[0m  [20/26], [94mLoss[0m : 10.16490
[1mStep[0m  [22/26], [94mLoss[0m : 10.18628
[1mStep[0m  [24/26], [94mLoss[0m : 10.10552

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.124, [92mTest[0m: 9.955, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.94572
[1mStep[0m  [2/26], [94mLoss[0m : 10.41537
[1mStep[0m  [4/26], [94mLoss[0m : 10.08334
[1mStep[0m  [6/26], [94mLoss[0m : 10.27267
[1mStep[0m  [8/26], [94mLoss[0m : 10.38692
[1mStep[0m  [10/26], [94mLoss[0m : 9.97651
[1mStep[0m  [12/26], [94mLoss[0m : 10.15144
[1mStep[0m  [14/26], [94mLoss[0m : 10.05113
[1mStep[0m  [16/26], [94mLoss[0m : 10.32669
[1mStep[0m  [18/26], [94mLoss[0m : 10.18103
[1mStep[0m  [20/26], [94mLoss[0m : 10.11432
[1mStep[0m  [22/26], [94mLoss[0m : 9.83949
[1mStep[0m  [24/26], [94mLoss[0m : 9.93900

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.102, [92mTest[0m: 9.878, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.19148
[1mStep[0m  [2/26], [94mLoss[0m : 10.25114
[1mStep[0m  [4/26], [94mLoss[0m : 9.96013
[1mStep[0m  [6/26], [94mLoss[0m : 9.96235
[1mStep[0m  [8/26], [94mLoss[0m : 9.99395
[1mStep[0m  [10/26], [94mLoss[0m : 9.98982
[1mStep[0m  [12/26], [94mLoss[0m : 10.09598
[1mStep[0m  [14/26], [94mLoss[0m : 10.11031
[1mStep[0m  [16/26], [94mLoss[0m : 10.02624
[1mStep[0m  [18/26], [94mLoss[0m : 10.40331
[1mStep[0m  [20/26], [94mLoss[0m : 10.06118
[1mStep[0m  [22/26], [94mLoss[0m : 10.22848
[1mStep[0m  [24/26], [94mLoss[0m : 9.79746

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.080, [92mTest[0m: 9.864, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.12537
[1mStep[0m  [2/26], [94mLoss[0m : 9.98898
[1mStep[0m  [4/26], [94mLoss[0m : 10.15795
[1mStep[0m  [6/26], [94mLoss[0m : 10.37508
[1mStep[0m  [8/26], [94mLoss[0m : 10.18374
[1mStep[0m  [10/26], [94mLoss[0m : 9.80577
[1mStep[0m  [12/26], [94mLoss[0m : 10.05644
[1mStep[0m  [14/26], [94mLoss[0m : 10.09393
[1mStep[0m  [16/26], [94mLoss[0m : 9.98704
[1mStep[0m  [18/26], [94mLoss[0m : 9.94477
[1mStep[0m  [20/26], [94mLoss[0m : 10.09773
[1mStep[0m  [22/26], [94mLoss[0m : 9.80800
[1mStep[0m  [24/26], [94mLoss[0m : 10.02067

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.038, [92mTest[0m: 9.825, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.13680
[1mStep[0m  [2/26], [94mLoss[0m : 10.08528
[1mStep[0m  [4/26], [94mLoss[0m : 10.10929
[1mStep[0m  [6/26], [94mLoss[0m : 9.93587
[1mStep[0m  [8/26], [94mLoss[0m : 10.23967
[1mStep[0m  [10/26], [94mLoss[0m : 10.10063
[1mStep[0m  [12/26], [94mLoss[0m : 10.02674
[1mStep[0m  [14/26], [94mLoss[0m : 10.06883
[1mStep[0m  [16/26], [94mLoss[0m : 9.77014
[1mStep[0m  [18/26], [94mLoss[0m : 9.93606
[1mStep[0m  [20/26], [94mLoss[0m : 10.28091
[1mStep[0m  [22/26], [94mLoss[0m : 10.06618
[1mStep[0m  [24/26], [94mLoss[0m : 9.80866

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.001, [92mTest[0m: 9.766, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.03908
[1mStep[0m  [2/26], [94mLoss[0m : 9.89927
[1mStep[0m  [4/26], [94mLoss[0m : 9.88955
[1mStep[0m  [6/26], [94mLoss[0m : 9.86674
[1mStep[0m  [8/26], [94mLoss[0m : 10.02489
[1mStep[0m  [10/26], [94mLoss[0m : 9.98700
[1mStep[0m  [12/26], [94mLoss[0m : 9.83985
[1mStep[0m  [14/26], [94mLoss[0m : 9.89219
[1mStep[0m  [16/26], [94mLoss[0m : 9.94778
[1mStep[0m  [18/26], [94mLoss[0m : 10.24743
[1mStep[0m  [20/26], [94mLoss[0m : 9.78499
[1mStep[0m  [22/26], [94mLoss[0m : 9.89530
[1mStep[0m  [24/26], [94mLoss[0m : 10.23744

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.973, [92mTest[0m: 9.733, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.77910
[1mStep[0m  [2/26], [94mLoss[0m : 9.86826
[1mStep[0m  [4/26], [94mLoss[0m : 9.86275
[1mStep[0m  [6/26], [94mLoss[0m : 9.60486
[1mStep[0m  [8/26], [94mLoss[0m : 9.85930
[1mStep[0m  [10/26], [94mLoss[0m : 10.27523
[1mStep[0m  [12/26], [94mLoss[0m : 9.80147
[1mStep[0m  [14/26], [94mLoss[0m : 9.94627
[1mStep[0m  [16/26], [94mLoss[0m : 9.94553
[1mStep[0m  [18/26], [94mLoss[0m : 10.06033
[1mStep[0m  [20/26], [94mLoss[0m : 9.98006
[1mStep[0m  [22/26], [94mLoss[0m : 10.27993
[1mStep[0m  [24/26], [94mLoss[0m : 9.90290

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.951, [92mTest[0m: 9.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.12636
[1mStep[0m  [2/26], [94mLoss[0m : 9.95828
[1mStep[0m  [4/26], [94mLoss[0m : 9.62831
[1mStep[0m  [6/26], [94mLoss[0m : 9.46565
[1mStep[0m  [8/26], [94mLoss[0m : 9.92983
[1mStep[0m  [10/26], [94mLoss[0m : 9.78456
[1mStep[0m  [12/26], [94mLoss[0m : 9.86885
[1mStep[0m  [14/26], [94mLoss[0m : 9.55791
[1mStep[0m  [16/26], [94mLoss[0m : 9.95560
[1mStep[0m  [18/26], [94mLoss[0m : 10.10568
[1mStep[0m  [20/26], [94mLoss[0m : 10.06246
[1mStep[0m  [22/26], [94mLoss[0m : 9.95827
[1mStep[0m  [24/26], [94mLoss[0m : 9.89857

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.901, [92mTest[0m: 9.668, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.93479
[1mStep[0m  [2/26], [94mLoss[0m : 10.19364
[1mStep[0m  [4/26], [94mLoss[0m : 9.94786
[1mStep[0m  [6/26], [94mLoss[0m : 9.84750
[1mStep[0m  [8/26], [94mLoss[0m : 9.76978
[1mStep[0m  [10/26], [94mLoss[0m : 10.02021
[1mStep[0m  [12/26], [94mLoss[0m : 9.85602
[1mStep[0m  [14/26], [94mLoss[0m : 9.79848
[1mStep[0m  [16/26], [94mLoss[0m : 9.79807
[1mStep[0m  [18/26], [94mLoss[0m : 9.90565
[1mStep[0m  [20/26], [94mLoss[0m : 9.72749
[1mStep[0m  [22/26], [94mLoss[0m : 9.73679
[1mStep[0m  [24/26], [94mLoss[0m : 10.12140

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.886, [92mTest[0m: 9.606, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.564
====================================

Phase 1 - Evaluation MAE:  9.564124327439528
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/26], [94mLoss[0m : 9.82375
[1mStep[0m  [2/26], [94mLoss[0m : 9.74971
[1mStep[0m  [4/26], [94mLoss[0m : 9.64087
[1mStep[0m  [6/26], [94mLoss[0m : 9.69713
[1mStep[0m  [8/26], [94mLoss[0m : 9.96450
[1mStep[0m  [10/26], [94mLoss[0m : 9.79522
[1mStep[0m  [12/26], [94mLoss[0m : 9.57792
[1mStep[0m  [14/26], [94mLoss[0m : 9.94516
[1mStep[0m  [16/26], [94mLoss[0m : 10.04286
[1mStep[0m  [18/26], [94mLoss[0m : 9.89274
[1mStep[0m  [20/26], [94mLoss[0m : 10.01873
[1mStep[0m  [22/26], [94mLoss[0m : 9.95677
[1mStep[0m  [24/26], [94mLoss[0m : 9.53141

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.847, [92mTest[0m: 9.567, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.56927
[1mStep[0m  [2/26], [94mLoss[0m : 9.60239
[1mStep[0m  [4/26], [94mLoss[0m : 9.91467
[1mStep[0m  [6/26], [94mLoss[0m : 10.06689
[1mStep[0m  [8/26], [94mLoss[0m : 10.08335
[1mStep[0m  [10/26], [94mLoss[0m : 10.03855
[1mStep[0m  [12/26], [94mLoss[0m : 9.73306
[1mStep[0m  [14/26], [94mLoss[0m : 9.86794
[1mStep[0m  [16/26], [94mLoss[0m : 9.67670
[1mStep[0m  [18/26], [94mLoss[0m : 9.90525
[1mStep[0m  [20/26], [94mLoss[0m : 9.53329
[1mStep[0m  [22/26], [94mLoss[0m : 9.86408
[1mStep[0m  [24/26], [94mLoss[0m : 9.61434

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.814, [92mTest[0m: 9.503, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.78239
[1mStep[0m  [2/26], [94mLoss[0m : 9.82195
[1mStep[0m  [4/26], [94mLoss[0m : 9.76081
[1mStep[0m  [6/26], [94mLoss[0m : 9.93292
[1mStep[0m  [8/26], [94mLoss[0m : 9.71988
[1mStep[0m  [10/26], [94mLoss[0m : 9.74971
[1mStep[0m  [12/26], [94mLoss[0m : 9.72323
[1mStep[0m  [14/26], [94mLoss[0m : 9.69789
[1mStep[0m  [16/26], [94mLoss[0m : 9.77755
[1mStep[0m  [18/26], [94mLoss[0m : 9.69187
[1mStep[0m  [20/26], [94mLoss[0m : 9.46240
[1mStep[0m  [22/26], [94mLoss[0m : 10.06219
[1mStep[0m  [24/26], [94mLoss[0m : 9.99756

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.767, [92mTest[0m: 9.443, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.54529
[1mStep[0m  [2/26], [94mLoss[0m : 9.87211
[1mStep[0m  [4/26], [94mLoss[0m : 9.74405
[1mStep[0m  [6/26], [94mLoss[0m : 9.48279
[1mStep[0m  [8/26], [94mLoss[0m : 9.37304
[1mStep[0m  [10/26], [94mLoss[0m : 9.90345
[1mStep[0m  [12/26], [94mLoss[0m : 9.77838
[1mStep[0m  [14/26], [94mLoss[0m : 9.84129
[1mStep[0m  [16/26], [94mLoss[0m : 9.78235
[1mStep[0m  [18/26], [94mLoss[0m : 9.80079
[1mStep[0m  [20/26], [94mLoss[0m : 9.87179
[1mStep[0m  [22/26], [94mLoss[0m : 9.70306
[1mStep[0m  [24/26], [94mLoss[0m : 9.81880

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.708, [92mTest[0m: 9.406, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.71404
[1mStep[0m  [2/26], [94mLoss[0m : 9.72797
[1mStep[0m  [4/26], [94mLoss[0m : 9.63648
[1mStep[0m  [6/26], [94mLoss[0m : 9.57471
[1mStep[0m  [8/26], [94mLoss[0m : 9.79528
[1mStep[0m  [10/26], [94mLoss[0m : 9.59766
[1mStep[0m  [12/26], [94mLoss[0m : 9.82290
[1mStep[0m  [14/26], [94mLoss[0m : 9.62040
[1mStep[0m  [16/26], [94mLoss[0m : 9.65206
[1mStep[0m  [18/26], [94mLoss[0m : 9.60258
[1mStep[0m  [20/26], [94mLoss[0m : 9.58255
[1mStep[0m  [22/26], [94mLoss[0m : 9.56689
[1mStep[0m  [24/26], [94mLoss[0m : 9.56190

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.666, [92mTest[0m: 9.354, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.62324
[1mStep[0m  [2/26], [94mLoss[0m : 9.61586
[1mStep[0m  [4/26], [94mLoss[0m : 9.87420
[1mStep[0m  [6/26], [94mLoss[0m : 9.78387
[1mStep[0m  [8/26], [94mLoss[0m : 9.43845
[1mStep[0m  [10/26], [94mLoss[0m : 9.50780
[1mStep[0m  [12/26], [94mLoss[0m : 9.63110
[1mStep[0m  [14/26], [94mLoss[0m : 9.63496
[1mStep[0m  [16/26], [94mLoss[0m : 9.71293
[1mStep[0m  [18/26], [94mLoss[0m : 9.55123
[1mStep[0m  [20/26], [94mLoss[0m : 9.75486
[1mStep[0m  [22/26], [94mLoss[0m : 9.50020
[1mStep[0m  [24/26], [94mLoss[0m : 9.39266

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.612, [92mTest[0m: 9.317, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.58220
[1mStep[0m  [2/26], [94mLoss[0m : 9.63754
[1mStep[0m  [4/26], [94mLoss[0m : 9.73024
[1mStep[0m  [6/26], [94mLoss[0m : 9.58557
[1mStep[0m  [8/26], [94mLoss[0m : 9.86030
[1mStep[0m  [10/26], [94mLoss[0m : 9.49863
[1mStep[0m  [12/26], [94mLoss[0m : 9.62085
[1mStep[0m  [14/26], [94mLoss[0m : 9.66197
[1mStep[0m  [16/26], [94mLoss[0m : 9.47295
[1mStep[0m  [18/26], [94mLoss[0m : 9.77707
[1mStep[0m  [20/26], [94mLoss[0m : 9.47612
[1mStep[0m  [22/26], [94mLoss[0m : 9.44056
[1mStep[0m  [24/26], [94mLoss[0m : 9.37534

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.565, [92mTest[0m: 9.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.59882
[1mStep[0m  [2/26], [94mLoss[0m : 9.43006
[1mStep[0m  [4/26], [94mLoss[0m : 9.83492
[1mStep[0m  [6/26], [94mLoss[0m : 9.32011
[1mStep[0m  [8/26], [94mLoss[0m : 9.33375
[1mStep[0m  [10/26], [94mLoss[0m : 9.19112
[1mStep[0m  [12/26], [94mLoss[0m : 9.36050
[1mStep[0m  [14/26], [94mLoss[0m : 9.61257
[1mStep[0m  [16/26], [94mLoss[0m : 9.47829
[1mStep[0m  [18/26], [94mLoss[0m : 9.37941
[1mStep[0m  [20/26], [94mLoss[0m : 9.79248
[1mStep[0m  [22/26], [94mLoss[0m : 9.47637
[1mStep[0m  [24/26], [94mLoss[0m : 9.75280

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.513, [92mTest[0m: 9.185, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.48269
[1mStep[0m  [2/26], [94mLoss[0m : 9.70995
[1mStep[0m  [4/26], [94mLoss[0m : 9.43545
[1mStep[0m  [6/26], [94mLoss[0m : 9.27348
[1mStep[0m  [8/26], [94mLoss[0m : 9.45843
[1mStep[0m  [10/26], [94mLoss[0m : 9.61295
[1mStep[0m  [12/26], [94mLoss[0m : 9.65571
[1mStep[0m  [14/26], [94mLoss[0m : 9.23127
[1mStep[0m  [16/26], [94mLoss[0m : 9.47341
[1mStep[0m  [18/26], [94mLoss[0m : 9.52387
[1mStep[0m  [20/26], [94mLoss[0m : 9.27051
[1mStep[0m  [22/26], [94mLoss[0m : 9.39678
[1mStep[0m  [24/26], [94mLoss[0m : 9.42659

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.468, [92mTest[0m: 9.170, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.41123
[1mStep[0m  [2/26], [94mLoss[0m : 9.38008
[1mStep[0m  [4/26], [94mLoss[0m : 9.65204
[1mStep[0m  [6/26], [94mLoss[0m : 9.36404
[1mStep[0m  [8/26], [94mLoss[0m : 9.39536
[1mStep[0m  [10/26], [94mLoss[0m : 9.51025
[1mStep[0m  [12/26], [94mLoss[0m : 9.37667
[1mStep[0m  [14/26], [94mLoss[0m : 9.36069
[1mStep[0m  [16/26], [94mLoss[0m : 9.31782
[1mStep[0m  [18/26], [94mLoss[0m : 9.34360
[1mStep[0m  [20/26], [94mLoss[0m : 9.61100
[1mStep[0m  [22/26], [94mLoss[0m : 9.04572
[1mStep[0m  [24/26], [94mLoss[0m : 9.29608

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.406, [92mTest[0m: 9.147, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.39269
[1mStep[0m  [2/26], [94mLoss[0m : 9.44253
[1mStep[0m  [4/26], [94mLoss[0m : 9.21063
[1mStep[0m  [6/26], [94mLoss[0m : 9.21057
[1mStep[0m  [8/26], [94mLoss[0m : 9.26932
[1mStep[0m  [10/26], [94mLoss[0m : 8.97752
[1mStep[0m  [12/26], [94mLoss[0m : 9.36452
[1mStep[0m  [14/26], [94mLoss[0m : 9.71532
[1mStep[0m  [16/26], [94mLoss[0m : 9.25401
[1mStep[0m  [18/26], [94mLoss[0m : 9.36653
[1mStep[0m  [20/26], [94mLoss[0m : 9.26683
[1mStep[0m  [22/26], [94mLoss[0m : 9.27571
[1mStep[0m  [24/26], [94mLoss[0m : 9.12160

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.352, [92mTest[0m: 9.056, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.17068
[1mStep[0m  [2/26], [94mLoss[0m : 9.40679
[1mStep[0m  [4/26], [94mLoss[0m : 9.43990
[1mStep[0m  [6/26], [94mLoss[0m : 9.04543
[1mStep[0m  [8/26], [94mLoss[0m : 9.24199
[1mStep[0m  [10/26], [94mLoss[0m : 9.13253
[1mStep[0m  [12/26], [94mLoss[0m : 9.59763
[1mStep[0m  [14/26], [94mLoss[0m : 9.57453
[1mStep[0m  [16/26], [94mLoss[0m : 9.35668
[1mStep[0m  [18/26], [94mLoss[0m : 9.21617
[1mStep[0m  [20/26], [94mLoss[0m : 9.26450
[1mStep[0m  [22/26], [94mLoss[0m : 9.34416
[1mStep[0m  [24/26], [94mLoss[0m : 9.45040

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.297, [92mTest[0m: 8.983, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.30098
[1mStep[0m  [2/26], [94mLoss[0m : 9.21291
[1mStep[0m  [4/26], [94mLoss[0m : 9.07406
[1mStep[0m  [6/26], [94mLoss[0m : 8.99675
[1mStep[0m  [8/26], [94mLoss[0m : 9.19060
[1mStep[0m  [10/26], [94mLoss[0m : 9.36779
[1mStep[0m  [12/26], [94mLoss[0m : 9.16685
[1mStep[0m  [14/26], [94mLoss[0m : 9.31525
[1mStep[0m  [16/26], [94mLoss[0m : 9.32989
[1mStep[0m  [18/26], [94mLoss[0m : 9.31944
[1mStep[0m  [20/26], [94mLoss[0m : 9.25107
[1mStep[0m  [22/26], [94mLoss[0m : 9.21360
[1mStep[0m  [24/26], [94mLoss[0m : 9.24775

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.243, [92mTest[0m: 8.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.14104
[1mStep[0m  [2/26], [94mLoss[0m : 8.96642
[1mStep[0m  [4/26], [94mLoss[0m : 9.63718
[1mStep[0m  [6/26], [94mLoss[0m : 8.93488
[1mStep[0m  [8/26], [94mLoss[0m : 9.17858
[1mStep[0m  [10/26], [94mLoss[0m : 9.03880
[1mStep[0m  [12/26], [94mLoss[0m : 9.20368
[1mStep[0m  [14/26], [94mLoss[0m : 9.10002
[1mStep[0m  [16/26], [94mLoss[0m : 9.21900
[1mStep[0m  [18/26], [94mLoss[0m : 9.22847
[1mStep[0m  [20/26], [94mLoss[0m : 9.38889
[1mStep[0m  [22/26], [94mLoss[0m : 9.29806
[1mStep[0m  [24/26], [94mLoss[0m : 9.23363

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.189, [92mTest[0m: 8.894, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.20981
[1mStep[0m  [2/26], [94mLoss[0m : 9.01587
[1mStep[0m  [4/26], [94mLoss[0m : 9.06368
[1mStep[0m  [6/26], [94mLoss[0m : 9.36049
[1mStep[0m  [8/26], [94mLoss[0m : 9.02701
[1mStep[0m  [10/26], [94mLoss[0m : 8.84689
[1mStep[0m  [12/26], [94mLoss[0m : 9.00770
[1mStep[0m  [14/26], [94mLoss[0m : 9.14495
[1mStep[0m  [16/26], [94mLoss[0m : 9.10608
[1mStep[0m  [18/26], [94mLoss[0m : 9.21173
[1mStep[0m  [20/26], [94mLoss[0m : 8.98372
[1mStep[0m  [22/26], [94mLoss[0m : 9.34741
[1mStep[0m  [24/26], [94mLoss[0m : 9.18668

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.132, [92mTest[0m: 8.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.24360
[1mStep[0m  [2/26], [94mLoss[0m : 8.99959
[1mStep[0m  [4/26], [94mLoss[0m : 9.12154
[1mStep[0m  [6/26], [94mLoss[0m : 9.04298
[1mStep[0m  [8/26], [94mLoss[0m : 8.96882
[1mStep[0m  [10/26], [94mLoss[0m : 9.32121
[1mStep[0m  [12/26], [94mLoss[0m : 9.22270
[1mStep[0m  [14/26], [94mLoss[0m : 9.15207
[1mStep[0m  [16/26], [94mLoss[0m : 9.08469
[1mStep[0m  [18/26], [94mLoss[0m : 8.97766
[1mStep[0m  [20/26], [94mLoss[0m : 8.99917
[1mStep[0m  [22/26], [94mLoss[0m : 9.03548
[1mStep[0m  [24/26], [94mLoss[0m : 8.98900

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.058, [92mTest[0m: 8.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.00215
[1mStep[0m  [2/26], [94mLoss[0m : 8.84212
[1mStep[0m  [4/26], [94mLoss[0m : 8.95110
[1mStep[0m  [6/26], [94mLoss[0m : 8.92164
[1mStep[0m  [8/26], [94mLoss[0m : 9.05643
[1mStep[0m  [10/26], [94mLoss[0m : 8.95173
[1mStep[0m  [12/26], [94mLoss[0m : 8.98466
[1mStep[0m  [14/26], [94mLoss[0m : 9.08488
[1mStep[0m  [16/26], [94mLoss[0m : 8.90145
[1mStep[0m  [18/26], [94mLoss[0m : 8.91932
[1mStep[0m  [20/26], [94mLoss[0m : 9.11943
[1mStep[0m  [22/26], [94mLoss[0m : 8.91028
[1mStep[0m  [24/26], [94mLoss[0m : 9.09758

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.992, [92mTest[0m: 8.654, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.18249
[1mStep[0m  [2/26], [94mLoss[0m : 9.29921
[1mStep[0m  [4/26], [94mLoss[0m : 8.86186
[1mStep[0m  [6/26], [94mLoss[0m : 8.98590
[1mStep[0m  [8/26], [94mLoss[0m : 8.62700
[1mStep[0m  [10/26], [94mLoss[0m : 8.82110
[1mStep[0m  [12/26], [94mLoss[0m : 8.99670
[1mStep[0m  [14/26], [94mLoss[0m : 8.90204
[1mStep[0m  [16/26], [94mLoss[0m : 9.08250
[1mStep[0m  [18/26], [94mLoss[0m : 8.86185
[1mStep[0m  [20/26], [94mLoss[0m : 9.00871
[1mStep[0m  [22/26], [94mLoss[0m : 8.84946
[1mStep[0m  [24/26], [94mLoss[0m : 8.83818

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.952, [92mTest[0m: 8.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.91452
[1mStep[0m  [2/26], [94mLoss[0m : 8.90108
[1mStep[0m  [4/26], [94mLoss[0m : 8.91494
[1mStep[0m  [6/26], [94mLoss[0m : 8.84990
[1mStep[0m  [8/26], [94mLoss[0m : 8.85836
[1mStep[0m  [10/26], [94mLoss[0m : 8.73667
[1mStep[0m  [12/26], [94mLoss[0m : 8.88125
[1mStep[0m  [14/26], [94mLoss[0m : 8.91444
[1mStep[0m  [16/26], [94mLoss[0m : 8.56027
[1mStep[0m  [18/26], [94mLoss[0m : 8.78557
[1mStep[0m  [20/26], [94mLoss[0m : 9.10485
[1mStep[0m  [22/26], [94mLoss[0m : 8.75115
[1mStep[0m  [24/26], [94mLoss[0m : 8.85472

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.893, [92mTest[0m: 8.475, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.76095
[1mStep[0m  [2/26], [94mLoss[0m : 8.89619
[1mStep[0m  [4/26], [94mLoss[0m : 8.78446
[1mStep[0m  [6/26], [94mLoss[0m : 8.89775
[1mStep[0m  [8/26], [94mLoss[0m : 8.78980
[1mStep[0m  [10/26], [94mLoss[0m : 8.71511
[1mStep[0m  [12/26], [94mLoss[0m : 8.90254
[1mStep[0m  [14/26], [94mLoss[0m : 9.00255
[1mStep[0m  [16/26], [94mLoss[0m : 8.92388
[1mStep[0m  [18/26], [94mLoss[0m : 8.74856
[1mStep[0m  [20/26], [94mLoss[0m : 8.86184
[1mStep[0m  [22/26], [94mLoss[0m : 8.53500
[1mStep[0m  [24/26], [94mLoss[0m : 8.69611

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.818, [92mTest[0m: 8.486, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.02198
[1mStep[0m  [2/26], [94mLoss[0m : 8.65533
[1mStep[0m  [4/26], [94mLoss[0m : 8.91105
[1mStep[0m  [6/26], [94mLoss[0m : 8.61297
[1mStep[0m  [8/26], [94mLoss[0m : 8.53368
[1mStep[0m  [10/26], [94mLoss[0m : 8.65820
[1mStep[0m  [12/26], [94mLoss[0m : 8.86945
[1mStep[0m  [14/26], [94mLoss[0m : 8.99273
[1mStep[0m  [16/26], [94mLoss[0m : 8.54047
[1mStep[0m  [18/26], [94mLoss[0m : 8.64494
[1mStep[0m  [20/26], [94mLoss[0m : 8.73940
[1mStep[0m  [22/26], [94mLoss[0m : 8.48004
[1mStep[0m  [24/26], [94mLoss[0m : 8.55298

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.761, [92mTest[0m: 8.342, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.69694
[1mStep[0m  [2/26], [94mLoss[0m : 8.74548
[1mStep[0m  [4/26], [94mLoss[0m : 8.66385
[1mStep[0m  [6/26], [94mLoss[0m : 8.65926
[1mStep[0m  [8/26], [94mLoss[0m : 8.63793
[1mStep[0m  [10/26], [94mLoss[0m : 8.91031
[1mStep[0m  [12/26], [94mLoss[0m : 8.70574
[1mStep[0m  [14/26], [94mLoss[0m : 8.50016
[1mStep[0m  [16/26], [94mLoss[0m : 8.54173
[1mStep[0m  [18/26], [94mLoss[0m : 8.68150
[1mStep[0m  [20/26], [94mLoss[0m : 8.68550
[1mStep[0m  [22/26], [94mLoss[0m : 8.46773
[1mStep[0m  [24/26], [94mLoss[0m : 8.48259

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.690, [92mTest[0m: 8.228, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.61442
[1mStep[0m  [2/26], [94mLoss[0m : 8.61742
[1mStep[0m  [4/26], [94mLoss[0m : 8.57300
[1mStep[0m  [6/26], [94mLoss[0m : 8.99132
[1mStep[0m  [8/26], [94mLoss[0m : 8.60463
[1mStep[0m  [10/26], [94mLoss[0m : 8.56043
[1mStep[0m  [12/26], [94mLoss[0m : 8.89202
[1mStep[0m  [14/26], [94mLoss[0m : 8.69987
[1mStep[0m  [16/26], [94mLoss[0m : 8.09129
[1mStep[0m  [18/26], [94mLoss[0m : 8.47398
[1mStep[0m  [20/26], [94mLoss[0m : 8.76286
[1mStep[0m  [22/26], [94mLoss[0m : 8.93087
[1mStep[0m  [24/26], [94mLoss[0m : 8.35456

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.617, [92mTest[0m: 8.191, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.33494
[1mStep[0m  [2/26], [94mLoss[0m : 8.38810
[1mStep[0m  [4/26], [94mLoss[0m : 8.89589
[1mStep[0m  [6/26], [94mLoss[0m : 8.59882
[1mStep[0m  [8/26], [94mLoss[0m : 8.65728
[1mStep[0m  [10/26], [94mLoss[0m : 8.67787
[1mStep[0m  [12/26], [94mLoss[0m : 8.59480
[1mStep[0m  [14/26], [94mLoss[0m : 8.86141
[1mStep[0m  [16/26], [94mLoss[0m : 8.61755
[1mStep[0m  [18/26], [94mLoss[0m : 8.52853
[1mStep[0m  [20/26], [94mLoss[0m : 8.61203
[1mStep[0m  [22/26], [94mLoss[0m : 8.66900
[1mStep[0m  [24/26], [94mLoss[0m : 8.43483

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.564, [92mTest[0m: 8.157, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.45512
[1mStep[0m  [2/26], [94mLoss[0m : 8.37375
[1mStep[0m  [4/26], [94mLoss[0m : 8.71633
[1mStep[0m  [6/26], [94mLoss[0m : 8.41067
[1mStep[0m  [8/26], [94mLoss[0m : 8.63016
[1mStep[0m  [10/26], [94mLoss[0m : 8.45804
[1mStep[0m  [12/26], [94mLoss[0m : 8.51122
[1mStep[0m  [14/26], [94mLoss[0m : 8.44645
[1mStep[0m  [16/26], [94mLoss[0m : 8.71559
[1mStep[0m  [18/26], [94mLoss[0m : 8.70355
[1mStep[0m  [20/26], [94mLoss[0m : 8.70132
[1mStep[0m  [22/26], [94mLoss[0m : 8.17070
[1mStep[0m  [24/26], [94mLoss[0m : 8.35711

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.490, [92mTest[0m: 8.122, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.42730
[1mStep[0m  [2/26], [94mLoss[0m : 8.30519
[1mStep[0m  [4/26], [94mLoss[0m : 8.40793
[1mStep[0m  [6/26], [94mLoss[0m : 8.46177
[1mStep[0m  [8/26], [94mLoss[0m : 8.46583
[1mStep[0m  [10/26], [94mLoss[0m : 8.57111
[1mStep[0m  [12/26], [94mLoss[0m : 8.57415
[1mStep[0m  [14/26], [94mLoss[0m : 8.39362
[1mStep[0m  [16/26], [94mLoss[0m : 8.39360
[1mStep[0m  [18/26], [94mLoss[0m : 8.56678
[1mStep[0m  [20/26], [94mLoss[0m : 8.24691
[1mStep[0m  [22/26], [94mLoss[0m : 8.46939
[1mStep[0m  [24/26], [94mLoss[0m : 8.58657

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.417, [92mTest[0m: 7.927, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.29592
[1mStep[0m  [2/26], [94mLoss[0m : 8.39512
[1mStep[0m  [4/26], [94mLoss[0m : 8.40171
[1mStep[0m  [6/26], [94mLoss[0m : 8.38070
[1mStep[0m  [8/26], [94mLoss[0m : 8.58190
[1mStep[0m  [10/26], [94mLoss[0m : 8.20773
[1mStep[0m  [12/26], [94mLoss[0m : 8.41844
[1mStep[0m  [14/26], [94mLoss[0m : 8.43137
[1mStep[0m  [16/26], [94mLoss[0m : 8.45830
[1mStep[0m  [18/26], [94mLoss[0m : 8.39662
[1mStep[0m  [20/26], [94mLoss[0m : 8.52869
[1mStep[0m  [22/26], [94mLoss[0m : 8.11627
[1mStep[0m  [24/26], [94mLoss[0m : 8.36253

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.349, [92mTest[0m: 7.862, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.59154
[1mStep[0m  [2/26], [94mLoss[0m : 8.28763
[1mStep[0m  [4/26], [94mLoss[0m : 8.54515
[1mStep[0m  [6/26], [94mLoss[0m : 8.08096
[1mStep[0m  [8/26], [94mLoss[0m : 8.25423
[1mStep[0m  [10/26], [94mLoss[0m : 8.44402
[1mStep[0m  [12/26], [94mLoss[0m : 8.30775
[1mStep[0m  [14/26], [94mLoss[0m : 8.05812
[1mStep[0m  [16/26], [94mLoss[0m : 8.23716
[1mStep[0m  [18/26], [94mLoss[0m : 8.16937
[1mStep[0m  [20/26], [94mLoss[0m : 8.19886
[1mStep[0m  [22/26], [94mLoss[0m : 8.28413
[1mStep[0m  [24/26], [94mLoss[0m : 8.11669

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.270, [92mTest[0m: 7.812, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.30182
[1mStep[0m  [2/26], [94mLoss[0m : 8.24862
[1mStep[0m  [4/26], [94mLoss[0m : 8.27909
[1mStep[0m  [6/26], [94mLoss[0m : 8.31314
[1mStep[0m  [8/26], [94mLoss[0m : 8.29729
[1mStep[0m  [10/26], [94mLoss[0m : 8.08153
[1mStep[0m  [12/26], [94mLoss[0m : 8.24300
[1mStep[0m  [14/26], [94mLoss[0m : 8.01498
[1mStep[0m  [16/26], [94mLoss[0m : 8.21153
[1mStep[0m  [18/26], [94mLoss[0m : 8.08663
[1mStep[0m  [20/26], [94mLoss[0m : 8.14591
[1mStep[0m  [22/26], [94mLoss[0m : 8.09317
[1mStep[0m  [24/26], [94mLoss[0m : 8.35797

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.199, [92mTest[0m: 7.660, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.25882
[1mStep[0m  [2/26], [94mLoss[0m : 8.17617
[1mStep[0m  [4/26], [94mLoss[0m : 8.22846
[1mStep[0m  [6/26], [94mLoss[0m : 8.06090
[1mStep[0m  [8/26], [94mLoss[0m : 8.05422
[1mStep[0m  [10/26], [94mLoss[0m : 8.10814
[1mStep[0m  [12/26], [94mLoss[0m : 7.96882
[1mStep[0m  [14/26], [94mLoss[0m : 8.10661
[1mStep[0m  [16/26], [94mLoss[0m : 8.20304
[1mStep[0m  [18/26], [94mLoss[0m : 7.93598
[1mStep[0m  [20/26], [94mLoss[0m : 8.06988
[1mStep[0m  [22/26], [94mLoss[0m : 8.05039
[1mStep[0m  [24/26], [94mLoss[0m : 8.11121

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.128, [92mTest[0m: 7.604, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.558
====================================

Phase 2 - Evaluation MAE:  7.558251527639536
MAE score P1        9.564124
MAE score P2        7.558252
loss                  8.1283
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay          0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 10.86604
[1mStep[0m  [5/53], [94mLoss[0m : 10.71717
[1mStep[0m  [10/53], [94mLoss[0m : 10.64612
[1mStep[0m  [15/53], [94mLoss[0m : 10.76642
[1mStep[0m  [20/53], [94mLoss[0m : 11.05204
[1mStep[0m  [25/53], [94mLoss[0m : 11.12077
[1mStep[0m  [30/53], [94mLoss[0m : 11.32003
[1mStep[0m  [35/53], [94mLoss[0m : 11.29334
[1mStep[0m  [40/53], [94mLoss[0m : 10.98461
[1mStep[0m  [45/53], [94mLoss[0m : 10.90484
[1mStep[0m  [50/53], [94mLoss[0m : 10.64233

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.76089
[1mStep[0m  [5/53], [94mLoss[0m : 10.81468
[1mStep[0m  [10/53], [94mLoss[0m : 10.84121
[1mStep[0m  [15/53], [94mLoss[0m : 10.75651
[1mStep[0m  [20/53], [94mLoss[0m : 10.51165
[1mStep[0m  [25/53], [94mLoss[0m : 10.75208
[1mStep[0m  [30/53], [94mLoss[0m : 10.58284
[1mStep[0m  [35/53], [94mLoss[0m : 10.68011
[1mStep[0m  [40/53], [94mLoss[0m : 10.74372
[1mStep[0m  [45/53], [94mLoss[0m : 10.44682
[1mStep[0m  [50/53], [94mLoss[0m : 10.44737

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.802, [92mTest[0m: 10.899, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.53254
[1mStep[0m  [5/53], [94mLoss[0m : 10.87866
[1mStep[0m  [10/53], [94mLoss[0m : 10.96187
[1mStep[0m  [15/53], [94mLoss[0m : 10.85810
[1mStep[0m  [20/53], [94mLoss[0m : 10.89412
[1mStep[0m  [25/53], [94mLoss[0m : 10.66497
[1mStep[0m  [30/53], [94mLoss[0m : 10.64792
[1mStep[0m  [35/53], [94mLoss[0m : 10.69467
[1mStep[0m  [40/53], [94mLoss[0m : 10.83080
[1mStep[0m  [45/53], [94mLoss[0m : 11.02196
[1mStep[0m  [50/53], [94mLoss[0m : 10.57954

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.04314
[1mStep[0m  [5/53], [94mLoss[0m : 10.54727
[1mStep[0m  [10/53], [94mLoss[0m : 10.70251
[1mStep[0m  [15/53], [94mLoss[0m : 10.48377
[1mStep[0m  [20/53], [94mLoss[0m : 10.61633
[1mStep[0m  [25/53], [94mLoss[0m : 10.76885
[1mStep[0m  [30/53], [94mLoss[0m : 10.87556
[1mStep[0m  [35/53], [94mLoss[0m : 10.93006
[1mStep[0m  [40/53], [94mLoss[0m : 10.31198
[1mStep[0m  [45/53], [94mLoss[0m : 10.50611
[1mStep[0m  [50/53], [94mLoss[0m : 10.80971

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.623, [92mTest[0m: 10.756, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.80208
[1mStep[0m  [5/53], [94mLoss[0m : 10.30609
[1mStep[0m  [10/53], [94mLoss[0m : 10.88002
[1mStep[0m  [15/53], [94mLoss[0m : 10.92023
[1mStep[0m  [20/53], [94mLoss[0m : 10.68161
[1mStep[0m  [25/53], [94mLoss[0m : 10.12570
[1mStep[0m  [30/53], [94mLoss[0m : 10.26515
[1mStep[0m  [35/53], [94mLoss[0m : 10.71739
[1mStep[0m  [40/53], [94mLoss[0m : 10.04621
[1mStep[0m  [45/53], [94mLoss[0m : 10.32177
[1mStep[0m  [50/53], [94mLoss[0m : 10.55340

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.527, [92mTest[0m: 10.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.12670
[1mStep[0m  [5/53], [94mLoss[0m : 10.33406
[1mStep[0m  [10/53], [94mLoss[0m : 10.27527
[1mStep[0m  [15/53], [94mLoss[0m : 10.67960
[1mStep[0m  [20/53], [94mLoss[0m : 10.08591
[1mStep[0m  [25/53], [94mLoss[0m : 10.02410
[1mStep[0m  [30/53], [94mLoss[0m : 10.46604
[1mStep[0m  [35/53], [94mLoss[0m : 10.46246
[1mStep[0m  [40/53], [94mLoss[0m : 10.44198
[1mStep[0m  [45/53], [94mLoss[0m : 10.44139
[1mStep[0m  [50/53], [94mLoss[0m : 10.79025

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.436, [92mTest[0m: 10.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.45785
[1mStep[0m  [5/53], [94mLoss[0m : 10.62479
[1mStep[0m  [10/53], [94mLoss[0m : 10.84070
[1mStep[0m  [15/53], [94mLoss[0m : 10.61905
[1mStep[0m  [20/53], [94mLoss[0m : 10.34082
[1mStep[0m  [25/53], [94mLoss[0m : 10.39324
[1mStep[0m  [30/53], [94mLoss[0m : 10.45590
[1mStep[0m  [35/53], [94mLoss[0m : 10.59665
[1mStep[0m  [40/53], [94mLoss[0m : 10.31492
[1mStep[0m  [45/53], [94mLoss[0m : 10.64946
[1mStep[0m  [50/53], [94mLoss[0m : 10.28679

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.353, [92mTest[0m: 10.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.38178
[1mStep[0m  [5/53], [94mLoss[0m : 10.61998
[1mStep[0m  [10/53], [94mLoss[0m : 10.48137
[1mStep[0m  [15/53], [94mLoss[0m : 10.83213
[1mStep[0m  [20/53], [94mLoss[0m : 10.11837
[1mStep[0m  [25/53], [94mLoss[0m : 10.27972
[1mStep[0m  [30/53], [94mLoss[0m : 10.57763
[1mStep[0m  [35/53], [94mLoss[0m : 10.19733
[1mStep[0m  [40/53], [94mLoss[0m : 10.34497
[1mStep[0m  [45/53], [94mLoss[0m : 10.26230
[1mStep[0m  [50/53], [94mLoss[0m : 10.38527

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.266, [92mTest[0m: 10.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.19507
[1mStep[0m  [5/53], [94mLoss[0m : 10.18108
[1mStep[0m  [10/53], [94mLoss[0m : 10.43851
[1mStep[0m  [15/53], [94mLoss[0m : 10.41074
[1mStep[0m  [20/53], [94mLoss[0m : 10.33556
[1mStep[0m  [25/53], [94mLoss[0m : 10.38637
[1mStep[0m  [30/53], [94mLoss[0m : 10.41291
[1mStep[0m  [35/53], [94mLoss[0m : 10.12282
[1mStep[0m  [40/53], [94mLoss[0m : 10.34015
[1mStep[0m  [45/53], [94mLoss[0m : 10.09332
[1mStep[0m  [50/53], [94mLoss[0m : 9.83259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.170, [92mTest[0m: 10.416, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.20437
[1mStep[0m  [5/53], [94mLoss[0m : 9.87933
[1mStep[0m  [10/53], [94mLoss[0m : 10.84979
[1mStep[0m  [15/53], [94mLoss[0m : 9.88336
[1mStep[0m  [20/53], [94mLoss[0m : 10.16416
[1mStep[0m  [25/53], [94mLoss[0m : 10.22574
[1mStep[0m  [30/53], [94mLoss[0m : 9.65665
[1mStep[0m  [35/53], [94mLoss[0m : 10.41186
[1mStep[0m  [40/53], [94mLoss[0m : 10.01946
[1mStep[0m  [45/53], [94mLoss[0m : 9.88231
[1mStep[0m  [50/53], [94mLoss[0m : 9.77041

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.086, [92mTest[0m: 10.352, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.87896
[1mStep[0m  [5/53], [94mLoss[0m : 9.63194
[1mStep[0m  [10/53], [94mLoss[0m : 10.07957
[1mStep[0m  [15/53], [94mLoss[0m : 9.63182
[1mStep[0m  [20/53], [94mLoss[0m : 9.93308
[1mStep[0m  [25/53], [94mLoss[0m : 10.00952
[1mStep[0m  [30/53], [94mLoss[0m : 10.27321
[1mStep[0m  [35/53], [94mLoss[0m : 9.91830
[1mStep[0m  [40/53], [94mLoss[0m : 9.85125
[1mStep[0m  [45/53], [94mLoss[0m : 10.07030
[1mStep[0m  [50/53], [94mLoss[0m : 10.08104

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.995, [92mTest[0m: 10.293, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.59362
[1mStep[0m  [5/53], [94mLoss[0m : 9.80722
[1mStep[0m  [10/53], [94mLoss[0m : 10.11804
[1mStep[0m  [15/53], [94mLoss[0m : 9.48587
[1mStep[0m  [20/53], [94mLoss[0m : 9.94302
[1mStep[0m  [25/53], [94mLoss[0m : 10.20578
[1mStep[0m  [30/53], [94mLoss[0m : 9.86181
[1mStep[0m  [35/53], [94mLoss[0m : 9.83759
[1mStep[0m  [40/53], [94mLoss[0m : 10.01246
[1mStep[0m  [45/53], [94mLoss[0m : 10.05272
[1mStep[0m  [50/53], [94mLoss[0m : 9.90592

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.894, [92mTest[0m: 10.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.54967
[1mStep[0m  [5/53], [94mLoss[0m : 9.83743
[1mStep[0m  [10/53], [94mLoss[0m : 9.63661
[1mStep[0m  [15/53], [94mLoss[0m : 9.42235
[1mStep[0m  [20/53], [94mLoss[0m : 9.54353
[1mStep[0m  [25/53], [94mLoss[0m : 10.05215
[1mStep[0m  [30/53], [94mLoss[0m : 10.03466
[1mStep[0m  [35/53], [94mLoss[0m : 9.83656
[1mStep[0m  [40/53], [94mLoss[0m : 9.46471
[1mStep[0m  [45/53], [94mLoss[0m : 10.09681
[1mStep[0m  [50/53], [94mLoss[0m : 9.77590

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.806, [92mTest[0m: 10.142, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.34893
[1mStep[0m  [5/53], [94mLoss[0m : 9.94133
[1mStep[0m  [10/53], [94mLoss[0m : 9.91801
[1mStep[0m  [15/53], [94mLoss[0m : 9.96500
[1mStep[0m  [20/53], [94mLoss[0m : 9.90138
[1mStep[0m  [25/53], [94mLoss[0m : 9.71586
[1mStep[0m  [30/53], [94mLoss[0m : 9.90234
[1mStep[0m  [35/53], [94mLoss[0m : 9.51063
[1mStep[0m  [40/53], [94mLoss[0m : 9.57930
[1mStep[0m  [45/53], [94mLoss[0m : 10.00729
[1mStep[0m  [50/53], [94mLoss[0m : 9.89370

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.714, [92mTest[0m: 10.086, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.48262
[1mStep[0m  [5/53], [94mLoss[0m : 10.03747
[1mStep[0m  [10/53], [94mLoss[0m : 9.58665
[1mStep[0m  [15/53], [94mLoss[0m : 9.61544
[1mStep[0m  [20/53], [94mLoss[0m : 9.51006
[1mStep[0m  [25/53], [94mLoss[0m : 9.97841
[1mStep[0m  [30/53], [94mLoss[0m : 9.74868
[1mStep[0m  [35/53], [94mLoss[0m : 9.78420
[1mStep[0m  [40/53], [94mLoss[0m : 9.73184
[1mStep[0m  [45/53], [94mLoss[0m : 9.39313
[1mStep[0m  [50/53], [94mLoss[0m : 9.29677

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.619, [92mTest[0m: 9.994, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.30012
[1mStep[0m  [5/53], [94mLoss[0m : 9.30984
[1mStep[0m  [10/53], [94mLoss[0m : 10.08543
[1mStep[0m  [15/53], [94mLoss[0m : 9.78715
[1mStep[0m  [20/53], [94mLoss[0m : 9.23263
[1mStep[0m  [25/53], [94mLoss[0m : 9.56024
[1mStep[0m  [30/53], [94mLoss[0m : 9.46770
[1mStep[0m  [35/53], [94mLoss[0m : 9.73937
[1mStep[0m  [40/53], [94mLoss[0m : 9.39820
[1mStep[0m  [45/53], [94mLoss[0m : 9.34014
[1mStep[0m  [50/53], [94mLoss[0m : 9.24453

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.532, [92mTest[0m: 9.940, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.51635
[1mStep[0m  [5/53], [94mLoss[0m : 9.37767
[1mStep[0m  [10/53], [94mLoss[0m : 9.78617
[1mStep[0m  [15/53], [94mLoss[0m : 9.29972
[1mStep[0m  [20/53], [94mLoss[0m : 9.59877
[1mStep[0m  [25/53], [94mLoss[0m : 9.66140
[1mStep[0m  [30/53], [94mLoss[0m : 9.43908
[1mStep[0m  [35/53], [94mLoss[0m : 9.37658
[1mStep[0m  [40/53], [94mLoss[0m : 9.50647
[1mStep[0m  [45/53], [94mLoss[0m : 9.09962
[1mStep[0m  [50/53], [94mLoss[0m : 9.32859

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.442, [92mTest[0m: 9.869, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.29053
[1mStep[0m  [5/53], [94mLoss[0m : 9.48969
[1mStep[0m  [10/53], [94mLoss[0m : 9.48120
[1mStep[0m  [15/53], [94mLoss[0m : 9.03072
[1mStep[0m  [20/53], [94mLoss[0m : 9.49665
[1mStep[0m  [25/53], [94mLoss[0m : 9.44817
[1mStep[0m  [30/53], [94mLoss[0m : 9.36942
[1mStep[0m  [35/53], [94mLoss[0m : 9.38656
[1mStep[0m  [40/53], [94mLoss[0m : 9.48058
[1mStep[0m  [45/53], [94mLoss[0m : 8.95026
[1mStep[0m  [50/53], [94mLoss[0m : 9.31303

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.349, [92mTest[0m: 9.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.28568
[1mStep[0m  [5/53], [94mLoss[0m : 9.25479
[1mStep[0m  [10/53], [94mLoss[0m : 9.24269
[1mStep[0m  [15/53], [94mLoss[0m : 9.09727
[1mStep[0m  [20/53], [94mLoss[0m : 9.08536
[1mStep[0m  [25/53], [94mLoss[0m : 9.29833
[1mStep[0m  [30/53], [94mLoss[0m : 8.87837
[1mStep[0m  [35/53], [94mLoss[0m : 9.00287
[1mStep[0m  [40/53], [94mLoss[0m : 8.96225
[1mStep[0m  [45/53], [94mLoss[0m : 9.44327
[1mStep[0m  [50/53], [94mLoss[0m : 9.28798

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.261, [92mTest[0m: 9.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.85181
[1mStep[0m  [5/53], [94mLoss[0m : 9.15674
[1mStep[0m  [10/53], [94mLoss[0m : 8.92077
[1mStep[0m  [15/53], [94mLoss[0m : 9.33224
[1mStep[0m  [20/53], [94mLoss[0m : 9.10352
[1mStep[0m  [25/53], [94mLoss[0m : 8.88325
[1mStep[0m  [30/53], [94mLoss[0m : 8.99599
[1mStep[0m  [35/53], [94mLoss[0m : 8.85882
[1mStep[0m  [40/53], [94mLoss[0m : 9.27727
[1mStep[0m  [45/53], [94mLoss[0m : 9.23788
[1mStep[0m  [50/53], [94mLoss[0m : 9.06188

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.165, [92mTest[0m: 9.645, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.37749
[1mStep[0m  [5/53], [94mLoss[0m : 9.18116
[1mStep[0m  [10/53], [94mLoss[0m : 9.24971
[1mStep[0m  [15/53], [94mLoss[0m : 9.31734
[1mStep[0m  [20/53], [94mLoss[0m : 9.27211
[1mStep[0m  [25/53], [94mLoss[0m : 8.98551
[1mStep[0m  [30/53], [94mLoss[0m : 9.15244
[1mStep[0m  [35/53], [94mLoss[0m : 8.86054
[1mStep[0m  [40/53], [94mLoss[0m : 9.33358
[1mStep[0m  [45/53], [94mLoss[0m : 8.91831
[1mStep[0m  [50/53], [94mLoss[0m : 8.89829

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.075, [92mTest[0m: 9.584, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.02567
[1mStep[0m  [5/53], [94mLoss[0m : 8.87413
[1mStep[0m  [10/53], [94mLoss[0m : 8.84011
[1mStep[0m  [15/53], [94mLoss[0m : 8.83851
[1mStep[0m  [20/53], [94mLoss[0m : 9.32792
[1mStep[0m  [25/53], [94mLoss[0m : 8.81406
[1mStep[0m  [30/53], [94mLoss[0m : 9.11624
[1mStep[0m  [35/53], [94mLoss[0m : 9.00226
[1mStep[0m  [40/53], [94mLoss[0m : 8.81003
[1mStep[0m  [45/53], [94mLoss[0m : 8.79837
[1mStep[0m  [50/53], [94mLoss[0m : 8.71718

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.990, [92mTest[0m: 9.505, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.59581
[1mStep[0m  [5/53], [94mLoss[0m : 8.75427
[1mStep[0m  [10/53], [94mLoss[0m : 8.63642
[1mStep[0m  [15/53], [94mLoss[0m : 8.85365
[1mStep[0m  [20/53], [94mLoss[0m : 9.26184
[1mStep[0m  [25/53], [94mLoss[0m : 8.88107
[1mStep[0m  [30/53], [94mLoss[0m : 8.77756
[1mStep[0m  [35/53], [94mLoss[0m : 8.78116
[1mStep[0m  [40/53], [94mLoss[0m : 9.05941
[1mStep[0m  [45/53], [94mLoss[0m : 9.06384
[1mStep[0m  [50/53], [94mLoss[0m : 8.94586

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.903, [92mTest[0m: 9.470, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.41808
[1mStep[0m  [5/53], [94mLoss[0m : 8.60374
[1mStep[0m  [10/53], [94mLoss[0m : 9.20947
[1mStep[0m  [15/53], [94mLoss[0m : 9.18707
[1mStep[0m  [20/53], [94mLoss[0m : 9.10238
[1mStep[0m  [25/53], [94mLoss[0m : 9.06782
[1mStep[0m  [30/53], [94mLoss[0m : 8.79701
[1mStep[0m  [35/53], [94mLoss[0m : 8.95311
[1mStep[0m  [40/53], [94mLoss[0m : 8.90125
[1mStep[0m  [45/53], [94mLoss[0m : 8.90178
[1mStep[0m  [50/53], [94mLoss[0m : 8.78255

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.818, [92mTest[0m: 9.389, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.63809
[1mStep[0m  [5/53], [94mLoss[0m : 9.09350
[1mStep[0m  [10/53], [94mLoss[0m : 9.00860
[1mStep[0m  [15/53], [94mLoss[0m : 8.78464
[1mStep[0m  [20/53], [94mLoss[0m : 8.41252
[1mStep[0m  [25/53], [94mLoss[0m : 8.63905
[1mStep[0m  [30/53], [94mLoss[0m : 8.32425
[1mStep[0m  [35/53], [94mLoss[0m : 8.78169
[1mStep[0m  [40/53], [94mLoss[0m : 8.55366
[1mStep[0m  [45/53], [94mLoss[0m : 8.56995
[1mStep[0m  [50/53], [94mLoss[0m : 8.65844

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.735, [92mTest[0m: 9.324, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.88712
[1mStep[0m  [5/53], [94mLoss[0m : 8.63638
[1mStep[0m  [10/53], [94mLoss[0m : 8.63145
[1mStep[0m  [15/53], [94mLoss[0m : 8.80570
[1mStep[0m  [20/53], [94mLoss[0m : 8.49084
[1mStep[0m  [25/53], [94mLoss[0m : 8.59707
[1mStep[0m  [30/53], [94mLoss[0m : 8.56536
[1mStep[0m  [35/53], [94mLoss[0m : 8.45937
[1mStep[0m  [40/53], [94mLoss[0m : 9.19840
[1mStep[0m  [45/53], [94mLoss[0m : 8.46561
[1mStep[0m  [50/53], [94mLoss[0m : 8.62368

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.648, [92mTest[0m: 9.238, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.62817
[1mStep[0m  [5/53], [94mLoss[0m : 8.61996
[1mStep[0m  [10/53], [94mLoss[0m : 8.65152
[1mStep[0m  [15/53], [94mLoss[0m : 8.45919
[1mStep[0m  [20/53], [94mLoss[0m : 8.24885
[1mStep[0m  [25/53], [94mLoss[0m : 8.37089
[1mStep[0m  [30/53], [94mLoss[0m : 8.24268
[1mStep[0m  [35/53], [94mLoss[0m : 8.38975
[1mStep[0m  [40/53], [94mLoss[0m : 8.39735
[1mStep[0m  [45/53], [94mLoss[0m : 8.69261
[1mStep[0m  [50/53], [94mLoss[0m : 8.51998

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.556, [92mTest[0m: 9.195, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.46276
[1mStep[0m  [5/53], [94mLoss[0m : 8.66877
[1mStep[0m  [10/53], [94mLoss[0m : 8.53006
[1mStep[0m  [15/53], [94mLoss[0m : 8.24641
[1mStep[0m  [20/53], [94mLoss[0m : 8.39341
[1mStep[0m  [25/53], [94mLoss[0m : 8.16778
[1mStep[0m  [30/53], [94mLoss[0m : 8.43978
[1mStep[0m  [35/53], [94mLoss[0m : 8.48202
[1mStep[0m  [40/53], [94mLoss[0m : 8.44603
[1mStep[0m  [45/53], [94mLoss[0m : 8.49269
[1mStep[0m  [50/53], [94mLoss[0m : 8.46148

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.473, [92mTest[0m: 9.115, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.50460
[1mStep[0m  [5/53], [94mLoss[0m : 8.50683
[1mStep[0m  [10/53], [94mLoss[0m : 8.13483
[1mStep[0m  [15/53], [94mLoss[0m : 8.31877
[1mStep[0m  [20/53], [94mLoss[0m : 7.99708
[1mStep[0m  [25/53], [94mLoss[0m : 8.44576
[1mStep[0m  [30/53], [94mLoss[0m : 8.12449
[1mStep[0m  [35/53], [94mLoss[0m : 8.52569
[1mStep[0m  [40/53], [94mLoss[0m : 8.53152
[1mStep[0m  [45/53], [94mLoss[0m : 8.21723
[1mStep[0m  [50/53], [94mLoss[0m : 8.32541

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.384, [92mTest[0m: 9.057, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.15531
[1mStep[0m  [5/53], [94mLoss[0m : 8.40927
[1mStep[0m  [10/53], [94mLoss[0m : 8.15506
[1mStep[0m  [15/53], [94mLoss[0m : 8.94360
[1mStep[0m  [20/53], [94mLoss[0m : 7.82377
[1mStep[0m  [25/53], [94mLoss[0m : 8.28572
[1mStep[0m  [30/53], [94mLoss[0m : 8.62406
[1mStep[0m  [35/53], [94mLoss[0m : 7.83580
[1mStep[0m  [40/53], [94mLoss[0m : 8.05842
[1mStep[0m  [45/53], [94mLoss[0m : 8.37076
[1mStep[0m  [50/53], [94mLoss[0m : 8.31690

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.294, [92mTest[0m: 8.994, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.907
====================================

Phase 1 - Evaluation MAE:  8.907335941608135
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/53], [94mLoss[0m : 7.41589
[1mStep[0m  [5/53], [94mLoss[0m : 8.09840
[1mStep[0m  [10/53], [94mLoss[0m : 8.03230
[1mStep[0m  [15/53], [94mLoss[0m : 7.89692
[1mStep[0m  [20/53], [94mLoss[0m : 8.46276
[1mStep[0m  [25/53], [94mLoss[0m : 8.21450
[1mStep[0m  [30/53], [94mLoss[0m : 7.87970
[1mStep[0m  [35/53], [94mLoss[0m : 7.99175
[1mStep[0m  [40/53], [94mLoss[0m : 8.19785
[1mStep[0m  [45/53], [94mLoss[0m : 8.28513
[1mStep[0m  [50/53], [94mLoss[0m : 8.24990

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.203, [92mTest[0m: 8.910, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.42070
[1mStep[0m  [5/53], [94mLoss[0m : 8.22325
[1mStep[0m  [10/53], [94mLoss[0m : 8.16665
[1mStep[0m  [15/53], [94mLoss[0m : 7.91058
[1mStep[0m  [20/53], [94mLoss[0m : 8.13451
[1mStep[0m  [25/53], [94mLoss[0m : 8.24581
[1mStep[0m  [30/53], [94mLoss[0m : 8.01676
[1mStep[0m  [35/53], [94mLoss[0m : 7.69043
[1mStep[0m  [40/53], [94mLoss[0m : 7.84042
[1mStep[0m  [45/53], [94mLoss[0m : 8.17788
[1mStep[0m  [50/53], [94mLoss[0m : 8.21975

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.106, [92mTest[0m: 8.832, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.98123
[1mStep[0m  [5/53], [94mLoss[0m : 7.80408
[1mStep[0m  [10/53], [94mLoss[0m : 8.32290
[1mStep[0m  [15/53], [94mLoss[0m : 7.66084
[1mStep[0m  [20/53], [94mLoss[0m : 8.35137
[1mStep[0m  [25/53], [94mLoss[0m : 7.64939
[1mStep[0m  [30/53], [94mLoss[0m : 8.04610
[1mStep[0m  [35/53], [94mLoss[0m : 8.25279
[1mStep[0m  [40/53], [94mLoss[0m : 8.05397
[1mStep[0m  [45/53], [94mLoss[0m : 8.05440
[1mStep[0m  [50/53], [94mLoss[0m : 7.70272

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.001, [92mTest[0m: 8.746, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.22931
[1mStep[0m  [5/53], [94mLoss[0m : 7.84943
[1mStep[0m  [10/53], [94mLoss[0m : 8.03779
[1mStep[0m  [15/53], [94mLoss[0m : 7.78748
[1mStep[0m  [20/53], [94mLoss[0m : 7.79908
[1mStep[0m  [25/53], [94mLoss[0m : 7.82581
[1mStep[0m  [30/53], [94mLoss[0m : 7.96921
[1mStep[0m  [35/53], [94mLoss[0m : 7.52204
[1mStep[0m  [40/53], [94mLoss[0m : 7.89003
[1mStep[0m  [45/53], [94mLoss[0m : 7.42940
[1mStep[0m  [50/53], [94mLoss[0m : 7.69084

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.894, [92mTest[0m: 8.658, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.79642
[1mStep[0m  [5/53], [94mLoss[0m : 8.04104
[1mStep[0m  [10/53], [94mLoss[0m : 7.78327
[1mStep[0m  [15/53], [94mLoss[0m : 7.99922
[1mStep[0m  [20/53], [94mLoss[0m : 7.59787
[1mStep[0m  [25/53], [94mLoss[0m : 7.66574
[1mStep[0m  [30/53], [94mLoss[0m : 7.77139
[1mStep[0m  [35/53], [94mLoss[0m : 7.86207
[1mStep[0m  [40/53], [94mLoss[0m : 7.90338
[1mStep[0m  [45/53], [94mLoss[0m : 7.41567
[1mStep[0m  [50/53], [94mLoss[0m : 7.70947

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.793, [92mTest[0m: 8.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.74339
[1mStep[0m  [5/53], [94mLoss[0m : 7.87518
[1mStep[0m  [10/53], [94mLoss[0m : 8.16768
[1mStep[0m  [15/53], [94mLoss[0m : 7.87503
[1mStep[0m  [20/53], [94mLoss[0m : 7.29366
[1mStep[0m  [25/53], [94mLoss[0m : 7.52102
[1mStep[0m  [30/53], [94mLoss[0m : 7.66783
[1mStep[0m  [35/53], [94mLoss[0m : 7.78068
[1mStep[0m  [40/53], [94mLoss[0m : 7.47412
[1mStep[0m  [45/53], [94mLoss[0m : 7.39906
[1mStep[0m  [50/53], [94mLoss[0m : 7.53480

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.694, [92mTest[0m: 8.411, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.98873
[1mStep[0m  [5/53], [94mLoss[0m : 7.48209
[1mStep[0m  [10/53], [94mLoss[0m : 7.42420
[1mStep[0m  [15/53], [94mLoss[0m : 7.51493
[1mStep[0m  [20/53], [94mLoss[0m : 7.29423
[1mStep[0m  [25/53], [94mLoss[0m : 7.93746
[1mStep[0m  [30/53], [94mLoss[0m : 7.88268
[1mStep[0m  [35/53], [94mLoss[0m : 7.69063
[1mStep[0m  [40/53], [94mLoss[0m : 7.43921
[1mStep[0m  [45/53], [94mLoss[0m : 7.45468
[1mStep[0m  [50/53], [94mLoss[0m : 7.55240

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.596, [92mTest[0m: 8.349, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.49042
[1mStep[0m  [5/53], [94mLoss[0m : 7.76831
[1mStep[0m  [10/53], [94mLoss[0m : 7.49817
[1mStep[0m  [15/53], [94mLoss[0m : 7.57851
[1mStep[0m  [20/53], [94mLoss[0m : 7.38086
[1mStep[0m  [25/53], [94mLoss[0m : 7.28792
[1mStep[0m  [30/53], [94mLoss[0m : 7.72822
[1mStep[0m  [35/53], [94mLoss[0m : 7.05964
[1mStep[0m  [40/53], [94mLoss[0m : 7.76432
[1mStep[0m  [45/53], [94mLoss[0m : 7.33040
[1mStep[0m  [50/53], [94mLoss[0m : 7.27959

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.481, [92mTest[0m: 8.234, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.10267
[1mStep[0m  [5/53], [94mLoss[0m : 7.74915
[1mStep[0m  [10/53], [94mLoss[0m : 7.03254
[1mStep[0m  [15/53], [94mLoss[0m : 7.62794
[1mStep[0m  [20/53], [94mLoss[0m : 7.84789
[1mStep[0m  [25/53], [94mLoss[0m : 7.68114
[1mStep[0m  [30/53], [94mLoss[0m : 7.80523
[1mStep[0m  [35/53], [94mLoss[0m : 7.46477
[1mStep[0m  [40/53], [94mLoss[0m : 7.50938
[1mStep[0m  [45/53], [94mLoss[0m : 7.13264
[1mStep[0m  [50/53], [94mLoss[0m : 7.22094

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.379, [92mTest[0m: 8.126, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.42060
[1mStep[0m  [5/53], [94mLoss[0m : 7.27690
[1mStep[0m  [10/53], [94mLoss[0m : 7.05722
[1mStep[0m  [15/53], [94mLoss[0m : 7.33680
[1mStep[0m  [20/53], [94mLoss[0m : 7.28677
[1mStep[0m  [25/53], [94mLoss[0m : 7.02732
[1mStep[0m  [30/53], [94mLoss[0m : 7.27638
[1mStep[0m  [35/53], [94mLoss[0m : 7.15647
[1mStep[0m  [40/53], [94mLoss[0m : 6.94980
[1mStep[0m  [45/53], [94mLoss[0m : 7.32252
[1mStep[0m  [50/53], [94mLoss[0m : 7.57916

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.269, [92mTest[0m: 8.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.23063
[1mStep[0m  [5/53], [94mLoss[0m : 6.91886
[1mStep[0m  [10/53], [94mLoss[0m : 7.43202
[1mStep[0m  [15/53], [94mLoss[0m : 6.86574
[1mStep[0m  [20/53], [94mLoss[0m : 6.96046
[1mStep[0m  [25/53], [94mLoss[0m : 6.97630
[1mStep[0m  [30/53], [94mLoss[0m : 6.86588
[1mStep[0m  [35/53], [94mLoss[0m : 7.12383
[1mStep[0m  [40/53], [94mLoss[0m : 7.35839
[1mStep[0m  [45/53], [94mLoss[0m : 7.04192
[1mStep[0m  [50/53], [94mLoss[0m : 7.02409

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.167, [92mTest[0m: 7.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.13501
[1mStep[0m  [5/53], [94mLoss[0m : 7.16275
[1mStep[0m  [10/53], [94mLoss[0m : 7.15631
[1mStep[0m  [15/53], [94mLoss[0m : 7.28944
[1mStep[0m  [20/53], [94mLoss[0m : 6.83155
[1mStep[0m  [25/53], [94mLoss[0m : 7.15334
[1mStep[0m  [30/53], [94mLoss[0m : 7.24865
[1mStep[0m  [35/53], [94mLoss[0m : 7.39611
[1mStep[0m  [40/53], [94mLoss[0m : 7.19979
[1mStep[0m  [45/53], [94mLoss[0m : 6.72979
[1mStep[0m  [50/53], [94mLoss[0m : 6.72292

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.064, [92mTest[0m: 7.805, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.43572
[1mStep[0m  [5/53], [94mLoss[0m : 7.16077
[1mStep[0m  [10/53], [94mLoss[0m : 6.93365
[1mStep[0m  [15/53], [94mLoss[0m : 6.99688
[1mStep[0m  [20/53], [94mLoss[0m : 6.93344
[1mStep[0m  [25/53], [94mLoss[0m : 7.02056
[1mStep[0m  [30/53], [94mLoss[0m : 7.17802
[1mStep[0m  [35/53], [94mLoss[0m : 6.76148
[1mStep[0m  [40/53], [94mLoss[0m : 6.91868
[1mStep[0m  [45/53], [94mLoss[0m : 6.67908
[1mStep[0m  [50/53], [94mLoss[0m : 6.85126

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.946, [92mTest[0m: 7.683, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.13634
[1mStep[0m  [5/53], [94mLoss[0m : 6.97765
[1mStep[0m  [10/53], [94mLoss[0m : 6.78398
[1mStep[0m  [15/53], [94mLoss[0m : 7.00276
[1mStep[0m  [20/53], [94mLoss[0m : 7.29359
[1mStep[0m  [25/53], [94mLoss[0m : 6.88554
[1mStep[0m  [30/53], [94mLoss[0m : 7.06055
[1mStep[0m  [35/53], [94mLoss[0m : 6.84553
[1mStep[0m  [40/53], [94mLoss[0m : 6.84835
[1mStep[0m  [45/53], [94mLoss[0m : 7.03503
[1mStep[0m  [50/53], [94mLoss[0m : 6.79453

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.867, [92mTest[0m: 7.570, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.76877
[1mStep[0m  [5/53], [94mLoss[0m : 6.69445
[1mStep[0m  [10/53], [94mLoss[0m : 6.89126
[1mStep[0m  [15/53], [94mLoss[0m : 6.98917
[1mStep[0m  [20/53], [94mLoss[0m : 6.90644
[1mStep[0m  [25/53], [94mLoss[0m : 6.83900
[1mStep[0m  [30/53], [94mLoss[0m : 6.47488
[1mStep[0m  [35/53], [94mLoss[0m : 6.74536
[1mStep[0m  [40/53], [94mLoss[0m : 6.77694
[1mStep[0m  [45/53], [94mLoss[0m : 7.29128
[1mStep[0m  [50/53], [94mLoss[0m : 6.67982

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.758, [92mTest[0m: 7.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.63907
[1mStep[0m  [5/53], [94mLoss[0m : 7.14399
[1mStep[0m  [10/53], [94mLoss[0m : 6.79828
[1mStep[0m  [15/53], [94mLoss[0m : 6.91972
[1mStep[0m  [20/53], [94mLoss[0m : 6.68124
[1mStep[0m  [25/53], [94mLoss[0m : 6.51484
[1mStep[0m  [30/53], [94mLoss[0m : 6.67467
[1mStep[0m  [35/53], [94mLoss[0m : 6.44366
[1mStep[0m  [40/53], [94mLoss[0m : 6.43676
[1mStep[0m  [45/53], [94mLoss[0m : 6.42019
[1mStep[0m  [50/53], [94mLoss[0m : 6.61106

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.642, [92mTest[0m: 7.340, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.27787
[1mStep[0m  [5/53], [94mLoss[0m : 6.61005
[1mStep[0m  [10/53], [94mLoss[0m : 6.57728
[1mStep[0m  [15/53], [94mLoss[0m : 6.70831
[1mStep[0m  [20/53], [94mLoss[0m : 6.87305
[1mStep[0m  [25/53], [94mLoss[0m : 6.41495
[1mStep[0m  [30/53], [94mLoss[0m : 6.59439
[1mStep[0m  [35/53], [94mLoss[0m : 6.43457
[1mStep[0m  [40/53], [94mLoss[0m : 6.53258
[1mStep[0m  [45/53], [94mLoss[0m : 6.42695
[1mStep[0m  [50/53], [94mLoss[0m : 6.12921

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.537, [92mTest[0m: 7.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.73610
[1mStep[0m  [5/53], [94mLoss[0m : 6.64673
[1mStep[0m  [10/53], [94mLoss[0m : 6.77290
[1mStep[0m  [15/53], [94mLoss[0m : 6.01857
[1mStep[0m  [20/53], [94mLoss[0m : 6.29095
[1mStep[0m  [25/53], [94mLoss[0m : 6.13804
[1mStep[0m  [30/53], [94mLoss[0m : 6.53672
[1mStep[0m  [35/53], [94mLoss[0m : 6.60765
[1mStep[0m  [40/53], [94mLoss[0m : 6.05548
[1mStep[0m  [45/53], [94mLoss[0m : 6.41412
[1mStep[0m  [50/53], [94mLoss[0m : 6.27680

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.443, [92mTest[0m: 7.107, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.40165
[1mStep[0m  [5/53], [94mLoss[0m : 6.40873
[1mStep[0m  [10/53], [94mLoss[0m : 6.55620
[1mStep[0m  [15/53], [94mLoss[0m : 6.24289
[1mStep[0m  [20/53], [94mLoss[0m : 6.32983
[1mStep[0m  [25/53], [94mLoss[0m : 6.49222
[1mStep[0m  [30/53], [94mLoss[0m : 6.24749
[1mStep[0m  [35/53], [94mLoss[0m : 6.20049
[1mStep[0m  [40/53], [94mLoss[0m : 6.58540
[1mStep[0m  [45/53], [94mLoss[0m : 6.26030
[1mStep[0m  [50/53], [94mLoss[0m : 6.05179

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.319, [92mTest[0m: 6.980, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.43613
[1mStep[0m  [5/53], [94mLoss[0m : 6.21630
[1mStep[0m  [10/53], [94mLoss[0m : 6.45234
[1mStep[0m  [15/53], [94mLoss[0m : 6.31292
[1mStep[0m  [20/53], [94mLoss[0m : 6.26177
[1mStep[0m  [25/53], [94mLoss[0m : 6.38920
[1mStep[0m  [30/53], [94mLoss[0m : 5.93257
[1mStep[0m  [35/53], [94mLoss[0m : 5.74459
[1mStep[0m  [40/53], [94mLoss[0m : 5.93790
[1mStep[0m  [45/53], [94mLoss[0m : 6.12913
[1mStep[0m  [50/53], [94mLoss[0m : 6.49565

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.211, [92mTest[0m: 6.874, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.44884
[1mStep[0m  [5/53], [94mLoss[0m : 6.20759
[1mStep[0m  [10/53], [94mLoss[0m : 5.81989
[1mStep[0m  [15/53], [94mLoss[0m : 5.99693
[1mStep[0m  [20/53], [94mLoss[0m : 6.35868
[1mStep[0m  [25/53], [94mLoss[0m : 5.97687
[1mStep[0m  [30/53], [94mLoss[0m : 6.26345
[1mStep[0m  [35/53], [94mLoss[0m : 5.96190
[1mStep[0m  [40/53], [94mLoss[0m : 5.74793
[1mStep[0m  [45/53], [94mLoss[0m : 6.09944
[1mStep[0m  [50/53], [94mLoss[0m : 5.72571

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.131, [92mTest[0m: 6.757, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.87468
[1mStep[0m  [5/53], [94mLoss[0m : 6.03729
[1mStep[0m  [10/53], [94mLoss[0m : 5.85899
[1mStep[0m  [15/53], [94mLoss[0m : 6.10271
[1mStep[0m  [20/53], [94mLoss[0m : 5.77902
[1mStep[0m  [25/53], [94mLoss[0m : 6.29047
[1mStep[0m  [30/53], [94mLoss[0m : 6.34322
[1mStep[0m  [35/53], [94mLoss[0m : 5.52274
[1mStep[0m  [40/53], [94mLoss[0m : 5.92305
[1mStep[0m  [45/53], [94mLoss[0m : 5.99520
[1mStep[0m  [50/53], [94mLoss[0m : 6.19150

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.027, [92mTest[0m: 6.628, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.86274
[1mStep[0m  [5/53], [94mLoss[0m : 6.07747
[1mStep[0m  [10/53], [94mLoss[0m : 6.01968
[1mStep[0m  [15/53], [94mLoss[0m : 5.85327
[1mStep[0m  [20/53], [94mLoss[0m : 6.02211
[1mStep[0m  [25/53], [94mLoss[0m : 5.68535
[1mStep[0m  [30/53], [94mLoss[0m : 6.02596
[1mStep[0m  [35/53], [94mLoss[0m : 5.91608
[1mStep[0m  [40/53], [94mLoss[0m : 5.79404
[1mStep[0m  [45/53], [94mLoss[0m : 5.90885
[1mStep[0m  [50/53], [94mLoss[0m : 5.72479

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.938, [92mTest[0m: 6.541, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.78906
[1mStep[0m  [5/53], [94mLoss[0m : 5.76389
[1mStep[0m  [10/53], [94mLoss[0m : 6.34537
[1mStep[0m  [15/53], [94mLoss[0m : 5.98976
[1mStep[0m  [20/53], [94mLoss[0m : 5.71963
[1mStep[0m  [25/53], [94mLoss[0m : 6.13320
[1mStep[0m  [30/53], [94mLoss[0m : 5.66314
[1mStep[0m  [35/53], [94mLoss[0m : 5.74561
[1mStep[0m  [40/53], [94mLoss[0m : 5.96763
[1mStep[0m  [45/53], [94mLoss[0m : 5.86646
[1mStep[0m  [50/53], [94mLoss[0m : 6.05362

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.851, [92mTest[0m: 6.404, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.05094
[1mStep[0m  [5/53], [94mLoss[0m : 6.06004
[1mStep[0m  [10/53], [94mLoss[0m : 5.74335
[1mStep[0m  [15/53], [94mLoss[0m : 6.01984
[1mStep[0m  [20/53], [94mLoss[0m : 5.66211
[1mStep[0m  [25/53], [94mLoss[0m : 5.82143
[1mStep[0m  [30/53], [94mLoss[0m : 5.66193
[1mStep[0m  [35/53], [94mLoss[0m : 5.63287
[1mStep[0m  [40/53], [94mLoss[0m : 5.52769
[1mStep[0m  [45/53], [94mLoss[0m : 5.56060
[1mStep[0m  [50/53], [94mLoss[0m : 6.04292

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.744, [92mTest[0m: 6.279, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.69364
[1mStep[0m  [5/53], [94mLoss[0m : 5.65228
[1mStep[0m  [10/53], [94mLoss[0m : 5.48134
[1mStep[0m  [15/53], [94mLoss[0m : 5.56058
[1mStep[0m  [20/53], [94mLoss[0m : 6.00975
[1mStep[0m  [25/53], [94mLoss[0m : 5.72270
[1mStep[0m  [30/53], [94mLoss[0m : 6.10662
[1mStep[0m  [35/53], [94mLoss[0m : 5.37831
[1mStep[0m  [40/53], [94mLoss[0m : 5.69145
[1mStep[0m  [45/53], [94mLoss[0m : 5.93800
[1mStep[0m  [50/53], [94mLoss[0m : 5.73151

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.641, [92mTest[0m: 6.211, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.96532
[1mStep[0m  [5/53], [94mLoss[0m : 5.45991
[1mStep[0m  [10/53], [94mLoss[0m : 5.97058
[1mStep[0m  [15/53], [94mLoss[0m : 5.72389
[1mStep[0m  [20/53], [94mLoss[0m : 5.68452
[1mStep[0m  [25/53], [94mLoss[0m : 5.28184
[1mStep[0m  [30/53], [94mLoss[0m : 5.33218
[1mStep[0m  [35/53], [94mLoss[0m : 5.40200
[1mStep[0m  [40/53], [94mLoss[0m : 5.32822
[1mStep[0m  [45/53], [94mLoss[0m : 5.36280
[1mStep[0m  [50/53], [94mLoss[0m : 5.47755

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.554, [92mTest[0m: 6.132, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.12052
[1mStep[0m  [5/53], [94mLoss[0m : 5.59742
[1mStep[0m  [10/53], [94mLoss[0m : 5.26104
[1mStep[0m  [15/53], [94mLoss[0m : 5.46426
[1mStep[0m  [20/53], [94mLoss[0m : 5.39643
[1mStep[0m  [25/53], [94mLoss[0m : 5.53600
[1mStep[0m  [30/53], [94mLoss[0m : 5.10336
[1mStep[0m  [35/53], [94mLoss[0m : 5.22496
[1mStep[0m  [40/53], [94mLoss[0m : 5.26018
[1mStep[0m  [45/53], [94mLoss[0m : 5.15712
[1mStep[0m  [50/53], [94mLoss[0m : 5.29964

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 5.483, [92mTest[0m: 5.960, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.08185
[1mStep[0m  [5/53], [94mLoss[0m : 5.18463
[1mStep[0m  [10/53], [94mLoss[0m : 5.53228
[1mStep[0m  [15/53], [94mLoss[0m : 5.37513
[1mStep[0m  [20/53], [94mLoss[0m : 5.66108
[1mStep[0m  [25/53], [94mLoss[0m : 5.87891
[1mStep[0m  [30/53], [94mLoss[0m : 5.15957
[1mStep[0m  [35/53], [94mLoss[0m : 5.27034
[1mStep[0m  [40/53], [94mLoss[0m : 5.54649
[1mStep[0m  [45/53], [94mLoss[0m : 5.12069
[1mStep[0m  [50/53], [94mLoss[0m : 5.30536

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 5.377, [92mTest[0m: 5.865, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.59967
[1mStep[0m  [5/53], [94mLoss[0m : 4.84701
[1mStep[0m  [10/53], [94mLoss[0m : 5.14306
[1mStep[0m  [15/53], [94mLoss[0m : 5.28579
[1mStep[0m  [20/53], [94mLoss[0m : 5.28291
[1mStep[0m  [25/53], [94mLoss[0m : 5.62353
[1mStep[0m  [30/53], [94mLoss[0m : 5.30892
[1mStep[0m  [35/53], [94mLoss[0m : 5.30447
[1mStep[0m  [40/53], [94mLoss[0m : 5.88987
[1mStep[0m  [45/53], [94mLoss[0m : 5.69783
[1mStep[0m  [50/53], [94mLoss[0m : 5.11952

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 5.291, [92mTest[0m: 5.724, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.638
====================================

Phase 2 - Evaluation MAE:  5.637930099780743
MAE score P1       8.907336
MAE score P2        5.63793
loss                5.29068
learning_rate        0.0001
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.91638
[1mStep[0m  [5/53], [94mLoss[0m : 10.65635
[1mStep[0m  [10/53], [94mLoss[0m : 10.26032
[1mStep[0m  [15/53], [94mLoss[0m : 10.16260
[1mStep[0m  [20/53], [94mLoss[0m : 10.44538
[1mStep[0m  [25/53], [94mLoss[0m : 10.25934
[1mStep[0m  [30/53], [94mLoss[0m : 10.72882
[1mStep[0m  [35/53], [94mLoss[0m : 10.57728
[1mStep[0m  [40/53], [94mLoss[0m : 10.39948
[1mStep[0m  [45/53], [94mLoss[0m : 10.69551
[1mStep[0m  [50/53], [94mLoss[0m : 10.40136

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.508, [92mTest[0m: 10.833, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.38734
[1mStep[0m  [5/53], [94mLoss[0m : 9.88093
[1mStep[0m  [10/53], [94mLoss[0m : 10.25940
[1mStep[0m  [15/53], [94mLoss[0m : 10.01229
[1mStep[0m  [20/53], [94mLoss[0m : 9.70592
[1mStep[0m  [25/53], [94mLoss[0m : 9.75459
[1mStep[0m  [30/53], [94mLoss[0m : 9.88031
[1mStep[0m  [35/53], [94mLoss[0m : 9.87760
[1mStep[0m  [40/53], [94mLoss[0m : 9.49981
[1mStep[0m  [45/53], [94mLoss[0m : 9.58878
[1mStep[0m  [50/53], [94mLoss[0m : 9.50134

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.833, [92mTest[0m: 10.176, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.32962
[1mStep[0m  [5/53], [94mLoss[0m : 9.73445
[1mStep[0m  [10/53], [94mLoss[0m : 9.36911
[1mStep[0m  [15/53], [94mLoss[0m : 9.31863
[1mStep[0m  [20/53], [94mLoss[0m : 9.48046
[1mStep[0m  [25/53], [94mLoss[0m : 9.03282
[1mStep[0m  [30/53], [94mLoss[0m : 8.84501
[1mStep[0m  [35/53], [94mLoss[0m : 8.72617
[1mStep[0m  [40/53], [94mLoss[0m : 8.90902
[1mStep[0m  [45/53], [94mLoss[0m : 9.17080
[1mStep[0m  [50/53], [94mLoss[0m : 8.95454

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.157, [92mTest[0m: 9.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.53382
[1mStep[0m  [5/53], [94mLoss[0m : 8.91004
[1mStep[0m  [10/53], [94mLoss[0m : 8.76614
[1mStep[0m  [15/53], [94mLoss[0m : 8.96039
[1mStep[0m  [20/53], [94mLoss[0m : 8.25192
[1mStep[0m  [25/53], [94mLoss[0m : 8.47378
[1mStep[0m  [30/53], [94mLoss[0m : 8.65588
[1mStep[0m  [35/53], [94mLoss[0m : 8.48571
[1mStep[0m  [40/53], [94mLoss[0m : 8.43963
[1mStep[0m  [45/53], [94mLoss[0m : 8.07962
[1mStep[0m  [50/53], [94mLoss[0m : 7.94933

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.487, [92mTest[0m: 8.814, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.16164
[1mStep[0m  [5/53], [94mLoss[0m : 8.33331
[1mStep[0m  [10/53], [94mLoss[0m : 7.95450
[1mStep[0m  [15/53], [94mLoss[0m : 7.91696
[1mStep[0m  [20/53], [94mLoss[0m : 7.79294
[1mStep[0m  [25/53], [94mLoss[0m : 7.65301
[1mStep[0m  [30/53], [94mLoss[0m : 7.84556
[1mStep[0m  [35/53], [94mLoss[0m : 7.66643
[1mStep[0m  [40/53], [94mLoss[0m : 7.29021
[1mStep[0m  [45/53], [94mLoss[0m : 7.38927
[1mStep[0m  [50/53], [94mLoss[0m : 7.52917

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.815, [92mTest[0m: 8.163, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.38269
[1mStep[0m  [5/53], [94mLoss[0m : 7.50448
[1mStep[0m  [10/53], [94mLoss[0m : 6.90219
[1mStep[0m  [15/53], [94mLoss[0m : 7.38576
[1mStep[0m  [20/53], [94mLoss[0m : 7.52506
[1mStep[0m  [25/53], [94mLoss[0m : 7.10650
[1mStep[0m  [30/53], [94mLoss[0m : 7.16357
[1mStep[0m  [35/53], [94mLoss[0m : 6.80577
[1mStep[0m  [40/53], [94mLoss[0m : 6.72186
[1mStep[0m  [45/53], [94mLoss[0m : 6.98203
[1mStep[0m  [50/53], [94mLoss[0m : 6.90823

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.140, [92mTest[0m: 7.501, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.87458
[1mStep[0m  [5/53], [94mLoss[0m : 7.00121
[1mStep[0m  [10/53], [94mLoss[0m : 6.76969
[1mStep[0m  [15/53], [94mLoss[0m : 6.72968
[1mStep[0m  [20/53], [94mLoss[0m : 6.20625
[1mStep[0m  [25/53], [94mLoss[0m : 6.53206
[1mStep[0m  [30/53], [94mLoss[0m : 6.54742
[1mStep[0m  [35/53], [94mLoss[0m : 6.10846
[1mStep[0m  [40/53], [94mLoss[0m : 6.10223
[1mStep[0m  [45/53], [94mLoss[0m : 5.95340
[1mStep[0m  [50/53], [94mLoss[0m : 5.79678

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.468, [92mTest[0m: 6.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.24117
[1mStep[0m  [5/53], [94mLoss[0m : 5.78628
[1mStep[0m  [10/53], [94mLoss[0m : 5.82815
[1mStep[0m  [15/53], [94mLoss[0m : 5.96064
[1mStep[0m  [20/53], [94mLoss[0m : 5.98994
[1mStep[0m  [25/53], [94mLoss[0m : 5.62187
[1mStep[0m  [30/53], [94mLoss[0m : 5.59665
[1mStep[0m  [35/53], [94mLoss[0m : 5.91032
[1mStep[0m  [40/53], [94mLoss[0m : 6.21594
[1mStep[0m  [45/53], [94mLoss[0m : 5.38390
[1mStep[0m  [50/53], [94mLoss[0m : 5.27617

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.800, [92mTest[0m: 6.155, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.32428
[1mStep[0m  [5/53], [94mLoss[0m : 5.23932
[1mStep[0m  [10/53], [94mLoss[0m : 5.17080
[1mStep[0m  [15/53], [94mLoss[0m : 5.67112
[1mStep[0m  [20/53], [94mLoss[0m : 5.37899
[1mStep[0m  [25/53], [94mLoss[0m : 5.16796
[1mStep[0m  [30/53], [94mLoss[0m : 4.93181
[1mStep[0m  [35/53], [94mLoss[0m : 5.23536
[1mStep[0m  [40/53], [94mLoss[0m : 4.91440
[1mStep[0m  [45/53], [94mLoss[0m : 5.04210
[1mStep[0m  [50/53], [94mLoss[0m : 4.91857

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.187, [92mTest[0m: 5.493, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.08289
[1mStep[0m  [5/53], [94mLoss[0m : 4.60894
[1mStep[0m  [10/53], [94mLoss[0m : 4.78176
[1mStep[0m  [15/53], [94mLoss[0m : 4.58738
[1mStep[0m  [20/53], [94mLoss[0m : 4.67300
[1mStep[0m  [25/53], [94mLoss[0m : 4.76324
[1mStep[0m  [30/53], [94mLoss[0m : 4.66986
[1mStep[0m  [35/53], [94mLoss[0m : 4.57526
[1mStep[0m  [40/53], [94mLoss[0m : 4.77701
[1mStep[0m  [45/53], [94mLoss[0m : 4.72595
[1mStep[0m  [50/53], [94mLoss[0m : 4.55077

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.656, [92mTest[0m: 4.912, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.28770
[1mStep[0m  [5/53], [94mLoss[0m : 4.59025
[1mStep[0m  [10/53], [94mLoss[0m : 4.14702
[1mStep[0m  [15/53], [94mLoss[0m : 4.34181
[1mStep[0m  [20/53], [94mLoss[0m : 4.21108
[1mStep[0m  [25/53], [94mLoss[0m : 4.27581
[1mStep[0m  [30/53], [94mLoss[0m : 4.39946
[1mStep[0m  [35/53], [94mLoss[0m : 4.25544
[1mStep[0m  [40/53], [94mLoss[0m : 4.38231
[1mStep[0m  [45/53], [94mLoss[0m : 4.21371
[1mStep[0m  [50/53], [94mLoss[0m : 4.40770

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.223, [92mTest[0m: 4.425, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.44131
[1mStep[0m  [5/53], [94mLoss[0m : 3.99788
[1mStep[0m  [10/53], [94mLoss[0m : 3.91939
[1mStep[0m  [15/53], [94mLoss[0m : 4.35926
[1mStep[0m  [20/53], [94mLoss[0m : 3.52736
[1mStep[0m  [25/53], [94mLoss[0m : 3.73605
[1mStep[0m  [30/53], [94mLoss[0m : 4.10838
[1mStep[0m  [35/53], [94mLoss[0m : 3.71407
[1mStep[0m  [40/53], [94mLoss[0m : 3.68598
[1mStep[0m  [45/53], [94mLoss[0m : 3.64788
[1mStep[0m  [50/53], [94mLoss[0m : 4.01140

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.893, [92mTest[0m: 4.040, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.99852
[1mStep[0m  [5/53], [94mLoss[0m : 3.84832
[1mStep[0m  [10/53], [94mLoss[0m : 3.57838
[1mStep[0m  [15/53], [94mLoss[0m : 3.63901
[1mStep[0m  [20/53], [94mLoss[0m : 3.63929
[1mStep[0m  [25/53], [94mLoss[0m : 3.67500
[1mStep[0m  [30/53], [94mLoss[0m : 3.69586
[1mStep[0m  [35/53], [94mLoss[0m : 3.49282
[1mStep[0m  [40/53], [94mLoss[0m : 3.83774
[1mStep[0m  [45/53], [94mLoss[0m : 3.51819
[1mStep[0m  [50/53], [94mLoss[0m : 3.32842

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.631, [92mTest[0m: 3.742, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.64544
[1mStep[0m  [5/53], [94mLoss[0m : 3.22524
[1mStep[0m  [10/53], [94mLoss[0m : 3.72895
[1mStep[0m  [15/53], [94mLoss[0m : 3.41204
[1mStep[0m  [20/53], [94mLoss[0m : 3.48172
[1mStep[0m  [25/53], [94mLoss[0m : 2.97486
[1mStep[0m  [30/53], [94mLoss[0m : 3.60304
[1mStep[0m  [35/53], [94mLoss[0m : 3.52336
[1mStep[0m  [40/53], [94mLoss[0m : 3.18699
[1mStep[0m  [45/53], [94mLoss[0m : 3.38293
[1mStep[0m  [50/53], [94mLoss[0m : 3.37193

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.432, [92mTest[0m: 3.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.34875
[1mStep[0m  [5/53], [94mLoss[0m : 3.43019
[1mStep[0m  [10/53], [94mLoss[0m : 3.55423
[1mStep[0m  [15/53], [94mLoss[0m : 3.08140
[1mStep[0m  [20/53], [94mLoss[0m : 3.34716
[1mStep[0m  [25/53], [94mLoss[0m : 3.05648
[1mStep[0m  [30/53], [94mLoss[0m : 3.28086
[1mStep[0m  [35/53], [94mLoss[0m : 3.20787
[1mStep[0m  [40/53], [94mLoss[0m : 3.42725
[1mStep[0m  [45/53], [94mLoss[0m : 3.09319
[1mStep[0m  [50/53], [94mLoss[0m : 3.43307

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.288, [92mTest[0m: 3.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.33593
[1mStep[0m  [5/53], [94mLoss[0m : 3.04437
[1mStep[0m  [10/53], [94mLoss[0m : 3.02113
[1mStep[0m  [15/53], [94mLoss[0m : 3.16149
[1mStep[0m  [20/53], [94mLoss[0m : 3.12844
[1mStep[0m  [25/53], [94mLoss[0m : 3.11780
[1mStep[0m  [30/53], [94mLoss[0m : 2.84292
[1mStep[0m  [35/53], [94mLoss[0m : 3.09642
[1mStep[0m  [40/53], [94mLoss[0m : 2.98239
[1mStep[0m  [45/53], [94mLoss[0m : 3.00134
[1mStep[0m  [50/53], [94mLoss[0m : 3.24788

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.178, [92mTest[0m: 3.211, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.02838
[1mStep[0m  [5/53], [94mLoss[0m : 2.86349
[1mStep[0m  [10/53], [94mLoss[0m : 3.41197
[1mStep[0m  [15/53], [94mLoss[0m : 3.13469
[1mStep[0m  [20/53], [94mLoss[0m : 3.00233
[1mStep[0m  [25/53], [94mLoss[0m : 3.03900
[1mStep[0m  [30/53], [94mLoss[0m : 3.18977
[1mStep[0m  [35/53], [94mLoss[0m : 3.27439
[1mStep[0m  [40/53], [94mLoss[0m : 2.94877
[1mStep[0m  [45/53], [94mLoss[0m : 3.10929
[1mStep[0m  [50/53], [94mLoss[0m : 3.20980

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.086, [92mTest[0m: 3.115, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.02201
[1mStep[0m  [5/53], [94mLoss[0m : 3.14933
[1mStep[0m  [10/53], [94mLoss[0m : 3.20965
[1mStep[0m  [15/53], [94mLoss[0m : 2.89577
[1mStep[0m  [20/53], [94mLoss[0m : 2.84242
[1mStep[0m  [25/53], [94mLoss[0m : 2.74176
[1mStep[0m  [30/53], [94mLoss[0m : 3.24135
[1mStep[0m  [35/53], [94mLoss[0m : 2.98795
[1mStep[0m  [40/53], [94mLoss[0m : 3.06941
[1mStep[0m  [45/53], [94mLoss[0m : 2.93855
[1mStep[0m  [50/53], [94mLoss[0m : 2.69887

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.031, [92mTest[0m: 3.033, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.97725
[1mStep[0m  [5/53], [94mLoss[0m : 3.07862
[1mStep[0m  [10/53], [94mLoss[0m : 3.00004
[1mStep[0m  [15/53], [94mLoss[0m : 2.95285
[1mStep[0m  [20/53], [94mLoss[0m : 3.08456
[1mStep[0m  [25/53], [94mLoss[0m : 2.92924
[1mStep[0m  [30/53], [94mLoss[0m : 2.95932
[1mStep[0m  [35/53], [94mLoss[0m : 3.13875
[1mStep[0m  [40/53], [94mLoss[0m : 3.12691
[1mStep[0m  [45/53], [94mLoss[0m : 2.92047
[1mStep[0m  [50/53], [94mLoss[0m : 2.92157

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.975, [92mTest[0m: 2.973, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.83581
[1mStep[0m  [5/53], [94mLoss[0m : 3.02271
[1mStep[0m  [10/53], [94mLoss[0m : 3.12773
[1mStep[0m  [15/53], [94mLoss[0m : 2.84832
[1mStep[0m  [20/53], [94mLoss[0m : 3.12299
[1mStep[0m  [25/53], [94mLoss[0m : 2.76669
[1mStep[0m  [30/53], [94mLoss[0m : 2.81561
[1mStep[0m  [35/53], [94mLoss[0m : 2.84701
[1mStep[0m  [40/53], [94mLoss[0m : 2.91389
[1mStep[0m  [45/53], [94mLoss[0m : 3.16614
[1mStep[0m  [50/53], [94mLoss[0m : 3.03731

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.934, [92mTest[0m: 2.928, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.01132
[1mStep[0m  [5/53], [94mLoss[0m : 3.07028
[1mStep[0m  [10/53], [94mLoss[0m : 2.67041
[1mStep[0m  [15/53], [94mLoss[0m : 2.77245
[1mStep[0m  [20/53], [94mLoss[0m : 2.94435
[1mStep[0m  [25/53], [94mLoss[0m : 2.92468
[1mStep[0m  [30/53], [94mLoss[0m : 2.76502
[1mStep[0m  [35/53], [94mLoss[0m : 2.91427
[1mStep[0m  [40/53], [94mLoss[0m : 2.94363
[1mStep[0m  [45/53], [94mLoss[0m : 2.72025
[1mStep[0m  [50/53], [94mLoss[0m : 3.10714

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.896, [92mTest[0m: 2.882, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.91903
[1mStep[0m  [5/53], [94mLoss[0m : 2.97652
[1mStep[0m  [10/53], [94mLoss[0m : 3.23679
[1mStep[0m  [15/53], [94mLoss[0m : 2.76284
[1mStep[0m  [20/53], [94mLoss[0m : 2.62120
[1mStep[0m  [25/53], [94mLoss[0m : 2.83876
[1mStep[0m  [30/53], [94mLoss[0m : 3.04733
[1mStep[0m  [35/53], [94mLoss[0m : 2.64012
[1mStep[0m  [40/53], [94mLoss[0m : 2.68200
[1mStep[0m  [45/53], [94mLoss[0m : 2.86509
[1mStep[0m  [50/53], [94mLoss[0m : 2.80433

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.871, [92mTest[0m: 2.853, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.89951
[1mStep[0m  [5/53], [94mLoss[0m : 2.97548
[1mStep[0m  [10/53], [94mLoss[0m : 2.95286
[1mStep[0m  [15/53], [94mLoss[0m : 2.89371
[1mStep[0m  [20/53], [94mLoss[0m : 2.92896
[1mStep[0m  [25/53], [94mLoss[0m : 2.75517
[1mStep[0m  [30/53], [94mLoss[0m : 2.80969
[1mStep[0m  [35/53], [94mLoss[0m : 2.72118
[1mStep[0m  [40/53], [94mLoss[0m : 2.72888
[1mStep[0m  [45/53], [94mLoss[0m : 3.00280
[1mStep[0m  [50/53], [94mLoss[0m : 2.86259

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.838, [92mTest[0m: 2.821, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.89457
[1mStep[0m  [5/53], [94mLoss[0m : 2.56855
[1mStep[0m  [10/53], [94mLoss[0m : 2.92108
[1mStep[0m  [15/53], [94mLoss[0m : 2.74885
[1mStep[0m  [20/53], [94mLoss[0m : 2.63211
[1mStep[0m  [25/53], [94mLoss[0m : 2.88634
[1mStep[0m  [30/53], [94mLoss[0m : 2.78635
[1mStep[0m  [35/53], [94mLoss[0m : 2.88729
[1mStep[0m  [40/53], [94mLoss[0m : 2.76656
[1mStep[0m  [45/53], [94mLoss[0m : 2.96066
[1mStep[0m  [50/53], [94mLoss[0m : 2.88366

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.813, [92mTest[0m: 2.807, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71165
[1mStep[0m  [5/53], [94mLoss[0m : 2.82893
[1mStep[0m  [10/53], [94mLoss[0m : 2.78851
[1mStep[0m  [15/53], [94mLoss[0m : 2.62218
[1mStep[0m  [20/53], [94mLoss[0m : 3.00755
[1mStep[0m  [25/53], [94mLoss[0m : 2.69168
[1mStep[0m  [30/53], [94mLoss[0m : 2.88335
[1mStep[0m  [35/53], [94mLoss[0m : 2.83121
[1mStep[0m  [40/53], [94mLoss[0m : 2.74388
[1mStep[0m  [45/53], [94mLoss[0m : 3.09214
[1mStep[0m  [50/53], [94mLoss[0m : 2.55535

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.795, [92mTest[0m: 2.778, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.89900
[1mStep[0m  [5/53], [94mLoss[0m : 2.86724
[1mStep[0m  [10/53], [94mLoss[0m : 2.82269
[1mStep[0m  [15/53], [94mLoss[0m : 2.98030
[1mStep[0m  [20/53], [94mLoss[0m : 2.71575
[1mStep[0m  [25/53], [94mLoss[0m : 2.87236
[1mStep[0m  [30/53], [94mLoss[0m : 2.80755
[1mStep[0m  [35/53], [94mLoss[0m : 2.89437
[1mStep[0m  [40/53], [94mLoss[0m : 2.87764
[1mStep[0m  [45/53], [94mLoss[0m : 2.68119
[1mStep[0m  [50/53], [94mLoss[0m : 2.81777

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.785, [92mTest[0m: 2.761, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.63225
[1mStep[0m  [5/53], [94mLoss[0m : 2.88645
[1mStep[0m  [10/53], [94mLoss[0m : 3.03935
[1mStep[0m  [15/53], [94mLoss[0m : 2.66902
[1mStep[0m  [20/53], [94mLoss[0m : 2.85788
[1mStep[0m  [25/53], [94mLoss[0m : 2.91469
[1mStep[0m  [30/53], [94mLoss[0m : 2.85465
[1mStep[0m  [35/53], [94mLoss[0m : 2.79274
[1mStep[0m  [40/53], [94mLoss[0m : 2.81421
[1mStep[0m  [45/53], [94mLoss[0m : 2.85068
[1mStep[0m  [50/53], [94mLoss[0m : 2.65127

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.770, [92mTest[0m: 2.747, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.75438
[1mStep[0m  [5/53], [94mLoss[0m : 2.67462
[1mStep[0m  [10/53], [94mLoss[0m : 2.80186
[1mStep[0m  [15/53], [94mLoss[0m : 2.92487
[1mStep[0m  [20/53], [94mLoss[0m : 2.78160
[1mStep[0m  [25/53], [94mLoss[0m : 2.56994
[1mStep[0m  [30/53], [94mLoss[0m : 2.68786
[1mStep[0m  [35/53], [94mLoss[0m : 2.82958
[1mStep[0m  [40/53], [94mLoss[0m : 2.69868
[1mStep[0m  [45/53], [94mLoss[0m : 2.98293
[1mStep[0m  [50/53], [94mLoss[0m : 2.77303

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.756, [92mTest[0m: 2.719, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.70701
[1mStep[0m  [5/53], [94mLoss[0m : 2.67159
[1mStep[0m  [10/53], [94mLoss[0m : 2.60304
[1mStep[0m  [15/53], [94mLoss[0m : 2.87832
[1mStep[0m  [20/53], [94mLoss[0m : 2.61478
[1mStep[0m  [25/53], [94mLoss[0m : 2.72522
[1mStep[0m  [30/53], [94mLoss[0m : 2.78578
[1mStep[0m  [35/53], [94mLoss[0m : 2.59505
[1mStep[0m  [40/53], [94mLoss[0m : 2.72373
[1mStep[0m  [45/53], [94mLoss[0m : 2.69016
[1mStep[0m  [50/53], [94mLoss[0m : 2.70817

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.739, [92mTest[0m: 2.704, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.78510
[1mStep[0m  [5/53], [94mLoss[0m : 2.83726
[1mStep[0m  [10/53], [94mLoss[0m : 2.71541
[1mStep[0m  [15/53], [94mLoss[0m : 2.55541
[1mStep[0m  [20/53], [94mLoss[0m : 2.69354
[1mStep[0m  [25/53], [94mLoss[0m : 2.71319
[1mStep[0m  [30/53], [94mLoss[0m : 2.94639
[1mStep[0m  [35/53], [94mLoss[0m : 2.92277
[1mStep[0m  [40/53], [94mLoss[0m : 2.80814
[1mStep[0m  [45/53], [94mLoss[0m : 2.66394
[1mStep[0m  [50/53], [94mLoss[0m : 3.04964

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.726, [92mTest[0m: 2.704, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.683
====================================

Phase 1 - Evaluation MAE:  2.683393982740549
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 2.82504
[1mStep[0m  [5/53], [94mLoss[0m : 2.79719
[1mStep[0m  [10/53], [94mLoss[0m : 2.72602
[1mStep[0m  [15/53], [94mLoss[0m : 2.67630
[1mStep[0m  [20/53], [94mLoss[0m : 2.70401
[1mStep[0m  [25/53], [94mLoss[0m : 2.72124
[1mStep[0m  [30/53], [94mLoss[0m : 2.72486
[1mStep[0m  [35/53], [94mLoss[0m : 2.91848
[1mStep[0m  [40/53], [94mLoss[0m : 2.45618
[1mStep[0m  [45/53], [94mLoss[0m : 2.92158
[1mStep[0m  [50/53], [94mLoss[0m : 2.90772

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.702, [92mTest[0m: 2.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.76363
[1mStep[0m  [5/53], [94mLoss[0m : 2.67827
[1mStep[0m  [10/53], [94mLoss[0m : 2.61152
[1mStep[0m  [15/53], [94mLoss[0m : 2.57670
[1mStep[0m  [20/53], [94mLoss[0m : 2.70151
[1mStep[0m  [25/53], [94mLoss[0m : 2.74643
[1mStep[0m  [30/53], [94mLoss[0m : 2.67305
[1mStep[0m  [35/53], [94mLoss[0m : 2.64465
[1mStep[0m  [40/53], [94mLoss[0m : 2.30920
[1mStep[0m  [45/53], [94mLoss[0m : 2.54001
[1mStep[0m  [50/53], [94mLoss[0m : 2.67503

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.661, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61469
[1mStep[0m  [5/53], [94mLoss[0m : 2.58666
[1mStep[0m  [10/53], [94mLoss[0m : 2.48268
[1mStep[0m  [15/53], [94mLoss[0m : 2.61993
[1mStep[0m  [20/53], [94mLoss[0m : 2.84238
[1mStep[0m  [25/53], [94mLoss[0m : 2.80094
[1mStep[0m  [30/53], [94mLoss[0m : 2.64524
[1mStep[0m  [35/53], [94mLoss[0m : 2.64776
[1mStep[0m  [40/53], [94mLoss[0m : 2.70636
[1mStep[0m  [45/53], [94mLoss[0m : 2.56900
[1mStep[0m  [50/53], [94mLoss[0m : 2.75222

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.633, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67365
[1mStep[0m  [5/53], [94mLoss[0m : 2.76499
[1mStep[0m  [10/53], [94mLoss[0m : 2.46113
[1mStep[0m  [15/53], [94mLoss[0m : 2.52418
[1mStep[0m  [20/53], [94mLoss[0m : 2.37346
[1mStep[0m  [25/53], [94mLoss[0m : 2.67911
[1mStep[0m  [30/53], [94mLoss[0m : 2.74011
[1mStep[0m  [35/53], [94mLoss[0m : 2.60912
[1mStep[0m  [40/53], [94mLoss[0m : 2.48200
[1mStep[0m  [45/53], [94mLoss[0m : 2.59397
[1mStep[0m  [50/53], [94mLoss[0m : 2.60398

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.655, [92mTest[0m: 2.616, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.73810
[1mStep[0m  [5/53], [94mLoss[0m : 2.64082
[1mStep[0m  [10/53], [94mLoss[0m : 2.69781
[1mStep[0m  [15/53], [94mLoss[0m : 2.72721
[1mStep[0m  [20/53], [94mLoss[0m : 2.75139
[1mStep[0m  [25/53], [94mLoss[0m : 2.58307
[1mStep[0m  [30/53], [94mLoss[0m : 2.53831
[1mStep[0m  [35/53], [94mLoss[0m : 2.68998
[1mStep[0m  [40/53], [94mLoss[0m : 2.83046
[1mStep[0m  [45/53], [94mLoss[0m : 2.62835
[1mStep[0m  [50/53], [94mLoss[0m : 2.53117

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.637, [92mTest[0m: 2.607, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.51966
[1mStep[0m  [5/53], [94mLoss[0m : 2.63648
[1mStep[0m  [10/53], [94mLoss[0m : 2.90397
[1mStep[0m  [15/53], [94mLoss[0m : 2.73398
[1mStep[0m  [20/53], [94mLoss[0m : 2.47279
[1mStep[0m  [25/53], [94mLoss[0m : 2.85273
[1mStep[0m  [30/53], [94mLoss[0m : 2.59964
[1mStep[0m  [35/53], [94mLoss[0m : 2.72225
[1mStep[0m  [40/53], [94mLoss[0m : 2.67142
[1mStep[0m  [45/53], [94mLoss[0m : 2.56802
[1mStep[0m  [50/53], [94mLoss[0m : 2.57276

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.596, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67284
[1mStep[0m  [5/53], [94mLoss[0m : 2.68499
[1mStep[0m  [10/53], [94mLoss[0m : 2.58660
[1mStep[0m  [15/53], [94mLoss[0m : 2.38208
[1mStep[0m  [20/53], [94mLoss[0m : 2.79771
[1mStep[0m  [25/53], [94mLoss[0m : 2.60235
[1mStep[0m  [30/53], [94mLoss[0m : 2.35317
[1mStep[0m  [35/53], [94mLoss[0m : 2.64949
[1mStep[0m  [40/53], [94mLoss[0m : 2.89193
[1mStep[0m  [45/53], [94mLoss[0m : 2.71855
[1mStep[0m  [50/53], [94mLoss[0m : 2.39044

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.58928
[1mStep[0m  [5/53], [94mLoss[0m : 2.57274
[1mStep[0m  [10/53], [94mLoss[0m : 2.85823
[1mStep[0m  [15/53], [94mLoss[0m : 2.60442
[1mStep[0m  [20/53], [94mLoss[0m : 2.54121
[1mStep[0m  [25/53], [94mLoss[0m : 2.53521
[1mStep[0m  [30/53], [94mLoss[0m : 2.74310
[1mStep[0m  [35/53], [94mLoss[0m : 2.53322
[1mStep[0m  [40/53], [94mLoss[0m : 2.65173
[1mStep[0m  [45/53], [94mLoss[0m : 2.49289
[1mStep[0m  [50/53], [94mLoss[0m : 2.65760

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.578, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.92497
[1mStep[0m  [5/53], [94mLoss[0m : 2.51921
[1mStep[0m  [10/53], [94mLoss[0m : 2.86243
[1mStep[0m  [15/53], [94mLoss[0m : 2.67715
[1mStep[0m  [20/53], [94mLoss[0m : 2.57890
[1mStep[0m  [25/53], [94mLoss[0m : 2.42125
[1mStep[0m  [30/53], [94mLoss[0m : 2.60290
[1mStep[0m  [35/53], [94mLoss[0m : 2.68909
[1mStep[0m  [40/53], [94mLoss[0m : 2.53646
[1mStep[0m  [45/53], [94mLoss[0m : 2.42812
[1mStep[0m  [50/53], [94mLoss[0m : 2.49779

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.573, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.61240
[1mStep[0m  [5/53], [94mLoss[0m : 2.67997
[1mStep[0m  [10/53], [94mLoss[0m : 2.55564
[1mStep[0m  [15/53], [94mLoss[0m : 2.41790
[1mStep[0m  [20/53], [94mLoss[0m : 2.61022
[1mStep[0m  [25/53], [94mLoss[0m : 2.48899
[1mStep[0m  [30/53], [94mLoss[0m : 2.52624
[1mStep[0m  [35/53], [94mLoss[0m : 2.42451
[1mStep[0m  [40/53], [94mLoss[0m : 2.69816
[1mStep[0m  [45/53], [94mLoss[0m : 2.44638
[1mStep[0m  [50/53], [94mLoss[0m : 2.35282

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.559, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.86686
[1mStep[0m  [5/53], [94mLoss[0m : 2.64078
[1mStep[0m  [10/53], [94mLoss[0m : 2.82893
[1mStep[0m  [15/53], [94mLoss[0m : 2.67336
[1mStep[0m  [20/53], [94mLoss[0m : 2.52212
[1mStep[0m  [25/53], [94mLoss[0m : 2.72705
[1mStep[0m  [30/53], [94mLoss[0m : 2.53092
[1mStep[0m  [35/53], [94mLoss[0m : 2.59565
[1mStep[0m  [40/53], [94mLoss[0m : 2.64679
[1mStep[0m  [45/53], [94mLoss[0m : 2.55213
[1mStep[0m  [50/53], [94mLoss[0m : 2.53742

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.580, [92mTest[0m: 2.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.47468
[1mStep[0m  [5/53], [94mLoss[0m : 2.57149
[1mStep[0m  [10/53], [94mLoss[0m : 2.72930
[1mStep[0m  [15/53], [94mLoss[0m : 2.65230
[1mStep[0m  [20/53], [94mLoss[0m : 2.64384
[1mStep[0m  [25/53], [94mLoss[0m : 2.45026
[1mStep[0m  [30/53], [94mLoss[0m : 2.51763
[1mStep[0m  [35/53], [94mLoss[0m : 2.78077
[1mStep[0m  [40/53], [94mLoss[0m : 2.62842
[1mStep[0m  [45/53], [94mLoss[0m : 2.57779
[1mStep[0m  [50/53], [94mLoss[0m : 2.81863

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.548, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.56062
[1mStep[0m  [5/53], [94mLoss[0m : 2.52500
[1mStep[0m  [10/53], [94mLoss[0m : 2.42291
[1mStep[0m  [15/53], [94mLoss[0m : 2.58148
[1mStep[0m  [20/53], [94mLoss[0m : 2.36687
[1mStep[0m  [25/53], [94mLoss[0m : 2.39396
[1mStep[0m  [30/53], [94mLoss[0m : 2.35989
[1mStep[0m  [35/53], [94mLoss[0m : 2.50927
[1mStep[0m  [40/53], [94mLoss[0m : 2.52821
[1mStep[0m  [45/53], [94mLoss[0m : 2.47432
[1mStep[0m  [50/53], [94mLoss[0m : 2.70632

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.547, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.45897
[1mStep[0m  [5/53], [94mLoss[0m : 2.48474
[1mStep[0m  [10/53], [94mLoss[0m : 2.53451
[1mStep[0m  [15/53], [94mLoss[0m : 2.79311
[1mStep[0m  [20/53], [94mLoss[0m : 2.69258
[1mStep[0m  [25/53], [94mLoss[0m : 2.51240
[1mStep[0m  [30/53], [94mLoss[0m : 2.54891
[1mStep[0m  [35/53], [94mLoss[0m : 2.33544
[1mStep[0m  [40/53], [94mLoss[0m : 2.68779
[1mStep[0m  [45/53], [94mLoss[0m : 2.55546
[1mStep[0m  [50/53], [94mLoss[0m : 2.52626

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55759
[1mStep[0m  [5/53], [94mLoss[0m : 2.30222
[1mStep[0m  [10/53], [94mLoss[0m : 2.58969
[1mStep[0m  [15/53], [94mLoss[0m : 2.66469
[1mStep[0m  [20/53], [94mLoss[0m : 2.40283
[1mStep[0m  [25/53], [94mLoss[0m : 2.54168
[1mStep[0m  [30/53], [94mLoss[0m : 2.52384
[1mStep[0m  [35/53], [94mLoss[0m : 2.63527
[1mStep[0m  [40/53], [94mLoss[0m : 2.46879
[1mStep[0m  [45/53], [94mLoss[0m : 2.49075
[1mStep[0m  [50/53], [94mLoss[0m : 2.30523

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.525, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.43802
[1mStep[0m  [5/53], [94mLoss[0m : 2.51188
[1mStep[0m  [10/53], [94mLoss[0m : 2.41753
[1mStep[0m  [15/53], [94mLoss[0m : 2.61746
[1mStep[0m  [20/53], [94mLoss[0m : 2.43454
[1mStep[0m  [25/53], [94mLoss[0m : 2.59187
[1mStep[0m  [30/53], [94mLoss[0m : 2.56489
[1mStep[0m  [35/53], [94mLoss[0m : 2.45162
[1mStep[0m  [40/53], [94mLoss[0m : 2.50474
[1mStep[0m  [45/53], [94mLoss[0m : 2.45854
[1mStep[0m  [50/53], [94mLoss[0m : 2.49898

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37440
[1mStep[0m  [5/53], [94mLoss[0m : 2.47884
[1mStep[0m  [10/53], [94mLoss[0m : 2.56612
[1mStep[0m  [15/53], [94mLoss[0m : 2.31374
[1mStep[0m  [20/53], [94mLoss[0m : 2.59106
[1mStep[0m  [25/53], [94mLoss[0m : 2.30251
[1mStep[0m  [30/53], [94mLoss[0m : 2.35666
[1mStep[0m  [35/53], [94mLoss[0m : 2.81630
[1mStep[0m  [40/53], [94mLoss[0m : 2.59836
[1mStep[0m  [45/53], [94mLoss[0m : 2.65745
[1mStep[0m  [50/53], [94mLoss[0m : 2.36130

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.522, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.67024
[1mStep[0m  [5/53], [94mLoss[0m : 2.40114
[1mStep[0m  [10/53], [94mLoss[0m : 2.60051
[1mStep[0m  [15/53], [94mLoss[0m : 2.63606
[1mStep[0m  [20/53], [94mLoss[0m : 2.57138
[1mStep[0m  [25/53], [94mLoss[0m : 2.50906
[1mStep[0m  [30/53], [94mLoss[0m : 2.40970
[1mStep[0m  [35/53], [94mLoss[0m : 2.61748
[1mStep[0m  [40/53], [94mLoss[0m : 2.58717
[1mStep[0m  [45/53], [94mLoss[0m : 2.60261
[1mStep[0m  [50/53], [94mLoss[0m : 2.61460

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.515, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.54886
[1mStep[0m  [5/53], [94mLoss[0m : 2.56937
[1mStep[0m  [10/53], [94mLoss[0m : 2.43368
[1mStep[0m  [15/53], [94mLoss[0m : 2.53861
[1mStep[0m  [20/53], [94mLoss[0m : 2.71565
[1mStep[0m  [25/53], [94mLoss[0m : 2.39095
[1mStep[0m  [30/53], [94mLoss[0m : 2.43327
[1mStep[0m  [35/53], [94mLoss[0m : 2.50723
[1mStep[0m  [40/53], [94mLoss[0m : 2.52516
[1mStep[0m  [45/53], [94mLoss[0m : 2.67730
[1mStep[0m  [50/53], [94mLoss[0m : 2.52910

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.512, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.48741
[1mStep[0m  [5/53], [94mLoss[0m : 2.41933
[1mStep[0m  [10/53], [94mLoss[0m : 2.57272
[1mStep[0m  [15/53], [94mLoss[0m : 2.50287
[1mStep[0m  [20/53], [94mLoss[0m : 2.46514
[1mStep[0m  [25/53], [94mLoss[0m : 2.56741
[1mStep[0m  [30/53], [94mLoss[0m : 2.66246
[1mStep[0m  [35/53], [94mLoss[0m : 2.56771
[1mStep[0m  [40/53], [94mLoss[0m : 2.41057
[1mStep[0m  [45/53], [94mLoss[0m : 2.54400
[1mStep[0m  [50/53], [94mLoss[0m : 2.52962

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.512, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.42711
[1mStep[0m  [5/53], [94mLoss[0m : 2.59128
[1mStep[0m  [10/53], [94mLoss[0m : 2.48790
[1mStep[0m  [15/53], [94mLoss[0m : 2.47513
[1mStep[0m  [20/53], [94mLoss[0m : 2.61261
[1mStep[0m  [25/53], [94mLoss[0m : 2.54675
[1mStep[0m  [30/53], [94mLoss[0m : 2.37704
[1mStep[0m  [35/53], [94mLoss[0m : 2.60554
[1mStep[0m  [40/53], [94mLoss[0m : 2.73124
[1mStep[0m  [45/53], [94mLoss[0m : 2.43235
[1mStep[0m  [50/53], [94mLoss[0m : 2.44386

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.509, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.44435
[1mStep[0m  [5/53], [94mLoss[0m : 2.72058
[1mStep[0m  [10/53], [94mLoss[0m : 2.53289
[1mStep[0m  [15/53], [94mLoss[0m : 2.41245
[1mStep[0m  [20/53], [94mLoss[0m : 2.47093
[1mStep[0m  [25/53], [94mLoss[0m : 2.54669
[1mStep[0m  [30/53], [94mLoss[0m : 2.58850
[1mStep[0m  [35/53], [94mLoss[0m : 2.48988
[1mStep[0m  [40/53], [94mLoss[0m : 2.58780
[1mStep[0m  [45/53], [94mLoss[0m : 2.55222
[1mStep[0m  [50/53], [94mLoss[0m : 2.38545

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.506, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.36255
[1mStep[0m  [5/53], [94mLoss[0m : 2.52828
[1mStep[0m  [10/53], [94mLoss[0m : 2.43801
[1mStep[0m  [15/53], [94mLoss[0m : 2.56210
[1mStep[0m  [20/53], [94mLoss[0m : 2.43494
[1mStep[0m  [25/53], [94mLoss[0m : 2.47246
[1mStep[0m  [30/53], [94mLoss[0m : 2.39647
[1mStep[0m  [35/53], [94mLoss[0m : 2.69721
[1mStep[0m  [40/53], [94mLoss[0m : 2.26840
[1mStep[0m  [45/53], [94mLoss[0m : 2.48648
[1mStep[0m  [50/53], [94mLoss[0m : 2.47056

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.499, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.60794
[1mStep[0m  [5/53], [94mLoss[0m : 2.56613
[1mStep[0m  [10/53], [94mLoss[0m : 2.63972
[1mStep[0m  [15/53], [94mLoss[0m : 2.62930
[1mStep[0m  [20/53], [94mLoss[0m : 2.37188
[1mStep[0m  [25/53], [94mLoss[0m : 2.56595
[1mStep[0m  [30/53], [94mLoss[0m : 2.44000
[1mStep[0m  [35/53], [94mLoss[0m : 2.70722
[1mStep[0m  [40/53], [94mLoss[0m : 2.46691
[1mStep[0m  [45/53], [94mLoss[0m : 2.50019
[1mStep[0m  [50/53], [94mLoss[0m : 2.30944

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.510, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.53076
[1mStep[0m  [5/53], [94mLoss[0m : 2.54219
[1mStep[0m  [10/53], [94mLoss[0m : 2.33278
[1mStep[0m  [15/53], [94mLoss[0m : 2.38277
[1mStep[0m  [20/53], [94mLoss[0m : 2.55891
[1mStep[0m  [25/53], [94mLoss[0m : 2.36810
[1mStep[0m  [30/53], [94mLoss[0m : 2.64373
[1mStep[0m  [35/53], [94mLoss[0m : 2.62122
[1mStep[0m  [40/53], [94mLoss[0m : 2.49725
[1mStep[0m  [45/53], [94mLoss[0m : 2.61104
[1mStep[0m  [50/53], [94mLoss[0m : 2.67605

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.491, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.41009
[1mStep[0m  [5/53], [94mLoss[0m : 2.59983
[1mStep[0m  [10/53], [94mLoss[0m : 2.75749
[1mStep[0m  [15/53], [94mLoss[0m : 2.28541
[1mStep[0m  [20/53], [94mLoss[0m : 2.34339
[1mStep[0m  [25/53], [94mLoss[0m : 2.41174
[1mStep[0m  [30/53], [94mLoss[0m : 2.41031
[1mStep[0m  [35/53], [94mLoss[0m : 2.49300
[1mStep[0m  [40/53], [94mLoss[0m : 2.57721
[1mStep[0m  [45/53], [94mLoss[0m : 2.60494
[1mStep[0m  [50/53], [94mLoss[0m : 2.56214

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.497, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.55559
[1mStep[0m  [5/53], [94mLoss[0m : 2.41593
[1mStep[0m  [10/53], [94mLoss[0m : 2.46645
[1mStep[0m  [15/53], [94mLoss[0m : 2.51116
[1mStep[0m  [20/53], [94mLoss[0m : 2.29919
[1mStep[0m  [25/53], [94mLoss[0m : 2.44112
[1mStep[0m  [30/53], [94mLoss[0m : 2.56843
[1mStep[0m  [35/53], [94mLoss[0m : 2.51203
[1mStep[0m  [40/53], [94mLoss[0m : 2.48567
[1mStep[0m  [45/53], [94mLoss[0m : 2.30160
[1mStep[0m  [50/53], [94mLoss[0m : 2.49664

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.489, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.40385
[1mStep[0m  [5/53], [94mLoss[0m : 2.45476
[1mStep[0m  [10/53], [94mLoss[0m : 2.37859
[1mStep[0m  [15/53], [94mLoss[0m : 2.64719
[1mStep[0m  [20/53], [94mLoss[0m : 2.69476
[1mStep[0m  [25/53], [94mLoss[0m : 2.50799
[1mStep[0m  [30/53], [94mLoss[0m : 2.50888
[1mStep[0m  [35/53], [94mLoss[0m : 2.61384
[1mStep[0m  [40/53], [94mLoss[0m : 2.74988
[1mStep[0m  [45/53], [94mLoss[0m : 2.59892
[1mStep[0m  [50/53], [94mLoss[0m : 2.62288

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.488, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.71803
[1mStep[0m  [5/53], [94mLoss[0m : 2.57581
[1mStep[0m  [10/53], [94mLoss[0m : 2.57559
[1mStep[0m  [15/53], [94mLoss[0m : 2.65335
[1mStep[0m  [20/53], [94mLoss[0m : 2.56884
[1mStep[0m  [25/53], [94mLoss[0m : 2.51861
[1mStep[0m  [30/53], [94mLoss[0m : 2.64434
[1mStep[0m  [35/53], [94mLoss[0m : 2.50922
[1mStep[0m  [40/53], [94mLoss[0m : 2.32334
[1mStep[0m  [45/53], [94mLoss[0m : 2.38988
[1mStep[0m  [50/53], [94mLoss[0m : 2.52609

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.495, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 2.37043
[1mStep[0m  [5/53], [94mLoss[0m : 2.30430
[1mStep[0m  [10/53], [94mLoss[0m : 2.53767
[1mStep[0m  [15/53], [94mLoss[0m : 2.48193
[1mStep[0m  [20/53], [94mLoss[0m : 2.40001
[1mStep[0m  [25/53], [94mLoss[0m : 2.56388
[1mStep[0m  [30/53], [94mLoss[0m : 2.46055
[1mStep[0m  [35/53], [94mLoss[0m : 2.58143
[1mStep[0m  [40/53], [94mLoss[0m : 2.47186
[1mStep[0m  [45/53], [94mLoss[0m : 2.55117
[1mStep[0m  [50/53], [94mLoss[0m : 2.50436

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.484, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.480
====================================

Phase 2 - Evaluation MAE:  2.4800187165920553
MAE score P1      2.683394
MAE score P2      2.480019
loss              2.501788
learning_rate       0.0001
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.95094
[1mStep[0m  [5/53], [94mLoss[0m : 11.01551
[1mStep[0m  [10/53], [94mLoss[0m : 10.76454
[1mStep[0m  [15/53], [94mLoss[0m : 10.54212
[1mStep[0m  [20/53], [94mLoss[0m : 10.58640
[1mStep[0m  [25/53], [94mLoss[0m : 10.54465
[1mStep[0m  [30/53], [94mLoss[0m : 10.84000
[1mStep[0m  [35/53], [94mLoss[0m : 10.66281
[1mStep[0m  [40/53], [94mLoss[0m : 11.03231
[1mStep[0m  [45/53], [94mLoss[0m : 10.47124
[1mStep[0m  [50/53], [94mLoss[0m : 10.92760

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.745, [92mTest[0m: 10.792, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.61515
[1mStep[0m  [5/53], [94mLoss[0m : 10.51504
[1mStep[0m  [10/53], [94mLoss[0m : 10.87171
[1mStep[0m  [15/53], [94mLoss[0m : 10.34830
[1mStep[0m  [20/53], [94mLoss[0m : 10.83804
[1mStep[0m  [25/53], [94mLoss[0m : 10.36920
[1mStep[0m  [30/53], [94mLoss[0m : 10.77466
[1mStep[0m  [35/53], [94mLoss[0m : 10.55615
[1mStep[0m  [40/53], [94mLoss[0m : 10.60763
[1mStep[0m  [45/53], [94mLoss[0m : 10.55808
[1mStep[0m  [50/53], [94mLoss[0m : 10.83646

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.664, [92mTest[0m: 10.696, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.48221
[1mStep[0m  [5/53], [94mLoss[0m : 11.30122
[1mStep[0m  [10/53], [94mLoss[0m : 10.39336
[1mStep[0m  [15/53], [94mLoss[0m : 10.39589
[1mStep[0m  [20/53], [94mLoss[0m : 10.65089
[1mStep[0m  [25/53], [94mLoss[0m : 10.70598
[1mStep[0m  [30/53], [94mLoss[0m : 10.45076
[1mStep[0m  [35/53], [94mLoss[0m : 10.35147
[1mStep[0m  [40/53], [94mLoss[0m : 10.88933
[1mStep[0m  [45/53], [94mLoss[0m : 10.44092
[1mStep[0m  [50/53], [94mLoss[0m : 10.78547

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.583, [92mTest[0m: 10.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.18663
[1mStep[0m  [5/53], [94mLoss[0m : 10.38151
[1mStep[0m  [10/53], [94mLoss[0m : 10.80709
[1mStep[0m  [15/53], [94mLoss[0m : 10.42680
[1mStep[0m  [20/53], [94mLoss[0m : 10.67445
[1mStep[0m  [25/53], [94mLoss[0m : 10.27171
[1mStep[0m  [30/53], [94mLoss[0m : 10.48577
[1mStep[0m  [35/53], [94mLoss[0m : 10.47407
[1mStep[0m  [40/53], [94mLoss[0m : 11.01236
[1mStep[0m  [45/53], [94mLoss[0m : 10.44642
[1mStep[0m  [50/53], [94mLoss[0m : 10.52451

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.504, [92mTest[0m: 10.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.42712
[1mStep[0m  [5/53], [94mLoss[0m : 10.46137
[1mStep[0m  [10/53], [94mLoss[0m : 10.61917
[1mStep[0m  [15/53], [94mLoss[0m : 10.27323
[1mStep[0m  [20/53], [94mLoss[0m : 10.24130
[1mStep[0m  [25/53], [94mLoss[0m : 10.09167
[1mStep[0m  [30/53], [94mLoss[0m : 10.24989
[1mStep[0m  [35/53], [94mLoss[0m : 10.71669
[1mStep[0m  [40/53], [94mLoss[0m : 10.45698
[1mStep[0m  [45/53], [94mLoss[0m : 10.69776
[1mStep[0m  [50/53], [94mLoss[0m : 10.66691

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.431, [92mTest[0m: 10.476, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.11274
[1mStep[0m  [5/53], [94mLoss[0m : 10.13855
[1mStep[0m  [10/53], [94mLoss[0m : 10.26944
[1mStep[0m  [15/53], [94mLoss[0m : 10.73128
[1mStep[0m  [20/53], [94mLoss[0m : 10.08418
[1mStep[0m  [25/53], [94mLoss[0m : 10.06326
[1mStep[0m  [30/53], [94mLoss[0m : 10.53187
[1mStep[0m  [35/53], [94mLoss[0m : 10.50318
[1mStep[0m  [40/53], [94mLoss[0m : 10.31085
[1mStep[0m  [45/53], [94mLoss[0m : 10.38979
[1mStep[0m  [50/53], [94mLoss[0m : 10.40386

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.344, [92mTest[0m: 10.374, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.41948
[1mStep[0m  [5/53], [94mLoss[0m : 10.50277
[1mStep[0m  [10/53], [94mLoss[0m : 10.45622
[1mStep[0m  [15/53], [94mLoss[0m : 10.66843
[1mStep[0m  [20/53], [94mLoss[0m : 10.21789
[1mStep[0m  [25/53], [94mLoss[0m : 10.24118
[1mStep[0m  [30/53], [94mLoss[0m : 10.42826
[1mStep[0m  [35/53], [94mLoss[0m : 10.05035
[1mStep[0m  [40/53], [94mLoss[0m : 9.96431
[1mStep[0m  [45/53], [94mLoss[0m : 10.64395
[1mStep[0m  [50/53], [94mLoss[0m : 10.08947

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.272, [92mTest[0m: 10.301, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.42455
[1mStep[0m  [5/53], [94mLoss[0m : 10.18445
[1mStep[0m  [10/53], [94mLoss[0m : 10.23333
[1mStep[0m  [15/53], [94mLoss[0m : 10.29635
[1mStep[0m  [20/53], [94mLoss[0m : 10.54458
[1mStep[0m  [25/53], [94mLoss[0m : 9.98519
[1mStep[0m  [30/53], [94mLoss[0m : 10.32800
[1mStep[0m  [35/53], [94mLoss[0m : 10.35701
[1mStep[0m  [40/53], [94mLoss[0m : 9.99598
[1mStep[0m  [45/53], [94mLoss[0m : 9.98977
[1mStep[0m  [50/53], [94mLoss[0m : 10.19055

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.185, [92mTest[0m: 10.213, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.57909
[1mStep[0m  [5/53], [94mLoss[0m : 10.24084
[1mStep[0m  [10/53], [94mLoss[0m : 10.10337
[1mStep[0m  [15/53], [94mLoss[0m : 10.10358
[1mStep[0m  [20/53], [94mLoss[0m : 10.26955
[1mStep[0m  [25/53], [94mLoss[0m : 10.12776
[1mStep[0m  [30/53], [94mLoss[0m : 9.77468
[1mStep[0m  [35/53], [94mLoss[0m : 10.19823
[1mStep[0m  [40/53], [94mLoss[0m : 9.92218
[1mStep[0m  [45/53], [94mLoss[0m : 10.21550
[1mStep[0m  [50/53], [94mLoss[0m : 9.93755

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.108, [92mTest[0m: 10.145, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.69619
[1mStep[0m  [5/53], [94mLoss[0m : 10.14660
[1mStep[0m  [10/53], [94mLoss[0m : 10.09562
[1mStep[0m  [15/53], [94mLoss[0m : 9.72863
[1mStep[0m  [20/53], [94mLoss[0m : 10.15225
[1mStep[0m  [25/53], [94mLoss[0m : 10.08822
[1mStep[0m  [30/53], [94mLoss[0m : 9.72279
[1mStep[0m  [35/53], [94mLoss[0m : 9.94951
[1mStep[0m  [40/53], [94mLoss[0m : 9.54083
[1mStep[0m  [45/53], [94mLoss[0m : 10.24916
[1mStep[0m  [50/53], [94mLoss[0m : 10.14460

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.025, [92mTest[0m: 10.070, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.86927
[1mStep[0m  [5/53], [94mLoss[0m : 9.83040
[1mStep[0m  [10/53], [94mLoss[0m : 10.13608
[1mStep[0m  [15/53], [94mLoss[0m : 9.82670
[1mStep[0m  [20/53], [94mLoss[0m : 9.73764
[1mStep[0m  [25/53], [94mLoss[0m : 9.67545
[1mStep[0m  [30/53], [94mLoss[0m : 9.90535
[1mStep[0m  [35/53], [94mLoss[0m : 10.30161
[1mStep[0m  [40/53], [94mLoss[0m : 9.84209
[1mStep[0m  [45/53], [94mLoss[0m : 10.15282
[1mStep[0m  [50/53], [94mLoss[0m : 9.75726

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.949, [92mTest[0m: 9.983, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.08749
[1mStep[0m  [5/53], [94mLoss[0m : 9.93598
[1mStep[0m  [10/53], [94mLoss[0m : 10.06685
[1mStep[0m  [15/53], [94mLoss[0m : 10.15879
[1mStep[0m  [20/53], [94mLoss[0m : 9.82529
[1mStep[0m  [25/53], [94mLoss[0m : 9.86380
[1mStep[0m  [30/53], [94mLoss[0m : 9.95233
[1mStep[0m  [35/53], [94mLoss[0m : 10.05407
[1mStep[0m  [40/53], [94mLoss[0m : 9.61551
[1mStep[0m  [45/53], [94mLoss[0m : 9.94914
[1mStep[0m  [50/53], [94mLoss[0m : 9.96686

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.866, [92mTest[0m: 9.916, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.15786
[1mStep[0m  [5/53], [94mLoss[0m : 9.70646
[1mStep[0m  [10/53], [94mLoss[0m : 10.11272
[1mStep[0m  [15/53], [94mLoss[0m : 9.64327
[1mStep[0m  [20/53], [94mLoss[0m : 9.92128
[1mStep[0m  [25/53], [94mLoss[0m : 9.65425
[1mStep[0m  [30/53], [94mLoss[0m : 9.98207
[1mStep[0m  [35/53], [94mLoss[0m : 10.18862
[1mStep[0m  [40/53], [94mLoss[0m : 9.52500
[1mStep[0m  [45/53], [94mLoss[0m : 9.69324
[1mStep[0m  [50/53], [94mLoss[0m : 9.34270

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.794, [92mTest[0m: 9.834, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.58052
[1mStep[0m  [5/53], [94mLoss[0m : 9.85443
[1mStep[0m  [10/53], [94mLoss[0m : 9.88503
[1mStep[0m  [15/53], [94mLoss[0m : 9.98924
[1mStep[0m  [20/53], [94mLoss[0m : 9.82061
[1mStep[0m  [25/53], [94mLoss[0m : 9.36840
[1mStep[0m  [30/53], [94mLoss[0m : 9.82429
[1mStep[0m  [35/53], [94mLoss[0m : 9.82499
[1mStep[0m  [40/53], [94mLoss[0m : 9.46134
[1mStep[0m  [45/53], [94mLoss[0m : 9.54501
[1mStep[0m  [50/53], [94mLoss[0m : 9.57664

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.709, [92mTest[0m: 9.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.28438
[1mStep[0m  [5/53], [94mLoss[0m : 9.70315
[1mStep[0m  [10/53], [94mLoss[0m : 9.66823
[1mStep[0m  [15/53], [94mLoss[0m : 9.86489
[1mStep[0m  [20/53], [94mLoss[0m : 9.77497
[1mStep[0m  [25/53], [94mLoss[0m : 9.50002
[1mStep[0m  [30/53], [94mLoss[0m : 9.16948
[1mStep[0m  [35/53], [94mLoss[0m : 9.29968
[1mStep[0m  [40/53], [94mLoss[0m : 9.38879
[1mStep[0m  [45/53], [94mLoss[0m : 9.34138
[1mStep[0m  [50/53], [94mLoss[0m : 9.88055

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.626, [92mTest[0m: 9.677, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.96479
[1mStep[0m  [5/53], [94mLoss[0m : 9.48985
[1mStep[0m  [10/53], [94mLoss[0m : 9.88811
[1mStep[0m  [15/53], [94mLoss[0m : 9.80084
[1mStep[0m  [20/53], [94mLoss[0m : 9.64970
[1mStep[0m  [25/53], [94mLoss[0m : 9.24677
[1mStep[0m  [30/53], [94mLoss[0m : 9.59801
[1mStep[0m  [35/53], [94mLoss[0m : 9.29474
[1mStep[0m  [40/53], [94mLoss[0m : 9.58484
[1mStep[0m  [45/53], [94mLoss[0m : 9.32710
[1mStep[0m  [50/53], [94mLoss[0m : 9.46290

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.543, [92mTest[0m: 9.571, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.94787
[1mStep[0m  [5/53], [94mLoss[0m : 9.53947
[1mStep[0m  [10/53], [94mLoss[0m : 9.29197
[1mStep[0m  [15/53], [94mLoss[0m : 9.33108
[1mStep[0m  [20/53], [94mLoss[0m : 9.45602
[1mStep[0m  [25/53], [94mLoss[0m : 9.79087
[1mStep[0m  [30/53], [94mLoss[0m : 9.62186
[1mStep[0m  [35/53], [94mLoss[0m : 9.95091
[1mStep[0m  [40/53], [94mLoss[0m : 9.32024
[1mStep[0m  [45/53], [94mLoss[0m : 9.57491
[1mStep[0m  [50/53], [94mLoss[0m : 9.13980

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.473, [92mTest[0m: 9.512, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.51430
[1mStep[0m  [5/53], [94mLoss[0m : 9.52094
[1mStep[0m  [10/53], [94mLoss[0m : 9.45920
[1mStep[0m  [15/53], [94mLoss[0m : 9.42972
[1mStep[0m  [20/53], [94mLoss[0m : 9.22463
[1mStep[0m  [25/53], [94mLoss[0m : 9.21005
[1mStep[0m  [30/53], [94mLoss[0m : 9.47743
[1mStep[0m  [35/53], [94mLoss[0m : 8.95528
[1mStep[0m  [40/53], [94mLoss[0m : 9.71163
[1mStep[0m  [45/53], [94mLoss[0m : 9.57211
[1mStep[0m  [50/53], [94mLoss[0m : 9.72967

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.389, [92mTest[0m: 9.422, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.65294
[1mStep[0m  [5/53], [94mLoss[0m : 9.33111
[1mStep[0m  [10/53], [94mLoss[0m : 9.47921
[1mStep[0m  [15/53], [94mLoss[0m : 9.03155
[1mStep[0m  [20/53], [94mLoss[0m : 9.04776
[1mStep[0m  [25/53], [94mLoss[0m : 9.50794
[1mStep[0m  [30/53], [94mLoss[0m : 9.10573
[1mStep[0m  [35/53], [94mLoss[0m : 9.43018
[1mStep[0m  [40/53], [94mLoss[0m : 8.82350
[1mStep[0m  [45/53], [94mLoss[0m : 9.26298
[1mStep[0m  [50/53], [94mLoss[0m : 9.22173

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.307, [92mTest[0m: 9.349, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.53030
[1mStep[0m  [5/53], [94mLoss[0m : 9.43188
[1mStep[0m  [10/53], [94mLoss[0m : 9.51635
[1mStep[0m  [15/53], [94mLoss[0m : 9.09601
[1mStep[0m  [20/53], [94mLoss[0m : 9.43851
[1mStep[0m  [25/53], [94mLoss[0m : 9.09282
[1mStep[0m  [30/53], [94mLoss[0m : 8.88293
[1mStep[0m  [35/53], [94mLoss[0m : 9.18935
[1mStep[0m  [40/53], [94mLoss[0m : 9.44384
[1mStep[0m  [45/53], [94mLoss[0m : 9.34107
[1mStep[0m  [50/53], [94mLoss[0m : 9.01221

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.230, [92mTest[0m: 9.275, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.36315
[1mStep[0m  [5/53], [94mLoss[0m : 9.38106
[1mStep[0m  [10/53], [94mLoss[0m : 9.04549
[1mStep[0m  [15/53], [94mLoss[0m : 8.90753
[1mStep[0m  [20/53], [94mLoss[0m : 9.32556
[1mStep[0m  [25/53], [94mLoss[0m : 9.22989
[1mStep[0m  [30/53], [94mLoss[0m : 9.14976
[1mStep[0m  [35/53], [94mLoss[0m : 9.14477
[1mStep[0m  [40/53], [94mLoss[0m : 9.24089
[1mStep[0m  [45/53], [94mLoss[0m : 9.24591
[1mStep[0m  [50/53], [94mLoss[0m : 9.15891

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.152, [92mTest[0m: 9.194, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.31024
[1mStep[0m  [5/53], [94mLoss[0m : 8.79799
[1mStep[0m  [10/53], [94mLoss[0m : 9.14314
[1mStep[0m  [15/53], [94mLoss[0m : 9.24220
[1mStep[0m  [20/53], [94mLoss[0m : 9.00118
[1mStep[0m  [25/53], [94mLoss[0m : 9.14472
[1mStep[0m  [30/53], [94mLoss[0m : 8.73655
[1mStep[0m  [35/53], [94mLoss[0m : 9.15890
[1mStep[0m  [40/53], [94mLoss[0m : 8.74939
[1mStep[0m  [45/53], [94mLoss[0m : 9.37727
[1mStep[0m  [50/53], [94mLoss[0m : 9.15202

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.079, [92mTest[0m: 9.120, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.66249
[1mStep[0m  [5/53], [94mLoss[0m : 8.79709
[1mStep[0m  [10/53], [94mLoss[0m : 8.98369
[1mStep[0m  [15/53], [94mLoss[0m : 8.63649
[1mStep[0m  [20/53], [94mLoss[0m : 9.37678
[1mStep[0m  [25/53], [94mLoss[0m : 9.00475
[1mStep[0m  [30/53], [94mLoss[0m : 8.99554
[1mStep[0m  [35/53], [94mLoss[0m : 8.83001
[1mStep[0m  [40/53], [94mLoss[0m : 9.20715
[1mStep[0m  [45/53], [94mLoss[0m : 8.99329
[1mStep[0m  [50/53], [94mLoss[0m : 8.82911

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.008, [92mTest[0m: 9.053, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.33978
[1mStep[0m  [5/53], [94mLoss[0m : 8.65432
[1mStep[0m  [10/53], [94mLoss[0m : 8.94288
[1mStep[0m  [15/53], [94mLoss[0m : 9.27058
[1mStep[0m  [20/53], [94mLoss[0m : 9.31591
[1mStep[0m  [25/53], [94mLoss[0m : 9.20586
[1mStep[0m  [30/53], [94mLoss[0m : 8.90263
[1mStep[0m  [35/53], [94mLoss[0m : 8.95092
[1mStep[0m  [40/53], [94mLoss[0m : 9.09051
[1mStep[0m  [45/53], [94mLoss[0m : 9.39305
[1mStep[0m  [50/53], [94mLoss[0m : 8.39173

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.941, [92mTest[0m: 8.978, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.84055
[1mStep[0m  [5/53], [94mLoss[0m : 8.85581
[1mStep[0m  [10/53], [94mLoss[0m : 8.99850
[1mStep[0m  [15/53], [94mLoss[0m : 9.18714
[1mStep[0m  [20/53], [94mLoss[0m : 8.87329
[1mStep[0m  [25/53], [94mLoss[0m : 8.91521
[1mStep[0m  [30/53], [94mLoss[0m : 8.82623
[1mStep[0m  [35/53], [94mLoss[0m : 8.94275
[1mStep[0m  [40/53], [94mLoss[0m : 8.69981
[1mStep[0m  [45/53], [94mLoss[0m : 8.88239
[1mStep[0m  [50/53], [94mLoss[0m : 8.76191

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.871, [92mTest[0m: 8.914, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.02525
[1mStep[0m  [5/53], [94mLoss[0m : 9.10114
[1mStep[0m  [10/53], [94mLoss[0m : 8.91243
[1mStep[0m  [15/53], [94mLoss[0m : 8.61695
[1mStep[0m  [20/53], [94mLoss[0m : 8.98060
[1mStep[0m  [25/53], [94mLoss[0m : 8.55155
[1mStep[0m  [30/53], [94mLoss[0m : 8.68419
[1mStep[0m  [35/53], [94mLoss[0m : 8.50460
[1mStep[0m  [40/53], [94mLoss[0m : 8.79059
[1mStep[0m  [45/53], [94mLoss[0m : 8.67802
[1mStep[0m  [50/53], [94mLoss[0m : 8.98550

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.796, [92mTest[0m: 8.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.81285
[1mStep[0m  [5/53], [94mLoss[0m : 9.06778
[1mStep[0m  [10/53], [94mLoss[0m : 8.30355
[1mStep[0m  [15/53], [94mLoss[0m : 8.63353
[1mStep[0m  [20/53], [94mLoss[0m : 8.46875
[1mStep[0m  [25/53], [94mLoss[0m : 8.79721
[1mStep[0m  [30/53], [94mLoss[0m : 8.82775
[1mStep[0m  [35/53], [94mLoss[0m : 8.73099
[1mStep[0m  [40/53], [94mLoss[0m : 8.41457
[1mStep[0m  [45/53], [94mLoss[0m : 8.43783
[1mStep[0m  [50/53], [94mLoss[0m : 8.91267

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.730, [92mTest[0m: 8.759, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.69497
[1mStep[0m  [5/53], [94mLoss[0m : 8.59992
[1mStep[0m  [10/53], [94mLoss[0m : 8.67445
[1mStep[0m  [15/53], [94mLoss[0m : 8.58384
[1mStep[0m  [20/53], [94mLoss[0m : 8.69045
[1mStep[0m  [25/53], [94mLoss[0m : 8.64095
[1mStep[0m  [30/53], [94mLoss[0m : 9.22624
[1mStep[0m  [35/53], [94mLoss[0m : 8.95540
[1mStep[0m  [40/53], [94mLoss[0m : 8.43917
[1mStep[0m  [45/53], [94mLoss[0m : 8.64408
[1mStep[0m  [50/53], [94mLoss[0m : 8.44336

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.658, [92mTest[0m: 8.690, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.39084
[1mStep[0m  [5/53], [94mLoss[0m : 8.44579
[1mStep[0m  [10/53], [94mLoss[0m : 8.68156
[1mStep[0m  [15/53], [94mLoss[0m : 8.95576
[1mStep[0m  [20/53], [94mLoss[0m : 8.98628
[1mStep[0m  [25/53], [94mLoss[0m : 8.98509
[1mStep[0m  [30/53], [94mLoss[0m : 8.40541
[1mStep[0m  [35/53], [94mLoss[0m : 8.58513
[1mStep[0m  [40/53], [94mLoss[0m : 8.48349
[1mStep[0m  [45/53], [94mLoss[0m : 8.66903
[1mStep[0m  [50/53], [94mLoss[0m : 8.33419

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.579, [92mTest[0m: 8.615, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.80168
[1mStep[0m  [5/53], [94mLoss[0m : 8.18085
[1mStep[0m  [10/53], [94mLoss[0m : 8.59025
[1mStep[0m  [15/53], [94mLoss[0m : 8.13735
[1mStep[0m  [20/53], [94mLoss[0m : 8.35291
[1mStep[0m  [25/53], [94mLoss[0m : 8.74019
[1mStep[0m  [30/53], [94mLoss[0m : 7.96205
[1mStep[0m  [35/53], [94mLoss[0m : 8.48692
[1mStep[0m  [40/53], [94mLoss[0m : 8.62159
[1mStep[0m  [45/53], [94mLoss[0m : 8.40820
[1mStep[0m  [50/53], [94mLoss[0m : 8.18729

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.508, [92mTest[0m: 8.549, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.476
====================================

Phase 1 - Evaluation MAE:  8.475735994485708
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 8.31628
[1mStep[0m  [5/53], [94mLoss[0m : 8.07407
[1mStep[0m  [10/53], [94mLoss[0m : 8.78975
[1mStep[0m  [15/53], [94mLoss[0m : 8.49374
[1mStep[0m  [20/53], [94mLoss[0m : 8.88169
[1mStep[0m  [25/53], [94mLoss[0m : 8.31019
[1mStep[0m  [30/53], [94mLoss[0m : 8.52433
[1mStep[0m  [35/53], [94mLoss[0m : 8.79785
[1mStep[0m  [40/53], [94mLoss[0m : 8.82837
[1mStep[0m  [45/53], [94mLoss[0m : 8.29472
[1mStep[0m  [50/53], [94mLoss[0m : 8.41700

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.434, [92mTest[0m: 8.478, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.40319
[1mStep[0m  [5/53], [94mLoss[0m : 8.14702
[1mStep[0m  [10/53], [94mLoss[0m : 8.51581
[1mStep[0m  [15/53], [94mLoss[0m : 8.49100
[1mStep[0m  [20/53], [94mLoss[0m : 8.31440
[1mStep[0m  [25/53], [94mLoss[0m : 8.32539
[1mStep[0m  [30/53], [94mLoss[0m : 8.66160
[1mStep[0m  [35/53], [94mLoss[0m : 8.24544
[1mStep[0m  [40/53], [94mLoss[0m : 8.03199
[1mStep[0m  [45/53], [94mLoss[0m : 8.25960
[1mStep[0m  [50/53], [94mLoss[0m : 8.05131

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.350, [92mTest[0m: 8.397, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.54525
[1mStep[0m  [5/53], [94mLoss[0m : 8.35648
[1mStep[0m  [10/53], [94mLoss[0m : 8.05983
[1mStep[0m  [15/53], [94mLoss[0m : 8.05367
[1mStep[0m  [20/53], [94mLoss[0m : 8.29391
[1mStep[0m  [25/53], [94mLoss[0m : 8.08196
[1mStep[0m  [30/53], [94mLoss[0m : 8.53252
[1mStep[0m  [35/53], [94mLoss[0m : 8.27257
[1mStep[0m  [40/53], [94mLoss[0m : 8.20286
[1mStep[0m  [45/53], [94mLoss[0m : 8.44375
[1mStep[0m  [50/53], [94mLoss[0m : 8.18185

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.279, [92mTest[0m: 8.329, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.01979
[1mStep[0m  [5/53], [94mLoss[0m : 8.52972
[1mStep[0m  [10/53], [94mLoss[0m : 7.95012
[1mStep[0m  [15/53], [94mLoss[0m : 7.85919
[1mStep[0m  [20/53], [94mLoss[0m : 8.06701
[1mStep[0m  [25/53], [94mLoss[0m : 8.56740
[1mStep[0m  [30/53], [94mLoss[0m : 8.08932
[1mStep[0m  [35/53], [94mLoss[0m : 8.61850
[1mStep[0m  [40/53], [94mLoss[0m : 8.38886
[1mStep[0m  [45/53], [94mLoss[0m : 7.82579
[1mStep[0m  [50/53], [94mLoss[0m : 8.17589

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.189, [92mTest[0m: 8.253, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.79206
[1mStep[0m  [5/53], [94mLoss[0m : 7.77926
[1mStep[0m  [10/53], [94mLoss[0m : 8.42966
[1mStep[0m  [15/53], [94mLoss[0m : 8.27054
[1mStep[0m  [20/53], [94mLoss[0m : 8.53942
[1mStep[0m  [25/53], [94mLoss[0m : 8.06074
[1mStep[0m  [30/53], [94mLoss[0m : 7.88310
[1mStep[0m  [35/53], [94mLoss[0m : 8.05377
[1mStep[0m  [40/53], [94mLoss[0m : 8.31060
[1mStep[0m  [45/53], [94mLoss[0m : 7.80946
[1mStep[0m  [50/53], [94mLoss[0m : 8.15826

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.116, [92mTest[0m: 8.173, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.18511
[1mStep[0m  [5/53], [94mLoss[0m : 8.13855
[1mStep[0m  [10/53], [94mLoss[0m : 8.00127
[1mStep[0m  [15/53], [94mLoss[0m : 7.87569
[1mStep[0m  [20/53], [94mLoss[0m : 7.94797
[1mStep[0m  [25/53], [94mLoss[0m : 7.94545
[1mStep[0m  [30/53], [94mLoss[0m : 8.07743
[1mStep[0m  [35/53], [94mLoss[0m : 8.05424
[1mStep[0m  [40/53], [94mLoss[0m : 8.40379
[1mStep[0m  [45/53], [94mLoss[0m : 7.78253
[1mStep[0m  [50/53], [94mLoss[0m : 8.07824

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.032, [92mTest[0m: 8.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.97146
[1mStep[0m  [5/53], [94mLoss[0m : 7.56671
[1mStep[0m  [10/53], [94mLoss[0m : 8.02542
[1mStep[0m  [15/53], [94mLoss[0m : 7.85197
[1mStep[0m  [20/53], [94mLoss[0m : 7.66645
[1mStep[0m  [25/53], [94mLoss[0m : 7.72364
[1mStep[0m  [30/53], [94mLoss[0m : 8.02233
[1mStep[0m  [35/53], [94mLoss[0m : 7.72615
[1mStep[0m  [40/53], [94mLoss[0m : 7.68786
[1mStep[0m  [45/53], [94mLoss[0m : 7.90163
[1mStep[0m  [50/53], [94mLoss[0m : 8.03088

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.951, [92mTest[0m: 8.008, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.81594
[1mStep[0m  [5/53], [94mLoss[0m : 7.74722
[1mStep[0m  [10/53], [94mLoss[0m : 7.98095
[1mStep[0m  [15/53], [94mLoss[0m : 7.88838
[1mStep[0m  [20/53], [94mLoss[0m : 7.80649
[1mStep[0m  [25/53], [94mLoss[0m : 8.20756
[1mStep[0m  [30/53], [94mLoss[0m : 7.94182
[1mStep[0m  [35/53], [94mLoss[0m : 7.91605
[1mStep[0m  [40/53], [94mLoss[0m : 7.95859
[1mStep[0m  [45/53], [94mLoss[0m : 8.18464
[1mStep[0m  [50/53], [94mLoss[0m : 7.88684

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.878, [92mTest[0m: 7.932, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.93607
[1mStep[0m  [5/53], [94mLoss[0m : 8.00598
[1mStep[0m  [10/53], [94mLoss[0m : 7.66689
[1mStep[0m  [15/53], [94mLoss[0m : 7.91953
[1mStep[0m  [20/53], [94mLoss[0m : 7.74662
[1mStep[0m  [25/53], [94mLoss[0m : 7.61481
[1mStep[0m  [30/53], [94mLoss[0m : 7.89086
[1mStep[0m  [35/53], [94mLoss[0m : 7.96221
[1mStep[0m  [40/53], [94mLoss[0m : 7.59177
[1mStep[0m  [45/53], [94mLoss[0m : 7.95882
[1mStep[0m  [50/53], [94mLoss[0m : 7.62745

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.792, [92mTest[0m: 7.840, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.03717
[1mStep[0m  [5/53], [94mLoss[0m : 8.03104
[1mStep[0m  [10/53], [94mLoss[0m : 7.70471
[1mStep[0m  [15/53], [94mLoss[0m : 8.46513
[1mStep[0m  [20/53], [94mLoss[0m : 7.61688
[1mStep[0m  [25/53], [94mLoss[0m : 7.75224
[1mStep[0m  [30/53], [94mLoss[0m : 7.42027
[1mStep[0m  [35/53], [94mLoss[0m : 7.61648
[1mStep[0m  [40/53], [94mLoss[0m : 7.91607
[1mStep[0m  [45/53], [94mLoss[0m : 7.68640
[1mStep[0m  [50/53], [94mLoss[0m : 7.64254

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.706, [92mTest[0m: 7.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.41225
[1mStep[0m  [5/53], [94mLoss[0m : 7.31821
[1mStep[0m  [10/53], [94mLoss[0m : 7.92047
[1mStep[0m  [15/53], [94mLoss[0m : 7.62282
[1mStep[0m  [20/53], [94mLoss[0m : 7.52799
[1mStep[0m  [25/53], [94mLoss[0m : 7.58464
[1mStep[0m  [30/53], [94mLoss[0m : 7.72799
[1mStep[0m  [35/53], [94mLoss[0m : 7.98367
[1mStep[0m  [40/53], [94mLoss[0m : 7.55363
[1mStep[0m  [45/53], [94mLoss[0m : 7.69933
[1mStep[0m  [50/53], [94mLoss[0m : 8.09510

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.633, [92mTest[0m: 7.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.43652
[1mStep[0m  [5/53], [94mLoss[0m : 7.47224
[1mStep[0m  [10/53], [94mLoss[0m : 7.55745
[1mStep[0m  [15/53], [94mLoss[0m : 7.51435
[1mStep[0m  [20/53], [94mLoss[0m : 7.72941
[1mStep[0m  [25/53], [94mLoss[0m : 7.97456
[1mStep[0m  [30/53], [94mLoss[0m : 7.52478
[1mStep[0m  [35/53], [94mLoss[0m : 7.75198
[1mStep[0m  [40/53], [94mLoss[0m : 7.24665
[1mStep[0m  [45/53], [94mLoss[0m : 7.19697
[1mStep[0m  [50/53], [94mLoss[0m : 7.41974

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.555, [92mTest[0m: 7.595, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.63518
[1mStep[0m  [5/53], [94mLoss[0m : 7.49431
[1mStep[0m  [10/53], [94mLoss[0m : 7.38916
[1mStep[0m  [15/53], [94mLoss[0m : 7.79600
[1mStep[0m  [20/53], [94mLoss[0m : 7.58554
[1mStep[0m  [25/53], [94mLoss[0m : 7.42657
[1mStep[0m  [30/53], [94mLoss[0m : 7.43209
[1mStep[0m  [35/53], [94mLoss[0m : 7.40542
[1mStep[0m  [40/53], [94mLoss[0m : 7.65233
[1mStep[0m  [45/53], [94mLoss[0m : 7.85912
[1mStep[0m  [50/53], [94mLoss[0m : 7.20193

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.477, [92mTest[0m: 7.546, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.35758
[1mStep[0m  [5/53], [94mLoss[0m : 7.87692
[1mStep[0m  [10/53], [94mLoss[0m : 7.39931
[1mStep[0m  [15/53], [94mLoss[0m : 7.27966
[1mStep[0m  [20/53], [94mLoss[0m : 7.56815
[1mStep[0m  [25/53], [94mLoss[0m : 7.05798
[1mStep[0m  [30/53], [94mLoss[0m : 7.34380
[1mStep[0m  [35/53], [94mLoss[0m : 7.04271
[1mStep[0m  [40/53], [94mLoss[0m : 7.21654
[1mStep[0m  [45/53], [94mLoss[0m : 7.26605
[1mStep[0m  [50/53], [94mLoss[0m : 8.06827

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.390, [92mTest[0m: 7.453, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.51688
[1mStep[0m  [5/53], [94mLoss[0m : 7.58677
[1mStep[0m  [10/53], [94mLoss[0m : 7.74742
[1mStep[0m  [15/53], [94mLoss[0m : 7.15372
[1mStep[0m  [20/53], [94mLoss[0m : 7.61751
[1mStep[0m  [25/53], [94mLoss[0m : 6.90609
[1mStep[0m  [30/53], [94mLoss[0m : 7.40989
[1mStep[0m  [35/53], [94mLoss[0m : 6.89335
[1mStep[0m  [40/53], [94mLoss[0m : 7.04807
[1mStep[0m  [45/53], [94mLoss[0m : 7.65312
[1mStep[0m  [50/53], [94mLoss[0m : 7.20675

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.316, [92mTest[0m: 7.352, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.57956
[1mStep[0m  [5/53], [94mLoss[0m : 7.34134
[1mStep[0m  [10/53], [94mLoss[0m : 7.32531
[1mStep[0m  [15/53], [94mLoss[0m : 7.53000
[1mStep[0m  [20/53], [94mLoss[0m : 6.97275
[1mStep[0m  [25/53], [94mLoss[0m : 7.47843
[1mStep[0m  [30/53], [94mLoss[0m : 7.35822
[1mStep[0m  [35/53], [94mLoss[0m : 7.04146
[1mStep[0m  [40/53], [94mLoss[0m : 7.30202
[1mStep[0m  [45/53], [94mLoss[0m : 7.43889
[1mStep[0m  [50/53], [94mLoss[0m : 7.29990

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.229, [92mTest[0m: 7.289, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.40559
[1mStep[0m  [5/53], [94mLoss[0m : 7.35344
[1mStep[0m  [10/53], [94mLoss[0m : 7.38050
[1mStep[0m  [15/53], [94mLoss[0m : 6.89813
[1mStep[0m  [20/53], [94mLoss[0m : 7.13722
[1mStep[0m  [25/53], [94mLoss[0m : 6.95097
[1mStep[0m  [30/53], [94mLoss[0m : 7.17373
[1mStep[0m  [35/53], [94mLoss[0m : 7.70018
[1mStep[0m  [40/53], [94mLoss[0m : 6.83763
[1mStep[0m  [45/53], [94mLoss[0m : 6.75061
[1mStep[0m  [50/53], [94mLoss[0m : 7.37383

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.150, [92mTest[0m: 7.213, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.23930
[1mStep[0m  [5/53], [94mLoss[0m : 6.84403
[1mStep[0m  [10/53], [94mLoss[0m : 7.01345
[1mStep[0m  [15/53], [94mLoss[0m : 7.11878
[1mStep[0m  [20/53], [94mLoss[0m : 7.41257
[1mStep[0m  [25/53], [94mLoss[0m : 6.82135
[1mStep[0m  [30/53], [94mLoss[0m : 7.17927
[1mStep[0m  [35/53], [94mLoss[0m : 7.22429
[1mStep[0m  [40/53], [94mLoss[0m : 7.31058
[1mStep[0m  [45/53], [94mLoss[0m : 7.01562
[1mStep[0m  [50/53], [94mLoss[0m : 7.21597

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.079, [92mTest[0m: 7.116, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.91508
[1mStep[0m  [5/53], [94mLoss[0m : 7.23646
[1mStep[0m  [10/53], [94mLoss[0m : 6.92085
[1mStep[0m  [15/53], [94mLoss[0m : 6.64814
[1mStep[0m  [20/53], [94mLoss[0m : 6.90340
[1mStep[0m  [25/53], [94mLoss[0m : 6.83202
[1mStep[0m  [30/53], [94mLoss[0m : 6.94760
[1mStep[0m  [35/53], [94mLoss[0m : 6.64566
[1mStep[0m  [40/53], [94mLoss[0m : 6.99348
[1mStep[0m  [45/53], [94mLoss[0m : 7.32887
[1mStep[0m  [50/53], [94mLoss[0m : 6.73266

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.991, [92mTest[0m: 7.029, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.74102
[1mStep[0m  [5/53], [94mLoss[0m : 7.06295
[1mStep[0m  [10/53], [94mLoss[0m : 6.97639
[1mStep[0m  [15/53], [94mLoss[0m : 7.17135
[1mStep[0m  [20/53], [94mLoss[0m : 6.76424
[1mStep[0m  [25/53], [94mLoss[0m : 6.42357
[1mStep[0m  [30/53], [94mLoss[0m : 7.06744
[1mStep[0m  [35/53], [94mLoss[0m : 7.03299
[1mStep[0m  [40/53], [94mLoss[0m : 6.99851
[1mStep[0m  [45/53], [94mLoss[0m : 7.01548
[1mStep[0m  [50/53], [94mLoss[0m : 6.93854

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.916, [92mTest[0m: 6.964, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.89003
[1mStep[0m  [5/53], [94mLoss[0m : 6.84035
[1mStep[0m  [10/53], [94mLoss[0m : 7.03392
[1mStep[0m  [15/53], [94mLoss[0m : 7.24390
[1mStep[0m  [20/53], [94mLoss[0m : 6.88915
[1mStep[0m  [25/53], [94mLoss[0m : 6.89057
[1mStep[0m  [30/53], [94mLoss[0m : 7.11658
[1mStep[0m  [35/53], [94mLoss[0m : 6.53513
[1mStep[0m  [40/53], [94mLoss[0m : 6.85192
[1mStep[0m  [45/53], [94mLoss[0m : 6.83654
[1mStep[0m  [50/53], [94mLoss[0m : 6.93691

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.839, [92mTest[0m: 6.887, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.65232
[1mStep[0m  [5/53], [94mLoss[0m : 6.70134
[1mStep[0m  [10/53], [94mLoss[0m : 6.70332
[1mStep[0m  [15/53], [94mLoss[0m : 6.56336
[1mStep[0m  [20/53], [94mLoss[0m : 6.44287
[1mStep[0m  [25/53], [94mLoss[0m : 6.76783
[1mStep[0m  [30/53], [94mLoss[0m : 7.09186
[1mStep[0m  [35/53], [94mLoss[0m : 7.02813
[1mStep[0m  [40/53], [94mLoss[0m : 6.64968
[1mStep[0m  [45/53], [94mLoss[0m : 6.59454
[1mStep[0m  [50/53], [94mLoss[0m : 6.71579

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.757, [92mTest[0m: 6.822, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.69948
[1mStep[0m  [5/53], [94mLoss[0m : 7.00172
[1mStep[0m  [10/53], [94mLoss[0m : 6.89959
[1mStep[0m  [15/53], [94mLoss[0m : 6.75363
[1mStep[0m  [20/53], [94mLoss[0m : 6.60271
[1mStep[0m  [25/53], [94mLoss[0m : 6.62913
[1mStep[0m  [30/53], [94mLoss[0m : 6.43647
[1mStep[0m  [35/53], [94mLoss[0m : 6.20013
[1mStep[0m  [40/53], [94mLoss[0m : 6.52910
[1mStep[0m  [45/53], [94mLoss[0m : 6.76208
[1mStep[0m  [50/53], [94mLoss[0m : 6.70616

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.690, [92mTest[0m: 6.750, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.78714
[1mStep[0m  [5/53], [94mLoss[0m : 6.67845
[1mStep[0m  [10/53], [94mLoss[0m : 6.61796
[1mStep[0m  [15/53], [94mLoss[0m : 6.57328
[1mStep[0m  [20/53], [94mLoss[0m : 6.30906
[1mStep[0m  [25/53], [94mLoss[0m : 6.92073
[1mStep[0m  [30/53], [94mLoss[0m : 6.76242
[1mStep[0m  [35/53], [94mLoss[0m : 6.37746
[1mStep[0m  [40/53], [94mLoss[0m : 6.95934
[1mStep[0m  [45/53], [94mLoss[0m : 6.49167
[1mStep[0m  [50/53], [94mLoss[0m : 6.23829

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.627, [92mTest[0m: 6.678, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.70760
[1mStep[0m  [5/53], [94mLoss[0m : 6.37896
[1mStep[0m  [10/53], [94mLoss[0m : 6.96066
[1mStep[0m  [15/53], [94mLoss[0m : 6.69529
[1mStep[0m  [20/53], [94mLoss[0m : 6.91713
[1mStep[0m  [25/53], [94mLoss[0m : 6.58894
[1mStep[0m  [30/53], [94mLoss[0m : 6.30450
[1mStep[0m  [35/53], [94mLoss[0m : 6.70119
[1mStep[0m  [40/53], [94mLoss[0m : 6.47399
[1mStep[0m  [45/53], [94mLoss[0m : 6.59163
[1mStep[0m  [50/53], [94mLoss[0m : 6.12511

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.550, [92mTest[0m: 6.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.04340
[1mStep[0m  [5/53], [94mLoss[0m : 6.55529
[1mStep[0m  [10/53], [94mLoss[0m : 6.36202
[1mStep[0m  [15/53], [94mLoss[0m : 6.44428
[1mStep[0m  [20/53], [94mLoss[0m : 6.70202
[1mStep[0m  [25/53], [94mLoss[0m : 6.32923
[1mStep[0m  [30/53], [94mLoss[0m : 6.38917
[1mStep[0m  [35/53], [94mLoss[0m : 6.56955
[1mStep[0m  [40/53], [94mLoss[0m : 6.13600
[1mStep[0m  [45/53], [94mLoss[0m : 6.66283
[1mStep[0m  [50/53], [94mLoss[0m : 6.58564

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.484, [92mTest[0m: 6.527, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.67780
[1mStep[0m  [5/53], [94mLoss[0m : 6.45409
[1mStep[0m  [10/53], [94mLoss[0m : 6.42173
[1mStep[0m  [15/53], [94mLoss[0m : 6.25213
[1mStep[0m  [20/53], [94mLoss[0m : 6.26630
[1mStep[0m  [25/53], [94mLoss[0m : 6.69659
[1mStep[0m  [30/53], [94mLoss[0m : 6.28929
[1mStep[0m  [35/53], [94mLoss[0m : 6.24513
[1mStep[0m  [40/53], [94mLoss[0m : 6.44938
[1mStep[0m  [45/53], [94mLoss[0m : 6.41839
[1mStep[0m  [50/53], [94mLoss[0m : 6.38990

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.406, [92mTest[0m: 6.445, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.40661
[1mStep[0m  [5/53], [94mLoss[0m : 6.18610
[1mStep[0m  [10/53], [94mLoss[0m : 5.90078
[1mStep[0m  [15/53], [94mLoss[0m : 6.30204
[1mStep[0m  [20/53], [94mLoss[0m : 6.42844
[1mStep[0m  [25/53], [94mLoss[0m : 6.26917
[1mStep[0m  [30/53], [94mLoss[0m : 6.11333
[1mStep[0m  [35/53], [94mLoss[0m : 6.32454
[1mStep[0m  [40/53], [94mLoss[0m : 6.58313
[1mStep[0m  [45/53], [94mLoss[0m : 6.13428
[1mStep[0m  [50/53], [94mLoss[0m : 6.22489

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.343, [92mTest[0m: 6.364, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.07733
[1mStep[0m  [5/53], [94mLoss[0m : 6.38359
[1mStep[0m  [10/53], [94mLoss[0m : 6.08987
[1mStep[0m  [15/53], [94mLoss[0m : 6.40201
[1mStep[0m  [20/53], [94mLoss[0m : 6.39516
[1mStep[0m  [25/53], [94mLoss[0m : 6.18100
[1mStep[0m  [30/53], [94mLoss[0m : 6.12700
[1mStep[0m  [35/53], [94mLoss[0m : 6.40972
[1mStep[0m  [40/53], [94mLoss[0m : 6.35358
[1mStep[0m  [45/53], [94mLoss[0m : 6.28958
[1mStep[0m  [50/53], [94mLoss[0m : 6.21609

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.263, [92mTest[0m: 6.311, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.26522
[1mStep[0m  [5/53], [94mLoss[0m : 5.98313
[1mStep[0m  [10/53], [94mLoss[0m : 6.05689
[1mStep[0m  [15/53], [94mLoss[0m : 6.29433
[1mStep[0m  [20/53], [94mLoss[0m : 6.55677
[1mStep[0m  [25/53], [94mLoss[0m : 6.40280
[1mStep[0m  [30/53], [94mLoss[0m : 6.10867
[1mStep[0m  [35/53], [94mLoss[0m : 6.42762
[1mStep[0m  [40/53], [94mLoss[0m : 6.31468
[1mStep[0m  [45/53], [94mLoss[0m : 6.13619
[1mStep[0m  [50/53], [94mLoss[0m : 6.16367

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.183, [92mTest[0m: 6.231, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.165
====================================

Phase 2 - Evaluation MAE:  6.164910426506629
MAE score P1       8.475736
MAE score P2        6.16491
loss               6.183344
learning_rate        0.0001
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.61803
[1mStep[0m  [2/26], [94mLoss[0m : 10.94682
[1mStep[0m  [4/26], [94mLoss[0m : 10.87359
[1mStep[0m  [6/26], [94mLoss[0m : 10.93610
[1mStep[0m  [8/26], [94mLoss[0m : 10.64500
[1mStep[0m  [10/26], [94mLoss[0m : 10.92804
[1mStep[0m  [12/26], [94mLoss[0m : 10.75511
[1mStep[0m  [14/26], [94mLoss[0m : 10.60331
[1mStep[0m  [16/26], [94mLoss[0m : 10.98099
[1mStep[0m  [18/26], [94mLoss[0m : 10.73956
[1mStep[0m  [20/26], [94mLoss[0m : 10.51273
[1mStep[0m  [22/26], [94mLoss[0m : 10.78948
[1mStep[0m  [24/26], [94mLoss[0m : 10.85055

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.810, [92mTest[0m: 10.972, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.05266
[1mStep[0m  [2/26], [94mLoss[0m : 10.76221
[1mStep[0m  [4/26], [94mLoss[0m : 10.55123
[1mStep[0m  [6/26], [94mLoss[0m : 10.75866
[1mStep[0m  [8/26], [94mLoss[0m : 10.75696
[1mStep[0m  [10/26], [94mLoss[0m : 10.67826
[1mStep[0m  [12/26], [94mLoss[0m : 10.99598
[1mStep[0m  [14/26], [94mLoss[0m : 10.58308
[1mStep[0m  [16/26], [94mLoss[0m : 10.76625
[1mStep[0m  [18/26], [94mLoss[0m : 10.56439
[1mStep[0m  [20/26], [94mLoss[0m : 10.66270
[1mStep[0m  [22/26], [94mLoss[0m : 10.56064
[1mStep[0m  [24/26], [94mLoss[0m : 10.70278

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.760, [92mTest[0m: 10.884, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.72049
[1mStep[0m  [2/26], [94mLoss[0m : 10.70318
[1mStep[0m  [4/26], [94mLoss[0m : 10.69703
[1mStep[0m  [6/26], [94mLoss[0m : 10.55617
[1mStep[0m  [8/26], [94mLoss[0m : 10.80317
[1mStep[0m  [10/26], [94mLoss[0m : 10.70469
[1mStep[0m  [12/26], [94mLoss[0m : 10.63989
[1mStep[0m  [14/26], [94mLoss[0m : 10.78150
[1mStep[0m  [16/26], [94mLoss[0m : 10.50588
[1mStep[0m  [18/26], [94mLoss[0m : 10.77239
[1mStep[0m  [20/26], [94mLoss[0m : 10.63859
[1mStep[0m  [22/26], [94mLoss[0m : 11.06892
[1mStep[0m  [24/26], [94mLoss[0m : 10.58004

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.714, [92mTest[0m: 10.824, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.67164
[1mStep[0m  [2/26], [94mLoss[0m : 10.75935
[1mStep[0m  [4/26], [94mLoss[0m : 10.67712
[1mStep[0m  [6/26], [94mLoss[0m : 10.39949
[1mStep[0m  [8/26], [94mLoss[0m : 10.45611
[1mStep[0m  [10/26], [94mLoss[0m : 10.58630
[1mStep[0m  [12/26], [94mLoss[0m : 10.74867
[1mStep[0m  [14/26], [94mLoss[0m : 10.74218
[1mStep[0m  [16/26], [94mLoss[0m : 10.51668
[1mStep[0m  [18/26], [94mLoss[0m : 10.73342
[1mStep[0m  [20/26], [94mLoss[0m : 10.74612
[1mStep[0m  [22/26], [94mLoss[0m : 10.64392
[1mStep[0m  [24/26], [94mLoss[0m : 10.65391

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.672, [92mTest[0m: 10.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.44745
[1mStep[0m  [2/26], [94mLoss[0m : 10.55533
[1mStep[0m  [4/26], [94mLoss[0m : 10.81206
[1mStep[0m  [6/26], [94mLoss[0m : 10.46527
[1mStep[0m  [8/26], [94mLoss[0m : 10.52574
[1mStep[0m  [10/26], [94mLoss[0m : 10.82671
[1mStep[0m  [12/26], [94mLoss[0m : 10.86227
[1mStep[0m  [14/26], [94mLoss[0m : 10.47205
[1mStep[0m  [16/26], [94mLoss[0m : 10.91581
[1mStep[0m  [18/26], [94mLoss[0m : 10.44265
[1mStep[0m  [20/26], [94mLoss[0m : 10.42469
[1mStep[0m  [22/26], [94mLoss[0m : 10.67348
[1mStep[0m  [24/26], [94mLoss[0m : 10.53168

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.616, [92mTest[0m: 10.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.61168
[1mStep[0m  [2/26], [94mLoss[0m : 10.62966
[1mStep[0m  [4/26], [94mLoss[0m : 10.30532
[1mStep[0m  [6/26], [94mLoss[0m : 10.75250
[1mStep[0m  [8/26], [94mLoss[0m : 10.55497
[1mStep[0m  [10/26], [94mLoss[0m : 10.69017
[1mStep[0m  [12/26], [94mLoss[0m : 10.45416
[1mStep[0m  [14/26], [94mLoss[0m : 10.89433
[1mStep[0m  [16/26], [94mLoss[0m : 10.38337
[1mStep[0m  [18/26], [94mLoss[0m : 10.49282
[1mStep[0m  [20/26], [94mLoss[0m : 10.62031
[1mStep[0m  [22/26], [94mLoss[0m : 10.64209
[1mStep[0m  [24/26], [94mLoss[0m : 10.53004

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.568, [92mTest[0m: 10.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.61956
[1mStep[0m  [2/26], [94mLoss[0m : 10.29659
[1mStep[0m  [4/26], [94mLoss[0m : 10.24670
[1mStep[0m  [6/26], [94mLoss[0m : 10.74769
[1mStep[0m  [8/26], [94mLoss[0m : 10.43396
[1mStep[0m  [10/26], [94mLoss[0m : 10.70012
[1mStep[0m  [12/26], [94mLoss[0m : 10.44406
[1mStep[0m  [14/26], [94mLoss[0m : 10.68520
[1mStep[0m  [16/26], [94mLoss[0m : 10.34367
[1mStep[0m  [18/26], [94mLoss[0m : 10.59343
[1mStep[0m  [20/26], [94mLoss[0m : 10.55803
[1mStep[0m  [22/26], [94mLoss[0m : 10.45015
[1mStep[0m  [24/26], [94mLoss[0m : 10.51796

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.528, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.82471
[1mStep[0m  [2/26], [94mLoss[0m : 10.37320
[1mStep[0m  [4/26], [94mLoss[0m : 10.84009
[1mStep[0m  [6/26], [94mLoss[0m : 10.34593
[1mStep[0m  [8/26], [94mLoss[0m : 10.63063
[1mStep[0m  [10/26], [94mLoss[0m : 10.49786
[1mStep[0m  [12/26], [94mLoss[0m : 10.11245
[1mStep[0m  [14/26], [94mLoss[0m : 10.48803
[1mStep[0m  [16/26], [94mLoss[0m : 10.36161
[1mStep[0m  [18/26], [94mLoss[0m : 10.32808
[1mStep[0m  [20/26], [94mLoss[0m : 10.73750
[1mStep[0m  [22/26], [94mLoss[0m : 10.61503
[1mStep[0m  [24/26], [94mLoss[0m : 10.25152

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.478, [92mTest[0m: 10.632, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.47021
[1mStep[0m  [2/26], [94mLoss[0m : 10.59967
[1mStep[0m  [4/26], [94mLoss[0m : 10.63797
[1mStep[0m  [6/26], [94mLoss[0m : 10.26904
[1mStep[0m  [8/26], [94mLoss[0m : 10.41734
[1mStep[0m  [10/26], [94mLoss[0m : 10.43579
[1mStep[0m  [12/26], [94mLoss[0m : 10.37552
[1mStep[0m  [14/26], [94mLoss[0m : 10.46062
[1mStep[0m  [16/26], [94mLoss[0m : 10.50137
[1mStep[0m  [18/26], [94mLoss[0m : 10.48626
[1mStep[0m  [20/26], [94mLoss[0m : 10.39969
[1mStep[0m  [22/26], [94mLoss[0m : 10.33010
[1mStep[0m  [24/26], [94mLoss[0m : 10.25495

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.438, [92mTest[0m: 10.598, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.78459
[1mStep[0m  [2/26], [94mLoss[0m : 10.37199
[1mStep[0m  [4/26], [94mLoss[0m : 10.35157
[1mStep[0m  [6/26], [94mLoss[0m : 10.54768
[1mStep[0m  [8/26], [94mLoss[0m : 10.55613
[1mStep[0m  [10/26], [94mLoss[0m : 10.37078
[1mStep[0m  [12/26], [94mLoss[0m : 10.37548
[1mStep[0m  [14/26], [94mLoss[0m : 10.53741
[1mStep[0m  [16/26], [94mLoss[0m : 10.05342
[1mStep[0m  [18/26], [94mLoss[0m : 10.31133
[1mStep[0m  [20/26], [94mLoss[0m : 10.53959
[1mStep[0m  [22/26], [94mLoss[0m : 10.43309
[1mStep[0m  [24/26], [94mLoss[0m : 10.51057

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.385, [92mTest[0m: 10.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.51295
[1mStep[0m  [2/26], [94mLoss[0m : 10.32076
[1mStep[0m  [4/26], [94mLoss[0m : 10.48507
[1mStep[0m  [6/26], [94mLoss[0m : 10.30729
[1mStep[0m  [8/26], [94mLoss[0m : 10.50734
[1mStep[0m  [10/26], [94mLoss[0m : 10.55370
[1mStep[0m  [12/26], [94mLoss[0m : 10.39136
[1mStep[0m  [14/26], [94mLoss[0m : 10.26545
[1mStep[0m  [16/26], [94mLoss[0m : 10.35525
[1mStep[0m  [18/26], [94mLoss[0m : 10.19831
[1mStep[0m  [20/26], [94mLoss[0m : 10.25281
[1mStep[0m  [22/26], [94mLoss[0m : 10.31095
[1mStep[0m  [24/26], [94mLoss[0m : 10.11820

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.354, [92mTest[0m: 10.506, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.26270
[1mStep[0m  [2/26], [94mLoss[0m : 10.30470
[1mStep[0m  [4/26], [94mLoss[0m : 10.26340
[1mStep[0m  [6/26], [94mLoss[0m : 10.32290
[1mStep[0m  [8/26], [94mLoss[0m : 10.23120
[1mStep[0m  [10/26], [94mLoss[0m : 10.38112
[1mStep[0m  [12/26], [94mLoss[0m : 10.28339
[1mStep[0m  [14/26], [94mLoss[0m : 10.45305
[1mStep[0m  [16/26], [94mLoss[0m : 10.29326
[1mStep[0m  [18/26], [94mLoss[0m : 10.45921
[1mStep[0m  [20/26], [94mLoss[0m : 10.03446
[1mStep[0m  [22/26], [94mLoss[0m : 10.27530
[1mStep[0m  [24/26], [94mLoss[0m : 10.27623

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.304, [92mTest[0m: 10.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.24852
[1mStep[0m  [2/26], [94mLoss[0m : 10.07082
[1mStep[0m  [4/26], [94mLoss[0m : 10.17352
[1mStep[0m  [6/26], [94mLoss[0m : 10.56955
[1mStep[0m  [8/26], [94mLoss[0m : 10.23657
[1mStep[0m  [10/26], [94mLoss[0m : 10.21047
[1mStep[0m  [12/26], [94mLoss[0m : 10.57209
[1mStep[0m  [14/26], [94mLoss[0m : 10.44676
[1mStep[0m  [16/26], [94mLoss[0m : 10.30064
[1mStep[0m  [18/26], [94mLoss[0m : 10.54427
[1mStep[0m  [20/26], [94mLoss[0m : 10.20220
[1mStep[0m  [22/26], [94mLoss[0m : 10.36727
[1mStep[0m  [24/26], [94mLoss[0m : 10.29061

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.247, [92mTest[0m: 10.444, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.09519
[1mStep[0m  [2/26], [94mLoss[0m : 10.26896
[1mStep[0m  [4/26], [94mLoss[0m : 10.32625
[1mStep[0m  [6/26], [94mLoss[0m : 10.16228
[1mStep[0m  [8/26], [94mLoss[0m : 10.04218
[1mStep[0m  [10/26], [94mLoss[0m : 10.26480
[1mStep[0m  [12/26], [94mLoss[0m : 10.27273
[1mStep[0m  [14/26], [94mLoss[0m : 10.33033
[1mStep[0m  [16/26], [94mLoss[0m : 10.14151
[1mStep[0m  [18/26], [94mLoss[0m : 9.88257
[1mStep[0m  [20/26], [94mLoss[0m : 10.30662
[1mStep[0m  [22/26], [94mLoss[0m : 10.29428
[1mStep[0m  [24/26], [94mLoss[0m : 9.95919

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.215, [92mTest[0m: 10.388, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.00670
[1mStep[0m  [2/26], [94mLoss[0m : 10.21106
[1mStep[0m  [4/26], [94mLoss[0m : 10.50384
[1mStep[0m  [6/26], [94mLoss[0m : 10.25049
[1mStep[0m  [8/26], [94mLoss[0m : 10.28843
[1mStep[0m  [10/26], [94mLoss[0m : 10.16120
[1mStep[0m  [12/26], [94mLoss[0m : 10.22143
[1mStep[0m  [14/26], [94mLoss[0m : 10.19514
[1mStep[0m  [16/26], [94mLoss[0m : 9.81478
[1mStep[0m  [18/26], [94mLoss[0m : 10.32917
[1mStep[0m  [20/26], [94mLoss[0m : 10.09531
[1mStep[0m  [22/26], [94mLoss[0m : 9.98526
[1mStep[0m  [24/26], [94mLoss[0m : 10.12951

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.153, [92mTest[0m: 10.366, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.05983
[1mStep[0m  [2/26], [94mLoss[0m : 10.25002
[1mStep[0m  [4/26], [94mLoss[0m : 10.48127
[1mStep[0m  [6/26], [94mLoss[0m : 10.27088
[1mStep[0m  [8/26], [94mLoss[0m : 9.93648
[1mStep[0m  [10/26], [94mLoss[0m : 10.16091
[1mStep[0m  [12/26], [94mLoss[0m : 10.14467
[1mStep[0m  [14/26], [94mLoss[0m : 10.03962
[1mStep[0m  [16/26], [94mLoss[0m : 10.00562
[1mStep[0m  [18/26], [94mLoss[0m : 9.98268
[1mStep[0m  [20/26], [94mLoss[0m : 10.41612
[1mStep[0m  [22/26], [94mLoss[0m : 10.19069
[1mStep[0m  [24/26], [94mLoss[0m : 10.11578

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.112, [92mTest[0m: 10.335, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.97727
[1mStep[0m  [2/26], [94mLoss[0m : 10.07132
[1mStep[0m  [4/26], [94mLoss[0m : 10.27316
[1mStep[0m  [6/26], [94mLoss[0m : 9.86791
[1mStep[0m  [8/26], [94mLoss[0m : 10.01292
[1mStep[0m  [10/26], [94mLoss[0m : 10.28143
[1mStep[0m  [12/26], [94mLoss[0m : 9.90583
[1mStep[0m  [14/26], [94mLoss[0m : 10.00885
[1mStep[0m  [16/26], [94mLoss[0m : 10.05202
[1mStep[0m  [18/26], [94mLoss[0m : 10.16816
[1mStep[0m  [20/26], [94mLoss[0m : 10.26764
[1mStep[0m  [22/26], [94mLoss[0m : 10.03510
[1mStep[0m  [24/26], [94mLoss[0m : 9.97090

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.055, [92mTest[0m: 10.278, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.17427
[1mStep[0m  [2/26], [94mLoss[0m : 9.90361
[1mStep[0m  [4/26], [94mLoss[0m : 9.99055
[1mStep[0m  [6/26], [94mLoss[0m : 10.19003
[1mStep[0m  [8/26], [94mLoss[0m : 10.19496
[1mStep[0m  [10/26], [94mLoss[0m : 9.98708
[1mStep[0m  [12/26], [94mLoss[0m : 10.02096
[1mStep[0m  [14/26], [94mLoss[0m : 10.02042
[1mStep[0m  [16/26], [94mLoss[0m : 9.80778
[1mStep[0m  [18/26], [94mLoss[0m : 9.78310
[1mStep[0m  [20/26], [94mLoss[0m : 9.89866
[1mStep[0m  [22/26], [94mLoss[0m : 10.01461
[1mStep[0m  [24/26], [94mLoss[0m : 9.96145

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.026, [92mTest[0m: 10.237, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.97092
[1mStep[0m  [2/26], [94mLoss[0m : 10.16149
[1mStep[0m  [4/26], [94mLoss[0m : 9.73489
[1mStep[0m  [6/26], [94mLoss[0m : 10.11228
[1mStep[0m  [8/26], [94mLoss[0m : 9.93392
[1mStep[0m  [10/26], [94mLoss[0m : 9.97234
[1mStep[0m  [12/26], [94mLoss[0m : 9.95160
[1mStep[0m  [14/26], [94mLoss[0m : 9.97278
[1mStep[0m  [16/26], [94mLoss[0m : 10.30393
[1mStep[0m  [18/26], [94mLoss[0m : 9.98133
[1mStep[0m  [20/26], [94mLoss[0m : 9.91700
[1mStep[0m  [22/26], [94mLoss[0m : 9.76619
[1mStep[0m  [24/26], [94mLoss[0m : 9.90581

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.972, [92mTest[0m: 10.194, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.78867
[1mStep[0m  [2/26], [94mLoss[0m : 10.08432
[1mStep[0m  [4/26], [94mLoss[0m : 9.95249
[1mStep[0m  [6/26], [94mLoss[0m : 10.08466
[1mStep[0m  [8/26], [94mLoss[0m : 9.88887
[1mStep[0m  [10/26], [94mLoss[0m : 10.18287
[1mStep[0m  [12/26], [94mLoss[0m : 9.77701
[1mStep[0m  [14/26], [94mLoss[0m : 10.02464
[1mStep[0m  [16/26], [94mLoss[0m : 9.86431
[1mStep[0m  [18/26], [94mLoss[0m : 9.74413
[1mStep[0m  [20/26], [94mLoss[0m : 9.99906
[1mStep[0m  [22/26], [94mLoss[0m : 10.01429
[1mStep[0m  [24/26], [94mLoss[0m : 9.75660

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.930, [92mTest[0m: 10.153, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.67291
[1mStep[0m  [2/26], [94mLoss[0m : 9.78327
[1mStep[0m  [4/26], [94mLoss[0m : 10.01574
[1mStep[0m  [6/26], [94mLoss[0m : 9.76215
[1mStep[0m  [8/26], [94mLoss[0m : 9.96340
[1mStep[0m  [10/26], [94mLoss[0m : 10.04279
[1mStep[0m  [12/26], [94mLoss[0m : 9.99768
[1mStep[0m  [14/26], [94mLoss[0m : 9.84149
[1mStep[0m  [16/26], [94mLoss[0m : 9.96490
[1mStep[0m  [18/26], [94mLoss[0m : 9.70955
[1mStep[0m  [20/26], [94mLoss[0m : 9.70477
[1mStep[0m  [22/26], [94mLoss[0m : 10.11319
[1mStep[0m  [24/26], [94mLoss[0m : 9.58379

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.888, [92mTest[0m: 10.114, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.92960
[1mStep[0m  [2/26], [94mLoss[0m : 9.77810
[1mStep[0m  [4/26], [94mLoss[0m : 10.02992
[1mStep[0m  [6/26], [94mLoss[0m : 9.76947
[1mStep[0m  [8/26], [94mLoss[0m : 9.59420
[1mStep[0m  [10/26], [94mLoss[0m : 10.01990
[1mStep[0m  [12/26], [94mLoss[0m : 9.85830
[1mStep[0m  [14/26], [94mLoss[0m : 9.95566
[1mStep[0m  [16/26], [94mLoss[0m : 9.88717
[1mStep[0m  [18/26], [94mLoss[0m : 9.72874
[1mStep[0m  [20/26], [94mLoss[0m : 9.80137
[1mStep[0m  [22/26], [94mLoss[0m : 9.92200
[1mStep[0m  [24/26], [94mLoss[0m : 9.81935

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.859, [92mTest[0m: 10.092, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.79973
[1mStep[0m  [2/26], [94mLoss[0m : 10.12421
[1mStep[0m  [4/26], [94mLoss[0m : 9.95576
[1mStep[0m  [6/26], [94mLoss[0m : 9.70106
[1mStep[0m  [8/26], [94mLoss[0m : 9.92336
[1mStep[0m  [10/26], [94mLoss[0m : 9.65515
[1mStep[0m  [12/26], [94mLoss[0m : 9.69160
[1mStep[0m  [14/26], [94mLoss[0m : 9.59070
[1mStep[0m  [16/26], [94mLoss[0m : 9.67436
[1mStep[0m  [18/26], [94mLoss[0m : 9.87448
[1mStep[0m  [20/26], [94mLoss[0m : 9.79386
[1mStep[0m  [22/26], [94mLoss[0m : 9.72694
[1mStep[0m  [24/26], [94mLoss[0m : 9.63135

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.808, [92mTest[0m: 10.059, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.69838
[1mStep[0m  [2/26], [94mLoss[0m : 10.01447
[1mStep[0m  [4/26], [94mLoss[0m : 9.79651
[1mStep[0m  [6/26], [94mLoss[0m : 9.82367
[1mStep[0m  [8/26], [94mLoss[0m : 9.75781
[1mStep[0m  [10/26], [94mLoss[0m : 9.56203
[1mStep[0m  [12/26], [94mLoss[0m : 9.76696
[1mStep[0m  [14/26], [94mLoss[0m : 9.54548
[1mStep[0m  [16/26], [94mLoss[0m : 9.76594
[1mStep[0m  [18/26], [94mLoss[0m : 9.80509
[1mStep[0m  [20/26], [94mLoss[0m : 9.97510
[1mStep[0m  [22/26], [94mLoss[0m : 9.69299
[1mStep[0m  [24/26], [94mLoss[0m : 9.78222

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.760, [92mTest[0m: 10.011, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.04619
[1mStep[0m  [2/26], [94mLoss[0m : 10.00948
[1mStep[0m  [4/26], [94mLoss[0m : 9.65588
[1mStep[0m  [6/26], [94mLoss[0m : 9.64161
[1mStep[0m  [8/26], [94mLoss[0m : 9.39601
[1mStep[0m  [10/26], [94mLoss[0m : 9.62718
[1mStep[0m  [12/26], [94mLoss[0m : 9.72239
[1mStep[0m  [14/26], [94mLoss[0m : 9.59865
[1mStep[0m  [16/26], [94mLoss[0m : 9.81837
[1mStep[0m  [18/26], [94mLoss[0m : 9.84743
[1mStep[0m  [20/26], [94mLoss[0m : 9.58846
[1mStep[0m  [22/26], [94mLoss[0m : 9.65583
[1mStep[0m  [24/26], [94mLoss[0m : 9.63211

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.726, [92mTest[0m: 9.990, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.51133
[1mStep[0m  [2/26], [94mLoss[0m : 9.78210
[1mStep[0m  [4/26], [94mLoss[0m : 9.95921
[1mStep[0m  [6/26], [94mLoss[0m : 9.82582
[1mStep[0m  [8/26], [94mLoss[0m : 9.74403
[1mStep[0m  [10/26], [94mLoss[0m : 9.71017
[1mStep[0m  [12/26], [94mLoss[0m : 9.80537
[1mStep[0m  [14/26], [94mLoss[0m : 9.50989
[1mStep[0m  [16/26], [94mLoss[0m : 9.60700
[1mStep[0m  [18/26], [94mLoss[0m : 9.74726
[1mStep[0m  [20/26], [94mLoss[0m : 9.67599
[1mStep[0m  [22/26], [94mLoss[0m : 9.59442
[1mStep[0m  [24/26], [94mLoss[0m : 9.80457

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.678, [92mTest[0m: 9.946, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.61694
[1mStep[0m  [2/26], [94mLoss[0m : 9.54520
[1mStep[0m  [4/26], [94mLoss[0m : 9.98198
[1mStep[0m  [6/26], [94mLoss[0m : 9.63654
[1mStep[0m  [8/26], [94mLoss[0m : 9.69285
[1mStep[0m  [10/26], [94mLoss[0m : 9.74229
[1mStep[0m  [12/26], [94mLoss[0m : 9.60356
[1mStep[0m  [14/26], [94mLoss[0m : 9.58625
[1mStep[0m  [16/26], [94mLoss[0m : 9.75966
[1mStep[0m  [18/26], [94mLoss[0m : 9.58287
[1mStep[0m  [20/26], [94mLoss[0m : 9.56547
[1mStep[0m  [22/26], [94mLoss[0m : 9.68047
[1mStep[0m  [24/26], [94mLoss[0m : 9.55579

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.634, [92mTest[0m: 9.890, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.51528
[1mStep[0m  [2/26], [94mLoss[0m : 9.37445
[1mStep[0m  [4/26], [94mLoss[0m : 9.62811
[1mStep[0m  [6/26], [94mLoss[0m : 9.72436
[1mStep[0m  [8/26], [94mLoss[0m : 9.66978
[1mStep[0m  [10/26], [94mLoss[0m : 9.64040
[1mStep[0m  [12/26], [94mLoss[0m : 9.58579
[1mStep[0m  [14/26], [94mLoss[0m : 9.84208
[1mStep[0m  [16/26], [94mLoss[0m : 9.75795
[1mStep[0m  [18/26], [94mLoss[0m : 9.48103
[1mStep[0m  [20/26], [94mLoss[0m : 9.44521
[1mStep[0m  [22/26], [94mLoss[0m : 9.63310
[1mStep[0m  [24/26], [94mLoss[0m : 9.65498

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.608, [92mTest[0m: 9.872, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.40167
[1mStep[0m  [2/26], [94mLoss[0m : 9.32701
[1mStep[0m  [4/26], [94mLoss[0m : 9.51980
[1mStep[0m  [6/26], [94mLoss[0m : 9.61102
[1mStep[0m  [8/26], [94mLoss[0m : 9.81223
[1mStep[0m  [10/26], [94mLoss[0m : 9.77497
[1mStep[0m  [12/26], [94mLoss[0m : 9.51045
[1mStep[0m  [14/26], [94mLoss[0m : 9.49601
[1mStep[0m  [16/26], [94mLoss[0m : 9.71572
[1mStep[0m  [18/26], [94mLoss[0m : 9.48985
[1mStep[0m  [20/26], [94mLoss[0m : 9.81247
[1mStep[0m  [22/26], [94mLoss[0m : 9.44342
[1mStep[0m  [24/26], [94mLoss[0m : 9.39578

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.550, [92mTest[0m: 9.828, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.70475
[1mStep[0m  [2/26], [94mLoss[0m : 9.48187
[1mStep[0m  [4/26], [94mLoss[0m : 9.76322
[1mStep[0m  [6/26], [94mLoss[0m : 9.66704
[1mStep[0m  [8/26], [94mLoss[0m : 9.38588
[1mStep[0m  [10/26], [94mLoss[0m : 9.47825
[1mStep[0m  [12/26], [94mLoss[0m : 9.50552
[1mStep[0m  [14/26], [94mLoss[0m : 9.44304
[1mStep[0m  [16/26], [94mLoss[0m : 9.48663
[1mStep[0m  [18/26], [94mLoss[0m : 9.61120
[1mStep[0m  [20/26], [94mLoss[0m : 9.50660
[1mStep[0m  [22/26], [94mLoss[0m : 9.53302
[1mStep[0m  [24/26], [94mLoss[0m : 9.52034

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.515, [92mTest[0m: 9.816, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.767
====================================

Phase 1 - Evaluation MAE:  9.767308235168457
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 9.65618
[1mStep[0m  [2/26], [94mLoss[0m : 9.48820
[1mStep[0m  [4/26], [94mLoss[0m : 9.28565
[1mStep[0m  [6/26], [94mLoss[0m : 9.12498
[1mStep[0m  [8/26], [94mLoss[0m : 9.50463
[1mStep[0m  [10/26], [94mLoss[0m : 9.52590
[1mStep[0m  [12/26], [94mLoss[0m : 9.55326
[1mStep[0m  [14/26], [94mLoss[0m : 9.27574
[1mStep[0m  [16/26], [94mLoss[0m : 9.48789
[1mStep[0m  [18/26], [94mLoss[0m : 9.40623
[1mStep[0m  [20/26], [94mLoss[0m : 9.58115
[1mStep[0m  [22/26], [94mLoss[0m : 9.21579
[1mStep[0m  [24/26], [94mLoss[0m : 9.14558

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.463, [92mTest[0m: 9.766, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.52483
[1mStep[0m  [2/26], [94mLoss[0m : 9.55679
[1mStep[0m  [4/26], [94mLoss[0m : 9.32687
[1mStep[0m  [6/26], [94mLoss[0m : 9.28095
[1mStep[0m  [8/26], [94mLoss[0m : 9.49004
[1mStep[0m  [10/26], [94mLoss[0m : 9.42672
[1mStep[0m  [12/26], [94mLoss[0m : 9.64795
[1mStep[0m  [14/26], [94mLoss[0m : 9.79866
[1mStep[0m  [16/26], [94mLoss[0m : 9.21466
[1mStep[0m  [18/26], [94mLoss[0m : 9.12050
[1mStep[0m  [20/26], [94mLoss[0m : 9.48622
[1mStep[0m  [22/26], [94mLoss[0m : 9.54783
[1mStep[0m  [24/26], [94mLoss[0m : 9.15316

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.428, [92mTest[0m: 9.712, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.44885
[1mStep[0m  [2/26], [94mLoss[0m : 9.22249
[1mStep[0m  [4/26], [94mLoss[0m : 9.48610
[1mStep[0m  [6/26], [94mLoss[0m : 9.50927
[1mStep[0m  [8/26], [94mLoss[0m : 9.56427
[1mStep[0m  [10/26], [94mLoss[0m : 9.21011
[1mStep[0m  [12/26], [94mLoss[0m : 9.31852
[1mStep[0m  [14/26], [94mLoss[0m : 9.29016
[1mStep[0m  [16/26], [94mLoss[0m : 9.33116
[1mStep[0m  [18/26], [94mLoss[0m : 9.48918
[1mStep[0m  [20/26], [94mLoss[0m : 9.27534
[1mStep[0m  [22/26], [94mLoss[0m : 9.41239
[1mStep[0m  [24/26], [94mLoss[0m : 9.41144

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.375, [92mTest[0m: 9.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.35843
[1mStep[0m  [2/26], [94mLoss[0m : 9.28804
[1mStep[0m  [4/26], [94mLoss[0m : 9.35707
[1mStep[0m  [6/26], [94mLoss[0m : 9.12525
[1mStep[0m  [8/26], [94mLoss[0m : 9.54197
[1mStep[0m  [10/26], [94mLoss[0m : 9.18476
[1mStep[0m  [12/26], [94mLoss[0m : 9.42259
[1mStep[0m  [14/26], [94mLoss[0m : 9.26235
[1mStep[0m  [16/26], [94mLoss[0m : 9.63741
[1mStep[0m  [18/26], [94mLoss[0m : 9.37789
[1mStep[0m  [20/26], [94mLoss[0m : 9.44533
[1mStep[0m  [22/26], [94mLoss[0m : 9.13981
[1mStep[0m  [24/26], [94mLoss[0m : 9.08117

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.320, [92mTest[0m: 9.647, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.07992
[1mStep[0m  [2/26], [94mLoss[0m : 9.08863
[1mStep[0m  [4/26], [94mLoss[0m : 9.23193
[1mStep[0m  [6/26], [94mLoss[0m : 9.03362
[1mStep[0m  [8/26], [94mLoss[0m : 9.31786
[1mStep[0m  [10/26], [94mLoss[0m : 9.21004
[1mStep[0m  [12/26], [94mLoss[0m : 9.24590
[1mStep[0m  [14/26], [94mLoss[0m : 9.23290
[1mStep[0m  [16/26], [94mLoss[0m : 9.16526
[1mStep[0m  [18/26], [94mLoss[0m : 9.11387
[1mStep[0m  [20/26], [94mLoss[0m : 9.35844
[1mStep[0m  [22/26], [94mLoss[0m : 9.17285
[1mStep[0m  [24/26], [94mLoss[0m : 9.21112

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.271, [92mTest[0m: 9.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.19821
[1mStep[0m  [2/26], [94mLoss[0m : 9.23270
[1mStep[0m  [4/26], [94mLoss[0m : 9.21102
[1mStep[0m  [6/26], [94mLoss[0m : 9.02393
[1mStep[0m  [8/26], [94mLoss[0m : 9.25544
[1mStep[0m  [10/26], [94mLoss[0m : 9.14544
[1mStep[0m  [12/26], [94mLoss[0m : 9.12526
[1mStep[0m  [14/26], [94mLoss[0m : 9.47340
[1mStep[0m  [16/26], [94mLoss[0m : 9.01075
[1mStep[0m  [18/26], [94mLoss[0m : 9.30144
[1mStep[0m  [20/26], [94mLoss[0m : 9.27912
[1mStep[0m  [22/26], [94mLoss[0m : 9.14365
[1mStep[0m  [24/26], [94mLoss[0m : 9.28040

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.230, [92mTest[0m: 9.554, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.29651
[1mStep[0m  [2/26], [94mLoss[0m : 9.18400
[1mStep[0m  [4/26], [94mLoss[0m : 9.04284
[1mStep[0m  [6/26], [94mLoss[0m : 9.08802
[1mStep[0m  [8/26], [94mLoss[0m : 9.05970
[1mStep[0m  [10/26], [94mLoss[0m : 9.19758
[1mStep[0m  [12/26], [94mLoss[0m : 9.01490
[1mStep[0m  [14/26], [94mLoss[0m : 9.08370
[1mStep[0m  [16/26], [94mLoss[0m : 9.40603
[1mStep[0m  [18/26], [94mLoss[0m : 9.13145
[1mStep[0m  [20/26], [94mLoss[0m : 9.34944
[1mStep[0m  [22/26], [94mLoss[0m : 9.24758
[1mStep[0m  [24/26], [94mLoss[0m : 9.03739

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.179, [92mTest[0m: 9.501, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.37521
[1mStep[0m  [2/26], [94mLoss[0m : 9.05870
[1mStep[0m  [4/26], [94mLoss[0m : 9.17437
[1mStep[0m  [6/26], [94mLoss[0m : 9.17923
[1mStep[0m  [8/26], [94mLoss[0m : 9.06654
[1mStep[0m  [10/26], [94mLoss[0m : 8.98774
[1mStep[0m  [12/26], [94mLoss[0m : 9.02666
[1mStep[0m  [14/26], [94mLoss[0m : 9.22580
[1mStep[0m  [16/26], [94mLoss[0m : 9.30209
[1mStep[0m  [18/26], [94mLoss[0m : 9.18081
[1mStep[0m  [20/26], [94mLoss[0m : 9.05857
[1mStep[0m  [22/26], [94mLoss[0m : 9.18633
[1mStep[0m  [24/26], [94mLoss[0m : 9.01632

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.133, [92mTest[0m: 9.461, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.99733
[1mStep[0m  [2/26], [94mLoss[0m : 8.87745
[1mStep[0m  [4/26], [94mLoss[0m : 9.10061
[1mStep[0m  [6/26], [94mLoss[0m : 9.33346
[1mStep[0m  [8/26], [94mLoss[0m : 8.99933
[1mStep[0m  [10/26], [94mLoss[0m : 9.14699
[1mStep[0m  [12/26], [94mLoss[0m : 9.02323
[1mStep[0m  [14/26], [94mLoss[0m : 9.13744
[1mStep[0m  [16/26], [94mLoss[0m : 9.16080
[1mStep[0m  [18/26], [94mLoss[0m : 8.72065
[1mStep[0m  [20/26], [94mLoss[0m : 8.76621
[1mStep[0m  [22/26], [94mLoss[0m : 8.75386
[1mStep[0m  [24/26], [94mLoss[0m : 8.79905

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.092, [92mTest[0m: 9.425, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.03202
[1mStep[0m  [2/26], [94mLoss[0m : 9.29908
[1mStep[0m  [4/26], [94mLoss[0m : 9.01837
[1mStep[0m  [6/26], [94mLoss[0m : 9.15302
[1mStep[0m  [8/26], [94mLoss[0m : 9.11234
[1mStep[0m  [10/26], [94mLoss[0m : 8.88094
[1mStep[0m  [12/26], [94mLoss[0m : 8.80547
[1mStep[0m  [14/26], [94mLoss[0m : 9.05995
[1mStep[0m  [16/26], [94mLoss[0m : 8.94224
[1mStep[0m  [18/26], [94mLoss[0m : 9.01639
[1mStep[0m  [20/26], [94mLoss[0m : 9.03662
[1mStep[0m  [22/26], [94mLoss[0m : 9.24382
[1mStep[0m  [24/26], [94mLoss[0m : 8.90943

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.039, [92mTest[0m: 9.381, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.21435
[1mStep[0m  [2/26], [94mLoss[0m : 9.27444
[1mStep[0m  [4/26], [94mLoss[0m : 9.12097
[1mStep[0m  [6/26], [94mLoss[0m : 8.80865
[1mStep[0m  [8/26], [94mLoss[0m : 9.09793
[1mStep[0m  [10/26], [94mLoss[0m : 8.97670
[1mStep[0m  [12/26], [94mLoss[0m : 8.94256
[1mStep[0m  [14/26], [94mLoss[0m : 9.19176
[1mStep[0m  [16/26], [94mLoss[0m : 8.86563
[1mStep[0m  [18/26], [94mLoss[0m : 9.04037
[1mStep[0m  [20/26], [94mLoss[0m : 9.23060
[1mStep[0m  [22/26], [94mLoss[0m : 9.06063
[1mStep[0m  [24/26], [94mLoss[0m : 8.96326

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.999, [92mTest[0m: 9.348, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.91430
[1mStep[0m  [2/26], [94mLoss[0m : 9.00277
[1mStep[0m  [4/26], [94mLoss[0m : 8.93335
[1mStep[0m  [6/26], [94mLoss[0m : 8.81839
[1mStep[0m  [8/26], [94mLoss[0m : 9.26445
[1mStep[0m  [10/26], [94mLoss[0m : 9.00415
[1mStep[0m  [12/26], [94mLoss[0m : 9.09360
[1mStep[0m  [14/26], [94mLoss[0m : 8.94592
[1mStep[0m  [16/26], [94mLoss[0m : 8.82381
[1mStep[0m  [18/26], [94mLoss[0m : 9.19526
[1mStep[0m  [20/26], [94mLoss[0m : 8.86622
[1mStep[0m  [22/26], [94mLoss[0m : 8.71438
[1mStep[0m  [24/26], [94mLoss[0m : 8.60015

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.950, [92mTest[0m: 9.312, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.94206
[1mStep[0m  [2/26], [94mLoss[0m : 8.70031
[1mStep[0m  [4/26], [94mLoss[0m : 8.96924
[1mStep[0m  [6/26], [94mLoss[0m : 9.03800
[1mStep[0m  [8/26], [94mLoss[0m : 8.70549
[1mStep[0m  [10/26], [94mLoss[0m : 8.72731
[1mStep[0m  [12/26], [94mLoss[0m : 8.97713
[1mStep[0m  [14/26], [94mLoss[0m : 8.97285
[1mStep[0m  [16/26], [94mLoss[0m : 9.10795
[1mStep[0m  [18/26], [94mLoss[0m : 9.25376
[1mStep[0m  [20/26], [94mLoss[0m : 8.95374
[1mStep[0m  [22/26], [94mLoss[0m : 9.22981
[1mStep[0m  [24/26], [94mLoss[0m : 8.77878

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.903, [92mTest[0m: 9.267, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.78369
[1mStep[0m  [2/26], [94mLoss[0m : 8.96475
[1mStep[0m  [4/26], [94mLoss[0m : 8.67310
[1mStep[0m  [6/26], [94mLoss[0m : 8.83685
[1mStep[0m  [8/26], [94mLoss[0m : 8.77732
[1mStep[0m  [10/26], [94mLoss[0m : 9.05646
[1mStep[0m  [12/26], [94mLoss[0m : 8.82407
[1mStep[0m  [14/26], [94mLoss[0m : 8.97721
[1mStep[0m  [16/26], [94mLoss[0m : 8.80967
[1mStep[0m  [18/26], [94mLoss[0m : 8.69900
[1mStep[0m  [20/26], [94mLoss[0m : 8.82637
[1mStep[0m  [22/26], [94mLoss[0m : 8.75230
[1mStep[0m  [24/26], [94mLoss[0m : 8.72105

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.850, [92mTest[0m: 9.217, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.97465
[1mStep[0m  [2/26], [94mLoss[0m : 8.58267
[1mStep[0m  [4/26], [94mLoss[0m : 8.76659
[1mStep[0m  [6/26], [94mLoss[0m : 8.73543
[1mStep[0m  [8/26], [94mLoss[0m : 8.90697
[1mStep[0m  [10/26], [94mLoss[0m : 8.83681
[1mStep[0m  [12/26], [94mLoss[0m : 8.85883
[1mStep[0m  [14/26], [94mLoss[0m : 8.92691
[1mStep[0m  [16/26], [94mLoss[0m : 8.69068
[1mStep[0m  [18/26], [94mLoss[0m : 8.81236
[1mStep[0m  [20/26], [94mLoss[0m : 8.57621
[1mStep[0m  [22/26], [94mLoss[0m : 8.98885
[1mStep[0m  [24/26], [94mLoss[0m : 8.85480

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.808, [92mTest[0m: 9.174, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.76362
[1mStep[0m  [2/26], [94mLoss[0m : 8.72317
[1mStep[0m  [4/26], [94mLoss[0m : 8.80625
[1mStep[0m  [6/26], [94mLoss[0m : 8.89972
[1mStep[0m  [8/26], [94mLoss[0m : 8.64897
[1mStep[0m  [10/26], [94mLoss[0m : 8.39073
[1mStep[0m  [12/26], [94mLoss[0m : 8.84737
[1mStep[0m  [14/26], [94mLoss[0m : 8.89301
[1mStep[0m  [16/26], [94mLoss[0m : 8.28047
[1mStep[0m  [18/26], [94mLoss[0m : 8.89374
[1mStep[0m  [20/26], [94mLoss[0m : 9.03033
[1mStep[0m  [22/26], [94mLoss[0m : 8.67959
[1mStep[0m  [24/26], [94mLoss[0m : 8.50605

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.753, [92mTest[0m: 9.136, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.51606
[1mStep[0m  [2/26], [94mLoss[0m : 8.97888
[1mStep[0m  [4/26], [94mLoss[0m : 8.74587
[1mStep[0m  [6/26], [94mLoss[0m : 8.85540
[1mStep[0m  [8/26], [94mLoss[0m : 8.42665
[1mStep[0m  [10/26], [94mLoss[0m : 8.72346
[1mStep[0m  [12/26], [94mLoss[0m : 8.84520
[1mStep[0m  [14/26], [94mLoss[0m : 8.57797
[1mStep[0m  [16/26], [94mLoss[0m : 8.57287
[1mStep[0m  [18/26], [94mLoss[0m : 9.00934
[1mStep[0m  [20/26], [94mLoss[0m : 8.79890
[1mStep[0m  [22/26], [94mLoss[0m : 8.60441
[1mStep[0m  [24/26], [94mLoss[0m : 8.60884

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.702, [92mTest[0m: 9.087, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.61690
[1mStep[0m  [2/26], [94mLoss[0m : 8.27888
[1mStep[0m  [4/26], [94mLoss[0m : 8.78169
[1mStep[0m  [6/26], [94mLoss[0m : 8.56617
[1mStep[0m  [8/26], [94mLoss[0m : 8.74986
[1mStep[0m  [10/26], [94mLoss[0m : 8.75226
[1mStep[0m  [12/26], [94mLoss[0m : 8.37241
[1mStep[0m  [14/26], [94mLoss[0m : 8.38611
[1mStep[0m  [16/26], [94mLoss[0m : 8.80032
[1mStep[0m  [18/26], [94mLoss[0m : 8.90969
[1mStep[0m  [20/26], [94mLoss[0m : 8.47933
[1mStep[0m  [22/26], [94mLoss[0m : 8.63411
[1mStep[0m  [24/26], [94mLoss[0m : 8.81327

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.660, [92mTest[0m: 9.050, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.72688
[1mStep[0m  [2/26], [94mLoss[0m : 8.74962
[1mStep[0m  [4/26], [94mLoss[0m : 8.50662
[1mStep[0m  [6/26], [94mLoss[0m : 8.58690
[1mStep[0m  [8/26], [94mLoss[0m : 8.63121
[1mStep[0m  [10/26], [94mLoss[0m : 8.97035
[1mStep[0m  [12/26], [94mLoss[0m : 8.79050
[1mStep[0m  [14/26], [94mLoss[0m : 8.38429
[1mStep[0m  [16/26], [94mLoss[0m : 8.78827
[1mStep[0m  [18/26], [94mLoss[0m : 8.46575
[1mStep[0m  [20/26], [94mLoss[0m : 8.37398
[1mStep[0m  [22/26], [94mLoss[0m : 8.50967
[1mStep[0m  [24/26], [94mLoss[0m : 8.54287

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.607, [92mTest[0m: 9.008, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.38447
[1mStep[0m  [2/26], [94mLoss[0m : 8.20545
[1mStep[0m  [4/26], [94mLoss[0m : 8.68778
[1mStep[0m  [6/26], [94mLoss[0m : 8.54556
[1mStep[0m  [8/26], [94mLoss[0m : 8.58189
[1mStep[0m  [10/26], [94mLoss[0m : 8.59051
[1mStep[0m  [12/26], [94mLoss[0m : 8.48949
[1mStep[0m  [14/26], [94mLoss[0m : 8.39140
[1mStep[0m  [16/26], [94mLoss[0m : 8.40853
[1mStep[0m  [18/26], [94mLoss[0m : 8.62727
[1mStep[0m  [20/26], [94mLoss[0m : 8.32644
[1mStep[0m  [22/26], [94mLoss[0m : 8.66002
[1mStep[0m  [24/26], [94mLoss[0m : 8.52317

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.560, [92mTest[0m: 8.966, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.35558
[1mStep[0m  [2/26], [94mLoss[0m : 8.79035
[1mStep[0m  [4/26], [94mLoss[0m : 8.53025
[1mStep[0m  [6/26], [94mLoss[0m : 8.49264
[1mStep[0m  [8/26], [94mLoss[0m : 8.58691
[1mStep[0m  [10/26], [94mLoss[0m : 8.44694
[1mStep[0m  [12/26], [94mLoss[0m : 8.36589
[1mStep[0m  [14/26], [94mLoss[0m : 8.62572
[1mStep[0m  [16/26], [94mLoss[0m : 8.27654
[1mStep[0m  [18/26], [94mLoss[0m : 8.48681
[1mStep[0m  [20/26], [94mLoss[0m : 8.28522
[1mStep[0m  [22/26], [94mLoss[0m : 8.43214
[1mStep[0m  [24/26], [94mLoss[0m : 8.63316

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.515, [92mTest[0m: 8.928, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.52650
[1mStep[0m  [2/26], [94mLoss[0m : 8.41358
[1mStep[0m  [4/26], [94mLoss[0m : 8.58301
[1mStep[0m  [6/26], [94mLoss[0m : 8.70751
[1mStep[0m  [8/26], [94mLoss[0m : 8.48166
[1mStep[0m  [10/26], [94mLoss[0m : 8.57194
[1mStep[0m  [12/26], [94mLoss[0m : 8.32643
[1mStep[0m  [14/26], [94mLoss[0m : 8.62865
[1mStep[0m  [16/26], [94mLoss[0m : 8.56806
[1mStep[0m  [18/26], [94mLoss[0m : 8.23776
[1mStep[0m  [20/26], [94mLoss[0m : 8.32815
[1mStep[0m  [22/26], [94mLoss[0m : 8.72659
[1mStep[0m  [24/26], [94mLoss[0m : 8.52868

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.474, [92mTest[0m: 8.885, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.45984
[1mStep[0m  [2/26], [94mLoss[0m : 8.45898
[1mStep[0m  [4/26], [94mLoss[0m : 8.61861
[1mStep[0m  [6/26], [94mLoss[0m : 8.78742
[1mStep[0m  [8/26], [94mLoss[0m : 8.47431
[1mStep[0m  [10/26], [94mLoss[0m : 8.67567
[1mStep[0m  [12/26], [94mLoss[0m : 8.00440
[1mStep[0m  [14/26], [94mLoss[0m : 8.28872
[1mStep[0m  [16/26], [94mLoss[0m : 8.41728
[1mStep[0m  [18/26], [94mLoss[0m : 8.50331
[1mStep[0m  [20/26], [94mLoss[0m : 8.37135
[1mStep[0m  [22/26], [94mLoss[0m : 8.58518
[1mStep[0m  [24/26], [94mLoss[0m : 8.74096

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.438, [92mTest[0m: 8.849, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.28381
[1mStep[0m  [2/26], [94mLoss[0m : 8.49029
[1mStep[0m  [4/26], [94mLoss[0m : 8.17400
[1mStep[0m  [6/26], [94mLoss[0m : 8.59021
[1mStep[0m  [8/26], [94mLoss[0m : 8.30820
[1mStep[0m  [10/26], [94mLoss[0m : 8.43709
[1mStep[0m  [12/26], [94mLoss[0m : 8.19922
[1mStep[0m  [14/26], [94mLoss[0m : 8.26804
[1mStep[0m  [16/26], [94mLoss[0m : 8.65766
[1mStep[0m  [18/26], [94mLoss[0m : 8.32619
[1mStep[0m  [20/26], [94mLoss[0m : 8.44209
[1mStep[0m  [22/26], [94mLoss[0m : 8.43573
[1mStep[0m  [24/26], [94mLoss[0m : 8.35427

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.387, [92mTest[0m: 8.795, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.15073
[1mStep[0m  [2/26], [94mLoss[0m : 8.24189
[1mStep[0m  [4/26], [94mLoss[0m : 8.69957
[1mStep[0m  [6/26], [94mLoss[0m : 8.17233
[1mStep[0m  [8/26], [94mLoss[0m : 8.43503
[1mStep[0m  [10/26], [94mLoss[0m : 8.51841
[1mStep[0m  [12/26], [94mLoss[0m : 8.34168
[1mStep[0m  [14/26], [94mLoss[0m : 8.45280
[1mStep[0m  [16/26], [94mLoss[0m : 8.26085
[1mStep[0m  [18/26], [94mLoss[0m : 8.37708
[1mStep[0m  [20/26], [94mLoss[0m : 8.04536
[1mStep[0m  [22/26], [94mLoss[0m : 8.33783
[1mStep[0m  [24/26], [94mLoss[0m : 8.55894

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.347, [92mTest[0m: 8.781, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.32374
[1mStep[0m  [2/26], [94mLoss[0m : 8.19633
[1mStep[0m  [4/26], [94mLoss[0m : 8.29218
[1mStep[0m  [6/26], [94mLoss[0m : 8.22725
[1mStep[0m  [8/26], [94mLoss[0m : 8.27779
[1mStep[0m  [10/26], [94mLoss[0m : 8.31273
[1mStep[0m  [12/26], [94mLoss[0m : 8.30329
[1mStep[0m  [14/26], [94mLoss[0m : 8.31143
[1mStep[0m  [16/26], [94mLoss[0m : 8.62292
[1mStep[0m  [18/26], [94mLoss[0m : 8.63502
[1mStep[0m  [20/26], [94mLoss[0m : 8.43711
[1mStep[0m  [22/26], [94mLoss[0m : 8.18711
[1mStep[0m  [24/26], [94mLoss[0m : 8.06661

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.305, [92mTest[0m: 8.728, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.09382
[1mStep[0m  [2/26], [94mLoss[0m : 8.32533
[1mStep[0m  [4/26], [94mLoss[0m : 8.17978
[1mStep[0m  [6/26], [94mLoss[0m : 8.53484
[1mStep[0m  [8/26], [94mLoss[0m : 8.16976
[1mStep[0m  [10/26], [94mLoss[0m : 8.29878
[1mStep[0m  [12/26], [94mLoss[0m : 8.52807
[1mStep[0m  [14/26], [94mLoss[0m : 8.33721
[1mStep[0m  [16/26], [94mLoss[0m : 8.11643
[1mStep[0m  [18/26], [94mLoss[0m : 8.28911
[1mStep[0m  [20/26], [94mLoss[0m : 8.03421
[1mStep[0m  [22/26], [94mLoss[0m : 8.35420
[1mStep[0m  [24/26], [94mLoss[0m : 8.17478

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.258, [92mTest[0m: 8.702, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.21241
[1mStep[0m  [2/26], [94mLoss[0m : 8.05083
[1mStep[0m  [4/26], [94mLoss[0m : 8.12150
[1mStep[0m  [6/26], [94mLoss[0m : 8.09072
[1mStep[0m  [8/26], [94mLoss[0m : 8.31817
[1mStep[0m  [10/26], [94mLoss[0m : 8.39675
[1mStep[0m  [12/26], [94mLoss[0m : 7.95039
[1mStep[0m  [14/26], [94mLoss[0m : 8.43250
[1mStep[0m  [16/26], [94mLoss[0m : 8.17271
[1mStep[0m  [18/26], [94mLoss[0m : 8.51018
[1mStep[0m  [20/26], [94mLoss[0m : 8.21263
[1mStep[0m  [22/26], [94mLoss[0m : 8.43909
[1mStep[0m  [24/26], [94mLoss[0m : 8.15133

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.221, [92mTest[0m: 8.658, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.29511
[1mStep[0m  [2/26], [94mLoss[0m : 8.17390
[1mStep[0m  [4/26], [94mLoss[0m : 8.31737
[1mStep[0m  [6/26], [94mLoss[0m : 8.26867
[1mStep[0m  [8/26], [94mLoss[0m : 8.42400
[1mStep[0m  [10/26], [94mLoss[0m : 8.08245
[1mStep[0m  [12/26], [94mLoss[0m : 8.31054
[1mStep[0m  [14/26], [94mLoss[0m : 8.34564
[1mStep[0m  [16/26], [94mLoss[0m : 8.40280
[1mStep[0m  [18/26], [94mLoss[0m : 7.90651
[1mStep[0m  [20/26], [94mLoss[0m : 8.22612
[1mStep[0m  [22/26], [94mLoss[0m : 8.01512
[1mStep[0m  [24/26], [94mLoss[0m : 8.26346

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.170, [92mTest[0m: 8.618, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.23152
[1mStep[0m  [2/26], [94mLoss[0m : 8.18490
[1mStep[0m  [4/26], [94mLoss[0m : 8.51243
[1mStep[0m  [6/26], [94mLoss[0m : 8.09901
[1mStep[0m  [8/26], [94mLoss[0m : 8.09810
[1mStep[0m  [10/26], [94mLoss[0m : 7.94137
[1mStep[0m  [12/26], [94mLoss[0m : 7.93880
[1mStep[0m  [14/26], [94mLoss[0m : 8.13668
[1mStep[0m  [16/26], [94mLoss[0m : 7.90541
[1mStep[0m  [18/26], [94mLoss[0m : 8.30217
[1mStep[0m  [20/26], [94mLoss[0m : 8.01719
[1mStep[0m  [22/26], [94mLoss[0m : 8.25009
[1mStep[0m  [24/26], [94mLoss[0m : 8.25113

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.135, [92mTest[0m: 8.578, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.549
====================================

Phase 2 - Evaluation MAE:  8.549034412090595
MAE score P1      9.767308
MAE score P2      8.549034
loss                8.1352
learning_rate       0.0001
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay          0.01
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 10.97576
[1mStep[0m  [2/26], [94mLoss[0m : 10.72555
[1mStep[0m  [4/26], [94mLoss[0m : 11.00335
[1mStep[0m  [6/26], [94mLoss[0m : 10.75364
[1mStep[0m  [8/26], [94mLoss[0m : 10.93715
[1mStep[0m  [10/26], [94mLoss[0m : 10.46794
[1mStep[0m  [12/26], [94mLoss[0m : 10.63442
[1mStep[0m  [14/26], [94mLoss[0m : 10.68871
[1mStep[0m  [16/26], [94mLoss[0m : 10.87408
[1mStep[0m  [18/26], [94mLoss[0m : 10.80109
[1mStep[0m  [20/26], [94mLoss[0m : 10.63039
[1mStep[0m  [22/26], [94mLoss[0m : 10.74758
[1mStep[0m  [24/26], [94mLoss[0m : 10.80577

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.759, [92mTest[0m: 10.807, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 11.01083
[1mStep[0m  [2/26], [94mLoss[0m : 10.71798
[1mStep[0m  [4/26], [94mLoss[0m : 10.56476
[1mStep[0m  [6/26], [94mLoss[0m : 10.53184
[1mStep[0m  [8/26], [94mLoss[0m : 10.47265
[1mStep[0m  [10/26], [94mLoss[0m : 10.65778
[1mStep[0m  [12/26], [94mLoss[0m : 10.63066
[1mStep[0m  [14/26], [94mLoss[0m : 10.75578
[1mStep[0m  [16/26], [94mLoss[0m : 10.25714
[1mStep[0m  [18/26], [94mLoss[0m : 10.40723
[1mStep[0m  [20/26], [94mLoss[0m : 10.51383
[1mStep[0m  [22/26], [94mLoss[0m : 10.50472
[1mStep[0m  [24/26], [94mLoss[0m : 10.70476

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.538, [92mTest[0m: 10.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.42940
[1mStep[0m  [2/26], [94mLoss[0m : 10.38420
[1mStep[0m  [4/26], [94mLoss[0m : 10.47557
[1mStep[0m  [6/26], [94mLoss[0m : 10.35863
[1mStep[0m  [8/26], [94mLoss[0m : 10.36909
[1mStep[0m  [10/26], [94mLoss[0m : 10.17752
[1mStep[0m  [12/26], [94mLoss[0m : 10.27909
[1mStep[0m  [14/26], [94mLoss[0m : 10.47046
[1mStep[0m  [16/26], [94mLoss[0m : 10.14244
[1mStep[0m  [18/26], [94mLoss[0m : 10.24247
[1mStep[0m  [20/26], [94mLoss[0m : 10.29047
[1mStep[0m  [22/26], [94mLoss[0m : 10.36638
[1mStep[0m  [24/26], [94mLoss[0m : 10.14427

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.289, [92mTest[0m: 10.452, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 10.05394
[1mStep[0m  [2/26], [94mLoss[0m : 9.91947
[1mStep[0m  [4/26], [94mLoss[0m : 10.08753
[1mStep[0m  [6/26], [94mLoss[0m : 10.13362
[1mStep[0m  [8/26], [94mLoss[0m : 10.29932
[1mStep[0m  [10/26], [94mLoss[0m : 10.08861
[1mStep[0m  [12/26], [94mLoss[0m : 9.92683
[1mStep[0m  [14/26], [94mLoss[0m : 9.88938
[1mStep[0m  [16/26], [94mLoss[0m : 9.55949
[1mStep[0m  [18/26], [94mLoss[0m : 9.87675
[1mStep[0m  [20/26], [94mLoss[0m : 9.80961
[1mStep[0m  [22/26], [94mLoss[0m : 9.88831
[1mStep[0m  [24/26], [94mLoss[0m : 9.91940

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.042, [92mTest[0m: 10.252, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.92595
[1mStep[0m  [2/26], [94mLoss[0m : 9.95922
[1mStep[0m  [4/26], [94mLoss[0m : 9.77472
[1mStep[0m  [6/26], [94mLoss[0m : 9.99740
[1mStep[0m  [8/26], [94mLoss[0m : 9.96051
[1mStep[0m  [10/26], [94mLoss[0m : 9.84054
[1mStep[0m  [12/26], [94mLoss[0m : 9.80424
[1mStep[0m  [14/26], [94mLoss[0m : 9.55704
[1mStep[0m  [16/26], [94mLoss[0m : 9.61486
[1mStep[0m  [18/26], [94mLoss[0m : 9.75495
[1mStep[0m  [20/26], [94mLoss[0m : 9.75399
[1mStep[0m  [22/26], [94mLoss[0m : 9.79023
[1mStep[0m  [24/26], [94mLoss[0m : 9.71231

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.813, [92mTest[0m: 10.067, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.61362
[1mStep[0m  [2/26], [94mLoss[0m : 9.85673
[1mStep[0m  [4/26], [94mLoss[0m : 9.66040
[1mStep[0m  [6/26], [94mLoss[0m : 9.57461
[1mStep[0m  [8/26], [94mLoss[0m : 9.62077
[1mStep[0m  [10/26], [94mLoss[0m : 9.57325
[1mStep[0m  [12/26], [94mLoss[0m : 9.58212
[1mStep[0m  [14/26], [94mLoss[0m : 9.57178
[1mStep[0m  [16/26], [94mLoss[0m : 9.45339
[1mStep[0m  [18/26], [94mLoss[0m : 9.36696
[1mStep[0m  [20/26], [94mLoss[0m : 9.54936
[1mStep[0m  [22/26], [94mLoss[0m : 9.60413
[1mStep[0m  [24/26], [94mLoss[0m : 9.45261

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.582, [92mTest[0m: 9.869, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.49834
[1mStep[0m  [2/26], [94mLoss[0m : 9.73895
[1mStep[0m  [4/26], [94mLoss[0m : 9.36307
[1mStep[0m  [6/26], [94mLoss[0m : 9.29447
[1mStep[0m  [8/26], [94mLoss[0m : 9.19390
[1mStep[0m  [10/26], [94mLoss[0m : 9.27959
[1mStep[0m  [12/26], [94mLoss[0m : 9.28711
[1mStep[0m  [14/26], [94mLoss[0m : 9.12289
[1mStep[0m  [16/26], [94mLoss[0m : 9.27951
[1mStep[0m  [18/26], [94mLoss[0m : 9.42563
[1mStep[0m  [20/26], [94mLoss[0m : 9.23563
[1mStep[0m  [22/26], [94mLoss[0m : 9.42390
[1mStep[0m  [24/26], [94mLoss[0m : 8.92051

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.348, [92mTest[0m: 9.673, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.26066
[1mStep[0m  [2/26], [94mLoss[0m : 9.23910
[1mStep[0m  [4/26], [94mLoss[0m : 9.08863
[1mStep[0m  [6/26], [94mLoss[0m : 9.21467
[1mStep[0m  [8/26], [94mLoss[0m : 8.96082
[1mStep[0m  [10/26], [94mLoss[0m : 9.12764
[1mStep[0m  [12/26], [94mLoss[0m : 9.25139
[1mStep[0m  [14/26], [94mLoss[0m : 9.04643
[1mStep[0m  [16/26], [94mLoss[0m : 9.05901
[1mStep[0m  [18/26], [94mLoss[0m : 9.06229
[1mStep[0m  [20/26], [94mLoss[0m : 9.05134
[1mStep[0m  [22/26], [94mLoss[0m : 8.85403
[1mStep[0m  [24/26], [94mLoss[0m : 9.11373

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.105, [92mTest[0m: 9.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 9.17084
[1mStep[0m  [2/26], [94mLoss[0m : 8.91314
[1mStep[0m  [4/26], [94mLoss[0m : 8.85776
[1mStep[0m  [6/26], [94mLoss[0m : 9.07272
[1mStep[0m  [8/26], [94mLoss[0m : 8.96959
[1mStep[0m  [10/26], [94mLoss[0m : 8.87118
[1mStep[0m  [12/26], [94mLoss[0m : 8.25844
[1mStep[0m  [14/26], [94mLoss[0m : 8.76700
[1mStep[0m  [16/26], [94mLoss[0m : 8.50344
[1mStep[0m  [18/26], [94mLoss[0m : 8.90022
[1mStep[0m  [20/26], [94mLoss[0m : 8.81968
[1mStep[0m  [22/26], [94mLoss[0m : 9.09712
[1mStep[0m  [24/26], [94mLoss[0m : 8.78166

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.871, [92mTest[0m: 9.268, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.68086
[1mStep[0m  [2/26], [94mLoss[0m : 8.76419
[1mStep[0m  [4/26], [94mLoss[0m : 8.65294
[1mStep[0m  [6/26], [94mLoss[0m : 8.78652
[1mStep[0m  [8/26], [94mLoss[0m : 8.85314
[1mStep[0m  [10/26], [94mLoss[0m : 8.66543
[1mStep[0m  [12/26], [94mLoss[0m : 8.56675
[1mStep[0m  [14/26], [94mLoss[0m : 8.56367
[1mStep[0m  [16/26], [94mLoss[0m : 8.56979
[1mStep[0m  [18/26], [94mLoss[0m : 8.97142
[1mStep[0m  [20/26], [94mLoss[0m : 8.52339
[1mStep[0m  [22/26], [94mLoss[0m : 8.63268
[1mStep[0m  [24/26], [94mLoss[0m : 8.41108

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.629, [92mTest[0m: 9.070, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.38874
[1mStep[0m  [2/26], [94mLoss[0m : 8.55014
[1mStep[0m  [4/26], [94mLoss[0m : 8.42795
[1mStep[0m  [6/26], [94mLoss[0m : 8.15086
[1mStep[0m  [8/26], [94mLoss[0m : 8.50719
[1mStep[0m  [10/26], [94mLoss[0m : 8.31167
[1mStep[0m  [12/26], [94mLoss[0m : 8.37877
[1mStep[0m  [14/26], [94mLoss[0m : 8.41047
[1mStep[0m  [16/26], [94mLoss[0m : 8.60654
[1mStep[0m  [18/26], [94mLoss[0m : 8.07969
[1mStep[0m  [20/26], [94mLoss[0m : 8.49259
[1mStep[0m  [22/26], [94mLoss[0m : 8.31748
[1mStep[0m  [24/26], [94mLoss[0m : 8.38575

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.398, [92mTest[0m: 8.880, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 8.47503
[1mStep[0m  [2/26], [94mLoss[0m : 8.06054
[1mStep[0m  [4/26], [94mLoss[0m : 8.34069
[1mStep[0m  [6/26], [94mLoss[0m : 8.39295
[1mStep[0m  [8/26], [94mLoss[0m : 7.78801
[1mStep[0m  [10/26], [94mLoss[0m : 8.02906
[1mStep[0m  [12/26], [94mLoss[0m : 7.92098
[1mStep[0m  [14/26], [94mLoss[0m : 8.01745
[1mStep[0m  [16/26], [94mLoss[0m : 8.14490
[1mStep[0m  [18/26], [94mLoss[0m : 8.19022
[1mStep[0m  [20/26], [94mLoss[0m : 7.97788
[1mStep[0m  [22/26], [94mLoss[0m : 7.95237
[1mStep[0m  [24/26], [94mLoss[0m : 8.15626

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.154, [92mTest[0m: 8.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.81499
[1mStep[0m  [2/26], [94mLoss[0m : 7.72890
[1mStep[0m  [4/26], [94mLoss[0m : 7.93487
[1mStep[0m  [6/26], [94mLoss[0m : 7.79026
[1mStep[0m  [8/26], [94mLoss[0m : 7.91182
[1mStep[0m  [10/26], [94mLoss[0m : 7.93493
[1mStep[0m  [12/26], [94mLoss[0m : 8.17642
[1mStep[0m  [14/26], [94mLoss[0m : 8.28045
[1mStep[0m  [16/26], [94mLoss[0m : 7.52788
[1mStep[0m  [18/26], [94mLoss[0m : 8.22666
[1mStep[0m  [20/26], [94mLoss[0m : 7.81579
[1mStep[0m  [22/26], [94mLoss[0m : 8.09694
[1mStep[0m  [24/26], [94mLoss[0m : 7.99186

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.927, [92mTest[0m: 8.493, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.81217
[1mStep[0m  [2/26], [94mLoss[0m : 7.97803
[1mStep[0m  [4/26], [94mLoss[0m : 7.67915
[1mStep[0m  [6/26], [94mLoss[0m : 7.82083
[1mStep[0m  [8/26], [94mLoss[0m : 7.52192
[1mStep[0m  [10/26], [94mLoss[0m : 7.72021
[1mStep[0m  [12/26], [94mLoss[0m : 7.65287
[1mStep[0m  [14/26], [94mLoss[0m : 7.62605
[1mStep[0m  [16/26], [94mLoss[0m : 7.50299
[1mStep[0m  [18/26], [94mLoss[0m : 7.75976
[1mStep[0m  [20/26], [94mLoss[0m : 7.80348
[1mStep[0m  [22/26], [94mLoss[0m : 7.66565
[1mStep[0m  [24/26], [94mLoss[0m : 7.21904

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.696, [92mTest[0m: 8.301, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.24227
[1mStep[0m  [2/26], [94mLoss[0m : 7.56742
[1mStep[0m  [4/26], [94mLoss[0m : 7.32020
[1mStep[0m  [6/26], [94mLoss[0m : 7.53776
[1mStep[0m  [8/26], [94mLoss[0m : 7.65924
[1mStep[0m  [10/26], [94mLoss[0m : 7.65526
[1mStep[0m  [12/26], [94mLoss[0m : 7.54269
[1mStep[0m  [14/26], [94mLoss[0m : 7.36792
[1mStep[0m  [16/26], [94mLoss[0m : 7.38325
[1mStep[0m  [18/26], [94mLoss[0m : 7.46330
[1mStep[0m  [20/26], [94mLoss[0m : 7.44303
[1mStep[0m  [22/26], [94mLoss[0m : 7.42449
[1mStep[0m  [24/26], [94mLoss[0m : 7.32234

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.464, [92mTest[0m: 8.104, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.42391
[1mStep[0m  [2/26], [94mLoss[0m : 7.57785
[1mStep[0m  [4/26], [94mLoss[0m : 7.07572
[1mStep[0m  [6/26], [94mLoss[0m : 7.52855
[1mStep[0m  [8/26], [94mLoss[0m : 7.63042
[1mStep[0m  [10/26], [94mLoss[0m : 7.14734
[1mStep[0m  [12/26], [94mLoss[0m : 7.13328
[1mStep[0m  [14/26], [94mLoss[0m : 7.37957
[1mStep[0m  [16/26], [94mLoss[0m : 7.15172
[1mStep[0m  [18/26], [94mLoss[0m : 7.14502
[1mStep[0m  [20/26], [94mLoss[0m : 7.31085
[1mStep[0m  [22/26], [94mLoss[0m : 6.93286
[1mStep[0m  [24/26], [94mLoss[0m : 7.22025

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.251, [92mTest[0m: 7.891, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 7.06266
[1mStep[0m  [2/26], [94mLoss[0m : 7.27318
[1mStep[0m  [4/26], [94mLoss[0m : 7.13284
[1mStep[0m  [6/26], [94mLoss[0m : 7.20423
[1mStep[0m  [8/26], [94mLoss[0m : 7.26844
[1mStep[0m  [10/26], [94mLoss[0m : 7.13198
[1mStep[0m  [12/26], [94mLoss[0m : 7.18949
[1mStep[0m  [14/26], [94mLoss[0m : 6.84706
[1mStep[0m  [16/26], [94mLoss[0m : 7.17441
[1mStep[0m  [18/26], [94mLoss[0m : 7.10810
[1mStep[0m  [20/26], [94mLoss[0m : 6.98007
[1mStep[0m  [22/26], [94mLoss[0m : 6.85155
[1mStep[0m  [24/26], [94mLoss[0m : 6.81640

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.021, [92mTest[0m: 7.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.79088
[1mStep[0m  [2/26], [94mLoss[0m : 6.69694
[1mStep[0m  [4/26], [94mLoss[0m : 6.76341
[1mStep[0m  [6/26], [94mLoss[0m : 6.97433
[1mStep[0m  [8/26], [94mLoss[0m : 6.61714
[1mStep[0m  [10/26], [94mLoss[0m : 7.04259
[1mStep[0m  [12/26], [94mLoss[0m : 6.64278
[1mStep[0m  [14/26], [94mLoss[0m : 6.60354
[1mStep[0m  [16/26], [94mLoss[0m : 6.82489
[1mStep[0m  [18/26], [94mLoss[0m : 6.89674
[1mStep[0m  [20/26], [94mLoss[0m : 6.77167
[1mStep[0m  [22/26], [94mLoss[0m : 6.72338
[1mStep[0m  [24/26], [94mLoss[0m : 6.96681

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.801, [92mTest[0m: 7.546, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.63650
[1mStep[0m  [2/26], [94mLoss[0m : 6.59649
[1mStep[0m  [4/26], [94mLoss[0m : 6.60535
[1mStep[0m  [6/26], [94mLoss[0m : 6.58057
[1mStep[0m  [8/26], [94mLoss[0m : 6.64113
[1mStep[0m  [10/26], [94mLoss[0m : 6.63341
[1mStep[0m  [12/26], [94mLoss[0m : 6.61395
[1mStep[0m  [14/26], [94mLoss[0m : 6.57318
[1mStep[0m  [16/26], [94mLoss[0m : 6.68805
[1mStep[0m  [18/26], [94mLoss[0m : 6.50058
[1mStep[0m  [20/26], [94mLoss[0m : 6.56876
[1mStep[0m  [22/26], [94mLoss[0m : 6.18295
[1mStep[0m  [24/26], [94mLoss[0m : 6.67319

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.595, [92mTest[0m: 7.356, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.35245
[1mStep[0m  [2/26], [94mLoss[0m : 6.45425
[1mStep[0m  [4/26], [94mLoss[0m : 6.53186
[1mStep[0m  [6/26], [94mLoss[0m : 6.18728
[1mStep[0m  [8/26], [94mLoss[0m : 6.32739
[1mStep[0m  [10/26], [94mLoss[0m : 6.39187
[1mStep[0m  [12/26], [94mLoss[0m : 6.20007
[1mStep[0m  [14/26], [94mLoss[0m : 6.32801
[1mStep[0m  [16/26], [94mLoss[0m : 6.30339
[1mStep[0m  [18/26], [94mLoss[0m : 6.49245
[1mStep[0m  [20/26], [94mLoss[0m : 6.43865
[1mStep[0m  [22/26], [94mLoss[0m : 6.45412
[1mStep[0m  [24/26], [94mLoss[0m : 6.28616

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.386, [92mTest[0m: 7.165, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.62001
[1mStep[0m  [2/26], [94mLoss[0m : 6.09413
[1mStep[0m  [4/26], [94mLoss[0m : 6.07831
[1mStep[0m  [6/26], [94mLoss[0m : 6.49320
[1mStep[0m  [8/26], [94mLoss[0m : 5.98952
[1mStep[0m  [10/26], [94mLoss[0m : 6.19955
[1mStep[0m  [12/26], [94mLoss[0m : 6.12491
[1mStep[0m  [14/26], [94mLoss[0m : 5.94839
[1mStep[0m  [16/26], [94mLoss[0m : 6.14044
[1mStep[0m  [18/26], [94mLoss[0m : 6.17036
[1mStep[0m  [20/26], [94mLoss[0m : 6.13737
[1mStep[0m  [22/26], [94mLoss[0m : 5.81905
[1mStep[0m  [24/26], [94mLoss[0m : 6.18713

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.207, [92mTest[0m: 6.981, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.19625
[1mStep[0m  [2/26], [94mLoss[0m : 6.31568
[1mStep[0m  [4/26], [94mLoss[0m : 6.28045
[1mStep[0m  [6/26], [94mLoss[0m : 6.08771
[1mStep[0m  [8/26], [94mLoss[0m : 6.13261
[1mStep[0m  [10/26], [94mLoss[0m : 6.35415
[1mStep[0m  [12/26], [94mLoss[0m : 6.06186
[1mStep[0m  [14/26], [94mLoss[0m : 6.08185
[1mStep[0m  [16/26], [94mLoss[0m : 5.97355
[1mStep[0m  [18/26], [94mLoss[0m : 5.78259
[1mStep[0m  [20/26], [94mLoss[0m : 5.78673
[1mStep[0m  [22/26], [94mLoss[0m : 5.96850
[1mStep[0m  [24/26], [94mLoss[0m : 5.96616

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.044, [92mTest[0m: 6.831, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 6.10936
[1mStep[0m  [2/26], [94mLoss[0m : 6.02406
[1mStep[0m  [4/26], [94mLoss[0m : 5.78741
[1mStep[0m  [6/26], [94mLoss[0m : 6.12238
[1mStep[0m  [8/26], [94mLoss[0m : 5.92020
[1mStep[0m  [10/26], [94mLoss[0m : 5.90687
[1mStep[0m  [12/26], [94mLoss[0m : 6.18600
[1mStep[0m  [14/26], [94mLoss[0m : 5.79343
[1mStep[0m  [16/26], [94mLoss[0m : 5.63060
[1mStep[0m  [18/26], [94mLoss[0m : 5.93275
[1mStep[0m  [20/26], [94mLoss[0m : 5.58643
[1mStep[0m  [22/26], [94mLoss[0m : 5.80072
[1mStep[0m  [24/26], [94mLoss[0m : 5.90570

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.882, [92mTest[0m: 6.675, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.83598
[1mStep[0m  [2/26], [94mLoss[0m : 5.76120
[1mStep[0m  [4/26], [94mLoss[0m : 6.00108
[1mStep[0m  [6/26], [94mLoss[0m : 5.51581
[1mStep[0m  [8/26], [94mLoss[0m : 5.54952
[1mStep[0m  [10/26], [94mLoss[0m : 5.85103
[1mStep[0m  [12/26], [94mLoss[0m : 5.78958
[1mStep[0m  [14/26], [94mLoss[0m : 5.79534
[1mStep[0m  [16/26], [94mLoss[0m : 5.79721
[1mStep[0m  [18/26], [94mLoss[0m : 5.73346
[1mStep[0m  [20/26], [94mLoss[0m : 5.74652
[1mStep[0m  [22/26], [94mLoss[0m : 5.80626
[1mStep[0m  [24/26], [94mLoss[0m : 5.75138

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.729, [92mTest[0m: 6.524, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.59701
[1mStep[0m  [2/26], [94mLoss[0m : 5.61482
[1mStep[0m  [4/26], [94mLoss[0m : 5.68659
[1mStep[0m  [6/26], [94mLoss[0m : 5.73099
[1mStep[0m  [8/26], [94mLoss[0m : 5.67053
[1mStep[0m  [10/26], [94mLoss[0m : 5.42242
[1mStep[0m  [12/26], [94mLoss[0m : 5.68891
[1mStep[0m  [14/26], [94mLoss[0m : 5.47105
[1mStep[0m  [16/26], [94mLoss[0m : 5.65355
[1mStep[0m  [18/26], [94mLoss[0m : 5.70571
[1mStep[0m  [20/26], [94mLoss[0m : 5.79211
[1mStep[0m  [22/26], [94mLoss[0m : 5.77781
[1mStep[0m  [24/26], [94mLoss[0m : 5.55284

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.567, [92mTest[0m: 6.366, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.52458
[1mStep[0m  [2/26], [94mLoss[0m : 5.40798
[1mStep[0m  [4/26], [94mLoss[0m : 5.57484
[1mStep[0m  [6/26], [94mLoss[0m : 5.59763
[1mStep[0m  [8/26], [94mLoss[0m : 5.53388
[1mStep[0m  [10/26], [94mLoss[0m : 5.56653
[1mStep[0m  [12/26], [94mLoss[0m : 5.31613
[1mStep[0m  [14/26], [94mLoss[0m : 5.41548
[1mStep[0m  [16/26], [94mLoss[0m : 5.26824
[1mStep[0m  [18/26], [94mLoss[0m : 5.35168
[1mStep[0m  [20/26], [94mLoss[0m : 5.38206
[1mStep[0m  [22/26], [94mLoss[0m : 5.41360
[1mStep[0m  [24/26], [94mLoss[0m : 5.36513

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.425, [92mTest[0m: 6.239, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.41218
[1mStep[0m  [2/26], [94mLoss[0m : 5.45771
[1mStep[0m  [4/26], [94mLoss[0m : 5.19527
[1mStep[0m  [6/26], [94mLoss[0m : 5.23262
[1mStep[0m  [8/26], [94mLoss[0m : 5.53318
[1mStep[0m  [10/26], [94mLoss[0m : 5.37169
[1mStep[0m  [12/26], [94mLoss[0m : 4.86669
[1mStep[0m  [14/26], [94mLoss[0m : 5.14297
[1mStep[0m  [16/26], [94mLoss[0m : 5.43447
[1mStep[0m  [18/26], [94mLoss[0m : 5.33839
[1mStep[0m  [20/26], [94mLoss[0m : 5.17961
[1mStep[0m  [22/26], [94mLoss[0m : 5.13120
[1mStep[0m  [24/26], [94mLoss[0m : 5.48051

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.285, [92mTest[0m: 6.090, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.10773
[1mStep[0m  [2/26], [94mLoss[0m : 5.43749
[1mStep[0m  [4/26], [94mLoss[0m : 5.12987
[1mStep[0m  [6/26], [94mLoss[0m : 5.21765
[1mStep[0m  [8/26], [94mLoss[0m : 5.38653
[1mStep[0m  [10/26], [94mLoss[0m : 5.21563
[1mStep[0m  [12/26], [94mLoss[0m : 5.20769
[1mStep[0m  [14/26], [94mLoss[0m : 4.81764
[1mStep[0m  [16/26], [94mLoss[0m : 5.07648
[1mStep[0m  [18/26], [94mLoss[0m : 5.18243
[1mStep[0m  [20/26], [94mLoss[0m : 4.94831
[1mStep[0m  [22/26], [94mLoss[0m : 5.31899
[1mStep[0m  [24/26], [94mLoss[0m : 5.12997

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 5.153, [92mTest[0m: 5.940, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.06906
[1mStep[0m  [2/26], [94mLoss[0m : 4.83094
[1mStep[0m  [4/26], [94mLoss[0m : 4.83252
[1mStep[0m  [6/26], [94mLoss[0m : 4.93237
[1mStep[0m  [8/26], [94mLoss[0m : 5.29244
[1mStep[0m  [10/26], [94mLoss[0m : 4.93087
[1mStep[0m  [12/26], [94mLoss[0m : 5.14727
[1mStep[0m  [14/26], [94mLoss[0m : 5.01022
[1mStep[0m  [16/26], [94mLoss[0m : 5.27966
[1mStep[0m  [18/26], [94mLoss[0m : 4.91950
[1mStep[0m  [20/26], [94mLoss[0m : 5.19653
[1mStep[0m  [22/26], [94mLoss[0m : 4.98955
[1mStep[0m  [24/26], [94mLoss[0m : 5.07145

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 5.015, [92mTest[0m: 5.828, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 5.26075
[1mStep[0m  [2/26], [94mLoss[0m : 4.95325
[1mStep[0m  [4/26], [94mLoss[0m : 4.79920
[1mStep[0m  [6/26], [94mLoss[0m : 5.22904
[1mStep[0m  [8/26], [94mLoss[0m : 4.85292
[1mStep[0m  [10/26], [94mLoss[0m : 4.70297
[1mStep[0m  [12/26], [94mLoss[0m : 4.96528
[1mStep[0m  [14/26], [94mLoss[0m : 4.93080
[1mStep[0m  [16/26], [94mLoss[0m : 4.82062
[1mStep[0m  [18/26], [94mLoss[0m : 4.98262
[1mStep[0m  [20/26], [94mLoss[0m : 4.97965
[1mStep[0m  [22/26], [94mLoss[0m : 4.94115
[1mStep[0m  [24/26], [94mLoss[0m : 4.97305

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.899, [92mTest[0m: 5.674, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.550
====================================

Phase 1 - Evaluation MAE:  5.549677041860727
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/26], [94mLoss[0m : 4.79876
[1mStep[0m  [2/26], [94mLoss[0m : 4.74886
[1mStep[0m  [4/26], [94mLoss[0m : 4.69932
[1mStep[0m  [6/26], [94mLoss[0m : 5.02548
[1mStep[0m  [8/26], [94mLoss[0m : 5.07148
[1mStep[0m  [10/26], [94mLoss[0m : 4.56283
[1mStep[0m  [12/26], [94mLoss[0m : 4.89182
[1mStep[0m  [14/26], [94mLoss[0m : 4.87178
[1mStep[0m  [16/26], [94mLoss[0m : 4.80144
[1mStep[0m  [18/26], [94mLoss[0m : 4.53038
[1mStep[0m  [20/26], [94mLoss[0m : 4.63600
[1mStep[0m  [22/26], [94mLoss[0m : 4.68163
[1mStep[0m  [24/26], [94mLoss[0m : 4.71742

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.752, [92mTest[0m: 5.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.66871
[1mStep[0m  [2/26], [94mLoss[0m : 4.69852
[1mStep[0m  [4/26], [94mLoss[0m : 4.40319
[1mStep[0m  [6/26], [94mLoss[0m : 4.82126
[1mStep[0m  [8/26], [94mLoss[0m : 4.35267
[1mStep[0m  [10/26], [94mLoss[0m : 4.67246
[1mStep[0m  [12/26], [94mLoss[0m : 4.56518
[1mStep[0m  [14/26], [94mLoss[0m : 4.68685
[1mStep[0m  [16/26], [94mLoss[0m : 4.26814
[1mStep[0m  [18/26], [94mLoss[0m : 4.69225
[1mStep[0m  [20/26], [94mLoss[0m : 4.50406
[1mStep[0m  [22/26], [94mLoss[0m : 4.49430
[1mStep[0m  [24/26], [94mLoss[0m : 4.37662

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.600, [92mTest[0m: 5.406, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.35839
[1mStep[0m  [2/26], [94mLoss[0m : 4.63403
[1mStep[0m  [4/26], [94mLoss[0m : 4.36084
[1mStep[0m  [6/26], [94mLoss[0m : 4.53956
[1mStep[0m  [8/26], [94mLoss[0m : 4.35643
[1mStep[0m  [10/26], [94mLoss[0m : 4.35263
[1mStep[0m  [12/26], [94mLoss[0m : 4.57862
[1mStep[0m  [14/26], [94mLoss[0m : 4.21737
[1mStep[0m  [16/26], [94mLoss[0m : 4.45503
[1mStep[0m  [18/26], [94mLoss[0m : 4.43630
[1mStep[0m  [20/26], [94mLoss[0m : 4.33761
[1mStep[0m  [22/26], [94mLoss[0m : 4.41283
[1mStep[0m  [24/26], [94mLoss[0m : 4.45675

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.435, [92mTest[0m: 5.236, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.15269
[1mStep[0m  [2/26], [94mLoss[0m : 4.25982
[1mStep[0m  [4/26], [94mLoss[0m : 4.28083
[1mStep[0m  [6/26], [94mLoss[0m : 4.21665
[1mStep[0m  [8/26], [94mLoss[0m : 4.35201
[1mStep[0m  [10/26], [94mLoss[0m : 4.26020
[1mStep[0m  [12/26], [94mLoss[0m : 4.39408
[1mStep[0m  [14/26], [94mLoss[0m : 4.38784
[1mStep[0m  [16/26], [94mLoss[0m : 4.39278
[1mStep[0m  [18/26], [94mLoss[0m : 4.31849
[1mStep[0m  [20/26], [94mLoss[0m : 4.19781
[1mStep[0m  [22/26], [94mLoss[0m : 4.11317
[1mStep[0m  [24/26], [94mLoss[0m : 4.36831

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.294, [92mTest[0m: 5.060, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.39588
[1mStep[0m  [2/26], [94mLoss[0m : 4.15952
[1mStep[0m  [4/26], [94mLoss[0m : 3.94768
[1mStep[0m  [6/26], [94mLoss[0m : 3.77706
[1mStep[0m  [8/26], [94mLoss[0m : 4.29377
[1mStep[0m  [10/26], [94mLoss[0m : 3.99266
[1mStep[0m  [12/26], [94mLoss[0m : 3.89911
[1mStep[0m  [14/26], [94mLoss[0m : 4.29921
[1mStep[0m  [16/26], [94mLoss[0m : 4.11666
[1mStep[0m  [18/26], [94mLoss[0m : 4.26070
[1mStep[0m  [20/26], [94mLoss[0m : 4.39471
[1mStep[0m  [22/26], [94mLoss[0m : 4.44908
[1mStep[0m  [24/26], [94mLoss[0m : 4.23635

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.151, [92mTest[0m: 4.911, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.87704
[1mStep[0m  [2/26], [94mLoss[0m : 3.98717
[1mStep[0m  [4/26], [94mLoss[0m : 4.09438
[1mStep[0m  [6/26], [94mLoss[0m : 4.09634
[1mStep[0m  [8/26], [94mLoss[0m : 4.18282
[1mStep[0m  [10/26], [94mLoss[0m : 4.04218
[1mStep[0m  [12/26], [94mLoss[0m : 3.88037
[1mStep[0m  [14/26], [94mLoss[0m : 4.18735
[1mStep[0m  [16/26], [94mLoss[0m : 3.97736
[1mStep[0m  [18/26], [94mLoss[0m : 4.03423
[1mStep[0m  [20/26], [94mLoss[0m : 4.05411
[1mStep[0m  [22/26], [94mLoss[0m : 3.99531
[1mStep[0m  [24/26], [94mLoss[0m : 4.05322

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.019, [92mTest[0m: 4.754, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.04309
[1mStep[0m  [2/26], [94mLoss[0m : 4.08551
[1mStep[0m  [4/26], [94mLoss[0m : 3.89569
[1mStep[0m  [6/26], [94mLoss[0m : 3.98972
[1mStep[0m  [8/26], [94mLoss[0m : 4.29576
[1mStep[0m  [10/26], [94mLoss[0m : 3.73824
[1mStep[0m  [12/26], [94mLoss[0m : 3.67554
[1mStep[0m  [14/26], [94mLoss[0m : 3.95163
[1mStep[0m  [16/26], [94mLoss[0m : 3.62589
[1mStep[0m  [18/26], [94mLoss[0m : 3.96922
[1mStep[0m  [20/26], [94mLoss[0m : 4.03087
[1mStep[0m  [22/26], [94mLoss[0m : 3.69670
[1mStep[0m  [24/26], [94mLoss[0m : 3.90884

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.898, [92mTest[0m: 4.597, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.77598
[1mStep[0m  [2/26], [94mLoss[0m : 3.76314
[1mStep[0m  [4/26], [94mLoss[0m : 3.66437
[1mStep[0m  [6/26], [94mLoss[0m : 4.05679
[1mStep[0m  [8/26], [94mLoss[0m : 3.71427
[1mStep[0m  [10/26], [94mLoss[0m : 3.79651
[1mStep[0m  [12/26], [94mLoss[0m : 3.72826
[1mStep[0m  [14/26], [94mLoss[0m : 3.60153
[1mStep[0m  [16/26], [94mLoss[0m : 3.82751
[1mStep[0m  [18/26], [94mLoss[0m : 3.62990
[1mStep[0m  [20/26], [94mLoss[0m : 3.79004
[1mStep[0m  [22/26], [94mLoss[0m : 3.71563
[1mStep[0m  [24/26], [94mLoss[0m : 3.81703

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.781, [92mTest[0m: 4.452, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 4.00907
[1mStep[0m  [2/26], [94mLoss[0m : 3.83427
[1mStep[0m  [4/26], [94mLoss[0m : 3.75316
[1mStep[0m  [6/26], [94mLoss[0m : 3.64539
[1mStep[0m  [8/26], [94mLoss[0m : 3.39801
[1mStep[0m  [10/26], [94mLoss[0m : 3.72324
[1mStep[0m  [12/26], [94mLoss[0m : 3.82639
[1mStep[0m  [14/26], [94mLoss[0m : 3.63881
[1mStep[0m  [16/26], [94mLoss[0m : 3.54969
[1mStep[0m  [18/26], [94mLoss[0m : 3.32824
[1mStep[0m  [20/26], [94mLoss[0m : 3.78313
[1mStep[0m  [22/26], [94mLoss[0m : 3.31438
[1mStep[0m  [24/26], [94mLoss[0m : 3.64879

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.684, [92mTest[0m: 4.320, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.52899
[1mStep[0m  [2/26], [94mLoss[0m : 3.79291
[1mStep[0m  [4/26], [94mLoss[0m : 3.94377
[1mStep[0m  [6/26], [94mLoss[0m : 3.45583
[1mStep[0m  [8/26], [94mLoss[0m : 3.57431
[1mStep[0m  [10/26], [94mLoss[0m : 3.66351
[1mStep[0m  [12/26], [94mLoss[0m : 3.70401
[1mStep[0m  [14/26], [94mLoss[0m : 3.59648
[1mStep[0m  [16/26], [94mLoss[0m : 3.41764
[1mStep[0m  [18/26], [94mLoss[0m : 3.45662
[1mStep[0m  [20/26], [94mLoss[0m : 3.70988
[1mStep[0m  [22/26], [94mLoss[0m : 3.48254
[1mStep[0m  [24/26], [94mLoss[0m : 3.45244

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.586, [92mTest[0m: 4.190, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.63232
[1mStep[0m  [2/26], [94mLoss[0m : 3.65267
[1mStep[0m  [4/26], [94mLoss[0m : 3.41825
[1mStep[0m  [6/26], [94mLoss[0m : 3.51068
[1mStep[0m  [8/26], [94mLoss[0m : 3.48866
[1mStep[0m  [10/26], [94mLoss[0m : 3.36577
[1mStep[0m  [12/26], [94mLoss[0m : 3.58114
[1mStep[0m  [14/26], [94mLoss[0m : 3.22074
[1mStep[0m  [16/26], [94mLoss[0m : 3.25141
[1mStep[0m  [18/26], [94mLoss[0m : 3.43740
[1mStep[0m  [20/26], [94mLoss[0m : 3.52153
[1mStep[0m  [22/26], [94mLoss[0m : 3.37687
[1mStep[0m  [24/26], [94mLoss[0m : 3.40713

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.488, [92mTest[0m: 4.064, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.48821
[1mStep[0m  [2/26], [94mLoss[0m : 3.42480
[1mStep[0m  [4/26], [94mLoss[0m : 3.49845
[1mStep[0m  [6/26], [94mLoss[0m : 3.11365
[1mStep[0m  [8/26], [94mLoss[0m : 3.58111
[1mStep[0m  [10/26], [94mLoss[0m : 3.46661
[1mStep[0m  [12/26], [94mLoss[0m : 3.23096
[1mStep[0m  [14/26], [94mLoss[0m : 3.41306
[1mStep[0m  [16/26], [94mLoss[0m : 3.44403
[1mStep[0m  [18/26], [94mLoss[0m : 3.52642
[1mStep[0m  [20/26], [94mLoss[0m : 3.38465
[1mStep[0m  [22/26], [94mLoss[0m : 3.29663
[1mStep[0m  [24/26], [94mLoss[0m : 3.21204

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.397, [92mTest[0m: 3.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.30987
[1mStep[0m  [2/26], [94mLoss[0m : 3.46114
[1mStep[0m  [4/26], [94mLoss[0m : 3.38051
[1mStep[0m  [6/26], [94mLoss[0m : 3.23858
[1mStep[0m  [8/26], [94mLoss[0m : 3.49854
[1mStep[0m  [10/26], [94mLoss[0m : 3.73127
[1mStep[0m  [12/26], [94mLoss[0m : 3.26148
[1mStep[0m  [14/26], [94mLoss[0m : 3.15916
[1mStep[0m  [16/26], [94mLoss[0m : 3.23300
[1mStep[0m  [18/26], [94mLoss[0m : 3.45606
[1mStep[0m  [20/26], [94mLoss[0m : 3.23943
[1mStep[0m  [22/26], [94mLoss[0m : 3.38106
[1mStep[0m  [24/26], [94mLoss[0m : 3.06355

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.316, [92mTest[0m: 3.855, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.20012
[1mStep[0m  [2/26], [94mLoss[0m : 3.18324
[1mStep[0m  [4/26], [94mLoss[0m : 3.25988
[1mStep[0m  [6/26], [94mLoss[0m : 3.15543
[1mStep[0m  [8/26], [94mLoss[0m : 3.49299
[1mStep[0m  [10/26], [94mLoss[0m : 3.29687
[1mStep[0m  [12/26], [94mLoss[0m : 3.51640
[1mStep[0m  [14/26], [94mLoss[0m : 3.26517
[1mStep[0m  [16/26], [94mLoss[0m : 3.21553
[1mStep[0m  [18/26], [94mLoss[0m : 2.92751
[1mStep[0m  [20/26], [94mLoss[0m : 3.27510
[1mStep[0m  [22/26], [94mLoss[0m : 3.34940
[1mStep[0m  [24/26], [94mLoss[0m : 3.08926

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.258, [92mTest[0m: 3.739, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.38694
[1mStep[0m  [2/26], [94mLoss[0m : 3.26941
[1mStep[0m  [4/26], [94mLoss[0m : 3.17110
[1mStep[0m  [6/26], [94mLoss[0m : 3.40836
[1mStep[0m  [8/26], [94mLoss[0m : 3.00246
[1mStep[0m  [10/26], [94mLoss[0m : 3.16914
[1mStep[0m  [12/26], [94mLoss[0m : 3.04059
[1mStep[0m  [14/26], [94mLoss[0m : 3.04482
[1mStep[0m  [16/26], [94mLoss[0m : 2.97503
[1mStep[0m  [18/26], [94mLoss[0m : 2.95937
[1mStep[0m  [20/26], [94mLoss[0m : 2.97168
[1mStep[0m  [22/26], [94mLoss[0m : 3.09868
[1mStep[0m  [24/26], [94mLoss[0m : 3.05344

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.184, [92mTest[0m: 3.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.91680
[1mStep[0m  [2/26], [94mLoss[0m : 3.34431
[1mStep[0m  [4/26], [94mLoss[0m : 3.26733
[1mStep[0m  [6/26], [94mLoss[0m : 2.83456
[1mStep[0m  [8/26], [94mLoss[0m : 3.11008
[1mStep[0m  [10/26], [94mLoss[0m : 3.14916
[1mStep[0m  [12/26], [94mLoss[0m : 3.07283
[1mStep[0m  [14/26], [94mLoss[0m : 3.07896
[1mStep[0m  [16/26], [94mLoss[0m : 2.93208
[1mStep[0m  [18/26], [94mLoss[0m : 3.23765
[1mStep[0m  [20/26], [94mLoss[0m : 3.11353
[1mStep[0m  [22/26], [94mLoss[0m : 3.14622
[1mStep[0m  [24/26], [94mLoss[0m : 3.25510

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.099, [92mTest[0m: 3.566, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.85923
[1mStep[0m  [2/26], [94mLoss[0m : 3.01076
[1mStep[0m  [4/26], [94mLoss[0m : 2.99372
[1mStep[0m  [6/26], [94mLoss[0m : 2.95687
[1mStep[0m  [8/26], [94mLoss[0m : 3.13788
[1mStep[0m  [10/26], [94mLoss[0m : 3.06457
[1mStep[0m  [12/26], [94mLoss[0m : 3.19767
[1mStep[0m  [14/26], [94mLoss[0m : 2.97476
[1mStep[0m  [16/26], [94mLoss[0m : 2.84641
[1mStep[0m  [18/26], [94mLoss[0m : 2.90967
[1mStep[0m  [20/26], [94mLoss[0m : 2.80289
[1mStep[0m  [22/26], [94mLoss[0m : 2.95300
[1mStep[0m  [24/26], [94mLoss[0m : 3.03740

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.043, [92mTest[0m: 3.493, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 3.14880
[1mStep[0m  [2/26], [94mLoss[0m : 2.80554
[1mStep[0m  [4/26], [94mLoss[0m : 2.91156
[1mStep[0m  [6/26], [94mLoss[0m : 3.02382
[1mStep[0m  [8/26], [94mLoss[0m : 3.14415
[1mStep[0m  [10/26], [94mLoss[0m : 2.87339
[1mStep[0m  [12/26], [94mLoss[0m : 3.12311
[1mStep[0m  [14/26], [94mLoss[0m : 2.90486
[1mStep[0m  [16/26], [94mLoss[0m : 3.09381
[1mStep[0m  [18/26], [94mLoss[0m : 3.13016
[1mStep[0m  [20/26], [94mLoss[0m : 3.15727
[1mStep[0m  [22/26], [94mLoss[0m : 3.17932
[1mStep[0m  [24/26], [94mLoss[0m : 2.95720

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.996, [92mTest[0m: 3.435, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79515
[1mStep[0m  [2/26], [94mLoss[0m : 3.00470
[1mStep[0m  [4/26], [94mLoss[0m : 2.97313
[1mStep[0m  [6/26], [94mLoss[0m : 2.89793
[1mStep[0m  [8/26], [94mLoss[0m : 2.88658
[1mStep[0m  [10/26], [94mLoss[0m : 2.96894
[1mStep[0m  [12/26], [94mLoss[0m : 2.99681
[1mStep[0m  [14/26], [94mLoss[0m : 2.93489
[1mStep[0m  [16/26], [94mLoss[0m : 3.12831
[1mStep[0m  [18/26], [94mLoss[0m : 2.97112
[1mStep[0m  [20/26], [94mLoss[0m : 3.01585
[1mStep[0m  [22/26], [94mLoss[0m : 2.88847
[1mStep[0m  [24/26], [94mLoss[0m : 3.15667

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.957, [92mTest[0m: 3.345, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.88503
[1mStep[0m  [2/26], [94mLoss[0m : 2.82715
[1mStep[0m  [4/26], [94mLoss[0m : 2.87782
[1mStep[0m  [6/26], [94mLoss[0m : 3.04751
[1mStep[0m  [8/26], [94mLoss[0m : 2.90981
[1mStep[0m  [10/26], [94mLoss[0m : 2.70762
[1mStep[0m  [12/26], [94mLoss[0m : 2.86658
[1mStep[0m  [14/26], [94mLoss[0m : 3.12309
[1mStep[0m  [16/26], [94mLoss[0m : 2.75699
[1mStep[0m  [18/26], [94mLoss[0m : 2.94139
[1mStep[0m  [20/26], [94mLoss[0m : 2.78151
[1mStep[0m  [22/26], [94mLoss[0m : 2.88834
[1mStep[0m  [24/26], [94mLoss[0m : 2.92477

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.911, [92mTest[0m: 3.270, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.84684
[1mStep[0m  [2/26], [94mLoss[0m : 2.92392
[1mStep[0m  [4/26], [94mLoss[0m : 2.97734
[1mStep[0m  [6/26], [94mLoss[0m : 3.06507
[1mStep[0m  [8/26], [94mLoss[0m : 2.86882
[1mStep[0m  [10/26], [94mLoss[0m : 2.80807
[1mStep[0m  [12/26], [94mLoss[0m : 2.98708
[1mStep[0m  [14/26], [94mLoss[0m : 3.02052
[1mStep[0m  [16/26], [94mLoss[0m : 2.73397
[1mStep[0m  [18/26], [94mLoss[0m : 2.86535
[1mStep[0m  [20/26], [94mLoss[0m : 2.86578
[1mStep[0m  [22/26], [94mLoss[0m : 2.78075
[1mStep[0m  [24/26], [94mLoss[0m : 3.02791

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.878, [92mTest[0m: 3.205, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.79919
[1mStep[0m  [2/26], [94mLoss[0m : 2.71227
[1mStep[0m  [4/26], [94mLoss[0m : 2.79778
[1mStep[0m  [6/26], [94mLoss[0m : 2.71989
[1mStep[0m  [8/26], [94mLoss[0m : 2.74552
[1mStep[0m  [10/26], [94mLoss[0m : 2.72004
[1mStep[0m  [12/26], [94mLoss[0m : 2.98698
[1mStep[0m  [14/26], [94mLoss[0m : 2.81356
[1mStep[0m  [16/26], [94mLoss[0m : 2.59657
[1mStep[0m  [18/26], [94mLoss[0m : 2.79672
[1mStep[0m  [20/26], [94mLoss[0m : 2.78408
[1mStep[0m  [22/26], [94mLoss[0m : 3.00901
[1mStep[0m  [24/26], [94mLoss[0m : 2.80518

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.850, [92mTest[0m: 3.155, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.71960
[1mStep[0m  [2/26], [94mLoss[0m : 2.65156
[1mStep[0m  [4/26], [94mLoss[0m : 3.01162
[1mStep[0m  [6/26], [94mLoss[0m : 2.80844
[1mStep[0m  [8/26], [94mLoss[0m : 2.97892
[1mStep[0m  [10/26], [94mLoss[0m : 2.74474
[1mStep[0m  [12/26], [94mLoss[0m : 2.92181
[1mStep[0m  [14/26], [94mLoss[0m : 2.93061
[1mStep[0m  [16/26], [94mLoss[0m : 2.79439
[1mStep[0m  [18/26], [94mLoss[0m : 2.81452
[1mStep[0m  [20/26], [94mLoss[0m : 2.72044
[1mStep[0m  [22/26], [94mLoss[0m : 2.82630
[1mStep[0m  [24/26], [94mLoss[0m : 2.74364

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.813, [92mTest[0m: 3.130, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.81761
[1mStep[0m  [2/26], [94mLoss[0m : 2.84226
[1mStep[0m  [4/26], [94mLoss[0m : 2.78356
[1mStep[0m  [6/26], [94mLoss[0m : 2.73564
[1mStep[0m  [8/26], [94mLoss[0m : 2.77010
[1mStep[0m  [10/26], [94mLoss[0m : 2.72566
[1mStep[0m  [12/26], [94mLoss[0m : 2.90332
[1mStep[0m  [14/26], [94mLoss[0m : 2.64945
[1mStep[0m  [16/26], [94mLoss[0m : 2.79121
[1mStep[0m  [18/26], [94mLoss[0m : 2.88107
[1mStep[0m  [20/26], [94mLoss[0m : 2.66914
[1mStep[0m  [22/26], [94mLoss[0m : 2.81336
[1mStep[0m  [24/26], [94mLoss[0m : 2.82289

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.794, [92mTest[0m: 3.097, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.83352
[1mStep[0m  [2/26], [94mLoss[0m : 2.75258
[1mStep[0m  [4/26], [94mLoss[0m : 2.70846
[1mStep[0m  [6/26], [94mLoss[0m : 2.69583
[1mStep[0m  [8/26], [94mLoss[0m : 2.71970
[1mStep[0m  [10/26], [94mLoss[0m : 2.76414
[1mStep[0m  [12/26], [94mLoss[0m : 2.61890
[1mStep[0m  [14/26], [94mLoss[0m : 2.87478
[1mStep[0m  [16/26], [94mLoss[0m : 2.56320
[1mStep[0m  [18/26], [94mLoss[0m : 2.68579
[1mStep[0m  [20/26], [94mLoss[0m : 2.88534
[1mStep[0m  [22/26], [94mLoss[0m : 2.90609
[1mStep[0m  [24/26], [94mLoss[0m : 2.74737

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.759, [92mTest[0m: 3.051, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.73556
[1mStep[0m  [2/26], [94mLoss[0m : 2.83415
[1mStep[0m  [4/26], [94mLoss[0m : 2.56325
[1mStep[0m  [6/26], [94mLoss[0m : 2.86848
[1mStep[0m  [8/26], [94mLoss[0m : 2.58533
[1mStep[0m  [10/26], [94mLoss[0m : 2.66649
[1mStep[0m  [12/26], [94mLoss[0m : 2.69018
[1mStep[0m  [14/26], [94mLoss[0m : 2.79290
[1mStep[0m  [16/26], [94mLoss[0m : 2.76305
[1mStep[0m  [18/26], [94mLoss[0m : 2.59305
[1mStep[0m  [20/26], [94mLoss[0m : 2.75261
[1mStep[0m  [22/26], [94mLoss[0m : 2.59953
[1mStep[0m  [24/26], [94mLoss[0m : 2.90588

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.724, [92mTest[0m: 3.035, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.41032
[1mStep[0m  [2/26], [94mLoss[0m : 2.78391
[1mStep[0m  [4/26], [94mLoss[0m : 2.84487
[1mStep[0m  [6/26], [94mLoss[0m : 2.67906
[1mStep[0m  [8/26], [94mLoss[0m : 2.68112
[1mStep[0m  [10/26], [94mLoss[0m : 2.58391
[1mStep[0m  [12/26], [94mLoss[0m : 2.85826
[1mStep[0m  [14/26], [94mLoss[0m : 2.89473
[1mStep[0m  [16/26], [94mLoss[0m : 2.73377
[1mStep[0m  [18/26], [94mLoss[0m : 2.58019
[1mStep[0m  [20/26], [94mLoss[0m : 2.69286
[1mStep[0m  [22/26], [94mLoss[0m : 2.69937
[1mStep[0m  [24/26], [94mLoss[0m : 2.78635

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.984, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.66250
[1mStep[0m  [2/26], [94mLoss[0m : 2.82572
[1mStep[0m  [4/26], [94mLoss[0m : 2.76699
[1mStep[0m  [6/26], [94mLoss[0m : 2.78613
[1mStep[0m  [8/26], [94mLoss[0m : 2.67732
[1mStep[0m  [10/26], [94mLoss[0m : 2.76467
[1mStep[0m  [12/26], [94mLoss[0m : 2.69209
[1mStep[0m  [14/26], [94mLoss[0m : 2.75283
[1mStep[0m  [16/26], [94mLoss[0m : 2.61653
[1mStep[0m  [18/26], [94mLoss[0m : 2.71885
[1mStep[0m  [20/26], [94mLoss[0m : 2.69647
[1mStep[0m  [22/26], [94mLoss[0m : 2.66219
[1mStep[0m  [24/26], [94mLoss[0m : 2.74772

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.956, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.70435
[1mStep[0m  [2/26], [94mLoss[0m : 2.95869
[1mStep[0m  [4/26], [94mLoss[0m : 2.51256
[1mStep[0m  [6/26], [94mLoss[0m : 2.68323
[1mStep[0m  [8/26], [94mLoss[0m : 2.83345
[1mStep[0m  [10/26], [94mLoss[0m : 2.74195
[1mStep[0m  [12/26], [94mLoss[0m : 2.75274
[1mStep[0m  [14/26], [94mLoss[0m : 2.52158
[1mStep[0m  [16/26], [94mLoss[0m : 2.74797
[1mStep[0m  [18/26], [94mLoss[0m : 2.55676
[1mStep[0m  [20/26], [94mLoss[0m : 2.68454
[1mStep[0m  [22/26], [94mLoss[0m : 2.79397
[1mStep[0m  [24/26], [94mLoss[0m : 2.43382

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.947, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/26], [94mLoss[0m : 2.69378
[1mStep[0m  [2/26], [94mLoss[0m : 2.63039
[1mStep[0m  [4/26], [94mLoss[0m : 2.71338
[1mStep[0m  [6/26], [94mLoss[0m : 2.62989
[1mStep[0m  [8/26], [94mLoss[0m : 2.72238
[1mStep[0m  [10/26], [94mLoss[0m : 2.69108
[1mStep[0m  [12/26], [94mLoss[0m : 2.78890
[1mStep[0m  [14/26], [94mLoss[0m : 2.68656
[1mStep[0m  [16/26], [94mLoss[0m : 2.69127
[1mStep[0m  [18/26], [94mLoss[0m : 2.66998
[1mStep[0m  [20/26], [94mLoss[0m : 2.43649
[1mStep[0m  [22/26], [94mLoss[0m : 2.82334
[1mStep[0m  [24/26], [94mLoss[0m : 2.65289

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.912, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.881
====================================

Phase 2 - Evaluation MAE:  2.880924316552969
MAE score P1      5.549677
MAE score P2      2.880924
loss              2.662802
learning_rate       0.0001
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay          0.01
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 11.15586
[1mStep[0m  [5/53], [94mLoss[0m : 10.82418
[1mStep[0m  [10/53], [94mLoss[0m : 11.19752
[1mStep[0m  [15/53], [94mLoss[0m : 11.16967
[1mStep[0m  [20/53], [94mLoss[0m : 10.61781
[1mStep[0m  [25/53], [94mLoss[0m : 11.31399
[1mStep[0m  [30/53], [94mLoss[0m : 11.09586
[1mStep[0m  [35/53], [94mLoss[0m : 11.04472
[1mStep[0m  [40/53], [94mLoss[0m : 10.53463
[1mStep[0m  [45/53], [94mLoss[0m : 10.79683
[1mStep[0m  [50/53], [94mLoss[0m : 10.70514

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.941, [92mTest[0m: 11.110, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.67835
[1mStep[0m  [5/53], [94mLoss[0m : 11.03030
[1mStep[0m  [10/53], [94mLoss[0m : 11.20928
[1mStep[0m  [15/53], [94mLoss[0m : 11.38584
[1mStep[0m  [20/53], [94mLoss[0m : 11.02577
[1mStep[0m  [25/53], [94mLoss[0m : 11.20450
[1mStep[0m  [30/53], [94mLoss[0m : 10.93405
[1mStep[0m  [35/53], [94mLoss[0m : 10.64415
[1mStep[0m  [40/53], [94mLoss[0m : 10.74498
[1mStep[0m  [45/53], [94mLoss[0m : 11.04399
[1mStep[0m  [50/53], [94mLoss[0m : 11.06681

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.931, [92mTest[0m: 10.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.85244
[1mStep[0m  [5/53], [94mLoss[0m : 10.62295
[1mStep[0m  [10/53], [94mLoss[0m : 10.64648
[1mStep[0m  [15/53], [94mLoss[0m : 10.97378
[1mStep[0m  [20/53], [94mLoss[0m : 10.97958
[1mStep[0m  [25/53], [94mLoss[0m : 10.90757
[1mStep[0m  [30/53], [94mLoss[0m : 11.23509
[1mStep[0m  [35/53], [94mLoss[0m : 11.20762
[1mStep[0m  [40/53], [94mLoss[0m : 11.28833
[1mStep[0m  [45/53], [94mLoss[0m : 11.00983
[1mStep[0m  [50/53], [94mLoss[0m : 11.39431

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.913, [92mTest[0m: 10.922, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.71337
[1mStep[0m  [5/53], [94mLoss[0m : 10.89750
[1mStep[0m  [10/53], [94mLoss[0m : 10.97882
[1mStep[0m  [15/53], [94mLoss[0m : 10.82462
[1mStep[0m  [20/53], [94mLoss[0m : 10.62778
[1mStep[0m  [25/53], [94mLoss[0m : 10.47610
[1mStep[0m  [30/53], [94mLoss[0m : 11.12079
[1mStep[0m  [35/53], [94mLoss[0m : 10.81396
[1mStep[0m  [40/53], [94mLoss[0m : 11.12755
[1mStep[0m  [45/53], [94mLoss[0m : 11.03428
[1mStep[0m  [50/53], [94mLoss[0m : 11.20349

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.896, [92mTest[0m: 10.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.31396
[1mStep[0m  [5/53], [94mLoss[0m : 10.98606
[1mStep[0m  [10/53], [94mLoss[0m : 10.77445
[1mStep[0m  [15/53], [94mLoss[0m : 10.92809
[1mStep[0m  [20/53], [94mLoss[0m : 11.14302
[1mStep[0m  [25/53], [94mLoss[0m : 10.85839
[1mStep[0m  [30/53], [94mLoss[0m : 10.68861
[1mStep[0m  [35/53], [94mLoss[0m : 10.49106
[1mStep[0m  [40/53], [94mLoss[0m : 10.64198
[1mStep[0m  [45/53], [94mLoss[0m : 10.75616
[1mStep[0m  [50/53], [94mLoss[0m : 11.00262

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.876, [92mTest[0m: 10.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.66892
[1mStep[0m  [5/53], [94mLoss[0m : 10.94837
[1mStep[0m  [10/53], [94mLoss[0m : 10.95891
[1mStep[0m  [15/53], [94mLoss[0m : 10.63986
[1mStep[0m  [20/53], [94mLoss[0m : 10.72675
[1mStep[0m  [25/53], [94mLoss[0m : 10.70015
[1mStep[0m  [30/53], [94mLoss[0m : 10.90972
[1mStep[0m  [35/53], [94mLoss[0m : 10.76406
[1mStep[0m  [40/53], [94mLoss[0m : 10.72780
[1mStep[0m  [45/53], [94mLoss[0m : 10.75470
[1mStep[0m  [50/53], [94mLoss[0m : 10.55166

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.73968
[1mStep[0m  [5/53], [94mLoss[0m : 10.71879
[1mStep[0m  [10/53], [94mLoss[0m : 11.10028
[1mStep[0m  [15/53], [94mLoss[0m : 10.75618
[1mStep[0m  [20/53], [94mLoss[0m : 10.83251
[1mStep[0m  [25/53], [94mLoss[0m : 10.85285
[1mStep[0m  [30/53], [94mLoss[0m : 11.01190
[1mStep[0m  [35/53], [94mLoss[0m : 10.83662
[1mStep[0m  [40/53], [94mLoss[0m : 11.44222
[1mStep[0m  [45/53], [94mLoss[0m : 10.62564
[1mStep[0m  [50/53], [94mLoss[0m : 10.63943

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.847, [92mTest[0m: 10.857, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.94061
[1mStep[0m  [5/53], [94mLoss[0m : 10.76256
[1mStep[0m  [10/53], [94mLoss[0m : 10.58167
[1mStep[0m  [15/53], [94mLoss[0m : 10.56807
[1mStep[0m  [20/53], [94mLoss[0m : 10.75216
[1mStep[0m  [25/53], [94mLoss[0m : 10.64702
[1mStep[0m  [30/53], [94mLoss[0m : 10.91668
[1mStep[0m  [35/53], [94mLoss[0m : 10.91815
[1mStep[0m  [40/53], [94mLoss[0m : 10.87944
[1mStep[0m  [45/53], [94mLoss[0m : 11.05283
[1mStep[0m  [50/53], [94mLoss[0m : 11.02976

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.833, [92mTest[0m: 10.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.66830
[1mStep[0m  [5/53], [94mLoss[0m : 10.87343
[1mStep[0m  [10/53], [94mLoss[0m : 10.75851
[1mStep[0m  [15/53], [94mLoss[0m : 11.02274
[1mStep[0m  [20/53], [94mLoss[0m : 10.86202
[1mStep[0m  [25/53], [94mLoss[0m : 10.83807
[1mStep[0m  [30/53], [94mLoss[0m : 10.51854
[1mStep[0m  [35/53], [94mLoss[0m : 10.62868
[1mStep[0m  [40/53], [94mLoss[0m : 10.97417
[1mStep[0m  [45/53], [94mLoss[0m : 11.06967
[1mStep[0m  [50/53], [94mLoss[0m : 10.55202

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.92425
[1mStep[0m  [5/53], [94mLoss[0m : 11.34143
[1mStep[0m  [10/53], [94mLoss[0m : 10.97105
[1mStep[0m  [15/53], [94mLoss[0m : 10.80900
[1mStep[0m  [20/53], [94mLoss[0m : 10.66358
[1mStep[0m  [25/53], [94mLoss[0m : 10.69367
[1mStep[0m  [30/53], [94mLoss[0m : 10.80148
[1mStep[0m  [35/53], [94mLoss[0m : 11.02101
[1mStep[0m  [40/53], [94mLoss[0m : 10.86505
[1mStep[0m  [45/53], [94mLoss[0m : 10.90981
[1mStep[0m  [50/53], [94mLoss[0m : 10.71160

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.805, [92mTest[0m: 10.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.15592
[1mStep[0m  [5/53], [94mLoss[0m : 10.78695
[1mStep[0m  [10/53], [94mLoss[0m : 10.66505
[1mStep[0m  [15/53], [94mLoss[0m : 11.05791
[1mStep[0m  [20/53], [94mLoss[0m : 10.71283
[1mStep[0m  [25/53], [94mLoss[0m : 10.85321
[1mStep[0m  [30/53], [94mLoss[0m : 10.68140
[1mStep[0m  [35/53], [94mLoss[0m : 10.94291
[1mStep[0m  [40/53], [94mLoss[0m : 10.96432
[1mStep[0m  [45/53], [94mLoss[0m : 10.78667
[1mStep[0m  [50/53], [94mLoss[0m : 10.80465

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.779, [92mTest[0m: 10.765, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.63987
[1mStep[0m  [5/53], [94mLoss[0m : 10.89426
[1mStep[0m  [10/53], [94mLoss[0m : 10.63647
[1mStep[0m  [15/53], [94mLoss[0m : 10.63288
[1mStep[0m  [20/53], [94mLoss[0m : 10.78715
[1mStep[0m  [25/53], [94mLoss[0m : 10.57239
[1mStep[0m  [30/53], [94mLoss[0m : 10.94613
[1mStep[0m  [35/53], [94mLoss[0m : 10.66144
[1mStep[0m  [40/53], [94mLoss[0m : 10.84876
[1mStep[0m  [45/53], [94mLoss[0m : 10.85512
[1mStep[0m  [50/53], [94mLoss[0m : 10.59835

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.768, [92mTest[0m: 10.728, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.80774
[1mStep[0m  [5/53], [94mLoss[0m : 10.40986
[1mStep[0m  [10/53], [94mLoss[0m : 10.44599
[1mStep[0m  [15/53], [94mLoss[0m : 10.58092
[1mStep[0m  [20/53], [94mLoss[0m : 10.69373
[1mStep[0m  [25/53], [94mLoss[0m : 11.03632
[1mStep[0m  [30/53], [94mLoss[0m : 10.35476
[1mStep[0m  [35/53], [94mLoss[0m : 10.69568
[1mStep[0m  [40/53], [94mLoss[0m : 10.58417
[1mStep[0m  [45/53], [94mLoss[0m : 10.62327
[1mStep[0m  [50/53], [94mLoss[0m : 10.76196

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.759, [92mTest[0m: 10.719, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.68343
[1mStep[0m  [5/53], [94mLoss[0m : 11.15199
[1mStep[0m  [10/53], [94mLoss[0m : 11.03026
[1mStep[0m  [15/53], [94mLoss[0m : 11.08303
[1mStep[0m  [20/53], [94mLoss[0m : 10.88698
[1mStep[0m  [25/53], [94mLoss[0m : 10.42383
[1mStep[0m  [30/53], [94mLoss[0m : 10.90592
[1mStep[0m  [35/53], [94mLoss[0m : 10.83200
[1mStep[0m  [40/53], [94mLoss[0m : 10.57070
[1mStep[0m  [45/53], [94mLoss[0m : 10.34704
[1mStep[0m  [50/53], [94mLoss[0m : 10.73444

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.736, [92mTest[0m: 10.702, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.55196
[1mStep[0m  [5/53], [94mLoss[0m : 11.00488
[1mStep[0m  [10/53], [94mLoss[0m : 11.09008
[1mStep[0m  [15/53], [94mLoss[0m : 10.86467
[1mStep[0m  [20/53], [94mLoss[0m : 10.75929
[1mStep[0m  [25/53], [94mLoss[0m : 10.92033
[1mStep[0m  [30/53], [94mLoss[0m : 10.89741
[1mStep[0m  [35/53], [94mLoss[0m : 11.24769
[1mStep[0m  [40/53], [94mLoss[0m : 10.91223
[1mStep[0m  [45/53], [94mLoss[0m : 10.73432
[1mStep[0m  [50/53], [94mLoss[0m : 10.70611

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.75746
[1mStep[0m  [5/53], [94mLoss[0m : 10.76136
[1mStep[0m  [10/53], [94mLoss[0m : 10.61948
[1mStep[0m  [15/53], [94mLoss[0m : 10.46321
[1mStep[0m  [20/53], [94mLoss[0m : 10.41469
[1mStep[0m  [25/53], [94mLoss[0m : 10.69526
[1mStep[0m  [30/53], [94mLoss[0m : 10.64450
[1mStep[0m  [35/53], [94mLoss[0m : 10.57838
[1mStep[0m  [40/53], [94mLoss[0m : 11.11863
[1mStep[0m  [45/53], [94mLoss[0m : 10.64038
[1mStep[0m  [50/53], [94mLoss[0m : 10.40895

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.707, [92mTest[0m: 10.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.60136
[1mStep[0m  [5/53], [94mLoss[0m : 10.99567
[1mStep[0m  [10/53], [94mLoss[0m : 10.48258
[1mStep[0m  [15/53], [94mLoss[0m : 10.87874
[1mStep[0m  [20/53], [94mLoss[0m : 10.79241
[1mStep[0m  [25/53], [94mLoss[0m : 10.60064
[1mStep[0m  [30/53], [94mLoss[0m : 10.49970
[1mStep[0m  [35/53], [94mLoss[0m : 10.50215
[1mStep[0m  [40/53], [94mLoss[0m : 10.76893
[1mStep[0m  [45/53], [94mLoss[0m : 10.45579
[1mStep[0m  [50/53], [94mLoss[0m : 10.76793

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.690, [92mTest[0m: 10.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.30330
[1mStep[0m  [5/53], [94mLoss[0m : 10.85271
[1mStep[0m  [10/53], [94mLoss[0m : 10.77566
[1mStep[0m  [15/53], [94mLoss[0m : 10.57736
[1mStep[0m  [20/53], [94mLoss[0m : 10.65436
[1mStep[0m  [25/53], [94mLoss[0m : 10.59799
[1mStep[0m  [30/53], [94mLoss[0m : 10.38379
[1mStep[0m  [35/53], [94mLoss[0m : 10.76059
[1mStep[0m  [40/53], [94mLoss[0m : 10.70569
[1mStep[0m  [45/53], [94mLoss[0m : 10.45589
[1mStep[0m  [50/53], [94mLoss[0m : 11.02968

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.677, [92mTest[0m: 10.604, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.66114
[1mStep[0m  [5/53], [94mLoss[0m : 10.54074
[1mStep[0m  [10/53], [94mLoss[0m : 10.76103
[1mStep[0m  [15/53], [94mLoss[0m : 10.55090
[1mStep[0m  [20/53], [94mLoss[0m : 10.89337
[1mStep[0m  [25/53], [94mLoss[0m : 10.85658
[1mStep[0m  [30/53], [94mLoss[0m : 10.59675
[1mStep[0m  [35/53], [94mLoss[0m : 10.53055
[1mStep[0m  [40/53], [94mLoss[0m : 10.36615
[1mStep[0m  [45/53], [94mLoss[0m : 10.59360
[1mStep[0m  [50/53], [94mLoss[0m : 10.68472

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.655, [92mTest[0m: 10.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.50760
[1mStep[0m  [5/53], [94mLoss[0m : 10.18728
[1mStep[0m  [10/53], [94mLoss[0m : 10.34371
[1mStep[0m  [15/53], [94mLoss[0m : 10.71048
[1mStep[0m  [20/53], [94mLoss[0m : 10.43000
[1mStep[0m  [25/53], [94mLoss[0m : 10.89522
[1mStep[0m  [30/53], [94mLoss[0m : 11.07652
[1mStep[0m  [35/53], [94mLoss[0m : 11.03476
[1mStep[0m  [40/53], [94mLoss[0m : 10.74706
[1mStep[0m  [45/53], [94mLoss[0m : 10.66395
[1mStep[0m  [50/53], [94mLoss[0m : 10.71577

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.644, [92mTest[0m: 10.574, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.57649
[1mStep[0m  [5/53], [94mLoss[0m : 10.70619
[1mStep[0m  [10/53], [94mLoss[0m : 10.83799
[1mStep[0m  [15/53], [94mLoss[0m : 10.69418
[1mStep[0m  [20/53], [94mLoss[0m : 10.78226
[1mStep[0m  [25/53], [94mLoss[0m : 10.47657
[1mStep[0m  [30/53], [94mLoss[0m : 10.71726
[1mStep[0m  [35/53], [94mLoss[0m : 10.43451
[1mStep[0m  [40/53], [94mLoss[0m : 10.39625
[1mStep[0m  [45/53], [94mLoss[0m : 10.58572
[1mStep[0m  [50/53], [94mLoss[0m : 10.10059

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.619, [92mTest[0m: 10.543, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.45082
[1mStep[0m  [5/53], [94mLoss[0m : 10.66710
[1mStep[0m  [10/53], [94mLoss[0m : 10.89678
[1mStep[0m  [15/53], [94mLoss[0m : 10.52694
[1mStep[0m  [20/53], [94mLoss[0m : 10.67762
[1mStep[0m  [25/53], [94mLoss[0m : 10.58488
[1mStep[0m  [30/53], [94mLoss[0m : 11.00614
[1mStep[0m  [35/53], [94mLoss[0m : 10.72936
[1mStep[0m  [40/53], [94mLoss[0m : 10.36504
[1mStep[0m  [45/53], [94mLoss[0m : 10.54700
[1mStep[0m  [50/53], [94mLoss[0m : 10.73119

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.612, [92mTest[0m: 10.518, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.19485
[1mStep[0m  [5/53], [94mLoss[0m : 10.71294
[1mStep[0m  [10/53], [94mLoss[0m : 10.23939
[1mStep[0m  [15/53], [94mLoss[0m : 10.67170
[1mStep[0m  [20/53], [94mLoss[0m : 10.38904
[1mStep[0m  [25/53], [94mLoss[0m : 10.55185
[1mStep[0m  [30/53], [94mLoss[0m : 10.44603
[1mStep[0m  [35/53], [94mLoss[0m : 10.41133
[1mStep[0m  [40/53], [94mLoss[0m : 10.84964
[1mStep[0m  [45/53], [94mLoss[0m : 10.68387
[1mStep[0m  [50/53], [94mLoss[0m : 10.74594

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.502, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.39218
[1mStep[0m  [5/53], [94mLoss[0m : 10.88505
[1mStep[0m  [10/53], [94mLoss[0m : 10.67499
[1mStep[0m  [15/53], [94mLoss[0m : 10.62767
[1mStep[0m  [20/53], [94mLoss[0m : 10.42463
[1mStep[0m  [25/53], [94mLoss[0m : 10.51786
[1mStep[0m  [30/53], [94mLoss[0m : 10.91167
[1mStep[0m  [35/53], [94mLoss[0m : 10.40124
[1mStep[0m  [40/53], [94mLoss[0m : 10.59724
[1mStep[0m  [45/53], [94mLoss[0m : 10.26317
[1mStep[0m  [50/53], [94mLoss[0m : 10.36560

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.586, [92mTest[0m: 10.488, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.65629
[1mStep[0m  [5/53], [94mLoss[0m : 10.50949
[1mStep[0m  [10/53], [94mLoss[0m : 10.33557
[1mStep[0m  [15/53], [94mLoss[0m : 10.50213
[1mStep[0m  [20/53], [94mLoss[0m : 10.83517
[1mStep[0m  [25/53], [94mLoss[0m : 10.41655
[1mStep[0m  [30/53], [94mLoss[0m : 10.63321
[1mStep[0m  [35/53], [94mLoss[0m : 10.43648
[1mStep[0m  [40/53], [94mLoss[0m : 10.69543
[1mStep[0m  [45/53], [94mLoss[0m : 10.76931
[1mStep[0m  [50/53], [94mLoss[0m : 10.81949

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.567, [92mTest[0m: 10.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.78337
[1mStep[0m  [5/53], [94mLoss[0m : 10.65380
[1mStep[0m  [10/53], [94mLoss[0m : 10.40059
[1mStep[0m  [15/53], [94mLoss[0m : 10.45858
[1mStep[0m  [20/53], [94mLoss[0m : 10.64409
[1mStep[0m  [25/53], [94mLoss[0m : 10.56449
[1mStep[0m  [30/53], [94mLoss[0m : 10.57130
[1mStep[0m  [35/53], [94mLoss[0m : 10.50988
[1mStep[0m  [40/53], [94mLoss[0m : 10.37935
[1mStep[0m  [45/53], [94mLoss[0m : 10.64231
[1mStep[0m  [50/53], [94mLoss[0m : 10.54618

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.555, [92mTest[0m: 10.467, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.71735
[1mStep[0m  [5/53], [94mLoss[0m : 10.71048
[1mStep[0m  [10/53], [94mLoss[0m : 10.60727
[1mStep[0m  [15/53], [94mLoss[0m : 10.75735
[1mStep[0m  [20/53], [94mLoss[0m : 10.59575
[1mStep[0m  [25/53], [94mLoss[0m : 10.50465
[1mStep[0m  [30/53], [94mLoss[0m : 10.61277
[1mStep[0m  [35/53], [94mLoss[0m : 10.76215
[1mStep[0m  [40/53], [94mLoss[0m : 10.54867
[1mStep[0m  [45/53], [94mLoss[0m : 10.12840
[1mStep[0m  [50/53], [94mLoss[0m : 10.60098

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.539, [92mTest[0m: 10.422, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.29562
[1mStep[0m  [5/53], [94mLoss[0m : 10.45262
[1mStep[0m  [10/53], [94mLoss[0m : 10.83368
[1mStep[0m  [15/53], [94mLoss[0m : 10.55223
[1mStep[0m  [20/53], [94mLoss[0m : 10.47517
[1mStep[0m  [25/53], [94mLoss[0m : 10.71634
[1mStep[0m  [30/53], [94mLoss[0m : 10.50191
[1mStep[0m  [35/53], [94mLoss[0m : 10.56355
[1mStep[0m  [40/53], [94mLoss[0m : 10.74121
[1mStep[0m  [45/53], [94mLoss[0m : 10.47921
[1mStep[0m  [50/53], [94mLoss[0m : 10.83520

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.529, [92mTest[0m: 10.395, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.26535
[1mStep[0m  [5/53], [94mLoss[0m : 10.46632
[1mStep[0m  [10/53], [94mLoss[0m : 10.29146
[1mStep[0m  [15/53], [94mLoss[0m : 10.47579
[1mStep[0m  [20/53], [94mLoss[0m : 10.82934
[1mStep[0m  [25/53], [94mLoss[0m : 10.81096
[1mStep[0m  [30/53], [94mLoss[0m : 10.62970
[1mStep[0m  [35/53], [94mLoss[0m : 10.25947
[1mStep[0m  [40/53], [94mLoss[0m : 10.30589
[1mStep[0m  [45/53], [94mLoss[0m : 10.76162
[1mStep[0m  [50/53], [94mLoss[0m : 10.33201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.510, [92mTest[0m: 10.403, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.70452
[1mStep[0m  [5/53], [94mLoss[0m : 10.66106
[1mStep[0m  [10/53], [94mLoss[0m : 10.71868
[1mStep[0m  [15/53], [94mLoss[0m : 10.35031
[1mStep[0m  [20/53], [94mLoss[0m : 10.51908
[1mStep[0m  [25/53], [94mLoss[0m : 10.48300
[1mStep[0m  [30/53], [94mLoss[0m : 10.52916
[1mStep[0m  [35/53], [94mLoss[0m : 10.50812
[1mStep[0m  [40/53], [94mLoss[0m : 11.01244
[1mStep[0m  [45/53], [94mLoss[0m : 10.47104
[1mStep[0m  [50/53], [94mLoss[0m : 10.18378

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.495, [92mTest[0m: 10.357, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.375
====================================

Phase 1 - Evaluation MAE:  10.37547834102924
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.57224
[1mStep[0m  [5/53], [94mLoss[0m : 10.36743
[1mStep[0m  [10/53], [94mLoss[0m : 9.89652
[1mStep[0m  [15/53], [94mLoss[0m : 10.37399
[1mStep[0m  [20/53], [94mLoss[0m : 10.25709
[1mStep[0m  [25/53], [94mLoss[0m : 10.43405
[1mStep[0m  [30/53], [94mLoss[0m : 10.64190
[1mStep[0m  [35/53], [94mLoss[0m : 10.65575
[1mStep[0m  [40/53], [94mLoss[0m : 10.20811
[1mStep[0m  [45/53], [94mLoss[0m : 10.45989
[1mStep[0m  [50/53], [94mLoss[0m : 10.40060

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.486, [92mTest[0m: 10.369, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.08005
[1mStep[0m  [5/53], [94mLoss[0m : 10.79194
[1mStep[0m  [10/53], [94mLoss[0m : 10.75265
[1mStep[0m  [15/53], [94mLoss[0m : 10.91903
[1mStep[0m  [20/53], [94mLoss[0m : 10.57962
[1mStep[0m  [25/53], [94mLoss[0m : 10.30912
[1mStep[0m  [30/53], [94mLoss[0m : 10.13870
[1mStep[0m  [35/53], [94mLoss[0m : 10.66922
[1mStep[0m  [40/53], [94mLoss[0m : 10.21409
[1mStep[0m  [45/53], [94mLoss[0m : 10.72969
[1mStep[0m  [50/53], [94mLoss[0m : 10.63475

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.459, [92mTest[0m: 10.360, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.02683
[1mStep[0m  [5/53], [94mLoss[0m : 10.19216
[1mStep[0m  [10/53], [94mLoss[0m : 10.55818
[1mStep[0m  [15/53], [94mLoss[0m : 10.30357
[1mStep[0m  [20/53], [94mLoss[0m : 10.37198
[1mStep[0m  [25/53], [94mLoss[0m : 10.18661
[1mStep[0m  [30/53], [94mLoss[0m : 10.99644
[1mStep[0m  [35/53], [94mLoss[0m : 10.48350
[1mStep[0m  [40/53], [94mLoss[0m : 10.50778
[1mStep[0m  [45/53], [94mLoss[0m : 10.25593
[1mStep[0m  [50/53], [94mLoss[0m : 10.84299

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.429, [92mTest[0m: 10.310, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.69717
[1mStep[0m  [5/53], [94mLoss[0m : 10.21251
[1mStep[0m  [10/53], [94mLoss[0m : 10.69049
[1mStep[0m  [15/53], [94mLoss[0m : 10.44450
[1mStep[0m  [20/53], [94mLoss[0m : 10.16324
[1mStep[0m  [25/53], [94mLoss[0m : 10.29465
[1mStep[0m  [30/53], [94mLoss[0m : 10.20619
[1mStep[0m  [35/53], [94mLoss[0m : 10.59763
[1mStep[0m  [40/53], [94mLoss[0m : 10.36279
[1mStep[0m  [45/53], [94mLoss[0m : 10.43688
[1mStep[0m  [50/53], [94mLoss[0m : 10.22506

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.409, [92mTest[0m: 10.267, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.55711
[1mStep[0m  [5/53], [94mLoss[0m : 10.33006
[1mStep[0m  [10/53], [94mLoss[0m : 10.20569
[1mStep[0m  [15/53], [94mLoss[0m : 10.22662
[1mStep[0m  [20/53], [94mLoss[0m : 10.34438
[1mStep[0m  [25/53], [94mLoss[0m : 10.74924
[1mStep[0m  [30/53], [94mLoss[0m : 10.19268
[1mStep[0m  [35/53], [94mLoss[0m : 10.15688
[1mStep[0m  [40/53], [94mLoss[0m : 10.41229
[1mStep[0m  [45/53], [94mLoss[0m : 10.31367
[1mStep[0m  [50/53], [94mLoss[0m : 10.13592

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.379, [92mTest[0m: 10.255, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.46278
[1mStep[0m  [5/53], [94mLoss[0m : 10.29300
[1mStep[0m  [10/53], [94mLoss[0m : 10.38436
[1mStep[0m  [15/53], [94mLoss[0m : 10.37077
[1mStep[0m  [20/53], [94mLoss[0m : 10.14125
[1mStep[0m  [25/53], [94mLoss[0m : 10.67973
[1mStep[0m  [30/53], [94mLoss[0m : 10.45594
[1mStep[0m  [35/53], [94mLoss[0m : 10.34527
[1mStep[0m  [40/53], [94mLoss[0m : 10.06733
[1mStep[0m  [45/53], [94mLoss[0m : 9.86121
[1mStep[0m  [50/53], [94mLoss[0m : 10.80225

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.346, [92mTest[0m: 10.222, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.17757
[1mStep[0m  [5/53], [94mLoss[0m : 10.34555
[1mStep[0m  [10/53], [94mLoss[0m : 10.26762
[1mStep[0m  [15/53], [94mLoss[0m : 10.06951
[1mStep[0m  [20/53], [94mLoss[0m : 10.39610
[1mStep[0m  [25/53], [94mLoss[0m : 10.17244
[1mStep[0m  [30/53], [94mLoss[0m : 10.66761
[1mStep[0m  [35/53], [94mLoss[0m : 10.22819
[1mStep[0m  [40/53], [94mLoss[0m : 10.11771
[1mStep[0m  [45/53], [94mLoss[0m : 10.63455
[1mStep[0m  [50/53], [94mLoss[0m : 10.32839

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.315, [92mTest[0m: 10.198, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.25160
[1mStep[0m  [5/53], [94mLoss[0m : 10.13042
[1mStep[0m  [10/53], [94mLoss[0m : 10.60288
[1mStep[0m  [15/53], [94mLoss[0m : 10.79025
[1mStep[0m  [20/53], [94mLoss[0m : 10.57818
[1mStep[0m  [25/53], [94mLoss[0m : 10.96271
[1mStep[0m  [30/53], [94mLoss[0m : 10.16876
[1mStep[0m  [35/53], [94mLoss[0m : 9.81728
[1mStep[0m  [40/53], [94mLoss[0m : 10.18831
[1mStep[0m  [45/53], [94mLoss[0m : 10.22405
[1mStep[0m  [50/53], [94mLoss[0m : 10.42649

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.286, [92mTest[0m: 10.173, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.49722
[1mStep[0m  [5/53], [94mLoss[0m : 10.17514
[1mStep[0m  [10/53], [94mLoss[0m : 10.65661
[1mStep[0m  [15/53], [94mLoss[0m : 10.09743
[1mStep[0m  [20/53], [94mLoss[0m : 10.84208
[1mStep[0m  [25/53], [94mLoss[0m : 9.83096
[1mStep[0m  [30/53], [94mLoss[0m : 10.66682
[1mStep[0m  [35/53], [94mLoss[0m : 10.61969
[1mStep[0m  [40/53], [94mLoss[0m : 10.37762
[1mStep[0m  [45/53], [94mLoss[0m : 10.08793
[1mStep[0m  [50/53], [94mLoss[0m : 10.44409

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.242, [92mTest[0m: 10.142, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.25440
[1mStep[0m  [5/53], [94mLoss[0m : 10.17615
[1mStep[0m  [10/53], [94mLoss[0m : 10.32687
[1mStep[0m  [15/53], [94mLoss[0m : 10.39513
[1mStep[0m  [20/53], [94mLoss[0m : 10.13223
[1mStep[0m  [25/53], [94mLoss[0m : 10.39299
[1mStep[0m  [30/53], [94mLoss[0m : 9.99977
[1mStep[0m  [35/53], [94mLoss[0m : 9.81348
[1mStep[0m  [40/53], [94mLoss[0m : 10.61088
[1mStep[0m  [45/53], [94mLoss[0m : 10.50155
[1mStep[0m  [50/53], [94mLoss[0m : 10.33560

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.221, [92mTest[0m: 10.112, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.23505
[1mStep[0m  [5/53], [94mLoss[0m : 9.94280
[1mStep[0m  [10/53], [94mLoss[0m : 10.10168
[1mStep[0m  [15/53], [94mLoss[0m : 9.97241
[1mStep[0m  [20/53], [94mLoss[0m : 10.14972
[1mStep[0m  [25/53], [94mLoss[0m : 9.89341
[1mStep[0m  [30/53], [94mLoss[0m : 10.80433
[1mStep[0m  [35/53], [94mLoss[0m : 10.18066
[1mStep[0m  [40/53], [94mLoss[0m : 10.48963
[1mStep[0m  [45/53], [94mLoss[0m : 10.48790
[1mStep[0m  [50/53], [94mLoss[0m : 10.29432

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.190, [92mTest[0m: 10.093, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.04875
[1mStep[0m  [5/53], [94mLoss[0m : 9.92445
[1mStep[0m  [10/53], [94mLoss[0m : 10.03269
[1mStep[0m  [15/53], [94mLoss[0m : 9.83816
[1mStep[0m  [20/53], [94mLoss[0m : 9.92611
[1mStep[0m  [25/53], [94mLoss[0m : 10.31835
[1mStep[0m  [30/53], [94mLoss[0m : 10.30779
[1mStep[0m  [35/53], [94mLoss[0m : 10.43880
[1mStep[0m  [40/53], [94mLoss[0m : 9.96414
[1mStep[0m  [45/53], [94mLoss[0m : 10.25053
[1mStep[0m  [50/53], [94mLoss[0m : 10.25190

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.167, [92mTest[0m: 10.045, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.87052
[1mStep[0m  [5/53], [94mLoss[0m : 9.72719
[1mStep[0m  [10/53], [94mLoss[0m : 10.31141
[1mStep[0m  [15/53], [94mLoss[0m : 10.09831
[1mStep[0m  [20/53], [94mLoss[0m : 9.90704
[1mStep[0m  [25/53], [94mLoss[0m : 10.21403
[1mStep[0m  [30/53], [94mLoss[0m : 10.21246
[1mStep[0m  [35/53], [94mLoss[0m : 10.23243
[1mStep[0m  [40/53], [94mLoss[0m : 9.87388
[1mStep[0m  [45/53], [94mLoss[0m : 10.65043
[1mStep[0m  [50/53], [94mLoss[0m : 10.31204

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.139, [92mTest[0m: 10.060, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.91680
[1mStep[0m  [5/53], [94mLoss[0m : 10.30439
[1mStep[0m  [10/53], [94mLoss[0m : 10.12472
[1mStep[0m  [15/53], [94mLoss[0m : 10.75961
[1mStep[0m  [20/53], [94mLoss[0m : 10.33569
[1mStep[0m  [25/53], [94mLoss[0m : 10.20084
[1mStep[0m  [30/53], [94mLoss[0m : 10.05403
[1mStep[0m  [35/53], [94mLoss[0m : 9.96779
[1mStep[0m  [40/53], [94mLoss[0m : 10.16417
[1mStep[0m  [45/53], [94mLoss[0m : 9.74879
[1mStep[0m  [50/53], [94mLoss[0m : 10.02865

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.105, [92mTest[0m: 10.010, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.05469
[1mStep[0m  [5/53], [94mLoss[0m : 10.02610
[1mStep[0m  [10/53], [94mLoss[0m : 10.01225
[1mStep[0m  [15/53], [94mLoss[0m : 10.14039
[1mStep[0m  [20/53], [94mLoss[0m : 9.54341
[1mStep[0m  [25/53], [94mLoss[0m : 9.91291
[1mStep[0m  [30/53], [94mLoss[0m : 10.26848
[1mStep[0m  [35/53], [94mLoss[0m : 10.10956
[1mStep[0m  [40/53], [94mLoss[0m : 9.99543
[1mStep[0m  [45/53], [94mLoss[0m : 10.47463
[1mStep[0m  [50/53], [94mLoss[0m : 10.36403

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.083, [92mTest[0m: 9.943, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.18068
[1mStep[0m  [5/53], [94mLoss[0m : 10.01781
[1mStep[0m  [10/53], [94mLoss[0m : 9.98145
[1mStep[0m  [15/53], [94mLoss[0m : 10.12494
[1mStep[0m  [20/53], [94mLoss[0m : 9.93235
[1mStep[0m  [25/53], [94mLoss[0m : 10.34201
[1mStep[0m  [30/53], [94mLoss[0m : 10.16122
[1mStep[0m  [35/53], [94mLoss[0m : 10.21682
[1mStep[0m  [40/53], [94mLoss[0m : 9.95785
[1mStep[0m  [45/53], [94mLoss[0m : 10.11347
[1mStep[0m  [50/53], [94mLoss[0m : 9.92880

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.049, [92mTest[0m: 9.950, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.93102
[1mStep[0m  [5/53], [94mLoss[0m : 9.94510
[1mStep[0m  [10/53], [94mLoss[0m : 9.65203
[1mStep[0m  [15/53], [94mLoss[0m : 10.18666
[1mStep[0m  [20/53], [94mLoss[0m : 10.25756
[1mStep[0m  [25/53], [94mLoss[0m : 10.00060
[1mStep[0m  [30/53], [94mLoss[0m : 10.35733
[1mStep[0m  [35/53], [94mLoss[0m : 10.23551
[1mStep[0m  [40/53], [94mLoss[0m : 9.90465
[1mStep[0m  [45/53], [94mLoss[0m : 9.93394
[1mStep[0m  [50/53], [94mLoss[0m : 10.12233

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.017, [92mTest[0m: 9.900, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.22492
[1mStep[0m  [5/53], [94mLoss[0m : 9.80550
[1mStep[0m  [10/53], [94mLoss[0m : 9.64401
[1mStep[0m  [15/53], [94mLoss[0m : 9.87910
[1mStep[0m  [20/53], [94mLoss[0m : 10.16595
[1mStep[0m  [25/53], [94mLoss[0m : 10.10117
[1mStep[0m  [30/53], [94mLoss[0m : 10.13846
[1mStep[0m  [35/53], [94mLoss[0m : 9.86904
[1mStep[0m  [40/53], [94mLoss[0m : 9.92227
[1mStep[0m  [45/53], [94mLoss[0m : 9.97392
[1mStep[0m  [50/53], [94mLoss[0m : 9.95625

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.992, [92mTest[0m: 9.831, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.65922
[1mStep[0m  [5/53], [94mLoss[0m : 10.23389
[1mStep[0m  [10/53], [94mLoss[0m : 10.01239
[1mStep[0m  [15/53], [94mLoss[0m : 9.93179
[1mStep[0m  [20/53], [94mLoss[0m : 9.66044
[1mStep[0m  [25/53], [94mLoss[0m : 9.91025
[1mStep[0m  [30/53], [94mLoss[0m : 9.95961
[1mStep[0m  [35/53], [94mLoss[0m : 10.39679
[1mStep[0m  [40/53], [94mLoss[0m : 10.13933
[1mStep[0m  [45/53], [94mLoss[0m : 10.00630
[1mStep[0m  [50/53], [94mLoss[0m : 9.93565

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.967, [92mTest[0m: 9.817, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.88899
[1mStep[0m  [5/53], [94mLoss[0m : 10.09232
[1mStep[0m  [10/53], [94mLoss[0m : 10.00189
[1mStep[0m  [15/53], [94mLoss[0m : 9.76175
[1mStep[0m  [20/53], [94mLoss[0m : 9.77849
[1mStep[0m  [25/53], [94mLoss[0m : 9.90465
[1mStep[0m  [30/53], [94mLoss[0m : 9.60036
[1mStep[0m  [35/53], [94mLoss[0m : 9.84664
[1mStep[0m  [40/53], [94mLoss[0m : 9.90152
[1mStep[0m  [45/53], [94mLoss[0m : 9.64896
[1mStep[0m  [50/53], [94mLoss[0m : 10.25069

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.938, [92mTest[0m: 9.805, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.55951
[1mStep[0m  [5/53], [94mLoss[0m : 9.83635
[1mStep[0m  [10/53], [94mLoss[0m : 9.91093
[1mStep[0m  [15/53], [94mLoss[0m : 10.24926
[1mStep[0m  [20/53], [94mLoss[0m : 9.91209
[1mStep[0m  [25/53], [94mLoss[0m : 9.81555
[1mStep[0m  [30/53], [94mLoss[0m : 10.14135
[1mStep[0m  [35/53], [94mLoss[0m : 9.55210
[1mStep[0m  [40/53], [94mLoss[0m : 9.70727
[1mStep[0m  [45/53], [94mLoss[0m : 9.61848
[1mStep[0m  [50/53], [94mLoss[0m : 9.49302

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.903, [92mTest[0m: 9.736, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.87562
[1mStep[0m  [5/53], [94mLoss[0m : 10.16713
[1mStep[0m  [10/53], [94mLoss[0m : 9.67469
[1mStep[0m  [15/53], [94mLoss[0m : 9.72861
[1mStep[0m  [20/53], [94mLoss[0m : 10.00841
[1mStep[0m  [25/53], [94mLoss[0m : 9.51961
[1mStep[0m  [30/53], [94mLoss[0m : 9.98353
[1mStep[0m  [35/53], [94mLoss[0m : 10.00710
[1mStep[0m  [40/53], [94mLoss[0m : 9.28854
[1mStep[0m  [45/53], [94mLoss[0m : 9.75895
[1mStep[0m  [50/53], [94mLoss[0m : 9.95321

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.891, [92mTest[0m: 9.725, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.69268
[1mStep[0m  [5/53], [94mLoss[0m : 9.76550
[1mStep[0m  [10/53], [94mLoss[0m : 9.73826
[1mStep[0m  [15/53], [94mLoss[0m : 9.91506
[1mStep[0m  [20/53], [94mLoss[0m : 10.00177
[1mStep[0m  [25/53], [94mLoss[0m : 10.20469
[1mStep[0m  [30/53], [94mLoss[0m : 10.28243
[1mStep[0m  [35/53], [94mLoss[0m : 9.75551
[1mStep[0m  [40/53], [94mLoss[0m : 9.65418
[1mStep[0m  [45/53], [94mLoss[0m : 9.76747
[1mStep[0m  [50/53], [94mLoss[0m : 9.99760

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.852, [92mTest[0m: 9.734, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.73143
[1mStep[0m  [5/53], [94mLoss[0m : 10.10265
[1mStep[0m  [10/53], [94mLoss[0m : 9.82545
[1mStep[0m  [15/53], [94mLoss[0m : 9.90982
[1mStep[0m  [20/53], [94mLoss[0m : 9.31512
[1mStep[0m  [25/53], [94mLoss[0m : 9.82103
[1mStep[0m  [30/53], [94mLoss[0m : 9.89354
[1mStep[0m  [35/53], [94mLoss[0m : 9.94802
[1mStep[0m  [40/53], [94mLoss[0m : 9.58811
[1mStep[0m  [45/53], [94mLoss[0m : 10.12619
[1mStep[0m  [50/53], [94mLoss[0m : 9.35370

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.829, [92mTest[0m: 9.698, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.44972
[1mStep[0m  [5/53], [94mLoss[0m : 9.65464
[1mStep[0m  [10/53], [94mLoss[0m : 9.58997
[1mStep[0m  [15/53], [94mLoss[0m : 9.70876
[1mStep[0m  [20/53], [94mLoss[0m : 9.78258
[1mStep[0m  [25/53], [94mLoss[0m : 9.64905
[1mStep[0m  [30/53], [94mLoss[0m : 10.25135
[1mStep[0m  [35/53], [94mLoss[0m : 9.62400
[1mStep[0m  [40/53], [94mLoss[0m : 9.92255
[1mStep[0m  [45/53], [94mLoss[0m : 9.80396
[1mStep[0m  [50/53], [94mLoss[0m : 9.73046

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.805, [92mTest[0m: 9.670, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.88999
[1mStep[0m  [5/53], [94mLoss[0m : 9.39242
[1mStep[0m  [10/53], [94mLoss[0m : 9.65899
[1mStep[0m  [15/53], [94mLoss[0m : 10.01044
[1mStep[0m  [20/53], [94mLoss[0m : 9.51198
[1mStep[0m  [25/53], [94mLoss[0m : 9.14980
[1mStep[0m  [30/53], [94mLoss[0m : 9.99216
[1mStep[0m  [35/53], [94mLoss[0m : 9.45531
[1mStep[0m  [40/53], [94mLoss[0m : 9.59498
[1mStep[0m  [45/53], [94mLoss[0m : 9.89696
[1mStep[0m  [50/53], [94mLoss[0m : 9.69306

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.775, [92mTest[0m: 9.657, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.98524
[1mStep[0m  [5/53], [94mLoss[0m : 9.77396
[1mStep[0m  [10/53], [94mLoss[0m : 9.45648
[1mStep[0m  [15/53], [94mLoss[0m : 9.74589
[1mStep[0m  [20/53], [94mLoss[0m : 9.78577
[1mStep[0m  [25/53], [94mLoss[0m : 9.83293
[1mStep[0m  [30/53], [94mLoss[0m : 9.74109
[1mStep[0m  [35/53], [94mLoss[0m : 9.50887
[1mStep[0m  [40/53], [94mLoss[0m : 9.65400
[1mStep[0m  [45/53], [94mLoss[0m : 9.98931
[1mStep[0m  [50/53], [94mLoss[0m : 10.25311

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.757, [92mTest[0m: 9.624, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.66190
[1mStep[0m  [5/53], [94mLoss[0m : 9.53091
[1mStep[0m  [10/53], [94mLoss[0m : 10.04558
[1mStep[0m  [15/53], [94mLoss[0m : 9.65081
[1mStep[0m  [20/53], [94mLoss[0m : 9.93592
[1mStep[0m  [25/53], [94mLoss[0m : 9.89166
[1mStep[0m  [30/53], [94mLoss[0m : 9.84688
[1mStep[0m  [35/53], [94mLoss[0m : 9.26007
[1mStep[0m  [40/53], [94mLoss[0m : 9.56532
[1mStep[0m  [45/53], [94mLoss[0m : 9.63407
[1mStep[0m  [50/53], [94mLoss[0m : 9.90553

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.726, [92mTest[0m: 9.583, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.59246
[1mStep[0m  [5/53], [94mLoss[0m : 9.47606
[1mStep[0m  [10/53], [94mLoss[0m : 9.48825
[1mStep[0m  [15/53], [94mLoss[0m : 10.27448
[1mStep[0m  [20/53], [94mLoss[0m : 9.39867
[1mStep[0m  [25/53], [94mLoss[0m : 9.21329
[1mStep[0m  [30/53], [94mLoss[0m : 9.67330
[1mStep[0m  [35/53], [94mLoss[0m : 9.64909
[1mStep[0m  [40/53], [94mLoss[0m : 9.93000
[1mStep[0m  [45/53], [94mLoss[0m : 9.84539
[1mStep[0m  [50/53], [94mLoss[0m : 9.67982

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.704, [92mTest[0m: 9.510, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.87084
[1mStep[0m  [5/53], [94mLoss[0m : 9.65368
[1mStep[0m  [10/53], [94mLoss[0m : 9.49279
[1mStep[0m  [15/53], [94mLoss[0m : 9.65237
[1mStep[0m  [20/53], [94mLoss[0m : 9.76040
[1mStep[0m  [25/53], [94mLoss[0m : 10.01005
[1mStep[0m  [30/53], [94mLoss[0m : 9.88909
[1mStep[0m  [35/53], [94mLoss[0m : 9.70488
[1mStep[0m  [40/53], [94mLoss[0m : 9.73987
[1mStep[0m  [45/53], [94mLoss[0m : 9.64237
[1mStep[0m  [50/53], [94mLoss[0m : 9.73796

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.681, [92mTest[0m: 9.540, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.474
====================================

Phase 2 - Evaluation MAE:  9.473599213820238
MAE score P1      10.375478
MAE score P2       9.473599
loss               9.681177
learning_rate        0.0001
batch_size              256
hidden_sizes          [100]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 11.80291
[1mStep[0m  [5/53], [94mLoss[0m : 11.56563
[1mStep[0m  [10/53], [94mLoss[0m : 11.32889
[1mStep[0m  [15/53], [94mLoss[0m : 11.68719
[1mStep[0m  [20/53], [94mLoss[0m : 11.50787
[1mStep[0m  [25/53], [94mLoss[0m : 11.13498
[1mStep[0m  [30/53], [94mLoss[0m : 11.14158
[1mStep[0m  [35/53], [94mLoss[0m : 11.20099
[1mStep[0m  [40/53], [94mLoss[0m : 11.31568
[1mStep[0m  [45/53], [94mLoss[0m : 11.08198
[1mStep[0m  [50/53], [94mLoss[0m : 11.10797

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.252, [92mTest[0m: 11.348, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.39861
[1mStep[0m  [5/53], [94mLoss[0m : 10.90161
[1mStep[0m  [10/53], [94mLoss[0m : 11.21673
[1mStep[0m  [15/53], [94mLoss[0m : 10.92240
[1mStep[0m  [20/53], [94mLoss[0m : 11.27385
[1mStep[0m  [25/53], [94mLoss[0m : 11.28087
[1mStep[0m  [30/53], [94mLoss[0m : 10.82435
[1mStep[0m  [35/53], [94mLoss[0m : 11.20709
[1mStep[0m  [40/53], [94mLoss[0m : 10.98096
[1mStep[0m  [45/53], [94mLoss[0m : 11.17266
[1mStep[0m  [50/53], [94mLoss[0m : 10.58278

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 11.099, [92mTest[0m: 11.161, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 11.12617
[1mStep[0m  [5/53], [94mLoss[0m : 10.30587
[1mStep[0m  [10/53], [94mLoss[0m : 11.11998
[1mStep[0m  [15/53], [94mLoss[0m : 10.43097
[1mStep[0m  [20/53], [94mLoss[0m : 11.16981
[1mStep[0m  [25/53], [94mLoss[0m : 10.56522
[1mStep[0m  [30/53], [94mLoss[0m : 11.27940
[1mStep[0m  [35/53], [94mLoss[0m : 10.96165
[1mStep[0m  [40/53], [94mLoss[0m : 11.04873
[1mStep[0m  [45/53], [94mLoss[0m : 10.95676
[1mStep[0m  [50/53], [94mLoss[0m : 11.03672

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.948, [92mTest[0m: 11.013, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.99758
[1mStep[0m  [5/53], [94mLoss[0m : 11.05230
[1mStep[0m  [10/53], [94mLoss[0m : 10.98090
[1mStep[0m  [15/53], [94mLoss[0m : 11.13347
[1mStep[0m  [20/53], [94mLoss[0m : 10.76050
[1mStep[0m  [25/53], [94mLoss[0m : 10.78369
[1mStep[0m  [30/53], [94mLoss[0m : 10.74923
[1mStep[0m  [35/53], [94mLoss[0m : 10.78469
[1mStep[0m  [40/53], [94mLoss[0m : 10.58415
[1mStep[0m  [45/53], [94mLoss[0m : 10.75563
[1mStep[0m  [50/53], [94mLoss[0m : 11.04585

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.798, [92mTest[0m: 10.852, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.67253
[1mStep[0m  [5/53], [94mLoss[0m : 10.89490
[1mStep[0m  [10/53], [94mLoss[0m : 10.70280
[1mStep[0m  [15/53], [94mLoss[0m : 10.71196
[1mStep[0m  [20/53], [94mLoss[0m : 10.84383
[1mStep[0m  [25/53], [94mLoss[0m : 10.50796
[1mStep[0m  [30/53], [94mLoss[0m : 10.54813
[1mStep[0m  [35/53], [94mLoss[0m : 10.76866
[1mStep[0m  [40/53], [94mLoss[0m : 10.62380
[1mStep[0m  [45/53], [94mLoss[0m : 10.76220
[1mStep[0m  [50/53], [94mLoss[0m : 10.26680

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.648, [92mTest[0m: 10.691, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.69220
[1mStep[0m  [5/53], [94mLoss[0m : 10.22709
[1mStep[0m  [10/53], [94mLoss[0m : 10.53618
[1mStep[0m  [15/53], [94mLoss[0m : 10.71028
[1mStep[0m  [20/53], [94mLoss[0m : 10.12992
[1mStep[0m  [25/53], [94mLoss[0m : 10.23072
[1mStep[0m  [30/53], [94mLoss[0m : 10.55682
[1mStep[0m  [35/53], [94mLoss[0m : 10.42579
[1mStep[0m  [40/53], [94mLoss[0m : 10.16580
[1mStep[0m  [45/53], [94mLoss[0m : 10.25108
[1mStep[0m  [50/53], [94mLoss[0m : 10.11997

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.488, [92mTest[0m: 10.550, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.14755
[1mStep[0m  [5/53], [94mLoss[0m : 10.27761
[1mStep[0m  [10/53], [94mLoss[0m : 9.76474
[1mStep[0m  [15/53], [94mLoss[0m : 10.47664
[1mStep[0m  [20/53], [94mLoss[0m : 10.63051
[1mStep[0m  [25/53], [94mLoss[0m : 10.49199
[1mStep[0m  [30/53], [94mLoss[0m : 10.50623
[1mStep[0m  [35/53], [94mLoss[0m : 10.24518
[1mStep[0m  [40/53], [94mLoss[0m : 10.21182
[1mStep[0m  [45/53], [94mLoss[0m : 10.40683
[1mStep[0m  [50/53], [94mLoss[0m : 10.24650

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.341, [92mTest[0m: 10.388, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.23238
[1mStep[0m  [5/53], [94mLoss[0m : 9.84838
[1mStep[0m  [10/53], [94mLoss[0m : 10.24029
[1mStep[0m  [15/53], [94mLoss[0m : 10.29140
[1mStep[0m  [20/53], [94mLoss[0m : 10.24085
[1mStep[0m  [25/53], [94mLoss[0m : 9.80852
[1mStep[0m  [30/53], [94mLoss[0m : 10.21557
[1mStep[0m  [35/53], [94mLoss[0m : 10.41478
[1mStep[0m  [40/53], [94mLoss[0m : 10.07575
[1mStep[0m  [45/53], [94mLoss[0m : 9.69573
[1mStep[0m  [50/53], [94mLoss[0m : 10.10768

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.181, [92mTest[0m: 10.244, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.87868
[1mStep[0m  [5/53], [94mLoss[0m : 9.97360
[1mStep[0m  [10/53], [94mLoss[0m : 10.15693
[1mStep[0m  [15/53], [94mLoss[0m : 10.16962
[1mStep[0m  [20/53], [94mLoss[0m : 10.01502
[1mStep[0m  [25/53], [94mLoss[0m : 9.83304
[1mStep[0m  [30/53], [94mLoss[0m : 9.84155
[1mStep[0m  [35/53], [94mLoss[0m : 9.89473
[1mStep[0m  [40/53], [94mLoss[0m : 9.98343
[1mStep[0m  [45/53], [94mLoss[0m : 9.95620
[1mStep[0m  [50/53], [94mLoss[0m : 10.16492

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.030, [92mTest[0m: 10.091, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.83560
[1mStep[0m  [5/53], [94mLoss[0m : 10.00378
[1mStep[0m  [10/53], [94mLoss[0m : 9.72289
[1mStep[0m  [15/53], [94mLoss[0m : 9.55463
[1mStep[0m  [20/53], [94mLoss[0m : 10.02728
[1mStep[0m  [25/53], [94mLoss[0m : 9.74771
[1mStep[0m  [30/53], [94mLoss[0m : 10.01045
[1mStep[0m  [35/53], [94mLoss[0m : 9.79655
[1mStep[0m  [40/53], [94mLoss[0m : 9.76302
[1mStep[0m  [45/53], [94mLoss[0m : 10.05886
[1mStep[0m  [50/53], [94mLoss[0m : 9.60225

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.879, [92mTest[0m: 9.929, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.81389
[1mStep[0m  [5/53], [94mLoss[0m : 9.45541
[1mStep[0m  [10/53], [94mLoss[0m : 9.66904
[1mStep[0m  [15/53], [94mLoss[0m : 9.54385
[1mStep[0m  [20/53], [94mLoss[0m : 9.80847
[1mStep[0m  [25/53], [94mLoss[0m : 9.99812
[1mStep[0m  [30/53], [94mLoss[0m : 9.89243
[1mStep[0m  [35/53], [94mLoss[0m : 9.59589
[1mStep[0m  [40/53], [94mLoss[0m : 9.73963
[1mStep[0m  [45/53], [94mLoss[0m : 9.92812
[1mStep[0m  [50/53], [94mLoss[0m : 9.30167

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.720, [92mTest[0m: 9.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.47811
[1mStep[0m  [5/53], [94mLoss[0m : 9.82611
[1mStep[0m  [10/53], [94mLoss[0m : 9.78487
[1mStep[0m  [15/53], [94mLoss[0m : 9.77714
[1mStep[0m  [20/53], [94mLoss[0m : 9.55586
[1mStep[0m  [25/53], [94mLoss[0m : 9.72840
[1mStep[0m  [30/53], [94mLoss[0m : 9.60433
[1mStep[0m  [35/53], [94mLoss[0m : 9.86572
[1mStep[0m  [40/53], [94mLoss[0m : 9.46474
[1mStep[0m  [45/53], [94mLoss[0m : 9.77850
[1mStep[0m  [50/53], [94mLoss[0m : 9.76645

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.566, [92mTest[0m: 9.629, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.38255
[1mStep[0m  [5/53], [94mLoss[0m : 9.31043
[1mStep[0m  [10/53], [94mLoss[0m : 8.99664
[1mStep[0m  [15/53], [94mLoss[0m : 9.82218
[1mStep[0m  [20/53], [94mLoss[0m : 9.39849
[1mStep[0m  [25/53], [94mLoss[0m : 9.26989
[1mStep[0m  [30/53], [94mLoss[0m : 9.16060
[1mStep[0m  [35/53], [94mLoss[0m : 9.40236
[1mStep[0m  [40/53], [94mLoss[0m : 9.49534
[1mStep[0m  [45/53], [94mLoss[0m : 9.12155
[1mStep[0m  [50/53], [94mLoss[0m : 9.75481

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.412, [92mTest[0m: 9.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.23317
[1mStep[0m  [5/53], [94mLoss[0m : 9.24552
[1mStep[0m  [10/53], [94mLoss[0m : 9.56941
[1mStep[0m  [15/53], [94mLoss[0m : 9.27796
[1mStep[0m  [20/53], [94mLoss[0m : 9.40347
[1mStep[0m  [25/53], [94mLoss[0m : 9.48396
[1mStep[0m  [30/53], [94mLoss[0m : 9.31748
[1mStep[0m  [35/53], [94mLoss[0m : 9.31587
[1mStep[0m  [40/53], [94mLoss[0m : 9.82939
[1mStep[0m  [45/53], [94mLoss[0m : 9.12939
[1mStep[0m  [50/53], [94mLoss[0m : 9.34518

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.264, [92mTest[0m: 9.335, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.44327
[1mStep[0m  [5/53], [94mLoss[0m : 9.15496
[1mStep[0m  [10/53], [94mLoss[0m : 8.97909
[1mStep[0m  [15/53], [94mLoss[0m : 9.24802
[1mStep[0m  [20/53], [94mLoss[0m : 8.88553
[1mStep[0m  [25/53], [94mLoss[0m : 9.06927
[1mStep[0m  [30/53], [94mLoss[0m : 9.28322
[1mStep[0m  [35/53], [94mLoss[0m : 9.37928
[1mStep[0m  [40/53], [94mLoss[0m : 9.00471
[1mStep[0m  [45/53], [94mLoss[0m : 8.60844
[1mStep[0m  [50/53], [94mLoss[0m : 9.08783

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.108, [92mTest[0m: 9.175, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.09499
[1mStep[0m  [5/53], [94mLoss[0m : 8.41892
[1mStep[0m  [10/53], [94mLoss[0m : 9.30112
[1mStep[0m  [15/53], [94mLoss[0m : 9.22260
[1mStep[0m  [20/53], [94mLoss[0m : 9.03353
[1mStep[0m  [25/53], [94mLoss[0m : 9.01219
[1mStep[0m  [30/53], [94mLoss[0m : 8.76713
[1mStep[0m  [35/53], [94mLoss[0m : 8.78728
[1mStep[0m  [40/53], [94mLoss[0m : 9.30125
[1mStep[0m  [45/53], [94mLoss[0m : 8.86430
[1mStep[0m  [50/53], [94mLoss[0m : 9.12774

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.957, [92mTest[0m: 9.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.92612
[1mStep[0m  [5/53], [94mLoss[0m : 8.72969
[1mStep[0m  [10/53], [94mLoss[0m : 8.95977
[1mStep[0m  [15/53], [94mLoss[0m : 8.75982
[1mStep[0m  [20/53], [94mLoss[0m : 9.06137
[1mStep[0m  [25/53], [94mLoss[0m : 8.39095
[1mStep[0m  [30/53], [94mLoss[0m : 8.93054
[1mStep[0m  [35/53], [94mLoss[0m : 8.55909
[1mStep[0m  [40/53], [94mLoss[0m : 9.22401
[1mStep[0m  [45/53], [94mLoss[0m : 8.73278
[1mStep[0m  [50/53], [94mLoss[0m : 8.31728

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.802, [92mTest[0m: 8.887, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.65873
[1mStep[0m  [5/53], [94mLoss[0m : 9.10818
[1mStep[0m  [10/53], [94mLoss[0m : 9.00153
[1mStep[0m  [15/53], [94mLoss[0m : 8.43744
[1mStep[0m  [20/53], [94mLoss[0m : 8.70885
[1mStep[0m  [25/53], [94mLoss[0m : 8.29405
[1mStep[0m  [30/53], [94mLoss[0m : 8.86412
[1mStep[0m  [35/53], [94mLoss[0m : 8.53323
[1mStep[0m  [40/53], [94mLoss[0m : 8.60494
[1mStep[0m  [45/53], [94mLoss[0m : 8.28805
[1mStep[0m  [50/53], [94mLoss[0m : 8.77095

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.654, [92mTest[0m: 8.724, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.59611
[1mStep[0m  [5/53], [94mLoss[0m : 8.61147
[1mStep[0m  [10/53], [94mLoss[0m : 8.23882
[1mStep[0m  [15/53], [94mLoss[0m : 9.15747
[1mStep[0m  [20/53], [94mLoss[0m : 8.48271
[1mStep[0m  [25/53], [94mLoss[0m : 8.67490
[1mStep[0m  [30/53], [94mLoss[0m : 8.29137
[1mStep[0m  [35/53], [94mLoss[0m : 8.42858
[1mStep[0m  [40/53], [94mLoss[0m : 8.57382
[1mStep[0m  [45/53], [94mLoss[0m : 8.31752
[1mStep[0m  [50/53], [94mLoss[0m : 8.46819

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.497, [92mTest[0m: 8.574, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.47658
[1mStep[0m  [5/53], [94mLoss[0m : 8.36394
[1mStep[0m  [10/53], [94mLoss[0m : 8.22918
[1mStep[0m  [15/53], [94mLoss[0m : 8.04049
[1mStep[0m  [20/53], [94mLoss[0m : 8.15367
[1mStep[0m  [25/53], [94mLoss[0m : 8.24459
[1mStep[0m  [30/53], [94mLoss[0m : 7.84338
[1mStep[0m  [35/53], [94mLoss[0m : 8.53444
[1mStep[0m  [40/53], [94mLoss[0m : 8.46752
[1mStep[0m  [45/53], [94mLoss[0m : 8.28383
[1mStep[0m  [50/53], [94mLoss[0m : 8.40388

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.347, [92mTest[0m: 8.421, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.20761
[1mStep[0m  [5/53], [94mLoss[0m : 8.21653
[1mStep[0m  [10/53], [94mLoss[0m : 8.35663
[1mStep[0m  [15/53], [94mLoss[0m : 8.20725
[1mStep[0m  [20/53], [94mLoss[0m : 7.81268
[1mStep[0m  [25/53], [94mLoss[0m : 8.35977
[1mStep[0m  [30/53], [94mLoss[0m : 8.19533
[1mStep[0m  [35/53], [94mLoss[0m : 8.30511
[1mStep[0m  [40/53], [94mLoss[0m : 8.06248
[1mStep[0m  [45/53], [94mLoss[0m : 8.30831
[1mStep[0m  [50/53], [94mLoss[0m : 7.90191

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.202, [92mTest[0m: 8.253, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.11347
[1mStep[0m  [5/53], [94mLoss[0m : 8.24117
[1mStep[0m  [10/53], [94mLoss[0m : 8.48920
[1mStep[0m  [15/53], [94mLoss[0m : 8.26346
[1mStep[0m  [20/53], [94mLoss[0m : 7.59928
[1mStep[0m  [25/53], [94mLoss[0m : 8.35951
[1mStep[0m  [30/53], [94mLoss[0m : 8.04723
[1mStep[0m  [35/53], [94mLoss[0m : 8.00547
[1mStep[0m  [40/53], [94mLoss[0m : 8.12417
[1mStep[0m  [45/53], [94mLoss[0m : 8.06686
[1mStep[0m  [50/53], [94mLoss[0m : 7.66218

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.066, [92mTest[0m: 8.122, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.95319
[1mStep[0m  [5/53], [94mLoss[0m : 8.17811
[1mStep[0m  [10/53], [94mLoss[0m : 8.05741
[1mStep[0m  [15/53], [94mLoss[0m : 7.91541
[1mStep[0m  [20/53], [94mLoss[0m : 7.91186
[1mStep[0m  [25/53], [94mLoss[0m : 7.67729
[1mStep[0m  [30/53], [94mLoss[0m : 8.02573
[1mStep[0m  [35/53], [94mLoss[0m : 8.04471
[1mStep[0m  [40/53], [94mLoss[0m : 7.98056
[1mStep[0m  [45/53], [94mLoss[0m : 8.03319
[1mStep[0m  [50/53], [94mLoss[0m : 7.84435

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.920, [92mTest[0m: 7.978, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.34828
[1mStep[0m  [5/53], [94mLoss[0m : 7.48472
[1mStep[0m  [10/53], [94mLoss[0m : 7.29636
[1mStep[0m  [15/53], [94mLoss[0m : 7.86040
[1mStep[0m  [20/53], [94mLoss[0m : 7.45064
[1mStep[0m  [25/53], [94mLoss[0m : 8.21447
[1mStep[0m  [30/53], [94mLoss[0m : 7.57991
[1mStep[0m  [35/53], [94mLoss[0m : 8.15600
[1mStep[0m  [40/53], [94mLoss[0m : 8.01416
[1mStep[0m  [45/53], [94mLoss[0m : 7.53528
[1mStep[0m  [50/53], [94mLoss[0m : 7.90135

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.784, [92mTest[0m: 7.841, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.48880
[1mStep[0m  [5/53], [94mLoss[0m : 7.47603
[1mStep[0m  [10/53], [94mLoss[0m : 7.93351
[1mStep[0m  [15/53], [94mLoss[0m : 7.73167
[1mStep[0m  [20/53], [94mLoss[0m : 7.26746
[1mStep[0m  [25/53], [94mLoss[0m : 7.85984
[1mStep[0m  [30/53], [94mLoss[0m : 7.40530
[1mStep[0m  [35/53], [94mLoss[0m : 7.44358
[1mStep[0m  [40/53], [94mLoss[0m : 7.60709
[1mStep[0m  [45/53], [94mLoss[0m : 8.02505
[1mStep[0m  [50/53], [94mLoss[0m : 7.66700

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.650, [92mTest[0m: 7.707, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.65193
[1mStep[0m  [5/53], [94mLoss[0m : 7.45531
[1mStep[0m  [10/53], [94mLoss[0m : 7.66213
[1mStep[0m  [15/53], [94mLoss[0m : 7.34725
[1mStep[0m  [20/53], [94mLoss[0m : 7.16075
[1mStep[0m  [25/53], [94mLoss[0m : 7.18330
[1mStep[0m  [30/53], [94mLoss[0m : 7.40359
[1mStep[0m  [35/53], [94mLoss[0m : 7.85993
[1mStep[0m  [40/53], [94mLoss[0m : 7.77388
[1mStep[0m  [45/53], [94mLoss[0m : 7.16851
[1mStep[0m  [50/53], [94mLoss[0m : 7.39557

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.511, [92mTest[0m: 7.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.39625
[1mStep[0m  [5/53], [94mLoss[0m : 7.26532
[1mStep[0m  [10/53], [94mLoss[0m : 7.45897
[1mStep[0m  [15/53], [94mLoss[0m : 7.66586
[1mStep[0m  [20/53], [94mLoss[0m : 7.22682
[1mStep[0m  [25/53], [94mLoss[0m : 7.11862
[1mStep[0m  [30/53], [94mLoss[0m : 7.10812
[1mStep[0m  [35/53], [94mLoss[0m : 7.33258
[1mStep[0m  [40/53], [94mLoss[0m : 7.44701
[1mStep[0m  [45/53], [94mLoss[0m : 7.12808
[1mStep[0m  [50/53], [94mLoss[0m : 7.40775

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.367, [92mTest[0m: 7.440, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.18934
[1mStep[0m  [5/53], [94mLoss[0m : 7.15269
[1mStep[0m  [10/53], [94mLoss[0m : 7.79074
[1mStep[0m  [15/53], [94mLoss[0m : 7.18036
[1mStep[0m  [20/53], [94mLoss[0m : 7.17316
[1mStep[0m  [25/53], [94mLoss[0m : 7.35180
[1mStep[0m  [30/53], [94mLoss[0m : 7.52687
[1mStep[0m  [35/53], [94mLoss[0m : 7.58998
[1mStep[0m  [40/53], [94mLoss[0m : 7.68071
[1mStep[0m  [45/53], [94mLoss[0m : 7.18513
[1mStep[0m  [50/53], [94mLoss[0m : 7.30055

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.227, [92mTest[0m: 7.310, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.09528
[1mStep[0m  [5/53], [94mLoss[0m : 7.11235
[1mStep[0m  [10/53], [94mLoss[0m : 6.77501
[1mStep[0m  [15/53], [94mLoss[0m : 7.23103
[1mStep[0m  [20/53], [94mLoss[0m : 7.51262
[1mStep[0m  [25/53], [94mLoss[0m : 7.10086
[1mStep[0m  [30/53], [94mLoss[0m : 6.85597
[1mStep[0m  [35/53], [94mLoss[0m : 7.42305
[1mStep[0m  [40/53], [94mLoss[0m : 7.15696
[1mStep[0m  [45/53], [94mLoss[0m : 6.97031
[1mStep[0m  [50/53], [94mLoss[0m : 6.78721

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.085, [92mTest[0m: 7.156, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 7.32399
[1mStep[0m  [5/53], [94mLoss[0m : 6.73924
[1mStep[0m  [10/53], [94mLoss[0m : 6.87638
[1mStep[0m  [15/53], [94mLoss[0m : 6.86142
[1mStep[0m  [20/53], [94mLoss[0m : 6.64713
[1mStep[0m  [25/53], [94mLoss[0m : 7.08345
[1mStep[0m  [30/53], [94mLoss[0m : 7.36380
[1mStep[0m  [35/53], [94mLoss[0m : 6.52091
[1mStep[0m  [40/53], [94mLoss[0m : 6.68352
[1mStep[0m  [45/53], [94mLoss[0m : 6.55782
[1mStep[0m  [50/53], [94mLoss[0m : 6.76296

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.960, [92mTest[0m: 7.020, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.880
====================================

Phase 1 - Evaluation MAE:  6.87964311012855
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/53], [94mLoss[0m : 6.94881
[1mStep[0m  [5/53], [94mLoss[0m : 6.71138
[1mStep[0m  [10/53], [94mLoss[0m : 6.36549
[1mStep[0m  [15/53], [94mLoss[0m : 7.00810
[1mStep[0m  [20/53], [94mLoss[0m : 7.16066
[1mStep[0m  [25/53], [94mLoss[0m : 6.87054
[1mStep[0m  [30/53], [94mLoss[0m : 6.76662
[1mStep[0m  [35/53], [94mLoss[0m : 6.94721
[1mStep[0m  [40/53], [94mLoss[0m : 6.91621
[1mStep[0m  [45/53], [94mLoss[0m : 6.95073
[1mStep[0m  [50/53], [94mLoss[0m : 6.58918

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.808, [92mTest[0m: 6.882, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.89174
[1mStep[0m  [5/53], [94mLoss[0m : 6.48738
[1mStep[0m  [10/53], [94mLoss[0m : 6.56605
[1mStep[0m  [15/53], [94mLoss[0m : 6.68444
[1mStep[0m  [20/53], [94mLoss[0m : 6.74076
[1mStep[0m  [25/53], [94mLoss[0m : 6.96897
[1mStep[0m  [30/53], [94mLoss[0m : 6.20843
[1mStep[0m  [35/53], [94mLoss[0m : 6.64484
[1mStep[0m  [40/53], [94mLoss[0m : 6.82830
[1mStep[0m  [45/53], [94mLoss[0m : 6.78982
[1mStep[0m  [50/53], [94mLoss[0m : 6.73586

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.655, [92mTest[0m: 6.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.65217
[1mStep[0m  [5/53], [94mLoss[0m : 6.34006
[1mStep[0m  [10/53], [94mLoss[0m : 6.56036
[1mStep[0m  [15/53], [94mLoss[0m : 6.57345
[1mStep[0m  [20/53], [94mLoss[0m : 6.71380
[1mStep[0m  [25/53], [94mLoss[0m : 6.58861
[1mStep[0m  [30/53], [94mLoss[0m : 6.91198
[1mStep[0m  [35/53], [94mLoss[0m : 6.25273
[1mStep[0m  [40/53], [94mLoss[0m : 6.98618
[1mStep[0m  [45/53], [94mLoss[0m : 6.13949
[1mStep[0m  [50/53], [94mLoss[0m : 6.54858

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.507, [92mTest[0m: 6.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.32743
[1mStep[0m  [5/53], [94mLoss[0m : 6.21356
[1mStep[0m  [10/53], [94mLoss[0m : 6.16322
[1mStep[0m  [15/53], [94mLoss[0m : 6.33928
[1mStep[0m  [20/53], [94mLoss[0m : 6.20251
[1mStep[0m  [25/53], [94mLoss[0m : 6.47371
[1mStep[0m  [30/53], [94mLoss[0m : 6.42505
[1mStep[0m  [35/53], [94mLoss[0m : 6.05085
[1mStep[0m  [40/53], [94mLoss[0m : 6.68502
[1mStep[0m  [45/53], [94mLoss[0m : 6.33555
[1mStep[0m  [50/53], [94mLoss[0m : 5.54441

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.346, [92mTest[0m: 6.422, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.10513
[1mStep[0m  [5/53], [94mLoss[0m : 6.23133
[1mStep[0m  [10/53], [94mLoss[0m : 6.06115
[1mStep[0m  [15/53], [94mLoss[0m : 6.27886
[1mStep[0m  [20/53], [94mLoss[0m : 6.46925
[1mStep[0m  [25/53], [94mLoss[0m : 6.00331
[1mStep[0m  [30/53], [94mLoss[0m : 6.49172
[1mStep[0m  [35/53], [94mLoss[0m : 5.73551
[1mStep[0m  [40/53], [94mLoss[0m : 6.05460
[1mStep[0m  [45/53], [94mLoss[0m : 6.29283
[1mStep[0m  [50/53], [94mLoss[0m : 6.21476

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.200, [92mTest[0m: 6.275, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 6.24948
[1mStep[0m  [5/53], [94mLoss[0m : 6.07110
[1mStep[0m  [10/53], [94mLoss[0m : 5.83597
[1mStep[0m  [15/53], [94mLoss[0m : 6.12771
[1mStep[0m  [20/53], [94mLoss[0m : 5.73719
[1mStep[0m  [25/53], [94mLoss[0m : 5.95013
[1mStep[0m  [30/53], [94mLoss[0m : 6.18773
[1mStep[0m  [35/53], [94mLoss[0m : 6.25487
[1mStep[0m  [40/53], [94mLoss[0m : 6.18767
[1mStep[0m  [45/53], [94mLoss[0m : 6.25011
[1mStep[0m  [50/53], [94mLoss[0m : 6.03310

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.049, [92mTest[0m: 6.125, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.90765
[1mStep[0m  [5/53], [94mLoss[0m : 6.00094
[1mStep[0m  [10/53], [94mLoss[0m : 5.94088
[1mStep[0m  [15/53], [94mLoss[0m : 5.77110
[1mStep[0m  [20/53], [94mLoss[0m : 5.71635
[1mStep[0m  [25/53], [94mLoss[0m : 5.98604
[1mStep[0m  [30/53], [94mLoss[0m : 6.21836
[1mStep[0m  [35/53], [94mLoss[0m : 6.09642
[1mStep[0m  [40/53], [94mLoss[0m : 5.78065
[1mStep[0m  [45/53], [94mLoss[0m : 5.53938
[1mStep[0m  [50/53], [94mLoss[0m : 6.26731

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.895, [92mTest[0m: 5.970, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.62770
[1mStep[0m  [5/53], [94mLoss[0m : 5.60520
[1mStep[0m  [10/53], [94mLoss[0m : 5.72370
[1mStep[0m  [15/53], [94mLoss[0m : 5.68229
[1mStep[0m  [20/53], [94mLoss[0m : 6.01348
[1mStep[0m  [25/53], [94mLoss[0m : 5.56563
[1mStep[0m  [30/53], [94mLoss[0m : 5.30805
[1mStep[0m  [35/53], [94mLoss[0m : 5.89713
[1mStep[0m  [40/53], [94mLoss[0m : 6.32352
[1mStep[0m  [45/53], [94mLoss[0m : 5.78468
[1mStep[0m  [50/53], [94mLoss[0m : 5.83359

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.740, [92mTest[0m: 5.818, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.39981
[1mStep[0m  [5/53], [94mLoss[0m : 5.47059
[1mStep[0m  [10/53], [94mLoss[0m : 5.75331
[1mStep[0m  [15/53], [94mLoss[0m : 5.61982
[1mStep[0m  [20/53], [94mLoss[0m : 5.27441
[1mStep[0m  [25/53], [94mLoss[0m : 5.60824
[1mStep[0m  [30/53], [94mLoss[0m : 5.45009
[1mStep[0m  [35/53], [94mLoss[0m : 5.72094
[1mStep[0m  [40/53], [94mLoss[0m : 5.43410
[1mStep[0m  [45/53], [94mLoss[0m : 5.65911
[1mStep[0m  [50/53], [94mLoss[0m : 5.56728

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.594, [92mTest[0m: 5.655, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.68114
[1mStep[0m  [5/53], [94mLoss[0m : 5.40433
[1mStep[0m  [10/53], [94mLoss[0m : 5.33273
[1mStep[0m  [15/53], [94mLoss[0m : 5.27509
[1mStep[0m  [20/53], [94mLoss[0m : 5.31888
[1mStep[0m  [25/53], [94mLoss[0m : 5.29360
[1mStep[0m  [30/53], [94mLoss[0m : 5.54483
[1mStep[0m  [35/53], [94mLoss[0m : 5.58392
[1mStep[0m  [40/53], [94mLoss[0m : 5.54880
[1mStep[0m  [45/53], [94mLoss[0m : 5.29076
[1mStep[0m  [50/53], [94mLoss[0m : 5.43162

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.461, [92mTest[0m: 5.509, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.32187
[1mStep[0m  [5/53], [94mLoss[0m : 5.18350
[1mStep[0m  [10/53], [94mLoss[0m : 5.45749
[1mStep[0m  [15/53], [94mLoss[0m : 5.36445
[1mStep[0m  [20/53], [94mLoss[0m : 4.98464
[1mStep[0m  [25/53], [94mLoss[0m : 5.51253
[1mStep[0m  [30/53], [94mLoss[0m : 4.73620
[1mStep[0m  [35/53], [94mLoss[0m : 5.11480
[1mStep[0m  [40/53], [94mLoss[0m : 5.19533
[1mStep[0m  [45/53], [94mLoss[0m : 4.93983
[1mStep[0m  [50/53], [94mLoss[0m : 5.11227

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.314, [92mTest[0m: 5.379, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.48942
[1mStep[0m  [5/53], [94mLoss[0m : 5.05643
[1mStep[0m  [10/53], [94mLoss[0m : 4.98418
[1mStep[0m  [15/53], [94mLoss[0m : 5.28905
[1mStep[0m  [20/53], [94mLoss[0m : 5.44927
[1mStep[0m  [25/53], [94mLoss[0m : 5.24566
[1mStep[0m  [30/53], [94mLoss[0m : 4.90361
[1mStep[0m  [35/53], [94mLoss[0m : 5.35833
[1mStep[0m  [40/53], [94mLoss[0m : 4.94318
[1mStep[0m  [45/53], [94mLoss[0m : 4.93030
[1mStep[0m  [50/53], [94mLoss[0m : 5.27749

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.174, [92mTest[0m: 5.232, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.09037
[1mStep[0m  [5/53], [94mLoss[0m : 4.78926
[1mStep[0m  [10/53], [94mLoss[0m : 5.14840
[1mStep[0m  [15/53], [94mLoss[0m : 5.07485
[1mStep[0m  [20/53], [94mLoss[0m : 4.83260
[1mStep[0m  [25/53], [94mLoss[0m : 4.80851
[1mStep[0m  [30/53], [94mLoss[0m : 4.86741
[1mStep[0m  [35/53], [94mLoss[0m : 5.03645
[1mStep[0m  [40/53], [94mLoss[0m : 4.98070
[1mStep[0m  [45/53], [94mLoss[0m : 5.25316
[1mStep[0m  [50/53], [94mLoss[0m : 4.65814

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.040, [92mTest[0m: 5.094, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.64094
[1mStep[0m  [5/53], [94mLoss[0m : 5.02747
[1mStep[0m  [10/53], [94mLoss[0m : 4.52992
[1mStep[0m  [15/53], [94mLoss[0m : 4.69946
[1mStep[0m  [20/53], [94mLoss[0m : 5.01377
[1mStep[0m  [25/53], [94mLoss[0m : 4.75582
[1mStep[0m  [30/53], [94mLoss[0m : 4.78050
[1mStep[0m  [35/53], [94mLoss[0m : 4.89361
[1mStep[0m  [40/53], [94mLoss[0m : 4.47513
[1mStep[0m  [45/53], [94mLoss[0m : 4.54534
[1mStep[0m  [50/53], [94mLoss[0m : 4.88711

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.916, [92mTest[0m: 4.947, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 5.10712
[1mStep[0m  [5/53], [94mLoss[0m : 4.87775
[1mStep[0m  [10/53], [94mLoss[0m : 4.65628
[1mStep[0m  [15/53], [94mLoss[0m : 4.92540
[1mStep[0m  [20/53], [94mLoss[0m : 5.08363
[1mStep[0m  [25/53], [94mLoss[0m : 4.88743
[1mStep[0m  [30/53], [94mLoss[0m : 4.70024
[1mStep[0m  [35/53], [94mLoss[0m : 4.43163
[1mStep[0m  [40/53], [94mLoss[0m : 4.56635
[1mStep[0m  [45/53], [94mLoss[0m : 4.87440
[1mStep[0m  [50/53], [94mLoss[0m : 4.41244

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.788, [92mTest[0m: 4.846, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.45010
[1mStep[0m  [5/53], [94mLoss[0m : 5.02176
[1mStep[0m  [10/53], [94mLoss[0m : 4.83506
[1mStep[0m  [15/53], [94mLoss[0m : 4.84474
[1mStep[0m  [20/53], [94mLoss[0m : 4.39345
[1mStep[0m  [25/53], [94mLoss[0m : 4.83046
[1mStep[0m  [30/53], [94mLoss[0m : 4.62674
[1mStep[0m  [35/53], [94mLoss[0m : 4.73511
[1mStep[0m  [40/53], [94mLoss[0m : 4.58241
[1mStep[0m  [45/53], [94mLoss[0m : 4.57262
[1mStep[0m  [50/53], [94mLoss[0m : 4.53952

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.671, [92mTest[0m: 4.707, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.49459
[1mStep[0m  [5/53], [94mLoss[0m : 4.54753
[1mStep[0m  [10/53], [94mLoss[0m : 4.46749
[1mStep[0m  [15/53], [94mLoss[0m : 4.80804
[1mStep[0m  [20/53], [94mLoss[0m : 4.43804
[1mStep[0m  [25/53], [94mLoss[0m : 4.63308
[1mStep[0m  [30/53], [94mLoss[0m : 4.62044
[1mStep[0m  [35/53], [94mLoss[0m : 4.49505
[1mStep[0m  [40/53], [94mLoss[0m : 4.29140
[1mStep[0m  [45/53], [94mLoss[0m : 4.58111
[1mStep[0m  [50/53], [94mLoss[0m : 4.35741

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.554, [92mTest[0m: 4.591, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.41703
[1mStep[0m  [5/53], [94mLoss[0m : 4.66695
[1mStep[0m  [10/53], [94mLoss[0m : 4.50428
[1mStep[0m  [15/53], [94mLoss[0m : 4.30855
[1mStep[0m  [20/53], [94mLoss[0m : 4.62548
[1mStep[0m  [25/53], [94mLoss[0m : 4.34315
[1mStep[0m  [30/53], [94mLoss[0m : 4.72554
[1mStep[0m  [35/53], [94mLoss[0m : 4.28107
[1mStep[0m  [40/53], [94mLoss[0m : 4.29688
[1mStep[0m  [45/53], [94mLoss[0m : 4.54856
[1mStep[0m  [50/53], [94mLoss[0m : 4.63957

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.442, [92mTest[0m: 4.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.34970
[1mStep[0m  [5/53], [94mLoss[0m : 4.52443
[1mStep[0m  [10/53], [94mLoss[0m : 4.56060
[1mStep[0m  [15/53], [94mLoss[0m : 4.31089
[1mStep[0m  [20/53], [94mLoss[0m : 4.27320
[1mStep[0m  [25/53], [94mLoss[0m : 4.27721
[1mStep[0m  [30/53], [94mLoss[0m : 4.18407
[1mStep[0m  [35/53], [94mLoss[0m : 4.31419
[1mStep[0m  [40/53], [94mLoss[0m : 4.38501
[1mStep[0m  [45/53], [94mLoss[0m : 4.09625
[1mStep[0m  [50/53], [94mLoss[0m : 4.26209

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.342, [92mTest[0m: 4.372, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.67368
[1mStep[0m  [5/53], [94mLoss[0m : 4.34008
[1mStep[0m  [10/53], [94mLoss[0m : 4.22714
[1mStep[0m  [15/53], [94mLoss[0m : 4.76324
[1mStep[0m  [20/53], [94mLoss[0m : 4.02023
[1mStep[0m  [25/53], [94mLoss[0m : 4.31062
[1mStep[0m  [30/53], [94mLoss[0m : 4.28390
[1mStep[0m  [35/53], [94mLoss[0m : 4.45666
[1mStep[0m  [40/53], [94mLoss[0m : 4.18654
[1mStep[0m  [45/53], [94mLoss[0m : 4.62556
[1mStep[0m  [50/53], [94mLoss[0m : 4.13673

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.237, [92mTest[0m: 4.253, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.03849
[1mStep[0m  [5/53], [94mLoss[0m : 4.29128
[1mStep[0m  [10/53], [94mLoss[0m : 4.37175
[1mStep[0m  [15/53], [94mLoss[0m : 4.40734
[1mStep[0m  [20/53], [94mLoss[0m : 4.16267
[1mStep[0m  [25/53], [94mLoss[0m : 3.98646
[1mStep[0m  [30/53], [94mLoss[0m : 4.15677
[1mStep[0m  [35/53], [94mLoss[0m : 4.20507
[1mStep[0m  [40/53], [94mLoss[0m : 3.97733
[1mStep[0m  [45/53], [94mLoss[0m : 4.14861
[1mStep[0m  [50/53], [94mLoss[0m : 4.43664

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.147, [92mTest[0m: 4.165, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.46987
[1mStep[0m  [5/53], [94mLoss[0m : 4.29405
[1mStep[0m  [10/53], [94mLoss[0m : 3.89357
[1mStep[0m  [15/53], [94mLoss[0m : 4.03337
[1mStep[0m  [20/53], [94mLoss[0m : 3.93831
[1mStep[0m  [25/53], [94mLoss[0m : 4.07886
[1mStep[0m  [30/53], [94mLoss[0m : 4.09463
[1mStep[0m  [35/53], [94mLoss[0m : 4.19565
[1mStep[0m  [40/53], [94mLoss[0m : 3.97437
[1mStep[0m  [45/53], [94mLoss[0m : 4.13871
[1mStep[0m  [50/53], [94mLoss[0m : 4.11536

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.066, [92mTest[0m: 4.073, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.12167
[1mStep[0m  [5/53], [94mLoss[0m : 4.18004
[1mStep[0m  [10/53], [94mLoss[0m : 3.75661
[1mStep[0m  [15/53], [94mLoss[0m : 3.94978
[1mStep[0m  [20/53], [94mLoss[0m : 4.05319
[1mStep[0m  [25/53], [94mLoss[0m : 3.84797
[1mStep[0m  [30/53], [94mLoss[0m : 4.17360
[1mStep[0m  [35/53], [94mLoss[0m : 4.07848
[1mStep[0m  [40/53], [94mLoss[0m : 3.89621
[1mStep[0m  [45/53], [94mLoss[0m : 4.02150
[1mStep[0m  [50/53], [94mLoss[0m : 4.12547

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.985, [92mTest[0m: 3.992, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 4.39399
[1mStep[0m  [5/53], [94mLoss[0m : 4.32039
[1mStep[0m  [10/53], [94mLoss[0m : 4.06354
[1mStep[0m  [15/53], [94mLoss[0m : 3.98392
[1mStep[0m  [20/53], [94mLoss[0m : 3.84300
[1mStep[0m  [25/53], [94mLoss[0m : 3.97429
[1mStep[0m  [30/53], [94mLoss[0m : 4.08854
[1mStep[0m  [35/53], [94mLoss[0m : 3.30592
[1mStep[0m  [40/53], [94mLoss[0m : 3.41982
[1mStep[0m  [45/53], [94mLoss[0m : 3.71500
[1mStep[0m  [50/53], [94mLoss[0m : 3.86064

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.916, [92mTest[0m: 3.920, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.87929
[1mStep[0m  [5/53], [94mLoss[0m : 4.16234
[1mStep[0m  [10/53], [94mLoss[0m : 3.90980
[1mStep[0m  [15/53], [94mLoss[0m : 3.94057
[1mStep[0m  [20/53], [94mLoss[0m : 4.02715
[1mStep[0m  [25/53], [94mLoss[0m : 3.51936
[1mStep[0m  [30/53], [94mLoss[0m : 4.16130
[1mStep[0m  [35/53], [94mLoss[0m : 3.61295
[1mStep[0m  [40/53], [94mLoss[0m : 3.53606
[1mStep[0m  [45/53], [94mLoss[0m : 3.91719
[1mStep[0m  [50/53], [94mLoss[0m : 3.83914

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.844, [92mTest[0m: 3.831, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.99853
[1mStep[0m  [5/53], [94mLoss[0m : 3.56401
[1mStep[0m  [10/53], [94mLoss[0m : 3.72406
[1mStep[0m  [15/53], [94mLoss[0m : 4.11995
[1mStep[0m  [20/53], [94mLoss[0m : 3.90973
[1mStep[0m  [25/53], [94mLoss[0m : 4.05835
[1mStep[0m  [30/53], [94mLoss[0m : 3.91404
[1mStep[0m  [35/53], [94mLoss[0m : 3.88262
[1mStep[0m  [40/53], [94mLoss[0m : 3.65494
[1mStep[0m  [45/53], [94mLoss[0m : 3.71137
[1mStep[0m  [50/53], [94mLoss[0m : 3.63794

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.775, [92mTest[0m: 3.769, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.96096
[1mStep[0m  [5/53], [94mLoss[0m : 3.82392
[1mStep[0m  [10/53], [94mLoss[0m : 3.79858
[1mStep[0m  [15/53], [94mLoss[0m : 3.55652
[1mStep[0m  [20/53], [94mLoss[0m : 3.56317
[1mStep[0m  [25/53], [94mLoss[0m : 3.88993
[1mStep[0m  [30/53], [94mLoss[0m : 3.89880
[1mStep[0m  [35/53], [94mLoss[0m : 3.65738
[1mStep[0m  [40/53], [94mLoss[0m : 3.88048
[1mStep[0m  [45/53], [94mLoss[0m : 3.46410
[1mStep[0m  [50/53], [94mLoss[0m : 3.48900

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.705, [92mTest[0m: 3.706, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.80749
[1mStep[0m  [5/53], [94mLoss[0m : 3.91733
[1mStep[0m  [10/53], [94mLoss[0m : 3.66441
[1mStep[0m  [15/53], [94mLoss[0m : 3.73364
[1mStep[0m  [20/53], [94mLoss[0m : 3.88550
[1mStep[0m  [25/53], [94mLoss[0m : 3.48135
[1mStep[0m  [30/53], [94mLoss[0m : 3.36752
[1mStep[0m  [35/53], [94mLoss[0m : 3.59096
[1mStep[0m  [40/53], [94mLoss[0m : 3.83532
[1mStep[0m  [45/53], [94mLoss[0m : 3.57176
[1mStep[0m  [50/53], [94mLoss[0m : 3.67772

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.649, [92mTest[0m: 3.633, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.70398
[1mStep[0m  [5/53], [94mLoss[0m : 3.47503
[1mStep[0m  [10/53], [94mLoss[0m : 3.69241
[1mStep[0m  [15/53], [94mLoss[0m : 3.57403
[1mStep[0m  [20/53], [94mLoss[0m : 3.77046
[1mStep[0m  [25/53], [94mLoss[0m : 3.85307
[1mStep[0m  [30/53], [94mLoss[0m : 3.51780
[1mStep[0m  [35/53], [94mLoss[0m : 3.40044
[1mStep[0m  [40/53], [94mLoss[0m : 3.30211
[1mStep[0m  [45/53], [94mLoss[0m : 3.83151
[1mStep[0m  [50/53], [94mLoss[0m : 3.26324

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.596, [92mTest[0m: 3.589, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 3.90653
[1mStep[0m  [5/53], [94mLoss[0m : 3.61335
[1mStep[0m  [10/53], [94mLoss[0m : 3.41566
[1mStep[0m  [15/53], [94mLoss[0m : 3.59437
[1mStep[0m  [20/53], [94mLoss[0m : 3.44486
[1mStep[0m  [25/53], [94mLoss[0m : 3.62327
[1mStep[0m  [30/53], [94mLoss[0m : 3.22869
[1mStep[0m  [35/53], [94mLoss[0m : 3.14494
[1mStep[0m  [40/53], [94mLoss[0m : 3.44788
[1mStep[0m  [45/53], [94mLoss[0m : 3.21376
[1mStep[0m  [50/53], [94mLoss[0m : 3.60381

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.530, [92mTest[0m: 3.526, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.457
====================================

Phase 2 - Evaluation MAE:  3.456665194951571
MAE score P1      6.879643
MAE score P2      3.456665
loss               3.52969
learning_rate       0.0001
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay          0.01
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/53], [94mLoss[0m : 10.54060
[1mStep[0m  [5/53], [94mLoss[0m : 10.91627
[1mStep[0m  [10/53], [94mLoss[0m : 10.85552
[1mStep[0m  [15/53], [94mLoss[0m : 11.27590
[1mStep[0m  [20/53], [94mLoss[0m : 11.16900
[1mStep[0m  [25/53], [94mLoss[0m : 10.47087
[1mStep[0m  [30/53], [94mLoss[0m : 10.60520
[1mStep[0m  [35/53], [94mLoss[0m : 10.79478
[1mStep[0m  [40/53], [94mLoss[0m : 11.02532
[1mStep[0m  [45/53], [94mLoss[0m : 10.76393
[1mStep[0m  [50/53], [94mLoss[0m : 11.04736

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.824, [92mTest[0m: 10.827, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.95028
[1mStep[0m  [5/53], [94mLoss[0m : 10.70310
[1mStep[0m  [10/53], [94mLoss[0m : 11.35039
[1mStep[0m  [15/53], [94mLoss[0m : 10.73389
[1mStep[0m  [20/53], [94mLoss[0m : 10.44914
[1mStep[0m  [25/53], [94mLoss[0m : 10.65448
[1mStep[0m  [30/53], [94mLoss[0m : 10.64403
[1mStep[0m  [35/53], [94mLoss[0m : 10.89373
[1mStep[0m  [40/53], [94mLoss[0m : 10.51888
[1mStep[0m  [45/53], [94mLoss[0m : 10.69256
[1mStep[0m  [50/53], [94mLoss[0m : 10.96104

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.753, [92mTest[0m: 10.791, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.63872
[1mStep[0m  [5/53], [94mLoss[0m : 10.23896
[1mStep[0m  [10/53], [94mLoss[0m : 10.67115
[1mStep[0m  [15/53], [94mLoss[0m : 10.64467
[1mStep[0m  [20/53], [94mLoss[0m : 10.77901
[1mStep[0m  [25/53], [94mLoss[0m : 10.87708
[1mStep[0m  [30/53], [94mLoss[0m : 10.70263
[1mStep[0m  [35/53], [94mLoss[0m : 10.62982
[1mStep[0m  [40/53], [94mLoss[0m : 10.40199
[1mStep[0m  [45/53], [94mLoss[0m : 10.59357
[1mStep[0m  [50/53], [94mLoss[0m : 10.37962

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.691, [92mTest[0m: 10.688, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.43837
[1mStep[0m  [5/53], [94mLoss[0m : 10.61650
[1mStep[0m  [10/53], [94mLoss[0m : 11.11482
[1mStep[0m  [15/53], [94mLoss[0m : 10.81539
[1mStep[0m  [20/53], [94mLoss[0m : 10.45648
[1mStep[0m  [25/53], [94mLoss[0m : 10.63789
[1mStep[0m  [30/53], [94mLoss[0m : 10.70422
[1mStep[0m  [35/53], [94mLoss[0m : 10.59954
[1mStep[0m  [40/53], [94mLoss[0m : 10.47906
[1mStep[0m  [45/53], [94mLoss[0m : 10.55648
[1mStep[0m  [50/53], [94mLoss[0m : 10.71829

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.635, [92mTest[0m: 10.621, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.93838
[1mStep[0m  [5/53], [94mLoss[0m : 10.56052
[1mStep[0m  [10/53], [94mLoss[0m : 10.27498
[1mStep[0m  [15/53], [94mLoss[0m : 10.44415
[1mStep[0m  [20/53], [94mLoss[0m : 10.93035
[1mStep[0m  [25/53], [94mLoss[0m : 10.25725
[1mStep[0m  [30/53], [94mLoss[0m : 10.63544
[1mStep[0m  [35/53], [94mLoss[0m : 10.23569
[1mStep[0m  [40/53], [94mLoss[0m : 10.52038
[1mStep[0m  [45/53], [94mLoss[0m : 10.58350
[1mStep[0m  [50/53], [94mLoss[0m : 10.15261

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.566, [92mTest[0m: 10.539, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.66334
[1mStep[0m  [5/53], [94mLoss[0m : 10.51401
[1mStep[0m  [10/53], [94mLoss[0m : 10.38020
[1mStep[0m  [15/53], [94mLoss[0m : 10.23738
[1mStep[0m  [20/53], [94mLoss[0m : 10.20909
[1mStep[0m  [25/53], [94mLoss[0m : 10.82534
[1mStep[0m  [30/53], [94mLoss[0m : 10.38527
[1mStep[0m  [35/53], [94mLoss[0m : 10.35003
[1mStep[0m  [40/53], [94mLoss[0m : 10.83085
[1mStep[0m  [45/53], [94mLoss[0m : 10.29210
[1mStep[0m  [50/53], [94mLoss[0m : 10.32056

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.501, [92mTest[0m: 10.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.30980
[1mStep[0m  [5/53], [94mLoss[0m : 10.27711
[1mStep[0m  [10/53], [94mLoss[0m : 10.54555
[1mStep[0m  [15/53], [94mLoss[0m : 10.33074
[1mStep[0m  [20/53], [94mLoss[0m : 10.51786
[1mStep[0m  [25/53], [94mLoss[0m : 10.16282
[1mStep[0m  [30/53], [94mLoss[0m : 10.40867
[1mStep[0m  [35/53], [94mLoss[0m : 10.79574
[1mStep[0m  [40/53], [94mLoss[0m : 10.53013
[1mStep[0m  [45/53], [94mLoss[0m : 10.65919
[1mStep[0m  [50/53], [94mLoss[0m : 10.16399

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.431, [92mTest[0m: 10.361, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.34660
[1mStep[0m  [5/53], [94mLoss[0m : 10.33508
[1mStep[0m  [10/53], [94mLoss[0m : 10.47312
[1mStep[0m  [15/53], [94mLoss[0m : 10.62680
[1mStep[0m  [20/53], [94mLoss[0m : 10.26063
[1mStep[0m  [25/53], [94mLoss[0m : 10.50457
[1mStep[0m  [30/53], [94mLoss[0m : 10.63092
[1mStep[0m  [35/53], [94mLoss[0m : 10.12216
[1mStep[0m  [40/53], [94mLoss[0m : 10.30009
[1mStep[0m  [45/53], [94mLoss[0m : 10.24516
[1mStep[0m  [50/53], [94mLoss[0m : 10.42148

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.371, [92mTest[0m: 10.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.33965
[1mStep[0m  [5/53], [94mLoss[0m : 10.52537
[1mStep[0m  [10/53], [94mLoss[0m : 10.39412
[1mStep[0m  [15/53], [94mLoss[0m : 10.42891
[1mStep[0m  [20/53], [94mLoss[0m : 10.59942
[1mStep[0m  [25/53], [94mLoss[0m : 10.30905
[1mStep[0m  [30/53], [94mLoss[0m : 10.60126
[1mStep[0m  [35/53], [94mLoss[0m : 10.15837
[1mStep[0m  [40/53], [94mLoss[0m : 10.72307
[1mStep[0m  [45/53], [94mLoss[0m : 10.34764
[1mStep[0m  [50/53], [94mLoss[0m : 10.21658

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.305, [92mTest[0m: 10.217, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.45469
[1mStep[0m  [5/53], [94mLoss[0m : 10.35380
[1mStep[0m  [10/53], [94mLoss[0m : 10.47541
[1mStep[0m  [15/53], [94mLoss[0m : 9.90626
[1mStep[0m  [20/53], [94mLoss[0m : 10.15880
[1mStep[0m  [25/53], [94mLoss[0m : 10.09469
[1mStep[0m  [30/53], [94mLoss[0m : 9.73640
[1mStep[0m  [35/53], [94mLoss[0m : 10.31853
[1mStep[0m  [40/53], [94mLoss[0m : 10.51308
[1mStep[0m  [45/53], [94mLoss[0m : 9.84172
[1mStep[0m  [50/53], [94mLoss[0m : 10.86244

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.237, [92mTest[0m: 10.143, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.13857
[1mStep[0m  [5/53], [94mLoss[0m : 10.02756
[1mStep[0m  [10/53], [94mLoss[0m : 10.14263
[1mStep[0m  [15/53], [94mLoss[0m : 9.85194
[1mStep[0m  [20/53], [94mLoss[0m : 9.99571
[1mStep[0m  [25/53], [94mLoss[0m : 10.29792
[1mStep[0m  [30/53], [94mLoss[0m : 9.84858
[1mStep[0m  [35/53], [94mLoss[0m : 10.05431
[1mStep[0m  [40/53], [94mLoss[0m : 10.12738
[1mStep[0m  [45/53], [94mLoss[0m : 9.87091
[1mStep[0m  [50/53], [94mLoss[0m : 10.12307

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.174, [92mTest[0m: 10.043, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.80424
[1mStep[0m  [5/53], [94mLoss[0m : 10.54921
[1mStep[0m  [10/53], [94mLoss[0m : 10.06563
[1mStep[0m  [15/53], [94mLoss[0m : 9.82248
[1mStep[0m  [20/53], [94mLoss[0m : 10.04250
[1mStep[0m  [25/53], [94mLoss[0m : 9.90822
[1mStep[0m  [30/53], [94mLoss[0m : 9.94742
[1mStep[0m  [35/53], [94mLoss[0m : 10.16768
[1mStep[0m  [40/53], [94mLoss[0m : 10.05133
[1mStep[0m  [45/53], [94mLoss[0m : 10.27199
[1mStep[0m  [50/53], [94mLoss[0m : 10.03347

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.102, [92mTest[0m: 9.983, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.56864
[1mStep[0m  [5/53], [94mLoss[0m : 9.66606
[1mStep[0m  [10/53], [94mLoss[0m : 10.21368
[1mStep[0m  [15/53], [94mLoss[0m : 9.81300
[1mStep[0m  [20/53], [94mLoss[0m : 10.07710
[1mStep[0m  [25/53], [94mLoss[0m : 10.12101
[1mStep[0m  [30/53], [94mLoss[0m : 9.56071
[1mStep[0m  [35/53], [94mLoss[0m : 9.82189
[1mStep[0m  [40/53], [94mLoss[0m : 9.45670
[1mStep[0m  [45/53], [94mLoss[0m : 10.17604
[1mStep[0m  [50/53], [94mLoss[0m : 10.24928

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.035, [92mTest[0m: 9.904, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.71663
[1mStep[0m  [5/53], [94mLoss[0m : 9.71481
[1mStep[0m  [10/53], [94mLoss[0m : 9.99217
[1mStep[0m  [15/53], [94mLoss[0m : 10.06069
[1mStep[0m  [20/53], [94mLoss[0m : 10.25419
[1mStep[0m  [25/53], [94mLoss[0m : 10.46665
[1mStep[0m  [30/53], [94mLoss[0m : 9.70319
[1mStep[0m  [35/53], [94mLoss[0m : 9.74557
[1mStep[0m  [40/53], [94mLoss[0m : 10.05963
[1mStep[0m  [45/53], [94mLoss[0m : 9.74918
[1mStep[0m  [50/53], [94mLoss[0m : 9.80733

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.965, [92mTest[0m: 9.813, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.53028
[1mStep[0m  [5/53], [94mLoss[0m : 10.00374
[1mStep[0m  [10/53], [94mLoss[0m : 9.75108
[1mStep[0m  [15/53], [94mLoss[0m : 10.03183
[1mStep[0m  [20/53], [94mLoss[0m : 9.69238
[1mStep[0m  [25/53], [94mLoss[0m : 10.11162
[1mStep[0m  [30/53], [94mLoss[0m : 10.03635
[1mStep[0m  [35/53], [94mLoss[0m : 10.17213
[1mStep[0m  [40/53], [94mLoss[0m : 9.87946
[1mStep[0m  [45/53], [94mLoss[0m : 9.82911
[1mStep[0m  [50/53], [94mLoss[0m : 9.73702

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.898, [92mTest[0m: 9.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.80402
[1mStep[0m  [5/53], [94mLoss[0m : 9.98055
[1mStep[0m  [10/53], [94mLoss[0m : 10.14491
[1mStep[0m  [15/53], [94mLoss[0m : 10.01712
[1mStep[0m  [20/53], [94mLoss[0m : 9.67070
[1mStep[0m  [25/53], [94mLoss[0m : 9.46947
[1mStep[0m  [30/53], [94mLoss[0m : 10.08785
[1mStep[0m  [35/53], [94mLoss[0m : 10.15131
[1mStep[0m  [40/53], [94mLoss[0m : 9.78549
[1mStep[0m  [45/53], [94mLoss[0m : 9.76543
[1mStep[0m  [50/53], [94mLoss[0m : 10.05442

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.838, [92mTest[0m: 9.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.29370
[1mStep[0m  [5/53], [94mLoss[0m : 9.98629
[1mStep[0m  [10/53], [94mLoss[0m : 9.58812
[1mStep[0m  [15/53], [94mLoss[0m : 9.60587
[1mStep[0m  [20/53], [94mLoss[0m : 10.11922
[1mStep[0m  [25/53], [94mLoss[0m : 9.79279
[1mStep[0m  [30/53], [94mLoss[0m : 9.63444
[1mStep[0m  [35/53], [94mLoss[0m : 9.37644
[1mStep[0m  [40/53], [94mLoss[0m : 9.72598
[1mStep[0m  [45/53], [94mLoss[0m : 9.84826
[1mStep[0m  [50/53], [94mLoss[0m : 9.78634

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.760, [92mTest[0m: 9.553, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.46455
[1mStep[0m  [5/53], [94mLoss[0m : 9.77471
[1mStep[0m  [10/53], [94mLoss[0m : 9.96935
[1mStep[0m  [15/53], [94mLoss[0m : 9.45276
[1mStep[0m  [20/53], [94mLoss[0m : 9.68156
[1mStep[0m  [25/53], [94mLoss[0m : 9.73595
[1mStep[0m  [30/53], [94mLoss[0m : 10.19227
[1mStep[0m  [35/53], [94mLoss[0m : 9.69520
[1mStep[0m  [40/53], [94mLoss[0m : 9.54448
[1mStep[0m  [45/53], [94mLoss[0m : 9.50697
[1mStep[0m  [50/53], [94mLoss[0m : 9.44043

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.681, [92mTest[0m: 9.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 10.32394
[1mStep[0m  [5/53], [94mLoss[0m : 9.53293
[1mStep[0m  [10/53], [94mLoss[0m : 9.75433
[1mStep[0m  [15/53], [94mLoss[0m : 9.41082
[1mStep[0m  [20/53], [94mLoss[0m : 9.59298
[1mStep[0m  [25/53], [94mLoss[0m : 9.30620
[1mStep[0m  [30/53], [94mLoss[0m : 9.35465
[1mStep[0m  [35/53], [94mLoss[0m : 9.62753
[1mStep[0m  [40/53], [94mLoss[0m : 9.49906
[1mStep[0m  [45/53], [94mLoss[0m : 9.83763
[1mStep[0m  [50/53], [94mLoss[0m : 9.44267

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.609, [92mTest[0m: 9.389, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.43007
[1mStep[0m  [5/53], [94mLoss[0m : 9.81458
[1mStep[0m  [10/53], [94mLoss[0m : 9.76413
[1mStep[0m  [15/53], [94mLoss[0m : 9.45262
[1mStep[0m  [20/53], [94mLoss[0m : 9.62470
[1mStep[0m  [25/53], [94mLoss[0m : 9.23696
[1mStep[0m  [30/53], [94mLoss[0m : 9.36932
[1mStep[0m  [35/53], [94mLoss[0m : 9.96052
[1mStep[0m  [40/53], [94mLoss[0m : 9.69419
[1mStep[0m  [45/53], [94mLoss[0m : 9.57155
[1mStep[0m  [50/53], [94mLoss[0m : 9.29421

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.534, [92mTest[0m: 9.281, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.48148
[1mStep[0m  [5/53], [94mLoss[0m : 9.05706
[1mStep[0m  [10/53], [94mLoss[0m : 9.28075
[1mStep[0m  [15/53], [94mLoss[0m : 9.61847
[1mStep[0m  [20/53], [94mLoss[0m : 9.55132
[1mStep[0m  [25/53], [94mLoss[0m : 9.60183
[1mStep[0m  [30/53], [94mLoss[0m : 9.28038
[1mStep[0m  [35/53], [94mLoss[0m : 9.43047
[1mStep[0m  [40/53], [94mLoss[0m : 9.60393
[1mStep[0m  [45/53], [94mLoss[0m : 9.55631
[1mStep[0m  [50/53], [94mLoss[0m : 9.22249

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.456, [92mTest[0m: 9.213, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.46056
[1mStep[0m  [5/53], [94mLoss[0m : 9.24916
[1mStep[0m  [10/53], [94mLoss[0m : 9.46029
[1mStep[0m  [15/53], [94mLoss[0m : 9.27532
[1mStep[0m  [20/53], [94mLoss[0m : 9.16782
[1mStep[0m  [25/53], [94mLoss[0m : 9.16691
[1mStep[0m  [30/53], [94mLoss[0m : 9.56066
[1mStep[0m  [35/53], [94mLoss[0m : 9.50352
[1mStep[0m  [40/53], [94mLoss[0m : 9.30576
[1mStep[0m  [45/53], [94mLoss[0m : 9.20795
[1mStep[0m  [50/53], [94mLoss[0m : 9.30521

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.386, [92mTest[0m: 9.115, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.25658
[1mStep[0m  [5/53], [94mLoss[0m : 9.34685
[1mStep[0m  [10/53], [94mLoss[0m : 9.35932
[1mStep[0m  [15/53], [94mLoss[0m : 9.76319
[1mStep[0m  [20/53], [94mLoss[0m : 9.27419
[1mStep[0m  [25/53], [94mLoss[0m : 8.92653
[1mStep[0m  [30/53], [94mLoss[0m : 9.19076
[1mStep[0m  [35/53], [94mLoss[0m : 9.29447
[1mStep[0m  [40/53], [94mLoss[0m : 8.80840
[1mStep[0m  [45/53], [94mLoss[0m : 9.40505
[1mStep[0m  [50/53], [94mLoss[0m : 9.49530

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.313, [92mTest[0m: 9.015, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.50920
[1mStep[0m  [5/53], [94mLoss[0m : 8.91680
[1mStep[0m  [10/53], [94mLoss[0m : 9.04001
[1mStep[0m  [15/53], [94mLoss[0m : 9.72086
[1mStep[0m  [20/53], [94mLoss[0m : 9.12744
[1mStep[0m  [25/53], [94mLoss[0m : 9.55853
[1mStep[0m  [30/53], [94mLoss[0m : 8.96915
[1mStep[0m  [35/53], [94mLoss[0m : 9.34961
[1mStep[0m  [40/53], [94mLoss[0m : 8.97615
[1mStep[0m  [45/53], [94mLoss[0m : 8.94931
[1mStep[0m  [50/53], [94mLoss[0m : 9.50004

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.239, [92mTest[0m: 8.951, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.79922
[1mStep[0m  [5/53], [94mLoss[0m : 9.45575
[1mStep[0m  [10/53], [94mLoss[0m : 9.59831
[1mStep[0m  [15/53], [94mLoss[0m : 9.03715
[1mStep[0m  [20/53], [94mLoss[0m : 9.08261
[1mStep[0m  [25/53], [94mLoss[0m : 9.49006
[1mStep[0m  [30/53], [94mLoss[0m : 9.38158
[1mStep[0m  [35/53], [94mLoss[0m : 9.17878
[1mStep[0m  [40/53], [94mLoss[0m : 8.71495
[1mStep[0m  [45/53], [94mLoss[0m : 9.25131
[1mStep[0m  [50/53], [94mLoss[0m : 9.33766

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.158, [92mTest[0m: 8.850, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.87802
[1mStep[0m  [5/53], [94mLoss[0m : 9.33891
[1mStep[0m  [10/53], [94mLoss[0m : 9.12353
[1mStep[0m  [15/53], [94mLoss[0m : 9.00331
[1mStep[0m  [20/53], [94mLoss[0m : 9.16366
[1mStep[0m  [25/53], [94mLoss[0m : 9.25086
[1mStep[0m  [30/53], [94mLoss[0m : 8.91805
[1mStep[0m  [35/53], [94mLoss[0m : 9.43061
[1mStep[0m  [40/53], [94mLoss[0m : 9.04556
[1mStep[0m  [45/53], [94mLoss[0m : 9.24607
[1mStep[0m  [50/53], [94mLoss[0m : 9.30772

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.083, [92mTest[0m: 8.764, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.72670
[1mStep[0m  [5/53], [94mLoss[0m : 8.93996
[1mStep[0m  [10/53], [94mLoss[0m : 8.83246
[1mStep[0m  [15/53], [94mLoss[0m : 9.16105
[1mStep[0m  [20/53], [94mLoss[0m : 9.02267
[1mStep[0m  [25/53], [94mLoss[0m : 9.27283
[1mStep[0m  [30/53], [94mLoss[0m : 8.74930
[1mStep[0m  [35/53], [94mLoss[0m : 9.05718
[1mStep[0m  [40/53], [94mLoss[0m : 9.09105
[1mStep[0m  [45/53], [94mLoss[0m : 8.81705
[1mStep[0m  [50/53], [94mLoss[0m : 9.12687

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.005, [92mTest[0m: 8.673, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.47960
[1mStep[0m  [5/53], [94mLoss[0m : 8.98600
[1mStep[0m  [10/53], [94mLoss[0m : 8.83135
[1mStep[0m  [15/53], [94mLoss[0m : 8.85398
[1mStep[0m  [20/53], [94mLoss[0m : 9.07003
[1mStep[0m  [25/53], [94mLoss[0m : 8.90711
[1mStep[0m  [30/53], [94mLoss[0m : 8.83232
[1mStep[0m  [35/53], [94mLoss[0m : 9.07935
[1mStep[0m  [40/53], [94mLoss[0m : 8.89239
[1mStep[0m  [45/53], [94mLoss[0m : 8.70001
[1mStep[0m  [50/53], [94mLoss[0m : 8.75099

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.923, [92mTest[0m: 8.547, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 9.18200
[1mStep[0m  [5/53], [94mLoss[0m : 8.81543
[1mStep[0m  [10/53], [94mLoss[0m : 9.06230
[1mStep[0m  [15/53], [94mLoss[0m : 8.72458
[1mStep[0m  [20/53], [94mLoss[0m : 9.19311
[1mStep[0m  [25/53], [94mLoss[0m : 8.82144
[1mStep[0m  [30/53], [94mLoss[0m : 8.75756
[1mStep[0m  [35/53], [94mLoss[0m : 8.43883
[1mStep[0m  [40/53], [94mLoss[0m : 8.82798
[1mStep[0m  [45/53], [94mLoss[0m : 8.91107
[1mStep[0m  [50/53], [94mLoss[0m : 8.99875

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.835, [92mTest[0m: 8.499, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/53], [94mLoss[0m : 8.55067
[1mStep[0m  [5/53], [94mLoss[0m : 8.61249
[1mStep[0m  [10/53], [94mLoss[0m : 8.60764
[1mStep[0m  [15/53], [94mLoss[0m : 8.90534
[1mStep[0m  [20/53], [94mLoss[0m : 8.99425
[1mStep[0m  [25/53], [94mLoss[0m : 8.39169
[1mStep[0m  [30/53], [94mLoss[0m : 8.96972
[1mStep[0m  [35/53], [94mLoss[0m : 8.87508
[1mStep[0m  [40/53], [94mLoss[0m : 8.28961
[1mStep[0m  [45/53], [94mLoss[0m : 8.36087
[1mStep[0m  [50/53], [94mLoss[0m : 8.68979

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.751, [92mTest[0m: 8.416, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.235
====================================

Phase 1 - Evaluation MAE:  8.235376578110914
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
