no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  2
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.14886
[1mStep[0m  [4/42], [94mLoss[0m : 10.77842
[1mStep[0m  [8/42], [94mLoss[0m : 10.80040
[1mStep[0m  [12/42], [94mLoss[0m : 10.85955
[1mStep[0m  [16/42], [94mLoss[0m : 10.85389
[1mStep[0m  [20/42], [94mLoss[0m : 11.23703
[1mStep[0m  [24/42], [94mLoss[0m : 11.10439
[1mStep[0m  [28/42], [94mLoss[0m : 10.98098
[1mStep[0m  [32/42], [94mLoss[0m : 11.23938
[1mStep[0m  [36/42], [94mLoss[0m : 10.97577
[1mStep[0m  [40/42], [94mLoss[0m : 10.54851

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.959, [92mTest[0m: 11.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95928
[1mStep[0m  [4/42], [94mLoss[0m : 11.02774
[1mStep[0m  [8/42], [94mLoss[0m : 10.88586
[1mStep[0m  [12/42], [94mLoss[0m : 10.99393
[1mStep[0m  [16/42], [94mLoss[0m : 10.51716
[1mStep[0m  [20/42], [94mLoss[0m : 10.72927
[1mStep[0m  [24/42], [94mLoss[0m : 10.74377
[1mStep[0m  [28/42], [94mLoss[0m : 10.53458
[1mStep[0m  [32/42], [94mLoss[0m : 11.45419
[1mStep[0m  [36/42], [94mLoss[0m : 10.96968
[1mStep[0m  [40/42], [94mLoss[0m : 10.73656

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.909, [92mTest[0m: 10.958, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.77987
[1mStep[0m  [4/42], [94mLoss[0m : 10.98351
[1mStep[0m  [8/42], [94mLoss[0m : 10.82436
[1mStep[0m  [12/42], [94mLoss[0m : 11.04049
[1mStep[0m  [16/42], [94mLoss[0m : 10.91989
[1mStep[0m  [20/42], [94mLoss[0m : 10.31039
[1mStep[0m  [24/42], [94mLoss[0m : 11.20276
[1mStep[0m  [28/42], [94mLoss[0m : 10.89279
[1mStep[0m  [32/42], [94mLoss[0m : 10.75026
[1mStep[0m  [36/42], [94mLoss[0m : 10.93252
[1mStep[0m  [40/42], [94mLoss[0m : 10.83945

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.865, [92mTest[0m: 10.910, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68850
[1mStep[0m  [4/42], [94mLoss[0m : 10.85796
[1mStep[0m  [8/42], [94mLoss[0m : 10.91482
[1mStep[0m  [12/42], [94mLoss[0m : 10.72335
[1mStep[0m  [16/42], [94mLoss[0m : 10.45465
[1mStep[0m  [20/42], [94mLoss[0m : 10.99144
[1mStep[0m  [24/42], [94mLoss[0m : 11.02206
[1mStep[0m  [28/42], [94mLoss[0m : 10.98921
[1mStep[0m  [32/42], [94mLoss[0m : 10.68344
[1mStep[0m  [36/42], [94mLoss[0m : 10.50375
[1mStep[0m  [40/42], [94mLoss[0m : 10.63157

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72724
[1mStep[0m  [4/42], [94mLoss[0m : 10.50538
[1mStep[0m  [8/42], [94mLoss[0m : 10.90174
[1mStep[0m  [12/42], [94mLoss[0m : 10.62675
[1mStep[0m  [16/42], [94mLoss[0m : 11.10463
[1mStep[0m  [20/42], [94mLoss[0m : 10.72574
[1mStep[0m  [24/42], [94mLoss[0m : 10.54896
[1mStep[0m  [28/42], [94mLoss[0m : 10.65029
[1mStep[0m  [32/42], [94mLoss[0m : 10.89061
[1mStep[0m  [36/42], [94mLoss[0m : 10.94533
[1mStep[0m  [40/42], [94mLoss[0m : 10.93065

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.762, [92mTest[0m: 10.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93127
[1mStep[0m  [4/42], [94mLoss[0m : 10.76274
[1mStep[0m  [8/42], [94mLoss[0m : 10.63788
[1mStep[0m  [12/42], [94mLoss[0m : 10.86382
[1mStep[0m  [16/42], [94mLoss[0m : 10.40416
[1mStep[0m  [20/42], [94mLoss[0m : 10.90406
[1mStep[0m  [24/42], [94mLoss[0m : 10.69790
[1mStep[0m  [28/42], [94mLoss[0m : 10.74744
[1mStep[0m  [32/42], [94mLoss[0m : 10.83853
[1mStep[0m  [36/42], [94mLoss[0m : 10.86574
[1mStep[0m  [40/42], [94mLoss[0m : 10.40856

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.705, [92mTest[0m: 10.728, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.88126
[1mStep[0m  [4/42], [94mLoss[0m : 10.68148
[1mStep[0m  [8/42], [94mLoss[0m : 10.82281
[1mStep[0m  [12/42], [94mLoss[0m : 10.66851
[1mStep[0m  [16/42], [94mLoss[0m : 10.97451
[1mStep[0m  [20/42], [94mLoss[0m : 10.43741
[1mStep[0m  [24/42], [94mLoss[0m : 10.47323
[1mStep[0m  [28/42], [94mLoss[0m : 10.65827
[1mStep[0m  [32/42], [94mLoss[0m : 10.74710
[1mStep[0m  [36/42], [94mLoss[0m : 10.73613
[1mStep[0m  [40/42], [94mLoss[0m : 10.75013

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83923
[1mStep[0m  [4/42], [94mLoss[0m : 10.40605
[1mStep[0m  [8/42], [94mLoss[0m : 10.43605
[1mStep[0m  [12/42], [94mLoss[0m : 10.30395
[1mStep[0m  [16/42], [94mLoss[0m : 10.33706
[1mStep[0m  [20/42], [94mLoss[0m : 10.45051
[1mStep[0m  [24/42], [94mLoss[0m : 10.79154
[1mStep[0m  [28/42], [94mLoss[0m : 10.52950
[1mStep[0m  [32/42], [94mLoss[0m : 10.86360
[1mStep[0m  [36/42], [94mLoss[0m : 10.76009
[1mStep[0m  [40/42], [94mLoss[0m : 10.41399

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.606, [92mTest[0m: 10.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66784
[1mStep[0m  [4/42], [94mLoss[0m : 10.47186
[1mStep[0m  [8/42], [94mLoss[0m : 10.05151
[1mStep[0m  [12/42], [94mLoss[0m : 10.35043
[1mStep[0m  [16/42], [94mLoss[0m : 10.22364
[1mStep[0m  [20/42], [94mLoss[0m : 10.43506
[1mStep[0m  [24/42], [94mLoss[0m : 10.44515
[1mStep[0m  [28/42], [94mLoss[0m : 10.93332
[1mStep[0m  [32/42], [94mLoss[0m : 10.48164
[1mStep[0m  [36/42], [94mLoss[0m : 10.76203
[1mStep[0m  [40/42], [94mLoss[0m : 10.68228

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.554, [92mTest[0m: 10.538, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64385
[1mStep[0m  [4/42], [94mLoss[0m : 10.49400
[1mStep[0m  [8/42], [94mLoss[0m : 10.31431
[1mStep[0m  [12/42], [94mLoss[0m : 10.98713
[1mStep[0m  [16/42], [94mLoss[0m : 10.44500
[1mStep[0m  [20/42], [94mLoss[0m : 10.34261
[1mStep[0m  [24/42], [94mLoss[0m : 10.60029
[1mStep[0m  [28/42], [94mLoss[0m : 10.64419
[1mStep[0m  [32/42], [94mLoss[0m : 10.59218
[1mStep[0m  [36/42], [94mLoss[0m : 10.18012
[1mStep[0m  [40/42], [94mLoss[0m : 10.31413

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.513, [92mTest[0m: 10.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54778
[1mStep[0m  [4/42], [94mLoss[0m : 10.45470
[1mStep[0m  [8/42], [94mLoss[0m : 10.39700
[1mStep[0m  [12/42], [94mLoss[0m : 10.51066
[1mStep[0m  [16/42], [94mLoss[0m : 10.15458
[1mStep[0m  [20/42], [94mLoss[0m : 10.52315
[1mStep[0m  [24/42], [94mLoss[0m : 10.53786
[1mStep[0m  [28/42], [94mLoss[0m : 10.36984
[1mStep[0m  [32/42], [94mLoss[0m : 10.12010
[1mStep[0m  [36/42], [94mLoss[0m : 10.14920
[1mStep[0m  [40/42], [94mLoss[0m : 10.21013

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.451, [92mTest[0m: 10.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.31694
[1mStep[0m  [4/42], [94mLoss[0m : 10.22753
[1mStep[0m  [8/42], [94mLoss[0m : 10.44250
[1mStep[0m  [12/42], [94mLoss[0m : 10.72453
[1mStep[0m  [16/42], [94mLoss[0m : 9.94226
[1mStep[0m  [20/42], [94mLoss[0m : 10.66010
[1mStep[0m  [24/42], [94mLoss[0m : 10.16251
[1mStep[0m  [28/42], [94mLoss[0m : 10.91122
[1mStep[0m  [32/42], [94mLoss[0m : 10.53340
[1mStep[0m  [36/42], [94mLoss[0m : 10.16879
[1mStep[0m  [40/42], [94mLoss[0m : 10.36436

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.404, [92mTest[0m: 10.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46812
[1mStep[0m  [4/42], [94mLoss[0m : 10.46646
[1mStep[0m  [8/42], [94mLoss[0m : 10.26988
[1mStep[0m  [12/42], [94mLoss[0m : 10.26428
[1mStep[0m  [16/42], [94mLoss[0m : 10.51453
[1mStep[0m  [20/42], [94mLoss[0m : 10.54365
[1mStep[0m  [24/42], [94mLoss[0m : 10.22685
[1mStep[0m  [28/42], [94mLoss[0m : 10.23890
[1mStep[0m  [32/42], [94mLoss[0m : 10.77267
[1mStep[0m  [36/42], [94mLoss[0m : 10.29917
[1mStep[0m  [40/42], [94mLoss[0m : 10.11182

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.357, [92mTest[0m: 10.298, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65484
[1mStep[0m  [4/42], [94mLoss[0m : 10.32351
[1mStep[0m  [8/42], [94mLoss[0m : 10.22656
[1mStep[0m  [12/42], [94mLoss[0m : 10.38923
[1mStep[0m  [16/42], [94mLoss[0m : 10.43720
[1mStep[0m  [20/42], [94mLoss[0m : 10.27222
[1mStep[0m  [24/42], [94mLoss[0m : 10.50814
[1mStep[0m  [28/42], [94mLoss[0m : 10.47310
[1mStep[0m  [32/42], [94mLoss[0m : 10.10511
[1mStep[0m  [36/42], [94mLoss[0m : 10.05655
[1mStep[0m  [40/42], [94mLoss[0m : 10.05807

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.299, [92mTest[0m: 10.215, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.12794
[1mStep[0m  [4/42], [94mLoss[0m : 10.27704
[1mStep[0m  [8/42], [94mLoss[0m : 10.28656
[1mStep[0m  [12/42], [94mLoss[0m : 10.13140
[1mStep[0m  [16/42], [94mLoss[0m : 10.31776
[1mStep[0m  [20/42], [94mLoss[0m : 10.19854
[1mStep[0m  [24/42], [94mLoss[0m : 10.19061
[1mStep[0m  [28/42], [94mLoss[0m : 10.16799
[1mStep[0m  [32/42], [94mLoss[0m : 9.97458
[1mStep[0m  [36/42], [94mLoss[0m : 10.23825
[1mStep[0m  [40/42], [94mLoss[0m : 10.59367

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.242, [92mTest[0m: 10.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.29083
[1mStep[0m  [4/42], [94mLoss[0m : 10.16343
[1mStep[0m  [8/42], [94mLoss[0m : 10.13510
[1mStep[0m  [12/42], [94mLoss[0m : 10.23861
[1mStep[0m  [16/42], [94mLoss[0m : 10.37805
[1mStep[0m  [20/42], [94mLoss[0m : 10.27251
[1mStep[0m  [24/42], [94mLoss[0m : 10.39419
[1mStep[0m  [28/42], [94mLoss[0m : 10.27154
[1mStep[0m  [32/42], [94mLoss[0m : 10.29347
[1mStep[0m  [36/42], [94mLoss[0m : 10.27083
[1mStep[0m  [40/42], [94mLoss[0m : 9.96311

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.198, [92mTest[0m: 10.114, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.97891
[1mStep[0m  [4/42], [94mLoss[0m : 10.82668
[1mStep[0m  [8/42], [94mLoss[0m : 10.04988
[1mStep[0m  [12/42], [94mLoss[0m : 10.22516
[1mStep[0m  [16/42], [94mLoss[0m : 10.38004
[1mStep[0m  [20/42], [94mLoss[0m : 10.04117
[1mStep[0m  [24/42], [94mLoss[0m : 10.49323
[1mStep[0m  [28/42], [94mLoss[0m : 9.86268
[1mStep[0m  [32/42], [94mLoss[0m : 10.30226
[1mStep[0m  [36/42], [94mLoss[0m : 9.46080
[1mStep[0m  [40/42], [94mLoss[0m : 10.17970

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.142, [92mTest[0m: 10.023, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26304
[1mStep[0m  [4/42], [94mLoss[0m : 10.20771
[1mStep[0m  [8/42], [94mLoss[0m : 10.18167
[1mStep[0m  [12/42], [94mLoss[0m : 9.91331
[1mStep[0m  [16/42], [94mLoss[0m : 10.11563
[1mStep[0m  [20/42], [94mLoss[0m : 10.23470
[1mStep[0m  [24/42], [94mLoss[0m : 10.04135
[1mStep[0m  [28/42], [94mLoss[0m : 10.05363
[1mStep[0m  [32/42], [94mLoss[0m : 10.49143
[1mStep[0m  [36/42], [94mLoss[0m : 10.61354
[1mStep[0m  [40/42], [94mLoss[0m : 9.60671

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.089, [92mTest[0m: 9.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44465
[1mStep[0m  [4/42], [94mLoss[0m : 9.81152
[1mStep[0m  [8/42], [94mLoss[0m : 10.02423
[1mStep[0m  [12/42], [94mLoss[0m : 9.85007
[1mStep[0m  [16/42], [94mLoss[0m : 9.81760
[1mStep[0m  [20/42], [94mLoss[0m : 9.89320
[1mStep[0m  [24/42], [94mLoss[0m : 9.97069
[1mStep[0m  [28/42], [94mLoss[0m : 9.61405
[1mStep[0m  [32/42], [94mLoss[0m : 10.38861
[1mStep[0m  [36/42], [94mLoss[0m : 10.01866
[1mStep[0m  [40/42], [94mLoss[0m : 10.22405

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.033, [92mTest[0m: 9.920, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.92690
[1mStep[0m  [4/42], [94mLoss[0m : 10.17947
[1mStep[0m  [8/42], [94mLoss[0m : 9.94674
[1mStep[0m  [12/42], [94mLoss[0m : 9.91976
[1mStep[0m  [16/42], [94mLoss[0m : 10.08929
[1mStep[0m  [20/42], [94mLoss[0m : 10.02877
[1mStep[0m  [24/42], [94mLoss[0m : 9.71028
[1mStep[0m  [28/42], [94mLoss[0m : 10.11727
[1mStep[0m  [32/42], [94mLoss[0m : 10.45922
[1mStep[0m  [36/42], [94mLoss[0m : 9.97125
[1mStep[0m  [40/42], [94mLoss[0m : 9.75274

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.980, [92mTest[0m: 9.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.18071
[1mStep[0m  [4/42], [94mLoss[0m : 10.03657
[1mStep[0m  [8/42], [94mLoss[0m : 9.72530
[1mStep[0m  [12/42], [94mLoss[0m : 10.01635
[1mStep[0m  [16/42], [94mLoss[0m : 9.65096
[1mStep[0m  [20/42], [94mLoss[0m : 9.87024
[1mStep[0m  [24/42], [94mLoss[0m : 9.94394
[1mStep[0m  [28/42], [94mLoss[0m : 10.07765
[1mStep[0m  [32/42], [94mLoss[0m : 10.40762
[1mStep[0m  [36/42], [94mLoss[0m : 9.53219
[1mStep[0m  [40/42], [94mLoss[0m : 9.62024

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.934, [92mTest[0m: 9.770, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.13135
[1mStep[0m  [4/42], [94mLoss[0m : 9.94877
[1mStep[0m  [8/42], [94mLoss[0m : 9.60817
[1mStep[0m  [12/42], [94mLoss[0m : 9.56949
[1mStep[0m  [16/42], [94mLoss[0m : 10.17368
[1mStep[0m  [20/42], [94mLoss[0m : 9.92009
[1mStep[0m  [24/42], [94mLoss[0m : 9.74101
[1mStep[0m  [28/42], [94mLoss[0m : 9.61256
[1mStep[0m  [32/42], [94mLoss[0m : 10.21307
[1mStep[0m  [36/42], [94mLoss[0m : 10.13322
[1mStep[0m  [40/42], [94mLoss[0m : 9.87961

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.870, [92mTest[0m: 9.692, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.96108
[1mStep[0m  [4/42], [94mLoss[0m : 9.98815
[1mStep[0m  [8/42], [94mLoss[0m : 9.99281
[1mStep[0m  [12/42], [94mLoss[0m : 9.52489
[1mStep[0m  [16/42], [94mLoss[0m : 9.32711
[1mStep[0m  [20/42], [94mLoss[0m : 9.89324
[1mStep[0m  [24/42], [94mLoss[0m : 9.75704
[1mStep[0m  [28/42], [94mLoss[0m : 9.67664
[1mStep[0m  [32/42], [94mLoss[0m : 10.00811
[1mStep[0m  [36/42], [94mLoss[0m : 10.06447
[1mStep[0m  [40/42], [94mLoss[0m : 9.46191

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.822, [92mTest[0m: 9.640, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.38715
[1mStep[0m  [4/42], [94mLoss[0m : 10.02058
[1mStep[0m  [8/42], [94mLoss[0m : 9.82681
[1mStep[0m  [12/42], [94mLoss[0m : 9.88414
[1mStep[0m  [16/42], [94mLoss[0m : 10.08613
[1mStep[0m  [20/42], [94mLoss[0m : 9.88467
[1mStep[0m  [24/42], [94mLoss[0m : 9.77698
[1mStep[0m  [28/42], [94mLoss[0m : 9.64132
[1mStep[0m  [32/42], [94mLoss[0m : 9.80521
[1mStep[0m  [36/42], [94mLoss[0m : 9.87467
[1mStep[0m  [40/42], [94mLoss[0m : 9.71570

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.774, [92mTest[0m: 9.591, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.21773
[1mStep[0m  [4/42], [94mLoss[0m : 9.77859
[1mStep[0m  [8/42], [94mLoss[0m : 9.64065
[1mStep[0m  [12/42], [94mLoss[0m : 9.85203
[1mStep[0m  [16/42], [94mLoss[0m : 9.51581
[1mStep[0m  [20/42], [94mLoss[0m : 9.33973
[1mStep[0m  [24/42], [94mLoss[0m : 9.46435
[1mStep[0m  [28/42], [94mLoss[0m : 9.81695
[1mStep[0m  [32/42], [94mLoss[0m : 9.84052
[1mStep[0m  [36/42], [94mLoss[0m : 9.87293
[1mStep[0m  [40/42], [94mLoss[0m : 10.03867

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.712, [92mTest[0m: 9.515, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89224
[1mStep[0m  [4/42], [94mLoss[0m : 9.94701
[1mStep[0m  [8/42], [94mLoss[0m : 9.56583
[1mStep[0m  [12/42], [94mLoss[0m : 9.76952
[1mStep[0m  [16/42], [94mLoss[0m : 10.00143
[1mStep[0m  [20/42], [94mLoss[0m : 9.50064
[1mStep[0m  [24/42], [94mLoss[0m : 9.41945
[1mStep[0m  [28/42], [94mLoss[0m : 9.64650
[1mStep[0m  [32/42], [94mLoss[0m : 9.30487
[1mStep[0m  [36/42], [94mLoss[0m : 9.86725
[1mStep[0m  [40/42], [94mLoss[0m : 9.99980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.671, [92mTest[0m: 9.482, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.58315
[1mStep[0m  [4/42], [94mLoss[0m : 9.93517
[1mStep[0m  [8/42], [94mLoss[0m : 9.63700
[1mStep[0m  [12/42], [94mLoss[0m : 9.43216
[1mStep[0m  [16/42], [94mLoss[0m : 9.51027
[1mStep[0m  [20/42], [94mLoss[0m : 9.69471
[1mStep[0m  [24/42], [94mLoss[0m : 9.33057
[1mStep[0m  [28/42], [94mLoss[0m : 9.34218
[1mStep[0m  [32/42], [94mLoss[0m : 9.86767
[1mStep[0m  [36/42], [94mLoss[0m : 9.88712
[1mStep[0m  [40/42], [94mLoss[0m : 9.70340

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.609, [92mTest[0m: 9.399, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.92065
[1mStep[0m  [4/42], [94mLoss[0m : 9.66718
[1mStep[0m  [8/42], [94mLoss[0m : 9.42178
[1mStep[0m  [12/42], [94mLoss[0m : 9.45368
[1mStep[0m  [16/42], [94mLoss[0m : 9.48011
[1mStep[0m  [20/42], [94mLoss[0m : 9.46608
[1mStep[0m  [24/42], [94mLoss[0m : 9.64698
[1mStep[0m  [28/42], [94mLoss[0m : 9.56832
[1mStep[0m  [32/42], [94mLoss[0m : 9.50998
[1mStep[0m  [36/42], [94mLoss[0m : 9.50530
[1mStep[0m  [40/42], [94mLoss[0m : 9.13164

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.565, [92mTest[0m: 9.337, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.42589
[1mStep[0m  [4/42], [94mLoss[0m : 9.06823
[1mStep[0m  [8/42], [94mLoss[0m : 9.04670
[1mStep[0m  [12/42], [94mLoss[0m : 9.61650
[1mStep[0m  [16/42], [94mLoss[0m : 9.66317
[1mStep[0m  [20/42], [94mLoss[0m : 9.75733
[1mStep[0m  [24/42], [94mLoss[0m : 9.41029
[1mStep[0m  [28/42], [94mLoss[0m : 9.46039
[1mStep[0m  [32/42], [94mLoss[0m : 9.59560
[1mStep[0m  [36/42], [94mLoss[0m : 9.16854
[1mStep[0m  [40/42], [94mLoss[0m : 9.55299

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.502, [92mTest[0m: 9.287, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.53617
[1mStep[0m  [4/42], [94mLoss[0m : 9.35870
[1mStep[0m  [8/42], [94mLoss[0m : 9.33490
[1mStep[0m  [12/42], [94mLoss[0m : 9.74553
[1mStep[0m  [16/42], [94mLoss[0m : 9.34545
[1mStep[0m  [20/42], [94mLoss[0m : 9.17325
[1mStep[0m  [24/42], [94mLoss[0m : 9.49717
[1mStep[0m  [28/42], [94mLoss[0m : 9.11686
[1mStep[0m  [32/42], [94mLoss[0m : 9.60720
[1mStep[0m  [36/42], [94mLoss[0m : 9.67464
[1mStep[0m  [40/42], [94mLoss[0m : 9.42043

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.449, [92mTest[0m: 9.176, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.135
====================================

Phase 1 - Evaluation MAE:  9.135056018829346
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 9.74642
[1mStep[0m  [4/42], [94mLoss[0m : 9.21467
[1mStep[0m  [8/42], [94mLoss[0m : 9.38771
[1mStep[0m  [12/42], [94mLoss[0m : 9.32545
[1mStep[0m  [16/42], [94mLoss[0m : 9.17768
[1mStep[0m  [20/42], [94mLoss[0m : 9.33011
[1mStep[0m  [24/42], [94mLoss[0m : 9.38073
[1mStep[0m  [28/42], [94mLoss[0m : 8.95224
[1mStep[0m  [32/42], [94mLoss[0m : 9.48577
[1mStep[0m  [36/42], [94mLoss[0m : 9.55123
[1mStep[0m  [40/42], [94mLoss[0m : 9.10945

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.404, [92mTest[0m: 9.136, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.18679
[1mStep[0m  [4/42], [94mLoss[0m : 9.30293
[1mStep[0m  [8/42], [94mLoss[0m : 8.76954
[1mStep[0m  [12/42], [94mLoss[0m : 9.08573
[1mStep[0m  [16/42], [94mLoss[0m : 9.48486
[1mStep[0m  [20/42], [94mLoss[0m : 9.19190
[1mStep[0m  [24/42], [94mLoss[0m : 9.39890
[1mStep[0m  [28/42], [94mLoss[0m : 9.32054
[1mStep[0m  [32/42], [94mLoss[0m : 9.33874
[1mStep[0m  [36/42], [94mLoss[0m : 9.76363
[1mStep[0m  [40/42], [94mLoss[0m : 9.17422

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.337, [92mTest[0m: 9.057, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.44462
[1mStep[0m  [4/42], [94mLoss[0m : 9.24698
[1mStep[0m  [8/42], [94mLoss[0m : 9.56040
[1mStep[0m  [12/42], [94mLoss[0m : 9.15267
[1mStep[0m  [16/42], [94mLoss[0m : 9.14201
[1mStep[0m  [20/42], [94mLoss[0m : 9.54796
[1mStep[0m  [24/42], [94mLoss[0m : 8.99405
[1mStep[0m  [28/42], [94mLoss[0m : 9.37428
[1mStep[0m  [32/42], [94mLoss[0m : 9.32370
[1mStep[0m  [36/42], [94mLoss[0m : 9.61613
[1mStep[0m  [40/42], [94mLoss[0m : 9.01132

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.253, [92mTest[0m: 8.938, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.93830
[1mStep[0m  [4/42], [94mLoss[0m : 8.80034
[1mStep[0m  [8/42], [94mLoss[0m : 9.07688
[1mStep[0m  [12/42], [94mLoss[0m : 9.59789
[1mStep[0m  [16/42], [94mLoss[0m : 9.00634
[1mStep[0m  [20/42], [94mLoss[0m : 9.36516
[1mStep[0m  [24/42], [94mLoss[0m : 9.75541
[1mStep[0m  [28/42], [94mLoss[0m : 9.21993
[1mStep[0m  [32/42], [94mLoss[0m : 9.12764
[1mStep[0m  [36/42], [94mLoss[0m : 8.75079
[1mStep[0m  [40/42], [94mLoss[0m : 9.07916

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.185, [92mTest[0m: 8.919, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.37086
[1mStep[0m  [4/42], [94mLoss[0m : 9.15414
[1mStep[0m  [8/42], [94mLoss[0m : 9.34238
[1mStep[0m  [12/42], [94mLoss[0m : 9.15928
[1mStep[0m  [16/42], [94mLoss[0m : 8.76147
[1mStep[0m  [20/42], [94mLoss[0m : 8.85564
[1mStep[0m  [24/42], [94mLoss[0m : 8.98237
[1mStep[0m  [28/42], [94mLoss[0m : 9.34145
[1mStep[0m  [32/42], [94mLoss[0m : 9.15435
[1mStep[0m  [36/42], [94mLoss[0m : 9.30606
[1mStep[0m  [40/42], [94mLoss[0m : 9.11103

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.103, [92mTest[0m: 8.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.38144
[1mStep[0m  [4/42], [94mLoss[0m : 8.96119
[1mStep[0m  [8/42], [94mLoss[0m : 9.32930
[1mStep[0m  [12/42], [94mLoss[0m : 8.96048
[1mStep[0m  [16/42], [94mLoss[0m : 8.85251
[1mStep[0m  [20/42], [94mLoss[0m : 9.22604
[1mStep[0m  [24/42], [94mLoss[0m : 9.49219
[1mStep[0m  [28/42], [94mLoss[0m : 8.63371
[1mStep[0m  [32/42], [94mLoss[0m : 8.73911
[1mStep[0m  [36/42], [94mLoss[0m : 8.94430
[1mStep[0m  [40/42], [94mLoss[0m : 9.07032

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.027, [92mTest[0m: 8.725, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.61309
[1mStep[0m  [4/42], [94mLoss[0m : 8.58518
[1mStep[0m  [8/42], [94mLoss[0m : 8.81858
[1mStep[0m  [12/42], [94mLoss[0m : 8.73478
[1mStep[0m  [16/42], [94mLoss[0m : 9.38273
[1mStep[0m  [20/42], [94mLoss[0m : 9.10275
[1mStep[0m  [24/42], [94mLoss[0m : 8.44885
[1mStep[0m  [28/42], [94mLoss[0m : 8.88032
[1mStep[0m  [32/42], [94mLoss[0m : 8.67159
[1mStep[0m  [36/42], [94mLoss[0m : 9.01540
[1mStep[0m  [40/42], [94mLoss[0m : 8.97312

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.937, [92mTest[0m: 8.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.75648
[1mStep[0m  [4/42], [94mLoss[0m : 8.98937
[1mStep[0m  [8/42], [94mLoss[0m : 8.93847
[1mStep[0m  [12/42], [94mLoss[0m : 9.05759
[1mStep[0m  [16/42], [94mLoss[0m : 8.99182
[1mStep[0m  [20/42], [94mLoss[0m : 8.50613
[1mStep[0m  [24/42], [94mLoss[0m : 9.15090
[1mStep[0m  [28/42], [94mLoss[0m : 9.15388
[1mStep[0m  [32/42], [94mLoss[0m : 8.97621
[1mStep[0m  [36/42], [94mLoss[0m : 8.72092
[1mStep[0m  [40/42], [94mLoss[0m : 8.62566

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.851, [92mTest[0m: 8.626, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.87136
[1mStep[0m  [4/42], [94mLoss[0m : 8.54465
[1mStep[0m  [8/42], [94mLoss[0m : 8.81410
[1mStep[0m  [12/42], [94mLoss[0m : 8.81770
[1mStep[0m  [16/42], [94mLoss[0m : 8.55994
[1mStep[0m  [20/42], [94mLoss[0m : 8.42027
[1mStep[0m  [24/42], [94mLoss[0m : 8.77720
[1mStep[0m  [28/42], [94mLoss[0m : 9.00434
[1mStep[0m  [32/42], [94mLoss[0m : 8.91903
[1mStep[0m  [36/42], [94mLoss[0m : 8.86461
[1mStep[0m  [40/42], [94mLoss[0m : 8.12131

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.767, [92mTest[0m: 8.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.73751
[1mStep[0m  [4/42], [94mLoss[0m : 8.55779
[1mStep[0m  [8/42], [94mLoss[0m : 8.60423
[1mStep[0m  [12/42], [94mLoss[0m : 8.72233
[1mStep[0m  [16/42], [94mLoss[0m : 8.84093
[1mStep[0m  [20/42], [94mLoss[0m : 8.63879
[1mStep[0m  [24/42], [94mLoss[0m : 8.68874
[1mStep[0m  [28/42], [94mLoss[0m : 9.00864
[1mStep[0m  [32/42], [94mLoss[0m : 9.01344
[1mStep[0m  [36/42], [94mLoss[0m : 8.92234
[1mStep[0m  [40/42], [94mLoss[0m : 9.06942

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.684, [92mTest[0m: 8.428, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34633
[1mStep[0m  [4/42], [94mLoss[0m : 8.74780
[1mStep[0m  [8/42], [94mLoss[0m : 8.39550
[1mStep[0m  [12/42], [94mLoss[0m : 8.62765
[1mStep[0m  [16/42], [94mLoss[0m : 8.41746
[1mStep[0m  [20/42], [94mLoss[0m : 8.51240
[1mStep[0m  [24/42], [94mLoss[0m : 8.55627
[1mStep[0m  [28/42], [94mLoss[0m : 8.49718
[1mStep[0m  [32/42], [94mLoss[0m : 8.61635
[1mStep[0m  [36/42], [94mLoss[0m : 8.73983
[1mStep[0m  [40/42], [94mLoss[0m : 8.73490

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.584, [92mTest[0m: 8.320, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27121
[1mStep[0m  [4/42], [94mLoss[0m : 8.38789
[1mStep[0m  [8/42], [94mLoss[0m : 8.83226
[1mStep[0m  [12/42], [94mLoss[0m : 8.22137
[1mStep[0m  [16/42], [94mLoss[0m : 8.77205
[1mStep[0m  [20/42], [94mLoss[0m : 7.96128
[1mStep[0m  [24/42], [94mLoss[0m : 8.81882
[1mStep[0m  [28/42], [94mLoss[0m : 8.86123
[1mStep[0m  [32/42], [94mLoss[0m : 8.08850
[1mStep[0m  [36/42], [94mLoss[0m : 8.32664
[1mStep[0m  [40/42], [94mLoss[0m : 8.44946

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.487, [92mTest[0m: 8.139, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.68060
[1mStep[0m  [4/42], [94mLoss[0m : 8.35192
[1mStep[0m  [8/42], [94mLoss[0m : 8.41813
[1mStep[0m  [12/42], [94mLoss[0m : 8.40141
[1mStep[0m  [16/42], [94mLoss[0m : 8.48001
[1mStep[0m  [20/42], [94mLoss[0m : 8.53914
[1mStep[0m  [24/42], [94mLoss[0m : 8.43782
[1mStep[0m  [28/42], [94mLoss[0m : 8.51986
[1mStep[0m  [32/42], [94mLoss[0m : 8.42336
[1mStep[0m  [36/42], [94mLoss[0m : 7.84286
[1mStep[0m  [40/42], [94mLoss[0m : 7.97661

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.388, [92mTest[0m: 8.034, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.08319
[1mStep[0m  [4/42], [94mLoss[0m : 8.18322
[1mStep[0m  [8/42], [94mLoss[0m : 8.04729
[1mStep[0m  [12/42], [94mLoss[0m : 8.31660
[1mStep[0m  [16/42], [94mLoss[0m : 8.44638
[1mStep[0m  [20/42], [94mLoss[0m : 8.32544
[1mStep[0m  [24/42], [94mLoss[0m : 8.31187
[1mStep[0m  [28/42], [94mLoss[0m : 8.20170
[1mStep[0m  [32/42], [94mLoss[0m : 7.92099
[1mStep[0m  [36/42], [94mLoss[0m : 7.96688
[1mStep[0m  [40/42], [94mLoss[0m : 8.39500

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.271, [92mTest[0m: 7.784, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.69716
[1mStep[0m  [4/42], [94mLoss[0m : 7.98368
[1mStep[0m  [8/42], [94mLoss[0m : 8.66813
[1mStep[0m  [12/42], [94mLoss[0m : 8.24698
[1mStep[0m  [16/42], [94mLoss[0m : 7.91613
[1mStep[0m  [20/42], [94mLoss[0m : 8.27258
[1mStep[0m  [24/42], [94mLoss[0m : 8.36733
[1mStep[0m  [28/42], [94mLoss[0m : 8.46010
[1mStep[0m  [32/42], [94mLoss[0m : 8.26761
[1mStep[0m  [36/42], [94mLoss[0m : 8.16849
[1mStep[0m  [40/42], [94mLoss[0m : 8.25758

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.175, [92mTest[0m: 7.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.28985
[1mStep[0m  [4/42], [94mLoss[0m : 8.00718
[1mStep[0m  [8/42], [94mLoss[0m : 7.98947
[1mStep[0m  [12/42], [94mLoss[0m : 8.24952
[1mStep[0m  [16/42], [94mLoss[0m : 7.96203
[1mStep[0m  [20/42], [94mLoss[0m : 7.91030
[1mStep[0m  [24/42], [94mLoss[0m : 8.04957
[1mStep[0m  [28/42], [94mLoss[0m : 8.28158
[1mStep[0m  [32/42], [94mLoss[0m : 8.10500
[1mStep[0m  [36/42], [94mLoss[0m : 7.77929
[1mStep[0m  [40/42], [94mLoss[0m : 8.44515

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.072, [92mTest[0m: 7.719, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.00047
[1mStep[0m  [4/42], [94mLoss[0m : 7.89109
[1mStep[0m  [8/42], [94mLoss[0m : 8.14502
[1mStep[0m  [12/42], [94mLoss[0m : 7.96097
[1mStep[0m  [16/42], [94mLoss[0m : 7.77873
[1mStep[0m  [20/42], [94mLoss[0m : 8.13437
[1mStep[0m  [24/42], [94mLoss[0m : 8.13416
[1mStep[0m  [28/42], [94mLoss[0m : 7.99284
[1mStep[0m  [32/42], [94mLoss[0m : 8.12122
[1mStep[0m  [36/42], [94mLoss[0m : 7.61804
[1mStep[0m  [40/42], [94mLoss[0m : 7.73253

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.928, [92mTest[0m: 7.703, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.49596
[1mStep[0m  [4/42], [94mLoss[0m : 7.56905
[1mStep[0m  [8/42], [94mLoss[0m : 8.01459
[1mStep[0m  [12/42], [94mLoss[0m : 8.14543
[1mStep[0m  [16/42], [94mLoss[0m : 8.00408
[1mStep[0m  [20/42], [94mLoss[0m : 8.10384
[1mStep[0m  [24/42], [94mLoss[0m : 7.77933
[1mStep[0m  [28/42], [94mLoss[0m : 7.47772
[1mStep[0m  [32/42], [94mLoss[0m : 7.56972
[1mStep[0m  [36/42], [94mLoss[0m : 7.55718
[1mStep[0m  [40/42], [94mLoss[0m : 8.01760

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.824, [92mTest[0m: 7.684, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.79246
[1mStep[0m  [4/42], [94mLoss[0m : 7.89720
[1mStep[0m  [8/42], [94mLoss[0m : 7.36882
[1mStep[0m  [12/42], [94mLoss[0m : 7.76575
[1mStep[0m  [16/42], [94mLoss[0m : 7.90474
[1mStep[0m  [20/42], [94mLoss[0m : 7.79548
[1mStep[0m  [24/42], [94mLoss[0m : 7.92833
[1mStep[0m  [28/42], [94mLoss[0m : 7.55974
[1mStep[0m  [32/42], [94mLoss[0m : 7.49498
[1mStep[0m  [36/42], [94mLoss[0m : 7.68853
[1mStep[0m  [40/42], [94mLoss[0m : 7.67868

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.704, [92mTest[0m: 7.399, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.89553
[1mStep[0m  [4/42], [94mLoss[0m : 7.86521
[1mStep[0m  [8/42], [94mLoss[0m : 7.59772
[1mStep[0m  [12/42], [94mLoss[0m : 7.95143
[1mStep[0m  [16/42], [94mLoss[0m : 8.01385
[1mStep[0m  [20/42], [94mLoss[0m : 7.64874
[1mStep[0m  [24/42], [94mLoss[0m : 7.37233
[1mStep[0m  [28/42], [94mLoss[0m : 7.91833
[1mStep[0m  [32/42], [94mLoss[0m : 7.53619
[1mStep[0m  [36/42], [94mLoss[0m : 7.65939
[1mStep[0m  [40/42], [94mLoss[0m : 7.28111

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.566, [92mTest[0m: 7.049, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.18639
[1mStep[0m  [4/42], [94mLoss[0m : 7.31963
[1mStep[0m  [8/42], [94mLoss[0m : 7.19640
[1mStep[0m  [12/42], [94mLoss[0m : 7.12149
[1mStep[0m  [16/42], [94mLoss[0m : 7.78609
[1mStep[0m  [20/42], [94mLoss[0m : 7.16277
[1mStep[0m  [24/42], [94mLoss[0m : 7.42988
[1mStep[0m  [28/42], [94mLoss[0m : 7.21249
[1mStep[0m  [32/42], [94mLoss[0m : 7.19919
[1mStep[0m  [36/42], [94mLoss[0m : 7.33474
[1mStep[0m  [40/42], [94mLoss[0m : 7.30594

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.430, [92mTest[0m: 6.842, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.04524
[1mStep[0m  [4/42], [94mLoss[0m : 7.20624
[1mStep[0m  [8/42], [94mLoss[0m : 7.34905
[1mStep[0m  [12/42], [94mLoss[0m : 7.56426
[1mStep[0m  [16/42], [94mLoss[0m : 7.36344
[1mStep[0m  [20/42], [94mLoss[0m : 7.30453
[1mStep[0m  [24/42], [94mLoss[0m : 7.42925
[1mStep[0m  [28/42], [94mLoss[0m : 7.35044
[1mStep[0m  [32/42], [94mLoss[0m : 7.61241
[1mStep[0m  [36/42], [94mLoss[0m : 7.40846
[1mStep[0m  [40/42], [94mLoss[0m : 7.54021

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.311, [92mTest[0m: 6.721, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.08588
[1mStep[0m  [4/42], [94mLoss[0m : 6.97458
[1mStep[0m  [8/42], [94mLoss[0m : 7.06464
[1mStep[0m  [12/42], [94mLoss[0m : 7.33935
[1mStep[0m  [16/42], [94mLoss[0m : 7.17266
[1mStep[0m  [20/42], [94mLoss[0m : 6.92734
[1mStep[0m  [24/42], [94mLoss[0m : 7.44491
[1mStep[0m  [28/42], [94mLoss[0m : 7.48629
[1mStep[0m  [32/42], [94mLoss[0m : 7.39384
[1mStep[0m  [36/42], [94mLoss[0m : 7.27644
[1mStep[0m  [40/42], [94mLoss[0m : 7.38210

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.203, [92mTest[0m: 7.065, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.35527
[1mStep[0m  [4/42], [94mLoss[0m : 6.96015
[1mStep[0m  [8/42], [94mLoss[0m : 6.75774
[1mStep[0m  [12/42], [94mLoss[0m : 6.66897
[1mStep[0m  [16/42], [94mLoss[0m : 7.10687
[1mStep[0m  [20/42], [94mLoss[0m : 7.15359
[1mStep[0m  [24/42], [94mLoss[0m : 7.00220
[1mStep[0m  [28/42], [94mLoss[0m : 7.22461
[1mStep[0m  [32/42], [94mLoss[0m : 7.14078
[1mStep[0m  [36/42], [94mLoss[0m : 6.92707
[1mStep[0m  [40/42], [94mLoss[0m : 7.40125

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.052, [92mTest[0m: 6.650, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.14628
[1mStep[0m  [4/42], [94mLoss[0m : 6.75161
[1mStep[0m  [8/42], [94mLoss[0m : 7.10757
[1mStep[0m  [12/42], [94mLoss[0m : 6.98055
[1mStep[0m  [16/42], [94mLoss[0m : 7.03687
[1mStep[0m  [20/42], [94mLoss[0m : 6.69817
[1mStep[0m  [24/42], [94mLoss[0m : 7.00858
[1mStep[0m  [28/42], [94mLoss[0m : 6.93712
[1mStep[0m  [32/42], [94mLoss[0m : 6.94500
[1mStep[0m  [36/42], [94mLoss[0m : 6.88188
[1mStep[0m  [40/42], [94mLoss[0m : 6.66045

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.929, [92mTest[0m: 7.404, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.72409
[1mStep[0m  [4/42], [94mLoss[0m : 6.71218
[1mStep[0m  [8/42], [94mLoss[0m : 6.76634
[1mStep[0m  [12/42], [94mLoss[0m : 6.89981
[1mStep[0m  [16/42], [94mLoss[0m : 7.09396
[1mStep[0m  [20/42], [94mLoss[0m : 6.74817
[1mStep[0m  [24/42], [94mLoss[0m : 6.66259
[1mStep[0m  [28/42], [94mLoss[0m : 6.94469
[1mStep[0m  [32/42], [94mLoss[0m : 6.88898
[1mStep[0m  [36/42], [94mLoss[0m : 6.81970
[1mStep[0m  [40/42], [94mLoss[0m : 6.67675

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.798, [92mTest[0m: 6.676, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.61577
[1mStep[0m  [4/42], [94mLoss[0m : 6.87702
[1mStep[0m  [8/42], [94mLoss[0m : 6.95135
[1mStep[0m  [12/42], [94mLoss[0m : 6.69622
[1mStep[0m  [16/42], [94mLoss[0m : 6.50911
[1mStep[0m  [20/42], [94mLoss[0m : 6.32963
[1mStep[0m  [24/42], [94mLoss[0m : 6.63527
[1mStep[0m  [28/42], [94mLoss[0m : 6.79995
[1mStep[0m  [32/42], [94mLoss[0m : 6.12429
[1mStep[0m  [36/42], [94mLoss[0m : 6.62545
[1mStep[0m  [40/42], [94mLoss[0m : 6.59952

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.659, [92mTest[0m: 5.979, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.80070
[1mStep[0m  [4/42], [94mLoss[0m : 6.62878
[1mStep[0m  [8/42], [94mLoss[0m : 6.85628
[1mStep[0m  [12/42], [94mLoss[0m : 6.71708
[1mStep[0m  [16/42], [94mLoss[0m : 6.52368
[1mStep[0m  [20/42], [94mLoss[0m : 6.20575
[1mStep[0m  [24/42], [94mLoss[0m : 6.44873
[1mStep[0m  [28/42], [94mLoss[0m : 6.20889
[1mStep[0m  [32/42], [94mLoss[0m : 6.35417
[1mStep[0m  [36/42], [94mLoss[0m : 6.06854
[1mStep[0m  [40/42], [94mLoss[0m : 6.39585

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.534, [92mTest[0m: 5.753, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.40455
[1mStep[0m  [4/42], [94mLoss[0m : 6.22917
[1mStep[0m  [8/42], [94mLoss[0m : 6.30591
[1mStep[0m  [12/42], [94mLoss[0m : 6.23987
[1mStep[0m  [16/42], [94mLoss[0m : 6.27320
[1mStep[0m  [20/42], [94mLoss[0m : 6.30171
[1mStep[0m  [24/42], [94mLoss[0m : 6.26619
[1mStep[0m  [28/42], [94mLoss[0m : 6.56841
[1mStep[0m  [32/42], [94mLoss[0m : 6.22323
[1mStep[0m  [36/42], [94mLoss[0m : 6.55166
[1mStep[0m  [40/42], [94mLoss[0m : 6.54610

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.392, [92mTest[0m: 6.206, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.41240
[1mStep[0m  [4/42], [94mLoss[0m : 6.13819
[1mStep[0m  [8/42], [94mLoss[0m : 6.02787
[1mStep[0m  [12/42], [94mLoss[0m : 6.29822
[1mStep[0m  [16/42], [94mLoss[0m : 6.40415
[1mStep[0m  [20/42], [94mLoss[0m : 6.48345
[1mStep[0m  [24/42], [94mLoss[0m : 6.44408
[1mStep[0m  [28/42], [94mLoss[0m : 6.29343
[1mStep[0m  [32/42], [94mLoss[0m : 6.21328
[1mStep[0m  [36/42], [94mLoss[0m : 6.05769
[1mStep[0m  [40/42], [94mLoss[0m : 6.22536

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.264, [92mTest[0m: 5.712, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.493
====================================

Phase 2 - Evaluation MAE:  5.492551667349679
MAE score P1       9.135056
MAE score P2       5.492552
loss               6.263828
learning_rate        0.0001
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay           0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.29097
[1mStep[0m  [4/42], [94mLoss[0m : 10.79070
[1mStep[0m  [8/42], [94mLoss[0m : 10.82632
[1mStep[0m  [12/42], [94mLoss[0m : 10.83553
[1mStep[0m  [16/42], [94mLoss[0m : 10.74669
[1mStep[0m  [20/42], [94mLoss[0m : 10.93804
[1mStep[0m  [24/42], [94mLoss[0m : 10.87062
[1mStep[0m  [28/42], [94mLoss[0m : 11.01699
[1mStep[0m  [32/42], [94mLoss[0m : 11.14258
[1mStep[0m  [36/42], [94mLoss[0m : 11.16607
[1mStep[0m  [40/42], [94mLoss[0m : 11.00310

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.884, [92mTest[0m: 10.881, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.89340
[1mStep[0m  [4/42], [94mLoss[0m : 10.77799
[1mStep[0m  [8/42], [94mLoss[0m : 11.04500
[1mStep[0m  [12/42], [94mLoss[0m : 10.92506
[1mStep[0m  [16/42], [94mLoss[0m : 10.91495
[1mStep[0m  [20/42], [94mLoss[0m : 11.13556
[1mStep[0m  [24/42], [94mLoss[0m : 10.55239
[1mStep[0m  [28/42], [94mLoss[0m : 10.87190
[1mStep[0m  [32/42], [94mLoss[0m : 10.64488
[1mStep[0m  [36/42], [94mLoss[0m : 11.21749
[1mStep[0m  [40/42], [94mLoss[0m : 10.53345

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.877, [92mTest[0m: 10.855, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.27366
[1mStep[0m  [4/42], [94mLoss[0m : 10.97961
[1mStep[0m  [8/42], [94mLoss[0m : 10.86272
[1mStep[0m  [12/42], [94mLoss[0m : 10.82567
[1mStep[0m  [16/42], [94mLoss[0m : 10.50784
[1mStep[0m  [20/42], [94mLoss[0m : 10.45154
[1mStep[0m  [24/42], [94mLoss[0m : 10.93779
[1mStep[0m  [28/42], [94mLoss[0m : 10.63904
[1mStep[0m  [32/42], [94mLoss[0m : 10.95066
[1mStep[0m  [36/42], [94mLoss[0m : 10.96494
[1mStep[0m  [40/42], [94mLoss[0m : 11.21676

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.878, [92mTest[0m: 10.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64837
[1mStep[0m  [4/42], [94mLoss[0m : 11.22951
[1mStep[0m  [8/42], [94mLoss[0m : 10.83832
[1mStep[0m  [12/42], [94mLoss[0m : 11.13727
[1mStep[0m  [16/42], [94mLoss[0m : 10.62876
[1mStep[0m  [20/42], [94mLoss[0m : 10.87721
[1mStep[0m  [24/42], [94mLoss[0m : 10.75627
[1mStep[0m  [28/42], [94mLoss[0m : 11.10610
[1mStep[0m  [32/42], [94mLoss[0m : 10.99218
[1mStep[0m  [36/42], [94mLoss[0m : 10.76828
[1mStep[0m  [40/42], [94mLoss[0m : 10.71655

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.859, [92mTest[0m: 10.827, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.80294
[1mStep[0m  [4/42], [94mLoss[0m : 10.70114
[1mStep[0m  [8/42], [94mLoss[0m : 10.56056
[1mStep[0m  [12/42], [94mLoss[0m : 10.72788
[1mStep[0m  [16/42], [94mLoss[0m : 10.88287
[1mStep[0m  [20/42], [94mLoss[0m : 10.73635
[1mStep[0m  [24/42], [94mLoss[0m : 10.90298
[1mStep[0m  [28/42], [94mLoss[0m : 10.99397
[1mStep[0m  [32/42], [94mLoss[0m : 10.81456
[1mStep[0m  [36/42], [94mLoss[0m : 10.99935
[1mStep[0m  [40/42], [94mLoss[0m : 10.66139

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.851, [92mTest[0m: 10.818, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.73235
[1mStep[0m  [4/42], [94mLoss[0m : 11.07841
[1mStep[0m  [8/42], [94mLoss[0m : 10.64846
[1mStep[0m  [12/42], [94mLoss[0m : 11.03841
[1mStep[0m  [16/42], [94mLoss[0m : 10.72557
[1mStep[0m  [20/42], [94mLoss[0m : 10.73817
[1mStep[0m  [24/42], [94mLoss[0m : 10.55043
[1mStep[0m  [28/42], [94mLoss[0m : 11.06888
[1mStep[0m  [32/42], [94mLoss[0m : 10.98488
[1mStep[0m  [36/42], [94mLoss[0m : 11.10704
[1mStep[0m  [40/42], [94mLoss[0m : 10.52088

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.854, [92mTest[0m: 10.808, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74078
[1mStep[0m  [4/42], [94mLoss[0m : 10.70468
[1mStep[0m  [8/42], [94mLoss[0m : 10.78222
[1mStep[0m  [12/42], [94mLoss[0m : 11.00231
[1mStep[0m  [16/42], [94mLoss[0m : 11.14746
[1mStep[0m  [20/42], [94mLoss[0m : 11.18316
[1mStep[0m  [24/42], [94mLoss[0m : 10.89886
[1mStep[0m  [28/42], [94mLoss[0m : 11.03209
[1mStep[0m  [32/42], [94mLoss[0m : 10.94958
[1mStep[0m  [36/42], [94mLoss[0m : 10.96170
[1mStep[0m  [40/42], [94mLoss[0m : 11.25727

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.839, [92mTest[0m: 10.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90724
[1mStep[0m  [4/42], [94mLoss[0m : 11.00129
[1mStep[0m  [8/42], [94mLoss[0m : 10.94127
[1mStep[0m  [12/42], [94mLoss[0m : 10.74817
[1mStep[0m  [16/42], [94mLoss[0m : 10.94641
[1mStep[0m  [20/42], [94mLoss[0m : 10.66480
[1mStep[0m  [24/42], [94mLoss[0m : 10.31401
[1mStep[0m  [28/42], [94mLoss[0m : 10.87317
[1mStep[0m  [32/42], [94mLoss[0m : 10.43647
[1mStep[0m  [36/42], [94mLoss[0m : 10.96241
[1mStep[0m  [40/42], [94mLoss[0m : 10.89420

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.833, [92mTest[0m: 10.784, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.48029
[1mStep[0m  [4/42], [94mLoss[0m : 10.77368
[1mStep[0m  [8/42], [94mLoss[0m : 10.45002
[1mStep[0m  [12/42], [94mLoss[0m : 10.71426
[1mStep[0m  [16/42], [94mLoss[0m : 11.20607
[1mStep[0m  [20/42], [94mLoss[0m : 11.00491
[1mStep[0m  [24/42], [94mLoss[0m : 10.67806
[1mStep[0m  [28/42], [94mLoss[0m : 10.73141
[1mStep[0m  [32/42], [94mLoss[0m : 10.83274
[1mStep[0m  [36/42], [94mLoss[0m : 10.88913
[1mStep[0m  [40/42], [94mLoss[0m : 10.66430

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.827, [92mTest[0m: 10.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68888
[1mStep[0m  [4/42], [94mLoss[0m : 11.16204
[1mStep[0m  [8/42], [94mLoss[0m : 11.09318
[1mStep[0m  [12/42], [94mLoss[0m : 10.70968
[1mStep[0m  [16/42], [94mLoss[0m : 10.76889
[1mStep[0m  [20/42], [94mLoss[0m : 10.56881
[1mStep[0m  [24/42], [94mLoss[0m : 10.64406
[1mStep[0m  [28/42], [94mLoss[0m : 10.99980
[1mStep[0m  [32/42], [94mLoss[0m : 10.62244
[1mStep[0m  [36/42], [94mLoss[0m : 10.90961
[1mStep[0m  [40/42], [94mLoss[0m : 10.77585

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.814, [92mTest[0m: 10.768, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72385
[1mStep[0m  [4/42], [94mLoss[0m : 10.56615
[1mStep[0m  [8/42], [94mLoss[0m : 10.93426
[1mStep[0m  [12/42], [94mLoss[0m : 10.61544
[1mStep[0m  [16/42], [94mLoss[0m : 11.08376
[1mStep[0m  [20/42], [94mLoss[0m : 10.54391
[1mStep[0m  [24/42], [94mLoss[0m : 10.65737
[1mStep[0m  [28/42], [94mLoss[0m : 10.64329
[1mStep[0m  [32/42], [94mLoss[0m : 11.16809
[1mStep[0m  [36/42], [94mLoss[0m : 10.74879
[1mStep[0m  [40/42], [94mLoss[0m : 10.77038

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.809, [92mTest[0m: 10.751, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.17704
[1mStep[0m  [4/42], [94mLoss[0m : 10.59450
[1mStep[0m  [8/42], [94mLoss[0m : 10.85640
[1mStep[0m  [12/42], [94mLoss[0m : 10.84429
[1mStep[0m  [16/42], [94mLoss[0m : 10.92452
[1mStep[0m  [20/42], [94mLoss[0m : 10.99685
[1mStep[0m  [24/42], [94mLoss[0m : 11.04145
[1mStep[0m  [28/42], [94mLoss[0m : 10.68008
[1mStep[0m  [32/42], [94mLoss[0m : 10.80749
[1mStep[0m  [36/42], [94mLoss[0m : 10.65131
[1mStep[0m  [40/42], [94mLoss[0m : 10.62359

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.801, [92mTest[0m: 10.750, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.41366
[1mStep[0m  [4/42], [94mLoss[0m : 10.91367
[1mStep[0m  [8/42], [94mLoss[0m : 10.78679
[1mStep[0m  [12/42], [94mLoss[0m : 11.21527
[1mStep[0m  [16/42], [94mLoss[0m : 10.75990
[1mStep[0m  [20/42], [94mLoss[0m : 11.00455
[1mStep[0m  [24/42], [94mLoss[0m : 10.78586
[1mStep[0m  [28/42], [94mLoss[0m : 10.40884
[1mStep[0m  [32/42], [94mLoss[0m : 11.07462
[1mStep[0m  [36/42], [94mLoss[0m : 10.73660
[1mStep[0m  [40/42], [94mLoss[0m : 11.02908

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.792, [92mTest[0m: 10.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.82892
[1mStep[0m  [4/42], [94mLoss[0m : 10.77796
[1mStep[0m  [8/42], [94mLoss[0m : 10.62649
[1mStep[0m  [12/42], [94mLoss[0m : 10.84272
[1mStep[0m  [16/42], [94mLoss[0m : 10.39803
[1mStep[0m  [20/42], [94mLoss[0m : 10.46525
[1mStep[0m  [24/42], [94mLoss[0m : 10.70511
[1mStep[0m  [28/42], [94mLoss[0m : 10.65591
[1mStep[0m  [32/42], [94mLoss[0m : 10.70905
[1mStep[0m  [36/42], [94mLoss[0m : 11.17489
[1mStep[0m  [40/42], [94mLoss[0m : 10.71452

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.796, [92mTest[0m: 10.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.04396
[1mStep[0m  [4/42], [94mLoss[0m : 10.77663
[1mStep[0m  [8/42], [94mLoss[0m : 10.66807
[1mStep[0m  [12/42], [94mLoss[0m : 10.88173
[1mStep[0m  [16/42], [94mLoss[0m : 11.42698
[1mStep[0m  [20/42], [94mLoss[0m : 10.62604
[1mStep[0m  [24/42], [94mLoss[0m : 10.69164
[1mStep[0m  [28/42], [94mLoss[0m : 10.96688
[1mStep[0m  [32/42], [94mLoss[0m : 10.80309
[1mStep[0m  [36/42], [94mLoss[0m : 10.86479
[1mStep[0m  [40/42], [94mLoss[0m : 10.78904

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.780, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.75671
[1mStep[0m  [4/42], [94mLoss[0m : 11.17005
[1mStep[0m  [8/42], [94mLoss[0m : 10.74482
[1mStep[0m  [12/42], [94mLoss[0m : 10.81051
[1mStep[0m  [16/42], [94mLoss[0m : 10.43866
[1mStep[0m  [20/42], [94mLoss[0m : 11.09269
[1mStep[0m  [24/42], [94mLoss[0m : 10.36600
[1mStep[0m  [28/42], [94mLoss[0m : 10.69828
[1mStep[0m  [32/42], [94mLoss[0m : 10.48422
[1mStep[0m  [36/42], [94mLoss[0m : 10.26976
[1mStep[0m  [40/42], [94mLoss[0m : 10.69287

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.773, [92mTest[0m: 10.705, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.59241
[1mStep[0m  [4/42], [94mLoss[0m : 10.85986
[1mStep[0m  [8/42], [94mLoss[0m : 10.96819
[1mStep[0m  [12/42], [94mLoss[0m : 11.12679
[1mStep[0m  [16/42], [94mLoss[0m : 11.07443
[1mStep[0m  [20/42], [94mLoss[0m : 10.92800
[1mStep[0m  [24/42], [94mLoss[0m : 10.76762
[1mStep[0m  [28/42], [94mLoss[0m : 10.60290
[1mStep[0m  [32/42], [94mLoss[0m : 11.07036
[1mStep[0m  [36/42], [94mLoss[0m : 10.85672
[1mStep[0m  [40/42], [94mLoss[0m : 10.76578

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.758, [92mTest[0m: 10.687, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54282
[1mStep[0m  [4/42], [94mLoss[0m : 11.01760
[1mStep[0m  [8/42], [94mLoss[0m : 10.59185
[1mStep[0m  [12/42], [94mLoss[0m : 10.49924
[1mStep[0m  [16/42], [94mLoss[0m : 10.69470
[1mStep[0m  [20/42], [94mLoss[0m : 10.79575
[1mStep[0m  [24/42], [94mLoss[0m : 11.09349
[1mStep[0m  [28/42], [94mLoss[0m : 10.70281
[1mStep[0m  [32/42], [94mLoss[0m : 10.64661
[1mStep[0m  [36/42], [94mLoss[0m : 10.75362
[1mStep[0m  [40/42], [94mLoss[0m : 11.02573

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.756, [92mTest[0m: 10.686, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22802
[1mStep[0m  [4/42], [94mLoss[0m : 10.45096
[1mStep[0m  [8/42], [94mLoss[0m : 10.98789
[1mStep[0m  [12/42], [94mLoss[0m : 10.84134
[1mStep[0m  [16/42], [94mLoss[0m : 10.83161
[1mStep[0m  [20/42], [94mLoss[0m : 10.50131
[1mStep[0m  [24/42], [94mLoss[0m : 10.73709
[1mStep[0m  [28/42], [94mLoss[0m : 10.85029
[1mStep[0m  [32/42], [94mLoss[0m : 10.93176
[1mStep[0m  [36/42], [94mLoss[0m : 10.94089
[1mStep[0m  [40/42], [94mLoss[0m : 10.64942

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.750, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54043
[1mStep[0m  [4/42], [94mLoss[0m : 10.50919
[1mStep[0m  [8/42], [94mLoss[0m : 10.68618
[1mStep[0m  [12/42], [94mLoss[0m : 10.78241
[1mStep[0m  [16/42], [94mLoss[0m : 10.53014
[1mStep[0m  [20/42], [94mLoss[0m : 10.57847
[1mStep[0m  [24/42], [94mLoss[0m : 10.88592
[1mStep[0m  [28/42], [94mLoss[0m : 10.80506
[1mStep[0m  [32/42], [94mLoss[0m : 10.81952
[1mStep[0m  [36/42], [94mLoss[0m : 10.81339
[1mStep[0m  [40/42], [94mLoss[0m : 11.01963

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.739, [92mTest[0m: 10.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93285
[1mStep[0m  [4/42], [94mLoss[0m : 10.59078
[1mStep[0m  [8/42], [94mLoss[0m : 10.50148
[1mStep[0m  [12/42], [94mLoss[0m : 10.32232
[1mStep[0m  [16/42], [94mLoss[0m : 10.90967
[1mStep[0m  [20/42], [94mLoss[0m : 10.74945
[1mStep[0m  [24/42], [94mLoss[0m : 10.56187
[1mStep[0m  [28/42], [94mLoss[0m : 10.84480
[1mStep[0m  [32/42], [94mLoss[0m : 10.53157
[1mStep[0m  [36/42], [94mLoss[0m : 10.70782
[1mStep[0m  [40/42], [94mLoss[0m : 10.89692

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.737, [92mTest[0m: 10.651, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52299
[1mStep[0m  [4/42], [94mLoss[0m : 10.82083
[1mStep[0m  [8/42], [94mLoss[0m : 11.22150
[1mStep[0m  [12/42], [94mLoss[0m : 10.46144
[1mStep[0m  [16/42], [94mLoss[0m : 10.64870
[1mStep[0m  [20/42], [94mLoss[0m : 10.92176
[1mStep[0m  [24/42], [94mLoss[0m : 10.98070
[1mStep[0m  [28/42], [94mLoss[0m : 10.20478
[1mStep[0m  [32/42], [94mLoss[0m : 10.56101
[1mStep[0m  [36/42], [94mLoss[0m : 11.11230
[1mStep[0m  [40/42], [94mLoss[0m : 10.76825

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.730, [92mTest[0m: 10.648, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74762
[1mStep[0m  [4/42], [94mLoss[0m : 10.56399
[1mStep[0m  [8/42], [94mLoss[0m : 10.97062
[1mStep[0m  [12/42], [94mLoss[0m : 10.43349
[1mStep[0m  [16/42], [94mLoss[0m : 10.71937
[1mStep[0m  [20/42], [94mLoss[0m : 10.61290
[1mStep[0m  [24/42], [94mLoss[0m : 10.31258
[1mStep[0m  [28/42], [94mLoss[0m : 10.40442
[1mStep[0m  [32/42], [94mLoss[0m : 10.70070
[1mStep[0m  [36/42], [94mLoss[0m : 10.93706
[1mStep[0m  [40/42], [94mLoss[0m : 10.70933

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.629, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81169
[1mStep[0m  [4/42], [94mLoss[0m : 10.43835
[1mStep[0m  [8/42], [94mLoss[0m : 10.43169
[1mStep[0m  [12/42], [94mLoss[0m : 10.95687
[1mStep[0m  [16/42], [94mLoss[0m : 11.11370
[1mStep[0m  [20/42], [94mLoss[0m : 10.63716
[1mStep[0m  [24/42], [94mLoss[0m : 10.59543
[1mStep[0m  [28/42], [94mLoss[0m : 10.97098
[1mStep[0m  [32/42], [94mLoss[0m : 10.61399
[1mStep[0m  [36/42], [94mLoss[0m : 10.81859
[1mStep[0m  [40/42], [94mLoss[0m : 10.75899

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.616, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71842
[1mStep[0m  [4/42], [94mLoss[0m : 10.89182
[1mStep[0m  [8/42], [94mLoss[0m : 10.82740
[1mStep[0m  [12/42], [94mLoss[0m : 10.68703
[1mStep[0m  [16/42], [94mLoss[0m : 10.53223
[1mStep[0m  [20/42], [94mLoss[0m : 10.69779
[1mStep[0m  [24/42], [94mLoss[0m : 10.88498
[1mStep[0m  [28/42], [94mLoss[0m : 10.47288
[1mStep[0m  [32/42], [94mLoss[0m : 10.71738
[1mStep[0m  [36/42], [94mLoss[0m : 10.66961
[1mStep[0m  [40/42], [94mLoss[0m : 10.40284

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.703, [92mTest[0m: 10.611, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76578
[1mStep[0m  [4/42], [94mLoss[0m : 10.99917
[1mStep[0m  [8/42], [94mLoss[0m : 10.64724
[1mStep[0m  [12/42], [94mLoss[0m : 11.06954
[1mStep[0m  [16/42], [94mLoss[0m : 10.39252
[1mStep[0m  [20/42], [94mLoss[0m : 10.72430
[1mStep[0m  [24/42], [94mLoss[0m : 10.77894
[1mStep[0m  [28/42], [94mLoss[0m : 10.90866
[1mStep[0m  [32/42], [94mLoss[0m : 10.32106
[1mStep[0m  [36/42], [94mLoss[0m : 10.85853
[1mStep[0m  [40/42], [94mLoss[0m : 10.57830

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.703, [92mTest[0m: 10.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57327
[1mStep[0m  [4/42], [94mLoss[0m : 10.88097
[1mStep[0m  [8/42], [94mLoss[0m : 10.38960
[1mStep[0m  [12/42], [94mLoss[0m : 11.07002
[1mStep[0m  [16/42], [94mLoss[0m : 10.57295
[1mStep[0m  [20/42], [94mLoss[0m : 11.02427
[1mStep[0m  [24/42], [94mLoss[0m : 10.95166
[1mStep[0m  [28/42], [94mLoss[0m : 10.66524
[1mStep[0m  [32/42], [94mLoss[0m : 10.52188
[1mStep[0m  [36/42], [94mLoss[0m : 10.68009
[1mStep[0m  [40/42], [94mLoss[0m : 10.43784

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.690, [92mTest[0m: 10.603, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.86742
[1mStep[0m  [4/42], [94mLoss[0m : 10.63706
[1mStep[0m  [8/42], [94mLoss[0m : 10.50108
[1mStep[0m  [12/42], [94mLoss[0m : 10.23445
[1mStep[0m  [16/42], [94mLoss[0m : 10.39645
[1mStep[0m  [20/42], [94mLoss[0m : 10.64370
[1mStep[0m  [24/42], [94mLoss[0m : 10.90696
[1mStep[0m  [28/42], [94mLoss[0m : 10.47787
[1mStep[0m  [32/42], [94mLoss[0m : 10.57481
[1mStep[0m  [36/42], [94mLoss[0m : 10.57150
[1mStep[0m  [40/42], [94mLoss[0m : 10.58786

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.684, [92mTest[0m: 10.585, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.05484
[1mStep[0m  [4/42], [94mLoss[0m : 10.50696
[1mStep[0m  [8/42], [94mLoss[0m : 11.19440
[1mStep[0m  [12/42], [94mLoss[0m : 10.81574
[1mStep[0m  [16/42], [94mLoss[0m : 10.61337
[1mStep[0m  [20/42], [94mLoss[0m : 10.54786
[1mStep[0m  [24/42], [94mLoss[0m : 10.43473
[1mStep[0m  [28/42], [94mLoss[0m : 10.60684
[1mStep[0m  [32/42], [94mLoss[0m : 11.08573
[1mStep[0m  [36/42], [94mLoss[0m : 11.12082
[1mStep[0m  [40/42], [94mLoss[0m : 10.82210

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.674, [92mTest[0m: 10.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.67024
[1mStep[0m  [4/42], [94mLoss[0m : 10.38850
[1mStep[0m  [8/42], [94mLoss[0m : 10.67858
[1mStep[0m  [12/42], [94mLoss[0m : 10.59480
[1mStep[0m  [16/42], [94mLoss[0m : 10.57345
[1mStep[0m  [20/42], [94mLoss[0m : 10.68223
[1mStep[0m  [24/42], [94mLoss[0m : 10.62391
[1mStep[0m  [28/42], [94mLoss[0m : 10.56908
[1mStep[0m  [32/42], [94mLoss[0m : 10.75543
[1mStep[0m  [36/42], [94mLoss[0m : 10.36164
[1mStep[0m  [40/42], [94mLoss[0m : 10.52295

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.672, [92mTest[0m: 10.566, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.551
====================================

Phase 1 - Evaluation MAE:  10.55103063583374
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.67169
[1mStep[0m  [4/42], [94mLoss[0m : 10.78387
[1mStep[0m  [8/42], [94mLoss[0m : 10.51139
[1mStep[0m  [12/42], [94mLoss[0m : 10.46146
[1mStep[0m  [16/42], [94mLoss[0m : 10.36124
[1mStep[0m  [20/42], [94mLoss[0m : 10.37919
[1mStep[0m  [24/42], [94mLoss[0m : 10.61598
[1mStep[0m  [28/42], [94mLoss[0m : 10.40846
[1mStep[0m  [32/42], [94mLoss[0m : 10.76405
[1mStep[0m  [36/42], [94mLoss[0m : 10.32141
[1mStep[0m  [40/42], [94mLoss[0m : 10.65012

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.664, [92mTest[0m: 10.554, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36341
[1mStep[0m  [4/42], [94mLoss[0m : 10.61286
[1mStep[0m  [8/42], [94mLoss[0m : 10.17721
[1mStep[0m  [12/42], [94mLoss[0m : 10.56511
[1mStep[0m  [16/42], [94mLoss[0m : 10.53091
[1mStep[0m  [20/42], [94mLoss[0m : 10.74704
[1mStep[0m  [24/42], [94mLoss[0m : 10.32933
[1mStep[0m  [28/42], [94mLoss[0m : 10.84183
[1mStep[0m  [32/42], [94mLoss[0m : 10.52687
[1mStep[0m  [36/42], [94mLoss[0m : 10.55876
[1mStep[0m  [40/42], [94mLoss[0m : 10.94393

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.656, [92mTest[0m: 10.543, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42557
[1mStep[0m  [4/42], [94mLoss[0m : 11.07545
[1mStep[0m  [8/42], [94mLoss[0m : 10.25146
[1mStep[0m  [12/42], [94mLoss[0m : 10.77527
[1mStep[0m  [16/42], [94mLoss[0m : 10.47996
[1mStep[0m  [20/42], [94mLoss[0m : 10.58104
[1mStep[0m  [24/42], [94mLoss[0m : 10.59127
[1mStep[0m  [28/42], [94mLoss[0m : 10.39625
[1mStep[0m  [32/42], [94mLoss[0m : 10.29954
[1mStep[0m  [36/42], [94mLoss[0m : 11.03351
[1mStep[0m  [40/42], [94mLoss[0m : 10.28074

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.638, [92mTest[0m: 10.533, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.69282
[1mStep[0m  [4/42], [94mLoss[0m : 10.73623
[1mStep[0m  [8/42], [94mLoss[0m : 10.42695
[1mStep[0m  [12/42], [94mLoss[0m : 10.79336
[1mStep[0m  [16/42], [94mLoss[0m : 10.43546
[1mStep[0m  [20/42], [94mLoss[0m : 10.45008
[1mStep[0m  [24/42], [94mLoss[0m : 10.39310
[1mStep[0m  [28/42], [94mLoss[0m : 10.73402
[1mStep[0m  [32/42], [94mLoss[0m : 10.74221
[1mStep[0m  [36/42], [94mLoss[0m : 10.64696
[1mStep[0m  [40/42], [94mLoss[0m : 10.48261

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.501, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53645
[1mStep[0m  [4/42], [94mLoss[0m : 10.65483
[1mStep[0m  [8/42], [94mLoss[0m : 10.70843
[1mStep[0m  [12/42], [94mLoss[0m : 10.31495
[1mStep[0m  [16/42], [94mLoss[0m : 10.89660
[1mStep[0m  [20/42], [94mLoss[0m : 11.03880
[1mStep[0m  [24/42], [94mLoss[0m : 11.03059
[1mStep[0m  [28/42], [94mLoss[0m : 10.61895
[1mStep[0m  [32/42], [94mLoss[0m : 10.39930
[1mStep[0m  [36/42], [94mLoss[0m : 10.60074
[1mStep[0m  [40/42], [94mLoss[0m : 10.91899

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.620, [92mTest[0m: 10.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.60346
[1mStep[0m  [4/42], [94mLoss[0m : 10.43086
[1mStep[0m  [8/42], [94mLoss[0m : 10.67119
[1mStep[0m  [12/42], [94mLoss[0m : 10.60637
[1mStep[0m  [16/42], [94mLoss[0m : 10.60252
[1mStep[0m  [20/42], [94mLoss[0m : 10.54004
[1mStep[0m  [24/42], [94mLoss[0m : 10.51592
[1mStep[0m  [28/42], [94mLoss[0m : 10.98554
[1mStep[0m  [32/42], [94mLoss[0m : 10.47999
[1mStep[0m  [36/42], [94mLoss[0m : 10.89244
[1mStep[0m  [40/42], [94mLoss[0m : 10.48759

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.61826
[1mStep[0m  [4/42], [94mLoss[0m : 10.76237
[1mStep[0m  [8/42], [94mLoss[0m : 10.38324
[1mStep[0m  [12/42], [94mLoss[0m : 10.52896
[1mStep[0m  [16/42], [94mLoss[0m : 10.54657
[1mStep[0m  [20/42], [94mLoss[0m : 10.67563
[1mStep[0m  [24/42], [94mLoss[0m : 10.90547
[1mStep[0m  [28/42], [94mLoss[0m : 10.61711
[1mStep[0m  [32/42], [94mLoss[0m : 10.21358
[1mStep[0m  [36/42], [94mLoss[0m : 10.46737
[1mStep[0m  [40/42], [94mLoss[0m : 10.45376

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.595, [92mTest[0m: 10.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72571
[1mStep[0m  [4/42], [94mLoss[0m : 10.23549
[1mStep[0m  [8/42], [94mLoss[0m : 10.55261
[1mStep[0m  [12/42], [94mLoss[0m : 10.56063
[1mStep[0m  [16/42], [94mLoss[0m : 10.67134
[1mStep[0m  [20/42], [94mLoss[0m : 10.59073
[1mStep[0m  [24/42], [94mLoss[0m : 10.55791
[1mStep[0m  [28/42], [94mLoss[0m : 10.81516
[1mStep[0m  [32/42], [94mLoss[0m : 10.89930
[1mStep[0m  [36/42], [94mLoss[0m : 10.67141
[1mStep[0m  [40/42], [94mLoss[0m : 10.42903

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.578, [92mTest[0m: 10.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53540
[1mStep[0m  [4/42], [94mLoss[0m : 10.88851
[1mStep[0m  [8/42], [94mLoss[0m : 10.85409
[1mStep[0m  [12/42], [94mLoss[0m : 10.27610
[1mStep[0m  [16/42], [94mLoss[0m : 10.55863
[1mStep[0m  [20/42], [94mLoss[0m : 10.38450
[1mStep[0m  [24/42], [94mLoss[0m : 10.29898
[1mStep[0m  [28/42], [94mLoss[0m : 10.59332
[1mStep[0m  [32/42], [94mLoss[0m : 10.55607
[1mStep[0m  [36/42], [94mLoss[0m : 10.41835
[1mStep[0m  [40/42], [94mLoss[0m : 10.61581

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.561, [92mTest[0m: 10.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32948
[1mStep[0m  [4/42], [94mLoss[0m : 10.28448
[1mStep[0m  [8/42], [94mLoss[0m : 10.74677
[1mStep[0m  [12/42], [94mLoss[0m : 10.57002
[1mStep[0m  [16/42], [94mLoss[0m : 10.27009
[1mStep[0m  [20/42], [94mLoss[0m : 10.16340
[1mStep[0m  [24/42], [94mLoss[0m : 10.33082
[1mStep[0m  [28/42], [94mLoss[0m : 10.68523
[1mStep[0m  [32/42], [94mLoss[0m : 10.59195
[1mStep[0m  [36/42], [94mLoss[0m : 10.61486
[1mStep[0m  [40/42], [94mLoss[0m : 10.14453

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.557, [92mTest[0m: 10.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.58879
[1mStep[0m  [4/42], [94mLoss[0m : 10.36447
[1mStep[0m  [8/42], [94mLoss[0m : 10.57848
[1mStep[0m  [12/42], [94mLoss[0m : 10.41941
[1mStep[0m  [16/42], [94mLoss[0m : 10.46214
[1mStep[0m  [20/42], [94mLoss[0m : 10.32670
[1mStep[0m  [24/42], [94mLoss[0m : 10.38933
[1mStep[0m  [28/42], [94mLoss[0m : 10.72557
[1mStep[0m  [32/42], [94mLoss[0m : 10.33887
[1mStep[0m  [36/42], [94mLoss[0m : 10.59208
[1mStep[0m  [40/42], [94mLoss[0m : 10.73798

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.542, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23008
[1mStep[0m  [4/42], [94mLoss[0m : 10.51946
[1mStep[0m  [8/42], [94mLoss[0m : 10.28447
[1mStep[0m  [12/42], [94mLoss[0m : 10.41266
[1mStep[0m  [16/42], [94mLoss[0m : 10.50855
[1mStep[0m  [20/42], [94mLoss[0m : 10.93315
[1mStep[0m  [24/42], [94mLoss[0m : 10.24876
[1mStep[0m  [28/42], [94mLoss[0m : 10.80182
[1mStep[0m  [32/42], [94mLoss[0m : 10.64515
[1mStep[0m  [36/42], [94mLoss[0m : 10.54438
[1mStep[0m  [40/42], [94mLoss[0m : 10.50639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.536, [92mTest[0m: 10.398, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46193
[1mStep[0m  [4/42], [94mLoss[0m : 10.37159
[1mStep[0m  [8/42], [94mLoss[0m : 10.67695
[1mStep[0m  [12/42], [94mLoss[0m : 10.30134
[1mStep[0m  [16/42], [94mLoss[0m : 10.41175
[1mStep[0m  [20/42], [94mLoss[0m : 10.34120
[1mStep[0m  [24/42], [94mLoss[0m : 10.44190
[1mStep[0m  [28/42], [94mLoss[0m : 10.27051
[1mStep[0m  [32/42], [94mLoss[0m : 10.50588
[1mStep[0m  [36/42], [94mLoss[0m : 10.30075
[1mStep[0m  [40/42], [94mLoss[0m : 10.13384

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.388, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36350
[1mStep[0m  [4/42], [94mLoss[0m : 10.62087
[1mStep[0m  [8/42], [94mLoss[0m : 10.82612
[1mStep[0m  [12/42], [94mLoss[0m : 10.37389
[1mStep[0m  [16/42], [94mLoss[0m : 10.27040
[1mStep[0m  [20/42], [94mLoss[0m : 10.64999
[1mStep[0m  [24/42], [94mLoss[0m : 10.56230
[1mStep[0m  [28/42], [94mLoss[0m : 10.52598
[1mStep[0m  [32/42], [94mLoss[0m : 10.40293
[1mStep[0m  [36/42], [94mLoss[0m : 10.39368
[1mStep[0m  [40/42], [94mLoss[0m : 10.70567

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.497, [92mTest[0m: 10.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26057
[1mStep[0m  [4/42], [94mLoss[0m : 10.55352
[1mStep[0m  [8/42], [94mLoss[0m : 10.27639
[1mStep[0m  [12/42], [94mLoss[0m : 10.68398
[1mStep[0m  [16/42], [94mLoss[0m : 10.53403
[1mStep[0m  [20/42], [94mLoss[0m : 10.68661
[1mStep[0m  [24/42], [94mLoss[0m : 10.50122
[1mStep[0m  [28/42], [94mLoss[0m : 10.50782
[1mStep[0m  [32/42], [94mLoss[0m : 10.55922
[1mStep[0m  [36/42], [94mLoss[0m : 10.62578
[1mStep[0m  [40/42], [94mLoss[0m : 10.08010

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.482, [92mTest[0m: 10.371, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.64451
[1mStep[0m  [4/42], [94mLoss[0m : 10.48927
[1mStep[0m  [8/42], [94mLoss[0m : 10.41912
[1mStep[0m  [12/42], [94mLoss[0m : 10.46181
[1mStep[0m  [16/42], [94mLoss[0m : 10.49047
[1mStep[0m  [20/42], [94mLoss[0m : 10.33095
[1mStep[0m  [24/42], [94mLoss[0m : 10.85958
[1mStep[0m  [28/42], [94mLoss[0m : 10.51812
[1mStep[0m  [32/42], [94mLoss[0m : 10.67965
[1mStep[0m  [36/42], [94mLoss[0m : 10.27052
[1mStep[0m  [40/42], [94mLoss[0m : 10.49875

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.470, [92mTest[0m: 10.361, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.89598
[1mStep[0m  [4/42], [94mLoss[0m : 10.44367
[1mStep[0m  [8/42], [94mLoss[0m : 10.60505
[1mStep[0m  [12/42], [94mLoss[0m : 10.37758
[1mStep[0m  [16/42], [94mLoss[0m : 10.50598
[1mStep[0m  [20/42], [94mLoss[0m : 10.20710
[1mStep[0m  [24/42], [94mLoss[0m : 10.38902
[1mStep[0m  [28/42], [94mLoss[0m : 10.69140
[1mStep[0m  [32/42], [94mLoss[0m : 10.78268
[1mStep[0m  [36/42], [94mLoss[0m : 10.31072
[1mStep[0m  [40/42], [94mLoss[0m : 10.17398

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.460, [92mTest[0m: 10.340, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.24460
[1mStep[0m  [4/42], [94mLoss[0m : 10.61376
[1mStep[0m  [8/42], [94mLoss[0m : 10.48022
[1mStep[0m  [12/42], [94mLoss[0m : 10.02723
[1mStep[0m  [16/42], [94mLoss[0m : 10.77702
[1mStep[0m  [20/42], [94mLoss[0m : 9.97850
[1mStep[0m  [24/42], [94mLoss[0m : 10.20188
[1mStep[0m  [28/42], [94mLoss[0m : 10.34150
[1mStep[0m  [32/42], [94mLoss[0m : 10.72417
[1mStep[0m  [36/42], [94mLoss[0m : 10.25457
[1mStep[0m  [40/42], [94mLoss[0m : 9.98488

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.443, [92mTest[0m: 10.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57015
[1mStep[0m  [4/42], [94mLoss[0m : 10.27035
[1mStep[0m  [8/42], [94mLoss[0m : 10.42882
[1mStep[0m  [12/42], [94mLoss[0m : 10.65877
[1mStep[0m  [16/42], [94mLoss[0m : 10.03500
[1mStep[0m  [20/42], [94mLoss[0m : 10.58255
[1mStep[0m  [24/42], [94mLoss[0m : 10.15423
[1mStep[0m  [28/42], [94mLoss[0m : 10.23915
[1mStep[0m  [32/42], [94mLoss[0m : 10.34532
[1mStep[0m  [36/42], [94mLoss[0m : 10.48684
[1mStep[0m  [40/42], [94mLoss[0m : 10.73666

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.426, [92mTest[0m: 10.288, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.79449
[1mStep[0m  [4/42], [94mLoss[0m : 10.66517
[1mStep[0m  [8/42], [94mLoss[0m : 10.57917
[1mStep[0m  [12/42], [94mLoss[0m : 10.61826
[1mStep[0m  [16/42], [94mLoss[0m : 10.67463
[1mStep[0m  [20/42], [94mLoss[0m : 10.50487
[1mStep[0m  [24/42], [94mLoss[0m : 10.19836
[1mStep[0m  [28/42], [94mLoss[0m : 10.11141
[1mStep[0m  [32/42], [94mLoss[0m : 10.61264
[1mStep[0m  [36/42], [94mLoss[0m : 10.21922
[1mStep[0m  [40/42], [94mLoss[0m : 10.45100

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.410, [92mTest[0m: 10.297, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.70908
[1mStep[0m  [4/42], [94mLoss[0m : 11.11978
[1mStep[0m  [8/42], [94mLoss[0m : 10.71813
[1mStep[0m  [12/42], [94mLoss[0m : 10.35740
[1mStep[0m  [16/42], [94mLoss[0m : 10.24630
[1mStep[0m  [20/42], [94mLoss[0m : 10.25779
[1mStep[0m  [24/42], [94mLoss[0m : 10.44284
[1mStep[0m  [28/42], [94mLoss[0m : 10.55289
[1mStep[0m  [32/42], [94mLoss[0m : 10.15582
[1mStep[0m  [36/42], [94mLoss[0m : 10.40741
[1mStep[0m  [40/42], [94mLoss[0m : 10.50510

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.260, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51223
[1mStep[0m  [4/42], [94mLoss[0m : 10.12063
[1mStep[0m  [8/42], [94mLoss[0m : 10.60136
[1mStep[0m  [12/42], [94mLoss[0m : 10.26774
[1mStep[0m  [16/42], [94mLoss[0m : 10.13853
[1mStep[0m  [20/42], [94mLoss[0m : 10.38877
[1mStep[0m  [24/42], [94mLoss[0m : 10.17423
[1mStep[0m  [28/42], [94mLoss[0m : 10.28179
[1mStep[0m  [32/42], [94mLoss[0m : 10.71244
[1mStep[0m  [36/42], [94mLoss[0m : 10.39647
[1mStep[0m  [40/42], [94mLoss[0m : 10.36506

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.382, [92mTest[0m: 10.272, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68544
[1mStep[0m  [4/42], [94mLoss[0m : 10.16725
[1mStep[0m  [8/42], [94mLoss[0m : 10.31884
[1mStep[0m  [12/42], [94mLoss[0m : 10.37614
[1mStep[0m  [16/42], [94mLoss[0m : 10.76433
[1mStep[0m  [20/42], [94mLoss[0m : 10.49255
[1mStep[0m  [24/42], [94mLoss[0m : 10.46292
[1mStep[0m  [28/42], [94mLoss[0m : 10.53828
[1mStep[0m  [32/42], [94mLoss[0m : 10.20187
[1mStep[0m  [36/42], [94mLoss[0m : 10.38574
[1mStep[0m  [40/42], [94mLoss[0m : 10.38345

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.368, [92mTest[0m: 10.231, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.62543
[1mStep[0m  [4/42], [94mLoss[0m : 10.64053
[1mStep[0m  [8/42], [94mLoss[0m : 10.01714
[1mStep[0m  [12/42], [94mLoss[0m : 10.22435
[1mStep[0m  [16/42], [94mLoss[0m : 10.12448
[1mStep[0m  [20/42], [94mLoss[0m : 10.29277
[1mStep[0m  [24/42], [94mLoss[0m : 10.37702
[1mStep[0m  [28/42], [94mLoss[0m : 10.29115
[1mStep[0m  [32/42], [94mLoss[0m : 10.41033
[1mStep[0m  [36/42], [94mLoss[0m : 10.48519
[1mStep[0m  [40/42], [94mLoss[0m : 10.59065

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.346, [92mTest[0m: 10.212, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63689
[1mStep[0m  [4/42], [94mLoss[0m : 9.96818
[1mStep[0m  [8/42], [94mLoss[0m : 10.22194
[1mStep[0m  [12/42], [94mLoss[0m : 10.30500
[1mStep[0m  [16/42], [94mLoss[0m : 10.34773
[1mStep[0m  [20/42], [94mLoss[0m : 10.94298
[1mStep[0m  [24/42], [94mLoss[0m : 10.13616
[1mStep[0m  [28/42], [94mLoss[0m : 10.32853
[1mStep[0m  [32/42], [94mLoss[0m : 10.34101
[1mStep[0m  [36/42], [94mLoss[0m : 9.96710
[1mStep[0m  [40/42], [94mLoss[0m : 10.42264

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.340, [92mTest[0m: 10.233, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.47504
[1mStep[0m  [4/42], [94mLoss[0m : 10.21550
[1mStep[0m  [8/42], [94mLoss[0m : 10.15707
[1mStep[0m  [12/42], [94mLoss[0m : 10.26975
[1mStep[0m  [16/42], [94mLoss[0m : 10.41798
[1mStep[0m  [20/42], [94mLoss[0m : 10.20389
[1mStep[0m  [24/42], [94mLoss[0m : 10.12100
[1mStep[0m  [28/42], [94mLoss[0m : 10.58753
[1mStep[0m  [32/42], [94mLoss[0m : 10.26712
[1mStep[0m  [36/42], [94mLoss[0m : 10.06223
[1mStep[0m  [40/42], [94mLoss[0m : 10.53684

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.325, [92mTest[0m: 10.218, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65139
[1mStep[0m  [4/42], [94mLoss[0m : 10.76105
[1mStep[0m  [8/42], [94mLoss[0m : 10.14128
[1mStep[0m  [12/42], [94mLoss[0m : 10.38742
[1mStep[0m  [16/42], [94mLoss[0m : 10.31944
[1mStep[0m  [20/42], [94mLoss[0m : 10.56918
[1mStep[0m  [24/42], [94mLoss[0m : 10.00078
[1mStep[0m  [28/42], [94mLoss[0m : 10.39537
[1mStep[0m  [32/42], [94mLoss[0m : 10.40629
[1mStep[0m  [36/42], [94mLoss[0m : 10.31640
[1mStep[0m  [40/42], [94mLoss[0m : 10.27395

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.316, [92mTest[0m: 10.160, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15471
[1mStep[0m  [4/42], [94mLoss[0m : 10.12125
[1mStep[0m  [8/42], [94mLoss[0m : 10.36208
[1mStep[0m  [12/42], [94mLoss[0m : 10.14924
[1mStep[0m  [16/42], [94mLoss[0m : 10.04927
[1mStep[0m  [20/42], [94mLoss[0m : 10.01548
[1mStep[0m  [24/42], [94mLoss[0m : 10.41338
[1mStep[0m  [28/42], [94mLoss[0m : 10.17721
[1mStep[0m  [32/42], [94mLoss[0m : 10.01675
[1mStep[0m  [36/42], [94mLoss[0m : 10.25153
[1mStep[0m  [40/42], [94mLoss[0m : 10.47995

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.304, [92mTest[0m: 10.172, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.98189
[1mStep[0m  [4/42], [94mLoss[0m : 10.41049
[1mStep[0m  [8/42], [94mLoss[0m : 10.37721
[1mStep[0m  [12/42], [94mLoss[0m : 10.48646
[1mStep[0m  [16/42], [94mLoss[0m : 10.26812
[1mStep[0m  [20/42], [94mLoss[0m : 10.52621
[1mStep[0m  [24/42], [94mLoss[0m : 10.39232
[1mStep[0m  [28/42], [94mLoss[0m : 10.20516
[1mStep[0m  [32/42], [94mLoss[0m : 10.21775
[1mStep[0m  [36/42], [94mLoss[0m : 10.58236
[1mStep[0m  [40/42], [94mLoss[0m : 10.52077

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.281, [92mTest[0m: 10.188, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32043
[1mStep[0m  [4/42], [94mLoss[0m : 10.39856
[1mStep[0m  [8/42], [94mLoss[0m : 10.55984
[1mStep[0m  [12/42], [94mLoss[0m : 10.49690
[1mStep[0m  [16/42], [94mLoss[0m : 10.10979
[1mStep[0m  [20/42], [94mLoss[0m : 10.13434
[1mStep[0m  [24/42], [94mLoss[0m : 10.47996
[1mStep[0m  [28/42], [94mLoss[0m : 10.27427
[1mStep[0m  [32/42], [94mLoss[0m : 10.09857
[1mStep[0m  [36/42], [94mLoss[0m : 10.29100
[1mStep[0m  [40/42], [94mLoss[0m : 10.18056

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.279, [92mTest[0m: 10.168, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.112
====================================

Phase 2 - Evaluation MAE:  10.112481662205287
MAE score P1      10.551031
MAE score P2      10.112482
loss               10.27871
learning_rate        0.0001
batch_size              256
hidden_sizes          [250]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.97274
[1mStep[0m  [2/21], [94mLoss[0m : 10.88113
[1mStep[0m  [4/21], [94mLoss[0m : 10.74964
[1mStep[0m  [6/21], [94mLoss[0m : 11.01050
[1mStep[0m  [8/21], [94mLoss[0m : 10.83226
[1mStep[0m  [10/21], [94mLoss[0m : 10.80913
[1mStep[0m  [12/21], [94mLoss[0m : 11.09075
[1mStep[0m  [14/21], [94mLoss[0m : 11.02156
[1mStep[0m  [16/21], [94mLoss[0m : 10.70485
[1mStep[0m  [18/21], [94mLoss[0m : 10.93973
[1mStep[0m  [20/21], [94mLoss[0m : 10.93730

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.878, [92mTest[0m: 10.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99590
[1mStep[0m  [2/21], [94mLoss[0m : 11.07378
[1mStep[0m  [4/21], [94mLoss[0m : 10.77475
[1mStep[0m  [6/21], [94mLoss[0m : 11.11478
[1mStep[0m  [8/21], [94mLoss[0m : 10.99633
[1mStep[0m  [10/21], [94mLoss[0m : 11.30933
[1mStep[0m  [12/21], [94mLoss[0m : 10.79878
[1mStep[0m  [14/21], [94mLoss[0m : 11.01757
[1mStep[0m  [16/21], [94mLoss[0m : 10.62740
[1mStep[0m  [18/21], [94mLoss[0m : 10.79787
[1mStep[0m  [20/21], [94mLoss[0m : 10.69762

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.865, [92mTest[0m: 10.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74831
[1mStep[0m  [2/21], [94mLoss[0m : 10.88974
[1mStep[0m  [4/21], [94mLoss[0m : 10.68114
[1mStep[0m  [6/21], [94mLoss[0m : 10.85163
[1mStep[0m  [8/21], [94mLoss[0m : 11.04019
[1mStep[0m  [10/21], [94mLoss[0m : 11.21480
[1mStep[0m  [12/21], [94mLoss[0m : 10.87417
[1mStep[0m  [14/21], [94mLoss[0m : 11.06043
[1mStep[0m  [16/21], [94mLoss[0m : 10.72441
[1mStep[0m  [18/21], [94mLoss[0m : 10.98875
[1mStep[0m  [20/21], [94mLoss[0m : 10.41391

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.866, [92mTest[0m: 10.854, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.83000
[1mStep[0m  [2/21], [94mLoss[0m : 10.84226
[1mStep[0m  [4/21], [94mLoss[0m : 10.83471
[1mStep[0m  [6/21], [94mLoss[0m : 10.66128
[1mStep[0m  [8/21], [94mLoss[0m : 11.13410
[1mStep[0m  [10/21], [94mLoss[0m : 10.55437
[1mStep[0m  [12/21], [94mLoss[0m : 11.00154
[1mStep[0m  [14/21], [94mLoss[0m : 10.82792
[1mStep[0m  [16/21], [94mLoss[0m : 10.82428
[1mStep[0m  [18/21], [94mLoss[0m : 10.87931
[1mStep[0m  [20/21], [94mLoss[0m : 10.58682

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.852, [92mTest[0m: 10.861, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70198
[1mStep[0m  [2/21], [94mLoss[0m : 10.70302
[1mStep[0m  [4/21], [94mLoss[0m : 10.75599
[1mStep[0m  [6/21], [94mLoss[0m : 10.91622
[1mStep[0m  [8/21], [94mLoss[0m : 10.91733
[1mStep[0m  [10/21], [94mLoss[0m : 10.98491
[1mStep[0m  [12/21], [94mLoss[0m : 10.98895
[1mStep[0m  [14/21], [94mLoss[0m : 10.69456
[1mStep[0m  [16/21], [94mLoss[0m : 10.83839
[1mStep[0m  [18/21], [94mLoss[0m : 10.84143
[1mStep[0m  [20/21], [94mLoss[0m : 10.72627

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.853, [92mTest[0m: 10.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.10728
[1mStep[0m  [2/21], [94mLoss[0m : 10.99936
[1mStep[0m  [4/21], [94mLoss[0m : 10.72207
[1mStep[0m  [6/21], [94mLoss[0m : 10.98158
[1mStep[0m  [8/21], [94mLoss[0m : 10.54177
[1mStep[0m  [10/21], [94mLoss[0m : 10.88531
[1mStep[0m  [12/21], [94mLoss[0m : 10.81360
[1mStep[0m  [14/21], [94mLoss[0m : 10.91006
[1mStep[0m  [16/21], [94mLoss[0m : 10.85317
[1mStep[0m  [18/21], [94mLoss[0m : 10.65299
[1mStep[0m  [20/21], [94mLoss[0m : 10.98329

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.844, [92mTest[0m: 10.822, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72010
[1mStep[0m  [2/21], [94mLoss[0m : 10.79080
[1mStep[0m  [4/21], [94mLoss[0m : 10.94122
[1mStep[0m  [6/21], [94mLoss[0m : 10.69622
[1mStep[0m  [8/21], [94mLoss[0m : 10.78810
[1mStep[0m  [10/21], [94mLoss[0m : 10.98349
[1mStep[0m  [12/21], [94mLoss[0m : 10.98677
[1mStep[0m  [14/21], [94mLoss[0m : 10.77587
[1mStep[0m  [16/21], [94mLoss[0m : 11.15075
[1mStep[0m  [18/21], [94mLoss[0m : 10.65161
[1mStep[0m  [20/21], [94mLoss[0m : 10.80118

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.846, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77000
[1mStep[0m  [2/21], [94mLoss[0m : 11.09324
[1mStep[0m  [4/21], [94mLoss[0m : 11.09779
[1mStep[0m  [6/21], [94mLoss[0m : 10.98582
[1mStep[0m  [8/21], [94mLoss[0m : 10.77237
[1mStep[0m  [10/21], [94mLoss[0m : 10.82796
[1mStep[0m  [12/21], [94mLoss[0m : 10.76569
[1mStep[0m  [14/21], [94mLoss[0m : 10.76996
[1mStep[0m  [16/21], [94mLoss[0m : 10.80751
[1mStep[0m  [18/21], [94mLoss[0m : 10.70316
[1mStep[0m  [20/21], [94mLoss[0m : 11.26837

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.14415
[1mStep[0m  [2/21], [94mLoss[0m : 10.77735
[1mStep[0m  [4/21], [94mLoss[0m : 10.84417
[1mStep[0m  [6/21], [94mLoss[0m : 10.66930
[1mStep[0m  [8/21], [94mLoss[0m : 10.88417
[1mStep[0m  [10/21], [94mLoss[0m : 10.83279
[1mStep[0m  [12/21], [94mLoss[0m : 10.59071
[1mStep[0m  [14/21], [94mLoss[0m : 10.79601
[1mStep[0m  [16/21], [94mLoss[0m : 10.85921
[1mStep[0m  [18/21], [94mLoss[0m : 11.11592
[1mStep[0m  [20/21], [94mLoss[0m : 10.61709

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82457
[1mStep[0m  [2/21], [94mLoss[0m : 10.83295
[1mStep[0m  [4/21], [94mLoss[0m : 10.57803
[1mStep[0m  [6/21], [94mLoss[0m : 10.68434
[1mStep[0m  [8/21], [94mLoss[0m : 10.84425
[1mStep[0m  [10/21], [94mLoss[0m : 10.95699
[1mStep[0m  [12/21], [94mLoss[0m : 10.74031
[1mStep[0m  [14/21], [94mLoss[0m : 10.69013
[1mStep[0m  [16/21], [94mLoss[0m : 10.62759
[1mStep[0m  [18/21], [94mLoss[0m : 11.08183
[1mStep[0m  [20/21], [94mLoss[0m : 10.86856

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.829, [92mTest[0m: 10.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99295
[1mStep[0m  [2/21], [94mLoss[0m : 11.05340
[1mStep[0m  [4/21], [94mLoss[0m : 10.82736
[1mStep[0m  [6/21], [94mLoss[0m : 10.75204
[1mStep[0m  [8/21], [94mLoss[0m : 10.73418
[1mStep[0m  [10/21], [94mLoss[0m : 10.70277
[1mStep[0m  [12/21], [94mLoss[0m : 10.90381
[1mStep[0m  [14/21], [94mLoss[0m : 10.83977
[1mStep[0m  [16/21], [94mLoss[0m : 10.59590
[1mStep[0m  [18/21], [94mLoss[0m : 11.04809
[1mStep[0m  [20/21], [94mLoss[0m : 11.04772

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74655
[1mStep[0m  [2/21], [94mLoss[0m : 11.16893
[1mStep[0m  [4/21], [94mLoss[0m : 10.88108
[1mStep[0m  [6/21], [94mLoss[0m : 10.67094
[1mStep[0m  [8/21], [94mLoss[0m : 10.70607
[1mStep[0m  [10/21], [94mLoss[0m : 10.83278
[1mStep[0m  [12/21], [94mLoss[0m : 10.87657
[1mStep[0m  [14/21], [94mLoss[0m : 10.48971
[1mStep[0m  [16/21], [94mLoss[0m : 10.74745
[1mStep[0m  [18/21], [94mLoss[0m : 10.68479
[1mStep[0m  [20/21], [94mLoss[0m : 10.58746

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.809, [92mTest[0m: 10.796, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.95784
[1mStep[0m  [2/21], [94mLoss[0m : 10.72557
[1mStep[0m  [4/21], [94mLoss[0m : 10.83907
[1mStep[0m  [6/21], [94mLoss[0m : 10.76302
[1mStep[0m  [8/21], [94mLoss[0m : 10.46969
[1mStep[0m  [10/21], [94mLoss[0m : 10.75492
[1mStep[0m  [12/21], [94mLoss[0m : 10.72535
[1mStep[0m  [14/21], [94mLoss[0m : 10.66922
[1mStep[0m  [16/21], [94mLoss[0m : 10.97342
[1mStep[0m  [18/21], [94mLoss[0m : 10.86422
[1mStep[0m  [20/21], [94mLoss[0m : 11.04569

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.813, [92mTest[0m: 10.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88750
[1mStep[0m  [2/21], [94mLoss[0m : 10.92489
[1mStep[0m  [4/21], [94mLoss[0m : 10.78721
[1mStep[0m  [6/21], [94mLoss[0m : 10.84597
[1mStep[0m  [8/21], [94mLoss[0m : 10.94038
[1mStep[0m  [10/21], [94mLoss[0m : 10.84404
[1mStep[0m  [12/21], [94mLoss[0m : 10.85047
[1mStep[0m  [14/21], [94mLoss[0m : 10.64125
[1mStep[0m  [16/21], [94mLoss[0m : 10.79605
[1mStep[0m  [18/21], [94mLoss[0m : 10.67026
[1mStep[0m  [20/21], [94mLoss[0m : 10.64728

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.791, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62194
[1mStep[0m  [2/21], [94mLoss[0m : 10.86858
[1mStep[0m  [4/21], [94mLoss[0m : 10.45924
[1mStep[0m  [6/21], [94mLoss[0m : 10.42319
[1mStep[0m  [8/21], [94mLoss[0m : 10.69595
[1mStep[0m  [10/21], [94mLoss[0m : 11.02474
[1mStep[0m  [12/21], [94mLoss[0m : 10.85176
[1mStep[0m  [14/21], [94mLoss[0m : 10.72346
[1mStep[0m  [16/21], [94mLoss[0m : 10.76348
[1mStep[0m  [18/21], [94mLoss[0m : 10.88430
[1mStep[0m  [20/21], [94mLoss[0m : 10.60996

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.804, [92mTest[0m: 10.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88978
[1mStep[0m  [2/21], [94mLoss[0m : 10.99958
[1mStep[0m  [4/21], [94mLoss[0m : 10.62653
[1mStep[0m  [6/21], [94mLoss[0m : 10.74958
[1mStep[0m  [8/21], [94mLoss[0m : 11.03302
[1mStep[0m  [10/21], [94mLoss[0m : 10.91824
[1mStep[0m  [12/21], [94mLoss[0m : 10.82502
[1mStep[0m  [14/21], [94mLoss[0m : 10.76007
[1mStep[0m  [16/21], [94mLoss[0m : 10.91199
[1mStep[0m  [18/21], [94mLoss[0m : 10.69528
[1mStep[0m  [20/21], [94mLoss[0m : 10.91981

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.798, [92mTest[0m: 10.770, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.14226
[1mStep[0m  [2/21], [94mLoss[0m : 10.49918
[1mStep[0m  [4/21], [94mLoss[0m : 11.11569
[1mStep[0m  [6/21], [94mLoss[0m : 10.90954
[1mStep[0m  [8/21], [94mLoss[0m : 10.74473
[1mStep[0m  [10/21], [94mLoss[0m : 10.99401
[1mStep[0m  [12/21], [94mLoss[0m : 10.94290
[1mStep[0m  [14/21], [94mLoss[0m : 10.62062
[1mStep[0m  [16/21], [94mLoss[0m : 10.86662
[1mStep[0m  [18/21], [94mLoss[0m : 10.60919
[1mStep[0m  [20/21], [94mLoss[0m : 10.88444

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.790, [92mTest[0m: 10.761, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68396
[1mStep[0m  [2/21], [94mLoss[0m : 10.88458
[1mStep[0m  [4/21], [94mLoss[0m : 10.92850
[1mStep[0m  [6/21], [94mLoss[0m : 10.79274
[1mStep[0m  [8/21], [94mLoss[0m : 10.68675
[1mStep[0m  [10/21], [94mLoss[0m : 10.83173
[1mStep[0m  [12/21], [94mLoss[0m : 10.80496
[1mStep[0m  [14/21], [94mLoss[0m : 11.08104
[1mStep[0m  [16/21], [94mLoss[0m : 10.73668
[1mStep[0m  [18/21], [94mLoss[0m : 10.54101
[1mStep[0m  [20/21], [94mLoss[0m : 10.73926

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.789, [92mTest[0m: 10.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81469
[1mStep[0m  [2/21], [94mLoss[0m : 10.66331
[1mStep[0m  [4/21], [94mLoss[0m : 10.68940
[1mStep[0m  [6/21], [94mLoss[0m : 11.10332
[1mStep[0m  [8/21], [94mLoss[0m : 10.72316
[1mStep[0m  [10/21], [94mLoss[0m : 10.76016
[1mStep[0m  [12/21], [94mLoss[0m : 10.60785
[1mStep[0m  [14/21], [94mLoss[0m : 10.79904
[1mStep[0m  [16/21], [94mLoss[0m : 10.93945
[1mStep[0m  [18/21], [94mLoss[0m : 11.19710
[1mStep[0m  [20/21], [94mLoss[0m : 10.57405

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.777, [92mTest[0m: 10.764, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60272
[1mStep[0m  [2/21], [94mLoss[0m : 10.66787
[1mStep[0m  [4/21], [94mLoss[0m : 10.67510
[1mStep[0m  [6/21], [94mLoss[0m : 10.80907
[1mStep[0m  [8/21], [94mLoss[0m : 10.69847
[1mStep[0m  [10/21], [94mLoss[0m : 11.08875
[1mStep[0m  [12/21], [94mLoss[0m : 11.18092
[1mStep[0m  [14/21], [94mLoss[0m : 10.57906
[1mStep[0m  [16/21], [94mLoss[0m : 10.70693
[1mStep[0m  [18/21], [94mLoss[0m : 10.95984
[1mStep[0m  [20/21], [94mLoss[0m : 11.02462

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.747, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47818
[1mStep[0m  [2/21], [94mLoss[0m : 10.83256
[1mStep[0m  [4/21], [94mLoss[0m : 10.84228
[1mStep[0m  [6/21], [94mLoss[0m : 10.63134
[1mStep[0m  [8/21], [94mLoss[0m : 10.85241
[1mStep[0m  [10/21], [94mLoss[0m : 10.86176
[1mStep[0m  [12/21], [94mLoss[0m : 10.83864
[1mStep[0m  [14/21], [94mLoss[0m : 10.61283
[1mStep[0m  [16/21], [94mLoss[0m : 10.89151
[1mStep[0m  [18/21], [94mLoss[0m : 11.07480
[1mStep[0m  [20/21], [94mLoss[0m : 10.95278

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.752, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59239
[1mStep[0m  [2/21], [94mLoss[0m : 10.68226
[1mStep[0m  [4/21], [94mLoss[0m : 10.81708
[1mStep[0m  [6/21], [94mLoss[0m : 10.71097
[1mStep[0m  [8/21], [94mLoss[0m : 10.83643
[1mStep[0m  [10/21], [94mLoss[0m : 10.51433
[1mStep[0m  [12/21], [94mLoss[0m : 10.58420
[1mStep[0m  [14/21], [94mLoss[0m : 10.62643
[1mStep[0m  [16/21], [94mLoss[0m : 10.79380
[1mStep[0m  [18/21], [94mLoss[0m : 11.10456
[1mStep[0m  [20/21], [94mLoss[0m : 10.81960

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.769, [92mTest[0m: 10.748, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69480
[1mStep[0m  [2/21], [94mLoss[0m : 10.42045
[1mStep[0m  [4/21], [94mLoss[0m : 10.79322
[1mStep[0m  [6/21], [94mLoss[0m : 10.89722
[1mStep[0m  [8/21], [94mLoss[0m : 11.04687
[1mStep[0m  [10/21], [94mLoss[0m : 10.54037
[1mStep[0m  [12/21], [94mLoss[0m : 10.87222
[1mStep[0m  [14/21], [94mLoss[0m : 11.08016
[1mStep[0m  [16/21], [94mLoss[0m : 10.54079
[1mStep[0m  [18/21], [94mLoss[0m : 10.62245
[1mStep[0m  [20/21], [94mLoss[0m : 11.00585

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.770, [92mTest[0m: 10.732, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70584
[1mStep[0m  [2/21], [94mLoss[0m : 10.77142
[1mStep[0m  [4/21], [94mLoss[0m : 10.97752
[1mStep[0m  [6/21], [94mLoss[0m : 10.63959
[1mStep[0m  [8/21], [94mLoss[0m : 10.86617
[1mStep[0m  [10/21], [94mLoss[0m : 10.77967
[1mStep[0m  [12/21], [94mLoss[0m : 10.68880
[1mStep[0m  [14/21], [94mLoss[0m : 10.78868
[1mStep[0m  [16/21], [94mLoss[0m : 10.67839
[1mStep[0m  [18/21], [94mLoss[0m : 10.32012
[1mStep[0m  [20/21], [94mLoss[0m : 10.44729

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.714, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77361
[1mStep[0m  [2/21], [94mLoss[0m : 10.58418
[1mStep[0m  [4/21], [94mLoss[0m : 10.98382
[1mStep[0m  [6/21], [94mLoss[0m : 10.75637
[1mStep[0m  [8/21], [94mLoss[0m : 10.75233
[1mStep[0m  [10/21], [94mLoss[0m : 10.79197
[1mStep[0m  [12/21], [94mLoss[0m : 10.66937
[1mStep[0m  [14/21], [94mLoss[0m : 10.63094
[1mStep[0m  [16/21], [94mLoss[0m : 10.50897
[1mStep[0m  [18/21], [94mLoss[0m : 10.50279
[1mStep[0m  [20/21], [94mLoss[0m : 10.92007

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.713, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62030
[1mStep[0m  [2/21], [94mLoss[0m : 10.71788
[1mStep[0m  [4/21], [94mLoss[0m : 10.73981
[1mStep[0m  [6/21], [94mLoss[0m : 10.73691
[1mStep[0m  [8/21], [94mLoss[0m : 10.63311
[1mStep[0m  [10/21], [94mLoss[0m : 10.76714
[1mStep[0m  [12/21], [94mLoss[0m : 10.72818
[1mStep[0m  [14/21], [94mLoss[0m : 10.97577
[1mStep[0m  [16/21], [94mLoss[0m : 10.81167
[1mStep[0m  [18/21], [94mLoss[0m : 10.85871
[1mStep[0m  [20/21], [94mLoss[0m : 10.78481

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.756, [92mTest[0m: 10.717, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81124
[1mStep[0m  [2/21], [94mLoss[0m : 10.73811
[1mStep[0m  [4/21], [94mLoss[0m : 10.87908
[1mStep[0m  [6/21], [94mLoss[0m : 10.97276
[1mStep[0m  [8/21], [94mLoss[0m : 10.83374
[1mStep[0m  [10/21], [94mLoss[0m : 10.72976
[1mStep[0m  [12/21], [94mLoss[0m : 10.92404
[1mStep[0m  [14/21], [94mLoss[0m : 10.65051
[1mStep[0m  [16/21], [94mLoss[0m : 10.67105
[1mStep[0m  [18/21], [94mLoss[0m : 10.58202
[1mStep[0m  [20/21], [94mLoss[0m : 10.82082

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.716, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71320
[1mStep[0m  [2/21], [94mLoss[0m : 11.05269
[1mStep[0m  [4/21], [94mLoss[0m : 10.85292
[1mStep[0m  [6/21], [94mLoss[0m : 10.74537
[1mStep[0m  [8/21], [94mLoss[0m : 10.85665
[1mStep[0m  [10/21], [94mLoss[0m : 10.39832
[1mStep[0m  [12/21], [94mLoss[0m : 10.74593
[1mStep[0m  [14/21], [94mLoss[0m : 10.60846
[1mStep[0m  [16/21], [94mLoss[0m : 10.85725
[1mStep[0m  [18/21], [94mLoss[0m : 10.71033
[1mStep[0m  [20/21], [94mLoss[0m : 10.61804

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.733, [92mTest[0m: 10.700, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63210
[1mStep[0m  [2/21], [94mLoss[0m : 10.79954
[1mStep[0m  [4/21], [94mLoss[0m : 10.61071
[1mStep[0m  [6/21], [94mLoss[0m : 10.76204
[1mStep[0m  [8/21], [94mLoss[0m : 10.61598
[1mStep[0m  [10/21], [94mLoss[0m : 10.84784
[1mStep[0m  [12/21], [94mLoss[0m : 10.69270
[1mStep[0m  [14/21], [94mLoss[0m : 10.86425
[1mStep[0m  [16/21], [94mLoss[0m : 10.75255
[1mStep[0m  [18/21], [94mLoss[0m : 10.88775
[1mStep[0m  [20/21], [94mLoss[0m : 10.70501

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.736, [92mTest[0m: 10.686, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82901
[1mStep[0m  [2/21], [94mLoss[0m : 10.96388
[1mStep[0m  [4/21], [94mLoss[0m : 10.68603
[1mStep[0m  [6/21], [94mLoss[0m : 10.71745
[1mStep[0m  [8/21], [94mLoss[0m : 10.73454
[1mStep[0m  [10/21], [94mLoss[0m : 10.70594
[1mStep[0m  [12/21], [94mLoss[0m : 10.48967
[1mStep[0m  [14/21], [94mLoss[0m : 10.44853
[1mStep[0m  [16/21], [94mLoss[0m : 10.75822
[1mStep[0m  [18/21], [94mLoss[0m : 10.92238
[1mStep[0m  [20/21], [94mLoss[0m : 10.78607

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.728, [92mTest[0m: 10.700, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.675
====================================

Phase 1 - Evaluation MAE:  10.675141743251256
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.54160
[1mStep[0m  [2/21], [94mLoss[0m : 11.00776
[1mStep[0m  [4/21], [94mLoss[0m : 10.74431
[1mStep[0m  [6/21], [94mLoss[0m : 10.71904
[1mStep[0m  [8/21], [94mLoss[0m : 10.86288
[1mStep[0m  [10/21], [94mLoss[0m : 10.89097
[1mStep[0m  [12/21], [94mLoss[0m : 10.49796
[1mStep[0m  [14/21], [94mLoss[0m : 10.82873
[1mStep[0m  [16/21], [94mLoss[0m : 10.57245
[1mStep[0m  [18/21], [94mLoss[0m : 10.76244
[1mStep[0m  [20/21], [94mLoss[0m : 10.73077

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.722, [92mTest[0m: 10.690, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65586
[1mStep[0m  [2/21], [94mLoss[0m : 10.59143
[1mStep[0m  [4/21], [94mLoss[0m : 10.62750
[1mStep[0m  [6/21], [94mLoss[0m : 10.71857
[1mStep[0m  [8/21], [94mLoss[0m : 10.52463
[1mStep[0m  [10/21], [94mLoss[0m : 10.62641
[1mStep[0m  [12/21], [94mLoss[0m : 11.00536
[1mStep[0m  [14/21], [94mLoss[0m : 10.82856
[1mStep[0m  [16/21], [94mLoss[0m : 10.79477
[1mStep[0m  [18/21], [94mLoss[0m : 10.68137
[1mStep[0m  [20/21], [94mLoss[0m : 10.76824

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.718, [92mTest[0m: 10.690, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.41569
[1mStep[0m  [2/21], [94mLoss[0m : 10.63443
[1mStep[0m  [4/21], [94mLoss[0m : 10.77760
[1mStep[0m  [6/21], [94mLoss[0m : 10.64114
[1mStep[0m  [8/21], [94mLoss[0m : 10.97195
[1mStep[0m  [10/21], [94mLoss[0m : 10.65229
[1mStep[0m  [12/21], [94mLoss[0m : 10.61144
[1mStep[0m  [14/21], [94mLoss[0m : 11.10056
[1mStep[0m  [16/21], [94mLoss[0m : 10.57015
[1mStep[0m  [18/21], [94mLoss[0m : 10.62700
[1mStep[0m  [20/21], [94mLoss[0m : 10.68847

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.712, [92mTest[0m: 10.678, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58421
[1mStep[0m  [2/21], [94mLoss[0m : 10.75341
[1mStep[0m  [4/21], [94mLoss[0m : 10.58926
[1mStep[0m  [6/21], [94mLoss[0m : 10.79803
[1mStep[0m  [8/21], [94mLoss[0m : 10.69674
[1mStep[0m  [10/21], [94mLoss[0m : 10.80168
[1mStep[0m  [12/21], [94mLoss[0m : 10.70765
[1mStep[0m  [14/21], [94mLoss[0m : 10.85095
[1mStep[0m  [16/21], [94mLoss[0m : 10.82498
[1mStep[0m  [18/21], [94mLoss[0m : 10.83031
[1mStep[0m  [20/21], [94mLoss[0m : 10.54176

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.699, [92mTest[0m: 10.673, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51044
[1mStep[0m  [2/21], [94mLoss[0m : 10.71379
[1mStep[0m  [4/21], [94mLoss[0m : 10.83385
[1mStep[0m  [6/21], [94mLoss[0m : 11.01928
[1mStep[0m  [8/21], [94mLoss[0m : 10.88634
[1mStep[0m  [10/21], [94mLoss[0m : 10.93216
[1mStep[0m  [12/21], [94mLoss[0m : 10.74979
[1mStep[0m  [14/21], [94mLoss[0m : 10.64549
[1mStep[0m  [16/21], [94mLoss[0m : 10.82732
[1mStep[0m  [18/21], [94mLoss[0m : 10.67013
[1mStep[0m  [20/21], [94mLoss[0m : 10.57536

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50694
[1mStep[0m  [2/21], [94mLoss[0m : 10.59331
[1mStep[0m  [4/21], [94mLoss[0m : 10.60526
[1mStep[0m  [6/21], [94mLoss[0m : 10.44475
[1mStep[0m  [8/21], [94mLoss[0m : 10.74913
[1mStep[0m  [10/21], [94mLoss[0m : 10.86604
[1mStep[0m  [12/21], [94mLoss[0m : 10.63548
[1mStep[0m  [14/21], [94mLoss[0m : 10.71393
[1mStep[0m  [16/21], [94mLoss[0m : 10.57153
[1mStep[0m  [18/21], [94mLoss[0m : 10.76623
[1mStep[0m  [20/21], [94mLoss[0m : 10.87947

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.693, [92mTest[0m: 10.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88377
[1mStep[0m  [2/21], [94mLoss[0m : 10.60244
[1mStep[0m  [4/21], [94mLoss[0m : 10.43854
[1mStep[0m  [6/21], [94mLoss[0m : 10.80113
[1mStep[0m  [8/21], [94mLoss[0m : 10.85663
[1mStep[0m  [10/21], [94mLoss[0m : 10.78886
[1mStep[0m  [12/21], [94mLoss[0m : 10.54982
[1mStep[0m  [14/21], [94mLoss[0m : 10.70654
[1mStep[0m  [16/21], [94mLoss[0m : 10.60905
[1mStep[0m  [18/21], [94mLoss[0m : 10.56289
[1mStep[0m  [20/21], [94mLoss[0m : 10.61591

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.696, [92mTest[0m: 10.651, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67408
[1mStep[0m  [2/21], [94mLoss[0m : 10.57002
[1mStep[0m  [4/21], [94mLoss[0m : 10.73054
[1mStep[0m  [6/21], [94mLoss[0m : 10.66355
[1mStep[0m  [8/21], [94mLoss[0m : 10.54968
[1mStep[0m  [10/21], [94mLoss[0m : 10.76022
[1mStep[0m  [12/21], [94mLoss[0m : 10.80333
[1mStep[0m  [14/21], [94mLoss[0m : 10.56370
[1mStep[0m  [16/21], [94mLoss[0m : 10.73612
[1mStep[0m  [18/21], [94mLoss[0m : 10.90380
[1mStep[0m  [20/21], [94mLoss[0m : 10.81951

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.691, [92mTest[0m: 10.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51993
[1mStep[0m  [2/21], [94mLoss[0m : 10.71368
[1mStep[0m  [4/21], [94mLoss[0m : 10.67369
[1mStep[0m  [6/21], [94mLoss[0m : 10.58509
[1mStep[0m  [8/21], [94mLoss[0m : 11.00939
[1mStep[0m  [10/21], [94mLoss[0m : 10.68914
[1mStep[0m  [12/21], [94mLoss[0m : 10.65144
[1mStep[0m  [14/21], [94mLoss[0m : 10.45688
[1mStep[0m  [16/21], [94mLoss[0m : 10.61646
[1mStep[0m  [18/21], [94mLoss[0m : 11.00187
[1mStep[0m  [20/21], [94mLoss[0m : 10.79640

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.649, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69953
[1mStep[0m  [2/21], [94mLoss[0m : 10.69909
[1mStep[0m  [4/21], [94mLoss[0m : 10.69924
[1mStep[0m  [6/21], [94mLoss[0m : 10.56339
[1mStep[0m  [8/21], [94mLoss[0m : 10.60132
[1mStep[0m  [10/21], [94mLoss[0m : 10.54918
[1mStep[0m  [12/21], [94mLoss[0m : 10.70408
[1mStep[0m  [14/21], [94mLoss[0m : 10.39499
[1mStep[0m  [16/21], [94mLoss[0m : 10.66983
[1mStep[0m  [18/21], [94mLoss[0m : 10.76100
[1mStep[0m  [20/21], [94mLoss[0m : 10.80244

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.677, [92mTest[0m: 10.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.64780
[1mStep[0m  [2/21], [94mLoss[0m : 10.58250
[1mStep[0m  [4/21], [94mLoss[0m : 10.91204
[1mStep[0m  [6/21], [94mLoss[0m : 10.72967
[1mStep[0m  [8/21], [94mLoss[0m : 10.58343
[1mStep[0m  [10/21], [94mLoss[0m : 10.58646
[1mStep[0m  [12/21], [94mLoss[0m : 10.58706
[1mStep[0m  [14/21], [94mLoss[0m : 10.54700
[1mStep[0m  [16/21], [94mLoss[0m : 10.67874
[1mStep[0m  [18/21], [94mLoss[0m : 10.79268
[1mStep[0m  [20/21], [94mLoss[0m : 10.45678

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.667, [92mTest[0m: 10.624, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79998
[1mStep[0m  [2/21], [94mLoss[0m : 10.58756
[1mStep[0m  [4/21], [94mLoss[0m : 10.64288
[1mStep[0m  [6/21], [94mLoss[0m : 10.63147
[1mStep[0m  [8/21], [94mLoss[0m : 10.56773
[1mStep[0m  [10/21], [94mLoss[0m : 10.65458
[1mStep[0m  [12/21], [94mLoss[0m : 10.63181
[1mStep[0m  [14/21], [94mLoss[0m : 10.83534
[1mStep[0m  [16/21], [94mLoss[0m : 10.69713
[1mStep[0m  [18/21], [94mLoss[0m : 10.88961
[1mStep[0m  [20/21], [94mLoss[0m : 10.49954

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.655, [92mTest[0m: 10.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54333
[1mStep[0m  [2/21], [94mLoss[0m : 10.62961
[1mStep[0m  [4/21], [94mLoss[0m : 10.60168
[1mStep[0m  [6/21], [94mLoss[0m : 10.52088
[1mStep[0m  [8/21], [94mLoss[0m : 11.01348
[1mStep[0m  [10/21], [94mLoss[0m : 10.50978
[1mStep[0m  [12/21], [94mLoss[0m : 10.59679
[1mStep[0m  [14/21], [94mLoss[0m : 10.73445
[1mStep[0m  [16/21], [94mLoss[0m : 11.00506
[1mStep[0m  [18/21], [94mLoss[0m : 10.49004
[1mStep[0m  [20/21], [94mLoss[0m : 10.46120

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.660, [92mTest[0m: 10.615, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.84697
[1mStep[0m  [2/21], [94mLoss[0m : 10.50829
[1mStep[0m  [4/21], [94mLoss[0m : 10.54857
[1mStep[0m  [6/21], [94mLoss[0m : 10.51216
[1mStep[0m  [8/21], [94mLoss[0m : 10.86953
[1mStep[0m  [10/21], [94mLoss[0m : 10.65349
[1mStep[0m  [12/21], [94mLoss[0m : 10.57055
[1mStep[0m  [14/21], [94mLoss[0m : 10.59487
[1mStep[0m  [16/21], [94mLoss[0m : 10.63639
[1mStep[0m  [18/21], [94mLoss[0m : 10.85189
[1mStep[0m  [20/21], [94mLoss[0m : 10.69067

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.656, [92mTest[0m: 10.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59219
[1mStep[0m  [2/21], [94mLoss[0m : 10.59704
[1mStep[0m  [4/21], [94mLoss[0m : 10.54610
[1mStep[0m  [6/21], [94mLoss[0m : 10.59817
[1mStep[0m  [8/21], [94mLoss[0m : 10.59160
[1mStep[0m  [10/21], [94mLoss[0m : 10.91362
[1mStep[0m  [12/21], [94mLoss[0m : 10.51462
[1mStep[0m  [14/21], [94mLoss[0m : 10.62655
[1mStep[0m  [16/21], [94mLoss[0m : 10.71026
[1mStep[0m  [18/21], [94mLoss[0m : 10.61810
[1mStep[0m  [20/21], [94mLoss[0m : 10.70721

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.654, [92mTest[0m: 10.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75887
[1mStep[0m  [2/21], [94mLoss[0m : 10.62904
[1mStep[0m  [4/21], [94mLoss[0m : 10.42445
[1mStep[0m  [6/21], [94mLoss[0m : 10.91044
[1mStep[0m  [8/21], [94mLoss[0m : 10.76417
[1mStep[0m  [10/21], [94mLoss[0m : 10.38429
[1mStep[0m  [12/21], [94mLoss[0m : 10.56402
[1mStep[0m  [14/21], [94mLoss[0m : 10.65086
[1mStep[0m  [16/21], [94mLoss[0m : 10.75037
[1mStep[0m  [18/21], [94mLoss[0m : 10.64445
[1mStep[0m  [20/21], [94mLoss[0m : 10.37893

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.643, [92mTest[0m: 10.594, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.53658
[1mStep[0m  [2/21], [94mLoss[0m : 10.67337
[1mStep[0m  [4/21], [94mLoss[0m : 10.67324
[1mStep[0m  [6/21], [94mLoss[0m : 10.58317
[1mStep[0m  [8/21], [94mLoss[0m : 10.49213
[1mStep[0m  [10/21], [94mLoss[0m : 10.81606
[1mStep[0m  [12/21], [94mLoss[0m : 10.82809
[1mStep[0m  [14/21], [94mLoss[0m : 10.44040
[1mStep[0m  [16/21], [94mLoss[0m : 10.70665
[1mStep[0m  [18/21], [94mLoss[0m : 10.59467
[1mStep[0m  [20/21], [94mLoss[0m : 10.76260

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.637, [92mTest[0m: 10.573, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82868
[1mStep[0m  [2/21], [94mLoss[0m : 10.58622
[1mStep[0m  [4/21], [94mLoss[0m : 10.74097
[1mStep[0m  [6/21], [94mLoss[0m : 10.29476
[1mStep[0m  [8/21], [94mLoss[0m : 10.50923
[1mStep[0m  [10/21], [94mLoss[0m : 10.47945
[1mStep[0m  [12/21], [94mLoss[0m : 10.39509
[1mStep[0m  [14/21], [94mLoss[0m : 10.79780
[1mStep[0m  [16/21], [94mLoss[0m : 10.65111
[1mStep[0m  [18/21], [94mLoss[0m : 10.76157
[1mStep[0m  [20/21], [94mLoss[0m : 10.86201

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48328
[1mStep[0m  [2/21], [94mLoss[0m : 10.72875
[1mStep[0m  [4/21], [94mLoss[0m : 10.60909
[1mStep[0m  [6/21], [94mLoss[0m : 10.76690
[1mStep[0m  [8/21], [94mLoss[0m : 10.43271
[1mStep[0m  [10/21], [94mLoss[0m : 10.60721
[1mStep[0m  [12/21], [94mLoss[0m : 10.53446
[1mStep[0m  [14/21], [94mLoss[0m : 10.55334
[1mStep[0m  [16/21], [94mLoss[0m : 10.62318
[1mStep[0m  [18/21], [94mLoss[0m : 10.77534
[1mStep[0m  [20/21], [94mLoss[0m : 10.70536

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.630, [92mTest[0m: 10.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61111
[1mStep[0m  [2/21], [94mLoss[0m : 10.67504
[1mStep[0m  [4/21], [94mLoss[0m : 10.68191
[1mStep[0m  [6/21], [94mLoss[0m : 10.57961
[1mStep[0m  [8/21], [94mLoss[0m : 10.29584
[1mStep[0m  [10/21], [94mLoss[0m : 10.73256
[1mStep[0m  [12/21], [94mLoss[0m : 10.62190
[1mStep[0m  [14/21], [94mLoss[0m : 10.48454
[1mStep[0m  [16/21], [94mLoss[0m : 10.47728
[1mStep[0m  [18/21], [94mLoss[0m : 10.45530
[1mStep[0m  [20/21], [94mLoss[0m : 10.72444

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.626, [92mTest[0m: 10.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63621
[1mStep[0m  [2/21], [94mLoss[0m : 10.60907
[1mStep[0m  [4/21], [94mLoss[0m : 10.73018
[1mStep[0m  [6/21], [94mLoss[0m : 10.81149
[1mStep[0m  [8/21], [94mLoss[0m : 10.45297
[1mStep[0m  [10/21], [94mLoss[0m : 10.77956
[1mStep[0m  [12/21], [94mLoss[0m : 10.46272
[1mStep[0m  [14/21], [94mLoss[0m : 10.69327
[1mStep[0m  [16/21], [94mLoss[0m : 10.67217
[1mStep[0m  [18/21], [94mLoss[0m : 10.42138
[1mStep[0m  [20/21], [94mLoss[0m : 10.60683

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.621, [92mTest[0m: 10.549, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61765
[1mStep[0m  [2/21], [94mLoss[0m : 10.55948
[1mStep[0m  [4/21], [94mLoss[0m : 10.45725
[1mStep[0m  [6/21], [94mLoss[0m : 10.74974
[1mStep[0m  [8/21], [94mLoss[0m : 10.59873
[1mStep[0m  [10/21], [94mLoss[0m : 10.72410
[1mStep[0m  [12/21], [94mLoss[0m : 11.05609
[1mStep[0m  [14/21], [94mLoss[0m : 10.43437
[1mStep[0m  [16/21], [94mLoss[0m : 10.36877
[1mStep[0m  [18/21], [94mLoss[0m : 10.60204
[1mStep[0m  [20/21], [94mLoss[0m : 10.57128

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74508
[1mStep[0m  [2/21], [94mLoss[0m : 10.71399
[1mStep[0m  [4/21], [94mLoss[0m : 10.64035
[1mStep[0m  [6/21], [94mLoss[0m : 10.68986
[1mStep[0m  [8/21], [94mLoss[0m : 10.52063
[1mStep[0m  [10/21], [94mLoss[0m : 10.62170
[1mStep[0m  [12/21], [94mLoss[0m : 10.32346
[1mStep[0m  [14/21], [94mLoss[0m : 10.59635
[1mStep[0m  [16/21], [94mLoss[0m : 10.63542
[1mStep[0m  [18/21], [94mLoss[0m : 10.85183
[1mStep[0m  [20/21], [94mLoss[0m : 10.47790

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.546, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39762
[1mStep[0m  [2/21], [94mLoss[0m : 10.68655
[1mStep[0m  [4/21], [94mLoss[0m : 10.58248
[1mStep[0m  [6/21], [94mLoss[0m : 10.27944
[1mStep[0m  [8/21], [94mLoss[0m : 10.53896
[1mStep[0m  [10/21], [94mLoss[0m : 10.73231
[1mStep[0m  [12/21], [94mLoss[0m : 10.66086
[1mStep[0m  [14/21], [94mLoss[0m : 11.07967
[1mStep[0m  [16/21], [94mLoss[0m : 10.59218
[1mStep[0m  [18/21], [94mLoss[0m : 10.88344
[1mStep[0m  [20/21], [94mLoss[0m : 10.47388

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.600, [92mTest[0m: 10.531, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.57820
[1mStep[0m  [2/21], [94mLoss[0m : 10.27556
[1mStep[0m  [4/21], [94mLoss[0m : 10.26284
[1mStep[0m  [6/21], [94mLoss[0m : 10.63518
[1mStep[0m  [8/21], [94mLoss[0m : 10.51893
[1mStep[0m  [10/21], [94mLoss[0m : 10.51458
[1mStep[0m  [12/21], [94mLoss[0m : 10.56329
[1mStep[0m  [14/21], [94mLoss[0m : 10.57010
[1mStep[0m  [16/21], [94mLoss[0m : 10.66742
[1mStep[0m  [18/21], [94mLoss[0m : 10.56212
[1mStep[0m  [20/21], [94mLoss[0m : 10.58125

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.532, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.35584
[1mStep[0m  [2/21], [94mLoss[0m : 10.44556
[1mStep[0m  [4/21], [94mLoss[0m : 10.55929
[1mStep[0m  [6/21], [94mLoss[0m : 10.28414
[1mStep[0m  [8/21], [94mLoss[0m : 10.48156
[1mStep[0m  [10/21], [94mLoss[0m : 10.87212
[1mStep[0m  [12/21], [94mLoss[0m : 10.59735
[1mStep[0m  [14/21], [94mLoss[0m : 10.33922
[1mStep[0m  [16/21], [94mLoss[0m : 10.77232
[1mStep[0m  [18/21], [94mLoss[0m : 10.62677
[1mStep[0m  [20/21], [94mLoss[0m : 10.41629

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.593, [92mTest[0m: 10.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.43050
[1mStep[0m  [2/21], [94mLoss[0m : 10.76192
[1mStep[0m  [4/21], [94mLoss[0m : 10.84576
[1mStep[0m  [6/21], [94mLoss[0m : 10.44848
[1mStep[0m  [8/21], [94mLoss[0m : 10.59559
[1mStep[0m  [10/21], [94mLoss[0m : 10.47361
[1mStep[0m  [12/21], [94mLoss[0m : 10.58932
[1mStep[0m  [14/21], [94mLoss[0m : 10.59059
[1mStep[0m  [16/21], [94mLoss[0m : 10.71431
[1mStep[0m  [18/21], [94mLoss[0m : 10.54884
[1mStep[0m  [20/21], [94mLoss[0m : 10.48903

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.582, [92mTest[0m: 10.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79590
[1mStep[0m  [2/21], [94mLoss[0m : 10.31556
[1mStep[0m  [4/21], [94mLoss[0m : 10.68075
[1mStep[0m  [6/21], [94mLoss[0m : 10.76614
[1mStep[0m  [8/21], [94mLoss[0m : 10.43988
[1mStep[0m  [10/21], [94mLoss[0m : 10.23541
[1mStep[0m  [12/21], [94mLoss[0m : 10.57053
[1mStep[0m  [14/21], [94mLoss[0m : 10.67169
[1mStep[0m  [16/21], [94mLoss[0m : 10.44151
[1mStep[0m  [18/21], [94mLoss[0m : 10.46642
[1mStep[0m  [20/21], [94mLoss[0m : 10.75868

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.583, [92mTest[0m: 10.515, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72321
[1mStep[0m  [2/21], [94mLoss[0m : 10.51277
[1mStep[0m  [4/21], [94mLoss[0m : 10.48100
[1mStep[0m  [6/21], [94mLoss[0m : 10.74359
[1mStep[0m  [8/21], [94mLoss[0m : 10.67933
[1mStep[0m  [10/21], [94mLoss[0m : 10.34026
[1mStep[0m  [12/21], [94mLoss[0m : 10.36293
[1mStep[0m  [14/21], [94mLoss[0m : 10.53677
[1mStep[0m  [16/21], [94mLoss[0m : 10.60943
[1mStep[0m  [18/21], [94mLoss[0m : 10.74717
[1mStep[0m  [20/21], [94mLoss[0m : 10.60499

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.567, [92mTest[0m: 10.509, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.31405
[1mStep[0m  [2/21], [94mLoss[0m : 10.59729
[1mStep[0m  [4/21], [94mLoss[0m : 10.46753
[1mStep[0m  [6/21], [94mLoss[0m : 10.60018
[1mStep[0m  [8/21], [94mLoss[0m : 10.37397
[1mStep[0m  [10/21], [94mLoss[0m : 10.50184
[1mStep[0m  [12/21], [94mLoss[0m : 10.82570
[1mStep[0m  [14/21], [94mLoss[0m : 10.34484
[1mStep[0m  [16/21], [94mLoss[0m : 10.34978
[1mStep[0m  [18/21], [94mLoss[0m : 10.88975
[1mStep[0m  [20/21], [94mLoss[0m : 10.78924

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.576, [92mTest[0m: 10.491, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.504
====================================

Phase 2 - Evaluation MAE:  10.50377709524972
MAE score P1       10.675142
MAE score P2       10.503777
loss               10.567032
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay          0.0001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.72153
[1mStep[0m  [4/42], [94mLoss[0m : 11.14010
[1mStep[0m  [8/42], [94mLoss[0m : 11.07225
[1mStep[0m  [12/42], [94mLoss[0m : 10.76218
[1mStep[0m  [16/42], [94mLoss[0m : 10.78569
[1mStep[0m  [20/42], [94mLoss[0m : 11.09395
[1mStep[0m  [24/42], [94mLoss[0m : 10.67431
[1mStep[0m  [28/42], [94mLoss[0m : 11.25818
[1mStep[0m  [32/42], [94mLoss[0m : 10.85147
[1mStep[0m  [36/42], [94mLoss[0m : 10.76558
[1mStep[0m  [40/42], [94mLoss[0m : 10.36812

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.925, [92mTest[0m: 11.146, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.77716
[1mStep[0m  [4/42], [94mLoss[0m : 10.85846
[1mStep[0m  [8/42], [94mLoss[0m : 11.14678
[1mStep[0m  [12/42], [94mLoss[0m : 11.02400
[1mStep[0m  [16/42], [94mLoss[0m : 10.71466
[1mStep[0m  [20/42], [94mLoss[0m : 11.17821
[1mStep[0m  [24/42], [94mLoss[0m : 10.70963
[1mStep[0m  [28/42], [94mLoss[0m : 10.83047
[1mStep[0m  [32/42], [94mLoss[0m : 10.41055
[1mStep[0m  [36/42], [94mLoss[0m : 11.06853
[1mStep[0m  [40/42], [94mLoss[0m : 10.80063

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.921, [92mTest[0m: 10.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.62021
[1mStep[0m  [4/42], [94mLoss[0m : 10.72482
[1mStep[0m  [8/42], [94mLoss[0m : 10.92020
[1mStep[0m  [12/42], [94mLoss[0m : 11.37157
[1mStep[0m  [16/42], [94mLoss[0m : 10.79471
[1mStep[0m  [20/42], [94mLoss[0m : 10.61018
[1mStep[0m  [24/42], [94mLoss[0m : 10.90607
[1mStep[0m  [28/42], [94mLoss[0m : 10.84479
[1mStep[0m  [32/42], [94mLoss[0m : 11.30593
[1mStep[0m  [36/42], [94mLoss[0m : 11.27610
[1mStep[0m  [40/42], [94mLoss[0m : 10.74107

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.910, [92mTest[0m: 10.945, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.42619
[1mStep[0m  [4/42], [94mLoss[0m : 10.57798
[1mStep[0m  [8/42], [94mLoss[0m : 10.79698
[1mStep[0m  [12/42], [94mLoss[0m : 11.09361
[1mStep[0m  [16/42], [94mLoss[0m : 10.46673
[1mStep[0m  [20/42], [94mLoss[0m : 10.86550
[1mStep[0m  [24/42], [94mLoss[0m : 10.67565
[1mStep[0m  [28/42], [94mLoss[0m : 10.33904
[1mStep[0m  [32/42], [94mLoss[0m : 10.85115
[1mStep[0m  [36/42], [94mLoss[0m : 10.99854
[1mStep[0m  [40/42], [94mLoss[0m : 10.97437

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.904, [92mTest[0m: 10.917, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.27668
[1mStep[0m  [4/42], [94mLoss[0m : 10.47236
[1mStep[0m  [8/42], [94mLoss[0m : 10.62294
[1mStep[0m  [12/42], [94mLoss[0m : 10.89215
[1mStep[0m  [16/42], [94mLoss[0m : 10.75298
[1mStep[0m  [20/42], [94mLoss[0m : 10.64790
[1mStep[0m  [24/42], [94mLoss[0m : 10.86356
[1mStep[0m  [28/42], [94mLoss[0m : 10.32779
[1mStep[0m  [32/42], [94mLoss[0m : 11.12031
[1mStep[0m  [36/42], [94mLoss[0m : 11.16612
[1mStep[0m  [40/42], [94mLoss[0m : 10.68160

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.911, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.19475
[1mStep[0m  [4/42], [94mLoss[0m : 10.80805
[1mStep[0m  [8/42], [94mLoss[0m : 10.41185
[1mStep[0m  [12/42], [94mLoss[0m : 10.84962
[1mStep[0m  [16/42], [94mLoss[0m : 10.87784
[1mStep[0m  [20/42], [94mLoss[0m : 11.07770
[1mStep[0m  [24/42], [94mLoss[0m : 11.03628
[1mStep[0m  [28/42], [94mLoss[0m : 11.09545
[1mStep[0m  [32/42], [94mLoss[0m : 10.87600
[1mStep[0m  [36/42], [94mLoss[0m : 10.96433
[1mStep[0m  [40/42], [94mLoss[0m : 10.97426

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.884, [92mTest[0m: 10.915, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.77847
[1mStep[0m  [4/42], [94mLoss[0m : 10.75077
[1mStep[0m  [8/42], [94mLoss[0m : 10.66595
[1mStep[0m  [12/42], [94mLoss[0m : 10.82129
[1mStep[0m  [16/42], [94mLoss[0m : 10.90729
[1mStep[0m  [20/42], [94mLoss[0m : 11.12855
[1mStep[0m  [24/42], [94mLoss[0m : 10.72354
[1mStep[0m  [28/42], [94mLoss[0m : 10.71703
[1mStep[0m  [32/42], [94mLoss[0m : 10.84565
[1mStep[0m  [36/42], [94mLoss[0m : 11.00301
[1mStep[0m  [40/42], [94mLoss[0m : 10.76984

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.881, [92mTest[0m: 10.886, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.59427
[1mStep[0m  [4/42], [94mLoss[0m : 10.41480
[1mStep[0m  [8/42], [94mLoss[0m : 10.36096
[1mStep[0m  [12/42], [94mLoss[0m : 11.03665
[1mStep[0m  [16/42], [94mLoss[0m : 10.97492
[1mStep[0m  [20/42], [94mLoss[0m : 11.29395
[1mStep[0m  [24/42], [94mLoss[0m : 10.81358
[1mStep[0m  [28/42], [94mLoss[0m : 10.63507
[1mStep[0m  [32/42], [94mLoss[0m : 10.97157
[1mStep[0m  [36/42], [94mLoss[0m : 11.25482
[1mStep[0m  [40/42], [94mLoss[0m : 11.21238

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.868, [92mTest[0m: 10.862, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.06708
[1mStep[0m  [4/42], [94mLoss[0m : 10.61529
[1mStep[0m  [8/42], [94mLoss[0m : 10.85519
[1mStep[0m  [12/42], [94mLoss[0m : 10.90153
[1mStep[0m  [16/42], [94mLoss[0m : 11.05409
[1mStep[0m  [20/42], [94mLoss[0m : 11.01442
[1mStep[0m  [24/42], [94mLoss[0m : 10.95034
[1mStep[0m  [28/42], [94mLoss[0m : 10.83769
[1mStep[0m  [32/42], [94mLoss[0m : 10.73950
[1mStep[0m  [36/42], [94mLoss[0m : 11.11109
[1mStep[0m  [40/42], [94mLoss[0m : 10.76816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.868, [92mTest[0m: 10.869, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68627
[1mStep[0m  [4/42], [94mLoss[0m : 10.89530
[1mStep[0m  [8/42], [94mLoss[0m : 10.72541
[1mStep[0m  [12/42], [94mLoss[0m : 11.00792
[1mStep[0m  [16/42], [94mLoss[0m : 11.30960
[1mStep[0m  [20/42], [94mLoss[0m : 10.98240
[1mStep[0m  [24/42], [94mLoss[0m : 11.00320
[1mStep[0m  [28/42], [94mLoss[0m : 10.98123
[1mStep[0m  [32/42], [94mLoss[0m : 10.87017
[1mStep[0m  [36/42], [94mLoss[0m : 10.79856
[1mStep[0m  [40/42], [94mLoss[0m : 10.70355

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.857, [92mTest[0m: 10.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.89857
[1mStep[0m  [4/42], [94mLoss[0m : 10.86314
[1mStep[0m  [8/42], [94mLoss[0m : 10.96946
[1mStep[0m  [12/42], [94mLoss[0m : 11.18436
[1mStep[0m  [16/42], [94mLoss[0m : 10.71327
[1mStep[0m  [20/42], [94mLoss[0m : 10.84617
[1mStep[0m  [24/42], [94mLoss[0m : 10.91520
[1mStep[0m  [28/42], [94mLoss[0m : 10.76764
[1mStep[0m  [32/42], [94mLoss[0m : 10.80130
[1mStep[0m  [36/42], [94mLoss[0m : 10.90463
[1mStep[0m  [40/42], [94mLoss[0m : 10.51612

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.848, [92mTest[0m: 10.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.14871
[1mStep[0m  [4/42], [94mLoss[0m : 10.83908
[1mStep[0m  [8/42], [94mLoss[0m : 11.10882
[1mStep[0m  [12/42], [94mLoss[0m : 11.11201
[1mStep[0m  [16/42], [94mLoss[0m : 10.68972
[1mStep[0m  [20/42], [94mLoss[0m : 10.73173
[1mStep[0m  [24/42], [94mLoss[0m : 10.71036
[1mStep[0m  [28/42], [94mLoss[0m : 10.87076
[1mStep[0m  [32/42], [94mLoss[0m : 10.90586
[1mStep[0m  [36/42], [94mLoss[0m : 10.65055
[1mStep[0m  [40/42], [94mLoss[0m : 10.42995

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.843, [92mTest[0m: 10.844, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.04430
[1mStep[0m  [4/42], [94mLoss[0m : 10.92739
[1mStep[0m  [8/42], [94mLoss[0m : 10.82386
[1mStep[0m  [12/42], [94mLoss[0m : 10.28621
[1mStep[0m  [16/42], [94mLoss[0m : 10.79950
[1mStep[0m  [20/42], [94mLoss[0m : 11.15023
[1mStep[0m  [24/42], [94mLoss[0m : 11.25574
[1mStep[0m  [28/42], [94mLoss[0m : 10.76231
[1mStep[0m  [32/42], [94mLoss[0m : 11.05777
[1mStep[0m  [36/42], [94mLoss[0m : 10.42788
[1mStep[0m  [40/42], [94mLoss[0m : 10.55328

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.837, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66549
[1mStep[0m  [4/42], [94mLoss[0m : 11.05198
[1mStep[0m  [8/42], [94mLoss[0m : 11.01795
[1mStep[0m  [12/42], [94mLoss[0m : 10.62166
[1mStep[0m  [16/42], [94mLoss[0m : 10.56435
[1mStep[0m  [20/42], [94mLoss[0m : 10.92084
[1mStep[0m  [24/42], [94mLoss[0m : 10.66253
[1mStep[0m  [28/42], [94mLoss[0m : 10.51763
[1mStep[0m  [32/42], [94mLoss[0m : 10.86128
[1mStep[0m  [36/42], [94mLoss[0m : 11.07262
[1mStep[0m  [40/42], [94mLoss[0m : 10.58090

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.835, [92mTest[0m: 10.820, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94022
[1mStep[0m  [4/42], [94mLoss[0m : 11.03173
[1mStep[0m  [8/42], [94mLoss[0m : 10.77665
[1mStep[0m  [12/42], [94mLoss[0m : 10.26032
[1mStep[0m  [16/42], [94mLoss[0m : 10.28759
[1mStep[0m  [20/42], [94mLoss[0m : 10.87223
[1mStep[0m  [24/42], [94mLoss[0m : 10.61835
[1mStep[0m  [28/42], [94mLoss[0m : 10.53079
[1mStep[0m  [32/42], [94mLoss[0m : 10.55703
[1mStep[0m  [36/42], [94mLoss[0m : 10.86450
[1mStep[0m  [40/42], [94mLoss[0m : 10.84678

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.820, [92mTest[0m: 10.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.98138
[1mStep[0m  [4/42], [94mLoss[0m : 10.88906
[1mStep[0m  [8/42], [94mLoss[0m : 10.88895
[1mStep[0m  [12/42], [94mLoss[0m : 10.70160
[1mStep[0m  [16/42], [94mLoss[0m : 11.25661
[1mStep[0m  [20/42], [94mLoss[0m : 10.68548
[1mStep[0m  [24/42], [94mLoss[0m : 10.79802
[1mStep[0m  [28/42], [94mLoss[0m : 10.70029
[1mStep[0m  [32/42], [94mLoss[0m : 10.81820
[1mStep[0m  [36/42], [94mLoss[0m : 10.94487
[1mStep[0m  [40/42], [94mLoss[0m : 10.60432

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.814, [92mTest[0m: 10.794, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63752
[1mStep[0m  [4/42], [94mLoss[0m : 10.52975
[1mStep[0m  [8/42], [94mLoss[0m : 10.67454
[1mStep[0m  [12/42], [94mLoss[0m : 10.90941
[1mStep[0m  [16/42], [94mLoss[0m : 10.79511
[1mStep[0m  [20/42], [94mLoss[0m : 10.98380
[1mStep[0m  [24/42], [94mLoss[0m : 10.90849
[1mStep[0m  [28/42], [94mLoss[0m : 10.53138
[1mStep[0m  [32/42], [94mLoss[0m : 10.62202
[1mStep[0m  [36/42], [94mLoss[0m : 11.10981
[1mStep[0m  [40/42], [94mLoss[0m : 10.76153

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.814, [92mTest[0m: 10.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90617
[1mStep[0m  [4/42], [94mLoss[0m : 10.72078
[1mStep[0m  [8/42], [94mLoss[0m : 10.59801
[1mStep[0m  [12/42], [94mLoss[0m : 11.00550
[1mStep[0m  [16/42], [94mLoss[0m : 10.78853
[1mStep[0m  [20/42], [94mLoss[0m : 10.76859
[1mStep[0m  [24/42], [94mLoss[0m : 11.10537
[1mStep[0m  [28/42], [94mLoss[0m : 10.98039
[1mStep[0m  [32/42], [94mLoss[0m : 10.23752
[1mStep[0m  [36/42], [94mLoss[0m : 11.05658
[1mStep[0m  [40/42], [94mLoss[0m : 10.65995

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.803, [92mTest[0m: 10.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71402
[1mStep[0m  [4/42], [94mLoss[0m : 10.66478
[1mStep[0m  [8/42], [94mLoss[0m : 10.71315
[1mStep[0m  [12/42], [94mLoss[0m : 10.64529
[1mStep[0m  [16/42], [94mLoss[0m : 10.96800
[1mStep[0m  [20/42], [94mLoss[0m : 10.68221
[1mStep[0m  [24/42], [94mLoss[0m : 10.58173
[1mStep[0m  [28/42], [94mLoss[0m : 11.17440
[1mStep[0m  [32/42], [94mLoss[0m : 10.67338
[1mStep[0m  [36/42], [94mLoss[0m : 10.79759
[1mStep[0m  [40/42], [94mLoss[0m : 10.60334

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.788, [92mTest[0m: 10.765, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.88835
[1mStep[0m  [4/42], [94mLoss[0m : 10.89340
[1mStep[0m  [8/42], [94mLoss[0m : 11.10067
[1mStep[0m  [12/42], [94mLoss[0m : 10.89262
[1mStep[0m  [16/42], [94mLoss[0m : 11.22790
[1mStep[0m  [20/42], [94mLoss[0m : 11.20659
[1mStep[0m  [24/42], [94mLoss[0m : 10.55995
[1mStep[0m  [28/42], [94mLoss[0m : 10.51524
[1mStep[0m  [32/42], [94mLoss[0m : 10.98121
[1mStep[0m  [36/42], [94mLoss[0m : 10.86436
[1mStep[0m  [40/42], [94mLoss[0m : 10.83232

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.755, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54186
[1mStep[0m  [4/42], [94mLoss[0m : 10.84299
[1mStep[0m  [8/42], [94mLoss[0m : 10.50556
[1mStep[0m  [12/42], [94mLoss[0m : 10.43484
[1mStep[0m  [16/42], [94mLoss[0m : 10.71189
[1mStep[0m  [20/42], [94mLoss[0m : 10.75245
[1mStep[0m  [24/42], [94mLoss[0m : 10.72453
[1mStep[0m  [28/42], [94mLoss[0m : 10.66613
[1mStep[0m  [32/42], [94mLoss[0m : 10.51451
[1mStep[0m  [36/42], [94mLoss[0m : 10.71318
[1mStep[0m  [40/42], [94mLoss[0m : 10.61744

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.764, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83154
[1mStep[0m  [4/42], [94mLoss[0m : 10.56755
[1mStep[0m  [8/42], [94mLoss[0m : 10.67550
[1mStep[0m  [12/42], [94mLoss[0m : 11.10114
[1mStep[0m  [16/42], [94mLoss[0m : 11.08630
[1mStep[0m  [20/42], [94mLoss[0m : 10.78342
[1mStep[0m  [24/42], [94mLoss[0m : 11.11979
[1mStep[0m  [28/42], [94mLoss[0m : 10.43578
[1mStep[0m  [32/42], [94mLoss[0m : 10.47813
[1mStep[0m  [36/42], [94mLoss[0m : 10.86464
[1mStep[0m  [40/42], [94mLoss[0m : 11.01775

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.730, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.61968
[1mStep[0m  [4/42], [94mLoss[0m : 10.95244
[1mStep[0m  [8/42], [94mLoss[0m : 10.52533
[1mStep[0m  [12/42], [94mLoss[0m : 11.04622
[1mStep[0m  [16/42], [94mLoss[0m : 10.72653
[1mStep[0m  [20/42], [94mLoss[0m : 11.00746
[1mStep[0m  [24/42], [94mLoss[0m : 10.80243
[1mStep[0m  [28/42], [94mLoss[0m : 10.84607
[1mStep[0m  [32/42], [94mLoss[0m : 10.67608
[1mStep[0m  [36/42], [94mLoss[0m : 10.74879
[1mStep[0m  [40/42], [94mLoss[0m : 10.52440

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.778, [92mTest[0m: 10.736, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.70886
[1mStep[0m  [4/42], [94mLoss[0m : 11.03251
[1mStep[0m  [8/42], [94mLoss[0m : 10.78406
[1mStep[0m  [12/42], [94mLoss[0m : 10.90676
[1mStep[0m  [16/42], [94mLoss[0m : 10.34190
[1mStep[0m  [20/42], [94mLoss[0m : 10.42206
[1mStep[0m  [24/42], [94mLoss[0m : 11.11842
[1mStep[0m  [28/42], [94mLoss[0m : 10.77837
[1mStep[0m  [32/42], [94mLoss[0m : 10.29754
[1mStep[0m  [36/42], [94mLoss[0m : 10.86096
[1mStep[0m  [40/42], [94mLoss[0m : 10.78263

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.761, [92mTest[0m: 10.715, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.61175
[1mStep[0m  [4/42], [94mLoss[0m : 10.94249
[1mStep[0m  [8/42], [94mLoss[0m : 10.58421
[1mStep[0m  [12/42], [94mLoss[0m : 10.63185
[1mStep[0m  [16/42], [94mLoss[0m : 10.83105
[1mStep[0m  [20/42], [94mLoss[0m : 10.63843
[1mStep[0m  [24/42], [94mLoss[0m : 10.96694
[1mStep[0m  [28/42], [94mLoss[0m : 10.86076
[1mStep[0m  [32/42], [94mLoss[0m : 10.90348
[1mStep[0m  [36/42], [94mLoss[0m : 10.51714
[1mStep[0m  [40/42], [94mLoss[0m : 10.87903

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.747, [92mTest[0m: 10.719, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81853
[1mStep[0m  [4/42], [94mLoss[0m : 11.19484
[1mStep[0m  [8/42], [94mLoss[0m : 10.07345
[1mStep[0m  [12/42], [94mLoss[0m : 11.03221
[1mStep[0m  [16/42], [94mLoss[0m : 10.69333
[1mStep[0m  [20/42], [94mLoss[0m : 10.07979
[1mStep[0m  [24/42], [94mLoss[0m : 10.64798
[1mStep[0m  [28/42], [94mLoss[0m : 10.91741
[1mStep[0m  [32/42], [94mLoss[0m : 10.86045
[1mStep[0m  [36/42], [94mLoss[0m : 11.06795
[1mStep[0m  [40/42], [94mLoss[0m : 11.09558

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.686, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.55224
[1mStep[0m  [4/42], [94mLoss[0m : 10.76329
[1mStep[0m  [8/42], [94mLoss[0m : 10.75630
[1mStep[0m  [12/42], [94mLoss[0m : 10.80152
[1mStep[0m  [16/42], [94mLoss[0m : 10.83883
[1mStep[0m  [20/42], [94mLoss[0m : 10.77891
[1mStep[0m  [24/42], [94mLoss[0m : 10.93861
[1mStep[0m  [28/42], [94mLoss[0m : 11.29542
[1mStep[0m  [32/42], [94mLoss[0m : 10.77259
[1mStep[0m  [36/42], [94mLoss[0m : 10.51094
[1mStep[0m  [40/42], [94mLoss[0m : 10.84523

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.732, [92mTest[0m: 10.687, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94518
[1mStep[0m  [4/42], [94mLoss[0m : 11.05505
[1mStep[0m  [8/42], [94mLoss[0m : 10.41157
[1mStep[0m  [12/42], [94mLoss[0m : 10.70779
[1mStep[0m  [16/42], [94mLoss[0m : 10.67073
[1mStep[0m  [20/42], [94mLoss[0m : 10.75368
[1mStep[0m  [24/42], [94mLoss[0m : 10.46687
[1mStep[0m  [28/42], [94mLoss[0m : 10.81259
[1mStep[0m  [32/42], [94mLoss[0m : 10.73920
[1mStep[0m  [36/42], [94mLoss[0m : 11.11749
[1mStep[0m  [40/42], [94mLoss[0m : 10.62394

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.724, [92mTest[0m: 10.678, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.07890
[1mStep[0m  [4/42], [94mLoss[0m : 10.99300
[1mStep[0m  [8/42], [94mLoss[0m : 10.58931
[1mStep[0m  [12/42], [94mLoss[0m : 10.63676
[1mStep[0m  [16/42], [94mLoss[0m : 10.80959
[1mStep[0m  [20/42], [94mLoss[0m : 10.41155
[1mStep[0m  [24/42], [94mLoss[0m : 10.54420
[1mStep[0m  [28/42], [94mLoss[0m : 10.97119
[1mStep[0m  [32/42], [94mLoss[0m : 10.78132
[1mStep[0m  [36/42], [94mLoss[0m : 10.69838
[1mStep[0m  [40/42], [94mLoss[0m : 10.56231

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.673, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53680
[1mStep[0m  [4/42], [94mLoss[0m : 10.53171
[1mStep[0m  [8/42], [94mLoss[0m : 10.78208
[1mStep[0m  [12/42], [94mLoss[0m : 10.68220
[1mStep[0m  [16/42], [94mLoss[0m : 10.40004
[1mStep[0m  [20/42], [94mLoss[0m : 10.62460
[1mStep[0m  [24/42], [94mLoss[0m : 10.95391
[1mStep[0m  [28/42], [94mLoss[0m : 10.87111
[1mStep[0m  [32/42], [94mLoss[0m : 10.71559
[1mStep[0m  [36/42], [94mLoss[0m : 10.83139
[1mStep[0m  [40/42], [94mLoss[0m : 10.65990

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.663, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.667
====================================

Phase 1 - Evaluation MAE:  10.66709818158831
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.66540
[1mStep[0m  [4/42], [94mLoss[0m : 10.98633
[1mStep[0m  [8/42], [94mLoss[0m : 10.75247
[1mStep[0m  [12/42], [94mLoss[0m : 10.96118
[1mStep[0m  [16/42], [94mLoss[0m : 11.01003
[1mStep[0m  [20/42], [94mLoss[0m : 10.61064
[1mStep[0m  [24/42], [94mLoss[0m : 10.70921
[1mStep[0m  [28/42], [94mLoss[0m : 10.83910
[1mStep[0m  [32/42], [94mLoss[0m : 10.68097
[1mStep[0m  [36/42], [94mLoss[0m : 10.71783
[1mStep[0m  [40/42], [94mLoss[0m : 10.40071

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.711, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63689
[1mStep[0m  [4/42], [94mLoss[0m : 10.81911
[1mStep[0m  [8/42], [94mLoss[0m : 10.52502
[1mStep[0m  [12/42], [94mLoss[0m : 11.10652
[1mStep[0m  [16/42], [94mLoss[0m : 10.46415
[1mStep[0m  [20/42], [94mLoss[0m : 10.47359
[1mStep[0m  [24/42], [94mLoss[0m : 10.88811
[1mStep[0m  [28/42], [94mLoss[0m : 10.67453
[1mStep[0m  [32/42], [94mLoss[0m : 10.91177
[1mStep[0m  [36/42], [94mLoss[0m : 10.97283
[1mStep[0m  [40/42], [94mLoss[0m : 10.96105

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.697, [92mTest[0m: 10.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81547
[1mStep[0m  [4/42], [94mLoss[0m : 10.33032
[1mStep[0m  [8/42], [94mLoss[0m : 10.77144
[1mStep[0m  [12/42], [94mLoss[0m : 10.78884
[1mStep[0m  [16/42], [94mLoss[0m : 11.05325
[1mStep[0m  [20/42], [94mLoss[0m : 10.75397
[1mStep[0m  [24/42], [94mLoss[0m : 10.68144
[1mStep[0m  [28/42], [94mLoss[0m : 10.54898
[1mStep[0m  [32/42], [94mLoss[0m : 11.23777
[1mStep[0m  [36/42], [94mLoss[0m : 10.85961
[1mStep[0m  [40/42], [94mLoss[0m : 10.77172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.686, [92mTest[0m: 10.628, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.28807
[1mStep[0m  [4/42], [94mLoss[0m : 10.91521
[1mStep[0m  [8/42], [94mLoss[0m : 10.85265
[1mStep[0m  [12/42], [94mLoss[0m : 10.86128
[1mStep[0m  [16/42], [94mLoss[0m : 10.65905
[1mStep[0m  [20/42], [94mLoss[0m : 10.68260
[1mStep[0m  [24/42], [94mLoss[0m : 10.49931
[1mStep[0m  [28/42], [94mLoss[0m : 10.45931
[1mStep[0m  [32/42], [94mLoss[0m : 10.76502
[1mStep[0m  [36/42], [94mLoss[0m : 10.55606
[1mStep[0m  [40/42], [94mLoss[0m : 10.47764

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.667, [92mTest[0m: 10.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81214
[1mStep[0m  [4/42], [94mLoss[0m : 10.61258
[1mStep[0m  [8/42], [94mLoss[0m : 10.84539
[1mStep[0m  [12/42], [94mLoss[0m : 10.78892
[1mStep[0m  [16/42], [94mLoss[0m : 10.52262
[1mStep[0m  [20/42], [94mLoss[0m : 10.49782
[1mStep[0m  [24/42], [94mLoss[0m : 10.97137
[1mStep[0m  [28/42], [94mLoss[0m : 10.43366
[1mStep[0m  [32/42], [94mLoss[0m : 10.52034
[1mStep[0m  [36/42], [94mLoss[0m : 10.88127
[1mStep[0m  [40/42], [94mLoss[0m : 10.72039

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.655, [92mTest[0m: 10.595, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66520
[1mStep[0m  [4/42], [94mLoss[0m : 10.73272
[1mStep[0m  [8/42], [94mLoss[0m : 10.71704
[1mStep[0m  [12/42], [94mLoss[0m : 10.84937
[1mStep[0m  [16/42], [94mLoss[0m : 10.54333
[1mStep[0m  [20/42], [94mLoss[0m : 10.42440
[1mStep[0m  [24/42], [94mLoss[0m : 10.53331
[1mStep[0m  [28/42], [94mLoss[0m : 10.56059
[1mStep[0m  [32/42], [94mLoss[0m : 10.61421
[1mStep[0m  [36/42], [94mLoss[0m : 10.51164
[1mStep[0m  [40/42], [94mLoss[0m : 10.44902

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.653, [92mTest[0m: 10.584, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.73075
[1mStep[0m  [4/42], [94mLoss[0m : 10.81313
[1mStep[0m  [8/42], [94mLoss[0m : 10.86994
[1mStep[0m  [12/42], [94mLoss[0m : 10.89146
[1mStep[0m  [16/42], [94mLoss[0m : 10.62186
[1mStep[0m  [20/42], [94mLoss[0m : 10.68149
[1mStep[0m  [24/42], [94mLoss[0m : 10.74063
[1mStep[0m  [28/42], [94mLoss[0m : 10.71700
[1mStep[0m  [32/42], [94mLoss[0m : 10.51030
[1mStep[0m  [36/42], [94mLoss[0m : 10.72802
[1mStep[0m  [40/42], [94mLoss[0m : 10.80492

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.632, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.58501
[1mStep[0m  [4/42], [94mLoss[0m : 10.06435
[1mStep[0m  [8/42], [94mLoss[0m : 10.77169
[1mStep[0m  [12/42], [94mLoss[0m : 10.80161
[1mStep[0m  [16/42], [94mLoss[0m : 11.07670
[1mStep[0m  [20/42], [94mLoss[0m : 10.67800
[1mStep[0m  [24/42], [94mLoss[0m : 10.74183
[1mStep[0m  [28/42], [94mLoss[0m : 11.05719
[1mStep[0m  [32/42], [94mLoss[0m : 10.68260
[1mStep[0m  [36/42], [94mLoss[0m : 10.76222
[1mStep[0m  [40/42], [94mLoss[0m : 10.18670

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.621, [92mTest[0m: 10.552, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71711
[1mStep[0m  [4/42], [94mLoss[0m : 10.64510
[1mStep[0m  [8/42], [94mLoss[0m : 10.62071
[1mStep[0m  [12/42], [94mLoss[0m : 10.70366
[1mStep[0m  [16/42], [94mLoss[0m : 10.41588
[1mStep[0m  [20/42], [94mLoss[0m : 10.42479
[1mStep[0m  [24/42], [94mLoss[0m : 10.51253
[1mStep[0m  [28/42], [94mLoss[0m : 10.40645
[1mStep[0m  [32/42], [94mLoss[0m : 10.36548
[1mStep[0m  [36/42], [94mLoss[0m : 10.75307
[1mStep[0m  [40/42], [94mLoss[0m : 10.53824

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.609, [92mTest[0m: 10.544, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.86982
[1mStep[0m  [4/42], [94mLoss[0m : 10.95301
[1mStep[0m  [8/42], [94mLoss[0m : 10.26538
[1mStep[0m  [12/42], [94mLoss[0m : 10.63351
[1mStep[0m  [16/42], [94mLoss[0m : 10.57205
[1mStep[0m  [20/42], [94mLoss[0m : 10.66785
[1mStep[0m  [24/42], [94mLoss[0m : 10.56589
[1mStep[0m  [28/42], [94mLoss[0m : 10.34713
[1mStep[0m  [32/42], [94mLoss[0m : 10.69556
[1mStep[0m  [36/42], [94mLoss[0m : 10.77344
[1mStep[0m  [40/42], [94mLoss[0m : 10.86139

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.509, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26269
[1mStep[0m  [4/42], [94mLoss[0m : 10.71778
[1mStep[0m  [8/42], [94mLoss[0m : 10.77591
[1mStep[0m  [12/42], [94mLoss[0m : 10.43625
[1mStep[0m  [16/42], [94mLoss[0m : 10.37524
[1mStep[0m  [20/42], [94mLoss[0m : 11.21559
[1mStep[0m  [24/42], [94mLoss[0m : 10.84689
[1mStep[0m  [28/42], [94mLoss[0m : 10.47523
[1mStep[0m  [32/42], [94mLoss[0m : 10.76384
[1mStep[0m  [36/42], [94mLoss[0m : 10.35021
[1mStep[0m  [40/42], [94mLoss[0m : 10.33899

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.596, [92mTest[0m: 10.522, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.69842
[1mStep[0m  [4/42], [94mLoss[0m : 10.88983
[1mStep[0m  [8/42], [94mLoss[0m : 10.19995
[1mStep[0m  [12/42], [94mLoss[0m : 10.34530
[1mStep[0m  [16/42], [94mLoss[0m : 10.92550
[1mStep[0m  [20/42], [94mLoss[0m : 11.20634
[1mStep[0m  [24/42], [94mLoss[0m : 10.48941
[1mStep[0m  [28/42], [94mLoss[0m : 10.66304
[1mStep[0m  [32/42], [94mLoss[0m : 10.60554
[1mStep[0m  [36/42], [94mLoss[0m : 10.49410
[1mStep[0m  [40/42], [94mLoss[0m : 10.35530

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.579, [92mTest[0m: 10.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.04917
[1mStep[0m  [4/42], [94mLoss[0m : 10.87174
[1mStep[0m  [8/42], [94mLoss[0m : 10.74674
[1mStep[0m  [12/42], [94mLoss[0m : 10.96698
[1mStep[0m  [16/42], [94mLoss[0m : 10.54000
[1mStep[0m  [20/42], [94mLoss[0m : 10.78543
[1mStep[0m  [24/42], [94mLoss[0m : 11.13826
[1mStep[0m  [28/42], [94mLoss[0m : 10.08565
[1mStep[0m  [32/42], [94mLoss[0m : 10.39847
[1mStep[0m  [36/42], [94mLoss[0m : 10.54762
[1mStep[0m  [40/42], [94mLoss[0m : 10.39638

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.573, [92mTest[0m: 10.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.21835
[1mStep[0m  [4/42], [94mLoss[0m : 10.25059
[1mStep[0m  [8/42], [94mLoss[0m : 10.64458
[1mStep[0m  [12/42], [94mLoss[0m : 10.49605
[1mStep[0m  [16/42], [94mLoss[0m : 10.68735
[1mStep[0m  [20/42], [94mLoss[0m : 10.75904
[1mStep[0m  [24/42], [94mLoss[0m : 10.50891
[1mStep[0m  [28/42], [94mLoss[0m : 10.41369
[1mStep[0m  [32/42], [94mLoss[0m : 10.58956
[1mStep[0m  [36/42], [94mLoss[0m : 10.61364
[1mStep[0m  [40/42], [94mLoss[0m : 10.40985

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.555, [92mTest[0m: 10.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.30483
[1mStep[0m  [4/42], [94mLoss[0m : 10.50121
[1mStep[0m  [8/42], [94mLoss[0m : 10.57077
[1mStep[0m  [12/42], [94mLoss[0m : 10.46767
[1mStep[0m  [16/42], [94mLoss[0m : 10.37267
[1mStep[0m  [20/42], [94mLoss[0m : 10.35999
[1mStep[0m  [24/42], [94mLoss[0m : 11.14554
[1mStep[0m  [28/42], [94mLoss[0m : 10.75051
[1mStep[0m  [32/42], [94mLoss[0m : 10.81975
[1mStep[0m  [36/42], [94mLoss[0m : 10.52736
[1mStep[0m  [40/42], [94mLoss[0m : 10.62543

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.546, [92mTest[0m: 10.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.80108
[1mStep[0m  [4/42], [94mLoss[0m : 10.66266
[1mStep[0m  [8/42], [94mLoss[0m : 10.35791
[1mStep[0m  [12/42], [94mLoss[0m : 10.28813
[1mStep[0m  [16/42], [94mLoss[0m : 10.61641
[1mStep[0m  [20/42], [94mLoss[0m : 10.41549
[1mStep[0m  [24/42], [94mLoss[0m : 10.72873
[1mStep[0m  [28/42], [94mLoss[0m : 10.25451
[1mStep[0m  [32/42], [94mLoss[0m : 10.42797
[1mStep[0m  [36/42], [94mLoss[0m : 10.17884
[1mStep[0m  [40/42], [94mLoss[0m : 10.52163

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.524, [92mTest[0m: 10.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53312
[1mStep[0m  [4/42], [94mLoss[0m : 10.73431
[1mStep[0m  [8/42], [94mLoss[0m : 10.69379
[1mStep[0m  [12/42], [94mLoss[0m : 10.38017
[1mStep[0m  [16/42], [94mLoss[0m : 10.54764
[1mStep[0m  [20/42], [94mLoss[0m : 10.79007
[1mStep[0m  [24/42], [94mLoss[0m : 10.19130
[1mStep[0m  [28/42], [94mLoss[0m : 10.01816
[1mStep[0m  [32/42], [94mLoss[0m : 10.50139
[1mStep[0m  [36/42], [94mLoss[0m : 10.30575
[1mStep[0m  [40/42], [94mLoss[0m : 10.65108

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.515, [92mTest[0m: 10.424, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.58907
[1mStep[0m  [4/42], [94mLoss[0m : 10.38058
[1mStep[0m  [8/42], [94mLoss[0m : 10.39525
[1mStep[0m  [12/42], [94mLoss[0m : 10.26720
[1mStep[0m  [16/42], [94mLoss[0m : 10.50277
[1mStep[0m  [20/42], [94mLoss[0m : 10.72924
[1mStep[0m  [24/42], [94mLoss[0m : 10.38703
[1mStep[0m  [28/42], [94mLoss[0m : 10.19082
[1mStep[0m  [32/42], [94mLoss[0m : 10.61732
[1mStep[0m  [36/42], [94mLoss[0m : 10.33467
[1mStep[0m  [40/42], [94mLoss[0m : 10.43834

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.502, [92mTest[0m: 10.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.20287
[1mStep[0m  [4/42], [94mLoss[0m : 10.32375
[1mStep[0m  [8/42], [94mLoss[0m : 10.48461
[1mStep[0m  [12/42], [94mLoss[0m : 10.42535
[1mStep[0m  [16/42], [94mLoss[0m : 10.62687
[1mStep[0m  [20/42], [94mLoss[0m : 10.71616
[1mStep[0m  [24/42], [94mLoss[0m : 10.57730
[1mStep[0m  [28/42], [94mLoss[0m : 10.44722
[1mStep[0m  [32/42], [94mLoss[0m : 10.49480
[1mStep[0m  [36/42], [94mLoss[0m : 10.67018
[1mStep[0m  [40/42], [94mLoss[0m : 10.68810

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.483, [92mTest[0m: 10.414, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.81868
[1mStep[0m  [4/42], [94mLoss[0m : 10.53097
[1mStep[0m  [8/42], [94mLoss[0m : 10.26009
[1mStep[0m  [12/42], [94mLoss[0m : 10.39744
[1mStep[0m  [16/42], [94mLoss[0m : 10.72131
[1mStep[0m  [20/42], [94mLoss[0m : 10.35000
[1mStep[0m  [24/42], [94mLoss[0m : 10.42015
[1mStep[0m  [28/42], [94mLoss[0m : 10.77026
[1mStep[0m  [32/42], [94mLoss[0m : 10.24926
[1mStep[0m  [36/42], [94mLoss[0m : 10.50674
[1mStep[0m  [40/42], [94mLoss[0m : 10.71733

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.473, [92mTest[0m: 10.364, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95390
[1mStep[0m  [4/42], [94mLoss[0m : 10.27020
[1mStep[0m  [8/42], [94mLoss[0m : 10.50774
[1mStep[0m  [12/42], [94mLoss[0m : 10.66438
[1mStep[0m  [16/42], [94mLoss[0m : 10.31098
[1mStep[0m  [20/42], [94mLoss[0m : 10.53895
[1mStep[0m  [24/42], [94mLoss[0m : 10.51916
[1mStep[0m  [28/42], [94mLoss[0m : 10.77312
[1mStep[0m  [32/42], [94mLoss[0m : 10.29575
[1mStep[0m  [36/42], [94mLoss[0m : 10.21775
[1mStep[0m  [40/42], [94mLoss[0m : 10.29030

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.459, [92mTest[0m: 10.374, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.66611
[1mStep[0m  [4/42], [94mLoss[0m : 10.60231
[1mStep[0m  [8/42], [94mLoss[0m : 10.56211
[1mStep[0m  [12/42], [94mLoss[0m : 10.59464
[1mStep[0m  [16/42], [94mLoss[0m : 10.43337
[1mStep[0m  [20/42], [94mLoss[0m : 10.59333
[1mStep[0m  [24/42], [94mLoss[0m : 10.05285
[1mStep[0m  [28/42], [94mLoss[0m : 10.25581
[1mStep[0m  [32/42], [94mLoss[0m : 10.44471
[1mStep[0m  [36/42], [94mLoss[0m : 10.26333
[1mStep[0m  [40/42], [94mLoss[0m : 10.57736

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.445, [92mTest[0m: 10.357, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.27615
[1mStep[0m  [4/42], [94mLoss[0m : 10.48598
[1mStep[0m  [8/42], [94mLoss[0m : 10.22335
[1mStep[0m  [12/42], [94mLoss[0m : 10.08603
[1mStep[0m  [16/42], [94mLoss[0m : 10.17543
[1mStep[0m  [20/42], [94mLoss[0m : 10.38121
[1mStep[0m  [24/42], [94mLoss[0m : 10.52851
[1mStep[0m  [28/42], [94mLoss[0m : 10.59716
[1mStep[0m  [32/42], [94mLoss[0m : 10.64098
[1mStep[0m  [36/42], [94mLoss[0m : 10.57419
[1mStep[0m  [40/42], [94mLoss[0m : 11.13778

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.436, [92mTest[0m: 10.334, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.04091
[1mStep[0m  [4/42], [94mLoss[0m : 10.83652
[1mStep[0m  [8/42], [94mLoss[0m : 10.65817
[1mStep[0m  [12/42], [94mLoss[0m : 10.29949
[1mStep[0m  [16/42], [94mLoss[0m : 9.99353
[1mStep[0m  [20/42], [94mLoss[0m : 10.52305
[1mStep[0m  [24/42], [94mLoss[0m : 10.68321
[1mStep[0m  [28/42], [94mLoss[0m : 10.68887
[1mStep[0m  [32/42], [94mLoss[0m : 10.87938
[1mStep[0m  [36/42], [94mLoss[0m : 10.17659
[1mStep[0m  [40/42], [94mLoss[0m : 10.06048

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.425, [92mTest[0m: 10.331, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.55840
[1mStep[0m  [4/42], [94mLoss[0m : 10.35014
[1mStep[0m  [8/42], [94mLoss[0m : 10.54432
[1mStep[0m  [12/42], [94mLoss[0m : 10.04972
[1mStep[0m  [16/42], [94mLoss[0m : 10.40333
[1mStep[0m  [20/42], [94mLoss[0m : 10.33767
[1mStep[0m  [24/42], [94mLoss[0m : 10.50962
[1mStep[0m  [28/42], [94mLoss[0m : 10.50058
[1mStep[0m  [32/42], [94mLoss[0m : 10.88678
[1mStep[0m  [36/42], [94mLoss[0m : 10.14452
[1mStep[0m  [40/42], [94mLoss[0m : 10.18733

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.405, [92mTest[0m: 10.282, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19150
[1mStep[0m  [4/42], [94mLoss[0m : 10.48657
[1mStep[0m  [8/42], [94mLoss[0m : 10.80142
[1mStep[0m  [12/42], [94mLoss[0m : 10.10112
[1mStep[0m  [16/42], [94mLoss[0m : 10.09066
[1mStep[0m  [20/42], [94mLoss[0m : 10.62686
[1mStep[0m  [24/42], [94mLoss[0m : 10.26846
[1mStep[0m  [28/42], [94mLoss[0m : 10.50280
[1mStep[0m  [32/42], [94mLoss[0m : 10.14159
[1mStep[0m  [36/42], [94mLoss[0m : 10.07163
[1mStep[0m  [40/42], [94mLoss[0m : 10.39761

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.396, [92mTest[0m: 10.306, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.39243
[1mStep[0m  [4/42], [94mLoss[0m : 10.31412
[1mStep[0m  [8/42], [94mLoss[0m : 10.28929
[1mStep[0m  [12/42], [94mLoss[0m : 10.35023
[1mStep[0m  [16/42], [94mLoss[0m : 10.60060
[1mStep[0m  [20/42], [94mLoss[0m : 10.01559
[1mStep[0m  [24/42], [94mLoss[0m : 10.28966
[1mStep[0m  [28/42], [94mLoss[0m : 10.49040
[1mStep[0m  [32/42], [94mLoss[0m : 10.18031
[1mStep[0m  [36/42], [94mLoss[0m : 10.10081
[1mStep[0m  [40/42], [94mLoss[0m : 10.33084

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.271, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23538
[1mStep[0m  [4/42], [94mLoss[0m : 10.30000
[1mStep[0m  [8/42], [94mLoss[0m : 10.35364
[1mStep[0m  [12/42], [94mLoss[0m : 10.52666
[1mStep[0m  [16/42], [94mLoss[0m : 10.16922
[1mStep[0m  [20/42], [94mLoss[0m : 10.84308
[1mStep[0m  [24/42], [94mLoss[0m : 10.20992
[1mStep[0m  [28/42], [94mLoss[0m : 10.11676
[1mStep[0m  [32/42], [94mLoss[0m : 10.69149
[1mStep[0m  [36/42], [94mLoss[0m : 10.59163
[1mStep[0m  [40/42], [94mLoss[0m : 10.23760

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.368, [92mTest[0m: 10.276, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.27044
[1mStep[0m  [4/42], [94mLoss[0m : 10.02172
[1mStep[0m  [8/42], [94mLoss[0m : 10.51917
[1mStep[0m  [12/42], [94mLoss[0m : 10.16768
[1mStep[0m  [16/42], [94mLoss[0m : 10.35534
[1mStep[0m  [20/42], [94mLoss[0m : 10.32109
[1mStep[0m  [24/42], [94mLoss[0m : 10.25410
[1mStep[0m  [28/42], [94mLoss[0m : 10.00744
[1mStep[0m  [32/42], [94mLoss[0m : 10.43894
[1mStep[0m  [36/42], [94mLoss[0m : 10.58595
[1mStep[0m  [40/42], [94mLoss[0m : 10.29630

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.352, [92mTest[0m: 10.249, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.41781
[1mStep[0m  [4/42], [94mLoss[0m : 10.63538
[1mStep[0m  [8/42], [94mLoss[0m : 10.50893
[1mStep[0m  [12/42], [94mLoss[0m : 10.62538
[1mStep[0m  [16/42], [94mLoss[0m : 10.44756
[1mStep[0m  [20/42], [94mLoss[0m : 10.52062
[1mStep[0m  [24/42], [94mLoss[0m : 9.73729
[1mStep[0m  [28/42], [94mLoss[0m : 9.69461
[1mStep[0m  [32/42], [94mLoss[0m : 10.59444
[1mStep[0m  [36/42], [94mLoss[0m : 10.33304
[1mStep[0m  [40/42], [94mLoss[0m : 10.15460

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.339, [92mTest[0m: 10.244, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.217
====================================

Phase 2 - Evaluation MAE:  10.217419283730644
MAE score P1      10.667098
MAE score P2      10.217419
loss              10.339014
learning_rate        0.0001
batch_size              256
hidden_sizes          [100]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 3, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.12153
[1mStep[0m  [4/42], [94mLoss[0m : 11.31394
[1mStep[0m  [8/42], [94mLoss[0m : 11.26901
[1mStep[0m  [12/42], [94mLoss[0m : 10.76888
[1mStep[0m  [16/42], [94mLoss[0m : 10.61751
[1mStep[0m  [20/42], [94mLoss[0m : 10.86526
[1mStep[0m  [24/42], [94mLoss[0m : 10.92130
[1mStep[0m  [28/42], [94mLoss[0m : 10.83137
[1mStep[0m  [32/42], [94mLoss[0m : 10.46839
[1mStep[0m  [36/42], [94mLoss[0m : 11.06954
[1mStep[0m  [40/42], [94mLoss[0m : 11.04015

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.900, [92mTest[0m: 10.791, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87519
[1mStep[0m  [4/42], [94mLoss[0m : 10.60203
[1mStep[0m  [8/42], [94mLoss[0m : 10.95785
[1mStep[0m  [12/42], [94mLoss[0m : 10.95693
[1mStep[0m  [16/42], [94mLoss[0m : 10.65236
[1mStep[0m  [20/42], [94mLoss[0m : 10.65232
[1mStep[0m  [24/42], [94mLoss[0m : 10.93017
[1mStep[0m  [28/42], [94mLoss[0m : 10.84460
[1mStep[0m  [32/42], [94mLoss[0m : 11.11719
[1mStep[0m  [36/42], [94mLoss[0m : 11.22384
[1mStep[0m  [40/42], [94mLoss[0m : 11.10111

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.859, [92mTest[0m: 10.895, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68004
[1mStep[0m  [4/42], [94mLoss[0m : 10.70459
[1mStep[0m  [8/42], [94mLoss[0m : 10.64865
[1mStep[0m  [12/42], [94mLoss[0m : 10.76558
[1mStep[0m  [16/42], [94mLoss[0m : 10.97787
[1mStep[0m  [20/42], [94mLoss[0m : 10.94673
[1mStep[0m  [24/42], [94mLoss[0m : 10.89382
[1mStep[0m  [28/42], [94mLoss[0m : 10.31602
[1mStep[0m  [32/42], [94mLoss[0m : 10.43530
[1mStep[0m  [36/42], [94mLoss[0m : 11.32563
[1mStep[0m  [40/42], [94mLoss[0m : 10.87505

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.808, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.99494
[1mStep[0m  [4/42], [94mLoss[0m : 10.84263
[1mStep[0m  [8/42], [94mLoss[0m : 10.52052
[1mStep[0m  [12/42], [94mLoss[0m : 10.75883
[1mStep[0m  [16/42], [94mLoss[0m : 10.70651
[1mStep[0m  [20/42], [94mLoss[0m : 10.57359
[1mStep[0m  [24/42], [94mLoss[0m : 10.84466
[1mStep[0m  [28/42], [94mLoss[0m : 11.11523
[1mStep[0m  [32/42], [94mLoss[0m : 10.79014
[1mStep[0m  [36/42], [94mLoss[0m : 10.69892
[1mStep[0m  [40/42], [94mLoss[0m : 10.42036

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.773, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94497
[1mStep[0m  [4/42], [94mLoss[0m : 10.58010
[1mStep[0m  [8/42], [94mLoss[0m : 10.72295
[1mStep[0m  [12/42], [94mLoss[0m : 10.75602
[1mStep[0m  [16/42], [94mLoss[0m : 10.79348
[1mStep[0m  [20/42], [94mLoss[0m : 10.55411
[1mStep[0m  [24/42], [94mLoss[0m : 10.99052
[1mStep[0m  [28/42], [94mLoss[0m : 10.71303
[1mStep[0m  [32/42], [94mLoss[0m : 10.80384
[1mStep[0m  [36/42], [94mLoss[0m : 11.00308
[1mStep[0m  [40/42], [94mLoss[0m : 10.09999

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.721, [92mTest[0m: 10.830, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23635
[1mStep[0m  [4/42], [94mLoss[0m : 10.63965
[1mStep[0m  [8/42], [94mLoss[0m : 10.83551
[1mStep[0m  [12/42], [94mLoss[0m : 10.64469
[1mStep[0m  [16/42], [94mLoss[0m : 11.15883
[1mStep[0m  [20/42], [94mLoss[0m : 10.56781
[1mStep[0m  [24/42], [94mLoss[0m : 10.96777
[1mStep[0m  [28/42], [94mLoss[0m : 10.87955
[1mStep[0m  [32/42], [94mLoss[0m : 10.80344
[1mStep[0m  [36/42], [94mLoss[0m : 10.54668
[1mStep[0m  [40/42], [94mLoss[0m : 10.43334

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.695, [92mTest[0m: 10.786, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36719
[1mStep[0m  [4/42], [94mLoss[0m : 10.90341
[1mStep[0m  [8/42], [94mLoss[0m : 10.67081
[1mStep[0m  [12/42], [94mLoss[0m : 10.77538
[1mStep[0m  [16/42], [94mLoss[0m : 10.53699
[1mStep[0m  [20/42], [94mLoss[0m : 10.52908
[1mStep[0m  [24/42], [94mLoss[0m : 10.74871
[1mStep[0m  [28/42], [94mLoss[0m : 10.69335
[1mStep[0m  [32/42], [94mLoss[0m : 10.68401
[1mStep[0m  [36/42], [94mLoss[0m : 10.58900
[1mStep[0m  [40/42], [94mLoss[0m : 10.27931

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.780, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.96537
[1mStep[0m  [4/42], [94mLoss[0m : 10.33982
[1mStep[0m  [8/42], [94mLoss[0m : 10.25046
[1mStep[0m  [12/42], [94mLoss[0m : 10.59969
[1mStep[0m  [16/42], [94mLoss[0m : 10.73532
[1mStep[0m  [20/42], [94mLoss[0m : 10.34148
[1mStep[0m  [24/42], [94mLoss[0m : 10.72094
[1mStep[0m  [28/42], [94mLoss[0m : 10.67103
[1mStep[0m  [32/42], [94mLoss[0m : 10.62884
[1mStep[0m  [36/42], [94mLoss[0m : 10.16185
[1mStep[0m  [40/42], [94mLoss[0m : 10.61957

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.744, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.34617
[1mStep[0m  [4/42], [94mLoss[0m : 10.93506
[1mStep[0m  [8/42], [94mLoss[0m : 10.54105
[1mStep[0m  [12/42], [94mLoss[0m : 10.56591
[1mStep[0m  [16/42], [94mLoss[0m : 10.55564
[1mStep[0m  [20/42], [94mLoss[0m : 10.37780
[1mStep[0m  [24/42], [94mLoss[0m : 10.99674
[1mStep[0m  [28/42], [94mLoss[0m : 10.44082
[1mStep[0m  [32/42], [94mLoss[0m : 10.53447
[1mStep[0m  [36/42], [94mLoss[0m : 10.50145
[1mStep[0m  [40/42], [94mLoss[0m : 10.79132

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.577, [92mTest[0m: 10.726, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.80452
[1mStep[0m  [4/42], [94mLoss[0m : 10.74386
[1mStep[0m  [8/42], [94mLoss[0m : 10.54487
[1mStep[0m  [12/42], [94mLoss[0m : 10.15576
[1mStep[0m  [16/42], [94mLoss[0m : 10.33423
[1mStep[0m  [20/42], [94mLoss[0m : 10.37797
[1mStep[0m  [24/42], [94mLoss[0m : 10.82754
[1mStep[0m  [28/42], [94mLoss[0m : 10.63427
[1mStep[0m  [32/42], [94mLoss[0m : 10.23741
[1mStep[0m  [36/42], [94mLoss[0m : 10.68878
[1mStep[0m  [40/42], [94mLoss[0m : 10.34723

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.538, [92mTest[0m: 10.700, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.02080
[1mStep[0m  [4/42], [94mLoss[0m : 10.79540
[1mStep[0m  [8/42], [94mLoss[0m : 10.25086
[1mStep[0m  [12/42], [94mLoss[0m : 10.65207
[1mStep[0m  [16/42], [94mLoss[0m : 10.64751
[1mStep[0m  [20/42], [94mLoss[0m : 10.33317
[1mStep[0m  [24/42], [94mLoss[0m : 10.34248
[1mStep[0m  [28/42], [94mLoss[0m : 10.33091
[1mStep[0m  [32/42], [94mLoss[0m : 10.55436
[1mStep[0m  [36/42], [94mLoss[0m : 9.95325
[1mStep[0m  [40/42], [94mLoss[0m : 10.35828

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.492, [92mTest[0m: 10.671, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.09255
[1mStep[0m  [4/42], [94mLoss[0m : 10.24044
[1mStep[0m  [8/42], [94mLoss[0m : 10.36125
[1mStep[0m  [12/42], [94mLoss[0m : 10.56795
[1mStep[0m  [16/42], [94mLoss[0m : 10.31314
[1mStep[0m  [20/42], [94mLoss[0m : 10.54470
[1mStep[0m  [24/42], [94mLoss[0m : 10.22614
[1mStep[0m  [28/42], [94mLoss[0m : 10.53904
[1mStep[0m  [32/42], [94mLoss[0m : 10.59310
[1mStep[0m  [36/42], [94mLoss[0m : 10.36687
[1mStep[0m  [40/42], [94mLoss[0m : 10.48069

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.456, [92mTest[0m: 10.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49878
[1mStep[0m  [4/42], [94mLoss[0m : 10.65268
[1mStep[0m  [8/42], [94mLoss[0m : 10.80461
[1mStep[0m  [12/42], [94mLoss[0m : 10.23112
[1mStep[0m  [16/42], [94mLoss[0m : 10.36834
[1mStep[0m  [20/42], [94mLoss[0m : 10.29390
[1mStep[0m  [24/42], [94mLoss[0m : 10.37379
[1mStep[0m  [28/42], [94mLoss[0m : 10.38906
[1mStep[0m  [32/42], [94mLoss[0m : 10.50187
[1mStep[0m  [36/42], [94mLoss[0m : 10.33601
[1mStep[0m  [40/42], [94mLoss[0m : 10.56500

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.421, [92mTest[0m: 10.610, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.91941
[1mStep[0m  [4/42], [94mLoss[0m : 10.38238
[1mStep[0m  [8/42], [94mLoss[0m : 10.41831
[1mStep[0m  [12/42], [94mLoss[0m : 10.66580
[1mStep[0m  [16/42], [94mLoss[0m : 9.90515
[1mStep[0m  [20/42], [94mLoss[0m : 10.58666
[1mStep[0m  [24/42], [94mLoss[0m : 10.67388
[1mStep[0m  [28/42], [94mLoss[0m : 10.44431
[1mStep[0m  [32/42], [94mLoss[0m : 10.40823
[1mStep[0m  [36/42], [94mLoss[0m : 10.34847
[1mStep[0m  [40/42], [94mLoss[0m : 10.59049

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.374, [92mTest[0m: 10.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49866
[1mStep[0m  [4/42], [94mLoss[0m : 10.49462
[1mStep[0m  [8/42], [94mLoss[0m : 10.27741
[1mStep[0m  [12/42], [94mLoss[0m : 10.19382
[1mStep[0m  [16/42], [94mLoss[0m : 10.35324
[1mStep[0m  [20/42], [94mLoss[0m : 10.06346
[1mStep[0m  [24/42], [94mLoss[0m : 10.25773
[1mStep[0m  [28/42], [94mLoss[0m : 10.39276
[1mStep[0m  [32/42], [94mLoss[0m : 10.54745
[1mStep[0m  [36/42], [94mLoss[0m : 10.32751
[1mStep[0m  [40/42], [94mLoss[0m : 10.32203

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.343, [92mTest[0m: 10.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83733
[1mStep[0m  [4/42], [94mLoss[0m : 10.30380
[1mStep[0m  [8/42], [94mLoss[0m : 10.33003
[1mStep[0m  [12/42], [94mLoss[0m : 10.39496
[1mStep[0m  [16/42], [94mLoss[0m : 9.95794
[1mStep[0m  [20/42], [94mLoss[0m : 10.36157
[1mStep[0m  [24/42], [94mLoss[0m : 10.66506
[1mStep[0m  [28/42], [94mLoss[0m : 10.18810
[1mStep[0m  [32/42], [94mLoss[0m : 10.04789
[1mStep[0m  [36/42], [94mLoss[0m : 10.43315
[1mStep[0m  [40/42], [94mLoss[0m : 10.46148

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.302, [92mTest[0m: 10.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.30927
[1mStep[0m  [4/42], [94mLoss[0m : 10.00556
[1mStep[0m  [8/42], [94mLoss[0m : 10.62854
[1mStep[0m  [12/42], [94mLoss[0m : 10.50850
[1mStep[0m  [16/42], [94mLoss[0m : 10.09414
[1mStep[0m  [20/42], [94mLoss[0m : 10.42710
[1mStep[0m  [24/42], [94mLoss[0m : 9.59169
[1mStep[0m  [28/42], [94mLoss[0m : 9.88105
[1mStep[0m  [32/42], [94mLoss[0m : 10.72156
[1mStep[0m  [36/42], [94mLoss[0m : 10.08642
[1mStep[0m  [40/42], [94mLoss[0m : 10.35387

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.261, [92mTest[0m: 10.504, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.07948
[1mStep[0m  [4/42], [94mLoss[0m : 10.09748
[1mStep[0m  [8/42], [94mLoss[0m : 10.80640
[1mStep[0m  [12/42], [94mLoss[0m : 10.44366
[1mStep[0m  [16/42], [94mLoss[0m : 10.07197
[1mStep[0m  [20/42], [94mLoss[0m : 9.94861
[1mStep[0m  [24/42], [94mLoss[0m : 10.51818
[1mStep[0m  [28/42], [94mLoss[0m : 10.14928
[1mStep[0m  [32/42], [94mLoss[0m : 10.19316
[1mStep[0m  [36/42], [94mLoss[0m : 10.36688
[1mStep[0m  [40/42], [94mLoss[0m : 10.17469

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.221, [92mTest[0m: 10.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.71318
[1mStep[0m  [4/42], [94mLoss[0m : 10.72612
[1mStep[0m  [8/42], [94mLoss[0m : 10.01075
[1mStep[0m  [12/42], [94mLoss[0m : 10.09010
[1mStep[0m  [16/42], [94mLoss[0m : 9.82122
[1mStep[0m  [20/42], [94mLoss[0m : 10.03999
[1mStep[0m  [24/42], [94mLoss[0m : 10.09345
[1mStep[0m  [28/42], [94mLoss[0m : 9.89609
[1mStep[0m  [32/42], [94mLoss[0m : 10.19890
[1mStep[0m  [36/42], [94mLoss[0m : 10.18476
[1mStep[0m  [40/42], [94mLoss[0m : 9.73318

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.169, [92mTest[0m: 10.449, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08770
[1mStep[0m  [4/42], [94mLoss[0m : 9.88201
[1mStep[0m  [8/42], [94mLoss[0m : 10.38825
[1mStep[0m  [12/42], [94mLoss[0m : 10.41947
[1mStep[0m  [16/42], [94mLoss[0m : 10.02236
[1mStep[0m  [20/42], [94mLoss[0m : 10.37136
[1mStep[0m  [24/42], [94mLoss[0m : 10.32348
[1mStep[0m  [28/42], [94mLoss[0m : 10.30266
[1mStep[0m  [32/42], [94mLoss[0m : 10.49705
[1mStep[0m  [36/42], [94mLoss[0m : 10.05863
[1mStep[0m  [40/42], [94mLoss[0m : 10.05600

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.142, [92mTest[0m: 10.432, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.24840
[1mStep[0m  [4/42], [94mLoss[0m : 9.77245
[1mStep[0m  [8/42], [94mLoss[0m : 10.01143
[1mStep[0m  [12/42], [94mLoss[0m : 10.23731
[1mStep[0m  [16/42], [94mLoss[0m : 10.30610
[1mStep[0m  [20/42], [94mLoss[0m : 10.01186
[1mStep[0m  [24/42], [94mLoss[0m : 10.04740
[1mStep[0m  [28/42], [94mLoss[0m : 9.89917
[1mStep[0m  [32/42], [94mLoss[0m : 10.19786
[1mStep[0m  [36/42], [94mLoss[0m : 10.18151
[1mStep[0m  [40/42], [94mLoss[0m : 9.93931

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.107, [92mTest[0m: 10.413, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.07175
[1mStep[0m  [4/42], [94mLoss[0m : 9.93738
[1mStep[0m  [8/42], [94mLoss[0m : 9.97917
[1mStep[0m  [12/42], [94mLoss[0m : 9.90469
[1mStep[0m  [16/42], [94mLoss[0m : 9.86652
[1mStep[0m  [20/42], [94mLoss[0m : 10.31023
[1mStep[0m  [24/42], [94mLoss[0m : 10.54728
[1mStep[0m  [28/42], [94mLoss[0m : 10.31196
[1mStep[0m  [32/42], [94mLoss[0m : 9.81259
[1mStep[0m  [36/42], [94mLoss[0m : 10.17707
[1mStep[0m  [40/42], [94mLoss[0m : 10.25171

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.056, [92mTest[0m: 10.400, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89814
[1mStep[0m  [4/42], [94mLoss[0m : 9.61906
[1mStep[0m  [8/42], [94mLoss[0m : 10.03689
[1mStep[0m  [12/42], [94mLoss[0m : 9.81701
[1mStep[0m  [16/42], [94mLoss[0m : 10.45988
[1mStep[0m  [20/42], [94mLoss[0m : 10.36931
[1mStep[0m  [24/42], [94mLoss[0m : 9.81646
[1mStep[0m  [28/42], [94mLoss[0m : 10.24384
[1mStep[0m  [32/42], [94mLoss[0m : 9.79387
[1mStep[0m  [36/42], [94mLoss[0m : 9.83181
[1mStep[0m  [40/42], [94mLoss[0m : 9.57380

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.017, [92mTest[0m: 10.362, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.97983
[1mStep[0m  [4/42], [94mLoss[0m : 10.25405
[1mStep[0m  [8/42], [94mLoss[0m : 10.10392
[1mStep[0m  [12/42], [94mLoss[0m : 9.71083
[1mStep[0m  [16/42], [94mLoss[0m : 9.92206
[1mStep[0m  [20/42], [94mLoss[0m : 10.15201
[1mStep[0m  [24/42], [94mLoss[0m : 9.91735
[1mStep[0m  [28/42], [94mLoss[0m : 9.77770
[1mStep[0m  [32/42], [94mLoss[0m : 9.89157
[1mStep[0m  [36/42], [94mLoss[0m : 10.11929
[1mStep[0m  [40/42], [94mLoss[0m : 10.29533

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.988, [92mTest[0m: 10.339, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89144
[1mStep[0m  [4/42], [94mLoss[0m : 9.85434
[1mStep[0m  [8/42], [94mLoss[0m : 10.12562
[1mStep[0m  [12/42], [94mLoss[0m : 9.57314
[1mStep[0m  [16/42], [94mLoss[0m : 9.93107
[1mStep[0m  [20/42], [94mLoss[0m : 10.12623
[1mStep[0m  [24/42], [94mLoss[0m : 10.07885
[1mStep[0m  [28/42], [94mLoss[0m : 9.43859
[1mStep[0m  [32/42], [94mLoss[0m : 10.38773
[1mStep[0m  [36/42], [94mLoss[0m : 10.05840
[1mStep[0m  [40/42], [94mLoss[0m : 10.02038

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.953, [92mTest[0m: 10.300, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.14147
[1mStep[0m  [4/42], [94mLoss[0m : 9.80652
[1mStep[0m  [8/42], [94mLoss[0m : 9.50668
[1mStep[0m  [12/42], [94mLoss[0m : 9.57237
[1mStep[0m  [16/42], [94mLoss[0m : 9.86262
[1mStep[0m  [20/42], [94mLoss[0m : 10.18554
[1mStep[0m  [24/42], [94mLoss[0m : 9.70165
[1mStep[0m  [28/42], [94mLoss[0m : 9.79915
[1mStep[0m  [32/42], [94mLoss[0m : 9.75851
[1mStep[0m  [36/42], [94mLoss[0m : 10.03486
[1mStep[0m  [40/42], [94mLoss[0m : 9.86318

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.917, [92mTest[0m: 10.282, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.76803
[1mStep[0m  [4/42], [94mLoss[0m : 10.43327
[1mStep[0m  [8/42], [94mLoss[0m : 9.93233
[1mStep[0m  [12/42], [94mLoss[0m : 9.66183
[1mStep[0m  [16/42], [94mLoss[0m : 9.88472
[1mStep[0m  [20/42], [94mLoss[0m : 9.63311
[1mStep[0m  [24/42], [94mLoss[0m : 9.82112
[1mStep[0m  [28/42], [94mLoss[0m : 10.17269
[1mStep[0m  [32/42], [94mLoss[0m : 9.65363
[1mStep[0m  [36/42], [94mLoss[0m : 10.04811
[1mStep[0m  [40/42], [94mLoss[0m : 9.96277

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.887, [92mTest[0m: 10.264, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.72285
[1mStep[0m  [4/42], [94mLoss[0m : 9.56265
[1mStep[0m  [8/42], [94mLoss[0m : 10.37634
[1mStep[0m  [12/42], [94mLoss[0m : 9.69368
[1mStep[0m  [16/42], [94mLoss[0m : 9.76360
[1mStep[0m  [20/42], [94mLoss[0m : 9.77897
[1mStep[0m  [24/42], [94mLoss[0m : 10.09138
[1mStep[0m  [28/42], [94mLoss[0m : 9.85653
[1mStep[0m  [32/42], [94mLoss[0m : 10.22177
[1mStep[0m  [36/42], [94mLoss[0m : 9.90973
[1mStep[0m  [40/42], [94mLoss[0m : 9.72152

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.846, [92mTest[0m: 10.216, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.83505
[1mStep[0m  [4/42], [94mLoss[0m : 9.71465
[1mStep[0m  [8/42], [94mLoss[0m : 9.98337
[1mStep[0m  [12/42], [94mLoss[0m : 10.02248
[1mStep[0m  [16/42], [94mLoss[0m : 10.15690
[1mStep[0m  [20/42], [94mLoss[0m : 10.14792
[1mStep[0m  [24/42], [94mLoss[0m : 9.70015
[1mStep[0m  [28/42], [94mLoss[0m : 9.74174
[1mStep[0m  [32/42], [94mLoss[0m : 9.75387
[1mStep[0m  [36/42], [94mLoss[0m : 9.76179
[1mStep[0m  [40/42], [94mLoss[0m : 10.05179

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.821, [92mTest[0m: 10.225, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89723
[1mStep[0m  [4/42], [94mLoss[0m : 9.76577
[1mStep[0m  [8/42], [94mLoss[0m : 9.61481
[1mStep[0m  [12/42], [94mLoss[0m : 9.57117
[1mStep[0m  [16/42], [94mLoss[0m : 9.77509
[1mStep[0m  [20/42], [94mLoss[0m : 9.75256
[1mStep[0m  [24/42], [94mLoss[0m : 9.36875
[1mStep[0m  [28/42], [94mLoss[0m : 9.92948
[1mStep[0m  [32/42], [94mLoss[0m : 9.68495
[1mStep[0m  [36/42], [94mLoss[0m : 9.93747
[1mStep[0m  [40/42], [94mLoss[0m : 9.59794

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.783, [92mTest[0m: 10.205, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.164
====================================

Phase 1 - Evaluation MAE:  10.16397762298584
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 9.85610
[1mStep[0m  [4/42], [94mLoss[0m : 9.40708
[1mStep[0m  [8/42], [94mLoss[0m : 9.86716
[1mStep[0m  [12/42], [94mLoss[0m : 9.77869
[1mStep[0m  [16/42], [94mLoss[0m : 10.38183
[1mStep[0m  [20/42], [94mLoss[0m : 9.73495
[1mStep[0m  [24/42], [94mLoss[0m : 9.97785
[1mStep[0m  [28/42], [94mLoss[0m : 9.75752
[1mStep[0m  [32/42], [94mLoss[0m : 9.62058
[1mStep[0m  [36/42], [94mLoss[0m : 9.71523
[1mStep[0m  [40/42], [94mLoss[0m : 9.69144

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.745, [92mTest[0m: 10.173, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.42085
[1mStep[0m  [4/42], [94mLoss[0m : 9.65606
[1mStep[0m  [8/42], [94mLoss[0m : 9.79599
[1mStep[0m  [12/42], [94mLoss[0m : 9.39939
[1mStep[0m  [16/42], [94mLoss[0m : 9.80493
[1mStep[0m  [20/42], [94mLoss[0m : 9.98033
[1mStep[0m  [24/42], [94mLoss[0m : 9.81753
[1mStep[0m  [28/42], [94mLoss[0m : 9.74007
[1mStep[0m  [32/42], [94mLoss[0m : 9.59831
[1mStep[0m  [36/42], [94mLoss[0m : 9.62068
[1mStep[0m  [40/42], [94mLoss[0m : 9.71370

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.689, [92mTest[0m: 10.140, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25013
[1mStep[0m  [4/42], [94mLoss[0m : 9.62840
[1mStep[0m  [8/42], [94mLoss[0m : 9.98411
[1mStep[0m  [12/42], [94mLoss[0m : 9.49652
[1mStep[0m  [16/42], [94mLoss[0m : 9.70937
[1mStep[0m  [20/42], [94mLoss[0m : 9.79995
[1mStep[0m  [24/42], [94mLoss[0m : 9.55861
[1mStep[0m  [28/42], [94mLoss[0m : 9.40409
[1mStep[0m  [32/42], [94mLoss[0m : 9.92662
[1mStep[0m  [36/42], [94mLoss[0m : 9.68961
[1mStep[0m  [40/42], [94mLoss[0m : 9.92052

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.668, [92mTest[0m: 10.125, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.56463
[1mStep[0m  [4/42], [94mLoss[0m : 9.85890
[1mStep[0m  [8/42], [94mLoss[0m : 9.58863
[1mStep[0m  [12/42], [94mLoss[0m : 9.75465
[1mStep[0m  [16/42], [94mLoss[0m : 9.36004
[1mStep[0m  [20/42], [94mLoss[0m : 9.81622
[1mStep[0m  [24/42], [94mLoss[0m : 9.94336
[1mStep[0m  [28/42], [94mLoss[0m : 9.90593
[1mStep[0m  [32/42], [94mLoss[0m : 9.59847
[1mStep[0m  [36/42], [94mLoss[0m : 9.76892
[1mStep[0m  [40/42], [94mLoss[0m : 9.44859

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.618, [92mTest[0m: 10.081, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.35895
[1mStep[0m  [4/42], [94mLoss[0m : 9.51757
[1mStep[0m  [8/42], [94mLoss[0m : 9.42073
[1mStep[0m  [12/42], [94mLoss[0m : 9.54847
[1mStep[0m  [16/42], [94mLoss[0m : 9.72483
[1mStep[0m  [20/42], [94mLoss[0m : 9.67258
[1mStep[0m  [24/42], [94mLoss[0m : 9.37834
[1mStep[0m  [28/42], [94mLoss[0m : 9.27089
[1mStep[0m  [32/42], [94mLoss[0m : 9.56799
[1mStep[0m  [36/42], [94mLoss[0m : 9.73348
[1mStep[0m  [40/42], [94mLoss[0m : 10.07509

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.570, [92mTest[0m: 10.065, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.82274
[1mStep[0m  [4/42], [94mLoss[0m : 9.42803
[1mStep[0m  [8/42], [94mLoss[0m : 9.45292
[1mStep[0m  [12/42], [94mLoss[0m : 9.51494
[1mStep[0m  [16/42], [94mLoss[0m : 9.42123
[1mStep[0m  [20/42], [94mLoss[0m : 9.31581
[1mStep[0m  [24/42], [94mLoss[0m : 9.68664
[1mStep[0m  [28/42], [94mLoss[0m : 9.59554
[1mStep[0m  [32/42], [94mLoss[0m : 9.46979
[1mStep[0m  [36/42], [94mLoss[0m : 9.56361
[1mStep[0m  [40/42], [94mLoss[0m : 9.46489

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.538, [92mTest[0m: 10.027, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.41776
[1mStep[0m  [4/42], [94mLoss[0m : 9.54813
[1mStep[0m  [8/42], [94mLoss[0m : 9.35065
[1mStep[0m  [12/42], [94mLoss[0m : 9.82894
[1mStep[0m  [16/42], [94mLoss[0m : 9.45920
[1mStep[0m  [20/42], [94mLoss[0m : 9.83610
[1mStep[0m  [24/42], [94mLoss[0m : 9.40082
[1mStep[0m  [28/42], [94mLoss[0m : 9.50518
[1mStep[0m  [32/42], [94mLoss[0m : 9.41078
[1mStep[0m  [36/42], [94mLoss[0m : 9.64602
[1mStep[0m  [40/42], [94mLoss[0m : 9.47332

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.494, [92mTest[0m: 10.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.16853
[1mStep[0m  [4/42], [94mLoss[0m : 9.16209
[1mStep[0m  [8/42], [94mLoss[0m : 9.34724
[1mStep[0m  [12/42], [94mLoss[0m : 9.13991
[1mStep[0m  [16/42], [94mLoss[0m : 9.55203
[1mStep[0m  [20/42], [94mLoss[0m : 9.22749
[1mStep[0m  [24/42], [94mLoss[0m : 9.54992
[1mStep[0m  [28/42], [94mLoss[0m : 9.57694
[1mStep[0m  [32/42], [94mLoss[0m : 9.65254
[1mStep[0m  [36/42], [94mLoss[0m : 9.53939
[1mStep[0m  [40/42], [94mLoss[0m : 9.67394

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.453, [92mTest[0m: 9.970, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.23087
[1mStep[0m  [4/42], [94mLoss[0m : 9.47136
[1mStep[0m  [8/42], [94mLoss[0m : 9.67070
[1mStep[0m  [12/42], [94mLoss[0m : 9.32250
[1mStep[0m  [16/42], [94mLoss[0m : 9.77349
[1mStep[0m  [20/42], [94mLoss[0m : 9.26451
[1mStep[0m  [24/42], [94mLoss[0m : 9.50221
[1mStep[0m  [28/42], [94mLoss[0m : 9.24042
[1mStep[0m  [32/42], [94mLoss[0m : 9.23860
[1mStep[0m  [36/42], [94mLoss[0m : 9.10586
[1mStep[0m  [40/42], [94mLoss[0m : 9.40102

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.407, [92mTest[0m: 9.964, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.33664
[1mStep[0m  [4/42], [94mLoss[0m : 9.23968
[1mStep[0m  [8/42], [94mLoss[0m : 9.07200
[1mStep[0m  [12/42], [94mLoss[0m : 9.76265
[1mStep[0m  [16/42], [94mLoss[0m : 9.32150
[1mStep[0m  [20/42], [94mLoss[0m : 9.17542
[1mStep[0m  [24/42], [94mLoss[0m : 9.14755
[1mStep[0m  [28/42], [94mLoss[0m : 9.52300
[1mStep[0m  [32/42], [94mLoss[0m : 9.17962
[1mStep[0m  [36/42], [94mLoss[0m : 9.18531
[1mStep[0m  [40/42], [94mLoss[0m : 9.37112

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.372, [92mTest[0m: 9.924, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.49378
[1mStep[0m  [4/42], [94mLoss[0m : 9.34739
[1mStep[0m  [8/42], [94mLoss[0m : 9.17244
[1mStep[0m  [12/42], [94mLoss[0m : 9.06691
[1mStep[0m  [16/42], [94mLoss[0m : 8.88321
[1mStep[0m  [20/42], [94mLoss[0m : 9.14615
[1mStep[0m  [24/42], [94mLoss[0m : 9.37684
[1mStep[0m  [28/42], [94mLoss[0m : 9.05407
[1mStep[0m  [32/42], [94mLoss[0m : 9.42937
[1mStep[0m  [36/42], [94mLoss[0m : 9.50788
[1mStep[0m  [40/42], [94mLoss[0m : 9.70448

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.336, [92mTest[0m: 9.913, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.19883
[1mStep[0m  [4/42], [94mLoss[0m : 9.54823
[1mStep[0m  [8/42], [94mLoss[0m : 9.37698
[1mStep[0m  [12/42], [94mLoss[0m : 9.56484
[1mStep[0m  [16/42], [94mLoss[0m : 9.69699
[1mStep[0m  [20/42], [94mLoss[0m : 9.23769
[1mStep[0m  [24/42], [94mLoss[0m : 9.50031
[1mStep[0m  [28/42], [94mLoss[0m : 9.31708
[1mStep[0m  [32/42], [94mLoss[0m : 9.17329
[1mStep[0m  [36/42], [94mLoss[0m : 8.79389
[1mStep[0m  [40/42], [94mLoss[0m : 9.03378

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.285, [92mTest[0m: 9.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.32199
[1mStep[0m  [4/42], [94mLoss[0m : 9.27810
[1mStep[0m  [8/42], [94mLoss[0m : 9.05099
[1mStep[0m  [12/42], [94mLoss[0m : 9.24012
[1mStep[0m  [16/42], [94mLoss[0m : 9.17247
[1mStep[0m  [20/42], [94mLoss[0m : 9.56086
[1mStep[0m  [24/42], [94mLoss[0m : 9.11979
[1mStep[0m  [28/42], [94mLoss[0m : 9.25823
[1mStep[0m  [32/42], [94mLoss[0m : 9.22352
[1mStep[0m  [36/42], [94mLoss[0m : 9.05626
[1mStep[0m  [40/42], [94mLoss[0m : 9.17000

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.245, [92mTest[0m: 9.836, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.10716
[1mStep[0m  [4/42], [94mLoss[0m : 9.06871
[1mStep[0m  [8/42], [94mLoss[0m : 8.92988
[1mStep[0m  [12/42], [94mLoss[0m : 9.52522
[1mStep[0m  [16/42], [94mLoss[0m : 9.57223
[1mStep[0m  [20/42], [94mLoss[0m : 9.44252
[1mStep[0m  [24/42], [94mLoss[0m : 8.97800
[1mStep[0m  [28/42], [94mLoss[0m : 9.05726
[1mStep[0m  [32/42], [94mLoss[0m : 9.12377
[1mStep[0m  [36/42], [94mLoss[0m : 9.44790
[1mStep[0m  [40/42], [94mLoss[0m : 9.40503

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.207, [92mTest[0m: 9.846, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.56612
[1mStep[0m  [4/42], [94mLoss[0m : 9.21243
[1mStep[0m  [8/42], [94mLoss[0m : 9.21318
[1mStep[0m  [12/42], [94mLoss[0m : 9.00439
[1mStep[0m  [16/42], [94mLoss[0m : 9.62065
[1mStep[0m  [20/42], [94mLoss[0m : 8.96366
[1mStep[0m  [24/42], [94mLoss[0m : 9.44309
[1mStep[0m  [28/42], [94mLoss[0m : 9.12127
[1mStep[0m  [32/42], [94mLoss[0m : 8.86866
[1mStep[0m  [36/42], [94mLoss[0m : 8.92144
[1mStep[0m  [40/42], [94mLoss[0m : 9.18804

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.163, [92mTest[0m: 9.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.77580
[1mStep[0m  [4/42], [94mLoss[0m : 9.00971
[1mStep[0m  [8/42], [94mLoss[0m : 9.46877
[1mStep[0m  [12/42], [94mLoss[0m : 9.17599
[1mStep[0m  [16/42], [94mLoss[0m : 9.13379
[1mStep[0m  [20/42], [94mLoss[0m : 9.26422
[1mStep[0m  [24/42], [94mLoss[0m : 8.73542
[1mStep[0m  [28/42], [94mLoss[0m : 9.27909
[1mStep[0m  [32/42], [94mLoss[0m : 8.95960
[1mStep[0m  [36/42], [94mLoss[0m : 9.26702
[1mStep[0m  [40/42], [94mLoss[0m : 9.39121

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.114, [92mTest[0m: 9.767, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.04250
[1mStep[0m  [4/42], [94mLoss[0m : 8.93574
[1mStep[0m  [8/42], [94mLoss[0m : 9.00344
[1mStep[0m  [12/42], [94mLoss[0m : 9.22950
[1mStep[0m  [16/42], [94mLoss[0m : 9.28178
[1mStep[0m  [20/42], [94mLoss[0m : 8.72283
[1mStep[0m  [24/42], [94mLoss[0m : 9.48130
[1mStep[0m  [28/42], [94mLoss[0m : 9.41331
[1mStep[0m  [32/42], [94mLoss[0m : 8.91623
[1mStep[0m  [36/42], [94mLoss[0m : 8.66046
[1mStep[0m  [40/42], [94mLoss[0m : 9.27566

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.071, [92mTest[0m: 9.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.82203
[1mStep[0m  [4/42], [94mLoss[0m : 8.99204
[1mStep[0m  [8/42], [94mLoss[0m : 8.83321
[1mStep[0m  [12/42], [94mLoss[0m : 8.87404
[1mStep[0m  [16/42], [94mLoss[0m : 8.81034
[1mStep[0m  [20/42], [94mLoss[0m : 8.98832
[1mStep[0m  [24/42], [94mLoss[0m : 9.24955
[1mStep[0m  [28/42], [94mLoss[0m : 8.95237
[1mStep[0m  [32/42], [94mLoss[0m : 8.49825
[1mStep[0m  [36/42], [94mLoss[0m : 9.27731
[1mStep[0m  [40/42], [94mLoss[0m : 9.28014

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.034, [92mTest[0m: 9.705, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.91331
[1mStep[0m  [4/42], [94mLoss[0m : 9.20936
[1mStep[0m  [8/42], [94mLoss[0m : 9.13023
[1mStep[0m  [12/42], [94mLoss[0m : 8.41422
[1mStep[0m  [16/42], [94mLoss[0m : 8.86589
[1mStep[0m  [20/42], [94mLoss[0m : 9.18148
[1mStep[0m  [24/42], [94mLoss[0m : 8.98829
[1mStep[0m  [28/42], [94mLoss[0m : 8.86043
[1mStep[0m  [32/42], [94mLoss[0m : 8.80093
[1mStep[0m  [36/42], [94mLoss[0m : 8.76257
[1mStep[0m  [40/42], [94mLoss[0m : 9.08343

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.000, [92mTest[0m: 9.685, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.02684
[1mStep[0m  [4/42], [94mLoss[0m : 9.15798
[1mStep[0m  [8/42], [94mLoss[0m : 8.89514
[1mStep[0m  [12/42], [94mLoss[0m : 8.85740
[1mStep[0m  [16/42], [94mLoss[0m : 9.13307
[1mStep[0m  [20/42], [94mLoss[0m : 9.02323
[1mStep[0m  [24/42], [94mLoss[0m : 9.32370
[1mStep[0m  [28/42], [94mLoss[0m : 8.51533
[1mStep[0m  [32/42], [94mLoss[0m : 8.91657
[1mStep[0m  [36/42], [94mLoss[0m : 9.03089
[1mStep[0m  [40/42], [94mLoss[0m : 8.77369

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.954, [92mTest[0m: 9.653, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.57487
[1mStep[0m  [4/42], [94mLoss[0m : 8.97078
[1mStep[0m  [8/42], [94mLoss[0m : 9.18969
[1mStep[0m  [12/42], [94mLoss[0m : 8.52434
[1mStep[0m  [16/42], [94mLoss[0m : 8.62042
[1mStep[0m  [20/42], [94mLoss[0m : 8.87373
[1mStep[0m  [24/42], [94mLoss[0m : 9.21574
[1mStep[0m  [28/42], [94mLoss[0m : 8.67924
[1mStep[0m  [32/42], [94mLoss[0m : 9.06813
[1mStep[0m  [36/42], [94mLoss[0m : 8.76092
[1mStep[0m  [40/42], [94mLoss[0m : 8.41549

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.906, [92mTest[0m: 9.621, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.61407
[1mStep[0m  [4/42], [94mLoss[0m : 8.76340
[1mStep[0m  [8/42], [94mLoss[0m : 9.20844
[1mStep[0m  [12/42], [94mLoss[0m : 8.52460
[1mStep[0m  [16/42], [94mLoss[0m : 8.67944
[1mStep[0m  [20/42], [94mLoss[0m : 9.10365
[1mStep[0m  [24/42], [94mLoss[0m : 8.93739
[1mStep[0m  [28/42], [94mLoss[0m : 8.86641
[1mStep[0m  [32/42], [94mLoss[0m : 8.68265
[1mStep[0m  [36/42], [94mLoss[0m : 9.02055
[1mStep[0m  [40/42], [94mLoss[0m : 8.96376

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.862, [92mTest[0m: 9.588, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.15359
[1mStep[0m  [4/42], [94mLoss[0m : 9.08757
[1mStep[0m  [8/42], [94mLoss[0m : 8.82226
[1mStep[0m  [12/42], [94mLoss[0m : 8.54888
[1mStep[0m  [16/42], [94mLoss[0m : 8.83061
[1mStep[0m  [20/42], [94mLoss[0m : 8.45126
[1mStep[0m  [24/42], [94mLoss[0m : 8.91776
[1mStep[0m  [28/42], [94mLoss[0m : 9.34885
[1mStep[0m  [32/42], [94mLoss[0m : 8.89139
[1mStep[0m  [36/42], [94mLoss[0m : 9.20481
[1mStep[0m  [40/42], [94mLoss[0m : 8.83532

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.848, [92mTest[0m: 9.586, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.79856
[1mStep[0m  [4/42], [94mLoss[0m : 8.88811
[1mStep[0m  [8/42], [94mLoss[0m : 8.76776
[1mStep[0m  [12/42], [94mLoss[0m : 8.88333
[1mStep[0m  [16/42], [94mLoss[0m : 8.76653
[1mStep[0m  [20/42], [94mLoss[0m : 8.97457
[1mStep[0m  [24/42], [94mLoss[0m : 8.74147
[1mStep[0m  [28/42], [94mLoss[0m : 8.44333
[1mStep[0m  [32/42], [94mLoss[0m : 8.92744
[1mStep[0m  [36/42], [94mLoss[0m : 8.63469
[1mStep[0m  [40/42], [94mLoss[0m : 8.87473

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.786, [92mTest[0m: 9.549, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.57985
[1mStep[0m  [4/42], [94mLoss[0m : 8.78377
[1mStep[0m  [8/42], [94mLoss[0m : 9.09785
[1mStep[0m  [12/42], [94mLoss[0m : 8.83167
[1mStep[0m  [16/42], [94mLoss[0m : 8.53061
[1mStep[0m  [20/42], [94mLoss[0m : 8.50992
[1mStep[0m  [24/42], [94mLoss[0m : 8.39976
[1mStep[0m  [28/42], [94mLoss[0m : 9.24045
[1mStep[0m  [32/42], [94mLoss[0m : 8.70182
[1mStep[0m  [36/42], [94mLoss[0m : 8.80371
[1mStep[0m  [40/42], [94mLoss[0m : 8.79557

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.765, [92mTest[0m: 9.528, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.46004
[1mStep[0m  [4/42], [94mLoss[0m : 8.37962
[1mStep[0m  [8/42], [94mLoss[0m : 8.71728
[1mStep[0m  [12/42], [94mLoss[0m : 8.22898
[1mStep[0m  [16/42], [94mLoss[0m : 8.32275
[1mStep[0m  [20/42], [94mLoss[0m : 8.79015
[1mStep[0m  [24/42], [94mLoss[0m : 9.27909
[1mStep[0m  [28/42], [94mLoss[0m : 9.02377
[1mStep[0m  [32/42], [94mLoss[0m : 8.85800
[1mStep[0m  [36/42], [94mLoss[0m : 8.50378
[1mStep[0m  [40/42], [94mLoss[0m : 8.51830

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.725, [92mTest[0m: 9.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.78631
[1mStep[0m  [4/42], [94mLoss[0m : 8.54681
[1mStep[0m  [8/42], [94mLoss[0m : 8.69218
[1mStep[0m  [12/42], [94mLoss[0m : 8.47693
[1mStep[0m  [16/42], [94mLoss[0m : 8.67109
[1mStep[0m  [20/42], [94mLoss[0m : 8.69287
[1mStep[0m  [24/42], [94mLoss[0m : 8.91830
[1mStep[0m  [28/42], [94mLoss[0m : 8.41299
[1mStep[0m  [32/42], [94mLoss[0m : 8.83738
[1mStep[0m  [36/42], [94mLoss[0m : 8.48502
[1mStep[0m  [40/42], [94mLoss[0m : 8.66078

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.692, [92mTest[0m: 9.459, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.77548
[1mStep[0m  [4/42], [94mLoss[0m : 8.78243
[1mStep[0m  [8/42], [94mLoss[0m : 9.05019
[1mStep[0m  [12/42], [94mLoss[0m : 8.69549
[1mStep[0m  [16/42], [94mLoss[0m : 8.17504
[1mStep[0m  [20/42], [94mLoss[0m : 8.77215
[1mStep[0m  [24/42], [94mLoss[0m : 8.70778
[1mStep[0m  [28/42], [94mLoss[0m : 8.70661
[1mStep[0m  [32/42], [94mLoss[0m : 8.34630
[1mStep[0m  [36/42], [94mLoss[0m : 8.27882
[1mStep[0m  [40/42], [94mLoss[0m : 8.73814

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.640, [92mTest[0m: 9.464, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.54859
[1mStep[0m  [4/42], [94mLoss[0m : 8.50443
[1mStep[0m  [8/42], [94mLoss[0m : 8.97059
[1mStep[0m  [12/42], [94mLoss[0m : 8.29153
[1mStep[0m  [16/42], [94mLoss[0m : 8.10748
[1mStep[0m  [20/42], [94mLoss[0m : 8.84461
[1mStep[0m  [24/42], [94mLoss[0m : 8.38232
[1mStep[0m  [28/42], [94mLoss[0m : 8.73668
[1mStep[0m  [32/42], [94mLoss[0m : 8.59699
[1mStep[0m  [36/42], [94mLoss[0m : 8.45662
[1mStep[0m  [40/42], [94mLoss[0m : 8.43146

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 8.611, [92mTest[0m: 9.418, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.72782
[1mStep[0m  [4/42], [94mLoss[0m : 8.64755
[1mStep[0m  [8/42], [94mLoss[0m : 8.62979
[1mStep[0m  [12/42], [94mLoss[0m : 8.69781
[1mStep[0m  [16/42], [94mLoss[0m : 8.65550
[1mStep[0m  [20/42], [94mLoss[0m : 8.15139
[1mStep[0m  [24/42], [94mLoss[0m : 8.26405
[1mStep[0m  [28/42], [94mLoss[0m : 8.41271
[1mStep[0m  [32/42], [94mLoss[0m : 8.02639
[1mStep[0m  [36/42], [94mLoss[0m : 8.51423
[1mStep[0m  [40/42], [94mLoss[0m : 8.83687

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 8.574, [92mTest[0m: 9.368, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.393
====================================

Phase 2 - Evaluation MAE:  9.39284883226667
MAE score P1      10.163978
MAE score P2       9.392849
loss               8.573877
learning_rate        0.0001
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay          0.001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.42835
[1mStep[0m  [4/42], [94mLoss[0m : 10.61558
[1mStep[0m  [8/42], [94mLoss[0m : 10.79872
[1mStep[0m  [12/42], [94mLoss[0m : 10.63146
[1mStep[0m  [16/42], [94mLoss[0m : 10.25769
[1mStep[0m  [20/42], [94mLoss[0m : 10.79447
[1mStep[0m  [24/42], [94mLoss[0m : 10.32593
[1mStep[0m  [28/42], [94mLoss[0m : 10.58788
[1mStep[0m  [32/42], [94mLoss[0m : 10.75148
[1mStep[0m  [36/42], [94mLoss[0m : 10.28097
[1mStep[0m  [40/42], [94mLoss[0m : 10.53701

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.612, [92mTest[0m: 10.739, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54094
[1mStep[0m  [4/42], [94mLoss[0m : 10.28995
[1mStep[0m  [8/42], [94mLoss[0m : 9.99058
[1mStep[0m  [12/42], [94mLoss[0m : 10.23445
[1mStep[0m  [16/42], [94mLoss[0m : 10.56219
[1mStep[0m  [20/42], [94mLoss[0m : 10.63872
[1mStep[0m  [24/42], [94mLoss[0m : 10.82374
[1mStep[0m  [28/42], [94mLoss[0m : 10.15373
[1mStep[0m  [32/42], [94mLoss[0m : 10.18765
[1mStep[0m  [36/42], [94mLoss[0m : 10.03912
[1mStep[0m  [40/42], [94mLoss[0m : 10.25289

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.312, [92mTest[0m: 10.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.36591
[1mStep[0m  [4/42], [94mLoss[0m : 10.27045
[1mStep[0m  [8/42], [94mLoss[0m : 10.14332
[1mStep[0m  [12/42], [94mLoss[0m : 10.30312
[1mStep[0m  [16/42], [94mLoss[0m : 9.70197
[1mStep[0m  [20/42], [94mLoss[0m : 10.18738
[1mStep[0m  [24/42], [94mLoss[0m : 10.31340
[1mStep[0m  [28/42], [94mLoss[0m : 9.74188
[1mStep[0m  [32/42], [94mLoss[0m : 10.12939
[1mStep[0m  [36/42], [94mLoss[0m : 9.51778
[1mStep[0m  [40/42], [94mLoss[0m : 9.71171

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.014, [92mTest[0m: 10.159, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.87954
[1mStep[0m  [4/42], [94mLoss[0m : 9.94926
[1mStep[0m  [8/42], [94mLoss[0m : 9.34963
[1mStep[0m  [12/42], [94mLoss[0m : 9.91695
[1mStep[0m  [16/42], [94mLoss[0m : 10.06371
[1mStep[0m  [20/42], [94mLoss[0m : 9.63603
[1mStep[0m  [24/42], [94mLoss[0m : 9.51238
[1mStep[0m  [28/42], [94mLoss[0m : 9.80516
[1mStep[0m  [32/42], [94mLoss[0m : 9.44312
[1mStep[0m  [36/42], [94mLoss[0m : 9.58319
[1mStep[0m  [40/42], [94mLoss[0m : 9.65602

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.713, [92mTest[0m: 9.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.61541
[1mStep[0m  [4/42], [94mLoss[0m : 9.68524
[1mStep[0m  [8/42], [94mLoss[0m : 9.36119
[1mStep[0m  [12/42], [94mLoss[0m : 9.69390
[1mStep[0m  [16/42], [94mLoss[0m : 9.79874
[1mStep[0m  [20/42], [94mLoss[0m : 9.42612
[1mStep[0m  [24/42], [94mLoss[0m : 9.33735
[1mStep[0m  [28/42], [94mLoss[0m : 9.10670
[1mStep[0m  [32/42], [94mLoss[0m : 9.48583
[1mStep[0m  [36/42], [94mLoss[0m : 9.13603
[1mStep[0m  [40/42], [94mLoss[0m : 9.56347

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.426, [92mTest[0m: 9.559, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.15894
[1mStep[0m  [4/42], [94mLoss[0m : 9.18082
[1mStep[0m  [8/42], [94mLoss[0m : 9.39003
[1mStep[0m  [12/42], [94mLoss[0m : 9.25844
[1mStep[0m  [16/42], [94mLoss[0m : 8.68418
[1mStep[0m  [20/42], [94mLoss[0m : 9.28806
[1mStep[0m  [24/42], [94mLoss[0m : 8.91386
[1mStep[0m  [28/42], [94mLoss[0m : 8.97920
[1mStep[0m  [32/42], [94mLoss[0m : 8.78570
[1mStep[0m  [36/42], [94mLoss[0m : 9.27561
[1mStep[0m  [40/42], [94mLoss[0m : 8.80035

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.116, [92mTest[0m: 9.273, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.24310
[1mStep[0m  [4/42], [94mLoss[0m : 8.85328
[1mStep[0m  [8/42], [94mLoss[0m : 8.83371
[1mStep[0m  [12/42], [94mLoss[0m : 8.90263
[1mStep[0m  [16/42], [94mLoss[0m : 8.54614
[1mStep[0m  [20/42], [94mLoss[0m : 9.22724
[1mStep[0m  [24/42], [94mLoss[0m : 9.06009
[1mStep[0m  [28/42], [94mLoss[0m : 8.65374
[1mStep[0m  [32/42], [94mLoss[0m : 8.47448
[1mStep[0m  [36/42], [94mLoss[0m : 9.28395
[1mStep[0m  [40/42], [94mLoss[0m : 8.74849

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.826, [92mTest[0m: 8.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.95146
[1mStep[0m  [4/42], [94mLoss[0m : 8.95861
[1mStep[0m  [8/42], [94mLoss[0m : 8.60078
[1mStep[0m  [12/42], [94mLoss[0m : 8.72912
[1mStep[0m  [16/42], [94mLoss[0m : 8.82262
[1mStep[0m  [20/42], [94mLoss[0m : 8.19971
[1mStep[0m  [24/42], [94mLoss[0m : 8.58667
[1mStep[0m  [28/42], [94mLoss[0m : 8.31043
[1mStep[0m  [32/42], [94mLoss[0m : 8.25469
[1mStep[0m  [36/42], [94mLoss[0m : 8.22256
[1mStep[0m  [40/42], [94mLoss[0m : 8.39613

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.531, [92mTest[0m: 8.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.08965
[1mStep[0m  [4/42], [94mLoss[0m : 8.38806
[1mStep[0m  [8/42], [94mLoss[0m : 8.08690
[1mStep[0m  [12/42], [94mLoss[0m : 8.35745
[1mStep[0m  [16/42], [94mLoss[0m : 8.31530
[1mStep[0m  [20/42], [94mLoss[0m : 8.72135
[1mStep[0m  [24/42], [94mLoss[0m : 8.08814
[1mStep[0m  [28/42], [94mLoss[0m : 7.65234
[1mStep[0m  [32/42], [94mLoss[0m : 7.88272
[1mStep[0m  [36/42], [94mLoss[0m : 7.99241
[1mStep[0m  [40/42], [94mLoss[0m : 7.93120

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.232, [92mTest[0m: 8.380, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.28630
[1mStep[0m  [4/42], [94mLoss[0m : 8.21904
[1mStep[0m  [8/42], [94mLoss[0m : 8.16469
[1mStep[0m  [12/42], [94mLoss[0m : 7.90255
[1mStep[0m  [16/42], [94mLoss[0m : 7.71704
[1mStep[0m  [20/42], [94mLoss[0m : 8.00183
[1mStep[0m  [24/42], [94mLoss[0m : 7.96581
[1mStep[0m  [28/42], [94mLoss[0m : 7.78522
[1mStep[0m  [32/42], [94mLoss[0m : 7.70836
[1mStep[0m  [36/42], [94mLoss[0m : 7.88638
[1mStep[0m  [40/42], [94mLoss[0m : 7.57099

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.933, [92mTest[0m: 8.084, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.71555
[1mStep[0m  [4/42], [94mLoss[0m : 7.85726
[1mStep[0m  [8/42], [94mLoss[0m : 8.47708
[1mStep[0m  [12/42], [94mLoss[0m : 7.91628
[1mStep[0m  [16/42], [94mLoss[0m : 7.81118
[1mStep[0m  [20/42], [94mLoss[0m : 7.80555
[1mStep[0m  [24/42], [94mLoss[0m : 7.68438
[1mStep[0m  [28/42], [94mLoss[0m : 7.75787
[1mStep[0m  [32/42], [94mLoss[0m : 7.52871
[1mStep[0m  [36/42], [94mLoss[0m : 7.60199
[1mStep[0m  [40/42], [94mLoss[0m : 7.28962

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.640, [92mTest[0m: 7.788, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.62493
[1mStep[0m  [4/42], [94mLoss[0m : 7.23880
[1mStep[0m  [8/42], [94mLoss[0m : 7.67016
[1mStep[0m  [12/42], [94mLoss[0m : 7.01439
[1mStep[0m  [16/42], [94mLoss[0m : 7.29452
[1mStep[0m  [20/42], [94mLoss[0m : 7.54633
[1mStep[0m  [24/42], [94mLoss[0m : 7.06222
[1mStep[0m  [28/42], [94mLoss[0m : 7.41838
[1mStep[0m  [32/42], [94mLoss[0m : 7.32889
[1mStep[0m  [36/42], [94mLoss[0m : 6.97666
[1mStep[0m  [40/42], [94mLoss[0m : 7.09699

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.340, [92mTest[0m: 7.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.16512
[1mStep[0m  [4/42], [94mLoss[0m : 7.24602
[1mStep[0m  [8/42], [94mLoss[0m : 7.26078
[1mStep[0m  [12/42], [94mLoss[0m : 7.26022
[1mStep[0m  [16/42], [94mLoss[0m : 7.15885
[1mStep[0m  [20/42], [94mLoss[0m : 7.09283
[1mStep[0m  [24/42], [94mLoss[0m : 6.62443
[1mStep[0m  [28/42], [94mLoss[0m : 6.88240
[1mStep[0m  [32/42], [94mLoss[0m : 7.19834
[1mStep[0m  [36/42], [94mLoss[0m : 6.93018
[1mStep[0m  [40/42], [94mLoss[0m : 6.89108

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.052, [92mTest[0m: 7.180, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.85378
[1mStep[0m  [4/42], [94mLoss[0m : 7.18618
[1mStep[0m  [8/42], [94mLoss[0m : 6.82466
[1mStep[0m  [12/42], [94mLoss[0m : 7.22139
[1mStep[0m  [16/42], [94mLoss[0m : 6.49808
[1mStep[0m  [20/42], [94mLoss[0m : 6.86631
[1mStep[0m  [24/42], [94mLoss[0m : 7.09953
[1mStep[0m  [28/42], [94mLoss[0m : 6.70739
[1mStep[0m  [32/42], [94mLoss[0m : 7.21760
[1mStep[0m  [36/42], [94mLoss[0m : 6.58899
[1mStep[0m  [40/42], [94mLoss[0m : 6.40137

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.749, [92mTest[0m: 6.892, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.76801
[1mStep[0m  [4/42], [94mLoss[0m : 6.46766
[1mStep[0m  [8/42], [94mLoss[0m : 6.33843
[1mStep[0m  [12/42], [94mLoss[0m : 6.66509
[1mStep[0m  [16/42], [94mLoss[0m : 6.41244
[1mStep[0m  [20/42], [94mLoss[0m : 6.39473
[1mStep[0m  [24/42], [94mLoss[0m : 6.30677
[1mStep[0m  [28/42], [94mLoss[0m : 6.76435
[1mStep[0m  [32/42], [94mLoss[0m : 6.36608
[1mStep[0m  [36/42], [94mLoss[0m : 6.18814
[1mStep[0m  [40/42], [94mLoss[0m : 6.38092

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.463, [92mTest[0m: 6.591, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.07854
[1mStep[0m  [4/42], [94mLoss[0m : 6.31717
[1mStep[0m  [8/42], [94mLoss[0m : 5.90738
[1mStep[0m  [12/42], [94mLoss[0m : 6.02787
[1mStep[0m  [16/42], [94mLoss[0m : 5.93833
[1mStep[0m  [20/42], [94mLoss[0m : 6.19827
[1mStep[0m  [24/42], [94mLoss[0m : 6.09776
[1mStep[0m  [28/42], [94mLoss[0m : 6.05623
[1mStep[0m  [32/42], [94mLoss[0m : 6.34535
[1mStep[0m  [36/42], [94mLoss[0m : 6.36982
[1mStep[0m  [40/42], [94mLoss[0m : 5.82581

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.153, [92mTest[0m: 6.295, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.01453
[1mStep[0m  [4/42], [94mLoss[0m : 6.24755
[1mStep[0m  [8/42], [94mLoss[0m : 6.02895
[1mStep[0m  [12/42], [94mLoss[0m : 6.08800
[1mStep[0m  [16/42], [94mLoss[0m : 5.74163
[1mStep[0m  [20/42], [94mLoss[0m : 5.99664
[1mStep[0m  [24/42], [94mLoss[0m : 5.73278
[1mStep[0m  [28/42], [94mLoss[0m : 5.57532
[1mStep[0m  [32/42], [94mLoss[0m : 6.27762
[1mStep[0m  [36/42], [94mLoss[0m : 5.64905
[1mStep[0m  [40/42], [94mLoss[0m : 5.58615

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.863, [92mTest[0m: 6.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.98441
[1mStep[0m  [4/42], [94mLoss[0m : 5.51898
[1mStep[0m  [8/42], [94mLoss[0m : 5.74593
[1mStep[0m  [12/42], [94mLoss[0m : 5.86241
[1mStep[0m  [16/42], [94mLoss[0m : 5.50586
[1mStep[0m  [20/42], [94mLoss[0m : 5.47349
[1mStep[0m  [24/42], [94mLoss[0m : 5.42639
[1mStep[0m  [28/42], [94mLoss[0m : 5.27374
[1mStep[0m  [32/42], [94mLoss[0m : 5.57438
[1mStep[0m  [36/42], [94mLoss[0m : 5.38998
[1mStep[0m  [40/42], [94mLoss[0m : 5.55876

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.571, [92mTest[0m: 5.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.58663
[1mStep[0m  [4/42], [94mLoss[0m : 5.54073
[1mStep[0m  [8/42], [94mLoss[0m : 5.83874
[1mStep[0m  [12/42], [94mLoss[0m : 5.57879
[1mStep[0m  [16/42], [94mLoss[0m : 4.98563
[1mStep[0m  [20/42], [94mLoss[0m : 5.35295
[1mStep[0m  [24/42], [94mLoss[0m : 5.17555
[1mStep[0m  [28/42], [94mLoss[0m : 5.18754
[1mStep[0m  [32/42], [94mLoss[0m : 5.27333
[1mStep[0m  [36/42], [94mLoss[0m : 5.50778
[1mStep[0m  [40/42], [94mLoss[0m : 4.92559

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.294, [92mTest[0m: 5.433, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.43915
[1mStep[0m  [4/42], [94mLoss[0m : 4.75846
[1mStep[0m  [8/42], [94mLoss[0m : 5.24241
[1mStep[0m  [12/42], [94mLoss[0m : 5.20868
[1mStep[0m  [16/42], [94mLoss[0m : 5.18595
[1mStep[0m  [20/42], [94mLoss[0m : 4.68909
[1mStep[0m  [24/42], [94mLoss[0m : 5.06141
[1mStep[0m  [28/42], [94mLoss[0m : 5.01231
[1mStep[0m  [32/42], [94mLoss[0m : 5.23012
[1mStep[0m  [36/42], [94mLoss[0m : 4.66909
[1mStep[0m  [40/42], [94mLoss[0m : 4.62204

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.037, [92mTest[0m: 5.161, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.88524
[1mStep[0m  [4/42], [94mLoss[0m : 5.05542
[1mStep[0m  [8/42], [94mLoss[0m : 4.83716
[1mStep[0m  [12/42], [94mLoss[0m : 4.99625
[1mStep[0m  [16/42], [94mLoss[0m : 4.72458
[1mStep[0m  [20/42], [94mLoss[0m : 4.76160
[1mStep[0m  [24/42], [94mLoss[0m : 4.74941
[1mStep[0m  [28/42], [94mLoss[0m : 5.02507
[1mStep[0m  [32/42], [94mLoss[0m : 4.98413
[1mStep[0m  [36/42], [94mLoss[0m : 4.49375
[1mStep[0m  [40/42], [94mLoss[0m : 4.47120

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.806, [92mTest[0m: 4.914, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.74161
[1mStep[0m  [4/42], [94mLoss[0m : 5.04496
[1mStep[0m  [8/42], [94mLoss[0m : 4.67390
[1mStep[0m  [12/42], [94mLoss[0m : 4.42726
[1mStep[0m  [16/42], [94mLoss[0m : 4.78830
[1mStep[0m  [20/42], [94mLoss[0m : 4.23940
[1mStep[0m  [24/42], [94mLoss[0m : 4.50001
[1mStep[0m  [28/42], [94mLoss[0m : 4.69512
[1mStep[0m  [32/42], [94mLoss[0m : 4.87239
[1mStep[0m  [36/42], [94mLoss[0m : 4.28041
[1mStep[0m  [40/42], [94mLoss[0m : 4.46701

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.606, [92mTest[0m: 4.705, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.12242
[1mStep[0m  [4/42], [94mLoss[0m : 4.86482
[1mStep[0m  [8/42], [94mLoss[0m : 4.86582
[1mStep[0m  [12/42], [94mLoss[0m : 4.56071
[1mStep[0m  [16/42], [94mLoss[0m : 4.15706
[1mStep[0m  [20/42], [94mLoss[0m : 4.01372
[1mStep[0m  [24/42], [94mLoss[0m : 3.96316
[1mStep[0m  [28/42], [94mLoss[0m : 4.19706
[1mStep[0m  [32/42], [94mLoss[0m : 4.41050
[1mStep[0m  [36/42], [94mLoss[0m : 4.68674
[1mStep[0m  [40/42], [94mLoss[0m : 4.49996

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.422, [92mTest[0m: 4.512, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.26889
[1mStep[0m  [4/42], [94mLoss[0m : 3.98764
[1mStep[0m  [8/42], [94mLoss[0m : 4.27725
[1mStep[0m  [12/42], [94mLoss[0m : 4.15424
[1mStep[0m  [16/42], [94mLoss[0m : 4.45953
[1mStep[0m  [20/42], [94mLoss[0m : 3.87396
[1mStep[0m  [24/42], [94mLoss[0m : 4.18716
[1mStep[0m  [28/42], [94mLoss[0m : 4.12132
[1mStep[0m  [32/42], [94mLoss[0m : 4.06243
[1mStep[0m  [36/42], [94mLoss[0m : 4.33667
[1mStep[0m  [40/42], [94mLoss[0m : 3.66764

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.252, [92mTest[0m: 4.337, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.00432
[1mStep[0m  [4/42], [94mLoss[0m : 4.08120
[1mStep[0m  [8/42], [94mLoss[0m : 4.28324
[1mStep[0m  [12/42], [94mLoss[0m : 3.96444
[1mStep[0m  [16/42], [94mLoss[0m : 4.61003
[1mStep[0m  [20/42], [94mLoss[0m : 4.08961
[1mStep[0m  [24/42], [94mLoss[0m : 3.85598
[1mStep[0m  [28/42], [94mLoss[0m : 3.96430
[1mStep[0m  [32/42], [94mLoss[0m : 4.05553
[1mStep[0m  [36/42], [94mLoss[0m : 3.73401
[1mStep[0m  [40/42], [94mLoss[0m : 3.95923

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.102, [92mTest[0m: 4.175, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.81399
[1mStep[0m  [4/42], [94mLoss[0m : 4.08177
[1mStep[0m  [8/42], [94mLoss[0m : 3.91668
[1mStep[0m  [12/42], [94mLoss[0m : 4.19120
[1mStep[0m  [16/42], [94mLoss[0m : 4.06864
[1mStep[0m  [20/42], [94mLoss[0m : 3.94242
[1mStep[0m  [24/42], [94mLoss[0m : 3.91420
[1mStep[0m  [28/42], [94mLoss[0m : 4.09090
[1mStep[0m  [32/42], [94mLoss[0m : 4.21663
[1mStep[0m  [36/42], [94mLoss[0m : 3.72585
[1mStep[0m  [40/42], [94mLoss[0m : 3.36467

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.958, [92mTest[0m: 4.045, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.78167
[1mStep[0m  [4/42], [94mLoss[0m : 3.67642
[1mStep[0m  [8/42], [94mLoss[0m : 3.87621
[1mStep[0m  [12/42], [94mLoss[0m : 3.73677
[1mStep[0m  [16/42], [94mLoss[0m : 4.03133
[1mStep[0m  [20/42], [94mLoss[0m : 3.62192
[1mStep[0m  [24/42], [94mLoss[0m : 3.96947
[1mStep[0m  [28/42], [94mLoss[0m : 3.51896
[1mStep[0m  [32/42], [94mLoss[0m : 3.88550
[1mStep[0m  [36/42], [94mLoss[0m : 3.52687
[1mStep[0m  [40/42], [94mLoss[0m : 3.57031

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.844, [92mTest[0m: 3.898, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.70475
[1mStep[0m  [4/42], [94mLoss[0m : 3.81304
[1mStep[0m  [8/42], [94mLoss[0m : 3.92990
[1mStep[0m  [12/42], [94mLoss[0m : 3.89342
[1mStep[0m  [16/42], [94mLoss[0m : 3.37381
[1mStep[0m  [20/42], [94mLoss[0m : 3.60207
[1mStep[0m  [24/42], [94mLoss[0m : 3.79110
[1mStep[0m  [28/42], [94mLoss[0m : 3.75155
[1mStep[0m  [32/42], [94mLoss[0m : 3.41787
[1mStep[0m  [36/42], [94mLoss[0m : 3.93102
[1mStep[0m  [40/42], [94mLoss[0m : 3.85108

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.728, [92mTest[0m: 3.790, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.48008
[1mStep[0m  [4/42], [94mLoss[0m : 3.82308
[1mStep[0m  [8/42], [94mLoss[0m : 3.53727
[1mStep[0m  [12/42], [94mLoss[0m : 3.74937
[1mStep[0m  [16/42], [94mLoss[0m : 3.28688
[1mStep[0m  [20/42], [94mLoss[0m : 3.71312
[1mStep[0m  [24/42], [94mLoss[0m : 3.58973
[1mStep[0m  [28/42], [94mLoss[0m : 3.73736
[1mStep[0m  [32/42], [94mLoss[0m : 3.50668
[1mStep[0m  [36/42], [94mLoss[0m : 3.59327
[1mStep[0m  [40/42], [94mLoss[0m : 3.96333

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.630, [92mTest[0m: 3.682, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.44037
[1mStep[0m  [4/42], [94mLoss[0m : 3.65699
[1mStep[0m  [8/42], [94mLoss[0m : 3.53708
[1mStep[0m  [12/42], [94mLoss[0m : 3.55539
[1mStep[0m  [16/42], [94mLoss[0m : 3.52045
[1mStep[0m  [20/42], [94mLoss[0m : 3.05506
[1mStep[0m  [24/42], [94mLoss[0m : 3.57527
[1mStep[0m  [28/42], [94mLoss[0m : 3.21116
[1mStep[0m  [32/42], [94mLoss[0m : 3.38308
[1mStep[0m  [36/42], [94mLoss[0m : 3.55667
[1mStep[0m  [40/42], [94mLoss[0m : 3.43263

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.545, [92mTest[0m: 3.574, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.498
====================================

Phase 1 - Evaluation MAE:  3.497568794659206
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 3.48918
[1mStep[0m  [4/42], [94mLoss[0m : 3.43685
[1mStep[0m  [8/42], [94mLoss[0m : 3.41376
[1mStep[0m  [12/42], [94mLoss[0m : 3.39677
[1mStep[0m  [16/42], [94mLoss[0m : 3.35629
[1mStep[0m  [20/42], [94mLoss[0m : 3.47465
[1mStep[0m  [24/42], [94mLoss[0m : 3.73439
[1mStep[0m  [28/42], [94mLoss[0m : 3.49515
[1mStep[0m  [32/42], [94mLoss[0m : 3.30729
[1mStep[0m  [36/42], [94mLoss[0m : 3.44445
[1mStep[0m  [40/42], [94mLoss[0m : 3.60345

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.450, [92mTest[0m: 3.499, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.47113
[1mStep[0m  [4/42], [94mLoss[0m : 3.35434
[1mStep[0m  [8/42], [94mLoss[0m : 3.47425
[1mStep[0m  [12/42], [94mLoss[0m : 3.58376
[1mStep[0m  [16/42], [94mLoss[0m : 3.59018
[1mStep[0m  [20/42], [94mLoss[0m : 3.42161
[1mStep[0m  [24/42], [94mLoss[0m : 3.28689
[1mStep[0m  [28/42], [94mLoss[0m : 3.53871
[1mStep[0m  [32/42], [94mLoss[0m : 3.39580
[1mStep[0m  [36/42], [94mLoss[0m : 3.31390
[1mStep[0m  [40/42], [94mLoss[0m : 3.42124

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.372, [92mTest[0m: 3.402, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.38877
[1mStep[0m  [4/42], [94mLoss[0m : 3.37836
[1mStep[0m  [8/42], [94mLoss[0m : 3.32166
[1mStep[0m  [12/42], [94mLoss[0m : 3.14262
[1mStep[0m  [16/42], [94mLoss[0m : 2.97143
[1mStep[0m  [20/42], [94mLoss[0m : 2.82128
[1mStep[0m  [24/42], [94mLoss[0m : 3.20039
[1mStep[0m  [28/42], [94mLoss[0m : 3.38902
[1mStep[0m  [32/42], [94mLoss[0m : 3.19506
[1mStep[0m  [36/42], [94mLoss[0m : 3.63502
[1mStep[0m  [40/42], [94mLoss[0m : 3.34761

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.314, [92mTest[0m: 3.316, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.01425
[1mStep[0m  [4/42], [94mLoss[0m : 3.39950
[1mStep[0m  [8/42], [94mLoss[0m : 3.55423
[1mStep[0m  [12/42], [94mLoss[0m : 3.16493
[1mStep[0m  [16/42], [94mLoss[0m : 3.40648
[1mStep[0m  [20/42], [94mLoss[0m : 3.12916
[1mStep[0m  [24/42], [94mLoss[0m : 3.20067
[1mStep[0m  [28/42], [94mLoss[0m : 3.23519
[1mStep[0m  [32/42], [94mLoss[0m : 3.00018
[1mStep[0m  [36/42], [94mLoss[0m : 3.08855
[1mStep[0m  [40/42], [94mLoss[0m : 3.39098

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.224, [92mTest[0m: 3.256, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07502
[1mStep[0m  [4/42], [94mLoss[0m : 3.08859
[1mStep[0m  [8/42], [94mLoss[0m : 3.35590
[1mStep[0m  [12/42], [94mLoss[0m : 3.51158
[1mStep[0m  [16/42], [94mLoss[0m : 3.47881
[1mStep[0m  [20/42], [94mLoss[0m : 3.28655
[1mStep[0m  [24/42], [94mLoss[0m : 2.90773
[1mStep[0m  [28/42], [94mLoss[0m : 2.97025
[1mStep[0m  [32/42], [94mLoss[0m : 3.05980
[1mStep[0m  [36/42], [94mLoss[0m : 3.69281
[1mStep[0m  [40/42], [94mLoss[0m : 3.14034

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.184, [92mTest[0m: 3.193, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.17201
[1mStep[0m  [4/42], [94mLoss[0m : 3.19658
[1mStep[0m  [8/42], [94mLoss[0m : 3.01064
[1mStep[0m  [12/42], [94mLoss[0m : 3.15429
[1mStep[0m  [16/42], [94mLoss[0m : 3.07405
[1mStep[0m  [20/42], [94mLoss[0m : 3.14791
[1mStep[0m  [24/42], [94mLoss[0m : 2.85119
[1mStep[0m  [28/42], [94mLoss[0m : 3.20126
[1mStep[0m  [32/42], [94mLoss[0m : 2.83720
[1mStep[0m  [36/42], [94mLoss[0m : 3.23479
[1mStep[0m  [40/42], [94mLoss[0m : 3.27914

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.128, [92mTest[0m: 3.150, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.35848
[1mStep[0m  [4/42], [94mLoss[0m : 3.15013
[1mStep[0m  [8/42], [94mLoss[0m : 2.93781
[1mStep[0m  [12/42], [94mLoss[0m : 3.27748
[1mStep[0m  [16/42], [94mLoss[0m : 3.08434
[1mStep[0m  [20/42], [94mLoss[0m : 2.91525
[1mStep[0m  [24/42], [94mLoss[0m : 3.09906
[1mStep[0m  [28/42], [94mLoss[0m : 2.86292
[1mStep[0m  [32/42], [94mLoss[0m : 3.07082
[1mStep[0m  [36/42], [94mLoss[0m : 3.30253
[1mStep[0m  [40/42], [94mLoss[0m : 3.09660

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.085, [92mTest[0m: 3.093, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.00997
[1mStep[0m  [4/42], [94mLoss[0m : 2.88908
[1mStep[0m  [8/42], [94mLoss[0m : 3.12373
[1mStep[0m  [12/42], [94mLoss[0m : 2.97712
[1mStep[0m  [16/42], [94mLoss[0m : 2.98648
[1mStep[0m  [20/42], [94mLoss[0m : 3.14685
[1mStep[0m  [24/42], [94mLoss[0m : 2.89811
[1mStep[0m  [28/42], [94mLoss[0m : 3.16253
[1mStep[0m  [32/42], [94mLoss[0m : 3.00770
[1mStep[0m  [36/42], [94mLoss[0m : 2.76159
[1mStep[0m  [40/42], [94mLoss[0m : 3.27856

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.045, [92mTest[0m: 3.037, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.24423
[1mStep[0m  [4/42], [94mLoss[0m : 3.24287
[1mStep[0m  [8/42], [94mLoss[0m : 3.02985
[1mStep[0m  [12/42], [94mLoss[0m : 3.05534
[1mStep[0m  [16/42], [94mLoss[0m : 3.21431
[1mStep[0m  [20/42], [94mLoss[0m : 3.07699
[1mStep[0m  [24/42], [94mLoss[0m : 3.06620
[1mStep[0m  [28/42], [94mLoss[0m : 2.67899
[1mStep[0m  [32/42], [94mLoss[0m : 3.35974
[1mStep[0m  [36/42], [94mLoss[0m : 2.84286
[1mStep[0m  [40/42], [94mLoss[0m : 2.90259

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.004, [92mTest[0m: 3.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.08128
[1mStep[0m  [4/42], [94mLoss[0m : 2.86831
[1mStep[0m  [8/42], [94mLoss[0m : 2.87687
[1mStep[0m  [12/42], [94mLoss[0m : 3.15349
[1mStep[0m  [16/42], [94mLoss[0m : 2.95935
[1mStep[0m  [20/42], [94mLoss[0m : 2.91341
[1mStep[0m  [24/42], [94mLoss[0m : 2.81300
[1mStep[0m  [28/42], [94mLoss[0m : 2.89678
[1mStep[0m  [32/42], [94mLoss[0m : 2.83185
[1mStep[0m  [36/42], [94mLoss[0m : 3.22445
[1mStep[0m  [40/42], [94mLoss[0m : 3.04314

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.973, [92mTest[0m: 2.973, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87064
[1mStep[0m  [4/42], [94mLoss[0m : 2.65214
[1mStep[0m  [8/42], [94mLoss[0m : 3.08453
[1mStep[0m  [12/42], [94mLoss[0m : 2.98388
[1mStep[0m  [16/42], [94mLoss[0m : 3.12170
[1mStep[0m  [20/42], [94mLoss[0m : 3.04079
[1mStep[0m  [24/42], [94mLoss[0m : 3.19451
[1mStep[0m  [28/42], [94mLoss[0m : 2.82128
[1mStep[0m  [32/42], [94mLoss[0m : 3.03825
[1mStep[0m  [36/42], [94mLoss[0m : 2.86446
[1mStep[0m  [40/42], [94mLoss[0m : 2.99916

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.948, [92mTest[0m: 2.933, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04608
[1mStep[0m  [4/42], [94mLoss[0m : 3.05786
[1mStep[0m  [8/42], [94mLoss[0m : 2.99091
[1mStep[0m  [12/42], [94mLoss[0m : 3.09486
[1mStep[0m  [16/42], [94mLoss[0m : 2.73540
[1mStep[0m  [20/42], [94mLoss[0m : 2.97659
[1mStep[0m  [24/42], [94mLoss[0m : 2.87250
[1mStep[0m  [28/42], [94mLoss[0m : 2.78102
[1mStep[0m  [32/42], [94mLoss[0m : 3.04398
[1mStep[0m  [36/42], [94mLoss[0m : 2.94261
[1mStep[0m  [40/42], [94mLoss[0m : 2.90970

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.910, [92mTest[0m: 2.903, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74647
[1mStep[0m  [4/42], [94mLoss[0m : 2.66328
[1mStep[0m  [8/42], [94mLoss[0m : 2.93542
[1mStep[0m  [12/42], [94mLoss[0m : 2.63938
[1mStep[0m  [16/42], [94mLoss[0m : 2.95222
[1mStep[0m  [20/42], [94mLoss[0m : 3.23594
[1mStep[0m  [24/42], [94mLoss[0m : 2.84161
[1mStep[0m  [28/42], [94mLoss[0m : 2.94950
[1mStep[0m  [32/42], [94mLoss[0m : 3.01806
[1mStep[0m  [36/42], [94mLoss[0m : 2.81974
[1mStep[0m  [40/42], [94mLoss[0m : 2.69013

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.887, [92mTest[0m: 2.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.12892
[1mStep[0m  [4/42], [94mLoss[0m : 2.66375
[1mStep[0m  [8/42], [94mLoss[0m : 2.75747
[1mStep[0m  [12/42], [94mLoss[0m : 2.71882
[1mStep[0m  [16/42], [94mLoss[0m : 2.73247
[1mStep[0m  [20/42], [94mLoss[0m : 2.91887
[1mStep[0m  [24/42], [94mLoss[0m : 2.84206
[1mStep[0m  [28/42], [94mLoss[0m : 2.73652
[1mStep[0m  [32/42], [94mLoss[0m : 2.89136
[1mStep[0m  [36/42], [94mLoss[0m : 2.64524
[1mStep[0m  [40/42], [94mLoss[0m : 3.12301

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.858, [92mTest[0m: 2.843, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.95252
[1mStep[0m  [4/42], [94mLoss[0m : 2.67511
[1mStep[0m  [8/42], [94mLoss[0m : 2.79775
[1mStep[0m  [12/42], [94mLoss[0m : 2.86044
[1mStep[0m  [16/42], [94mLoss[0m : 2.62232
[1mStep[0m  [20/42], [94mLoss[0m : 2.87220
[1mStep[0m  [24/42], [94mLoss[0m : 2.82227
[1mStep[0m  [28/42], [94mLoss[0m : 3.09517
[1mStep[0m  [32/42], [94mLoss[0m : 2.67844
[1mStep[0m  [36/42], [94mLoss[0m : 2.66863
[1mStep[0m  [40/42], [94mLoss[0m : 2.64577

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.830, [92mTest[0m: 2.823, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.99054
[1mStep[0m  [4/42], [94mLoss[0m : 2.68054
[1mStep[0m  [8/42], [94mLoss[0m : 2.97807
[1mStep[0m  [12/42], [94mLoss[0m : 2.76302
[1mStep[0m  [16/42], [94mLoss[0m : 2.78822
[1mStep[0m  [20/42], [94mLoss[0m : 2.98876
[1mStep[0m  [24/42], [94mLoss[0m : 2.77611
[1mStep[0m  [28/42], [94mLoss[0m : 2.85268
[1mStep[0m  [32/42], [94mLoss[0m : 2.86608
[1mStep[0m  [36/42], [94mLoss[0m : 2.71417
[1mStep[0m  [40/42], [94mLoss[0m : 2.77599

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.84041
[1mStep[0m  [4/42], [94mLoss[0m : 2.75355
[1mStep[0m  [8/42], [94mLoss[0m : 2.49378
[1mStep[0m  [12/42], [94mLoss[0m : 2.92452
[1mStep[0m  [16/42], [94mLoss[0m : 2.72465
[1mStep[0m  [20/42], [94mLoss[0m : 2.93611
[1mStep[0m  [24/42], [94mLoss[0m : 2.77305
[1mStep[0m  [28/42], [94mLoss[0m : 2.70978
[1mStep[0m  [32/42], [94mLoss[0m : 2.91905
[1mStep[0m  [36/42], [94mLoss[0m : 2.89776
[1mStep[0m  [40/42], [94mLoss[0m : 2.74834

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.788, [92mTest[0m: 2.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49178
[1mStep[0m  [4/42], [94mLoss[0m : 2.87503
[1mStep[0m  [8/42], [94mLoss[0m : 2.95274
[1mStep[0m  [12/42], [94mLoss[0m : 2.73012
[1mStep[0m  [16/42], [94mLoss[0m : 2.82442
[1mStep[0m  [20/42], [94mLoss[0m : 2.74065
[1mStep[0m  [24/42], [94mLoss[0m : 2.81323
[1mStep[0m  [28/42], [94mLoss[0m : 2.72493
[1mStep[0m  [32/42], [94mLoss[0m : 2.89297
[1mStep[0m  [36/42], [94mLoss[0m : 2.84849
[1mStep[0m  [40/42], [94mLoss[0m : 2.98665

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.778, [92mTest[0m: 2.752, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.98703
[1mStep[0m  [4/42], [94mLoss[0m : 2.69712
[1mStep[0m  [8/42], [94mLoss[0m : 2.63388
[1mStep[0m  [12/42], [94mLoss[0m : 2.73492
[1mStep[0m  [16/42], [94mLoss[0m : 2.65152
[1mStep[0m  [20/42], [94mLoss[0m : 2.82902
[1mStep[0m  [24/42], [94mLoss[0m : 2.70474
[1mStep[0m  [28/42], [94mLoss[0m : 2.66093
[1mStep[0m  [32/42], [94mLoss[0m : 2.91104
[1mStep[0m  [36/42], [94mLoss[0m : 2.69562
[1mStep[0m  [40/42], [94mLoss[0m : 2.85141

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.727, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92558
[1mStep[0m  [4/42], [94mLoss[0m : 2.75165
[1mStep[0m  [8/42], [94mLoss[0m : 2.33951
[1mStep[0m  [12/42], [94mLoss[0m : 2.60185
[1mStep[0m  [16/42], [94mLoss[0m : 2.69183
[1mStep[0m  [20/42], [94mLoss[0m : 2.45058
[1mStep[0m  [24/42], [94mLoss[0m : 2.79507
[1mStep[0m  [28/42], [94mLoss[0m : 2.78654
[1mStep[0m  [32/42], [94mLoss[0m : 3.06366
[1mStep[0m  [36/42], [94mLoss[0m : 2.76875
[1mStep[0m  [40/42], [94mLoss[0m : 2.80063

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.715, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61773
[1mStep[0m  [4/42], [94mLoss[0m : 2.76971
[1mStep[0m  [8/42], [94mLoss[0m : 2.73635
[1mStep[0m  [12/42], [94mLoss[0m : 2.70982
[1mStep[0m  [16/42], [94mLoss[0m : 2.75463
[1mStep[0m  [20/42], [94mLoss[0m : 2.68292
[1mStep[0m  [24/42], [94mLoss[0m : 2.86222
[1mStep[0m  [28/42], [94mLoss[0m : 2.94121
[1mStep[0m  [32/42], [94mLoss[0m : 2.68778
[1mStep[0m  [36/42], [94mLoss[0m : 2.78712
[1mStep[0m  [40/42], [94mLoss[0m : 2.62779

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46205
[1mStep[0m  [4/42], [94mLoss[0m : 2.83824
[1mStep[0m  [8/42], [94mLoss[0m : 2.59314
[1mStep[0m  [12/42], [94mLoss[0m : 2.84218
[1mStep[0m  [16/42], [94mLoss[0m : 2.81604
[1mStep[0m  [20/42], [94mLoss[0m : 2.64212
[1mStep[0m  [24/42], [94mLoss[0m : 2.76400
[1mStep[0m  [28/42], [94mLoss[0m : 2.64846
[1mStep[0m  [32/42], [94mLoss[0m : 2.70524
[1mStep[0m  [36/42], [94mLoss[0m : 2.65058
[1mStep[0m  [40/42], [94mLoss[0m : 2.96593

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.721, [92mTest[0m: 2.683, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.82349
[1mStep[0m  [4/42], [94mLoss[0m : 2.68899
[1mStep[0m  [8/42], [94mLoss[0m : 2.63896
[1mStep[0m  [12/42], [94mLoss[0m : 2.67697
[1mStep[0m  [16/42], [94mLoss[0m : 2.90257
[1mStep[0m  [20/42], [94mLoss[0m : 2.58535
[1mStep[0m  [24/42], [94mLoss[0m : 3.02169
[1mStep[0m  [28/42], [94mLoss[0m : 2.66567
[1mStep[0m  [32/42], [94mLoss[0m : 2.61732
[1mStep[0m  [36/42], [94mLoss[0m : 2.69883
[1mStep[0m  [40/42], [94mLoss[0m : 2.91118

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.712, [92mTest[0m: 2.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.48598
[1mStep[0m  [4/42], [94mLoss[0m : 2.40697
[1mStep[0m  [8/42], [94mLoss[0m : 2.65474
[1mStep[0m  [12/42], [94mLoss[0m : 2.80526
[1mStep[0m  [16/42], [94mLoss[0m : 2.88204
[1mStep[0m  [20/42], [94mLoss[0m : 2.64439
[1mStep[0m  [24/42], [94mLoss[0m : 2.61166
[1mStep[0m  [28/42], [94mLoss[0m : 2.74171
[1mStep[0m  [32/42], [94mLoss[0m : 2.62161
[1mStep[0m  [36/42], [94mLoss[0m : 2.84492
[1mStep[0m  [40/42], [94mLoss[0m : 2.46001

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.693, [92mTest[0m: 2.651, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56267
[1mStep[0m  [4/42], [94mLoss[0m : 2.79422
[1mStep[0m  [8/42], [94mLoss[0m : 2.72633
[1mStep[0m  [12/42], [94mLoss[0m : 2.70143
[1mStep[0m  [16/42], [94mLoss[0m : 2.36029
[1mStep[0m  [20/42], [94mLoss[0m : 2.63900
[1mStep[0m  [24/42], [94mLoss[0m : 2.67497
[1mStep[0m  [28/42], [94mLoss[0m : 2.51013
[1mStep[0m  [32/42], [94mLoss[0m : 2.73974
[1mStep[0m  [36/42], [94mLoss[0m : 2.73808
[1mStep[0m  [40/42], [94mLoss[0m : 2.42825

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.636, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73358
[1mStep[0m  [4/42], [94mLoss[0m : 2.61217
[1mStep[0m  [8/42], [94mLoss[0m : 2.84010
[1mStep[0m  [12/42], [94mLoss[0m : 2.68657
[1mStep[0m  [16/42], [94mLoss[0m : 2.73795
[1mStep[0m  [20/42], [94mLoss[0m : 2.39432
[1mStep[0m  [24/42], [94mLoss[0m : 2.63554
[1mStep[0m  [28/42], [94mLoss[0m : 2.65992
[1mStep[0m  [32/42], [94mLoss[0m : 2.66181
[1mStep[0m  [36/42], [94mLoss[0m : 2.67530
[1mStep[0m  [40/42], [94mLoss[0m : 2.74947

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.676, [92mTest[0m: 2.629, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73722
[1mStep[0m  [4/42], [94mLoss[0m : 2.63813
[1mStep[0m  [8/42], [94mLoss[0m : 2.52830
[1mStep[0m  [12/42], [94mLoss[0m : 2.69876
[1mStep[0m  [16/42], [94mLoss[0m : 2.78875
[1mStep[0m  [20/42], [94mLoss[0m : 2.76042
[1mStep[0m  [24/42], [94mLoss[0m : 2.47186
[1mStep[0m  [28/42], [94mLoss[0m : 2.57510
[1mStep[0m  [32/42], [94mLoss[0m : 2.63050
[1mStep[0m  [36/42], [94mLoss[0m : 2.67672
[1mStep[0m  [40/42], [94mLoss[0m : 2.77422

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.674, [92mTest[0m: 2.618, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62425
[1mStep[0m  [4/42], [94mLoss[0m : 2.66900
[1mStep[0m  [8/42], [94mLoss[0m : 2.66236
[1mStep[0m  [12/42], [94mLoss[0m : 2.37947
[1mStep[0m  [16/42], [94mLoss[0m : 2.53240
[1mStep[0m  [20/42], [94mLoss[0m : 2.64590
[1mStep[0m  [24/42], [94mLoss[0m : 2.78770
[1mStep[0m  [28/42], [94mLoss[0m : 2.48023
[1mStep[0m  [32/42], [94mLoss[0m : 2.53296
[1mStep[0m  [36/42], [94mLoss[0m : 2.76551
[1mStep[0m  [40/42], [94mLoss[0m : 2.73276

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.662, [92mTest[0m: 2.607, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51186
[1mStep[0m  [4/42], [94mLoss[0m : 2.82337
[1mStep[0m  [8/42], [94mLoss[0m : 2.73361
[1mStep[0m  [12/42], [94mLoss[0m : 2.77171
[1mStep[0m  [16/42], [94mLoss[0m : 2.53045
[1mStep[0m  [20/42], [94mLoss[0m : 2.60112
[1mStep[0m  [24/42], [94mLoss[0m : 2.36795
[1mStep[0m  [28/42], [94mLoss[0m : 2.70294
[1mStep[0m  [32/42], [94mLoss[0m : 2.79136
[1mStep[0m  [36/42], [94mLoss[0m : 2.75943
[1mStep[0m  [40/42], [94mLoss[0m : 2.58448

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.609, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58866
[1mStep[0m  [4/42], [94mLoss[0m : 2.66848
[1mStep[0m  [8/42], [94mLoss[0m : 2.80578
[1mStep[0m  [12/42], [94mLoss[0m : 2.50085
[1mStep[0m  [16/42], [94mLoss[0m : 2.57796
[1mStep[0m  [20/42], [94mLoss[0m : 2.28289
[1mStep[0m  [24/42], [94mLoss[0m : 2.74767
[1mStep[0m  [28/42], [94mLoss[0m : 2.93674
[1mStep[0m  [32/42], [94mLoss[0m : 2.62704
[1mStep[0m  [36/42], [94mLoss[0m : 2.79020
[1mStep[0m  [40/42], [94mLoss[0m : 2.59643

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.598, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.586
====================================

Phase 2 - Evaluation MAE:  2.585661734853472
MAE score P1      3.497569
MAE score P2      2.585662
loss              2.637636
learning_rate       0.0001
batch_size             256
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.59517
[1mStep[0m  [2/21], [94mLoss[0m : 10.81749
[1mStep[0m  [4/21], [94mLoss[0m : 10.21183
[1mStep[0m  [6/21], [94mLoss[0m : 10.69314
[1mStep[0m  [8/21], [94mLoss[0m : 10.70140
[1mStep[0m  [10/21], [94mLoss[0m : 10.97685
[1mStep[0m  [12/21], [94mLoss[0m : 10.32085
[1mStep[0m  [14/21], [94mLoss[0m : 10.62584
[1mStep[0m  [16/21], [94mLoss[0m : 10.60445
[1mStep[0m  [18/21], [94mLoss[0m : 10.37925
[1mStep[0m  [20/21], [94mLoss[0m : 10.28332

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.574, [92mTest[0m: 10.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55212
[1mStep[0m  [2/21], [94mLoss[0m : 10.53790
[1mStep[0m  [4/21], [94mLoss[0m : 10.39313
[1mStep[0m  [6/21], [94mLoss[0m : 10.66472
[1mStep[0m  [8/21], [94mLoss[0m : 10.60775
[1mStep[0m  [10/21], [94mLoss[0m : 10.56701
[1mStep[0m  [12/21], [94mLoss[0m : 10.36657
[1mStep[0m  [14/21], [94mLoss[0m : 10.40085
[1mStep[0m  [16/21], [94mLoss[0m : 10.18409
[1mStep[0m  [18/21], [94mLoss[0m : 10.48361
[1mStep[0m  [20/21], [94mLoss[0m : 10.25605

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.462, [92mTest[0m: 10.529, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.15890
[1mStep[0m  [2/21], [94mLoss[0m : 10.18399
[1mStep[0m  [4/21], [94mLoss[0m : 10.31382
[1mStep[0m  [6/21], [94mLoss[0m : 10.45946
[1mStep[0m  [8/21], [94mLoss[0m : 10.27827
[1mStep[0m  [10/21], [94mLoss[0m : 10.37403
[1mStep[0m  [12/21], [94mLoss[0m : 10.48241
[1mStep[0m  [14/21], [94mLoss[0m : 10.50295
[1mStep[0m  [16/21], [94mLoss[0m : 10.33714
[1mStep[0m  [18/21], [94mLoss[0m : 10.63410
[1mStep[0m  [20/21], [94mLoss[0m : 10.45741

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.363, [92mTest[0m: 10.396, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.27635
[1mStep[0m  [2/21], [94mLoss[0m : 10.36749
[1mStep[0m  [4/21], [94mLoss[0m : 10.36314
[1mStep[0m  [6/21], [94mLoss[0m : 10.30316
[1mStep[0m  [8/21], [94mLoss[0m : 10.55861
[1mStep[0m  [10/21], [94mLoss[0m : 10.01443
[1mStep[0m  [12/21], [94mLoss[0m : 10.16006
[1mStep[0m  [14/21], [94mLoss[0m : 10.00572
[1mStep[0m  [16/21], [94mLoss[0m : 10.30932
[1mStep[0m  [18/21], [94mLoss[0m : 10.10394
[1mStep[0m  [20/21], [94mLoss[0m : 10.17670

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.241, [92mTest[0m: 10.299, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.29034
[1mStep[0m  [2/21], [94mLoss[0m : 10.53065
[1mStep[0m  [4/21], [94mLoss[0m : 10.08359
[1mStep[0m  [6/21], [94mLoss[0m : 10.14995
[1mStep[0m  [8/21], [94mLoss[0m : 9.98161
[1mStep[0m  [10/21], [94mLoss[0m : 10.18581
[1mStep[0m  [12/21], [94mLoss[0m : 10.03810
[1mStep[0m  [14/21], [94mLoss[0m : 10.18983
[1mStep[0m  [16/21], [94mLoss[0m : 10.09130
[1mStep[0m  [18/21], [94mLoss[0m : 10.04177
[1mStep[0m  [20/21], [94mLoss[0m : 9.78260

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.138, [92mTest[0m: 10.189, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.04770
[1mStep[0m  [2/21], [94mLoss[0m : 10.02132
[1mStep[0m  [4/21], [94mLoss[0m : 10.26194
[1mStep[0m  [6/21], [94mLoss[0m : 10.03153
[1mStep[0m  [8/21], [94mLoss[0m : 9.97297
[1mStep[0m  [10/21], [94mLoss[0m : 10.27154
[1mStep[0m  [12/21], [94mLoss[0m : 9.87782
[1mStep[0m  [14/21], [94mLoss[0m : 9.94950
[1mStep[0m  [16/21], [94mLoss[0m : 9.77099
[1mStep[0m  [18/21], [94mLoss[0m : 9.88366
[1mStep[0m  [20/21], [94mLoss[0m : 9.71246

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.027, [92mTest[0m: 10.071, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.72030
[1mStep[0m  [2/21], [94mLoss[0m : 9.89918
[1mStep[0m  [4/21], [94mLoss[0m : 10.14261
[1mStep[0m  [6/21], [94mLoss[0m : 10.19066
[1mStep[0m  [8/21], [94mLoss[0m : 9.90248
[1mStep[0m  [10/21], [94mLoss[0m : 9.75079
[1mStep[0m  [12/21], [94mLoss[0m : 10.07297
[1mStep[0m  [14/21], [94mLoss[0m : 10.00855
[1mStep[0m  [16/21], [94mLoss[0m : 9.95461
[1mStep[0m  [18/21], [94mLoss[0m : 9.82810
[1mStep[0m  [20/21], [94mLoss[0m : 10.18496

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.918, [92mTest[0m: 9.961, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.80772
[1mStep[0m  [2/21], [94mLoss[0m : 9.74502
[1mStep[0m  [4/21], [94mLoss[0m : 9.95270
[1mStep[0m  [6/21], [94mLoss[0m : 9.69278
[1mStep[0m  [8/21], [94mLoss[0m : 9.59233
[1mStep[0m  [10/21], [94mLoss[0m : 10.04279
[1mStep[0m  [12/21], [94mLoss[0m : 10.33031
[1mStep[0m  [14/21], [94mLoss[0m : 9.88356
[1mStep[0m  [16/21], [94mLoss[0m : 9.70615
[1mStep[0m  [18/21], [94mLoss[0m : 9.70111
[1mStep[0m  [20/21], [94mLoss[0m : 9.72566

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.806, [92mTest[0m: 9.861, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.89908
[1mStep[0m  [2/21], [94mLoss[0m : 9.90069
[1mStep[0m  [4/21], [94mLoss[0m : 9.53547
[1mStep[0m  [6/21], [94mLoss[0m : 9.66179
[1mStep[0m  [8/21], [94mLoss[0m : 9.69512
[1mStep[0m  [10/21], [94mLoss[0m : 9.77784
[1mStep[0m  [12/21], [94mLoss[0m : 9.63654
[1mStep[0m  [14/21], [94mLoss[0m : 9.79882
[1mStep[0m  [16/21], [94mLoss[0m : 9.64718
[1mStep[0m  [18/21], [94mLoss[0m : 9.75534
[1mStep[0m  [20/21], [94mLoss[0m : 9.69012

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.703, [92mTest[0m: 9.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.70512
[1mStep[0m  [2/21], [94mLoss[0m : 9.85680
[1mStep[0m  [4/21], [94mLoss[0m : 9.67928
[1mStep[0m  [6/21], [94mLoss[0m : 9.49671
[1mStep[0m  [8/21], [94mLoss[0m : 9.77776
[1mStep[0m  [10/21], [94mLoss[0m : 9.64048
[1mStep[0m  [12/21], [94mLoss[0m : 9.50263
[1mStep[0m  [14/21], [94mLoss[0m : 9.63882
[1mStep[0m  [16/21], [94mLoss[0m : 9.49636
[1mStep[0m  [18/21], [94mLoss[0m : 9.74104
[1mStep[0m  [20/21], [94mLoss[0m : 9.39949

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.594, [92mTest[0m: 9.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.52160
[1mStep[0m  [2/21], [94mLoss[0m : 9.39721
[1mStep[0m  [4/21], [94mLoss[0m : 9.68992
[1mStep[0m  [6/21], [94mLoss[0m : 9.43982
[1mStep[0m  [8/21], [94mLoss[0m : 9.55187
[1mStep[0m  [10/21], [94mLoss[0m : 9.75310
[1mStep[0m  [12/21], [94mLoss[0m : 9.40173
[1mStep[0m  [14/21], [94mLoss[0m : 9.54190
[1mStep[0m  [16/21], [94mLoss[0m : 9.38101
[1mStep[0m  [18/21], [94mLoss[0m : 9.46720
[1mStep[0m  [20/21], [94mLoss[0m : 9.56962

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.477, [92mTest[0m: 9.530, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.26412
[1mStep[0m  [2/21], [94mLoss[0m : 9.11392
[1mStep[0m  [4/21], [94mLoss[0m : 9.41847
[1mStep[0m  [6/21], [94mLoss[0m : 9.31742
[1mStep[0m  [8/21], [94mLoss[0m : 9.28730
[1mStep[0m  [10/21], [94mLoss[0m : 9.42338
[1mStep[0m  [12/21], [94mLoss[0m : 9.62256
[1mStep[0m  [14/21], [94mLoss[0m : 9.47715
[1mStep[0m  [16/21], [94mLoss[0m : 9.35921
[1mStep[0m  [18/21], [94mLoss[0m : 9.23950
[1mStep[0m  [20/21], [94mLoss[0m : 9.20765

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.367, [92mTest[0m: 9.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.28609
[1mStep[0m  [2/21], [94mLoss[0m : 9.34480
[1mStep[0m  [4/21], [94mLoss[0m : 9.43804
[1mStep[0m  [6/21], [94mLoss[0m : 9.22324
[1mStep[0m  [8/21], [94mLoss[0m : 9.34286
[1mStep[0m  [10/21], [94mLoss[0m : 9.07831
[1mStep[0m  [12/21], [94mLoss[0m : 9.21287
[1mStep[0m  [14/21], [94mLoss[0m : 9.40150
[1mStep[0m  [16/21], [94mLoss[0m : 9.35341
[1mStep[0m  [18/21], [94mLoss[0m : 9.25953
[1mStep[0m  [20/21], [94mLoss[0m : 9.24362

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.265, [92mTest[0m: 9.320, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.36215
[1mStep[0m  [2/21], [94mLoss[0m : 9.14305
[1mStep[0m  [4/21], [94mLoss[0m : 9.04751
[1mStep[0m  [6/21], [94mLoss[0m : 9.07348
[1mStep[0m  [8/21], [94mLoss[0m : 9.18122
[1mStep[0m  [10/21], [94mLoss[0m : 9.16174
[1mStep[0m  [12/21], [94mLoss[0m : 9.33638
[1mStep[0m  [14/21], [94mLoss[0m : 9.22309
[1mStep[0m  [16/21], [94mLoss[0m : 9.14710
[1mStep[0m  [18/21], [94mLoss[0m : 9.35389
[1mStep[0m  [20/21], [94mLoss[0m : 8.95235

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.149, [92mTest[0m: 9.190, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.89428
[1mStep[0m  [2/21], [94mLoss[0m : 9.04347
[1mStep[0m  [4/21], [94mLoss[0m : 9.21415
[1mStep[0m  [6/21], [94mLoss[0m : 9.00737
[1mStep[0m  [8/21], [94mLoss[0m : 9.09836
[1mStep[0m  [10/21], [94mLoss[0m : 9.20951
[1mStep[0m  [12/21], [94mLoss[0m : 9.18863
[1mStep[0m  [14/21], [94mLoss[0m : 9.01037
[1mStep[0m  [16/21], [94mLoss[0m : 8.73311
[1mStep[0m  [18/21], [94mLoss[0m : 8.84968
[1mStep[0m  [20/21], [94mLoss[0m : 8.96787

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.040, [92mTest[0m: 9.093, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.16770
[1mStep[0m  [2/21], [94mLoss[0m : 8.75091
[1mStep[0m  [4/21], [94mLoss[0m : 9.40902
[1mStep[0m  [6/21], [94mLoss[0m : 8.81372
[1mStep[0m  [8/21], [94mLoss[0m : 8.94967
[1mStep[0m  [10/21], [94mLoss[0m : 9.07086
[1mStep[0m  [12/21], [94mLoss[0m : 9.12566
[1mStep[0m  [14/21], [94mLoss[0m : 9.04427
[1mStep[0m  [16/21], [94mLoss[0m : 8.90591
[1mStep[0m  [18/21], [94mLoss[0m : 8.78452
[1mStep[0m  [20/21], [94mLoss[0m : 8.90136

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.940, [92mTest[0m: 8.970, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.73986
[1mStep[0m  [2/21], [94mLoss[0m : 8.84917
[1mStep[0m  [4/21], [94mLoss[0m : 8.61354
[1mStep[0m  [6/21], [94mLoss[0m : 9.00682
[1mStep[0m  [8/21], [94mLoss[0m : 8.73622
[1mStep[0m  [10/21], [94mLoss[0m : 8.79449
[1mStep[0m  [12/21], [94mLoss[0m : 9.09388
[1mStep[0m  [14/21], [94mLoss[0m : 8.56991
[1mStep[0m  [16/21], [94mLoss[0m : 8.82764
[1mStep[0m  [18/21], [94mLoss[0m : 8.96007
[1mStep[0m  [20/21], [94mLoss[0m : 8.96781

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.821, [92mTest[0m: 8.866, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.78788
[1mStep[0m  [2/21], [94mLoss[0m : 8.58756
[1mStep[0m  [4/21], [94mLoss[0m : 8.66462
[1mStep[0m  [6/21], [94mLoss[0m : 8.93790
[1mStep[0m  [8/21], [94mLoss[0m : 8.64855
[1mStep[0m  [10/21], [94mLoss[0m : 8.74321
[1mStep[0m  [12/21], [94mLoss[0m : 8.77495
[1mStep[0m  [14/21], [94mLoss[0m : 8.81898
[1mStep[0m  [16/21], [94mLoss[0m : 8.61113
[1mStep[0m  [18/21], [94mLoss[0m : 8.50108
[1mStep[0m  [20/21], [94mLoss[0m : 8.67391

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.718, [92mTest[0m: 8.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.90182
[1mStep[0m  [2/21], [94mLoss[0m : 8.74339
[1mStep[0m  [4/21], [94mLoss[0m : 8.48282
[1mStep[0m  [6/21], [94mLoss[0m : 8.33018
[1mStep[0m  [8/21], [94mLoss[0m : 8.71300
[1mStep[0m  [10/21], [94mLoss[0m : 8.77467
[1mStep[0m  [12/21], [94mLoss[0m : 8.53824
[1mStep[0m  [14/21], [94mLoss[0m : 8.60323
[1mStep[0m  [16/21], [94mLoss[0m : 8.64701
[1mStep[0m  [18/21], [94mLoss[0m : 8.62775
[1mStep[0m  [20/21], [94mLoss[0m : 8.61809

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.608, [92mTest[0m: 8.650, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.56516
[1mStep[0m  [2/21], [94mLoss[0m : 8.32115
[1mStep[0m  [4/21], [94mLoss[0m : 8.29231
[1mStep[0m  [6/21], [94mLoss[0m : 8.41233
[1mStep[0m  [8/21], [94mLoss[0m : 8.32767
[1mStep[0m  [10/21], [94mLoss[0m : 8.48980
[1mStep[0m  [12/21], [94mLoss[0m : 8.70352
[1mStep[0m  [14/21], [94mLoss[0m : 8.39299
[1mStep[0m  [16/21], [94mLoss[0m : 8.65026
[1mStep[0m  [18/21], [94mLoss[0m : 8.50672
[1mStep[0m  [20/21], [94mLoss[0m : 8.55920

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.491, [92mTest[0m: 8.540, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.43097
[1mStep[0m  [2/21], [94mLoss[0m : 8.33582
[1mStep[0m  [4/21], [94mLoss[0m : 8.24582
[1mStep[0m  [6/21], [94mLoss[0m : 8.33343
[1mStep[0m  [8/21], [94mLoss[0m : 8.46164
[1mStep[0m  [10/21], [94mLoss[0m : 8.30158
[1mStep[0m  [12/21], [94mLoss[0m : 8.52592
[1mStep[0m  [14/21], [94mLoss[0m : 8.58105
[1mStep[0m  [16/21], [94mLoss[0m : 8.26076
[1mStep[0m  [18/21], [94mLoss[0m : 8.49506
[1mStep[0m  [20/21], [94mLoss[0m : 8.42293

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.388, [92mTest[0m: 8.438, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.16957
[1mStep[0m  [2/21], [94mLoss[0m : 8.29682
[1mStep[0m  [4/21], [94mLoss[0m : 8.72950
[1mStep[0m  [6/21], [94mLoss[0m : 8.53557
[1mStep[0m  [8/21], [94mLoss[0m : 8.16030
[1mStep[0m  [10/21], [94mLoss[0m : 8.16965
[1mStep[0m  [12/21], [94mLoss[0m : 8.32252
[1mStep[0m  [14/21], [94mLoss[0m : 8.58211
[1mStep[0m  [16/21], [94mLoss[0m : 8.29491
[1mStep[0m  [18/21], [94mLoss[0m : 8.26995
[1mStep[0m  [20/21], [94mLoss[0m : 8.38754

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.291, [92mTest[0m: 8.345, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.23122
[1mStep[0m  [2/21], [94mLoss[0m : 8.47458
[1mStep[0m  [4/21], [94mLoss[0m : 8.05261
[1mStep[0m  [6/21], [94mLoss[0m : 8.18301
[1mStep[0m  [8/21], [94mLoss[0m : 8.11039
[1mStep[0m  [10/21], [94mLoss[0m : 8.18331
[1mStep[0m  [12/21], [94mLoss[0m : 8.25472
[1mStep[0m  [14/21], [94mLoss[0m : 8.28106
[1mStep[0m  [16/21], [94mLoss[0m : 8.24076
[1mStep[0m  [18/21], [94mLoss[0m : 8.09859
[1mStep[0m  [20/21], [94mLoss[0m : 8.26780

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.192, [92mTest[0m: 8.235, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.19942
[1mStep[0m  [2/21], [94mLoss[0m : 8.37649
[1mStep[0m  [4/21], [94mLoss[0m : 8.03365
[1mStep[0m  [6/21], [94mLoss[0m : 8.24516
[1mStep[0m  [8/21], [94mLoss[0m : 7.77407
[1mStep[0m  [10/21], [94mLoss[0m : 8.08226
[1mStep[0m  [12/21], [94mLoss[0m : 7.90904
[1mStep[0m  [14/21], [94mLoss[0m : 8.01636
[1mStep[0m  [16/21], [94mLoss[0m : 8.09379
[1mStep[0m  [18/21], [94mLoss[0m : 8.04097
[1mStep[0m  [20/21], [94mLoss[0m : 8.01158

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.094, [92mTest[0m: 8.139, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.04368
[1mStep[0m  [2/21], [94mLoss[0m : 8.14013
[1mStep[0m  [4/21], [94mLoss[0m : 8.00097
[1mStep[0m  [6/21], [94mLoss[0m : 8.24160
[1mStep[0m  [8/21], [94mLoss[0m : 8.01685
[1mStep[0m  [10/21], [94mLoss[0m : 7.83947
[1mStep[0m  [12/21], [94mLoss[0m : 7.68714
[1mStep[0m  [14/21], [94mLoss[0m : 8.04669
[1mStep[0m  [16/21], [94mLoss[0m : 7.92358
[1mStep[0m  [18/21], [94mLoss[0m : 7.75685
[1mStep[0m  [20/21], [94mLoss[0m : 8.24124

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.995, [92mTest[0m: 8.042, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.00410
[1mStep[0m  [2/21], [94mLoss[0m : 7.76638
[1mStep[0m  [4/21], [94mLoss[0m : 7.90449
[1mStep[0m  [6/21], [94mLoss[0m : 7.46562
[1mStep[0m  [8/21], [94mLoss[0m : 8.09278
[1mStep[0m  [10/21], [94mLoss[0m : 7.62150
[1mStep[0m  [12/21], [94mLoss[0m : 8.07261
[1mStep[0m  [14/21], [94mLoss[0m : 8.18056
[1mStep[0m  [16/21], [94mLoss[0m : 7.99669
[1mStep[0m  [18/21], [94mLoss[0m : 7.81162
[1mStep[0m  [20/21], [94mLoss[0m : 7.93278

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.897, [92mTest[0m: 7.942, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.08957
[1mStep[0m  [2/21], [94mLoss[0m : 7.71516
[1mStep[0m  [4/21], [94mLoss[0m : 8.00577
[1mStep[0m  [6/21], [94mLoss[0m : 7.88075
[1mStep[0m  [8/21], [94mLoss[0m : 7.89145
[1mStep[0m  [10/21], [94mLoss[0m : 7.87222
[1mStep[0m  [12/21], [94mLoss[0m : 7.87587
[1mStep[0m  [14/21], [94mLoss[0m : 7.91150
[1mStep[0m  [16/21], [94mLoss[0m : 7.93641
[1mStep[0m  [18/21], [94mLoss[0m : 7.71766
[1mStep[0m  [20/21], [94mLoss[0m : 7.58455

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.802, [92mTest[0m: 7.839, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.62020
[1mStep[0m  [2/21], [94mLoss[0m : 7.80856
[1mStep[0m  [4/21], [94mLoss[0m : 7.65274
[1mStep[0m  [6/21], [94mLoss[0m : 7.79055
[1mStep[0m  [8/21], [94mLoss[0m : 7.95778
[1mStep[0m  [10/21], [94mLoss[0m : 7.75509
[1mStep[0m  [12/21], [94mLoss[0m : 7.73539
[1mStep[0m  [14/21], [94mLoss[0m : 8.14225
[1mStep[0m  [16/21], [94mLoss[0m : 7.97018
[1mStep[0m  [18/21], [94mLoss[0m : 7.38761
[1mStep[0m  [20/21], [94mLoss[0m : 7.68343

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.703, [92mTest[0m: 7.739, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.93436
[1mStep[0m  [2/21], [94mLoss[0m : 7.56248
[1mStep[0m  [4/21], [94mLoss[0m : 7.85049
[1mStep[0m  [6/21], [94mLoss[0m : 7.59969
[1mStep[0m  [8/21], [94mLoss[0m : 7.30906
[1mStep[0m  [10/21], [94mLoss[0m : 7.47918
[1mStep[0m  [12/21], [94mLoss[0m : 7.44605
[1mStep[0m  [14/21], [94mLoss[0m : 7.61710
[1mStep[0m  [16/21], [94mLoss[0m : 7.60861
[1mStep[0m  [18/21], [94mLoss[0m : 7.45167
[1mStep[0m  [20/21], [94mLoss[0m : 7.73018

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.608, [92mTest[0m: 7.643, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.45114
[1mStep[0m  [2/21], [94mLoss[0m : 7.55003
[1mStep[0m  [4/21], [94mLoss[0m : 7.45875
[1mStep[0m  [6/21], [94mLoss[0m : 7.78778
[1mStep[0m  [8/21], [94mLoss[0m : 7.57487
[1mStep[0m  [10/21], [94mLoss[0m : 7.41531
[1mStep[0m  [12/21], [94mLoss[0m : 7.36266
[1mStep[0m  [14/21], [94mLoss[0m : 7.58934
[1mStep[0m  [16/21], [94mLoss[0m : 7.72637
[1mStep[0m  [18/21], [94mLoss[0m : 7.24483
[1mStep[0m  [20/21], [94mLoss[0m : 7.59364

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.508, [92mTest[0m: 7.551, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.440
====================================

Phase 1 - Evaluation MAE:  7.439707006726946
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 7.44554
[1mStep[0m  [2/21], [94mLoss[0m : 7.24439
[1mStep[0m  [4/21], [94mLoss[0m : 7.48974
[1mStep[0m  [6/21], [94mLoss[0m : 7.17430
[1mStep[0m  [8/21], [94mLoss[0m : 7.52817
[1mStep[0m  [10/21], [94mLoss[0m : 7.34857
[1mStep[0m  [12/21], [94mLoss[0m : 7.36713
[1mStep[0m  [14/21], [94mLoss[0m : 7.28087
[1mStep[0m  [16/21], [94mLoss[0m : 7.71085
[1mStep[0m  [18/21], [94mLoss[0m : 7.56731
[1mStep[0m  [20/21], [94mLoss[0m : 7.14148

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.403, [92mTest[0m: 7.443, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.52820
[1mStep[0m  [2/21], [94mLoss[0m : 7.56523
[1mStep[0m  [4/21], [94mLoss[0m : 7.14412
[1mStep[0m  [6/21], [94mLoss[0m : 7.27057
[1mStep[0m  [8/21], [94mLoss[0m : 6.93208
[1mStep[0m  [10/21], [94mLoss[0m : 7.12335
[1mStep[0m  [12/21], [94mLoss[0m : 7.52102
[1mStep[0m  [14/21], [94mLoss[0m : 7.23559
[1mStep[0m  [16/21], [94mLoss[0m : 7.39507
[1mStep[0m  [18/21], [94mLoss[0m : 7.08518
[1mStep[0m  [20/21], [94mLoss[0m : 7.06495

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.299, [92mTest[0m: 7.336, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.11884
[1mStep[0m  [2/21], [94mLoss[0m : 7.51779
[1mStep[0m  [4/21], [94mLoss[0m : 7.26459
[1mStep[0m  [6/21], [94mLoss[0m : 6.98395
[1mStep[0m  [8/21], [94mLoss[0m : 7.14122
[1mStep[0m  [10/21], [94mLoss[0m : 6.86003
[1mStep[0m  [12/21], [94mLoss[0m : 7.26474
[1mStep[0m  [14/21], [94mLoss[0m : 7.16227
[1mStep[0m  [16/21], [94mLoss[0m : 7.26669
[1mStep[0m  [18/21], [94mLoss[0m : 7.01930
[1mStep[0m  [20/21], [94mLoss[0m : 7.15822

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.186, [92mTest[0m: 7.238, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.30335
[1mStep[0m  [2/21], [94mLoss[0m : 7.31653
[1mStep[0m  [4/21], [94mLoss[0m : 6.93901
[1mStep[0m  [6/21], [94mLoss[0m : 7.07109
[1mStep[0m  [8/21], [94mLoss[0m : 6.88920
[1mStep[0m  [10/21], [94mLoss[0m : 6.55295
[1mStep[0m  [12/21], [94mLoss[0m : 6.95505
[1mStep[0m  [14/21], [94mLoss[0m : 7.16878
[1mStep[0m  [16/21], [94mLoss[0m : 6.90362
[1mStep[0m  [18/21], [94mLoss[0m : 7.04245
[1mStep[0m  [20/21], [94mLoss[0m : 6.92233

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.080, [92mTest[0m: 7.129, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.82398
[1mStep[0m  [2/21], [94mLoss[0m : 6.92839
[1mStep[0m  [4/21], [94mLoss[0m : 6.75356
[1mStep[0m  [6/21], [94mLoss[0m : 7.17633
[1mStep[0m  [8/21], [94mLoss[0m : 7.01139
[1mStep[0m  [10/21], [94mLoss[0m : 7.08701
[1mStep[0m  [12/21], [94mLoss[0m : 6.89366
[1mStep[0m  [14/21], [94mLoss[0m : 6.77318
[1mStep[0m  [16/21], [94mLoss[0m : 6.73346
[1mStep[0m  [18/21], [94mLoss[0m : 6.97284
[1mStep[0m  [20/21], [94mLoss[0m : 7.02387

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.963, [92mTest[0m: 7.006, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.91521
[1mStep[0m  [2/21], [94mLoss[0m : 6.99977
[1mStep[0m  [4/21], [94mLoss[0m : 6.66054
[1mStep[0m  [6/21], [94mLoss[0m : 6.94781
[1mStep[0m  [8/21], [94mLoss[0m : 7.01956
[1mStep[0m  [10/21], [94mLoss[0m : 7.16023
[1mStep[0m  [12/21], [94mLoss[0m : 6.85733
[1mStep[0m  [14/21], [94mLoss[0m : 6.82181
[1mStep[0m  [16/21], [94mLoss[0m : 6.85900
[1mStep[0m  [18/21], [94mLoss[0m : 6.79982
[1mStep[0m  [20/21], [94mLoss[0m : 6.93698

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.858, [92mTest[0m: 6.899, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.61915
[1mStep[0m  [2/21], [94mLoss[0m : 7.02942
[1mStep[0m  [4/21], [94mLoss[0m : 6.68839
[1mStep[0m  [6/21], [94mLoss[0m : 6.87362
[1mStep[0m  [8/21], [94mLoss[0m : 6.70973
[1mStep[0m  [10/21], [94mLoss[0m : 6.62361
[1mStep[0m  [12/21], [94mLoss[0m : 6.75401
[1mStep[0m  [14/21], [94mLoss[0m : 6.61863
[1mStep[0m  [16/21], [94mLoss[0m : 6.66307
[1mStep[0m  [18/21], [94mLoss[0m : 6.40050
[1mStep[0m  [20/21], [94mLoss[0m : 6.68608

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.745, [92mTest[0m: 6.799, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.56939
[1mStep[0m  [2/21], [94mLoss[0m : 6.65913
[1mStep[0m  [4/21], [94mLoss[0m : 6.47514
[1mStep[0m  [6/21], [94mLoss[0m : 6.54921
[1mStep[0m  [8/21], [94mLoss[0m : 6.79296
[1mStep[0m  [10/21], [94mLoss[0m : 6.71342
[1mStep[0m  [12/21], [94mLoss[0m : 6.76896
[1mStep[0m  [14/21], [94mLoss[0m : 6.48283
[1mStep[0m  [16/21], [94mLoss[0m : 6.60866
[1mStep[0m  [18/21], [94mLoss[0m : 6.58579
[1mStep[0m  [20/21], [94mLoss[0m : 6.49051

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.638, [92mTest[0m: 6.682, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.63928
[1mStep[0m  [2/21], [94mLoss[0m : 6.65041
[1mStep[0m  [4/21], [94mLoss[0m : 6.43309
[1mStep[0m  [6/21], [94mLoss[0m : 6.74255
[1mStep[0m  [8/21], [94mLoss[0m : 6.35209
[1mStep[0m  [10/21], [94mLoss[0m : 6.30314
[1mStep[0m  [12/21], [94mLoss[0m : 6.47612
[1mStep[0m  [14/21], [94mLoss[0m : 6.40616
[1mStep[0m  [16/21], [94mLoss[0m : 6.58919
[1mStep[0m  [18/21], [94mLoss[0m : 6.64986
[1mStep[0m  [20/21], [94mLoss[0m : 6.51632

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.531, [92mTest[0m: 6.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.63926
[1mStep[0m  [2/21], [94mLoss[0m : 6.29055
[1mStep[0m  [4/21], [94mLoss[0m : 6.45016
[1mStep[0m  [6/21], [94mLoss[0m : 6.50992
[1mStep[0m  [8/21], [94mLoss[0m : 6.59824
[1mStep[0m  [10/21], [94mLoss[0m : 6.25851
[1mStep[0m  [12/21], [94mLoss[0m : 6.26419
[1mStep[0m  [14/21], [94mLoss[0m : 6.27630
[1mStep[0m  [16/21], [94mLoss[0m : 6.47524
[1mStep[0m  [18/21], [94mLoss[0m : 6.30012
[1mStep[0m  [20/21], [94mLoss[0m : 6.81919

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.420, [92mTest[0m: 6.453, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.40097
[1mStep[0m  [2/21], [94mLoss[0m : 6.66199
[1mStep[0m  [4/21], [94mLoss[0m : 6.49112
[1mStep[0m  [6/21], [94mLoss[0m : 6.40216
[1mStep[0m  [8/21], [94mLoss[0m : 6.08063
[1mStep[0m  [10/21], [94mLoss[0m : 6.41168
[1mStep[0m  [12/21], [94mLoss[0m : 5.93119
[1mStep[0m  [14/21], [94mLoss[0m : 6.30098
[1mStep[0m  [16/21], [94mLoss[0m : 6.28113
[1mStep[0m  [18/21], [94mLoss[0m : 6.40531
[1mStep[0m  [20/21], [94mLoss[0m : 6.51449

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.303, [92mTest[0m: 6.344, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.54950
[1mStep[0m  [2/21], [94mLoss[0m : 6.28325
[1mStep[0m  [4/21], [94mLoss[0m : 6.44920
[1mStep[0m  [6/21], [94mLoss[0m : 6.38244
[1mStep[0m  [8/21], [94mLoss[0m : 6.16180
[1mStep[0m  [10/21], [94mLoss[0m : 6.11223
[1mStep[0m  [12/21], [94mLoss[0m : 6.02995
[1mStep[0m  [14/21], [94mLoss[0m : 6.30575
[1mStep[0m  [16/21], [94mLoss[0m : 6.08678
[1mStep[0m  [18/21], [94mLoss[0m : 6.06270
[1mStep[0m  [20/21], [94mLoss[0m : 6.27037

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.196, [92mTest[0m: 6.227, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.00412
[1mStep[0m  [2/21], [94mLoss[0m : 6.30902
[1mStep[0m  [4/21], [94mLoss[0m : 6.21034
[1mStep[0m  [6/21], [94mLoss[0m : 6.11892
[1mStep[0m  [8/21], [94mLoss[0m : 5.94396
[1mStep[0m  [10/21], [94mLoss[0m : 5.94462
[1mStep[0m  [12/21], [94mLoss[0m : 6.08378
[1mStep[0m  [14/21], [94mLoss[0m : 6.00486
[1mStep[0m  [16/21], [94mLoss[0m : 5.95256
[1mStep[0m  [18/21], [94mLoss[0m : 6.25174
[1mStep[0m  [20/21], [94mLoss[0m : 6.13651

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.090, [92mTest[0m: 6.130, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.95805
[1mStep[0m  [2/21], [94mLoss[0m : 5.98234
[1mStep[0m  [4/21], [94mLoss[0m : 6.09423
[1mStep[0m  [6/21], [94mLoss[0m : 6.05922
[1mStep[0m  [8/21], [94mLoss[0m : 6.03381
[1mStep[0m  [10/21], [94mLoss[0m : 5.98240
[1mStep[0m  [12/21], [94mLoss[0m : 5.94921
[1mStep[0m  [14/21], [94mLoss[0m : 5.77785
[1mStep[0m  [16/21], [94mLoss[0m : 5.76892
[1mStep[0m  [18/21], [94mLoss[0m : 5.71109
[1mStep[0m  [20/21], [94mLoss[0m : 5.88823

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 5.968, [92mTest[0m: 6.022, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.95123
[1mStep[0m  [2/21], [94mLoss[0m : 5.93418
[1mStep[0m  [4/21], [94mLoss[0m : 5.90799
[1mStep[0m  [6/21], [94mLoss[0m : 5.90754
[1mStep[0m  [8/21], [94mLoss[0m : 5.92732
[1mStep[0m  [10/21], [94mLoss[0m : 5.84405
[1mStep[0m  [12/21], [94mLoss[0m : 5.69030
[1mStep[0m  [14/21], [94mLoss[0m : 5.79198
[1mStep[0m  [16/21], [94mLoss[0m : 5.61831
[1mStep[0m  [18/21], [94mLoss[0m : 6.12796
[1mStep[0m  [20/21], [94mLoss[0m : 5.96656

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 5.873, [92mTest[0m: 5.917, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.60302
[1mStep[0m  [2/21], [94mLoss[0m : 5.87803
[1mStep[0m  [4/21], [94mLoss[0m : 5.89060
[1mStep[0m  [6/21], [94mLoss[0m : 5.75377
[1mStep[0m  [8/21], [94mLoss[0m : 5.76596
[1mStep[0m  [10/21], [94mLoss[0m : 6.02468
[1mStep[0m  [12/21], [94mLoss[0m : 5.81116
[1mStep[0m  [14/21], [94mLoss[0m : 5.50879
[1mStep[0m  [16/21], [94mLoss[0m : 5.81699
[1mStep[0m  [18/21], [94mLoss[0m : 5.74917
[1mStep[0m  [20/21], [94mLoss[0m : 5.55415

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.754, [92mTest[0m: 5.796, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.79628
[1mStep[0m  [2/21], [94mLoss[0m : 5.86418
[1mStep[0m  [4/21], [94mLoss[0m : 5.50756
[1mStep[0m  [6/21], [94mLoss[0m : 5.68952
[1mStep[0m  [8/21], [94mLoss[0m : 5.71385
[1mStep[0m  [10/21], [94mLoss[0m : 5.73413
[1mStep[0m  [12/21], [94mLoss[0m : 5.60427
[1mStep[0m  [14/21], [94mLoss[0m : 5.56243
[1mStep[0m  [16/21], [94mLoss[0m : 5.56643
[1mStep[0m  [18/21], [94mLoss[0m : 5.64980
[1mStep[0m  [20/21], [94mLoss[0m : 5.47277

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.654, [92mTest[0m: 5.700, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.71819
[1mStep[0m  [2/21], [94mLoss[0m : 5.56099
[1mStep[0m  [4/21], [94mLoss[0m : 5.77742
[1mStep[0m  [6/21], [94mLoss[0m : 5.61652
[1mStep[0m  [8/21], [94mLoss[0m : 5.43029
[1mStep[0m  [10/21], [94mLoss[0m : 5.56963
[1mStep[0m  [12/21], [94mLoss[0m : 5.67745
[1mStep[0m  [14/21], [94mLoss[0m : 5.25800
[1mStep[0m  [16/21], [94mLoss[0m : 5.24838
[1mStep[0m  [18/21], [94mLoss[0m : 5.90786
[1mStep[0m  [20/21], [94mLoss[0m : 5.44496

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.541, [92mTest[0m: 5.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.61450
[1mStep[0m  [2/21], [94mLoss[0m : 5.64012
[1mStep[0m  [4/21], [94mLoss[0m : 5.58438
[1mStep[0m  [6/21], [94mLoss[0m : 5.29065
[1mStep[0m  [8/21], [94mLoss[0m : 5.52516
[1mStep[0m  [10/21], [94mLoss[0m : 5.34730
[1mStep[0m  [12/21], [94mLoss[0m : 5.43505
[1mStep[0m  [14/21], [94mLoss[0m : 5.77381
[1mStep[0m  [16/21], [94mLoss[0m : 5.32538
[1mStep[0m  [18/21], [94mLoss[0m : 5.48964
[1mStep[0m  [20/21], [94mLoss[0m : 5.26914

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.447, [92mTest[0m: 5.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.50620
[1mStep[0m  [2/21], [94mLoss[0m : 5.32783
[1mStep[0m  [4/21], [94mLoss[0m : 5.10765
[1mStep[0m  [6/21], [94mLoss[0m : 5.55542
[1mStep[0m  [8/21], [94mLoss[0m : 5.42610
[1mStep[0m  [10/21], [94mLoss[0m : 5.30936
[1mStep[0m  [12/21], [94mLoss[0m : 5.69527
[1mStep[0m  [14/21], [94mLoss[0m : 5.18003
[1mStep[0m  [16/21], [94mLoss[0m : 5.25826
[1mStep[0m  [18/21], [94mLoss[0m : 5.44259
[1mStep[0m  [20/21], [94mLoss[0m : 5.30070

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.341, [92mTest[0m: 5.370, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.10942
[1mStep[0m  [2/21], [94mLoss[0m : 5.15533
[1mStep[0m  [4/21], [94mLoss[0m : 5.22440
[1mStep[0m  [6/21], [94mLoss[0m : 5.34090
[1mStep[0m  [8/21], [94mLoss[0m : 5.22932
[1mStep[0m  [10/21], [94mLoss[0m : 5.29671
[1mStep[0m  [12/21], [94mLoss[0m : 5.35740
[1mStep[0m  [14/21], [94mLoss[0m : 5.26528
[1mStep[0m  [16/21], [94mLoss[0m : 4.97833
[1mStep[0m  [18/21], [94mLoss[0m : 5.57281
[1mStep[0m  [20/21], [94mLoss[0m : 5.19321

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.241, [92mTest[0m: 5.284, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.08785
[1mStep[0m  [2/21], [94mLoss[0m : 5.29561
[1mStep[0m  [4/21], [94mLoss[0m : 5.35484
[1mStep[0m  [6/21], [94mLoss[0m : 4.98923
[1mStep[0m  [8/21], [94mLoss[0m : 5.10458
[1mStep[0m  [10/21], [94mLoss[0m : 5.12426
[1mStep[0m  [12/21], [94mLoss[0m : 5.01758
[1mStep[0m  [14/21], [94mLoss[0m : 5.12166
[1mStep[0m  [16/21], [94mLoss[0m : 5.01618
[1mStep[0m  [18/21], [94mLoss[0m : 5.36888
[1mStep[0m  [20/21], [94mLoss[0m : 5.00040

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.149, [92mTest[0m: 5.187, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.42875
[1mStep[0m  [2/21], [94mLoss[0m : 4.98142
[1mStep[0m  [4/21], [94mLoss[0m : 5.16218
[1mStep[0m  [6/21], [94mLoss[0m : 5.06018
[1mStep[0m  [8/21], [94mLoss[0m : 5.36136
[1mStep[0m  [10/21], [94mLoss[0m : 4.97915
[1mStep[0m  [12/21], [94mLoss[0m : 4.70411
[1mStep[0m  [14/21], [94mLoss[0m : 5.04960
[1mStep[0m  [16/21], [94mLoss[0m : 4.99624
[1mStep[0m  [18/21], [94mLoss[0m : 5.03484
[1mStep[0m  [20/21], [94mLoss[0m : 5.10852

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.065, [92mTest[0m: 5.095, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16454
[1mStep[0m  [2/21], [94mLoss[0m : 5.10422
[1mStep[0m  [4/21], [94mLoss[0m : 5.03140
[1mStep[0m  [6/21], [94mLoss[0m : 5.03147
[1mStep[0m  [8/21], [94mLoss[0m : 4.96292
[1mStep[0m  [10/21], [94mLoss[0m : 4.73377
[1mStep[0m  [12/21], [94mLoss[0m : 5.12604
[1mStep[0m  [14/21], [94mLoss[0m : 4.96971
[1mStep[0m  [16/21], [94mLoss[0m : 4.71426
[1mStep[0m  [18/21], [94mLoss[0m : 4.79815
[1mStep[0m  [20/21], [94mLoss[0m : 4.85057

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.978, [92mTest[0m: 5.013, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.90900
[1mStep[0m  [2/21], [94mLoss[0m : 4.70566
[1mStep[0m  [4/21], [94mLoss[0m : 4.94012
[1mStep[0m  [6/21], [94mLoss[0m : 5.09716
[1mStep[0m  [8/21], [94mLoss[0m : 4.70616
[1mStep[0m  [10/21], [94mLoss[0m : 4.62794
[1mStep[0m  [12/21], [94mLoss[0m : 5.04474
[1mStep[0m  [14/21], [94mLoss[0m : 4.99555
[1mStep[0m  [16/21], [94mLoss[0m : 4.72379
[1mStep[0m  [18/21], [94mLoss[0m : 4.86475
[1mStep[0m  [20/21], [94mLoss[0m : 5.13838

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.896, [92mTest[0m: 4.930, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.06126
[1mStep[0m  [2/21], [94mLoss[0m : 4.64562
[1mStep[0m  [4/21], [94mLoss[0m : 4.84949
[1mStep[0m  [6/21], [94mLoss[0m : 4.98019
[1mStep[0m  [8/21], [94mLoss[0m : 4.69652
[1mStep[0m  [10/21], [94mLoss[0m : 4.86332
[1mStep[0m  [12/21], [94mLoss[0m : 4.69351
[1mStep[0m  [14/21], [94mLoss[0m : 4.73470
[1mStep[0m  [16/21], [94mLoss[0m : 4.73372
[1mStep[0m  [18/21], [94mLoss[0m : 4.78126
[1mStep[0m  [20/21], [94mLoss[0m : 4.88475

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.804, [92mTest[0m: 4.846, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.52104
[1mStep[0m  [2/21], [94mLoss[0m : 4.57474
[1mStep[0m  [4/21], [94mLoss[0m : 4.83343
[1mStep[0m  [6/21], [94mLoss[0m : 4.73215
[1mStep[0m  [8/21], [94mLoss[0m : 4.99269
[1mStep[0m  [10/21], [94mLoss[0m : 4.76975
[1mStep[0m  [12/21], [94mLoss[0m : 4.67284
[1mStep[0m  [14/21], [94mLoss[0m : 4.82419
[1mStep[0m  [16/21], [94mLoss[0m : 4.42353
[1mStep[0m  [18/21], [94mLoss[0m : 4.62873
[1mStep[0m  [20/21], [94mLoss[0m : 4.84728

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.741, [92mTest[0m: 4.775, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.61028
[1mStep[0m  [2/21], [94mLoss[0m : 4.39540
[1mStep[0m  [4/21], [94mLoss[0m : 4.62719
[1mStep[0m  [6/21], [94mLoss[0m : 4.81280
[1mStep[0m  [8/21], [94mLoss[0m : 4.55357
[1mStep[0m  [10/21], [94mLoss[0m : 4.65263
[1mStep[0m  [12/21], [94mLoss[0m : 4.45911
[1mStep[0m  [14/21], [94mLoss[0m : 4.48000
[1mStep[0m  [16/21], [94mLoss[0m : 4.53761
[1mStep[0m  [18/21], [94mLoss[0m : 4.77334
[1mStep[0m  [20/21], [94mLoss[0m : 4.88551

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.639, [92mTest[0m: 4.682, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.22976
[1mStep[0m  [2/21], [94mLoss[0m : 4.57973
[1mStep[0m  [4/21], [94mLoss[0m : 4.46548
[1mStep[0m  [6/21], [94mLoss[0m : 4.49195
[1mStep[0m  [8/21], [94mLoss[0m : 4.69900
[1mStep[0m  [10/21], [94mLoss[0m : 4.68670
[1mStep[0m  [12/21], [94mLoss[0m : 4.49026
[1mStep[0m  [14/21], [94mLoss[0m : 4.89190
[1mStep[0m  [16/21], [94mLoss[0m : 4.72132
[1mStep[0m  [18/21], [94mLoss[0m : 4.74167
[1mStep[0m  [20/21], [94mLoss[0m : 4.79302

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.585, [92mTest[0m: 4.608, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.23465
[1mStep[0m  [2/21], [94mLoss[0m : 4.66867
[1mStep[0m  [4/21], [94mLoss[0m : 4.49360
[1mStep[0m  [6/21], [94mLoss[0m : 4.53608
[1mStep[0m  [8/21], [94mLoss[0m : 4.49454
[1mStep[0m  [10/21], [94mLoss[0m : 4.46868
[1mStep[0m  [12/21], [94mLoss[0m : 4.41632
[1mStep[0m  [14/21], [94mLoss[0m : 4.60780
[1mStep[0m  [16/21], [94mLoss[0m : 4.59994
[1mStep[0m  [18/21], [94mLoss[0m : 4.33720
[1mStep[0m  [20/21], [94mLoss[0m : 4.35547

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.502, [92mTest[0m: 4.545, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.448
====================================

Phase 2 - Evaluation MAE:  4.447725704738072
MAE score P1      7.439707
MAE score P2      4.447726
loss              4.501743
learning_rate       0.0001
batch_size             512
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.02357
[1mStep[0m  [4/42], [94mLoss[0m : 10.45846
[1mStep[0m  [8/42], [94mLoss[0m : 10.89351
[1mStep[0m  [12/42], [94mLoss[0m : 10.88674
[1mStep[0m  [16/42], [94mLoss[0m : 10.72482
[1mStep[0m  [20/42], [94mLoss[0m : 10.98835
[1mStep[0m  [24/42], [94mLoss[0m : 10.76940
[1mStep[0m  [28/42], [94mLoss[0m : 10.85624
[1mStep[0m  [32/42], [94mLoss[0m : 10.80220
[1mStep[0m  [36/42], [94mLoss[0m : 11.22492
[1mStep[0m  [40/42], [94mLoss[0m : 10.88973

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.881, [92mTest[0m: 10.917, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22887
[1mStep[0m  [4/42], [94mLoss[0m : 10.65523
[1mStep[0m  [8/42], [94mLoss[0m : 10.87605
[1mStep[0m  [12/42], [94mLoss[0m : 10.58613
[1mStep[0m  [16/42], [94mLoss[0m : 10.41489
[1mStep[0m  [20/42], [94mLoss[0m : 10.34548
[1mStep[0m  [24/42], [94mLoss[0m : 10.53650
[1mStep[0m  [28/42], [94mLoss[0m : 10.42031
[1mStep[0m  [32/42], [94mLoss[0m : 10.28857
[1mStep[0m  [36/42], [94mLoss[0m : 10.53324
[1mStep[0m  [40/42], [94mLoss[0m : 9.93433

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.540, [92mTest[0m: 10.800, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37961
[1mStep[0m  [4/42], [94mLoss[0m : 10.25735
[1mStep[0m  [8/42], [94mLoss[0m : 10.44121
[1mStep[0m  [12/42], [94mLoss[0m : 10.65314
[1mStep[0m  [16/42], [94mLoss[0m : 10.24998
[1mStep[0m  [20/42], [94mLoss[0m : 9.97650
[1mStep[0m  [24/42], [94mLoss[0m : 10.35750
[1mStep[0m  [28/42], [94mLoss[0m : 10.22481
[1mStep[0m  [32/42], [94mLoss[0m : 10.32423
[1mStep[0m  [36/42], [94mLoss[0m : 10.13073
[1mStep[0m  [40/42], [94mLoss[0m : 9.98815

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.183, [92mTest[0m: 10.562, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.60639
[1mStep[0m  [4/42], [94mLoss[0m : 9.75584
[1mStep[0m  [8/42], [94mLoss[0m : 9.96594
[1mStep[0m  [12/42], [94mLoss[0m : 9.95408
[1mStep[0m  [16/42], [94mLoss[0m : 10.13057
[1mStep[0m  [20/42], [94mLoss[0m : 9.90115
[1mStep[0m  [24/42], [94mLoss[0m : 9.69671
[1mStep[0m  [28/42], [94mLoss[0m : 9.65876
[1mStep[0m  [32/42], [94mLoss[0m : 9.64434
[1mStep[0m  [36/42], [94mLoss[0m : 10.11019
[1mStep[0m  [40/42], [94mLoss[0m : 9.52514

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.814, [92mTest[0m: 10.347, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.85905
[1mStep[0m  [4/42], [94mLoss[0m : 9.63454
[1mStep[0m  [8/42], [94mLoss[0m : 9.46248
[1mStep[0m  [12/42], [94mLoss[0m : 9.55954
[1mStep[0m  [16/42], [94mLoss[0m : 9.23218
[1mStep[0m  [20/42], [94mLoss[0m : 9.66265
[1mStep[0m  [24/42], [94mLoss[0m : 9.38616
[1mStep[0m  [28/42], [94mLoss[0m : 9.63910
[1mStep[0m  [32/42], [94mLoss[0m : 9.28143
[1mStep[0m  [36/42], [94mLoss[0m : 9.23021
[1mStep[0m  [40/42], [94mLoss[0m : 8.79336

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.460, [92mTest[0m: 10.121, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.12159
[1mStep[0m  [4/42], [94mLoss[0m : 8.84424
[1mStep[0m  [8/42], [94mLoss[0m : 9.12849
[1mStep[0m  [12/42], [94mLoss[0m : 9.08946
[1mStep[0m  [16/42], [94mLoss[0m : 9.41495
[1mStep[0m  [20/42], [94mLoss[0m : 8.99930
[1mStep[0m  [24/42], [94mLoss[0m : 9.12306
[1mStep[0m  [28/42], [94mLoss[0m : 9.38598
[1mStep[0m  [32/42], [94mLoss[0m : 8.68843
[1mStep[0m  [36/42], [94mLoss[0m : 8.67005
[1mStep[0m  [40/42], [94mLoss[0m : 8.96920

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.091, [92mTest[0m: 9.871, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.91110
[1mStep[0m  [4/42], [94mLoss[0m : 8.36616
[1mStep[0m  [8/42], [94mLoss[0m : 8.78899
[1mStep[0m  [12/42], [94mLoss[0m : 9.14250
[1mStep[0m  [16/42], [94mLoss[0m : 9.04532
[1mStep[0m  [20/42], [94mLoss[0m : 8.65917
[1mStep[0m  [24/42], [94mLoss[0m : 9.04891
[1mStep[0m  [28/42], [94mLoss[0m : 8.33320
[1mStep[0m  [32/42], [94mLoss[0m : 8.65427
[1mStep[0m  [36/42], [94mLoss[0m : 8.50547
[1mStep[0m  [40/42], [94mLoss[0m : 8.73796

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.729, [92mTest[0m: 9.646, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.79564
[1mStep[0m  [4/42], [94mLoss[0m : 8.36579
[1mStep[0m  [8/42], [94mLoss[0m : 8.64261
[1mStep[0m  [12/42], [94mLoss[0m : 8.38502
[1mStep[0m  [16/42], [94mLoss[0m : 8.41845
[1mStep[0m  [20/42], [94mLoss[0m : 8.72970
[1mStep[0m  [24/42], [94mLoss[0m : 8.20504
[1mStep[0m  [28/42], [94mLoss[0m : 8.47472
[1mStep[0m  [32/42], [94mLoss[0m : 8.57100
[1mStep[0m  [36/42], [94mLoss[0m : 8.11700
[1mStep[0m  [40/42], [94mLoss[0m : 7.85294

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.342, [92mTest[0m: 9.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.75588
[1mStep[0m  [4/42], [94mLoss[0m : 8.03977
[1mStep[0m  [8/42], [94mLoss[0m : 8.00531
[1mStep[0m  [12/42], [94mLoss[0m : 7.81577
[1mStep[0m  [16/42], [94mLoss[0m : 7.64081
[1mStep[0m  [20/42], [94mLoss[0m : 8.05115
[1mStep[0m  [24/42], [94mLoss[0m : 7.84623
[1mStep[0m  [28/42], [94mLoss[0m : 8.25599
[1mStep[0m  [32/42], [94mLoss[0m : 8.00880
[1mStep[0m  [36/42], [94mLoss[0m : 8.00249
[1mStep[0m  [40/42], [94mLoss[0m : 7.90992

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.961, [92mTest[0m: 9.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.71714
[1mStep[0m  [4/42], [94mLoss[0m : 7.92621
[1mStep[0m  [8/42], [94mLoss[0m : 7.72020
[1mStep[0m  [12/42], [94mLoss[0m : 7.39566
[1mStep[0m  [16/42], [94mLoss[0m : 7.52720
[1mStep[0m  [20/42], [94mLoss[0m : 7.62081
[1mStep[0m  [24/42], [94mLoss[0m : 7.42923
[1mStep[0m  [28/42], [94mLoss[0m : 7.74148
[1mStep[0m  [32/42], [94mLoss[0m : 7.54722
[1mStep[0m  [36/42], [94mLoss[0m : 7.60794
[1mStep[0m  [40/42], [94mLoss[0m : 7.03039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.570, [92mTest[0m: 8.925, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.43006
[1mStep[0m  [4/42], [94mLoss[0m : 7.07905
[1mStep[0m  [8/42], [94mLoss[0m : 7.68699
[1mStep[0m  [12/42], [94mLoss[0m : 7.44547
[1mStep[0m  [16/42], [94mLoss[0m : 7.38844
[1mStep[0m  [20/42], [94mLoss[0m : 6.79352
[1mStep[0m  [24/42], [94mLoss[0m : 6.68663
[1mStep[0m  [28/42], [94mLoss[0m : 7.04630
[1mStep[0m  [32/42], [94mLoss[0m : 6.85564
[1mStep[0m  [36/42], [94mLoss[0m : 7.00653
[1mStep[0m  [40/42], [94mLoss[0m : 6.89516

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.176, [92mTest[0m: 8.648, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.12662
[1mStep[0m  [4/42], [94mLoss[0m : 6.98350
[1mStep[0m  [8/42], [94mLoss[0m : 6.76066
[1mStep[0m  [12/42], [94mLoss[0m : 6.53471
[1mStep[0m  [16/42], [94mLoss[0m : 6.97828
[1mStep[0m  [20/42], [94mLoss[0m : 6.66248
[1mStep[0m  [24/42], [94mLoss[0m : 6.88331
[1mStep[0m  [28/42], [94mLoss[0m : 6.52852
[1mStep[0m  [32/42], [94mLoss[0m : 6.52417
[1mStep[0m  [36/42], [94mLoss[0m : 6.36052
[1mStep[0m  [40/42], [94mLoss[0m : 6.51719

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.782, [92mTest[0m: 8.355, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.48849
[1mStep[0m  [4/42], [94mLoss[0m : 6.63607
[1mStep[0m  [8/42], [94mLoss[0m : 6.84747
[1mStep[0m  [12/42], [94mLoss[0m : 6.44289
[1mStep[0m  [16/42], [94mLoss[0m : 6.19937
[1mStep[0m  [20/42], [94mLoss[0m : 6.09852
[1mStep[0m  [24/42], [94mLoss[0m : 6.81312
[1mStep[0m  [28/42], [94mLoss[0m : 6.26528
[1mStep[0m  [32/42], [94mLoss[0m : 6.04860
[1mStep[0m  [36/42], [94mLoss[0m : 6.60420
[1mStep[0m  [40/42], [94mLoss[0m : 5.90981

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.400, [92mTest[0m: 8.040, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 12 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.737
====================================

Phase 1 - Evaluation MAE:  7.736732278551374
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 6.20117
[1mStep[0m  [4/42], [94mLoss[0m : 6.14179
[1mStep[0m  [8/42], [94mLoss[0m : 5.87397
[1mStep[0m  [12/42], [94mLoss[0m : 6.01840
[1mStep[0m  [16/42], [94mLoss[0m : 5.96454
[1mStep[0m  [20/42], [94mLoss[0m : 6.13379
[1mStep[0m  [24/42], [94mLoss[0m : 6.05244
[1mStep[0m  [28/42], [94mLoss[0m : 6.17521
[1mStep[0m  [32/42], [94mLoss[0m : 5.71290
[1mStep[0m  [36/42], [94mLoss[0m : 5.68884
[1mStep[0m  [40/42], [94mLoss[0m : 6.20133

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.080, [92mTest[0m: 7.734, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.436
====================================

Phase 2 - Evaluation MAE:  7.435987029756818
MAE score P1       7.736732
MAE score P2       7.435987
loss               6.079923
learning_rate        0.0001
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.9
weight_decay          0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.28051
[1mStep[0m  [4/42], [94mLoss[0m : 10.81010
[1mStep[0m  [8/42], [94mLoss[0m : 11.29729
[1mStep[0m  [12/42], [94mLoss[0m : 10.88285
[1mStep[0m  [16/42], [94mLoss[0m : 10.76284
[1mStep[0m  [20/42], [94mLoss[0m : 10.60859
[1mStep[0m  [24/42], [94mLoss[0m : 10.22848
[1mStep[0m  [28/42], [94mLoss[0m : 10.18137
[1mStep[0m  [32/42], [94mLoss[0m : 10.37414
[1mStep[0m  [36/42], [94mLoss[0m : 9.91081
[1mStep[0m  [40/42], [94mLoss[0m : 9.87815

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.666, [92mTest[0m: 11.021, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08882
[1mStep[0m  [4/42], [94mLoss[0m : 9.86107
[1mStep[0m  [8/42], [94mLoss[0m : 9.57269
[1mStep[0m  [12/42], [94mLoss[0m : 9.26687
[1mStep[0m  [16/42], [94mLoss[0m : 9.45565
[1mStep[0m  [20/42], [94mLoss[0m : 9.38470
[1mStep[0m  [24/42], [94mLoss[0m : 8.94618
[1mStep[0m  [28/42], [94mLoss[0m : 9.06605
[1mStep[0m  [32/42], [94mLoss[0m : 9.00717
[1mStep[0m  [36/42], [94mLoss[0m : 8.70110
[1mStep[0m  [40/42], [94mLoss[0m : 8.54594

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.133, [92mTest[0m: 10.113, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34796
[1mStep[0m  [4/42], [94mLoss[0m : 7.93289
[1mStep[0m  [8/42], [94mLoss[0m : 8.00848
[1mStep[0m  [12/42], [94mLoss[0m : 7.50792
[1mStep[0m  [16/42], [94mLoss[0m : 7.59878
[1mStep[0m  [20/42], [94mLoss[0m : 7.48022
[1mStep[0m  [24/42], [94mLoss[0m : 7.20739
[1mStep[0m  [28/42], [94mLoss[0m : 7.15354
[1mStep[0m  [32/42], [94mLoss[0m : 7.29007
[1mStep[0m  [36/42], [94mLoss[0m : 6.93893
[1mStep[0m  [40/42], [94mLoss[0m : 6.86521

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.525, [92mTest[0m: 8.778, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.49779
[1mStep[0m  [4/42], [94mLoss[0m : 6.57612
[1mStep[0m  [8/42], [94mLoss[0m : 6.69826
[1mStep[0m  [12/42], [94mLoss[0m : 6.46417
[1mStep[0m  [16/42], [94mLoss[0m : 6.41795
[1mStep[0m  [20/42], [94mLoss[0m : 5.93266
[1mStep[0m  [24/42], [94mLoss[0m : 6.05239
[1mStep[0m  [28/42], [94mLoss[0m : 5.65483
[1mStep[0m  [32/42], [94mLoss[0m : 5.99187
[1mStep[0m  [36/42], [94mLoss[0m : 5.90731
[1mStep[0m  [40/42], [94mLoss[0m : 5.60891

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.062, [92mTest[0m: 7.443, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.55942
[1mStep[0m  [4/42], [94mLoss[0m : 4.87009
[1mStep[0m  [8/42], [94mLoss[0m : 4.81433
[1mStep[0m  [12/42], [94mLoss[0m : 5.39942
[1mStep[0m  [16/42], [94mLoss[0m : 4.96476
[1mStep[0m  [20/42], [94mLoss[0m : 4.94679
[1mStep[0m  [24/42], [94mLoss[0m : 5.02570
[1mStep[0m  [28/42], [94mLoss[0m : 4.64526
[1mStep[0m  [32/42], [94mLoss[0m : 4.43928
[1mStep[0m  [36/42], [94mLoss[0m : 4.86889
[1mStep[0m  [40/42], [94mLoss[0m : 4.47561

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.922, [92mTest[0m: 6.262, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.41017
[1mStep[0m  [4/42], [94mLoss[0m : 4.21838
[1mStep[0m  [8/42], [94mLoss[0m : 3.65825
[1mStep[0m  [12/42], [94mLoss[0m : 4.22138
[1mStep[0m  [16/42], [94mLoss[0m : 4.28087
[1mStep[0m  [20/42], [94mLoss[0m : 4.07962
[1mStep[0m  [24/42], [94mLoss[0m : 3.82173
[1mStep[0m  [28/42], [94mLoss[0m : 3.73627
[1mStep[0m  [32/42], [94mLoss[0m : 3.97592
[1mStep[0m  [36/42], [94mLoss[0m : 3.90709
[1mStep[0m  [40/42], [94mLoss[0m : 3.88771

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.015, [92mTest[0m: 5.246, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.27332
[1mStep[0m  [4/42], [94mLoss[0m : 3.61024
[1mStep[0m  [8/42], [94mLoss[0m : 3.41293
[1mStep[0m  [12/42], [94mLoss[0m : 3.36102
[1mStep[0m  [16/42], [94mLoss[0m : 3.40624
[1mStep[0m  [20/42], [94mLoss[0m : 3.39127
[1mStep[0m  [24/42], [94mLoss[0m : 3.37461
[1mStep[0m  [28/42], [94mLoss[0m : 3.61845
[1mStep[0m  [32/42], [94mLoss[0m : 3.07154
[1mStep[0m  [36/42], [94mLoss[0m : 2.92912
[1mStep[0m  [40/42], [94mLoss[0m : 3.03647

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.371, [92mTest[0m: 4.299, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.25741
[1mStep[0m  [4/42], [94mLoss[0m : 3.21881
[1mStep[0m  [8/42], [94mLoss[0m : 2.81304
[1mStep[0m  [12/42], [94mLoss[0m : 3.15079
[1mStep[0m  [16/42], [94mLoss[0m : 2.97568
[1mStep[0m  [20/42], [94mLoss[0m : 2.95631
[1mStep[0m  [24/42], [94mLoss[0m : 3.15232
[1mStep[0m  [28/42], [94mLoss[0m : 2.91318
[1mStep[0m  [32/42], [94mLoss[0m : 2.92107
[1mStep[0m  [36/42], [94mLoss[0m : 2.84047
[1mStep[0m  [40/42], [94mLoss[0m : 2.95137

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.993, [92mTest[0m: 3.609, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.10094
[1mStep[0m  [4/42], [94mLoss[0m : 3.06289
[1mStep[0m  [8/42], [94mLoss[0m : 2.85035
[1mStep[0m  [12/42], [94mLoss[0m : 2.67634
[1mStep[0m  [16/42], [94mLoss[0m : 2.92031
[1mStep[0m  [20/42], [94mLoss[0m : 2.71144
[1mStep[0m  [24/42], [94mLoss[0m : 3.01566
[1mStep[0m  [28/42], [94mLoss[0m : 3.01204
[1mStep[0m  [32/42], [94mLoss[0m : 2.78982
[1mStep[0m  [36/42], [94mLoss[0m : 2.69758
[1mStep[0m  [40/42], [94mLoss[0m : 2.60412

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.807, [92mTest[0m: 3.167, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65426
[1mStep[0m  [4/42], [94mLoss[0m : 2.76311
[1mStep[0m  [8/42], [94mLoss[0m : 2.74386
[1mStep[0m  [12/42], [94mLoss[0m : 2.67234
[1mStep[0m  [16/42], [94mLoss[0m : 2.64223
[1mStep[0m  [20/42], [94mLoss[0m : 2.72270
[1mStep[0m  [24/42], [94mLoss[0m : 2.51374
[1mStep[0m  [28/42], [94mLoss[0m : 2.74635
[1mStep[0m  [32/42], [94mLoss[0m : 2.54324
[1mStep[0m  [36/42], [94mLoss[0m : 2.60925
[1mStep[0m  [40/42], [94mLoss[0m : 2.58303

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71124
[1mStep[0m  [4/42], [94mLoss[0m : 2.58277
[1mStep[0m  [8/42], [94mLoss[0m : 2.57603
[1mStep[0m  [12/42], [94mLoss[0m : 2.67669
[1mStep[0m  [16/42], [94mLoss[0m : 2.55710
[1mStep[0m  [20/42], [94mLoss[0m : 2.76817
[1mStep[0m  [24/42], [94mLoss[0m : 2.62064
[1mStep[0m  [28/42], [94mLoss[0m : 2.49755
[1mStep[0m  [32/42], [94mLoss[0m : 2.55228
[1mStep[0m  [36/42], [94mLoss[0m : 2.50557
[1mStep[0m  [40/42], [94mLoss[0m : 2.74099

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.659, [92mTest[0m: 2.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43622
[1mStep[0m  [4/42], [94mLoss[0m : 2.71605
[1mStep[0m  [8/42], [94mLoss[0m : 2.74127
[1mStep[0m  [12/42], [94mLoss[0m : 2.84244
[1mStep[0m  [16/42], [94mLoss[0m : 2.57853
[1mStep[0m  [20/42], [94mLoss[0m : 2.57695
[1mStep[0m  [24/42], [94mLoss[0m : 2.43780
[1mStep[0m  [28/42], [94mLoss[0m : 2.69344
[1mStep[0m  [32/42], [94mLoss[0m : 2.58934
[1mStep[0m  [36/42], [94mLoss[0m : 2.52550
[1mStep[0m  [40/42], [94mLoss[0m : 2.52476

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.753, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51944
[1mStep[0m  [4/42], [94mLoss[0m : 2.61539
[1mStep[0m  [8/42], [94mLoss[0m : 2.73770
[1mStep[0m  [12/42], [94mLoss[0m : 2.34425
[1mStep[0m  [16/42], [94mLoss[0m : 2.67678
[1mStep[0m  [20/42], [94mLoss[0m : 2.51990
[1mStep[0m  [24/42], [94mLoss[0m : 2.89448
[1mStep[0m  [28/42], [94mLoss[0m : 2.38450
[1mStep[0m  [32/42], [94mLoss[0m : 2.54119
[1mStep[0m  [36/42], [94mLoss[0m : 2.81634
[1mStep[0m  [40/42], [94mLoss[0m : 2.56677

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55992
[1mStep[0m  [4/42], [94mLoss[0m : 2.68540
[1mStep[0m  [8/42], [94mLoss[0m : 2.83202
[1mStep[0m  [12/42], [94mLoss[0m : 2.71263
[1mStep[0m  [16/42], [94mLoss[0m : 2.67400
[1mStep[0m  [20/42], [94mLoss[0m : 2.81869
[1mStep[0m  [24/42], [94mLoss[0m : 2.62701
[1mStep[0m  [28/42], [94mLoss[0m : 2.69421
[1mStep[0m  [32/42], [94mLoss[0m : 2.57700
[1mStep[0m  [36/42], [94mLoss[0m : 2.52466
[1mStep[0m  [40/42], [94mLoss[0m : 2.61506

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66957
[1mStep[0m  [4/42], [94mLoss[0m : 2.53972
[1mStep[0m  [8/42], [94mLoss[0m : 2.52592
[1mStep[0m  [12/42], [94mLoss[0m : 2.55102
[1mStep[0m  [16/42], [94mLoss[0m : 2.58718
[1mStep[0m  [20/42], [94mLoss[0m : 2.61537
[1mStep[0m  [24/42], [94mLoss[0m : 2.62315
[1mStep[0m  [28/42], [94mLoss[0m : 2.59950
[1mStep[0m  [32/42], [94mLoss[0m : 2.69506
[1mStep[0m  [36/42], [94mLoss[0m : 2.51140
[1mStep[0m  [40/42], [94mLoss[0m : 2.67295

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.663, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40376
[1mStep[0m  [4/42], [94mLoss[0m : 2.52470
[1mStep[0m  [8/42], [94mLoss[0m : 2.45948
[1mStep[0m  [12/42], [94mLoss[0m : 2.53557
[1mStep[0m  [16/42], [94mLoss[0m : 2.53300
[1mStep[0m  [20/42], [94mLoss[0m : 2.63694
[1mStep[0m  [24/42], [94mLoss[0m : 2.56737
[1mStep[0m  [28/42], [94mLoss[0m : 2.70006
[1mStep[0m  [32/42], [94mLoss[0m : 2.46161
[1mStep[0m  [36/42], [94mLoss[0m : 2.66458
[1mStep[0m  [40/42], [94mLoss[0m : 2.45656

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.631, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52216
[1mStep[0m  [4/42], [94mLoss[0m : 2.56719
[1mStep[0m  [8/42], [94mLoss[0m : 2.52330
[1mStep[0m  [12/42], [94mLoss[0m : 2.54314
[1mStep[0m  [16/42], [94mLoss[0m : 2.55516
[1mStep[0m  [20/42], [94mLoss[0m : 2.69331
[1mStep[0m  [24/42], [94mLoss[0m : 2.64500
[1mStep[0m  [28/42], [94mLoss[0m : 2.60752
[1mStep[0m  [32/42], [94mLoss[0m : 2.54777
[1mStep[0m  [36/42], [94mLoss[0m : 2.53105
[1mStep[0m  [40/42], [94mLoss[0m : 2.67168

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58768
[1mStep[0m  [4/42], [94mLoss[0m : 2.61916
[1mStep[0m  [8/42], [94mLoss[0m : 2.63683
[1mStep[0m  [12/42], [94mLoss[0m : 2.60439
[1mStep[0m  [16/42], [94mLoss[0m : 2.60675
[1mStep[0m  [20/42], [94mLoss[0m : 2.80758
[1mStep[0m  [24/42], [94mLoss[0m : 2.54488
[1mStep[0m  [28/42], [94mLoss[0m : 2.55590
[1mStep[0m  [32/42], [94mLoss[0m : 2.46005
[1mStep[0m  [36/42], [94mLoss[0m : 2.51018
[1mStep[0m  [40/42], [94mLoss[0m : 2.69151

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51799
[1mStep[0m  [4/42], [94mLoss[0m : 2.59872
[1mStep[0m  [8/42], [94mLoss[0m : 2.58137
[1mStep[0m  [12/42], [94mLoss[0m : 2.77923
[1mStep[0m  [16/42], [94mLoss[0m : 2.52313
[1mStep[0m  [20/42], [94mLoss[0m : 2.53945
[1mStep[0m  [24/42], [94mLoss[0m : 2.43949
[1mStep[0m  [28/42], [94mLoss[0m : 2.32255
[1mStep[0m  [32/42], [94mLoss[0m : 2.68656
[1mStep[0m  [36/42], [94mLoss[0m : 2.72567
[1mStep[0m  [40/42], [94mLoss[0m : 2.50989

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.568, [92mTest[0m: 2.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60669
[1mStep[0m  [4/42], [94mLoss[0m : 2.51773
[1mStep[0m  [8/42], [94mLoss[0m : 2.55137
[1mStep[0m  [12/42], [94mLoss[0m : 2.57606
[1mStep[0m  [16/42], [94mLoss[0m : 2.63364
[1mStep[0m  [20/42], [94mLoss[0m : 2.56874
[1mStep[0m  [24/42], [94mLoss[0m : 2.59482
[1mStep[0m  [28/42], [94mLoss[0m : 2.31072
[1mStep[0m  [32/42], [94mLoss[0m : 2.56872
[1mStep[0m  [36/42], [94mLoss[0m : 2.60467
[1mStep[0m  [40/42], [94mLoss[0m : 2.54374

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.587, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55737
[1mStep[0m  [4/42], [94mLoss[0m : 2.45840
[1mStep[0m  [8/42], [94mLoss[0m : 2.44608
[1mStep[0m  [12/42], [94mLoss[0m : 2.61689
[1mStep[0m  [16/42], [94mLoss[0m : 2.69363
[1mStep[0m  [20/42], [94mLoss[0m : 2.66958
[1mStep[0m  [24/42], [94mLoss[0m : 2.45057
[1mStep[0m  [28/42], [94mLoss[0m : 2.52955
[1mStep[0m  [32/42], [94mLoss[0m : 2.53251
[1mStep[0m  [36/42], [94mLoss[0m : 2.50684
[1mStep[0m  [40/42], [94mLoss[0m : 2.56721

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.592, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47039
[1mStep[0m  [4/42], [94mLoss[0m : 2.54025
[1mStep[0m  [8/42], [94mLoss[0m : 2.28864
[1mStep[0m  [12/42], [94mLoss[0m : 2.58856
[1mStep[0m  [16/42], [94mLoss[0m : 2.50066
[1mStep[0m  [20/42], [94mLoss[0m : 2.72672
[1mStep[0m  [24/42], [94mLoss[0m : 2.62694
[1mStep[0m  [28/42], [94mLoss[0m : 2.48391
[1mStep[0m  [32/42], [94mLoss[0m : 2.69823
[1mStep[0m  [36/42], [94mLoss[0m : 2.49456
[1mStep[0m  [40/42], [94mLoss[0m : 2.59297

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79086
[1mStep[0m  [4/42], [94mLoss[0m : 2.55718
[1mStep[0m  [8/42], [94mLoss[0m : 2.35545
[1mStep[0m  [12/42], [94mLoss[0m : 2.40811
[1mStep[0m  [16/42], [94mLoss[0m : 2.42982
[1mStep[0m  [20/42], [94mLoss[0m : 2.53066
[1mStep[0m  [24/42], [94mLoss[0m : 2.45939
[1mStep[0m  [28/42], [94mLoss[0m : 2.48029
[1mStep[0m  [32/42], [94mLoss[0m : 2.49767
[1mStep[0m  [36/42], [94mLoss[0m : 2.38672
[1mStep[0m  [40/42], [94mLoss[0m : 2.43127

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.569, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46153
[1mStep[0m  [4/42], [94mLoss[0m : 2.69231
[1mStep[0m  [8/42], [94mLoss[0m : 2.58792
[1mStep[0m  [12/42], [94mLoss[0m : 2.27433
[1mStep[0m  [16/42], [94mLoss[0m : 2.64788
[1mStep[0m  [20/42], [94mLoss[0m : 2.70734
[1mStep[0m  [24/42], [94mLoss[0m : 2.45029
[1mStep[0m  [28/42], [94mLoss[0m : 2.61765
[1mStep[0m  [32/42], [94mLoss[0m : 2.63197
[1mStep[0m  [36/42], [94mLoss[0m : 2.58178
[1mStep[0m  [40/42], [94mLoss[0m : 2.56208

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.565, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53084
[1mStep[0m  [4/42], [94mLoss[0m : 2.70474
[1mStep[0m  [8/42], [94mLoss[0m : 2.57338
[1mStep[0m  [12/42], [94mLoss[0m : 2.61288
[1mStep[0m  [16/42], [94mLoss[0m : 2.34045
[1mStep[0m  [20/42], [94mLoss[0m : 2.31539
[1mStep[0m  [24/42], [94mLoss[0m : 2.56175
[1mStep[0m  [28/42], [94mLoss[0m : 2.43408
[1mStep[0m  [32/42], [94mLoss[0m : 2.65838
[1mStep[0m  [36/42], [94mLoss[0m : 2.56968
[1mStep[0m  [40/42], [94mLoss[0m : 2.52231

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.576, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77143
[1mStep[0m  [4/42], [94mLoss[0m : 2.51690
[1mStep[0m  [8/42], [94mLoss[0m : 2.50941
[1mStep[0m  [12/42], [94mLoss[0m : 2.44601
[1mStep[0m  [16/42], [94mLoss[0m : 2.35863
[1mStep[0m  [20/42], [94mLoss[0m : 2.31044
[1mStep[0m  [24/42], [94mLoss[0m : 2.57177
[1mStep[0m  [28/42], [94mLoss[0m : 2.80687
[1mStep[0m  [32/42], [94mLoss[0m : 2.37268
[1mStep[0m  [36/42], [94mLoss[0m : 2.47400
[1mStep[0m  [40/42], [94mLoss[0m : 2.45854

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.71421
[1mStep[0m  [4/42], [94mLoss[0m : 2.32305
[1mStep[0m  [8/42], [94mLoss[0m : 2.45089
[1mStep[0m  [12/42], [94mLoss[0m : 2.46989
[1mStep[0m  [16/42], [94mLoss[0m : 2.40740
[1mStep[0m  [20/42], [94mLoss[0m : 2.41372
[1mStep[0m  [24/42], [94mLoss[0m : 2.48999
[1mStep[0m  [28/42], [94mLoss[0m : 2.53843
[1mStep[0m  [32/42], [94mLoss[0m : 2.54583
[1mStep[0m  [36/42], [94mLoss[0m : 2.49418
[1mStep[0m  [40/42], [94mLoss[0m : 2.37286

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.570, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43014
[1mStep[0m  [4/42], [94mLoss[0m : 2.50627
[1mStep[0m  [8/42], [94mLoss[0m : 2.56024
[1mStep[0m  [12/42], [94mLoss[0m : 2.58958
[1mStep[0m  [16/42], [94mLoss[0m : 2.47035
[1mStep[0m  [20/42], [94mLoss[0m : 2.35188
[1mStep[0m  [24/42], [94mLoss[0m : 2.59154
[1mStep[0m  [28/42], [94mLoss[0m : 2.35277
[1mStep[0m  [32/42], [94mLoss[0m : 2.37192
[1mStep[0m  [36/42], [94mLoss[0m : 2.35029
[1mStep[0m  [40/42], [94mLoss[0m : 2.57404

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33985
[1mStep[0m  [4/42], [94mLoss[0m : 2.58636
[1mStep[0m  [8/42], [94mLoss[0m : 2.70327
[1mStep[0m  [12/42], [94mLoss[0m : 2.56972
[1mStep[0m  [16/42], [94mLoss[0m : 2.49798
[1mStep[0m  [20/42], [94mLoss[0m : 2.43314
[1mStep[0m  [24/42], [94mLoss[0m : 2.38462
[1mStep[0m  [28/42], [94mLoss[0m : 2.36114
[1mStep[0m  [32/42], [94mLoss[0m : 2.57123
[1mStep[0m  [36/42], [94mLoss[0m : 2.63761
[1mStep[0m  [40/42], [94mLoss[0m : 2.71512

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.552, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43552
[1mStep[0m  [4/42], [94mLoss[0m : 2.51212
[1mStep[0m  [8/42], [94mLoss[0m : 2.42997
[1mStep[0m  [12/42], [94mLoss[0m : 2.38508
[1mStep[0m  [16/42], [94mLoss[0m : 2.26356
[1mStep[0m  [20/42], [94mLoss[0m : 2.57052
[1mStep[0m  [24/42], [94mLoss[0m : 2.69448
[1mStep[0m  [28/42], [94mLoss[0m : 2.65805
[1mStep[0m  [32/42], [94mLoss[0m : 2.75202
[1mStep[0m  [36/42], [94mLoss[0m : 2.37452
[1mStep[0m  [40/42], [94mLoss[0m : 2.54210

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.563
====================================

Phase 1 - Evaluation MAE:  2.5629040002822876
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.66751
[1mStep[0m  [4/42], [94mLoss[0m : 2.43258
[1mStep[0m  [8/42], [94mLoss[0m : 2.56171
[1mStep[0m  [12/42], [94mLoss[0m : 2.80669
[1mStep[0m  [16/42], [94mLoss[0m : 2.36649
[1mStep[0m  [20/42], [94mLoss[0m : 2.56267
[1mStep[0m  [24/42], [94mLoss[0m : 2.84837
[1mStep[0m  [28/42], [94mLoss[0m : 2.50949
[1mStep[0m  [32/42], [94mLoss[0m : 2.46141
[1mStep[0m  [36/42], [94mLoss[0m : 2.46433
[1mStep[0m  [40/42], [94mLoss[0m : 2.47472

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53916
[1mStep[0m  [4/42], [94mLoss[0m : 2.59791
[1mStep[0m  [8/42], [94mLoss[0m : 2.54819
[1mStep[0m  [12/42], [94mLoss[0m : 2.28575
[1mStep[0m  [16/42], [94mLoss[0m : 2.49831
[1mStep[0m  [20/42], [94mLoss[0m : 2.60041
[1mStep[0m  [24/42], [94mLoss[0m : 2.25072
[1mStep[0m  [28/42], [94mLoss[0m : 2.65498
[1mStep[0m  [32/42], [94mLoss[0m : 2.59334
[1mStep[0m  [36/42], [94mLoss[0m : 2.43091
[1mStep[0m  [40/42], [94mLoss[0m : 2.23357

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.555, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53424
[1mStep[0m  [4/42], [94mLoss[0m : 2.33201
[1mStep[0m  [8/42], [94mLoss[0m : 2.44750
[1mStep[0m  [12/42], [94mLoss[0m : 2.45269
[1mStep[0m  [16/42], [94mLoss[0m : 2.41543
[1mStep[0m  [20/42], [94mLoss[0m : 2.40748
[1mStep[0m  [24/42], [94mLoss[0m : 2.22078
[1mStep[0m  [28/42], [94mLoss[0m : 2.61172
[1mStep[0m  [32/42], [94mLoss[0m : 2.58392
[1mStep[0m  [36/42], [94mLoss[0m : 2.46780
[1mStep[0m  [40/42], [94mLoss[0m : 2.59022

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.451, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43746
[1mStep[0m  [4/42], [94mLoss[0m : 2.52481
[1mStep[0m  [8/42], [94mLoss[0m : 2.45353
[1mStep[0m  [12/42], [94mLoss[0m : 2.51792
[1mStep[0m  [16/42], [94mLoss[0m : 2.36394
[1mStep[0m  [20/42], [94mLoss[0m : 2.53456
[1mStep[0m  [24/42], [94mLoss[0m : 2.41915
[1mStep[0m  [28/42], [94mLoss[0m : 2.34720
[1mStep[0m  [32/42], [94mLoss[0m : 2.48776
[1mStep[0m  [36/42], [94mLoss[0m : 2.64361
[1mStep[0m  [40/42], [94mLoss[0m : 2.36009

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64398
[1mStep[0m  [4/42], [94mLoss[0m : 2.54576
[1mStep[0m  [8/42], [94mLoss[0m : 2.42837
[1mStep[0m  [12/42], [94mLoss[0m : 2.42621
[1mStep[0m  [16/42], [94mLoss[0m : 2.49312
[1mStep[0m  [20/42], [94mLoss[0m : 2.36485
[1mStep[0m  [24/42], [94mLoss[0m : 2.41812
[1mStep[0m  [28/42], [94mLoss[0m : 2.48142
[1mStep[0m  [32/42], [94mLoss[0m : 2.57742
[1mStep[0m  [36/42], [94mLoss[0m : 2.57991
[1mStep[0m  [40/42], [94mLoss[0m : 2.59122

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65083
[1mStep[0m  [4/42], [94mLoss[0m : 2.39806
[1mStep[0m  [8/42], [94mLoss[0m : 2.49107
[1mStep[0m  [12/42], [94mLoss[0m : 2.42090
[1mStep[0m  [16/42], [94mLoss[0m : 2.32850
[1mStep[0m  [20/42], [94mLoss[0m : 2.21352
[1mStep[0m  [24/42], [94mLoss[0m : 2.42025
[1mStep[0m  [28/42], [94mLoss[0m : 2.25319
[1mStep[0m  [32/42], [94mLoss[0m : 2.40055
[1mStep[0m  [36/42], [94mLoss[0m : 2.30384
[1mStep[0m  [40/42], [94mLoss[0m : 2.43126

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.454, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.73972
[1mStep[0m  [4/42], [94mLoss[0m : 2.62963
[1mStep[0m  [8/42], [94mLoss[0m : 2.45064
[1mStep[0m  [12/42], [94mLoss[0m : 2.50972
[1mStep[0m  [16/42], [94mLoss[0m : 2.39067
[1mStep[0m  [20/42], [94mLoss[0m : 2.30758
[1mStep[0m  [24/42], [94mLoss[0m : 2.58047
[1mStep[0m  [28/42], [94mLoss[0m : 2.49773
[1mStep[0m  [32/42], [94mLoss[0m : 2.45499
[1mStep[0m  [36/42], [94mLoss[0m : 2.50762
[1mStep[0m  [40/42], [94mLoss[0m : 2.37319

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34554
[1mStep[0m  [4/42], [94mLoss[0m : 2.42030
[1mStep[0m  [8/42], [94mLoss[0m : 2.32839
[1mStep[0m  [12/42], [94mLoss[0m : 2.38551
[1mStep[0m  [16/42], [94mLoss[0m : 2.39319
[1mStep[0m  [20/42], [94mLoss[0m : 2.30367
[1mStep[0m  [24/42], [94mLoss[0m : 2.24293
[1mStep[0m  [28/42], [94mLoss[0m : 2.60491
[1mStep[0m  [32/42], [94mLoss[0m : 2.35981
[1mStep[0m  [36/42], [94mLoss[0m : 2.52544
[1mStep[0m  [40/42], [94mLoss[0m : 2.14496

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.567, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33820
[1mStep[0m  [4/42], [94mLoss[0m : 2.34209
[1mStep[0m  [8/42], [94mLoss[0m : 2.11028
[1mStep[0m  [12/42], [94mLoss[0m : 2.32094
[1mStep[0m  [16/42], [94mLoss[0m : 2.56990
[1mStep[0m  [20/42], [94mLoss[0m : 2.43226
[1mStep[0m  [24/42], [94mLoss[0m : 2.43116
[1mStep[0m  [28/42], [94mLoss[0m : 2.20501
[1mStep[0m  [32/42], [94mLoss[0m : 2.52185
[1mStep[0m  [36/42], [94mLoss[0m : 2.49075
[1mStep[0m  [40/42], [94mLoss[0m : 2.40906

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.517, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42586
[1mStep[0m  [4/42], [94mLoss[0m : 2.46734
[1mStep[0m  [8/42], [94mLoss[0m : 2.46359
[1mStep[0m  [12/42], [94mLoss[0m : 2.20845
[1mStep[0m  [16/42], [94mLoss[0m : 2.49492
[1mStep[0m  [20/42], [94mLoss[0m : 2.42758
[1mStep[0m  [24/42], [94mLoss[0m : 2.47406
[1mStep[0m  [28/42], [94mLoss[0m : 2.37613
[1mStep[0m  [32/42], [94mLoss[0m : 2.30637
[1mStep[0m  [36/42], [94mLoss[0m : 2.64033
[1mStep[0m  [40/42], [94mLoss[0m : 2.28637

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.633, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41184
[1mStep[0m  [4/42], [94mLoss[0m : 2.31380
[1mStep[0m  [8/42], [94mLoss[0m : 2.35187
[1mStep[0m  [12/42], [94mLoss[0m : 2.45090
[1mStep[0m  [16/42], [94mLoss[0m : 2.64776
[1mStep[0m  [20/42], [94mLoss[0m : 2.37567
[1mStep[0m  [24/42], [94mLoss[0m : 2.31136
[1mStep[0m  [28/42], [94mLoss[0m : 2.34637
[1mStep[0m  [32/42], [94mLoss[0m : 2.36001
[1mStep[0m  [36/42], [94mLoss[0m : 2.47287
[1mStep[0m  [40/42], [94mLoss[0m : 2.87839

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21795
[1mStep[0m  [4/42], [94mLoss[0m : 2.55859
[1mStep[0m  [8/42], [94mLoss[0m : 2.40665
[1mStep[0m  [12/42], [94mLoss[0m : 2.50512
[1mStep[0m  [16/42], [94mLoss[0m : 2.16908
[1mStep[0m  [20/42], [94mLoss[0m : 2.41198
[1mStep[0m  [24/42], [94mLoss[0m : 2.59104
[1mStep[0m  [28/42], [94mLoss[0m : 2.36172
[1mStep[0m  [32/42], [94mLoss[0m : 2.07386
[1mStep[0m  [36/42], [94mLoss[0m : 2.35365
[1mStep[0m  [40/42], [94mLoss[0m : 2.48594

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.523, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50102
[1mStep[0m  [4/42], [94mLoss[0m : 2.20834
[1mStep[0m  [8/42], [94mLoss[0m : 2.47206
[1mStep[0m  [12/42], [94mLoss[0m : 2.43581
[1mStep[0m  [16/42], [94mLoss[0m : 2.32087
[1mStep[0m  [20/42], [94mLoss[0m : 2.44090
[1mStep[0m  [24/42], [94mLoss[0m : 2.27336
[1mStep[0m  [28/42], [94mLoss[0m : 2.17054
[1mStep[0m  [32/42], [94mLoss[0m : 2.55420
[1mStep[0m  [36/42], [94mLoss[0m : 2.40605
[1mStep[0m  [40/42], [94mLoss[0m : 2.28195

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.502, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42806
[1mStep[0m  [4/42], [94mLoss[0m : 2.49691
[1mStep[0m  [8/42], [94mLoss[0m : 2.54050
[1mStep[0m  [12/42], [94mLoss[0m : 2.19840
[1mStep[0m  [16/42], [94mLoss[0m : 2.37360
[1mStep[0m  [20/42], [94mLoss[0m : 2.73717
[1mStep[0m  [24/42], [94mLoss[0m : 2.61504
[1mStep[0m  [28/42], [94mLoss[0m : 2.39457
[1mStep[0m  [32/42], [94mLoss[0m : 2.21534
[1mStep[0m  [36/42], [94mLoss[0m : 2.36115
[1mStep[0m  [40/42], [94mLoss[0m : 2.48726

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46948
[1mStep[0m  [4/42], [94mLoss[0m : 2.26583
[1mStep[0m  [8/42], [94mLoss[0m : 2.08036
[1mStep[0m  [12/42], [94mLoss[0m : 2.40161
[1mStep[0m  [16/42], [94mLoss[0m : 2.38643
[1mStep[0m  [20/42], [94mLoss[0m : 2.25691
[1mStep[0m  [24/42], [94mLoss[0m : 2.35437
[1mStep[0m  [28/42], [94mLoss[0m : 2.13011
[1mStep[0m  [32/42], [94mLoss[0m : 2.42536
[1mStep[0m  [36/42], [94mLoss[0m : 2.43285
[1mStep[0m  [40/42], [94mLoss[0m : 2.34929

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.486, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26146
[1mStep[0m  [4/42], [94mLoss[0m : 2.27798
[1mStep[0m  [8/42], [94mLoss[0m : 2.09043
[1mStep[0m  [12/42], [94mLoss[0m : 2.44930
[1mStep[0m  [16/42], [94mLoss[0m : 2.33309
[1mStep[0m  [20/42], [94mLoss[0m : 2.29928
[1mStep[0m  [24/42], [94mLoss[0m : 2.22940
[1mStep[0m  [28/42], [94mLoss[0m : 2.18946
[1mStep[0m  [32/42], [94mLoss[0m : 2.50663
[1mStep[0m  [36/42], [94mLoss[0m : 2.37598
[1mStep[0m  [40/42], [94mLoss[0m : 2.42744

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57870
[1mStep[0m  [4/42], [94mLoss[0m : 2.30045
[1mStep[0m  [8/42], [94mLoss[0m : 2.42479
[1mStep[0m  [12/42], [94mLoss[0m : 2.35445
[1mStep[0m  [16/42], [94mLoss[0m : 2.35660
[1mStep[0m  [20/42], [94mLoss[0m : 2.28571
[1mStep[0m  [24/42], [94mLoss[0m : 2.59403
[1mStep[0m  [28/42], [94mLoss[0m : 2.44553
[1mStep[0m  [32/42], [94mLoss[0m : 2.47092
[1mStep[0m  [36/42], [94mLoss[0m : 2.30909
[1mStep[0m  [40/42], [94mLoss[0m : 2.32888

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.485, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21783
[1mStep[0m  [4/42], [94mLoss[0m : 2.38274
[1mStep[0m  [8/42], [94mLoss[0m : 2.30187
[1mStep[0m  [12/42], [94mLoss[0m : 2.40433
[1mStep[0m  [16/42], [94mLoss[0m : 2.35518
[1mStep[0m  [20/42], [94mLoss[0m : 2.40942
[1mStep[0m  [24/42], [94mLoss[0m : 2.21340
[1mStep[0m  [28/42], [94mLoss[0m : 2.26420
[1mStep[0m  [32/42], [94mLoss[0m : 2.25271
[1mStep[0m  [36/42], [94mLoss[0m : 2.59878
[1mStep[0m  [40/42], [94mLoss[0m : 2.35806

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.495, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51680
[1mStep[0m  [4/42], [94mLoss[0m : 2.29193
[1mStep[0m  [8/42], [94mLoss[0m : 2.35082
[1mStep[0m  [12/42], [94mLoss[0m : 2.42543
[1mStep[0m  [16/42], [94mLoss[0m : 2.32986
[1mStep[0m  [20/42], [94mLoss[0m : 2.17879
[1mStep[0m  [24/42], [94mLoss[0m : 2.29777
[1mStep[0m  [28/42], [94mLoss[0m : 2.36061
[1mStep[0m  [32/42], [94mLoss[0m : 2.44588
[1mStep[0m  [36/42], [94mLoss[0m : 2.33401
[1mStep[0m  [40/42], [94mLoss[0m : 2.22541

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34033
[1mStep[0m  [4/42], [94mLoss[0m : 2.43761
[1mStep[0m  [8/42], [94mLoss[0m : 2.16138
[1mStep[0m  [12/42], [94mLoss[0m : 2.36036
[1mStep[0m  [16/42], [94mLoss[0m : 2.26167
[1mStep[0m  [20/42], [94mLoss[0m : 2.32547
[1mStep[0m  [24/42], [94mLoss[0m : 2.26238
[1mStep[0m  [28/42], [94mLoss[0m : 2.40773
[1mStep[0m  [32/42], [94mLoss[0m : 2.39682
[1mStep[0m  [36/42], [94mLoss[0m : 2.56959
[1mStep[0m  [40/42], [94mLoss[0m : 2.25195

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.489, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39595
[1mStep[0m  [4/42], [94mLoss[0m : 2.25119
[1mStep[0m  [8/42], [94mLoss[0m : 2.29747
[1mStep[0m  [12/42], [94mLoss[0m : 2.36847
[1mStep[0m  [16/42], [94mLoss[0m : 2.37046
[1mStep[0m  [20/42], [94mLoss[0m : 2.38394
[1mStep[0m  [24/42], [94mLoss[0m : 2.41375
[1mStep[0m  [28/42], [94mLoss[0m : 2.12364
[1mStep[0m  [32/42], [94mLoss[0m : 2.39473
[1mStep[0m  [36/42], [94mLoss[0m : 2.26445
[1mStep[0m  [40/42], [94mLoss[0m : 2.21860

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.482, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34174
[1mStep[0m  [4/42], [94mLoss[0m : 2.35999
[1mStep[0m  [8/42], [94mLoss[0m : 2.31359
[1mStep[0m  [12/42], [94mLoss[0m : 2.28139
[1mStep[0m  [16/42], [94mLoss[0m : 2.31491
[1mStep[0m  [20/42], [94mLoss[0m : 2.33036
[1mStep[0m  [24/42], [94mLoss[0m : 2.27920
[1mStep[0m  [28/42], [94mLoss[0m : 2.46624
[1mStep[0m  [32/42], [94mLoss[0m : 2.36494
[1mStep[0m  [36/42], [94mLoss[0m : 2.21914
[1mStep[0m  [40/42], [94mLoss[0m : 2.31474

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25032
[1mStep[0m  [4/42], [94mLoss[0m : 2.46000
[1mStep[0m  [8/42], [94mLoss[0m : 2.31263
[1mStep[0m  [12/42], [94mLoss[0m : 2.32636
[1mStep[0m  [16/42], [94mLoss[0m : 2.28425
[1mStep[0m  [20/42], [94mLoss[0m : 2.36737
[1mStep[0m  [24/42], [94mLoss[0m : 2.40721
[1mStep[0m  [28/42], [94mLoss[0m : 2.46590
[1mStep[0m  [32/42], [94mLoss[0m : 2.41841
[1mStep[0m  [36/42], [94mLoss[0m : 2.33938
[1mStep[0m  [40/42], [94mLoss[0m : 2.42542

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39807
[1mStep[0m  [4/42], [94mLoss[0m : 2.40700
[1mStep[0m  [8/42], [94mLoss[0m : 2.48998
[1mStep[0m  [12/42], [94mLoss[0m : 2.52627
[1mStep[0m  [16/42], [94mLoss[0m : 2.24659
[1mStep[0m  [20/42], [94mLoss[0m : 2.39835
[1mStep[0m  [24/42], [94mLoss[0m : 2.11966
[1mStep[0m  [28/42], [94mLoss[0m : 2.38888
[1mStep[0m  [32/42], [94mLoss[0m : 2.35870
[1mStep[0m  [36/42], [94mLoss[0m : 2.32353
[1mStep[0m  [40/42], [94mLoss[0m : 2.38436

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.23394
[1mStep[0m  [4/42], [94mLoss[0m : 2.43812
[1mStep[0m  [8/42], [94mLoss[0m : 2.28684
[1mStep[0m  [12/42], [94mLoss[0m : 2.66727
[1mStep[0m  [16/42], [94mLoss[0m : 2.34039
[1mStep[0m  [20/42], [94mLoss[0m : 2.29115
[1mStep[0m  [24/42], [94mLoss[0m : 2.17729
[1mStep[0m  [28/42], [94mLoss[0m : 2.40277
[1mStep[0m  [32/42], [94mLoss[0m : 2.41140
[1mStep[0m  [36/42], [94mLoss[0m : 2.16933
[1mStep[0m  [40/42], [94mLoss[0m : 2.37762

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.315, [92mTest[0m: 2.454, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44021
[1mStep[0m  [4/42], [94mLoss[0m : 2.32999
[1mStep[0m  [8/42], [94mLoss[0m : 2.47378
[1mStep[0m  [12/42], [94mLoss[0m : 2.41937
[1mStep[0m  [16/42], [94mLoss[0m : 2.30492
[1mStep[0m  [20/42], [94mLoss[0m : 2.26629
[1mStep[0m  [24/42], [94mLoss[0m : 2.14990
[1mStep[0m  [28/42], [94mLoss[0m : 2.31951
[1mStep[0m  [32/42], [94mLoss[0m : 2.64888
[1mStep[0m  [36/42], [94mLoss[0m : 2.42203
[1mStep[0m  [40/42], [94mLoss[0m : 2.01563

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.17117
[1mStep[0m  [4/42], [94mLoss[0m : 2.22795
[1mStep[0m  [8/42], [94mLoss[0m : 2.46834
[1mStep[0m  [12/42], [94mLoss[0m : 2.35851
[1mStep[0m  [16/42], [94mLoss[0m : 2.41432
[1mStep[0m  [20/42], [94mLoss[0m : 2.36065
[1mStep[0m  [24/42], [94mLoss[0m : 2.28887
[1mStep[0m  [28/42], [94mLoss[0m : 2.36316
[1mStep[0m  [32/42], [94mLoss[0m : 2.21954
[1mStep[0m  [36/42], [94mLoss[0m : 2.40827
[1mStep[0m  [40/42], [94mLoss[0m : 2.25767

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.291, [92mTest[0m: 2.469, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.14584
[1mStep[0m  [4/42], [94mLoss[0m : 2.16367
[1mStep[0m  [8/42], [94mLoss[0m : 2.15780
[1mStep[0m  [12/42], [94mLoss[0m : 2.28175
[1mStep[0m  [16/42], [94mLoss[0m : 2.21447
[1mStep[0m  [20/42], [94mLoss[0m : 2.22861
[1mStep[0m  [24/42], [94mLoss[0m : 2.22470
[1mStep[0m  [28/42], [94mLoss[0m : 2.13185
[1mStep[0m  [32/42], [94mLoss[0m : 2.36592
[1mStep[0m  [36/42], [94mLoss[0m : 2.20930
[1mStep[0m  [40/42], [94mLoss[0m : 2.39352

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.276, [92mTest[0m: 2.449, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21843
[1mStep[0m  [4/42], [94mLoss[0m : 2.27276
[1mStep[0m  [8/42], [94mLoss[0m : 2.59944
[1mStep[0m  [12/42], [94mLoss[0m : 2.23775
[1mStep[0m  [16/42], [94mLoss[0m : 2.07926
[1mStep[0m  [20/42], [94mLoss[0m : 2.27403
[1mStep[0m  [24/42], [94mLoss[0m : 2.30214
[1mStep[0m  [28/42], [94mLoss[0m : 2.22649
[1mStep[0m  [32/42], [94mLoss[0m : 2.20901
[1mStep[0m  [36/42], [94mLoss[0m : 2.24574
[1mStep[0m  [40/42], [94mLoss[0m : 2.15938

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.294, [92mTest[0m: 2.451, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.13694
[1mStep[0m  [4/42], [94mLoss[0m : 2.18709
[1mStep[0m  [8/42], [94mLoss[0m : 2.39831
[1mStep[0m  [12/42], [94mLoss[0m : 2.02223
[1mStep[0m  [16/42], [94mLoss[0m : 2.30182
[1mStep[0m  [20/42], [94mLoss[0m : 2.34419
[1mStep[0m  [24/42], [94mLoss[0m : 2.16679
[1mStep[0m  [28/42], [94mLoss[0m : 2.29199
[1mStep[0m  [32/42], [94mLoss[0m : 1.90611
[1mStep[0m  [36/42], [94mLoss[0m : 2.33999
[1mStep[0m  [40/42], [94mLoss[0m : 2.45285

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.418
====================================

Phase 2 - Evaluation MAE:  2.417530723980495
MAE score P1      2.562904
MAE score P2      2.417531
loss              2.267877
learning_rate       0.0001
batch_size             256
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay         0.001
Name: 8, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.80951
[1mStep[0m  [2/21], [94mLoss[0m : 10.93556
[1mStep[0m  [4/21], [94mLoss[0m : 10.74454
[1mStep[0m  [6/21], [94mLoss[0m : 10.90201
[1mStep[0m  [8/21], [94mLoss[0m : 10.87796
[1mStep[0m  [10/21], [94mLoss[0m : 10.81299
[1mStep[0m  [12/21], [94mLoss[0m : 11.08576
[1mStep[0m  [14/21], [94mLoss[0m : 10.80932
[1mStep[0m  [16/21], [94mLoss[0m : 10.85832
[1mStep[0m  [18/21], [94mLoss[0m : 10.82970
[1mStep[0m  [20/21], [94mLoss[0m : 10.96139

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.903, [92mTest[0m: 10.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79074
[1mStep[0m  [2/21], [94mLoss[0m : 10.85137
[1mStep[0m  [4/21], [94mLoss[0m : 10.83388
[1mStep[0m  [6/21], [94mLoss[0m : 11.00640
[1mStep[0m  [8/21], [94mLoss[0m : 10.61938
[1mStep[0m  [10/21], [94mLoss[0m : 10.81643
[1mStep[0m  [12/21], [94mLoss[0m : 11.11787
[1mStep[0m  [14/21], [94mLoss[0m : 10.76423
[1mStep[0m  [16/21], [94mLoss[0m : 10.92006
[1mStep[0m  [18/21], [94mLoss[0m : 10.85557
[1mStep[0m  [20/21], [94mLoss[0m : 11.12740

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.922, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65120
[1mStep[0m  [2/21], [94mLoss[0m : 11.03664
[1mStep[0m  [4/21], [94mLoss[0m : 11.14472
[1mStep[0m  [6/21], [94mLoss[0m : 11.03809
[1mStep[0m  [8/21], [94mLoss[0m : 10.72875
[1mStep[0m  [10/21], [94mLoss[0m : 10.86415
[1mStep[0m  [12/21], [94mLoss[0m : 11.14936
[1mStep[0m  [14/21], [94mLoss[0m : 10.86957
[1mStep[0m  [16/21], [94mLoss[0m : 10.98267
[1mStep[0m  [18/21], [94mLoss[0m : 10.65443
[1mStep[0m  [20/21], [94mLoss[0m : 10.95384

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.886, [92mTest[0m: 10.908, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93341
[1mStep[0m  [2/21], [94mLoss[0m : 10.91423
[1mStep[0m  [4/21], [94mLoss[0m : 10.67571
[1mStep[0m  [6/21], [94mLoss[0m : 11.11070
[1mStep[0m  [8/21], [94mLoss[0m : 10.88339
[1mStep[0m  [10/21], [94mLoss[0m : 10.87459
[1mStep[0m  [12/21], [94mLoss[0m : 10.86917
[1mStep[0m  [14/21], [94mLoss[0m : 10.81484
[1mStep[0m  [16/21], [94mLoss[0m : 11.06024
[1mStep[0m  [18/21], [94mLoss[0m : 10.69322
[1mStep[0m  [20/21], [94mLoss[0m : 10.58666

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.873, [92mTest[0m: 10.898, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.03311
[1mStep[0m  [2/21], [94mLoss[0m : 10.99310
[1mStep[0m  [4/21], [94mLoss[0m : 10.74670
[1mStep[0m  [6/21], [94mLoss[0m : 11.14080
[1mStep[0m  [8/21], [94mLoss[0m : 10.75738
[1mStep[0m  [10/21], [94mLoss[0m : 10.79356
[1mStep[0m  [12/21], [94mLoss[0m : 10.91016
[1mStep[0m  [14/21], [94mLoss[0m : 10.67193
[1mStep[0m  [16/21], [94mLoss[0m : 10.73530
[1mStep[0m  [18/21], [94mLoss[0m : 10.76888
[1mStep[0m  [20/21], [94mLoss[0m : 10.81094

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.870, [92mTest[0m: 10.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.87527
[1mStep[0m  [2/21], [94mLoss[0m : 10.75453
[1mStep[0m  [4/21], [94mLoss[0m : 10.62049
[1mStep[0m  [6/21], [94mLoss[0m : 10.92424
[1mStep[0m  [8/21], [94mLoss[0m : 10.93293
[1mStep[0m  [10/21], [94mLoss[0m : 10.77154
[1mStep[0m  [12/21], [94mLoss[0m : 10.78378
[1mStep[0m  [14/21], [94mLoss[0m : 11.08602
[1mStep[0m  [16/21], [94mLoss[0m : 10.60857
[1mStep[0m  [18/21], [94mLoss[0m : 10.96676
[1mStep[0m  [20/21], [94mLoss[0m : 10.85204

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.863, [92mTest[0m: 10.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96774
[1mStep[0m  [2/21], [94mLoss[0m : 10.68416
[1mStep[0m  [4/21], [94mLoss[0m : 10.87214
[1mStep[0m  [6/21], [94mLoss[0m : 10.81534
[1mStep[0m  [8/21], [94mLoss[0m : 10.57739
[1mStep[0m  [10/21], [94mLoss[0m : 10.86902
[1mStep[0m  [12/21], [94mLoss[0m : 10.93585
[1mStep[0m  [14/21], [94mLoss[0m : 10.99964
[1mStep[0m  [16/21], [94mLoss[0m : 10.78572
[1mStep[0m  [18/21], [94mLoss[0m : 11.16607
[1mStep[0m  [20/21], [94mLoss[0m : 10.82440

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.855, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75355
[1mStep[0m  [2/21], [94mLoss[0m : 11.04253
[1mStep[0m  [4/21], [94mLoss[0m : 10.92780
[1mStep[0m  [6/21], [94mLoss[0m : 10.66062
[1mStep[0m  [8/21], [94mLoss[0m : 10.53903
[1mStep[0m  [10/21], [94mLoss[0m : 10.89272
[1mStep[0m  [12/21], [94mLoss[0m : 10.82638
[1mStep[0m  [14/21], [94mLoss[0m : 10.82084
[1mStep[0m  [16/21], [94mLoss[0m : 10.51553
[1mStep[0m  [18/21], [94mLoss[0m : 10.93546
[1mStep[0m  [20/21], [94mLoss[0m : 10.84105

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.842, [92mTest[0m: 10.850, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90470
[1mStep[0m  [2/21], [94mLoss[0m : 10.72355
[1mStep[0m  [4/21], [94mLoss[0m : 11.06102
[1mStep[0m  [6/21], [94mLoss[0m : 10.97968
[1mStep[0m  [8/21], [94mLoss[0m : 10.61411
[1mStep[0m  [10/21], [94mLoss[0m : 10.73932
[1mStep[0m  [12/21], [94mLoss[0m : 10.92674
[1mStep[0m  [14/21], [94mLoss[0m : 11.11120
[1mStep[0m  [16/21], [94mLoss[0m : 10.58200
[1mStep[0m  [18/21], [94mLoss[0m : 10.78848
[1mStep[0m  [20/21], [94mLoss[0m : 10.96755

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.838, [92mTest[0m: 10.835, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90626
[1mStep[0m  [2/21], [94mLoss[0m : 10.82613
[1mStep[0m  [4/21], [94mLoss[0m : 10.62813
[1mStep[0m  [6/21], [94mLoss[0m : 10.86747
[1mStep[0m  [8/21], [94mLoss[0m : 10.89399
[1mStep[0m  [10/21], [94mLoss[0m : 10.68798
[1mStep[0m  [12/21], [94mLoss[0m : 10.89074
[1mStep[0m  [14/21], [94mLoss[0m : 10.63421
[1mStep[0m  [16/21], [94mLoss[0m : 11.22266
[1mStep[0m  [18/21], [94mLoss[0m : 10.73687
[1mStep[0m  [20/21], [94mLoss[0m : 10.87039

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.838, [92mTest[0m: 10.827, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.91713
[1mStep[0m  [2/21], [94mLoss[0m : 10.77353
[1mStep[0m  [4/21], [94mLoss[0m : 10.95818
[1mStep[0m  [6/21], [94mLoss[0m : 10.80936
[1mStep[0m  [8/21], [94mLoss[0m : 10.82084
[1mStep[0m  [10/21], [94mLoss[0m : 10.99556
[1mStep[0m  [12/21], [94mLoss[0m : 11.02540
[1mStep[0m  [14/21], [94mLoss[0m : 10.93323
[1mStep[0m  [16/21], [94mLoss[0m : 10.81505
[1mStep[0m  [18/21], [94mLoss[0m : 10.93637
[1mStep[0m  [20/21], [94mLoss[0m : 10.55937

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.828, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.77357
[1mStep[0m  [2/21], [94mLoss[0m : 10.92934
[1mStep[0m  [4/21], [94mLoss[0m : 10.89673
[1mStep[0m  [6/21], [94mLoss[0m : 10.92359
[1mStep[0m  [8/21], [94mLoss[0m : 10.80156
[1mStep[0m  [10/21], [94mLoss[0m : 10.83067
[1mStep[0m  [12/21], [94mLoss[0m : 10.66759
[1mStep[0m  [14/21], [94mLoss[0m : 10.83468
[1mStep[0m  [16/21], [94mLoss[0m : 10.66477
[1mStep[0m  [18/21], [94mLoss[0m : 10.63996
[1mStep[0m  [20/21], [94mLoss[0m : 10.64377

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.813, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55591
[1mStep[0m  [2/21], [94mLoss[0m : 10.83618
[1mStep[0m  [4/21], [94mLoss[0m : 10.87293
[1mStep[0m  [6/21], [94mLoss[0m : 10.82271
[1mStep[0m  [8/21], [94mLoss[0m : 11.05472
[1mStep[0m  [10/21], [94mLoss[0m : 10.81845
[1mStep[0m  [12/21], [94mLoss[0m : 10.90735
[1mStep[0m  [14/21], [94mLoss[0m : 10.76456
[1mStep[0m  [16/21], [94mLoss[0m : 10.78837
[1mStep[0m  [18/21], [94mLoss[0m : 10.85934
[1mStep[0m  [20/21], [94mLoss[0m : 11.00858

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.811, [92mTest[0m: 10.805, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69408
[1mStep[0m  [2/21], [94mLoss[0m : 10.87434
[1mStep[0m  [4/21], [94mLoss[0m : 10.70594
[1mStep[0m  [6/21], [94mLoss[0m : 10.93237
[1mStep[0m  [8/21], [94mLoss[0m : 10.85981
[1mStep[0m  [10/21], [94mLoss[0m : 10.42479
[1mStep[0m  [12/21], [94mLoss[0m : 10.75381
[1mStep[0m  [14/21], [94mLoss[0m : 10.93600
[1mStep[0m  [16/21], [94mLoss[0m : 10.75181
[1mStep[0m  [18/21], [94mLoss[0m : 10.78079
[1mStep[0m  [20/21], [94mLoss[0m : 10.82646

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.804, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58828
[1mStep[0m  [2/21], [94mLoss[0m : 10.85374
[1mStep[0m  [4/21], [94mLoss[0m : 10.60693
[1mStep[0m  [6/21], [94mLoss[0m : 10.82623
[1mStep[0m  [8/21], [94mLoss[0m : 10.75470
[1mStep[0m  [10/21], [94mLoss[0m : 11.04156
[1mStep[0m  [12/21], [94mLoss[0m : 10.99791
[1mStep[0m  [14/21], [94mLoss[0m : 10.80465
[1mStep[0m  [16/21], [94mLoss[0m : 10.80853
[1mStep[0m  [18/21], [94mLoss[0m : 11.03279
[1mStep[0m  [20/21], [94mLoss[0m : 10.69119

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74767
[1mStep[0m  [2/21], [94mLoss[0m : 11.06818
[1mStep[0m  [4/21], [94mLoss[0m : 10.48548
[1mStep[0m  [6/21], [94mLoss[0m : 10.86822
[1mStep[0m  [8/21], [94mLoss[0m : 10.66889
[1mStep[0m  [10/21], [94mLoss[0m : 10.75305
[1mStep[0m  [12/21], [94mLoss[0m : 11.05798
[1mStep[0m  [14/21], [94mLoss[0m : 10.92886
[1mStep[0m  [16/21], [94mLoss[0m : 10.85568
[1mStep[0m  [18/21], [94mLoss[0m : 10.78411
[1mStep[0m  [20/21], [94mLoss[0m : 10.55302

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.795, [92mTest[0m: 10.777, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86808
[1mStep[0m  [2/21], [94mLoss[0m : 10.65281
[1mStep[0m  [4/21], [94mLoss[0m : 10.54592
[1mStep[0m  [6/21], [94mLoss[0m : 10.87262
[1mStep[0m  [8/21], [94mLoss[0m : 11.03928
[1mStep[0m  [10/21], [94mLoss[0m : 10.82289
[1mStep[0m  [12/21], [94mLoss[0m : 10.68886
[1mStep[0m  [14/21], [94mLoss[0m : 10.63922
[1mStep[0m  [16/21], [94mLoss[0m : 10.82266
[1mStep[0m  [18/21], [94mLoss[0m : 10.91739
[1mStep[0m  [20/21], [94mLoss[0m : 10.84805

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.783, [92mTest[0m: 10.769, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74057
[1mStep[0m  [2/21], [94mLoss[0m : 10.76848
[1mStep[0m  [4/21], [94mLoss[0m : 10.95999
[1mStep[0m  [6/21], [94mLoss[0m : 10.71218
[1mStep[0m  [8/21], [94mLoss[0m : 10.75697
[1mStep[0m  [10/21], [94mLoss[0m : 10.90398
[1mStep[0m  [12/21], [94mLoss[0m : 10.79383
[1mStep[0m  [14/21], [94mLoss[0m : 10.68849
[1mStep[0m  [16/21], [94mLoss[0m : 10.48373
[1mStep[0m  [18/21], [94mLoss[0m : 10.86101
[1mStep[0m  [20/21], [94mLoss[0m : 10.87163

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.780, [92mTest[0m: 10.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75112
[1mStep[0m  [2/21], [94mLoss[0m : 10.76866
[1mStep[0m  [4/21], [94mLoss[0m : 10.95742
[1mStep[0m  [6/21], [94mLoss[0m : 10.62471
[1mStep[0m  [8/21], [94mLoss[0m : 10.84258
[1mStep[0m  [10/21], [94mLoss[0m : 10.66597
[1mStep[0m  [12/21], [94mLoss[0m : 10.52934
[1mStep[0m  [14/21], [94mLoss[0m : 10.68753
[1mStep[0m  [16/21], [94mLoss[0m : 10.69459
[1mStep[0m  [18/21], [94mLoss[0m : 10.71222
[1mStep[0m  [20/21], [94mLoss[0m : 10.79453

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.746, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81669
[1mStep[0m  [2/21], [94mLoss[0m : 10.88218
[1mStep[0m  [4/21], [94mLoss[0m : 10.61536
[1mStep[0m  [6/21], [94mLoss[0m : 10.47771
[1mStep[0m  [8/21], [94mLoss[0m : 10.66098
[1mStep[0m  [10/21], [94mLoss[0m : 10.97038
[1mStep[0m  [12/21], [94mLoss[0m : 10.76425
[1mStep[0m  [14/21], [94mLoss[0m : 10.58396
[1mStep[0m  [16/21], [94mLoss[0m : 11.10900
[1mStep[0m  [18/21], [94mLoss[0m : 10.74361
[1mStep[0m  [20/21], [94mLoss[0m : 10.69842

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.767, [92mTest[0m: 10.737, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67681
[1mStep[0m  [2/21], [94mLoss[0m : 11.17911
[1mStep[0m  [4/21], [94mLoss[0m : 10.79730
[1mStep[0m  [6/21], [94mLoss[0m : 10.71704
[1mStep[0m  [8/21], [94mLoss[0m : 10.51148
[1mStep[0m  [10/21], [94mLoss[0m : 10.72665
[1mStep[0m  [12/21], [94mLoss[0m : 10.82853
[1mStep[0m  [14/21], [94mLoss[0m : 10.52201
[1mStep[0m  [16/21], [94mLoss[0m : 10.74387
[1mStep[0m  [18/21], [94mLoss[0m : 10.41651
[1mStep[0m  [20/21], [94mLoss[0m : 10.58045

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.751, [92mTest[0m: 10.719, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56328
[1mStep[0m  [2/21], [94mLoss[0m : 10.67247
[1mStep[0m  [4/21], [94mLoss[0m : 10.67606
[1mStep[0m  [6/21], [94mLoss[0m : 10.80175
[1mStep[0m  [8/21], [94mLoss[0m : 10.67872
[1mStep[0m  [10/21], [94mLoss[0m : 10.73508
[1mStep[0m  [12/21], [94mLoss[0m : 10.74883
[1mStep[0m  [14/21], [94mLoss[0m : 10.84823
[1mStep[0m  [16/21], [94mLoss[0m : 10.70359
[1mStep[0m  [18/21], [94mLoss[0m : 11.11427
[1mStep[0m  [20/21], [94mLoss[0m : 10.71480

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.750, [92mTest[0m: 10.708, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69525
[1mStep[0m  [2/21], [94mLoss[0m : 10.68474
[1mStep[0m  [4/21], [94mLoss[0m : 10.72188
[1mStep[0m  [6/21], [94mLoss[0m : 11.02122
[1mStep[0m  [8/21], [94mLoss[0m : 10.57261
[1mStep[0m  [10/21], [94mLoss[0m : 10.74373
[1mStep[0m  [12/21], [94mLoss[0m : 10.80364
[1mStep[0m  [14/21], [94mLoss[0m : 10.61881
[1mStep[0m  [16/21], [94mLoss[0m : 11.10223
[1mStep[0m  [18/21], [94mLoss[0m : 10.84302
[1mStep[0m  [20/21], [94mLoss[0m : 10.77047

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.748, [92mTest[0m: 10.712, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68057
[1mStep[0m  [2/21], [94mLoss[0m : 11.25726
[1mStep[0m  [4/21], [94mLoss[0m : 10.72197
[1mStep[0m  [6/21], [94mLoss[0m : 10.83076
[1mStep[0m  [8/21], [94mLoss[0m : 10.79052
[1mStep[0m  [10/21], [94mLoss[0m : 10.71570
[1mStep[0m  [12/21], [94mLoss[0m : 10.44091
[1mStep[0m  [14/21], [94mLoss[0m : 10.90951
[1mStep[0m  [16/21], [94mLoss[0m : 10.66742
[1mStep[0m  [18/21], [94mLoss[0m : 10.59260
[1mStep[0m  [20/21], [94mLoss[0m : 10.91739

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.732, [92mTest[0m: 10.709, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73024
[1mStep[0m  [2/21], [94mLoss[0m : 10.79910
[1mStep[0m  [4/21], [94mLoss[0m : 10.65282
[1mStep[0m  [6/21], [94mLoss[0m : 10.75317
[1mStep[0m  [8/21], [94mLoss[0m : 10.68521
[1mStep[0m  [10/21], [94mLoss[0m : 10.69833
[1mStep[0m  [12/21], [94mLoss[0m : 10.62023
[1mStep[0m  [14/21], [94mLoss[0m : 10.50190
[1mStep[0m  [16/21], [94mLoss[0m : 10.82884
[1mStep[0m  [18/21], [94mLoss[0m : 10.70819
[1mStep[0m  [20/21], [94mLoss[0m : 10.69544

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.728, [92mTest[0m: 10.697, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73699
[1mStep[0m  [2/21], [94mLoss[0m : 10.86311
[1mStep[0m  [4/21], [94mLoss[0m : 10.57451
[1mStep[0m  [6/21], [94mLoss[0m : 10.59746
[1mStep[0m  [8/21], [94mLoss[0m : 10.69580
[1mStep[0m  [10/21], [94mLoss[0m : 10.55293
[1mStep[0m  [12/21], [94mLoss[0m : 10.92845
[1mStep[0m  [14/21], [94mLoss[0m : 10.59408
[1mStep[0m  [16/21], [94mLoss[0m : 10.64338
[1mStep[0m  [18/21], [94mLoss[0m : 10.57262
[1mStep[0m  [20/21], [94mLoss[0m : 10.57897

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.729, [92mTest[0m: 10.678, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81964
[1mStep[0m  [2/21], [94mLoss[0m : 10.72407
[1mStep[0m  [4/21], [94mLoss[0m : 10.71523
[1mStep[0m  [6/21], [94mLoss[0m : 10.71622
[1mStep[0m  [8/21], [94mLoss[0m : 10.76047
[1mStep[0m  [10/21], [94mLoss[0m : 10.98935
[1mStep[0m  [12/21], [94mLoss[0m : 10.81905
[1mStep[0m  [14/21], [94mLoss[0m : 10.92319
[1mStep[0m  [16/21], [94mLoss[0m : 10.49716
[1mStep[0m  [18/21], [94mLoss[0m : 10.89301
[1mStep[0m  [20/21], [94mLoss[0m : 10.74327

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.727, [92mTest[0m: 10.664, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78971
[1mStep[0m  [2/21], [94mLoss[0m : 10.45995
[1mStep[0m  [4/21], [94mLoss[0m : 10.62553
[1mStep[0m  [6/21], [94mLoss[0m : 10.84349
[1mStep[0m  [8/21], [94mLoss[0m : 10.59153
[1mStep[0m  [10/21], [94mLoss[0m : 10.85365
[1mStep[0m  [12/21], [94mLoss[0m : 10.74578
[1mStep[0m  [14/21], [94mLoss[0m : 10.68283
[1mStep[0m  [16/21], [94mLoss[0m : 10.81286
[1mStep[0m  [18/21], [94mLoss[0m : 10.77304
[1mStep[0m  [20/21], [94mLoss[0m : 10.59371

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.711, [92mTest[0m: 10.652, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67524
[1mStep[0m  [2/21], [94mLoss[0m : 10.65502
[1mStep[0m  [4/21], [94mLoss[0m : 10.63638
[1mStep[0m  [6/21], [94mLoss[0m : 10.77513
[1mStep[0m  [8/21], [94mLoss[0m : 10.91203
[1mStep[0m  [10/21], [94mLoss[0m : 10.81227
[1mStep[0m  [12/21], [94mLoss[0m : 10.68355
[1mStep[0m  [14/21], [94mLoss[0m : 10.68142
[1mStep[0m  [16/21], [94mLoss[0m : 10.69327
[1mStep[0m  [18/21], [94mLoss[0m : 10.70537
[1mStep[0m  [20/21], [94mLoss[0m : 10.82628

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.707, [92mTest[0m: 10.660, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71418
[1mStep[0m  [2/21], [94mLoss[0m : 10.55758
[1mStep[0m  [4/21], [94mLoss[0m : 10.75594
[1mStep[0m  [6/21], [94mLoss[0m : 10.39764
[1mStep[0m  [8/21], [94mLoss[0m : 10.76992
[1mStep[0m  [10/21], [94mLoss[0m : 10.87375
[1mStep[0m  [12/21], [94mLoss[0m : 10.71930
[1mStep[0m  [14/21], [94mLoss[0m : 10.82996
[1mStep[0m  [16/21], [94mLoss[0m : 10.68283
[1mStep[0m  [18/21], [94mLoss[0m : 10.52223
[1mStep[0m  [20/21], [94mLoss[0m : 11.06259

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.706, [92mTest[0m: 10.648, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.636
====================================

Phase 1 - Evaluation MAE:  10.635661806379046
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.81327
[1mStep[0m  [2/21], [94mLoss[0m : 10.92544
[1mStep[0m  [4/21], [94mLoss[0m : 10.53013
[1mStep[0m  [6/21], [94mLoss[0m : 10.77717
[1mStep[0m  [8/21], [94mLoss[0m : 10.83082
[1mStep[0m  [10/21], [94mLoss[0m : 10.58713
[1mStep[0m  [12/21], [94mLoss[0m : 10.73034
[1mStep[0m  [14/21], [94mLoss[0m : 10.76851
[1mStep[0m  [16/21], [94mLoss[0m : 10.54251
[1mStep[0m  [18/21], [94mLoss[0m : 10.81055
[1mStep[0m  [20/21], [94mLoss[0m : 10.64855

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.696, [92mTest[0m: 10.642, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.76252
[1mStep[0m  [2/21], [94mLoss[0m : 10.65019
[1mStep[0m  [4/21], [94mLoss[0m : 10.56155
[1mStep[0m  [6/21], [94mLoss[0m : 10.75805
[1mStep[0m  [8/21], [94mLoss[0m : 10.65858
[1mStep[0m  [10/21], [94mLoss[0m : 10.61447
[1mStep[0m  [12/21], [94mLoss[0m : 10.55252
[1mStep[0m  [14/21], [94mLoss[0m : 10.91699
[1mStep[0m  [16/21], [94mLoss[0m : 10.59046
[1mStep[0m  [18/21], [94mLoss[0m : 10.77382
[1mStep[0m  [20/21], [94mLoss[0m : 10.48587

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.684, [92mTest[0m: 10.623, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.94647
[1mStep[0m  [2/21], [94mLoss[0m : 10.52740
[1mStep[0m  [4/21], [94mLoss[0m : 10.77537
[1mStep[0m  [6/21], [94mLoss[0m : 10.83864
[1mStep[0m  [8/21], [94mLoss[0m : 10.80533
[1mStep[0m  [10/21], [94mLoss[0m : 10.65294
[1mStep[0m  [12/21], [94mLoss[0m : 10.78097
[1mStep[0m  [14/21], [94mLoss[0m : 10.55079
[1mStep[0m  [16/21], [94mLoss[0m : 10.75548
[1mStep[0m  [18/21], [94mLoss[0m : 10.76398
[1mStep[0m  [20/21], [94mLoss[0m : 10.74551

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.678, [92mTest[0m: 10.605, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42130
[1mStep[0m  [2/21], [94mLoss[0m : 11.00578
[1mStep[0m  [4/21], [94mLoss[0m : 10.58809
[1mStep[0m  [6/21], [94mLoss[0m : 10.85908
[1mStep[0m  [8/21], [94mLoss[0m : 10.48028
[1mStep[0m  [10/21], [94mLoss[0m : 10.81304
[1mStep[0m  [12/21], [94mLoss[0m : 10.80764
[1mStep[0m  [14/21], [94mLoss[0m : 10.58466
[1mStep[0m  [16/21], [94mLoss[0m : 10.56182
[1mStep[0m  [18/21], [94mLoss[0m : 10.50839
[1mStep[0m  [20/21], [94mLoss[0m : 10.64664

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60887
[1mStep[0m  [2/21], [94mLoss[0m : 10.70379
[1mStep[0m  [4/21], [94mLoss[0m : 10.80404
[1mStep[0m  [6/21], [94mLoss[0m : 10.56344
[1mStep[0m  [8/21], [94mLoss[0m : 10.71642
[1mStep[0m  [10/21], [94mLoss[0m : 10.74727
[1mStep[0m  [12/21], [94mLoss[0m : 10.52998
[1mStep[0m  [14/21], [94mLoss[0m : 10.76773
[1mStep[0m  [16/21], [94mLoss[0m : 10.65696
[1mStep[0m  [18/21], [94mLoss[0m : 10.89029
[1mStep[0m  [20/21], [94mLoss[0m : 10.70422

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.650, [92mTest[0m: 10.593, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70951
[1mStep[0m  [2/21], [94mLoss[0m : 10.64552
[1mStep[0m  [4/21], [94mLoss[0m : 10.51182
[1mStep[0m  [6/21], [94mLoss[0m : 10.50543
[1mStep[0m  [8/21], [94mLoss[0m : 10.65601
[1mStep[0m  [10/21], [94mLoss[0m : 10.53470
[1mStep[0m  [12/21], [94mLoss[0m : 10.75673
[1mStep[0m  [14/21], [94mLoss[0m : 10.68567
[1mStep[0m  [16/21], [94mLoss[0m : 10.44889
[1mStep[0m  [18/21], [94mLoss[0m : 10.51007
[1mStep[0m  [20/21], [94mLoss[0m : 10.80647

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.639, [92mTest[0m: 10.574, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62333
[1mStep[0m  [2/21], [94mLoss[0m : 10.55317
[1mStep[0m  [4/21], [94mLoss[0m : 10.52390
[1mStep[0m  [6/21], [94mLoss[0m : 10.31941
[1mStep[0m  [8/21], [94mLoss[0m : 10.43870
[1mStep[0m  [10/21], [94mLoss[0m : 10.40220
[1mStep[0m  [12/21], [94mLoss[0m : 10.55862
[1mStep[0m  [14/21], [94mLoss[0m : 10.44007
[1mStep[0m  [16/21], [94mLoss[0m : 10.83434
[1mStep[0m  [18/21], [94mLoss[0m : 10.72619
[1mStep[0m  [20/21], [94mLoss[0m : 10.66582

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.629, [92mTest[0m: 10.570, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33881
[1mStep[0m  [2/21], [94mLoss[0m : 10.58292
[1mStep[0m  [4/21], [94mLoss[0m : 10.71591
[1mStep[0m  [6/21], [94mLoss[0m : 10.59572
[1mStep[0m  [8/21], [94mLoss[0m : 10.64261
[1mStep[0m  [10/21], [94mLoss[0m : 10.51186
[1mStep[0m  [12/21], [94mLoss[0m : 10.78474
[1mStep[0m  [14/21], [94mLoss[0m : 10.77605
[1mStep[0m  [16/21], [94mLoss[0m : 10.46244
[1mStep[0m  [18/21], [94mLoss[0m : 10.67815
[1mStep[0m  [20/21], [94mLoss[0m : 10.40675

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.618, [92mTest[0m: 10.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49158
[1mStep[0m  [2/21], [94mLoss[0m : 10.79897
[1mStep[0m  [4/21], [94mLoss[0m : 10.63146
[1mStep[0m  [6/21], [94mLoss[0m : 10.55503
[1mStep[0m  [8/21], [94mLoss[0m : 10.83746
[1mStep[0m  [10/21], [94mLoss[0m : 10.67273
[1mStep[0m  [12/21], [94mLoss[0m : 10.45600
[1mStep[0m  [14/21], [94mLoss[0m : 10.57291
[1mStep[0m  [16/21], [94mLoss[0m : 10.55932
[1mStep[0m  [18/21], [94mLoss[0m : 10.59192
[1mStep[0m  [20/21], [94mLoss[0m : 10.59052

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.537, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60424
[1mStep[0m  [2/21], [94mLoss[0m : 10.86798
[1mStep[0m  [4/21], [94mLoss[0m : 10.59612
[1mStep[0m  [6/21], [94mLoss[0m : 10.64635
[1mStep[0m  [8/21], [94mLoss[0m : 10.75548
[1mStep[0m  [10/21], [94mLoss[0m : 10.59878
[1mStep[0m  [12/21], [94mLoss[0m : 10.65163
[1mStep[0m  [14/21], [94mLoss[0m : 10.35071
[1mStep[0m  [16/21], [94mLoss[0m : 10.56005
[1mStep[0m  [18/21], [94mLoss[0m : 10.73601
[1mStep[0m  [20/21], [94mLoss[0m : 10.62731

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.599, [92mTest[0m: 10.529, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.61363
[1mStep[0m  [2/21], [94mLoss[0m : 10.22424
[1mStep[0m  [4/21], [94mLoss[0m : 10.58983
[1mStep[0m  [6/21], [94mLoss[0m : 10.55442
[1mStep[0m  [8/21], [94mLoss[0m : 10.75613
[1mStep[0m  [10/21], [94mLoss[0m : 11.00962
[1mStep[0m  [12/21], [94mLoss[0m : 10.52314
[1mStep[0m  [14/21], [94mLoss[0m : 10.61815
[1mStep[0m  [16/21], [94mLoss[0m : 10.59412
[1mStep[0m  [18/21], [94mLoss[0m : 10.47034
[1mStep[0m  [20/21], [94mLoss[0m : 10.42639

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.520, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67128
[1mStep[0m  [2/21], [94mLoss[0m : 10.65613
[1mStep[0m  [4/21], [94mLoss[0m : 10.57898
[1mStep[0m  [6/21], [94mLoss[0m : 10.61172
[1mStep[0m  [8/21], [94mLoss[0m : 10.43373
[1mStep[0m  [10/21], [94mLoss[0m : 10.64276
[1mStep[0m  [12/21], [94mLoss[0m : 10.66794
[1mStep[0m  [14/21], [94mLoss[0m : 10.35039
[1mStep[0m  [16/21], [94mLoss[0m : 10.44722
[1mStep[0m  [18/21], [94mLoss[0m : 10.72919
[1mStep[0m  [20/21], [94mLoss[0m : 10.66224

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.583, [92mTest[0m: 10.508, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82462
[1mStep[0m  [2/21], [94mLoss[0m : 10.49450
[1mStep[0m  [4/21], [94mLoss[0m : 10.56461
[1mStep[0m  [6/21], [94mLoss[0m : 10.87383
[1mStep[0m  [8/21], [94mLoss[0m : 10.58204
[1mStep[0m  [10/21], [94mLoss[0m : 10.70415
[1mStep[0m  [12/21], [94mLoss[0m : 10.52411
[1mStep[0m  [14/21], [94mLoss[0m : 10.50239
[1mStep[0m  [16/21], [94mLoss[0m : 10.33730
[1mStep[0m  [18/21], [94mLoss[0m : 10.36741
[1mStep[0m  [20/21], [94mLoss[0m : 10.68241

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.568, [92mTest[0m: 10.487, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56043
[1mStep[0m  [2/21], [94mLoss[0m : 10.65406
[1mStep[0m  [4/21], [94mLoss[0m : 10.40745
[1mStep[0m  [6/21], [94mLoss[0m : 10.65749
[1mStep[0m  [8/21], [94mLoss[0m : 10.69446
[1mStep[0m  [10/21], [94mLoss[0m : 10.66508
[1mStep[0m  [12/21], [94mLoss[0m : 10.48473
[1mStep[0m  [14/21], [94mLoss[0m : 10.49236
[1mStep[0m  [16/21], [94mLoss[0m : 10.43339
[1mStep[0m  [18/21], [94mLoss[0m : 10.69728
[1mStep[0m  [20/21], [94mLoss[0m : 10.57958

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48717
[1mStep[0m  [2/21], [94mLoss[0m : 10.54762
[1mStep[0m  [4/21], [94mLoss[0m : 10.25619
[1mStep[0m  [6/21], [94mLoss[0m : 10.78664
[1mStep[0m  [8/21], [94mLoss[0m : 10.53410
[1mStep[0m  [10/21], [94mLoss[0m : 10.31826
[1mStep[0m  [12/21], [94mLoss[0m : 10.34406
[1mStep[0m  [14/21], [94mLoss[0m : 10.29912
[1mStep[0m  [16/21], [94mLoss[0m : 10.71794
[1mStep[0m  [18/21], [94mLoss[0m : 10.66057
[1mStep[0m  [20/21], [94mLoss[0m : 10.52278

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.548, [92mTest[0m: 10.465, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58834
[1mStep[0m  [2/21], [94mLoss[0m : 10.42244
[1mStep[0m  [4/21], [94mLoss[0m : 10.68129
[1mStep[0m  [6/21], [94mLoss[0m : 10.33691
[1mStep[0m  [8/21], [94mLoss[0m : 10.68185
[1mStep[0m  [10/21], [94mLoss[0m : 10.29972
[1mStep[0m  [12/21], [94mLoss[0m : 10.74754
[1mStep[0m  [14/21], [94mLoss[0m : 10.41739
[1mStep[0m  [16/21], [94mLoss[0m : 10.78078
[1mStep[0m  [18/21], [94mLoss[0m : 10.44031
[1mStep[0m  [20/21], [94mLoss[0m : 10.48404

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.532, [92mTest[0m: 10.470, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.66250
[1mStep[0m  [2/21], [94mLoss[0m : 10.78733
[1mStep[0m  [4/21], [94mLoss[0m : 10.53717
[1mStep[0m  [6/21], [94mLoss[0m : 10.43887
[1mStep[0m  [8/21], [94mLoss[0m : 10.63035
[1mStep[0m  [10/21], [94mLoss[0m : 10.36862
[1mStep[0m  [12/21], [94mLoss[0m : 10.32116
[1mStep[0m  [14/21], [94mLoss[0m : 10.64229
[1mStep[0m  [16/21], [94mLoss[0m : 10.60633
[1mStep[0m  [18/21], [94mLoss[0m : 10.37291
[1mStep[0m  [20/21], [94mLoss[0m : 10.34912

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.520, [92mTest[0m: 10.435, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60711
[1mStep[0m  [2/21], [94mLoss[0m : 10.39066
[1mStep[0m  [4/21], [94mLoss[0m : 10.28428
[1mStep[0m  [6/21], [94mLoss[0m : 10.74907
[1mStep[0m  [8/21], [94mLoss[0m : 10.89783
[1mStep[0m  [10/21], [94mLoss[0m : 10.56085
[1mStep[0m  [12/21], [94mLoss[0m : 10.49645
[1mStep[0m  [14/21], [94mLoss[0m : 10.35151
[1mStep[0m  [16/21], [94mLoss[0m : 10.50899
[1mStep[0m  [18/21], [94mLoss[0m : 10.59032
[1mStep[0m  [20/21], [94mLoss[0m : 10.52913

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.511, [92mTest[0m: 10.410, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40623
[1mStep[0m  [2/21], [94mLoss[0m : 10.49428
[1mStep[0m  [4/21], [94mLoss[0m : 10.30140
[1mStep[0m  [6/21], [94mLoss[0m : 10.47326
[1mStep[0m  [8/21], [94mLoss[0m : 10.23181
[1mStep[0m  [10/21], [94mLoss[0m : 10.48072
[1mStep[0m  [12/21], [94mLoss[0m : 10.68477
[1mStep[0m  [14/21], [94mLoss[0m : 10.47816
[1mStep[0m  [16/21], [94mLoss[0m : 10.57551
[1mStep[0m  [18/21], [94mLoss[0m : 10.34207
[1mStep[0m  [20/21], [94mLoss[0m : 10.68031

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.490, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42410
[1mStep[0m  [2/21], [94mLoss[0m : 10.79102
[1mStep[0m  [4/21], [94mLoss[0m : 10.40124
[1mStep[0m  [6/21], [94mLoss[0m : 10.71422
[1mStep[0m  [8/21], [94mLoss[0m : 10.43494
[1mStep[0m  [10/21], [94mLoss[0m : 10.54127
[1mStep[0m  [12/21], [94mLoss[0m : 10.57376
[1mStep[0m  [14/21], [94mLoss[0m : 10.49597
[1mStep[0m  [16/21], [94mLoss[0m : 10.61936
[1mStep[0m  [18/21], [94mLoss[0m : 10.13388
[1mStep[0m  [20/21], [94mLoss[0m : 10.44269

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.492, [92mTest[0m: 10.391, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49676
[1mStep[0m  [2/21], [94mLoss[0m : 10.71370
[1mStep[0m  [4/21], [94mLoss[0m : 10.28520
[1mStep[0m  [6/21], [94mLoss[0m : 10.51353
[1mStep[0m  [8/21], [94mLoss[0m : 10.71964
[1mStep[0m  [10/21], [94mLoss[0m : 10.25204
[1mStep[0m  [12/21], [94mLoss[0m : 10.56932
[1mStep[0m  [14/21], [94mLoss[0m : 10.43223
[1mStep[0m  [16/21], [94mLoss[0m : 10.36950
[1mStep[0m  [18/21], [94mLoss[0m : 10.59460
[1mStep[0m  [20/21], [94mLoss[0m : 10.52216

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.478, [92mTest[0m: 10.395, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.52301
[1mStep[0m  [2/21], [94mLoss[0m : 10.49149
[1mStep[0m  [4/21], [94mLoss[0m : 10.33638
[1mStep[0m  [6/21], [94mLoss[0m : 10.48010
[1mStep[0m  [8/21], [94mLoss[0m : 10.31124
[1mStep[0m  [10/21], [94mLoss[0m : 10.32468
[1mStep[0m  [12/21], [94mLoss[0m : 10.54401
[1mStep[0m  [14/21], [94mLoss[0m : 10.61207
[1mStep[0m  [16/21], [94mLoss[0m : 10.36007
[1mStep[0m  [18/21], [94mLoss[0m : 10.51596
[1mStep[0m  [20/21], [94mLoss[0m : 10.32715

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.455, [92mTest[0m: 10.382, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47919
[1mStep[0m  [2/21], [94mLoss[0m : 10.45321
[1mStep[0m  [4/21], [94mLoss[0m : 10.44975
[1mStep[0m  [6/21], [94mLoss[0m : 10.57131
[1mStep[0m  [8/21], [94mLoss[0m : 10.37969
[1mStep[0m  [10/21], [94mLoss[0m : 10.42444
[1mStep[0m  [12/21], [94mLoss[0m : 10.20068
[1mStep[0m  [14/21], [94mLoss[0m : 10.54729
[1mStep[0m  [16/21], [94mLoss[0m : 10.31706
[1mStep[0m  [18/21], [94mLoss[0m : 10.57122
[1mStep[0m  [20/21], [94mLoss[0m : 10.47638

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.448, [92mTest[0m: 10.360, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38340
[1mStep[0m  [2/21], [94mLoss[0m : 10.70960
[1mStep[0m  [4/21], [94mLoss[0m : 10.42583
[1mStep[0m  [6/21], [94mLoss[0m : 10.53974
[1mStep[0m  [8/21], [94mLoss[0m : 10.79837
[1mStep[0m  [10/21], [94mLoss[0m : 10.55982
[1mStep[0m  [12/21], [94mLoss[0m : 10.43023
[1mStep[0m  [14/21], [94mLoss[0m : 10.08519
[1mStep[0m  [16/21], [94mLoss[0m : 10.24999
[1mStep[0m  [18/21], [94mLoss[0m : 10.52725
[1mStep[0m  [20/21], [94mLoss[0m : 10.27099

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.431, [92mTest[0m: 10.350, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55233
[1mStep[0m  [2/21], [94mLoss[0m : 10.50313
[1mStep[0m  [4/21], [94mLoss[0m : 10.31658
[1mStep[0m  [6/21], [94mLoss[0m : 10.52268
[1mStep[0m  [8/21], [94mLoss[0m : 10.72844
[1mStep[0m  [10/21], [94mLoss[0m : 10.45467
[1mStep[0m  [12/21], [94mLoss[0m : 10.45401
[1mStep[0m  [14/21], [94mLoss[0m : 10.56960
[1mStep[0m  [16/21], [94mLoss[0m : 10.30170
[1mStep[0m  [18/21], [94mLoss[0m : 10.25502
[1mStep[0m  [20/21], [94mLoss[0m : 10.36080

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.416, [92mTest[0m: 10.336, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.11908
[1mStep[0m  [2/21], [94mLoss[0m : 10.27821
[1mStep[0m  [4/21], [94mLoss[0m : 10.50554
[1mStep[0m  [6/21], [94mLoss[0m : 10.11324
[1mStep[0m  [8/21], [94mLoss[0m : 10.12346
[1mStep[0m  [10/21], [94mLoss[0m : 10.43369
[1mStep[0m  [12/21], [94mLoss[0m : 10.58202
[1mStep[0m  [14/21], [94mLoss[0m : 10.34428
[1mStep[0m  [16/21], [94mLoss[0m : 10.33258
[1mStep[0m  [18/21], [94mLoss[0m : 10.22476
[1mStep[0m  [20/21], [94mLoss[0m : 10.43274

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.401, [92mTest[0m: 10.318, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.28763
[1mStep[0m  [2/21], [94mLoss[0m : 10.55130
[1mStep[0m  [4/21], [94mLoss[0m : 10.45852
[1mStep[0m  [6/21], [94mLoss[0m : 10.79308
[1mStep[0m  [8/21], [94mLoss[0m : 10.38834
[1mStep[0m  [10/21], [94mLoss[0m : 10.56039
[1mStep[0m  [12/21], [94mLoss[0m : 10.11708
[1mStep[0m  [14/21], [94mLoss[0m : 10.19564
[1mStep[0m  [16/21], [94mLoss[0m : 10.26570
[1mStep[0m  [18/21], [94mLoss[0m : 10.38989
[1mStep[0m  [20/21], [94mLoss[0m : 10.38411

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.401, [92mTest[0m: 10.302, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36852
[1mStep[0m  [2/21], [94mLoss[0m : 10.40856
[1mStep[0m  [4/21], [94mLoss[0m : 10.41196
[1mStep[0m  [6/21], [94mLoss[0m : 10.42454
[1mStep[0m  [8/21], [94mLoss[0m : 10.53246
[1mStep[0m  [10/21], [94mLoss[0m : 10.19515
[1mStep[0m  [12/21], [94mLoss[0m : 10.48558
[1mStep[0m  [14/21], [94mLoss[0m : 10.12129
[1mStep[0m  [16/21], [94mLoss[0m : 10.39996
[1mStep[0m  [18/21], [94mLoss[0m : 10.47258
[1mStep[0m  [20/21], [94mLoss[0m : 10.46929

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.392, [92mTest[0m: 10.276, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.24465
[1mStep[0m  [2/21], [94mLoss[0m : 10.06150
[1mStep[0m  [4/21], [94mLoss[0m : 10.56404
[1mStep[0m  [6/21], [94mLoss[0m : 10.47450
[1mStep[0m  [8/21], [94mLoss[0m : 10.28418
[1mStep[0m  [10/21], [94mLoss[0m : 10.30694
[1mStep[0m  [12/21], [94mLoss[0m : 10.44606
[1mStep[0m  [14/21], [94mLoss[0m : 10.47886
[1mStep[0m  [16/21], [94mLoss[0m : 10.22465
[1mStep[0m  [18/21], [94mLoss[0m : 10.27535
[1mStep[0m  [20/21], [94mLoss[0m : 10.23349

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.374, [92mTest[0m: 10.291, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33832
[1mStep[0m  [2/21], [94mLoss[0m : 10.16748
[1mStep[0m  [4/21], [94mLoss[0m : 10.49269
[1mStep[0m  [6/21], [94mLoss[0m : 10.54034
[1mStep[0m  [8/21], [94mLoss[0m : 10.23411
[1mStep[0m  [10/21], [94mLoss[0m : 10.25037
[1mStep[0m  [12/21], [94mLoss[0m : 10.46445
[1mStep[0m  [14/21], [94mLoss[0m : 10.39371
[1mStep[0m  [16/21], [94mLoss[0m : 10.05334
[1mStep[0m  [18/21], [94mLoss[0m : 10.38499
[1mStep[0m  [20/21], [94mLoss[0m : 10.10720

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.365, [92mTest[0m: 10.262, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.241
====================================

Phase 2 - Evaluation MAE:  10.241426740373884
MAE score P1      10.635662
MAE score P2      10.241427
loss              10.365279
learning_rate        0.0001
batch_size              512
hidden_sizes          [250]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.35011
[1mStep[0m  [2/21], [94mLoss[0m : 11.06533
[1mStep[0m  [4/21], [94mLoss[0m : 11.08571
[1mStep[0m  [6/21], [94mLoss[0m : 10.95429
[1mStep[0m  [8/21], [94mLoss[0m : 11.08209
[1mStep[0m  [10/21], [94mLoss[0m : 10.82065
[1mStep[0m  [12/21], [94mLoss[0m : 10.88894
[1mStep[0m  [14/21], [94mLoss[0m : 10.80375
[1mStep[0m  [16/21], [94mLoss[0m : 11.32512
[1mStep[0m  [18/21], [94mLoss[0m : 11.06271
[1mStep[0m  [20/21], [94mLoss[0m : 10.84231

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.039, [92mTest[0m: 10.927, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.23005
[1mStep[0m  [2/21], [94mLoss[0m : 10.86442
[1mStep[0m  [4/21], [94mLoss[0m : 10.84718
[1mStep[0m  [6/21], [94mLoss[0m : 10.95609
[1mStep[0m  [8/21], [94mLoss[0m : 10.81714
[1mStep[0m  [10/21], [94mLoss[0m : 11.20213
[1mStep[0m  [12/21], [94mLoss[0m : 10.95642
[1mStep[0m  [14/21], [94mLoss[0m : 10.80373
[1mStep[0m  [16/21], [94mLoss[0m : 10.72666
[1mStep[0m  [18/21], [94mLoss[0m : 10.69709
[1mStep[0m  [20/21], [94mLoss[0m : 10.53925

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.879, [92mTest[0m: 10.909, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81028
[1mStep[0m  [2/21], [94mLoss[0m : 10.63657
[1mStep[0m  [4/21], [94mLoss[0m : 10.64267
[1mStep[0m  [6/21], [94mLoss[0m : 10.60233
[1mStep[0m  [8/21], [94mLoss[0m : 10.79496
[1mStep[0m  [10/21], [94mLoss[0m : 10.29122
[1mStep[0m  [12/21], [94mLoss[0m : 10.65653
[1mStep[0m  [14/21], [94mLoss[0m : 10.51143
[1mStep[0m  [16/21], [94mLoss[0m : 10.77199
[1mStep[0m  [18/21], [94mLoss[0m : 10.79532
[1mStep[0m  [20/21], [94mLoss[0m : 10.80954

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82140
[1mStep[0m  [2/21], [94mLoss[0m : 10.69899
[1mStep[0m  [4/21], [94mLoss[0m : 10.73545
[1mStep[0m  [6/21], [94mLoss[0m : 10.63125
[1mStep[0m  [8/21], [94mLoss[0m : 10.66965
[1mStep[0m  [10/21], [94mLoss[0m : 10.56234
[1mStep[0m  [12/21], [94mLoss[0m : 10.51609
[1mStep[0m  [14/21], [94mLoss[0m : 10.19706
[1mStep[0m  [16/21], [94mLoss[0m : 10.54766
[1mStep[0m  [18/21], [94mLoss[0m : 10.69938
[1mStep[0m  [20/21], [94mLoss[0m : 10.59796

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.558, [92mTest[0m: 10.657, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.40932
[1mStep[0m  [2/21], [94mLoss[0m : 10.26774
[1mStep[0m  [4/21], [94mLoss[0m : 10.26849
[1mStep[0m  [6/21], [94mLoss[0m : 10.32987
[1mStep[0m  [8/21], [94mLoss[0m : 10.71303
[1mStep[0m  [10/21], [94mLoss[0m : 10.39101
[1mStep[0m  [12/21], [94mLoss[0m : 10.26301
[1mStep[0m  [14/21], [94mLoss[0m : 10.62989
[1mStep[0m  [16/21], [94mLoss[0m : 10.55254
[1mStep[0m  [18/21], [94mLoss[0m : 10.31016
[1mStep[0m  [20/21], [94mLoss[0m : 10.36343

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.395, [92mTest[0m: 10.522, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.37524
[1mStep[0m  [2/21], [94mLoss[0m : 10.11882
[1mStep[0m  [4/21], [94mLoss[0m : 10.48044
[1mStep[0m  [6/21], [94mLoss[0m : 10.29634
[1mStep[0m  [8/21], [94mLoss[0m : 10.28672
[1mStep[0m  [10/21], [94mLoss[0m : 10.30624
[1mStep[0m  [12/21], [94mLoss[0m : 10.23343
[1mStep[0m  [14/21], [94mLoss[0m : 10.15541
[1mStep[0m  [16/21], [94mLoss[0m : 10.23521
[1mStep[0m  [18/21], [94mLoss[0m : 10.20422
[1mStep[0m  [20/21], [94mLoss[0m : 10.03632

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.240, [92mTest[0m: 10.386, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.04565
[1mStep[0m  [2/21], [94mLoss[0m : 10.30433
[1mStep[0m  [4/21], [94mLoss[0m : 10.26234
[1mStep[0m  [6/21], [94mLoss[0m : 10.00717
[1mStep[0m  [8/21], [94mLoss[0m : 9.85941
[1mStep[0m  [10/21], [94mLoss[0m : 9.50138
[1mStep[0m  [12/21], [94mLoss[0m : 10.13944
[1mStep[0m  [14/21], [94mLoss[0m : 10.00039
[1mStep[0m  [16/21], [94mLoss[0m : 10.04803
[1mStep[0m  [18/21], [94mLoss[0m : 9.87129
[1mStep[0m  [20/21], [94mLoss[0m : 10.14697

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.070, [92mTest[0m: 10.265, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.95545
[1mStep[0m  [2/21], [94mLoss[0m : 9.89668
[1mStep[0m  [4/21], [94mLoss[0m : 9.91631
[1mStep[0m  [6/21], [94mLoss[0m : 9.72891
[1mStep[0m  [8/21], [94mLoss[0m : 9.80674
[1mStep[0m  [10/21], [94mLoss[0m : 10.13722
[1mStep[0m  [12/21], [94mLoss[0m : 9.63791
[1mStep[0m  [14/21], [94mLoss[0m : 9.98154
[1mStep[0m  [16/21], [94mLoss[0m : 9.87962
[1mStep[0m  [18/21], [94mLoss[0m : 9.85757
[1mStep[0m  [20/21], [94mLoss[0m : 9.91854

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.919, [92mTest[0m: 10.128, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.82851
[1mStep[0m  [2/21], [94mLoss[0m : 9.69128
[1mStep[0m  [4/21], [94mLoss[0m : 10.14668
[1mStep[0m  [6/21], [94mLoss[0m : 10.09851
[1mStep[0m  [8/21], [94mLoss[0m : 9.51470
[1mStep[0m  [10/21], [94mLoss[0m : 9.75994
[1mStep[0m  [12/21], [94mLoss[0m : 9.92815
[1mStep[0m  [14/21], [94mLoss[0m : 9.61830
[1mStep[0m  [16/21], [94mLoss[0m : 9.45533
[1mStep[0m  [18/21], [94mLoss[0m : 9.82778
[1mStep[0m  [20/21], [94mLoss[0m : 9.87587

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.760, [92mTest[0m: 9.992, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.73132
[1mStep[0m  [2/21], [94mLoss[0m : 9.71390
[1mStep[0m  [4/21], [94mLoss[0m : 9.40299
[1mStep[0m  [6/21], [94mLoss[0m : 9.86124
[1mStep[0m  [8/21], [94mLoss[0m : 9.90615
[1mStep[0m  [10/21], [94mLoss[0m : 9.72746
[1mStep[0m  [12/21], [94mLoss[0m : 9.55199
[1mStep[0m  [14/21], [94mLoss[0m : 9.13125
[1mStep[0m  [16/21], [94mLoss[0m : 9.40835
[1mStep[0m  [18/21], [94mLoss[0m : 9.61272
[1mStep[0m  [20/21], [94mLoss[0m : 9.58827

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 9.604, [92mTest[0m: 9.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.59309
[1mStep[0m  [2/21], [94mLoss[0m : 9.56520
[1mStep[0m  [4/21], [94mLoss[0m : 9.70928
[1mStep[0m  [6/21], [94mLoss[0m : 9.44267
[1mStep[0m  [8/21], [94mLoss[0m : 9.52147
[1mStep[0m  [10/21], [94mLoss[0m : 9.66940
[1mStep[0m  [12/21], [94mLoss[0m : 9.43831
[1mStep[0m  [14/21], [94mLoss[0m : 9.59914
[1mStep[0m  [16/21], [94mLoss[0m : 9.33569
[1mStep[0m  [18/21], [94mLoss[0m : 9.56584
[1mStep[0m  [20/21], [94mLoss[0m : 9.32834

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.438, [92mTest[0m: 9.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.49846
[1mStep[0m  [2/21], [94mLoss[0m : 9.30077
[1mStep[0m  [4/21], [94mLoss[0m : 9.47335
[1mStep[0m  [6/21], [94mLoss[0m : 9.35958
[1mStep[0m  [8/21], [94mLoss[0m : 9.39192
[1mStep[0m  [10/21], [94mLoss[0m : 9.38153
[1mStep[0m  [12/21], [94mLoss[0m : 9.39768
[1mStep[0m  [14/21], [94mLoss[0m : 9.33550
[1mStep[0m  [16/21], [94mLoss[0m : 9.51840
[1mStep[0m  [18/21], [94mLoss[0m : 9.01818
[1mStep[0m  [20/21], [94mLoss[0m : 9.42972

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.281, [92mTest[0m: 9.599, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.44462
[1mStep[0m  [2/21], [94mLoss[0m : 9.28552
[1mStep[0m  [4/21], [94mLoss[0m : 9.25668
[1mStep[0m  [6/21], [94mLoss[0m : 8.88689
[1mStep[0m  [8/21], [94mLoss[0m : 9.38512
[1mStep[0m  [10/21], [94mLoss[0m : 9.22329
[1mStep[0m  [12/21], [94mLoss[0m : 8.90545
[1mStep[0m  [14/21], [94mLoss[0m : 8.84372
[1mStep[0m  [16/21], [94mLoss[0m : 9.14101
[1mStep[0m  [18/21], [94mLoss[0m : 9.04293
[1mStep[0m  [20/21], [94mLoss[0m : 8.99095

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.115, [92mTest[0m: 9.472, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.66934
[1mStep[0m  [2/21], [94mLoss[0m : 8.78593
[1mStep[0m  [4/21], [94mLoss[0m : 8.79217
[1mStep[0m  [6/21], [94mLoss[0m : 8.72691
[1mStep[0m  [8/21], [94mLoss[0m : 9.00784
[1mStep[0m  [10/21], [94mLoss[0m : 8.98776
[1mStep[0m  [12/21], [94mLoss[0m : 9.11893
[1mStep[0m  [14/21], [94mLoss[0m : 9.07639
[1mStep[0m  [16/21], [94mLoss[0m : 9.10497
[1mStep[0m  [18/21], [94mLoss[0m : 9.16127
[1mStep[0m  [20/21], [94mLoss[0m : 9.15938

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.951, [92mTest[0m: 9.342, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.09069
[1mStep[0m  [2/21], [94mLoss[0m : 8.76362
[1mStep[0m  [4/21], [94mLoss[0m : 8.61064
[1mStep[0m  [6/21], [94mLoss[0m : 8.70687
[1mStep[0m  [8/21], [94mLoss[0m : 8.90473
[1mStep[0m  [10/21], [94mLoss[0m : 8.72583
[1mStep[0m  [12/21], [94mLoss[0m : 8.79166
[1mStep[0m  [14/21], [94mLoss[0m : 9.01401
[1mStep[0m  [16/21], [94mLoss[0m : 8.80374
[1mStep[0m  [18/21], [94mLoss[0m : 8.77285
[1mStep[0m  [20/21], [94mLoss[0m : 8.35907

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.797, [92mTest[0m: 9.209, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.64421
[1mStep[0m  [2/21], [94mLoss[0m : 8.68667
[1mStep[0m  [4/21], [94mLoss[0m : 8.62547
[1mStep[0m  [6/21], [94mLoss[0m : 8.77679
[1mStep[0m  [8/21], [94mLoss[0m : 8.60473
[1mStep[0m  [10/21], [94mLoss[0m : 8.58245
[1mStep[0m  [12/21], [94mLoss[0m : 8.54250
[1mStep[0m  [14/21], [94mLoss[0m : 8.73900
[1mStep[0m  [16/21], [94mLoss[0m : 8.53678
[1mStep[0m  [18/21], [94mLoss[0m : 8.38912
[1mStep[0m  [20/21], [94mLoss[0m : 8.49962

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.636, [92mTest[0m: 9.061, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.65360
[1mStep[0m  [2/21], [94mLoss[0m : 8.66719
[1mStep[0m  [4/21], [94mLoss[0m : 8.60554
[1mStep[0m  [6/21], [94mLoss[0m : 8.71031
[1mStep[0m  [8/21], [94mLoss[0m : 8.61956
[1mStep[0m  [10/21], [94mLoss[0m : 8.28281
[1mStep[0m  [12/21], [94mLoss[0m : 8.39454
[1mStep[0m  [14/21], [94mLoss[0m : 8.54566
[1mStep[0m  [16/21], [94mLoss[0m : 8.33964
[1mStep[0m  [18/21], [94mLoss[0m : 8.45216
[1mStep[0m  [20/21], [94mLoss[0m : 8.23951

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.473, [92mTest[0m: 8.947, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.31462
[1mStep[0m  [2/21], [94mLoss[0m : 8.57560
[1mStep[0m  [4/21], [94mLoss[0m : 8.49016
[1mStep[0m  [6/21], [94mLoss[0m : 8.42548
[1mStep[0m  [8/21], [94mLoss[0m : 8.20702
[1mStep[0m  [10/21], [94mLoss[0m : 8.33133
[1mStep[0m  [12/21], [94mLoss[0m : 8.14235
[1mStep[0m  [14/21], [94mLoss[0m : 8.29200
[1mStep[0m  [16/21], [94mLoss[0m : 8.43781
[1mStep[0m  [18/21], [94mLoss[0m : 8.03157
[1mStep[0m  [20/21], [94mLoss[0m : 8.27614

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.326, [92mTest[0m: 8.816, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.14427
[1mStep[0m  [2/21], [94mLoss[0m : 8.16724
[1mStep[0m  [4/21], [94mLoss[0m : 7.99684
[1mStep[0m  [6/21], [94mLoss[0m : 8.19725
[1mStep[0m  [8/21], [94mLoss[0m : 8.21705
[1mStep[0m  [10/21], [94mLoss[0m : 8.30700
[1mStep[0m  [12/21], [94mLoss[0m : 7.91862
[1mStep[0m  [14/21], [94mLoss[0m : 8.10826
[1mStep[0m  [16/21], [94mLoss[0m : 7.84368
[1mStep[0m  [18/21], [94mLoss[0m : 8.04710
[1mStep[0m  [20/21], [94mLoss[0m : 8.18201

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.163, [92mTest[0m: 8.696, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.18994
[1mStep[0m  [2/21], [94mLoss[0m : 8.17402
[1mStep[0m  [4/21], [94mLoss[0m : 8.05915
[1mStep[0m  [6/21], [94mLoss[0m : 7.90108
[1mStep[0m  [8/21], [94mLoss[0m : 8.17779
[1mStep[0m  [10/21], [94mLoss[0m : 7.97307
[1mStep[0m  [12/21], [94mLoss[0m : 7.90327
[1mStep[0m  [14/21], [94mLoss[0m : 8.08846
[1mStep[0m  [16/21], [94mLoss[0m : 7.74176
[1mStep[0m  [18/21], [94mLoss[0m : 7.77763
[1mStep[0m  [20/21], [94mLoss[0m : 8.03951

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.999, [92mTest[0m: 8.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.98240
[1mStep[0m  [2/21], [94mLoss[0m : 7.68421
[1mStep[0m  [4/21], [94mLoss[0m : 7.95861
[1mStep[0m  [6/21], [94mLoss[0m : 8.05140
[1mStep[0m  [8/21], [94mLoss[0m : 8.10603
[1mStep[0m  [10/21], [94mLoss[0m : 7.60042
[1mStep[0m  [12/21], [94mLoss[0m : 7.81080
[1mStep[0m  [14/21], [94mLoss[0m : 7.93461
[1mStep[0m  [16/21], [94mLoss[0m : 8.12367
[1mStep[0m  [18/21], [94mLoss[0m : 7.76897
[1mStep[0m  [20/21], [94mLoss[0m : 7.79494

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.864, [92mTest[0m: 8.431, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.92085
[1mStep[0m  [2/21], [94mLoss[0m : 7.61543
[1mStep[0m  [4/21], [94mLoss[0m : 7.67539
[1mStep[0m  [6/21], [94mLoss[0m : 7.68169
[1mStep[0m  [8/21], [94mLoss[0m : 7.63805
[1mStep[0m  [10/21], [94mLoss[0m : 7.78175
[1mStep[0m  [12/21], [94mLoss[0m : 7.31292
[1mStep[0m  [14/21], [94mLoss[0m : 7.75777
[1mStep[0m  [16/21], [94mLoss[0m : 8.08738
[1mStep[0m  [18/21], [94mLoss[0m : 7.41504
[1mStep[0m  [20/21], [94mLoss[0m : 7.75632

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.721, [92mTest[0m: 8.301, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.65002
[1mStep[0m  [2/21], [94mLoss[0m : 7.65955
[1mStep[0m  [4/21], [94mLoss[0m : 7.45752
[1mStep[0m  [6/21], [94mLoss[0m : 7.79110
[1mStep[0m  [8/21], [94mLoss[0m : 7.82777
[1mStep[0m  [10/21], [94mLoss[0m : 7.49805
[1mStep[0m  [12/21], [94mLoss[0m : 7.70680
[1mStep[0m  [14/21], [94mLoss[0m : 7.41533
[1mStep[0m  [16/21], [94mLoss[0m : 7.69369
[1mStep[0m  [18/21], [94mLoss[0m : 7.62765
[1mStep[0m  [20/21], [94mLoss[0m : 7.47105

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.581, [92mTest[0m: 8.184, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.74594
[1mStep[0m  [2/21], [94mLoss[0m : 7.29664
[1mStep[0m  [4/21], [94mLoss[0m : 7.59335
[1mStep[0m  [6/21], [94mLoss[0m : 7.15506
[1mStep[0m  [8/21], [94mLoss[0m : 7.36390
[1mStep[0m  [10/21], [94mLoss[0m : 7.89466
[1mStep[0m  [12/21], [94mLoss[0m : 7.22122
[1mStep[0m  [14/21], [94mLoss[0m : 7.34773
[1mStep[0m  [16/21], [94mLoss[0m : 7.11106
[1mStep[0m  [18/21], [94mLoss[0m : 7.61706
[1mStep[0m  [20/21], [94mLoss[0m : 7.37936

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.449, [92mTest[0m: 8.074, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.44809
[1mStep[0m  [2/21], [94mLoss[0m : 7.67059
[1mStep[0m  [4/21], [94mLoss[0m : 7.42665
[1mStep[0m  [6/21], [94mLoss[0m : 7.33618
[1mStep[0m  [8/21], [94mLoss[0m : 7.20414
[1mStep[0m  [10/21], [94mLoss[0m : 7.22040
[1mStep[0m  [12/21], [94mLoss[0m : 7.21791
[1mStep[0m  [14/21], [94mLoss[0m : 7.15254
[1mStep[0m  [16/21], [94mLoss[0m : 7.21846
[1mStep[0m  [18/21], [94mLoss[0m : 7.38821
[1mStep[0m  [20/21], [94mLoss[0m : 7.33834

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.314, [92mTest[0m: 7.975, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.00518
[1mStep[0m  [2/21], [94mLoss[0m : 7.32385
[1mStep[0m  [4/21], [94mLoss[0m : 6.96284
[1mStep[0m  [6/21], [94mLoss[0m : 7.22079
[1mStep[0m  [8/21], [94mLoss[0m : 7.01508
[1mStep[0m  [10/21], [94mLoss[0m : 7.06357
[1mStep[0m  [12/21], [94mLoss[0m : 7.43301
[1mStep[0m  [14/21], [94mLoss[0m : 7.27368
[1mStep[0m  [16/21], [94mLoss[0m : 7.15967
[1mStep[0m  [18/21], [94mLoss[0m : 7.05287
[1mStep[0m  [20/21], [94mLoss[0m : 7.33897

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.177, [92mTest[0m: 7.860, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.22972
[1mStep[0m  [2/21], [94mLoss[0m : 6.82760
[1mStep[0m  [4/21], [94mLoss[0m : 7.24709
[1mStep[0m  [6/21], [94mLoss[0m : 7.23001
[1mStep[0m  [8/21], [94mLoss[0m : 7.02401
[1mStep[0m  [10/21], [94mLoss[0m : 7.08261
[1mStep[0m  [12/21], [94mLoss[0m : 7.01304
[1mStep[0m  [14/21], [94mLoss[0m : 7.01537
[1mStep[0m  [16/21], [94mLoss[0m : 7.12141
[1mStep[0m  [18/21], [94mLoss[0m : 6.98714
[1mStep[0m  [20/21], [94mLoss[0m : 6.81612

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.051, [92mTest[0m: 7.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.01248
[1mStep[0m  [2/21], [94mLoss[0m : 7.03000
[1mStep[0m  [4/21], [94mLoss[0m : 6.92345
[1mStep[0m  [6/21], [94mLoss[0m : 7.08068
[1mStep[0m  [8/21], [94mLoss[0m : 6.81825
[1mStep[0m  [10/21], [94mLoss[0m : 6.84282
[1mStep[0m  [12/21], [94mLoss[0m : 6.83327
[1mStep[0m  [14/21], [94mLoss[0m : 6.92623
[1mStep[0m  [16/21], [94mLoss[0m : 6.78542
[1mStep[0m  [18/21], [94mLoss[0m : 6.86016
[1mStep[0m  [20/21], [94mLoss[0m : 6.61322

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.924, [92mTest[0m: 7.628, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.73657
[1mStep[0m  [2/21], [94mLoss[0m : 6.74094
[1mStep[0m  [4/21], [94mLoss[0m : 6.81795
[1mStep[0m  [6/21], [94mLoss[0m : 6.93427
[1mStep[0m  [8/21], [94mLoss[0m : 6.97376
[1mStep[0m  [10/21], [94mLoss[0m : 6.88882
[1mStep[0m  [12/21], [94mLoss[0m : 6.71821
[1mStep[0m  [14/21], [94mLoss[0m : 6.69383
[1mStep[0m  [16/21], [94mLoss[0m : 6.92161
[1mStep[0m  [18/21], [94mLoss[0m : 6.60518
[1mStep[0m  [20/21], [94mLoss[0m : 6.68779

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.805, [92mTest[0m: 7.513, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.93912
[1mStep[0m  [2/21], [94mLoss[0m : 6.79814
[1mStep[0m  [4/21], [94mLoss[0m : 6.74633
[1mStep[0m  [6/21], [94mLoss[0m : 6.81439
[1mStep[0m  [8/21], [94mLoss[0m : 6.69036
[1mStep[0m  [10/21], [94mLoss[0m : 6.73775
[1mStep[0m  [12/21], [94mLoss[0m : 6.50026
[1mStep[0m  [14/21], [94mLoss[0m : 6.59718
[1mStep[0m  [16/21], [94mLoss[0m : 6.40130
[1mStep[0m  [18/21], [94mLoss[0m : 6.50847
[1mStep[0m  [20/21], [94mLoss[0m : 6.48563

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.671, [92mTest[0m: 7.409, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.291
====================================

Phase 1 - Evaluation MAE:  7.290859426770892
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 6.55097
[1mStep[0m  [2/21], [94mLoss[0m : 6.56230
[1mStep[0m  [4/21], [94mLoss[0m : 6.65463
[1mStep[0m  [6/21], [94mLoss[0m : 6.33396
[1mStep[0m  [8/21], [94mLoss[0m : 6.52990
[1mStep[0m  [10/21], [94mLoss[0m : 6.43631
[1mStep[0m  [12/21], [94mLoss[0m : 6.59662
[1mStep[0m  [14/21], [94mLoss[0m : 6.15825
[1mStep[0m  [16/21], [94mLoss[0m : 6.58764
[1mStep[0m  [18/21], [94mLoss[0m : 6.52609
[1mStep[0m  [20/21], [94mLoss[0m : 6.43582

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.547, [92mTest[0m: 7.292, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.18482
[1mStep[0m  [2/21], [94mLoss[0m : 6.22248
[1mStep[0m  [4/21], [94mLoss[0m : 6.65317
[1mStep[0m  [6/21], [94mLoss[0m : 6.59494
[1mStep[0m  [8/21], [94mLoss[0m : 6.24108
[1mStep[0m  [10/21], [94mLoss[0m : 6.31941
[1mStep[0m  [12/21], [94mLoss[0m : 6.29027
[1mStep[0m  [14/21], [94mLoss[0m : 6.52935
[1mStep[0m  [16/21], [94mLoss[0m : 6.08352
[1mStep[0m  [18/21], [94mLoss[0m : 6.32910
[1mStep[0m  [20/21], [94mLoss[0m : 6.23382

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.408, [92mTest[0m: 7.192, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.23687
[1mStep[0m  [2/21], [94mLoss[0m : 6.45706
[1mStep[0m  [4/21], [94mLoss[0m : 6.23275
[1mStep[0m  [6/21], [94mLoss[0m : 6.24076
[1mStep[0m  [8/21], [94mLoss[0m : 6.38982
[1mStep[0m  [10/21], [94mLoss[0m : 6.28757
[1mStep[0m  [12/21], [94mLoss[0m : 6.19948
[1mStep[0m  [14/21], [94mLoss[0m : 6.17863
[1mStep[0m  [16/21], [94mLoss[0m : 6.29699
[1mStep[0m  [18/21], [94mLoss[0m : 6.22171
[1mStep[0m  [20/21], [94mLoss[0m : 6.19226

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.257, [92mTest[0m: 7.060, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.05321
[1mStep[0m  [2/21], [94mLoss[0m : 6.14561
[1mStep[0m  [4/21], [94mLoss[0m : 5.95144
[1mStep[0m  [6/21], [94mLoss[0m : 6.26268
[1mStep[0m  [8/21], [94mLoss[0m : 6.00147
[1mStep[0m  [10/21], [94mLoss[0m : 6.02058
[1mStep[0m  [12/21], [94mLoss[0m : 6.14965
[1mStep[0m  [14/21], [94mLoss[0m : 6.22392
[1mStep[0m  [16/21], [94mLoss[0m : 6.17818
[1mStep[0m  [18/21], [94mLoss[0m : 6.10843
[1mStep[0m  [20/21], [94mLoss[0m : 6.34132

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 6.118, [92mTest[0m: 6.908, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.73215
[1mStep[0m  [2/21], [94mLoss[0m : 5.94942
[1mStep[0m  [4/21], [94mLoss[0m : 6.51277
[1mStep[0m  [6/21], [94mLoss[0m : 6.20318
[1mStep[0m  [8/21], [94mLoss[0m : 5.97393
[1mStep[0m  [10/21], [94mLoss[0m : 6.08033
[1mStep[0m  [12/21], [94mLoss[0m : 6.20414
[1mStep[0m  [14/21], [94mLoss[0m : 6.16644
[1mStep[0m  [16/21], [94mLoss[0m : 5.93111
[1mStep[0m  [18/21], [94mLoss[0m : 5.97749
[1mStep[0m  [20/21], [94mLoss[0m : 5.84506

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.982, [92mTest[0m: 6.778, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.87276
[1mStep[0m  [2/21], [94mLoss[0m : 6.14286
[1mStep[0m  [4/21], [94mLoss[0m : 5.98541
[1mStep[0m  [6/21], [94mLoss[0m : 6.04989
[1mStep[0m  [8/21], [94mLoss[0m : 5.66187
[1mStep[0m  [10/21], [94mLoss[0m : 5.85278
[1mStep[0m  [12/21], [94mLoss[0m : 5.62414
[1mStep[0m  [14/21], [94mLoss[0m : 5.67272
[1mStep[0m  [16/21], [94mLoss[0m : 5.73797
[1mStep[0m  [18/21], [94mLoss[0m : 5.94171
[1mStep[0m  [20/21], [94mLoss[0m : 5.81679

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.847, [92mTest[0m: 6.653, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.87183
[1mStep[0m  [2/21], [94mLoss[0m : 5.85736
[1mStep[0m  [4/21], [94mLoss[0m : 5.51019
[1mStep[0m  [6/21], [94mLoss[0m : 5.82241
[1mStep[0m  [8/21], [94mLoss[0m : 5.46736
[1mStep[0m  [10/21], [94mLoss[0m : 5.76532
[1mStep[0m  [12/21], [94mLoss[0m : 5.88296
[1mStep[0m  [14/21], [94mLoss[0m : 5.61717
[1mStep[0m  [16/21], [94mLoss[0m : 5.73420
[1mStep[0m  [18/21], [94mLoss[0m : 5.47708
[1mStep[0m  [20/21], [94mLoss[0m : 5.74830

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.717, [92mTest[0m: 6.510, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.82233
[1mStep[0m  [2/21], [94mLoss[0m : 5.78780
[1mStep[0m  [4/21], [94mLoss[0m : 5.51884
[1mStep[0m  [6/21], [94mLoss[0m : 5.68627
[1mStep[0m  [8/21], [94mLoss[0m : 5.47198
[1mStep[0m  [10/21], [94mLoss[0m : 5.35687
[1mStep[0m  [12/21], [94mLoss[0m : 5.46876
[1mStep[0m  [14/21], [94mLoss[0m : 5.80846
[1mStep[0m  [16/21], [94mLoss[0m : 5.65892
[1mStep[0m  [18/21], [94mLoss[0m : 5.44271
[1mStep[0m  [20/21], [94mLoss[0m : 5.56179

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.587, [92mTest[0m: 6.395, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.55862
[1mStep[0m  [2/21], [94mLoss[0m : 5.61921
[1mStep[0m  [4/21], [94mLoss[0m : 5.49405
[1mStep[0m  [6/21], [94mLoss[0m : 5.42343
[1mStep[0m  [8/21], [94mLoss[0m : 5.47727
[1mStep[0m  [10/21], [94mLoss[0m : 5.44472
[1mStep[0m  [12/21], [94mLoss[0m : 5.28394
[1mStep[0m  [14/21], [94mLoss[0m : 5.48285
[1mStep[0m  [16/21], [94mLoss[0m : 5.46023
[1mStep[0m  [18/21], [94mLoss[0m : 5.26961
[1mStep[0m  [20/21], [94mLoss[0m : 5.34788

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.463, [92mTest[0m: 6.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.40410
[1mStep[0m  [2/21], [94mLoss[0m : 5.32806
[1mStep[0m  [4/21], [94mLoss[0m : 5.12760
[1mStep[0m  [6/21], [94mLoss[0m : 5.34004
[1mStep[0m  [8/21], [94mLoss[0m : 5.20617
[1mStep[0m  [10/21], [94mLoss[0m : 5.08342
[1mStep[0m  [12/21], [94mLoss[0m : 5.69001
[1mStep[0m  [14/21], [94mLoss[0m : 5.41805
[1mStep[0m  [16/21], [94mLoss[0m : 5.20663
[1mStep[0m  [18/21], [94mLoss[0m : 5.19064
[1mStep[0m  [20/21], [94mLoss[0m : 5.04775

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.336, [92mTest[0m: 6.108, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16466
[1mStep[0m  [2/21], [94mLoss[0m : 5.29236
[1mStep[0m  [4/21], [94mLoss[0m : 5.16117
[1mStep[0m  [6/21], [94mLoss[0m : 5.26100
[1mStep[0m  [8/21], [94mLoss[0m : 5.15081
[1mStep[0m  [10/21], [94mLoss[0m : 5.18780
[1mStep[0m  [12/21], [94mLoss[0m : 5.21050
[1mStep[0m  [14/21], [94mLoss[0m : 5.13735
[1mStep[0m  [16/21], [94mLoss[0m : 5.27209
[1mStep[0m  [18/21], [94mLoss[0m : 5.00984
[1mStep[0m  [20/21], [94mLoss[0m : 5.11733

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.212, [92mTest[0m: 5.986, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.16548
[1mStep[0m  [2/21], [94mLoss[0m : 5.20877
[1mStep[0m  [4/21], [94mLoss[0m : 4.81822
[1mStep[0m  [6/21], [94mLoss[0m : 5.01363
[1mStep[0m  [8/21], [94mLoss[0m : 5.26386
[1mStep[0m  [10/21], [94mLoss[0m : 5.22620
[1mStep[0m  [12/21], [94mLoss[0m : 5.00568
[1mStep[0m  [14/21], [94mLoss[0m : 5.02217
[1mStep[0m  [16/21], [94mLoss[0m : 5.14396
[1mStep[0m  [18/21], [94mLoss[0m : 5.04440
[1mStep[0m  [20/21], [94mLoss[0m : 4.91848

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.082, [92mTest[0m: 5.867, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.94797
[1mStep[0m  [2/21], [94mLoss[0m : 5.06832
[1mStep[0m  [4/21], [94mLoss[0m : 4.78847
[1mStep[0m  [6/21], [94mLoss[0m : 5.07132
[1mStep[0m  [8/21], [94mLoss[0m : 5.09175
[1mStep[0m  [10/21], [94mLoss[0m : 5.01792
[1mStep[0m  [12/21], [94mLoss[0m : 4.79670
[1mStep[0m  [14/21], [94mLoss[0m : 4.86839
[1mStep[0m  [16/21], [94mLoss[0m : 5.07126
[1mStep[0m  [18/21], [94mLoss[0m : 5.17418
[1mStep[0m  [20/21], [94mLoss[0m : 5.00244

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.980, [92mTest[0m: 5.734, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.83732
[1mStep[0m  [2/21], [94mLoss[0m : 4.95090
[1mStep[0m  [4/21], [94mLoss[0m : 5.01991
[1mStep[0m  [6/21], [94mLoss[0m : 4.61926
[1mStep[0m  [8/21], [94mLoss[0m : 5.19142
[1mStep[0m  [10/21], [94mLoss[0m : 4.80058
[1mStep[0m  [12/21], [94mLoss[0m : 5.07832
[1mStep[0m  [14/21], [94mLoss[0m : 4.67890
[1mStep[0m  [16/21], [94mLoss[0m : 4.89705
[1mStep[0m  [18/21], [94mLoss[0m : 4.88805
[1mStep[0m  [20/21], [94mLoss[0m : 4.66998

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.855, [92mTest[0m: 5.643, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82985
[1mStep[0m  [2/21], [94mLoss[0m : 5.12720
[1mStep[0m  [4/21], [94mLoss[0m : 4.76003
[1mStep[0m  [6/21], [94mLoss[0m : 4.69048
[1mStep[0m  [8/21], [94mLoss[0m : 4.75243
[1mStep[0m  [10/21], [94mLoss[0m : 4.70217
[1mStep[0m  [12/21], [94mLoss[0m : 4.41649
[1mStep[0m  [14/21], [94mLoss[0m : 4.62634
[1mStep[0m  [16/21], [94mLoss[0m : 4.59978
[1mStep[0m  [18/21], [94mLoss[0m : 4.56823
[1mStep[0m  [20/21], [94mLoss[0m : 4.59142

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.743, [92mTest[0m: 5.491, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.82685
[1mStep[0m  [2/21], [94mLoss[0m : 4.83424
[1mStep[0m  [4/21], [94mLoss[0m : 4.69178
[1mStep[0m  [6/21], [94mLoss[0m : 4.45429
[1mStep[0m  [8/21], [94mLoss[0m : 4.67164
[1mStep[0m  [10/21], [94mLoss[0m : 4.77041
[1mStep[0m  [12/21], [94mLoss[0m : 4.57449
[1mStep[0m  [14/21], [94mLoss[0m : 4.67919
[1mStep[0m  [16/21], [94mLoss[0m : 4.75445
[1mStep[0m  [18/21], [94mLoss[0m : 4.79981
[1mStep[0m  [20/21], [94mLoss[0m : 4.44793

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 4.632, [92mTest[0m: 5.328, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.40113
[1mStep[0m  [2/21], [94mLoss[0m : 4.61750
[1mStep[0m  [4/21], [94mLoss[0m : 4.47273
[1mStep[0m  [6/21], [94mLoss[0m : 4.54100
[1mStep[0m  [8/21], [94mLoss[0m : 4.34101
[1mStep[0m  [10/21], [94mLoss[0m : 4.65101
[1mStep[0m  [12/21], [94mLoss[0m : 4.42811
[1mStep[0m  [14/21], [94mLoss[0m : 4.42507
[1mStep[0m  [16/21], [94mLoss[0m : 4.33766
[1mStep[0m  [18/21], [94mLoss[0m : 4.74392
[1mStep[0m  [20/21], [94mLoss[0m : 4.71222

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 4.526, [92mTest[0m: 5.224, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.48469
[1mStep[0m  [2/21], [94mLoss[0m : 4.63275
[1mStep[0m  [4/21], [94mLoss[0m : 4.29698
[1mStep[0m  [6/21], [94mLoss[0m : 4.40771
[1mStep[0m  [8/21], [94mLoss[0m : 4.23969
[1mStep[0m  [10/21], [94mLoss[0m : 4.25820
[1mStep[0m  [12/21], [94mLoss[0m : 4.54414
[1mStep[0m  [14/21], [94mLoss[0m : 4.38565
[1mStep[0m  [16/21], [94mLoss[0m : 4.47573
[1mStep[0m  [18/21], [94mLoss[0m : 4.68355
[1mStep[0m  [20/21], [94mLoss[0m : 4.29593

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 4.433, [92mTest[0m: 5.077, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.30957
[1mStep[0m  [2/21], [94mLoss[0m : 4.41984
[1mStep[0m  [4/21], [94mLoss[0m : 4.31127
[1mStep[0m  [6/21], [94mLoss[0m : 4.31754
[1mStep[0m  [8/21], [94mLoss[0m : 4.16348
[1mStep[0m  [10/21], [94mLoss[0m : 4.26643
[1mStep[0m  [12/21], [94mLoss[0m : 4.48040
[1mStep[0m  [14/21], [94mLoss[0m : 4.29862
[1mStep[0m  [16/21], [94mLoss[0m : 4.32668
[1mStep[0m  [18/21], [94mLoss[0m : 4.08191
[1mStep[0m  [20/21], [94mLoss[0m : 4.17551

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.327, [92mTest[0m: 4.978, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.33949
[1mStep[0m  [2/21], [94mLoss[0m : 4.28399
[1mStep[0m  [4/21], [94mLoss[0m : 4.18622
[1mStep[0m  [6/21], [94mLoss[0m : 4.07121
[1mStep[0m  [8/21], [94mLoss[0m : 4.41238
[1mStep[0m  [10/21], [94mLoss[0m : 4.21912
[1mStep[0m  [12/21], [94mLoss[0m : 4.39529
[1mStep[0m  [14/21], [94mLoss[0m : 4.19031
[1mStep[0m  [16/21], [94mLoss[0m : 4.17243
[1mStep[0m  [18/21], [94mLoss[0m : 4.04462
[1mStep[0m  [20/21], [94mLoss[0m : 4.29616

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.224, [92mTest[0m: 4.870, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.94239
[1mStep[0m  [2/21], [94mLoss[0m : 4.40262
[1mStep[0m  [4/21], [94mLoss[0m : 4.37133
[1mStep[0m  [6/21], [94mLoss[0m : 4.27457
[1mStep[0m  [8/21], [94mLoss[0m : 4.05860
[1mStep[0m  [10/21], [94mLoss[0m : 4.19435
[1mStep[0m  [12/21], [94mLoss[0m : 3.95571
[1mStep[0m  [14/21], [94mLoss[0m : 4.14623
[1mStep[0m  [16/21], [94mLoss[0m : 4.15972
[1mStep[0m  [18/21], [94mLoss[0m : 4.04877
[1mStep[0m  [20/21], [94mLoss[0m : 4.37435

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.144, [92mTest[0m: 4.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.31602
[1mStep[0m  [2/21], [94mLoss[0m : 4.11608
[1mStep[0m  [4/21], [94mLoss[0m : 4.23455
[1mStep[0m  [6/21], [94mLoss[0m : 3.82967
[1mStep[0m  [8/21], [94mLoss[0m : 4.16231
[1mStep[0m  [10/21], [94mLoss[0m : 3.86830
[1mStep[0m  [12/21], [94mLoss[0m : 4.08456
[1mStep[0m  [14/21], [94mLoss[0m : 4.02345
[1mStep[0m  [16/21], [94mLoss[0m : 4.05176
[1mStep[0m  [18/21], [94mLoss[0m : 4.12595
[1mStep[0m  [20/21], [94mLoss[0m : 3.78733

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.075, [92mTest[0m: 4.640, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.76225
[1mStep[0m  [2/21], [94mLoss[0m : 3.98210
[1mStep[0m  [4/21], [94mLoss[0m : 4.06840
[1mStep[0m  [6/21], [94mLoss[0m : 4.00011
[1mStep[0m  [8/21], [94mLoss[0m : 4.06998
[1mStep[0m  [10/21], [94mLoss[0m : 4.06776
[1mStep[0m  [12/21], [94mLoss[0m : 3.92391
[1mStep[0m  [14/21], [94mLoss[0m : 4.13348
[1mStep[0m  [16/21], [94mLoss[0m : 4.00203
[1mStep[0m  [18/21], [94mLoss[0m : 4.15863
[1mStep[0m  [20/21], [94mLoss[0m : 4.01668

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.995, [92mTest[0m: 4.528, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.95010
[1mStep[0m  [2/21], [94mLoss[0m : 3.89239
[1mStep[0m  [4/21], [94mLoss[0m : 3.99736
[1mStep[0m  [6/21], [94mLoss[0m : 4.01799
[1mStep[0m  [8/21], [94mLoss[0m : 3.88372
[1mStep[0m  [10/21], [94mLoss[0m : 3.79473
[1mStep[0m  [12/21], [94mLoss[0m : 3.88313
[1mStep[0m  [14/21], [94mLoss[0m : 3.68992
[1mStep[0m  [16/21], [94mLoss[0m : 3.81520
[1mStep[0m  [18/21], [94mLoss[0m : 3.90162
[1mStep[0m  [20/21], [94mLoss[0m : 3.82612

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.923, [92mTest[0m: 4.456, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.85582
[1mStep[0m  [2/21], [94mLoss[0m : 3.58221
[1mStep[0m  [4/21], [94mLoss[0m : 4.00671
[1mStep[0m  [6/21], [94mLoss[0m : 3.95336
[1mStep[0m  [8/21], [94mLoss[0m : 3.72590
[1mStep[0m  [10/21], [94mLoss[0m : 3.75495
[1mStep[0m  [12/21], [94mLoss[0m : 4.09262
[1mStep[0m  [14/21], [94mLoss[0m : 3.69971
[1mStep[0m  [16/21], [94mLoss[0m : 3.65267
[1mStep[0m  [18/21], [94mLoss[0m : 3.93239
[1mStep[0m  [20/21], [94mLoss[0m : 3.81080

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.839, [92mTest[0m: 4.349, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.73611
[1mStep[0m  [2/21], [94mLoss[0m : 3.76626
[1mStep[0m  [4/21], [94mLoss[0m : 3.77570
[1mStep[0m  [6/21], [94mLoss[0m : 3.85795
[1mStep[0m  [8/21], [94mLoss[0m : 3.68493
[1mStep[0m  [10/21], [94mLoss[0m : 3.87170
[1mStep[0m  [12/21], [94mLoss[0m : 3.74400
[1mStep[0m  [14/21], [94mLoss[0m : 3.87423
[1mStep[0m  [16/21], [94mLoss[0m : 3.88911
[1mStep[0m  [18/21], [94mLoss[0m : 3.75373
[1mStep[0m  [20/21], [94mLoss[0m : 3.79485

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.763, [92mTest[0m: 4.277, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.77810
[1mStep[0m  [2/21], [94mLoss[0m : 3.68366
[1mStep[0m  [4/21], [94mLoss[0m : 3.74497
[1mStep[0m  [6/21], [94mLoss[0m : 3.75119
[1mStep[0m  [8/21], [94mLoss[0m : 3.70994
[1mStep[0m  [10/21], [94mLoss[0m : 3.71309
[1mStep[0m  [12/21], [94mLoss[0m : 3.41782
[1mStep[0m  [14/21], [94mLoss[0m : 3.82898
[1mStep[0m  [16/21], [94mLoss[0m : 3.65578
[1mStep[0m  [18/21], [94mLoss[0m : 3.45261
[1mStep[0m  [20/21], [94mLoss[0m : 3.56213

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.706, [92mTest[0m: 4.160, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.62956
[1mStep[0m  [2/21], [94mLoss[0m : 3.70637
[1mStep[0m  [4/21], [94mLoss[0m : 3.39062
[1mStep[0m  [6/21], [94mLoss[0m : 3.67365
[1mStep[0m  [8/21], [94mLoss[0m : 3.66020
[1mStep[0m  [10/21], [94mLoss[0m : 3.72101
[1mStep[0m  [12/21], [94mLoss[0m : 3.49001
[1mStep[0m  [14/21], [94mLoss[0m : 3.54794
[1mStep[0m  [16/21], [94mLoss[0m : 3.45331
[1mStep[0m  [18/21], [94mLoss[0m : 3.53504
[1mStep[0m  [20/21], [94mLoss[0m : 3.62596

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.623, [92mTest[0m: 4.059, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.79229
[1mStep[0m  [2/21], [94mLoss[0m : 3.48561
[1mStep[0m  [4/21], [94mLoss[0m : 3.63886
[1mStep[0m  [6/21], [94mLoss[0m : 3.46424
[1mStep[0m  [8/21], [94mLoss[0m : 3.44169
[1mStep[0m  [10/21], [94mLoss[0m : 3.44541
[1mStep[0m  [12/21], [94mLoss[0m : 3.55481
[1mStep[0m  [14/21], [94mLoss[0m : 3.74727
[1mStep[0m  [16/21], [94mLoss[0m : 3.42887
[1mStep[0m  [18/21], [94mLoss[0m : 3.65716
[1mStep[0m  [20/21], [94mLoss[0m : 3.51653

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.584, [92mTest[0m: 4.000, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55035
[1mStep[0m  [2/21], [94mLoss[0m : 3.56198
[1mStep[0m  [4/21], [94mLoss[0m : 3.41136
[1mStep[0m  [6/21], [94mLoss[0m : 3.59196
[1mStep[0m  [8/21], [94mLoss[0m : 3.67817
[1mStep[0m  [10/21], [94mLoss[0m : 3.40224
[1mStep[0m  [12/21], [94mLoss[0m : 3.53714
[1mStep[0m  [14/21], [94mLoss[0m : 3.64773
[1mStep[0m  [16/21], [94mLoss[0m : 3.25154
[1mStep[0m  [18/21], [94mLoss[0m : 3.46351
[1mStep[0m  [20/21], [94mLoss[0m : 3.34642

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.515, [92mTest[0m: 3.925, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.823
====================================

Phase 2 - Evaluation MAE:  3.8226403849465505
MAE score P1      7.290859
MAE score P2       3.82264
loss              3.514953
learning_rate       0.0001
batch_size             512
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.24822
[1mStep[0m  [4/42], [94mLoss[0m : 11.23449
[1mStep[0m  [8/42], [94mLoss[0m : 10.93053
[1mStep[0m  [12/42], [94mLoss[0m : 10.76593
[1mStep[0m  [16/42], [94mLoss[0m : 10.87237
[1mStep[0m  [20/42], [94mLoss[0m : 11.11217
[1mStep[0m  [24/42], [94mLoss[0m : 10.64451
[1mStep[0m  [28/42], [94mLoss[0m : 10.70846
[1mStep[0m  [32/42], [94mLoss[0m : 10.71704
[1mStep[0m  [36/42], [94mLoss[0m : 10.64515
[1mStep[0m  [40/42], [94mLoss[0m : 10.61534

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.812, [92mTest[0m: 11.031, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.39325
[1mStep[0m  [4/42], [94mLoss[0m : 10.83932
[1mStep[0m  [8/42], [94mLoss[0m : 10.34424
[1mStep[0m  [12/42], [94mLoss[0m : 10.72727
[1mStep[0m  [16/42], [94mLoss[0m : 10.33433
[1mStep[0m  [20/42], [94mLoss[0m : 10.32917
[1mStep[0m  [24/42], [94mLoss[0m : 9.89569
[1mStep[0m  [28/42], [94mLoss[0m : 10.35474
[1mStep[0m  [32/42], [94mLoss[0m : 10.29029
[1mStep[0m  [36/42], [94mLoss[0m : 10.09519
[1mStep[0m  [40/42], [94mLoss[0m : 9.84022

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.181, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.66732
[1mStep[0m  [4/42], [94mLoss[0m : 9.89496
[1mStep[0m  [8/42], [94mLoss[0m : 10.03918
[1mStep[0m  [12/42], [94mLoss[0m : 9.85003
[1mStep[0m  [16/42], [94mLoss[0m : 9.71315
[1mStep[0m  [20/42], [94mLoss[0m : 9.42801
[1mStep[0m  [24/42], [94mLoss[0m : 9.52116
[1mStep[0m  [28/42], [94mLoss[0m : 9.40993
[1mStep[0m  [32/42], [94mLoss[0m : 9.57607
[1mStep[0m  [36/42], [94mLoss[0m : 9.18917
[1mStep[0m  [40/42], [94mLoss[0m : 9.55772

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.509, [92mTest[0m: 10.139, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.25941
[1mStep[0m  [4/42], [94mLoss[0m : 9.37904
[1mStep[0m  [8/42], [94mLoss[0m : 8.91560
[1mStep[0m  [12/42], [94mLoss[0m : 9.10219
[1mStep[0m  [16/42], [94mLoss[0m : 8.62568
[1mStep[0m  [20/42], [94mLoss[0m : 8.89935
[1mStep[0m  [24/42], [94mLoss[0m : 8.97171
[1mStep[0m  [28/42], [94mLoss[0m : 8.91676
[1mStep[0m  [32/42], [94mLoss[0m : 8.95330
[1mStep[0m  [36/42], [94mLoss[0m : 8.72508
[1mStep[0m  [40/42], [94mLoss[0m : 8.19646

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.832, [92mTest[0m: 9.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.65917
[1mStep[0m  [4/42], [94mLoss[0m : 8.46058
[1mStep[0m  [8/42], [94mLoss[0m : 8.49019
[1mStep[0m  [12/42], [94mLoss[0m : 8.23981
[1mStep[0m  [16/42], [94mLoss[0m : 8.04579
[1mStep[0m  [20/42], [94mLoss[0m : 8.31405
[1mStep[0m  [24/42], [94mLoss[0m : 7.75259
[1mStep[0m  [28/42], [94mLoss[0m : 7.81436
[1mStep[0m  [32/42], [94mLoss[0m : 8.47936
[1mStep[0m  [36/42], [94mLoss[0m : 7.92880
[1mStep[0m  [40/42], [94mLoss[0m : 7.70944

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.154, [92mTest[0m: 9.153, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.88563
[1mStep[0m  [4/42], [94mLoss[0m : 7.54198
[1mStep[0m  [8/42], [94mLoss[0m : 7.81149
[1mStep[0m  [12/42], [94mLoss[0m : 7.34904
[1mStep[0m  [16/42], [94mLoss[0m : 7.45281
[1mStep[0m  [20/42], [94mLoss[0m : 7.15866
[1mStep[0m  [24/42], [94mLoss[0m : 7.38636
[1mStep[0m  [28/42], [94mLoss[0m : 7.21822
[1mStep[0m  [32/42], [94mLoss[0m : 7.39258
[1mStep[0m  [36/42], [94mLoss[0m : 7.63999
[1mStep[0m  [40/42], [94mLoss[0m : 7.40594

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.506, [92mTest[0m: 8.636, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.65433
[1mStep[0m  [4/42], [94mLoss[0m : 7.03043
[1mStep[0m  [8/42], [94mLoss[0m : 7.03988
[1mStep[0m  [12/42], [94mLoss[0m : 6.92776
[1mStep[0m  [16/42], [94mLoss[0m : 6.87799
[1mStep[0m  [20/42], [94mLoss[0m : 6.65269
[1mStep[0m  [24/42], [94mLoss[0m : 6.55751
[1mStep[0m  [28/42], [94mLoss[0m : 6.54199
[1mStep[0m  [32/42], [94mLoss[0m : 6.61796
[1mStep[0m  [36/42], [94mLoss[0m : 7.13201
[1mStep[0m  [40/42], [94mLoss[0m : 6.39345

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.846, [92mTest[0m: 8.130, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.39769
[1mStep[0m  [4/42], [94mLoss[0m : 6.11662
[1mStep[0m  [8/42], [94mLoss[0m : 6.18777
[1mStep[0m  [12/42], [94mLoss[0m : 6.25606
[1mStep[0m  [16/42], [94mLoss[0m : 6.51258
[1mStep[0m  [20/42], [94mLoss[0m : 5.98248
[1mStep[0m  [24/42], [94mLoss[0m : 5.84859
[1mStep[0m  [28/42], [94mLoss[0m : 5.65494
[1mStep[0m  [32/42], [94mLoss[0m : 6.17525
[1mStep[0m  [36/42], [94mLoss[0m : 6.36295
[1mStep[0m  [40/42], [94mLoss[0m : 5.92503

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.195, [92mTest[0m: 7.592, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.02258
[1mStep[0m  [4/42], [94mLoss[0m : 5.72679
[1mStep[0m  [8/42], [94mLoss[0m : 5.92266
[1mStep[0m  [12/42], [94mLoss[0m : 5.52545
[1mStep[0m  [16/42], [94mLoss[0m : 5.60181
[1mStep[0m  [20/42], [94mLoss[0m : 5.85782
[1mStep[0m  [24/42], [94mLoss[0m : 5.67312
[1mStep[0m  [28/42], [94mLoss[0m : 5.68899
[1mStep[0m  [32/42], [94mLoss[0m : 5.20554
[1mStep[0m  [36/42], [94mLoss[0m : 5.34032
[1mStep[0m  [40/42], [94mLoss[0m : 5.18967

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.595, [92mTest[0m: 7.028, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.83727
[1mStep[0m  [4/42], [94mLoss[0m : 5.24356
[1mStep[0m  [8/42], [94mLoss[0m : 4.85208
[1mStep[0m  [12/42], [94mLoss[0m : 5.37083
[1mStep[0m  [16/42], [94mLoss[0m : 4.73664
[1mStep[0m  [20/42], [94mLoss[0m : 5.09649
[1mStep[0m  [24/42], [94mLoss[0m : 4.88076
[1mStep[0m  [28/42], [94mLoss[0m : 4.43498
[1mStep[0m  [32/42], [94mLoss[0m : 4.87784
[1mStep[0m  [36/42], [94mLoss[0m : 4.69502
[1mStep[0m  [40/42], [94mLoss[0m : 4.15847

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.991, [92mTest[0m: 6.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.30948
[1mStep[0m  [4/42], [94mLoss[0m : 4.43505
[1mStep[0m  [8/42], [94mLoss[0m : 4.57517
[1mStep[0m  [12/42], [94mLoss[0m : 4.64320
[1mStep[0m  [16/42], [94mLoss[0m : 4.39724
[1mStep[0m  [20/42], [94mLoss[0m : 4.15582
[1mStep[0m  [24/42], [94mLoss[0m : 4.56524
[1mStep[0m  [28/42], [94mLoss[0m : 4.39144
[1mStep[0m  [32/42], [94mLoss[0m : 4.54650
[1mStep[0m  [36/42], [94mLoss[0m : 4.11329
[1mStep[0m  [40/42], [94mLoss[0m : 4.30476

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.469, [92mTest[0m: 5.771, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 10 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.197
====================================

Phase 1 - Evaluation MAE:  5.196770599910191
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 4.16178
[1mStep[0m  [4/42], [94mLoss[0m : 4.04808
[1mStep[0m  [8/42], [94mLoss[0m : 4.35074
[1mStep[0m  [12/42], [94mLoss[0m : 4.27646
[1mStep[0m  [16/42], [94mLoss[0m : 4.29305
[1mStep[0m  [20/42], [94mLoss[0m : 4.44783
[1mStep[0m  [24/42], [94mLoss[0m : 3.72783
[1mStep[0m  [28/42], [94mLoss[0m : 3.86409
[1mStep[0m  [32/42], [94mLoss[0m : 4.23716
[1mStep[0m  [36/42], [94mLoss[0m : 4.04577
[1mStep[0m  [40/42], [94mLoss[0m : 3.81029

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.057, [92mTest[0m: 5.205, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.950
====================================

Phase 2 - Evaluation MAE:  4.950381687709263
MAE score P1        5.196771
MAE score P2        4.950382
loss                4.056811
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.9
weight_decay            0.01
Name: 11, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.28879
[1mStep[0m  [4/42], [94mLoss[0m : 11.09825
[1mStep[0m  [8/42], [94mLoss[0m : 11.15556
[1mStep[0m  [12/42], [94mLoss[0m : 10.91692
[1mStep[0m  [16/42], [94mLoss[0m : 11.37892
[1mStep[0m  [20/42], [94mLoss[0m : 11.48305
[1mStep[0m  [24/42], [94mLoss[0m : 10.98060
[1mStep[0m  [28/42], [94mLoss[0m : 10.97584
[1mStep[0m  [32/42], [94mLoss[0m : 11.14707
[1mStep[0m  [36/42], [94mLoss[0m : 11.33263
[1mStep[0m  [40/42], [94mLoss[0m : 10.80521

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.224, [92mTest[0m: 11.295, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.33710
[1mStep[0m  [4/42], [94mLoss[0m : 11.76149
[1mStep[0m  [8/42], [94mLoss[0m : 10.78778
[1mStep[0m  [12/42], [94mLoss[0m : 11.23542
[1mStep[0m  [16/42], [94mLoss[0m : 11.44287
[1mStep[0m  [20/42], [94mLoss[0m : 11.07012
[1mStep[0m  [24/42], [94mLoss[0m : 11.25587
[1mStep[0m  [28/42], [94mLoss[0m : 11.45192
[1mStep[0m  [32/42], [94mLoss[0m : 11.44699
[1mStep[0m  [36/42], [94mLoss[0m : 10.90384
[1mStep[0m  [40/42], [94mLoss[0m : 11.18679

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 11.103, [92mTest[0m: 11.162, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.95131
[1mStep[0m  [4/42], [94mLoss[0m : 11.17584
[1mStep[0m  [8/42], [94mLoss[0m : 11.23643
[1mStep[0m  [12/42], [94mLoss[0m : 11.13490
[1mStep[0m  [16/42], [94mLoss[0m : 11.16556
[1mStep[0m  [20/42], [94mLoss[0m : 11.05455
[1mStep[0m  [24/42], [94mLoss[0m : 11.06672
[1mStep[0m  [28/42], [94mLoss[0m : 10.50133
[1mStep[0m  [32/42], [94mLoss[0m : 10.87207
[1mStep[0m  [36/42], [94mLoss[0m : 10.93341
[1mStep[0m  [40/42], [94mLoss[0m : 10.98955

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.979, [92mTest[0m: 11.030, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76219
[1mStep[0m  [4/42], [94mLoss[0m : 11.03562
[1mStep[0m  [8/42], [94mLoss[0m : 10.74588
[1mStep[0m  [12/42], [94mLoss[0m : 10.66838
[1mStep[0m  [16/42], [94mLoss[0m : 10.71356
[1mStep[0m  [20/42], [94mLoss[0m : 10.99488
[1mStep[0m  [24/42], [94mLoss[0m : 10.69511
[1mStep[0m  [28/42], [94mLoss[0m : 10.66838
[1mStep[0m  [32/42], [94mLoss[0m : 10.71648
[1mStep[0m  [36/42], [94mLoss[0m : 11.01777
[1mStep[0m  [40/42], [94mLoss[0m : 10.96263

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.860, [92mTest[0m: 10.906, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90045
[1mStep[0m  [4/42], [94mLoss[0m : 10.73523
[1mStep[0m  [8/42], [94mLoss[0m : 11.10631
[1mStep[0m  [12/42], [94mLoss[0m : 10.89223
[1mStep[0m  [16/42], [94mLoss[0m : 10.88356
[1mStep[0m  [20/42], [94mLoss[0m : 10.32967
[1mStep[0m  [24/42], [94mLoss[0m : 10.64989
[1mStep[0m  [28/42], [94mLoss[0m : 10.99368
[1mStep[0m  [32/42], [94mLoss[0m : 10.45860
[1mStep[0m  [36/42], [94mLoss[0m : 11.16089
[1mStep[0m  [40/42], [94mLoss[0m : 10.56518

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.743, [92mTest[0m: 10.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71479
[1mStep[0m  [4/42], [94mLoss[0m : 10.74734
[1mStep[0m  [8/42], [94mLoss[0m : 11.04299
[1mStep[0m  [12/42], [94mLoss[0m : 11.12420
[1mStep[0m  [16/42], [94mLoss[0m : 10.35729
[1mStep[0m  [20/42], [94mLoss[0m : 10.43514
[1mStep[0m  [24/42], [94mLoss[0m : 10.47127
[1mStep[0m  [28/42], [94mLoss[0m : 10.39311
[1mStep[0m  [32/42], [94mLoss[0m : 10.74533
[1mStep[0m  [36/42], [94mLoss[0m : 10.41357
[1mStep[0m  [40/42], [94mLoss[0m : 10.92615

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08676
[1mStep[0m  [4/42], [94mLoss[0m : 10.59101
[1mStep[0m  [8/42], [94mLoss[0m : 10.58672
[1mStep[0m  [12/42], [94mLoss[0m : 10.35727
[1mStep[0m  [16/42], [94mLoss[0m : 10.62475
[1mStep[0m  [20/42], [94mLoss[0m : 10.47167
[1mStep[0m  [24/42], [94mLoss[0m : 10.60871
[1mStep[0m  [28/42], [94mLoss[0m : 10.75817
[1mStep[0m  [32/42], [94mLoss[0m : 10.10710
[1mStep[0m  [36/42], [94mLoss[0m : 10.27273
[1mStep[0m  [40/42], [94mLoss[0m : 10.33235

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.497, [92mTest[0m: 10.556, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.75987
[1mStep[0m  [4/42], [94mLoss[0m : 10.68788
[1mStep[0m  [8/42], [94mLoss[0m : 10.17095
[1mStep[0m  [12/42], [94mLoss[0m : 10.29875
[1mStep[0m  [16/42], [94mLoss[0m : 9.92832
[1mStep[0m  [20/42], [94mLoss[0m : 10.46258
[1mStep[0m  [24/42], [94mLoss[0m : 10.33290
[1mStep[0m  [28/42], [94mLoss[0m : 10.32612
[1mStep[0m  [32/42], [94mLoss[0m : 10.63154
[1mStep[0m  [36/42], [94mLoss[0m : 10.46485
[1mStep[0m  [40/42], [94mLoss[0m : 10.65966

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.388, [92mTest[0m: 10.420, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.68157
[1mStep[0m  [4/42], [94mLoss[0m : 9.94042
[1mStep[0m  [8/42], [94mLoss[0m : 10.62324
[1mStep[0m  [12/42], [94mLoss[0m : 10.52683
[1mStep[0m  [16/42], [94mLoss[0m : 10.27389
[1mStep[0m  [20/42], [94mLoss[0m : 10.27976
[1mStep[0m  [24/42], [94mLoss[0m : 10.10831
[1mStep[0m  [28/42], [94mLoss[0m : 10.26494
[1mStep[0m  [32/42], [94mLoss[0m : 10.17850
[1mStep[0m  [36/42], [94mLoss[0m : 10.23175
[1mStep[0m  [40/42], [94mLoss[0m : 10.50040

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.260, [92mTest[0m: 10.305, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.46302
[1mStep[0m  [4/42], [94mLoss[0m : 10.44179
[1mStep[0m  [8/42], [94mLoss[0m : 9.95130
[1mStep[0m  [12/42], [94mLoss[0m : 10.42064
[1mStep[0m  [16/42], [94mLoss[0m : 10.23403
[1mStep[0m  [20/42], [94mLoss[0m : 9.89837
[1mStep[0m  [24/42], [94mLoss[0m : 9.98800
[1mStep[0m  [28/42], [94mLoss[0m : 10.38156
[1mStep[0m  [32/42], [94mLoss[0m : 9.81874
[1mStep[0m  [36/42], [94mLoss[0m : 10.07772
[1mStep[0m  [40/42], [94mLoss[0m : 9.94019

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.137, [92mTest[0m: 10.187, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.96907
[1mStep[0m  [4/42], [94mLoss[0m : 10.09923
[1mStep[0m  [8/42], [94mLoss[0m : 9.61813
[1mStep[0m  [12/42], [94mLoss[0m : 10.01481
[1mStep[0m  [16/42], [94mLoss[0m : 9.86376
[1mStep[0m  [20/42], [94mLoss[0m : 9.92101
[1mStep[0m  [24/42], [94mLoss[0m : 10.05552
[1mStep[0m  [28/42], [94mLoss[0m : 9.79143
[1mStep[0m  [32/42], [94mLoss[0m : 9.69404
[1mStep[0m  [36/42], [94mLoss[0m : 10.04018
[1mStep[0m  [40/42], [94mLoss[0m : 9.33955

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.013, [92mTest[0m: 10.066, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.94431
[1mStep[0m  [4/42], [94mLoss[0m : 10.34047
[1mStep[0m  [8/42], [94mLoss[0m : 9.94998
[1mStep[0m  [12/42], [94mLoss[0m : 9.75353
[1mStep[0m  [16/42], [94mLoss[0m : 9.91836
[1mStep[0m  [20/42], [94mLoss[0m : 10.00914
[1mStep[0m  [24/42], [94mLoss[0m : 10.08763
[1mStep[0m  [28/42], [94mLoss[0m : 9.95679
[1mStep[0m  [32/42], [94mLoss[0m : 9.46627
[1mStep[0m  [36/42], [94mLoss[0m : 9.86956
[1mStep[0m  [40/42], [94mLoss[0m : 10.17171

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.901, [92mTest[0m: 9.928, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.86916
[1mStep[0m  [4/42], [94mLoss[0m : 10.00939
[1mStep[0m  [8/42], [94mLoss[0m : 9.75101
[1mStep[0m  [12/42], [94mLoss[0m : 9.78466
[1mStep[0m  [16/42], [94mLoss[0m : 9.78420
[1mStep[0m  [20/42], [94mLoss[0m : 9.31316
[1mStep[0m  [24/42], [94mLoss[0m : 9.69128
[1mStep[0m  [28/42], [94mLoss[0m : 9.68484
[1mStep[0m  [32/42], [94mLoss[0m : 9.53105
[1mStep[0m  [36/42], [94mLoss[0m : 9.89026
[1mStep[0m  [40/42], [94mLoss[0m : 9.70985

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.772, [92mTest[0m: 9.823, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.43239
[1mStep[0m  [4/42], [94mLoss[0m : 9.56601
[1mStep[0m  [8/42], [94mLoss[0m : 9.65451
[1mStep[0m  [12/42], [94mLoss[0m : 9.92629
[1mStep[0m  [16/42], [94mLoss[0m : 9.58600
[1mStep[0m  [20/42], [94mLoss[0m : 9.53678
[1mStep[0m  [24/42], [94mLoss[0m : 9.51517
[1mStep[0m  [28/42], [94mLoss[0m : 9.60109
[1mStep[0m  [32/42], [94mLoss[0m : 9.60812
[1mStep[0m  [36/42], [94mLoss[0m : 9.73871
[1mStep[0m  [40/42], [94mLoss[0m : 9.94319

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.655, [92mTest[0m: 9.699, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.31967
[1mStep[0m  [4/42], [94mLoss[0m : 9.76442
[1mStep[0m  [8/42], [94mLoss[0m : 9.70078
[1mStep[0m  [12/42], [94mLoss[0m : 9.88828
[1mStep[0m  [16/42], [94mLoss[0m : 9.88031
[1mStep[0m  [20/42], [94mLoss[0m : 9.52050
[1mStep[0m  [24/42], [94mLoss[0m : 9.58004
[1mStep[0m  [28/42], [94mLoss[0m : 9.65993
[1mStep[0m  [32/42], [94mLoss[0m : 9.68257
[1mStep[0m  [36/42], [94mLoss[0m : 9.43637
[1mStep[0m  [40/42], [94mLoss[0m : 9.46020

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.524, [92mTest[0m: 9.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.57475
[1mStep[0m  [4/42], [94mLoss[0m : 9.42543
[1mStep[0m  [8/42], [94mLoss[0m : 9.69330
[1mStep[0m  [12/42], [94mLoss[0m : 9.54479
[1mStep[0m  [16/42], [94mLoss[0m : 9.39841
[1mStep[0m  [20/42], [94mLoss[0m : 9.54315
[1mStep[0m  [24/42], [94mLoss[0m : 9.15313
[1mStep[0m  [28/42], [94mLoss[0m : 9.27029
[1mStep[0m  [32/42], [94mLoss[0m : 9.20476
[1mStep[0m  [36/42], [94mLoss[0m : 9.36521
[1mStep[0m  [40/42], [94mLoss[0m : 9.39998

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.400, [92mTest[0m: 9.447, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.51788
[1mStep[0m  [4/42], [94mLoss[0m : 8.90271
[1mStep[0m  [8/42], [94mLoss[0m : 9.61795
[1mStep[0m  [12/42], [94mLoss[0m : 9.29478
[1mStep[0m  [16/42], [94mLoss[0m : 8.94802
[1mStep[0m  [20/42], [94mLoss[0m : 9.08947
[1mStep[0m  [24/42], [94mLoss[0m : 9.46414
[1mStep[0m  [28/42], [94mLoss[0m : 9.05792
[1mStep[0m  [32/42], [94mLoss[0m : 9.30423
[1mStep[0m  [36/42], [94mLoss[0m : 8.81155
[1mStep[0m  [40/42], [94mLoss[0m : 9.40957

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.282, [92mTest[0m: 9.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.56494
[1mStep[0m  [4/42], [94mLoss[0m : 9.30448
[1mStep[0m  [8/42], [94mLoss[0m : 9.04306
[1mStep[0m  [12/42], [94mLoss[0m : 9.20003
[1mStep[0m  [16/42], [94mLoss[0m : 9.21222
[1mStep[0m  [20/42], [94mLoss[0m : 9.20328
[1mStep[0m  [24/42], [94mLoss[0m : 9.33101
[1mStep[0m  [28/42], [94mLoss[0m : 8.89925
[1mStep[0m  [32/42], [94mLoss[0m : 9.34220
[1mStep[0m  [36/42], [94mLoss[0m : 9.01673
[1mStep[0m  [40/42], [94mLoss[0m : 9.33431

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.155, [92mTest[0m: 9.210, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.37081
[1mStep[0m  [4/42], [94mLoss[0m : 9.13114
[1mStep[0m  [8/42], [94mLoss[0m : 8.74992
[1mStep[0m  [12/42], [94mLoss[0m : 9.04172
[1mStep[0m  [16/42], [94mLoss[0m : 9.06216
[1mStep[0m  [20/42], [94mLoss[0m : 8.64512
[1mStep[0m  [24/42], [94mLoss[0m : 8.76840
[1mStep[0m  [28/42], [94mLoss[0m : 8.84179
[1mStep[0m  [32/42], [94mLoss[0m : 8.98900
[1mStep[0m  [36/42], [94mLoss[0m : 9.12704
[1mStep[0m  [40/42], [94mLoss[0m : 8.59087

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.039, [92mTest[0m: 9.078, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.95735
[1mStep[0m  [4/42], [94mLoss[0m : 8.65215
[1mStep[0m  [8/42], [94mLoss[0m : 9.12733
[1mStep[0m  [12/42], [94mLoss[0m : 8.71334
[1mStep[0m  [16/42], [94mLoss[0m : 8.50074
[1mStep[0m  [20/42], [94mLoss[0m : 9.01631
[1mStep[0m  [24/42], [94mLoss[0m : 8.64325
[1mStep[0m  [28/42], [94mLoss[0m : 8.39250
[1mStep[0m  [32/42], [94mLoss[0m : 8.78183
[1mStep[0m  [36/42], [94mLoss[0m : 8.57412
[1mStep[0m  [40/42], [94mLoss[0m : 8.97537

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.928, [92mTest[0m: 8.981, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.39536
[1mStep[0m  [4/42], [94mLoss[0m : 8.94844
[1mStep[0m  [8/42], [94mLoss[0m : 8.73669
[1mStep[0m  [12/42], [94mLoss[0m : 8.56857
[1mStep[0m  [16/42], [94mLoss[0m : 9.13333
[1mStep[0m  [20/42], [94mLoss[0m : 8.64479
[1mStep[0m  [24/42], [94mLoss[0m : 9.19085
[1mStep[0m  [28/42], [94mLoss[0m : 8.55785
[1mStep[0m  [32/42], [94mLoss[0m : 8.91683
[1mStep[0m  [36/42], [94mLoss[0m : 8.49734
[1mStep[0m  [40/42], [94mLoss[0m : 8.80442

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.805, [92mTest[0m: 8.841, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34331
[1mStep[0m  [4/42], [94mLoss[0m : 8.99819
[1mStep[0m  [8/42], [94mLoss[0m : 8.39975
[1mStep[0m  [12/42], [94mLoss[0m : 8.45094
[1mStep[0m  [16/42], [94mLoss[0m : 8.77671
[1mStep[0m  [20/42], [94mLoss[0m : 8.97552
[1mStep[0m  [24/42], [94mLoss[0m : 8.60193
[1mStep[0m  [28/42], [94mLoss[0m : 8.60983
[1mStep[0m  [32/42], [94mLoss[0m : 8.98598
[1mStep[0m  [36/42], [94mLoss[0m : 8.34996
[1mStep[0m  [40/42], [94mLoss[0m : 8.64640

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.698, [92mTest[0m: 8.739, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.37454
[1mStep[0m  [4/42], [94mLoss[0m : 8.46626
[1mStep[0m  [8/42], [94mLoss[0m : 9.09061
[1mStep[0m  [12/42], [94mLoss[0m : 8.45261
[1mStep[0m  [16/42], [94mLoss[0m : 9.06264
[1mStep[0m  [20/42], [94mLoss[0m : 8.59814
[1mStep[0m  [24/42], [94mLoss[0m : 8.60049
[1mStep[0m  [28/42], [94mLoss[0m : 8.30712
[1mStep[0m  [32/42], [94mLoss[0m : 8.84610
[1mStep[0m  [36/42], [94mLoss[0m : 8.73695
[1mStep[0m  [40/42], [94mLoss[0m : 8.59880

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.591, [92mTest[0m: 8.624, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.78475
[1mStep[0m  [4/42], [94mLoss[0m : 8.59712
[1mStep[0m  [8/42], [94mLoss[0m : 8.65653
[1mStep[0m  [12/42], [94mLoss[0m : 8.48574
[1mStep[0m  [16/42], [94mLoss[0m : 8.51142
[1mStep[0m  [20/42], [94mLoss[0m : 8.77799
[1mStep[0m  [24/42], [94mLoss[0m : 8.34138
[1mStep[0m  [28/42], [94mLoss[0m : 8.27431
[1mStep[0m  [32/42], [94mLoss[0m : 8.47197
[1mStep[0m  [36/42], [94mLoss[0m : 8.83921
[1mStep[0m  [40/42], [94mLoss[0m : 8.11711

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.474, [92mTest[0m: 8.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.86041
[1mStep[0m  [4/42], [94mLoss[0m : 7.88774
[1mStep[0m  [8/42], [94mLoss[0m : 8.03723
[1mStep[0m  [12/42], [94mLoss[0m : 8.10876
[1mStep[0m  [16/42], [94mLoss[0m : 8.42024
[1mStep[0m  [20/42], [94mLoss[0m : 8.76060
[1mStep[0m  [24/42], [94mLoss[0m : 8.51639
[1mStep[0m  [28/42], [94mLoss[0m : 8.56439
[1mStep[0m  [32/42], [94mLoss[0m : 8.25731
[1mStep[0m  [36/42], [94mLoss[0m : 8.23820
[1mStep[0m  [40/42], [94mLoss[0m : 8.41995

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.367, [92mTest[0m: 8.410, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.27485
[1mStep[0m  [4/42], [94mLoss[0m : 8.08866
[1mStep[0m  [8/42], [94mLoss[0m : 8.23229
[1mStep[0m  [12/42], [94mLoss[0m : 7.87774
[1mStep[0m  [16/42], [94mLoss[0m : 8.69068
[1mStep[0m  [20/42], [94mLoss[0m : 8.33871
[1mStep[0m  [24/42], [94mLoss[0m : 8.03515
[1mStep[0m  [28/42], [94mLoss[0m : 8.19614
[1mStep[0m  [32/42], [94mLoss[0m : 8.36408
[1mStep[0m  [36/42], [94mLoss[0m : 8.37081
[1mStep[0m  [40/42], [94mLoss[0m : 8.34678

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.254, [92mTest[0m: 8.313, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.28846
[1mStep[0m  [4/42], [94mLoss[0m : 8.05380
[1mStep[0m  [8/42], [94mLoss[0m : 8.22441
[1mStep[0m  [12/42], [94mLoss[0m : 8.24147
[1mStep[0m  [16/42], [94mLoss[0m : 8.21960
[1mStep[0m  [20/42], [94mLoss[0m : 8.29711
[1mStep[0m  [24/42], [94mLoss[0m : 7.82119
[1mStep[0m  [28/42], [94mLoss[0m : 8.48285
[1mStep[0m  [32/42], [94mLoss[0m : 8.31525
[1mStep[0m  [36/42], [94mLoss[0m : 8.03558
[1mStep[0m  [40/42], [94mLoss[0m : 8.40582

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.150, [92mTest[0m: 8.189, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.31390
[1mStep[0m  [4/42], [94mLoss[0m : 8.05174
[1mStep[0m  [8/42], [94mLoss[0m : 8.24907
[1mStep[0m  [12/42], [94mLoss[0m : 7.83172
[1mStep[0m  [16/42], [94mLoss[0m : 8.46442
[1mStep[0m  [20/42], [94mLoss[0m : 8.11625
[1mStep[0m  [24/42], [94mLoss[0m : 8.07263
[1mStep[0m  [28/42], [94mLoss[0m : 7.94191
[1mStep[0m  [32/42], [94mLoss[0m : 8.36525
[1mStep[0m  [36/42], [94mLoss[0m : 8.09687
[1mStep[0m  [40/42], [94mLoss[0m : 8.14820

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.048, [92mTest[0m: 8.070, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.38992
[1mStep[0m  [4/42], [94mLoss[0m : 8.43274
[1mStep[0m  [8/42], [94mLoss[0m : 8.09335
[1mStep[0m  [12/42], [94mLoss[0m : 7.79906
[1mStep[0m  [16/42], [94mLoss[0m : 8.10292
[1mStep[0m  [20/42], [94mLoss[0m : 7.70729
[1mStep[0m  [24/42], [94mLoss[0m : 7.86660
[1mStep[0m  [28/42], [94mLoss[0m : 7.83245
[1mStep[0m  [32/42], [94mLoss[0m : 7.68142
[1mStep[0m  [36/42], [94mLoss[0m : 7.75261
[1mStep[0m  [40/42], [94mLoss[0m : 7.39391

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.928, [92mTest[0m: 7.974, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.07578
[1mStep[0m  [4/42], [94mLoss[0m : 7.87126
[1mStep[0m  [8/42], [94mLoss[0m : 7.81929
[1mStep[0m  [12/42], [94mLoss[0m : 7.46850
[1mStep[0m  [16/42], [94mLoss[0m : 8.19508
[1mStep[0m  [20/42], [94mLoss[0m : 8.00574
[1mStep[0m  [24/42], [94mLoss[0m : 7.60878
[1mStep[0m  [28/42], [94mLoss[0m : 7.96434
[1mStep[0m  [32/42], [94mLoss[0m : 7.55853
[1mStep[0m  [36/42], [94mLoss[0m : 8.24625
[1mStep[0m  [40/42], [94mLoss[0m : 7.49670

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.832, [92mTest[0m: 7.862, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.745
====================================

Phase 1 - Evaluation MAE:  7.74477618081229
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 7.77888
[1mStep[0m  [4/42], [94mLoss[0m : 7.49893
[1mStep[0m  [8/42], [94mLoss[0m : 7.96199
[1mStep[0m  [12/42], [94mLoss[0m : 7.21044
[1mStep[0m  [16/42], [94mLoss[0m : 7.43627
[1mStep[0m  [20/42], [94mLoss[0m : 7.47313
[1mStep[0m  [24/42], [94mLoss[0m : 7.79624
[1mStep[0m  [28/42], [94mLoss[0m : 7.47820
[1mStep[0m  [32/42], [94mLoss[0m : 8.03212
[1mStep[0m  [36/42], [94mLoss[0m : 7.81640
[1mStep[0m  [40/42], [94mLoss[0m : 7.94809

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.707, [92mTest[0m: 7.735, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.74302
[1mStep[0m  [4/42], [94mLoss[0m : 7.33313
[1mStep[0m  [8/42], [94mLoss[0m : 7.94837
[1mStep[0m  [12/42], [94mLoss[0m : 7.33908
[1mStep[0m  [16/42], [94mLoss[0m : 7.68921
[1mStep[0m  [20/42], [94mLoss[0m : 7.71065
[1mStep[0m  [24/42], [94mLoss[0m : 7.69096
[1mStep[0m  [28/42], [94mLoss[0m : 7.38698
[1mStep[0m  [32/42], [94mLoss[0m : 7.43406
[1mStep[0m  [36/42], [94mLoss[0m : 7.99544
[1mStep[0m  [40/42], [94mLoss[0m : 7.45653

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.577, [92mTest[0m: 7.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.39764
[1mStep[0m  [4/42], [94mLoss[0m : 7.30537
[1mStep[0m  [8/42], [94mLoss[0m : 7.60489
[1mStep[0m  [12/42], [94mLoss[0m : 7.63866
[1mStep[0m  [16/42], [94mLoss[0m : 7.41147
[1mStep[0m  [20/42], [94mLoss[0m : 7.46749
[1mStep[0m  [24/42], [94mLoss[0m : 7.68523
[1mStep[0m  [28/42], [94mLoss[0m : 7.21514
[1mStep[0m  [32/42], [94mLoss[0m : 7.53517
[1mStep[0m  [36/42], [94mLoss[0m : 7.13694
[1mStep[0m  [40/42], [94mLoss[0m : 7.42246

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.458, [92mTest[0m: 7.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.41847
[1mStep[0m  [4/42], [94mLoss[0m : 7.66584
[1mStep[0m  [8/42], [94mLoss[0m : 7.19660
[1mStep[0m  [12/42], [94mLoss[0m : 7.25208
[1mStep[0m  [16/42], [94mLoss[0m : 7.31139
[1mStep[0m  [20/42], [94mLoss[0m : 7.33836
[1mStep[0m  [24/42], [94mLoss[0m : 6.99508
[1mStep[0m  [28/42], [94mLoss[0m : 7.38085
[1mStep[0m  [32/42], [94mLoss[0m : 6.83107
[1mStep[0m  [36/42], [94mLoss[0m : 6.98119
[1mStep[0m  [40/42], [94mLoss[0m : 6.92331

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.337, [92mTest[0m: 7.377, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.20413
[1mStep[0m  [4/42], [94mLoss[0m : 7.52447
[1mStep[0m  [8/42], [94mLoss[0m : 7.26821
[1mStep[0m  [12/42], [94mLoss[0m : 7.10891
[1mStep[0m  [16/42], [94mLoss[0m : 7.10070
[1mStep[0m  [20/42], [94mLoss[0m : 6.92208
[1mStep[0m  [24/42], [94mLoss[0m : 7.18810
[1mStep[0m  [28/42], [94mLoss[0m : 7.19704
[1mStep[0m  [32/42], [94mLoss[0m : 7.04988
[1mStep[0m  [36/42], [94mLoss[0m : 7.41301
[1mStep[0m  [40/42], [94mLoss[0m : 6.76999

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.222, [92mTest[0m: 7.258, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.21186
[1mStep[0m  [4/42], [94mLoss[0m : 7.27356
[1mStep[0m  [8/42], [94mLoss[0m : 6.80169
[1mStep[0m  [12/42], [94mLoss[0m : 7.13772
[1mStep[0m  [16/42], [94mLoss[0m : 7.09179
[1mStep[0m  [20/42], [94mLoss[0m : 7.31878
[1mStep[0m  [24/42], [94mLoss[0m : 7.01596
[1mStep[0m  [28/42], [94mLoss[0m : 6.87090
[1mStep[0m  [32/42], [94mLoss[0m : 6.92436
[1mStep[0m  [36/42], [94mLoss[0m : 7.33233
[1mStep[0m  [40/42], [94mLoss[0m : 7.47874

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.092, [92mTest[0m: 7.137, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.93945
[1mStep[0m  [4/42], [94mLoss[0m : 7.40678
[1mStep[0m  [8/42], [94mLoss[0m : 6.92493
[1mStep[0m  [12/42], [94mLoss[0m : 6.91513
[1mStep[0m  [16/42], [94mLoss[0m : 6.77532
[1mStep[0m  [20/42], [94mLoss[0m : 6.84537
[1mStep[0m  [24/42], [94mLoss[0m : 6.77272
[1mStep[0m  [28/42], [94mLoss[0m : 6.77333
[1mStep[0m  [32/42], [94mLoss[0m : 7.04890
[1mStep[0m  [36/42], [94mLoss[0m : 6.71327
[1mStep[0m  [40/42], [94mLoss[0m : 6.71955

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.977, [92mTest[0m: 7.009, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.75023
[1mStep[0m  [4/42], [94mLoss[0m : 6.91892
[1mStep[0m  [8/42], [94mLoss[0m : 7.04727
[1mStep[0m  [12/42], [94mLoss[0m : 6.61374
[1mStep[0m  [16/42], [94mLoss[0m : 6.80196
[1mStep[0m  [20/42], [94mLoss[0m : 7.05622
[1mStep[0m  [24/42], [94mLoss[0m : 6.95993
[1mStep[0m  [28/42], [94mLoss[0m : 6.83840
[1mStep[0m  [32/42], [94mLoss[0m : 7.12620
[1mStep[0m  [36/42], [94mLoss[0m : 7.05217
[1mStep[0m  [40/42], [94mLoss[0m : 6.55956

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.853, [92mTest[0m: 6.882, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.65039
[1mStep[0m  [4/42], [94mLoss[0m : 6.94209
[1mStep[0m  [8/42], [94mLoss[0m : 6.58810
[1mStep[0m  [12/42], [94mLoss[0m : 6.93163
[1mStep[0m  [16/42], [94mLoss[0m : 7.17776
[1mStep[0m  [20/42], [94mLoss[0m : 6.72934
[1mStep[0m  [24/42], [94mLoss[0m : 6.48592
[1mStep[0m  [28/42], [94mLoss[0m : 6.30481
[1mStep[0m  [32/42], [94mLoss[0m : 6.40999
[1mStep[0m  [36/42], [94mLoss[0m : 6.72740
[1mStep[0m  [40/42], [94mLoss[0m : 6.60304

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.731, [92mTest[0m: 6.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.57677
[1mStep[0m  [4/42], [94mLoss[0m : 6.30613
[1mStep[0m  [8/42], [94mLoss[0m : 6.93446
[1mStep[0m  [12/42], [94mLoss[0m : 6.37457
[1mStep[0m  [16/42], [94mLoss[0m : 6.12887
[1mStep[0m  [20/42], [94mLoss[0m : 6.42251
[1mStep[0m  [24/42], [94mLoss[0m : 6.65115
[1mStep[0m  [28/42], [94mLoss[0m : 6.89764
[1mStep[0m  [32/42], [94mLoss[0m : 6.71634
[1mStep[0m  [36/42], [94mLoss[0m : 6.48784
[1mStep[0m  [40/42], [94mLoss[0m : 6.66744

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.601, [92mTest[0m: 6.637, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.43842
[1mStep[0m  [4/42], [94mLoss[0m : 6.35770
[1mStep[0m  [8/42], [94mLoss[0m : 6.33699
[1mStep[0m  [12/42], [94mLoss[0m : 6.83364
[1mStep[0m  [16/42], [94mLoss[0m : 6.26228
[1mStep[0m  [20/42], [94mLoss[0m : 6.93362
[1mStep[0m  [24/42], [94mLoss[0m : 6.26725
[1mStep[0m  [28/42], [94mLoss[0m : 6.43579
[1mStep[0m  [32/42], [94mLoss[0m : 6.35181
[1mStep[0m  [36/42], [94mLoss[0m : 6.14433
[1mStep[0m  [40/42], [94mLoss[0m : 6.70701

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.488, [92mTest[0m: 6.524, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.42100
[1mStep[0m  [4/42], [94mLoss[0m : 6.35576
[1mStep[0m  [8/42], [94mLoss[0m : 6.32865
[1mStep[0m  [12/42], [94mLoss[0m : 6.43411
[1mStep[0m  [16/42], [94mLoss[0m : 6.38478
[1mStep[0m  [20/42], [94mLoss[0m : 6.39722
[1mStep[0m  [24/42], [94mLoss[0m : 6.45138
[1mStep[0m  [28/42], [94mLoss[0m : 6.11422
[1mStep[0m  [32/42], [94mLoss[0m : 6.42284
[1mStep[0m  [36/42], [94mLoss[0m : 6.41327
[1mStep[0m  [40/42], [94mLoss[0m : 6.04429

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.372, [92mTest[0m: 6.384, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.18956
[1mStep[0m  [4/42], [94mLoss[0m : 6.33041
[1mStep[0m  [8/42], [94mLoss[0m : 6.37845
[1mStep[0m  [12/42], [94mLoss[0m : 6.57620
[1mStep[0m  [16/42], [94mLoss[0m : 6.65073
[1mStep[0m  [20/42], [94mLoss[0m : 6.40846
[1mStep[0m  [24/42], [94mLoss[0m : 5.84101
[1mStep[0m  [28/42], [94mLoss[0m : 6.08581
[1mStep[0m  [32/42], [94mLoss[0m : 5.89695
[1mStep[0m  [36/42], [94mLoss[0m : 6.23918
[1mStep[0m  [40/42], [94mLoss[0m : 6.16968

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.245, [92mTest[0m: 6.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.92997
[1mStep[0m  [4/42], [94mLoss[0m : 6.14262
[1mStep[0m  [8/42], [94mLoss[0m : 6.50653
[1mStep[0m  [12/42], [94mLoss[0m : 5.78920
[1mStep[0m  [16/42], [94mLoss[0m : 6.10457
[1mStep[0m  [20/42], [94mLoss[0m : 6.11740
[1mStep[0m  [24/42], [94mLoss[0m : 5.82627
[1mStep[0m  [28/42], [94mLoss[0m : 6.20356
[1mStep[0m  [32/42], [94mLoss[0m : 6.11696
[1mStep[0m  [36/42], [94mLoss[0m : 6.05594
[1mStep[0m  [40/42], [94mLoss[0m : 6.45811

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.122, [92mTest[0m: 6.164, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.35155
[1mStep[0m  [4/42], [94mLoss[0m : 5.99898
[1mStep[0m  [8/42], [94mLoss[0m : 6.24592
[1mStep[0m  [12/42], [94mLoss[0m : 5.80681
[1mStep[0m  [16/42], [94mLoss[0m : 6.10402
[1mStep[0m  [20/42], [94mLoss[0m : 6.05370
[1mStep[0m  [24/42], [94mLoss[0m : 6.28448
[1mStep[0m  [28/42], [94mLoss[0m : 6.17120
[1mStep[0m  [32/42], [94mLoss[0m : 5.46051
[1mStep[0m  [36/42], [94mLoss[0m : 5.60859
[1mStep[0m  [40/42], [94mLoss[0m : 5.95833

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.003, [92mTest[0m: 6.035, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.10666
[1mStep[0m  [4/42], [94mLoss[0m : 5.87949
[1mStep[0m  [8/42], [94mLoss[0m : 5.75718
[1mStep[0m  [12/42], [94mLoss[0m : 5.79713
[1mStep[0m  [16/42], [94mLoss[0m : 5.83613
[1mStep[0m  [20/42], [94mLoss[0m : 5.85898
[1mStep[0m  [24/42], [94mLoss[0m : 5.55247
[1mStep[0m  [28/42], [94mLoss[0m : 5.69751
[1mStep[0m  [32/42], [94mLoss[0m : 5.59434
[1mStep[0m  [36/42], [94mLoss[0m : 5.92937
[1mStep[0m  [40/42], [94mLoss[0m : 5.72093

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.881, [92mTest[0m: 5.898, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.57904
[1mStep[0m  [4/42], [94mLoss[0m : 5.51235
[1mStep[0m  [8/42], [94mLoss[0m : 5.84684
[1mStep[0m  [12/42], [94mLoss[0m : 5.64145
[1mStep[0m  [16/42], [94mLoss[0m : 5.67674
[1mStep[0m  [20/42], [94mLoss[0m : 5.66739
[1mStep[0m  [24/42], [94mLoss[0m : 5.73027
[1mStep[0m  [28/42], [94mLoss[0m : 5.98617
[1mStep[0m  [32/42], [94mLoss[0m : 5.75690
[1mStep[0m  [36/42], [94mLoss[0m : 5.74903
[1mStep[0m  [40/42], [94mLoss[0m : 5.66014

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.763, [92mTest[0m: 5.795, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.67426
[1mStep[0m  [4/42], [94mLoss[0m : 5.82272
[1mStep[0m  [8/42], [94mLoss[0m : 5.92062
[1mStep[0m  [12/42], [94mLoss[0m : 5.55849
[1mStep[0m  [16/42], [94mLoss[0m : 5.76679
[1mStep[0m  [20/42], [94mLoss[0m : 5.44578
[1mStep[0m  [24/42], [94mLoss[0m : 5.09216
[1mStep[0m  [28/42], [94mLoss[0m : 5.44980
[1mStep[0m  [32/42], [94mLoss[0m : 5.17510
[1mStep[0m  [36/42], [94mLoss[0m : 5.84405
[1mStep[0m  [40/42], [94mLoss[0m : 5.65821

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.635, [92mTest[0m: 5.677, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.50830
[1mStep[0m  [4/42], [94mLoss[0m : 5.33408
[1mStep[0m  [8/42], [94mLoss[0m : 5.36235
[1mStep[0m  [12/42], [94mLoss[0m : 5.81863
[1mStep[0m  [16/42], [94mLoss[0m : 5.80760
[1mStep[0m  [20/42], [94mLoss[0m : 5.50792
[1mStep[0m  [24/42], [94mLoss[0m : 5.35571
[1mStep[0m  [28/42], [94mLoss[0m : 5.15607
[1mStep[0m  [32/42], [94mLoss[0m : 5.79230
[1mStep[0m  [36/42], [94mLoss[0m : 5.41688
[1mStep[0m  [40/42], [94mLoss[0m : 5.62943

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.532, [92mTest[0m: 5.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.86889
[1mStep[0m  [4/42], [94mLoss[0m : 5.23909
[1mStep[0m  [8/42], [94mLoss[0m : 5.17914
[1mStep[0m  [12/42], [94mLoss[0m : 5.47325
[1mStep[0m  [16/42], [94mLoss[0m : 5.48039
[1mStep[0m  [20/42], [94mLoss[0m : 5.16820
[1mStep[0m  [24/42], [94mLoss[0m : 5.39934
[1mStep[0m  [28/42], [94mLoss[0m : 5.33776
[1mStep[0m  [32/42], [94mLoss[0m : 5.18654
[1mStep[0m  [36/42], [94mLoss[0m : 5.85244
[1mStep[0m  [40/42], [94mLoss[0m : 5.16542

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.412, [92mTest[0m: 5.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.25797
[1mStep[0m  [4/42], [94mLoss[0m : 5.72163
[1mStep[0m  [8/42], [94mLoss[0m : 5.20628
[1mStep[0m  [12/42], [94mLoss[0m : 5.30803
[1mStep[0m  [16/42], [94mLoss[0m : 5.28207
[1mStep[0m  [20/42], [94mLoss[0m : 5.48064
[1mStep[0m  [24/42], [94mLoss[0m : 4.93644
[1mStep[0m  [28/42], [94mLoss[0m : 4.97555
[1mStep[0m  [32/42], [94mLoss[0m : 5.35958
[1mStep[0m  [36/42], [94mLoss[0m : 5.29660
[1mStep[0m  [40/42], [94mLoss[0m : 5.11149

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.293, [92mTest[0m: 5.330, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.59471
[1mStep[0m  [4/42], [94mLoss[0m : 5.39352
[1mStep[0m  [8/42], [94mLoss[0m : 4.95301
[1mStep[0m  [12/42], [94mLoss[0m : 5.39402
[1mStep[0m  [16/42], [94mLoss[0m : 5.28330
[1mStep[0m  [20/42], [94mLoss[0m : 5.14032
[1mStep[0m  [24/42], [94mLoss[0m : 5.31171
[1mStep[0m  [28/42], [94mLoss[0m : 5.38612
[1mStep[0m  [32/42], [94mLoss[0m : 5.46418
[1mStep[0m  [36/42], [94mLoss[0m : 5.40072
[1mStep[0m  [40/42], [94mLoss[0m : 5.28624

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.208, [92mTest[0m: 5.241, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.46276
[1mStep[0m  [4/42], [94mLoss[0m : 4.83843
[1mStep[0m  [8/42], [94mLoss[0m : 4.95299
[1mStep[0m  [12/42], [94mLoss[0m : 4.94245
[1mStep[0m  [16/42], [94mLoss[0m : 5.40775
[1mStep[0m  [20/42], [94mLoss[0m : 4.81161
[1mStep[0m  [24/42], [94mLoss[0m : 5.20730
[1mStep[0m  [28/42], [94mLoss[0m : 5.11592
[1mStep[0m  [32/42], [94mLoss[0m : 5.32959
[1mStep[0m  [36/42], [94mLoss[0m : 5.20250
[1mStep[0m  [40/42], [94mLoss[0m : 5.09812

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.108, [92mTest[0m: 5.134, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.15485
[1mStep[0m  [4/42], [94mLoss[0m : 5.04234
[1mStep[0m  [8/42], [94mLoss[0m : 5.21403
[1mStep[0m  [12/42], [94mLoss[0m : 5.00744
[1mStep[0m  [16/42], [94mLoss[0m : 4.82132
[1mStep[0m  [20/42], [94mLoss[0m : 5.25731
[1mStep[0m  [24/42], [94mLoss[0m : 5.02260
[1mStep[0m  [28/42], [94mLoss[0m : 5.07469
[1mStep[0m  [32/42], [94mLoss[0m : 4.73636
[1mStep[0m  [36/42], [94mLoss[0m : 5.00210
[1mStep[0m  [40/42], [94mLoss[0m : 5.01074

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.027, [92mTest[0m: 5.034, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.22202
[1mStep[0m  [4/42], [94mLoss[0m : 4.90576
[1mStep[0m  [8/42], [94mLoss[0m : 5.06189
[1mStep[0m  [12/42], [94mLoss[0m : 4.69128
[1mStep[0m  [16/42], [94mLoss[0m : 5.02691
[1mStep[0m  [20/42], [94mLoss[0m : 4.45046
[1mStep[0m  [24/42], [94mLoss[0m : 4.77771
[1mStep[0m  [28/42], [94mLoss[0m : 4.83459
[1mStep[0m  [32/42], [94mLoss[0m : 4.90649
[1mStep[0m  [36/42], [94mLoss[0m : 4.66920
[1mStep[0m  [40/42], [94mLoss[0m : 5.01969

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.918, [92mTest[0m: 4.936, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.84930
[1mStep[0m  [4/42], [94mLoss[0m : 4.94376
[1mStep[0m  [8/42], [94mLoss[0m : 5.14349
[1mStep[0m  [12/42], [94mLoss[0m : 4.92918
[1mStep[0m  [16/42], [94mLoss[0m : 4.48671
[1mStep[0m  [20/42], [94mLoss[0m : 4.78279
[1mStep[0m  [24/42], [94mLoss[0m : 4.61782
[1mStep[0m  [28/42], [94mLoss[0m : 4.69957
[1mStep[0m  [32/42], [94mLoss[0m : 4.82607
[1mStep[0m  [36/42], [94mLoss[0m : 4.72534
[1mStep[0m  [40/42], [94mLoss[0m : 4.75384

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.844, [92mTest[0m: 4.853, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.02995
[1mStep[0m  [4/42], [94mLoss[0m : 4.49782
[1mStep[0m  [8/42], [94mLoss[0m : 4.72108
[1mStep[0m  [12/42], [94mLoss[0m : 4.85090
[1mStep[0m  [16/42], [94mLoss[0m : 4.85111
[1mStep[0m  [20/42], [94mLoss[0m : 4.67209
[1mStep[0m  [24/42], [94mLoss[0m : 4.88960
[1mStep[0m  [28/42], [94mLoss[0m : 4.64688
[1mStep[0m  [32/42], [94mLoss[0m : 5.03835
[1mStep[0m  [36/42], [94mLoss[0m : 4.63236
[1mStep[0m  [40/42], [94mLoss[0m : 4.39350

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.747, [92mTest[0m: 4.768, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.66433
[1mStep[0m  [4/42], [94mLoss[0m : 4.70659
[1mStep[0m  [8/42], [94mLoss[0m : 4.94027
[1mStep[0m  [12/42], [94mLoss[0m : 4.77393
[1mStep[0m  [16/42], [94mLoss[0m : 4.90259
[1mStep[0m  [20/42], [94mLoss[0m : 4.62321
[1mStep[0m  [24/42], [94mLoss[0m : 5.00501
[1mStep[0m  [28/42], [94mLoss[0m : 4.84655
[1mStep[0m  [32/42], [94mLoss[0m : 4.84931
[1mStep[0m  [36/42], [94mLoss[0m : 4.72994
[1mStep[0m  [40/42], [94mLoss[0m : 4.34295

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.665, [92mTest[0m: 4.688, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.36635
[1mStep[0m  [4/42], [94mLoss[0m : 4.19763
[1mStep[0m  [8/42], [94mLoss[0m : 4.25990
[1mStep[0m  [12/42], [94mLoss[0m : 4.67426
[1mStep[0m  [16/42], [94mLoss[0m : 4.25718
[1mStep[0m  [20/42], [94mLoss[0m : 4.21802
[1mStep[0m  [24/42], [94mLoss[0m : 4.58754
[1mStep[0m  [28/42], [94mLoss[0m : 4.98534
[1mStep[0m  [32/42], [94mLoss[0m : 4.61383
[1mStep[0m  [36/42], [94mLoss[0m : 4.43494
[1mStep[0m  [40/42], [94mLoss[0m : 4.62556

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.568, [92mTest[0m: 4.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.43587
[1mStep[0m  [4/42], [94mLoss[0m : 4.68886
[1mStep[0m  [8/42], [94mLoss[0m : 4.20148
[1mStep[0m  [12/42], [94mLoss[0m : 4.57120
[1mStep[0m  [16/42], [94mLoss[0m : 4.40061
[1mStep[0m  [20/42], [94mLoss[0m : 4.63577
[1mStep[0m  [24/42], [94mLoss[0m : 4.37813
[1mStep[0m  [28/42], [94mLoss[0m : 4.50591
[1mStep[0m  [32/42], [94mLoss[0m : 4.71224
[1mStep[0m  [36/42], [94mLoss[0m : 4.18307
[1mStep[0m  [40/42], [94mLoss[0m : 4.38860

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.511, [92mTest[0m: 4.507, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.440
====================================

Phase 2 - Evaluation MAE:  4.439740589686802
MAE score P1      7.744776
MAE score P2      4.439741
loss              4.511404
learning_rate       0.0001
batch_size             256
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 12, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.70362
[1mStep[0m  [4/42], [94mLoss[0m : 10.86732
[1mStep[0m  [8/42], [94mLoss[0m : 10.66448
[1mStep[0m  [12/42], [94mLoss[0m : 10.64651
[1mStep[0m  [16/42], [94mLoss[0m : 11.09313
[1mStep[0m  [20/42], [94mLoss[0m : 10.74553
[1mStep[0m  [24/42], [94mLoss[0m : 10.46167
[1mStep[0m  [28/42], [94mLoss[0m : 10.62943
[1mStep[0m  [32/42], [94mLoss[0m : 10.85359
[1mStep[0m  [36/42], [94mLoss[0m : 10.19735
[1mStep[0m  [40/42], [94mLoss[0m : 10.30696

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.635, [92mTest[0m: 10.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25686
[1mStep[0m  [4/42], [94mLoss[0m : 10.23633
[1mStep[0m  [8/42], [94mLoss[0m : 10.06115
[1mStep[0m  [12/42], [94mLoss[0m : 10.67063
[1mStep[0m  [16/42], [94mLoss[0m : 10.61853
[1mStep[0m  [20/42], [94mLoss[0m : 10.37175
[1mStep[0m  [24/42], [94mLoss[0m : 10.36659
[1mStep[0m  [28/42], [94mLoss[0m : 9.84236
[1mStep[0m  [32/42], [94mLoss[0m : 10.41260
[1mStep[0m  [36/42], [94mLoss[0m : 9.97459
[1mStep[0m  [40/42], [94mLoss[0m : 10.47750

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.411, [92mTest[0m: 10.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19131
[1mStep[0m  [4/42], [94mLoss[0m : 10.02021
[1mStep[0m  [8/42], [94mLoss[0m : 10.43954
[1mStep[0m  [12/42], [94mLoss[0m : 10.26334
[1mStep[0m  [16/42], [94mLoss[0m : 9.83780
[1mStep[0m  [20/42], [94mLoss[0m : 10.19111
[1mStep[0m  [24/42], [94mLoss[0m : 10.16857
[1mStep[0m  [28/42], [94mLoss[0m : 10.05395
[1mStep[0m  [32/42], [94mLoss[0m : 10.58484
[1mStep[0m  [36/42], [94mLoss[0m : 9.98507
[1mStep[0m  [40/42], [94mLoss[0m : 10.16729

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.203, [92mTest[0m: 10.291, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15004
[1mStep[0m  [4/42], [94mLoss[0m : 9.95427
[1mStep[0m  [8/42], [94mLoss[0m : 9.97785
[1mStep[0m  [12/42], [94mLoss[0m : 9.86463
[1mStep[0m  [16/42], [94mLoss[0m : 10.15403
[1mStep[0m  [20/42], [94mLoss[0m : 10.02506
[1mStep[0m  [24/42], [94mLoss[0m : 9.89878
[1mStep[0m  [28/42], [94mLoss[0m : 9.84333
[1mStep[0m  [32/42], [94mLoss[0m : 10.26064
[1mStep[0m  [36/42], [94mLoss[0m : 10.01307
[1mStep[0m  [40/42], [94mLoss[0m : 9.77278

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.975, [92mTest[0m: 10.074, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.84024
[1mStep[0m  [4/42], [94mLoss[0m : 10.25835
[1mStep[0m  [8/42], [94mLoss[0m : 9.29584
[1mStep[0m  [12/42], [94mLoss[0m : 9.90820
[1mStep[0m  [16/42], [94mLoss[0m : 9.45738
[1mStep[0m  [20/42], [94mLoss[0m : 10.01615
[1mStep[0m  [24/42], [94mLoss[0m : 9.72988
[1mStep[0m  [28/42], [94mLoss[0m : 9.61229
[1mStep[0m  [32/42], [94mLoss[0m : 9.81342
[1mStep[0m  [36/42], [94mLoss[0m : 9.48398
[1mStep[0m  [40/42], [94mLoss[0m : 9.31665

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.766, [92mTest[0m: 9.863, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.94671
[1mStep[0m  [4/42], [94mLoss[0m : 9.50440
[1mStep[0m  [8/42], [94mLoss[0m : 9.87722
[1mStep[0m  [12/42], [94mLoss[0m : 9.69206
[1mStep[0m  [16/42], [94mLoss[0m : 9.48883
[1mStep[0m  [20/42], [94mLoss[0m : 9.78360
[1mStep[0m  [24/42], [94mLoss[0m : 9.07018
[1mStep[0m  [28/42], [94mLoss[0m : 9.51166
[1mStep[0m  [32/42], [94mLoss[0m : 9.25992
[1mStep[0m  [36/42], [94mLoss[0m : 9.62219
[1mStep[0m  [40/42], [94mLoss[0m : 9.42220

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.534, [92mTest[0m: 9.638, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.12436
[1mStep[0m  [4/42], [94mLoss[0m : 9.50463
[1mStep[0m  [8/42], [94mLoss[0m : 9.42703
[1mStep[0m  [12/42], [94mLoss[0m : 9.49959
[1mStep[0m  [16/42], [94mLoss[0m : 9.65842
[1mStep[0m  [20/42], [94mLoss[0m : 9.38778
[1mStep[0m  [24/42], [94mLoss[0m : 9.46527
[1mStep[0m  [28/42], [94mLoss[0m : 9.11146
[1mStep[0m  [32/42], [94mLoss[0m : 9.48073
[1mStep[0m  [36/42], [94mLoss[0m : 8.83499
[1mStep[0m  [40/42], [94mLoss[0m : 9.00376

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.324, [92mTest[0m: 9.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.10738
[1mStep[0m  [4/42], [94mLoss[0m : 9.03067
[1mStep[0m  [8/42], [94mLoss[0m : 9.31515
[1mStep[0m  [12/42], [94mLoss[0m : 9.24522
[1mStep[0m  [16/42], [94mLoss[0m : 9.20654
[1mStep[0m  [20/42], [94mLoss[0m : 9.20635
[1mStep[0m  [24/42], [94mLoss[0m : 9.08099
[1mStep[0m  [28/42], [94mLoss[0m : 9.33452
[1mStep[0m  [32/42], [94mLoss[0m : 9.10979
[1mStep[0m  [36/42], [94mLoss[0m : 8.83201
[1mStep[0m  [40/42], [94mLoss[0m : 8.47313

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.102, [92mTest[0m: 9.194, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.75226
[1mStep[0m  [4/42], [94mLoss[0m : 8.87842
[1mStep[0m  [8/42], [94mLoss[0m : 9.40976
[1mStep[0m  [12/42], [94mLoss[0m : 8.80069
[1mStep[0m  [16/42], [94mLoss[0m : 9.07357
[1mStep[0m  [20/42], [94mLoss[0m : 8.72492
[1mStep[0m  [24/42], [94mLoss[0m : 9.11963
[1mStep[0m  [28/42], [94mLoss[0m : 8.87007
[1mStep[0m  [32/42], [94mLoss[0m : 8.98282
[1mStep[0m  [36/42], [94mLoss[0m : 8.67534
[1mStep[0m  [40/42], [94mLoss[0m : 8.92816

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.881, [92mTest[0m: 8.976, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.01840
[1mStep[0m  [4/42], [94mLoss[0m : 8.61977
[1mStep[0m  [8/42], [94mLoss[0m : 8.90911
[1mStep[0m  [12/42], [94mLoss[0m : 8.71046
[1mStep[0m  [16/42], [94mLoss[0m : 8.46084
[1mStep[0m  [20/42], [94mLoss[0m : 8.54516
[1mStep[0m  [24/42], [94mLoss[0m : 8.38768
[1mStep[0m  [28/42], [94mLoss[0m : 8.32253
[1mStep[0m  [32/42], [94mLoss[0m : 8.37301
[1mStep[0m  [36/42], [94mLoss[0m : 8.33900
[1mStep[0m  [40/42], [94mLoss[0m : 8.45210

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.668, [92mTest[0m: 8.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.67859
[1mStep[0m  [4/42], [94mLoss[0m : 8.42848
[1mStep[0m  [8/42], [94mLoss[0m : 8.58262
[1mStep[0m  [12/42], [94mLoss[0m : 8.68397
[1mStep[0m  [16/42], [94mLoss[0m : 8.82903
[1mStep[0m  [20/42], [94mLoss[0m : 8.09338
[1mStep[0m  [24/42], [94mLoss[0m : 8.37369
[1mStep[0m  [28/42], [94mLoss[0m : 8.52837
[1mStep[0m  [32/42], [94mLoss[0m : 8.39743
[1mStep[0m  [36/42], [94mLoss[0m : 8.32854
[1mStep[0m  [40/42], [94mLoss[0m : 8.11949

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.449, [92mTest[0m: 8.545, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.17332
[1mStep[0m  [4/42], [94mLoss[0m : 8.10131
[1mStep[0m  [8/42], [94mLoss[0m : 8.38413
[1mStep[0m  [12/42], [94mLoss[0m : 7.97705
[1mStep[0m  [16/42], [94mLoss[0m : 8.01221
[1mStep[0m  [20/42], [94mLoss[0m : 8.07137
[1mStep[0m  [24/42], [94mLoss[0m : 8.55279
[1mStep[0m  [28/42], [94mLoss[0m : 8.68739
[1mStep[0m  [32/42], [94mLoss[0m : 8.51985
[1mStep[0m  [36/42], [94mLoss[0m : 8.47406
[1mStep[0m  [40/42], [94mLoss[0m : 8.32639

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.231, [92mTest[0m: 8.340, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.04363
[1mStep[0m  [4/42], [94mLoss[0m : 8.29091
[1mStep[0m  [8/42], [94mLoss[0m : 8.31560
[1mStep[0m  [12/42], [94mLoss[0m : 7.98669
[1mStep[0m  [16/42], [94mLoss[0m : 7.83116
[1mStep[0m  [20/42], [94mLoss[0m : 7.54006
[1mStep[0m  [24/42], [94mLoss[0m : 8.45023
[1mStep[0m  [28/42], [94mLoss[0m : 7.76711
[1mStep[0m  [32/42], [94mLoss[0m : 7.87157
[1mStep[0m  [36/42], [94mLoss[0m : 8.18002
[1mStep[0m  [40/42], [94mLoss[0m : 7.69064

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.011, [92mTest[0m: 8.119, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.43261
[1mStep[0m  [4/42], [94mLoss[0m : 7.88990
[1mStep[0m  [8/42], [94mLoss[0m : 8.08521
[1mStep[0m  [12/42], [94mLoss[0m : 7.85171
[1mStep[0m  [16/42], [94mLoss[0m : 7.61696
[1mStep[0m  [20/42], [94mLoss[0m : 8.16706
[1mStep[0m  [24/42], [94mLoss[0m : 7.61313
[1mStep[0m  [28/42], [94mLoss[0m : 7.85370
[1mStep[0m  [32/42], [94mLoss[0m : 7.59499
[1mStep[0m  [36/42], [94mLoss[0m : 7.40084
[1mStep[0m  [40/42], [94mLoss[0m : 8.13714

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.787, [92mTest[0m: 7.891, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.50374
[1mStep[0m  [4/42], [94mLoss[0m : 7.76480
[1mStep[0m  [8/42], [94mLoss[0m : 7.80451
[1mStep[0m  [12/42], [94mLoss[0m : 6.96393
[1mStep[0m  [16/42], [94mLoss[0m : 7.45796
[1mStep[0m  [20/42], [94mLoss[0m : 7.41721
[1mStep[0m  [24/42], [94mLoss[0m : 7.79983
[1mStep[0m  [28/42], [94mLoss[0m : 7.37640
[1mStep[0m  [32/42], [94mLoss[0m : 7.42061
[1mStep[0m  [36/42], [94mLoss[0m : 7.88688
[1mStep[0m  [40/42], [94mLoss[0m : 7.42282

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.563, [92mTest[0m: 7.681, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.56736
[1mStep[0m  [4/42], [94mLoss[0m : 7.12462
[1mStep[0m  [8/42], [94mLoss[0m : 7.31727
[1mStep[0m  [12/42], [94mLoss[0m : 7.25973
[1mStep[0m  [16/42], [94mLoss[0m : 7.43093
[1mStep[0m  [20/42], [94mLoss[0m : 7.13840
[1mStep[0m  [24/42], [94mLoss[0m : 7.23663
[1mStep[0m  [28/42], [94mLoss[0m : 7.35194
[1mStep[0m  [32/42], [94mLoss[0m : 7.61882
[1mStep[0m  [36/42], [94mLoss[0m : 7.02460
[1mStep[0m  [40/42], [94mLoss[0m : 7.38482

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.348, [92mTest[0m: 7.460, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.31990
[1mStep[0m  [4/42], [94mLoss[0m : 7.30251
[1mStep[0m  [8/42], [94mLoss[0m : 7.22565
[1mStep[0m  [12/42], [94mLoss[0m : 7.22118
[1mStep[0m  [16/42], [94mLoss[0m : 7.13806
[1mStep[0m  [20/42], [94mLoss[0m : 7.26551
[1mStep[0m  [24/42], [94mLoss[0m : 7.12142
[1mStep[0m  [28/42], [94mLoss[0m : 6.92406
[1mStep[0m  [32/42], [94mLoss[0m : 6.93804
[1mStep[0m  [36/42], [94mLoss[0m : 7.14753
[1mStep[0m  [40/42], [94mLoss[0m : 7.05601

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.132, [92mTest[0m: 7.244, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.76835
[1mStep[0m  [4/42], [94mLoss[0m : 7.13658
[1mStep[0m  [8/42], [94mLoss[0m : 6.84480
[1mStep[0m  [12/42], [94mLoss[0m : 7.28168
[1mStep[0m  [16/42], [94mLoss[0m : 6.66107
[1mStep[0m  [20/42], [94mLoss[0m : 6.85631
[1mStep[0m  [24/42], [94mLoss[0m : 6.66068
[1mStep[0m  [28/42], [94mLoss[0m : 6.96982
[1mStep[0m  [32/42], [94mLoss[0m : 6.57901
[1mStep[0m  [36/42], [94mLoss[0m : 6.73151
[1mStep[0m  [40/42], [94mLoss[0m : 6.48959

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.918, [92mTest[0m: 7.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.66531
[1mStep[0m  [4/42], [94mLoss[0m : 6.56410
[1mStep[0m  [8/42], [94mLoss[0m : 6.61040
[1mStep[0m  [12/42], [94mLoss[0m : 6.65008
[1mStep[0m  [16/42], [94mLoss[0m : 6.83866
[1mStep[0m  [20/42], [94mLoss[0m : 7.10472
[1mStep[0m  [24/42], [94mLoss[0m : 6.51264
[1mStep[0m  [28/42], [94mLoss[0m : 6.85506
[1mStep[0m  [32/42], [94mLoss[0m : 6.59197
[1mStep[0m  [36/42], [94mLoss[0m : 5.84276
[1mStep[0m  [40/42], [94mLoss[0m : 6.77633

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 6.701, [92mTest[0m: 6.787, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.63329
[1mStep[0m  [4/42], [94mLoss[0m : 6.65799
[1mStep[0m  [8/42], [94mLoss[0m : 6.19953
[1mStep[0m  [12/42], [94mLoss[0m : 7.02629
[1mStep[0m  [16/42], [94mLoss[0m : 6.40185
[1mStep[0m  [20/42], [94mLoss[0m : 6.71563
[1mStep[0m  [24/42], [94mLoss[0m : 6.71319
[1mStep[0m  [28/42], [94mLoss[0m : 6.20570
[1mStep[0m  [32/42], [94mLoss[0m : 6.26954
[1mStep[0m  [36/42], [94mLoss[0m : 6.00666
[1mStep[0m  [40/42], [94mLoss[0m : 6.23896

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 6.482, [92mTest[0m: 6.580, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.66614
[1mStep[0m  [4/42], [94mLoss[0m : 6.33619
[1mStep[0m  [8/42], [94mLoss[0m : 6.14886
[1mStep[0m  [12/42], [94mLoss[0m : 6.28075
[1mStep[0m  [16/42], [94mLoss[0m : 6.45684
[1mStep[0m  [20/42], [94mLoss[0m : 6.14123
[1mStep[0m  [24/42], [94mLoss[0m : 6.20492
[1mStep[0m  [28/42], [94mLoss[0m : 6.66956
[1mStep[0m  [32/42], [94mLoss[0m : 6.27515
[1mStep[0m  [36/42], [94mLoss[0m : 6.16465
[1mStep[0m  [40/42], [94mLoss[0m : 6.03034

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 6.273, [92mTest[0m: 6.354, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.06388
[1mStep[0m  [4/42], [94mLoss[0m : 6.52797
[1mStep[0m  [8/42], [94mLoss[0m : 6.11139
[1mStep[0m  [12/42], [94mLoss[0m : 6.10047
[1mStep[0m  [16/42], [94mLoss[0m : 6.02293
[1mStep[0m  [20/42], [94mLoss[0m : 6.26110
[1mStep[0m  [24/42], [94mLoss[0m : 6.17252
[1mStep[0m  [28/42], [94mLoss[0m : 6.07816
[1mStep[0m  [32/42], [94mLoss[0m : 6.34923
[1mStep[0m  [36/42], [94mLoss[0m : 5.53020
[1mStep[0m  [40/42], [94mLoss[0m : 6.17503

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.077, [92mTest[0m: 6.161, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.17305
[1mStep[0m  [4/42], [94mLoss[0m : 5.74908
[1mStep[0m  [8/42], [94mLoss[0m : 5.81557
[1mStep[0m  [12/42], [94mLoss[0m : 6.00117
[1mStep[0m  [16/42], [94mLoss[0m : 6.19804
[1mStep[0m  [20/42], [94mLoss[0m : 5.32129
[1mStep[0m  [24/42], [94mLoss[0m : 5.47913
[1mStep[0m  [28/42], [94mLoss[0m : 5.81647
[1mStep[0m  [32/42], [94mLoss[0m : 5.95315
[1mStep[0m  [36/42], [94mLoss[0m : 5.69998
[1mStep[0m  [40/42], [94mLoss[0m : 5.65844

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.881, [92mTest[0m: 5.974, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.87368
[1mStep[0m  [4/42], [94mLoss[0m : 5.63003
[1mStep[0m  [8/42], [94mLoss[0m : 5.29434
[1mStep[0m  [12/42], [94mLoss[0m : 5.54585
[1mStep[0m  [16/42], [94mLoss[0m : 5.70635
[1mStep[0m  [20/42], [94mLoss[0m : 5.58037
[1mStep[0m  [24/42], [94mLoss[0m : 5.55183
[1mStep[0m  [28/42], [94mLoss[0m : 5.13039
[1mStep[0m  [32/42], [94mLoss[0m : 5.62985
[1mStep[0m  [36/42], [94mLoss[0m : 5.57617
[1mStep[0m  [40/42], [94mLoss[0m : 5.47650

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.679, [92mTest[0m: 5.773, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.36067
[1mStep[0m  [4/42], [94mLoss[0m : 5.21662
[1mStep[0m  [8/42], [94mLoss[0m : 5.53838
[1mStep[0m  [12/42], [94mLoss[0m : 5.82199
[1mStep[0m  [16/42], [94mLoss[0m : 5.42260
[1mStep[0m  [20/42], [94mLoss[0m : 5.46631
[1mStep[0m  [24/42], [94mLoss[0m : 5.45807
[1mStep[0m  [28/42], [94mLoss[0m : 5.64414
[1mStep[0m  [32/42], [94mLoss[0m : 5.39781
[1mStep[0m  [36/42], [94mLoss[0m : 5.31334
[1mStep[0m  [40/42], [94mLoss[0m : 5.32360

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.499, [92mTest[0m: 5.595, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.54659
[1mStep[0m  [4/42], [94mLoss[0m : 5.42952
[1mStep[0m  [8/42], [94mLoss[0m : 5.14554
[1mStep[0m  [12/42], [94mLoss[0m : 5.58127
[1mStep[0m  [16/42], [94mLoss[0m : 5.62921
[1mStep[0m  [20/42], [94mLoss[0m : 5.21841
[1mStep[0m  [24/42], [94mLoss[0m : 5.47160
[1mStep[0m  [28/42], [94mLoss[0m : 5.24013
[1mStep[0m  [32/42], [94mLoss[0m : 4.90059
[1mStep[0m  [36/42], [94mLoss[0m : 5.50850
[1mStep[0m  [40/42], [94mLoss[0m : 5.20306

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.324, [92mTest[0m: 5.412, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.30016
[1mStep[0m  [4/42], [94mLoss[0m : 5.37068
[1mStep[0m  [8/42], [94mLoss[0m : 5.61805
[1mStep[0m  [12/42], [94mLoss[0m : 5.40181
[1mStep[0m  [16/42], [94mLoss[0m : 5.50414
[1mStep[0m  [20/42], [94mLoss[0m : 5.19455
[1mStep[0m  [24/42], [94mLoss[0m : 5.21759
[1mStep[0m  [28/42], [94mLoss[0m : 5.66514
[1mStep[0m  [32/42], [94mLoss[0m : 4.93073
[1mStep[0m  [36/42], [94mLoss[0m : 5.08808
[1mStep[0m  [40/42], [94mLoss[0m : 5.28050

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.153, [92mTest[0m: 5.245, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.92894
[1mStep[0m  [4/42], [94mLoss[0m : 5.23966
[1mStep[0m  [8/42], [94mLoss[0m : 5.05436
[1mStep[0m  [12/42], [94mLoss[0m : 5.15728
[1mStep[0m  [16/42], [94mLoss[0m : 5.39687
[1mStep[0m  [20/42], [94mLoss[0m : 4.86774
[1mStep[0m  [24/42], [94mLoss[0m : 4.89309
[1mStep[0m  [28/42], [94mLoss[0m : 5.13133
[1mStep[0m  [32/42], [94mLoss[0m : 5.15284
[1mStep[0m  [36/42], [94mLoss[0m : 5.40718
[1mStep[0m  [40/42], [94mLoss[0m : 4.75520

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.986, [92mTest[0m: 5.061, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.80874
[1mStep[0m  [4/42], [94mLoss[0m : 5.30954
[1mStep[0m  [8/42], [94mLoss[0m : 4.76361
[1mStep[0m  [12/42], [94mLoss[0m : 4.88327
[1mStep[0m  [16/42], [94mLoss[0m : 4.84109
[1mStep[0m  [20/42], [94mLoss[0m : 4.69034
[1mStep[0m  [24/42], [94mLoss[0m : 4.33017
[1mStep[0m  [28/42], [94mLoss[0m : 4.82877
[1mStep[0m  [32/42], [94mLoss[0m : 4.98952
[1mStep[0m  [36/42], [94mLoss[0m : 5.02237
[1mStep[0m  [40/42], [94mLoss[0m : 5.10006

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.832, [92mTest[0m: 4.900, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.75579
[1mStep[0m  [4/42], [94mLoss[0m : 4.88306
[1mStep[0m  [8/42], [94mLoss[0m : 4.83162
[1mStep[0m  [12/42], [94mLoss[0m : 4.14521
[1mStep[0m  [16/42], [94mLoss[0m : 4.99705
[1mStep[0m  [20/42], [94mLoss[0m : 4.57194
[1mStep[0m  [24/42], [94mLoss[0m : 4.72344
[1mStep[0m  [28/42], [94mLoss[0m : 4.46562
[1mStep[0m  [32/42], [94mLoss[0m : 4.67234
[1mStep[0m  [36/42], [94mLoss[0m : 4.65865
[1mStep[0m  [40/42], [94mLoss[0m : 4.87292

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.691, [92mTest[0m: 4.755, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.616
====================================

Phase 1 - Evaluation MAE:  4.615614175796509
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 4.33007
[1mStep[0m  [4/42], [94mLoss[0m : 4.29354
[1mStep[0m  [8/42], [94mLoss[0m : 4.58637
[1mStep[0m  [12/42], [94mLoss[0m : 4.76349
[1mStep[0m  [16/42], [94mLoss[0m : 4.51473
[1mStep[0m  [20/42], [94mLoss[0m : 4.48942
[1mStep[0m  [24/42], [94mLoss[0m : 4.65418
[1mStep[0m  [28/42], [94mLoss[0m : 4.86385
[1mStep[0m  [32/42], [94mLoss[0m : 4.70357
[1mStep[0m  [36/42], [94mLoss[0m : 4.77822
[1mStep[0m  [40/42], [94mLoss[0m : 4.72770

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.541, [92mTest[0m: 4.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.31123
[1mStep[0m  [4/42], [94mLoss[0m : 4.71013
[1mStep[0m  [8/42], [94mLoss[0m : 4.22466
[1mStep[0m  [12/42], [94mLoss[0m : 4.43863
[1mStep[0m  [16/42], [94mLoss[0m : 4.56569
[1mStep[0m  [20/42], [94mLoss[0m : 4.02501
[1mStep[0m  [24/42], [94mLoss[0m : 4.40024
[1mStep[0m  [28/42], [94mLoss[0m : 4.37454
[1mStep[0m  [32/42], [94mLoss[0m : 4.48778
[1mStep[0m  [36/42], [94mLoss[0m : 4.82434
[1mStep[0m  [40/42], [94mLoss[0m : 4.60943

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.396, [92mTest[0m: 4.451, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.11563
[1mStep[0m  [4/42], [94mLoss[0m : 4.57123
[1mStep[0m  [8/42], [94mLoss[0m : 3.90586
[1mStep[0m  [12/42], [94mLoss[0m : 4.41328
[1mStep[0m  [16/42], [94mLoss[0m : 4.27581
[1mStep[0m  [20/42], [94mLoss[0m : 4.50012
[1mStep[0m  [24/42], [94mLoss[0m : 4.19761
[1mStep[0m  [28/42], [94mLoss[0m : 4.29121
[1mStep[0m  [32/42], [94mLoss[0m : 4.18896
[1mStep[0m  [36/42], [94mLoss[0m : 4.18411
[1mStep[0m  [40/42], [94mLoss[0m : 4.20475

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.252, [92mTest[0m: 4.312, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.51532
[1mStep[0m  [4/42], [94mLoss[0m : 4.01817
[1mStep[0m  [8/42], [94mLoss[0m : 4.48580
[1mStep[0m  [12/42], [94mLoss[0m : 4.41179
[1mStep[0m  [16/42], [94mLoss[0m : 3.74740
[1mStep[0m  [20/42], [94mLoss[0m : 4.13620
[1mStep[0m  [24/42], [94mLoss[0m : 3.74449
[1mStep[0m  [28/42], [94mLoss[0m : 3.79031
[1mStep[0m  [32/42], [94mLoss[0m : 4.27842
[1mStep[0m  [36/42], [94mLoss[0m : 4.11057
[1mStep[0m  [40/42], [94mLoss[0m : 4.02767

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 4.138, [92mTest[0m: 4.197, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.96635
[1mStep[0m  [4/42], [94mLoss[0m : 4.23044
[1mStep[0m  [8/42], [94mLoss[0m : 4.33983
[1mStep[0m  [12/42], [94mLoss[0m : 4.34893
[1mStep[0m  [16/42], [94mLoss[0m : 3.99521
[1mStep[0m  [20/42], [94mLoss[0m : 4.06332
[1mStep[0m  [24/42], [94mLoss[0m : 4.45846
[1mStep[0m  [28/42], [94mLoss[0m : 3.90825
[1mStep[0m  [32/42], [94mLoss[0m : 4.27297
[1mStep[0m  [36/42], [94mLoss[0m : 3.58758
[1mStep[0m  [40/42], [94mLoss[0m : 4.29236

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 4.025, [92mTest[0m: 4.062, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.67365
[1mStep[0m  [4/42], [94mLoss[0m : 4.01567
[1mStep[0m  [8/42], [94mLoss[0m : 3.86731
[1mStep[0m  [12/42], [94mLoss[0m : 3.85002
[1mStep[0m  [16/42], [94mLoss[0m : 4.36828
[1mStep[0m  [20/42], [94mLoss[0m : 4.15565
[1mStep[0m  [24/42], [94mLoss[0m : 4.09792
[1mStep[0m  [28/42], [94mLoss[0m : 3.92913
[1mStep[0m  [32/42], [94mLoss[0m : 3.85679
[1mStep[0m  [36/42], [94mLoss[0m : 4.37113
[1mStep[0m  [40/42], [94mLoss[0m : 4.11860

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.914, [92mTest[0m: 3.964, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.01563
[1mStep[0m  [4/42], [94mLoss[0m : 3.98086
[1mStep[0m  [8/42], [94mLoss[0m : 3.64002
[1mStep[0m  [12/42], [94mLoss[0m : 3.70547
[1mStep[0m  [16/42], [94mLoss[0m : 3.42791
[1mStep[0m  [20/42], [94mLoss[0m : 3.96874
[1mStep[0m  [24/42], [94mLoss[0m : 3.33121
[1mStep[0m  [28/42], [94mLoss[0m : 3.89656
[1mStep[0m  [32/42], [94mLoss[0m : 3.94337
[1mStep[0m  [36/42], [94mLoss[0m : 3.96391
[1mStep[0m  [40/42], [94mLoss[0m : 3.97374

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.819, [92mTest[0m: 3.849, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.64192
[1mStep[0m  [4/42], [94mLoss[0m : 3.83278
[1mStep[0m  [8/42], [94mLoss[0m : 4.03281
[1mStep[0m  [12/42], [94mLoss[0m : 3.33572
[1mStep[0m  [16/42], [94mLoss[0m : 3.94826
[1mStep[0m  [20/42], [94mLoss[0m : 3.85216
[1mStep[0m  [24/42], [94mLoss[0m : 3.64690
[1mStep[0m  [28/42], [94mLoss[0m : 3.88829
[1mStep[0m  [32/42], [94mLoss[0m : 3.67589
[1mStep[0m  [36/42], [94mLoss[0m : 3.95804
[1mStep[0m  [40/42], [94mLoss[0m : 3.65016

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.736, [92mTest[0m: 3.736, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.64292
[1mStep[0m  [4/42], [94mLoss[0m : 3.68984
[1mStep[0m  [8/42], [94mLoss[0m : 3.81869
[1mStep[0m  [12/42], [94mLoss[0m : 3.63866
[1mStep[0m  [16/42], [94mLoss[0m : 3.66346
[1mStep[0m  [20/42], [94mLoss[0m : 3.69724
[1mStep[0m  [24/42], [94mLoss[0m : 3.64867
[1mStep[0m  [28/42], [94mLoss[0m : 3.79195
[1mStep[0m  [32/42], [94mLoss[0m : 3.86275
[1mStep[0m  [36/42], [94mLoss[0m : 3.71312
[1mStep[0m  [40/42], [94mLoss[0m : 3.11558

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.658, [92mTest[0m: 3.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.81476
[1mStep[0m  [4/42], [94mLoss[0m : 3.74063
[1mStep[0m  [8/42], [94mLoss[0m : 3.46462
[1mStep[0m  [12/42], [94mLoss[0m : 3.49040
[1mStep[0m  [16/42], [94mLoss[0m : 3.70769
[1mStep[0m  [20/42], [94mLoss[0m : 3.69224
[1mStep[0m  [24/42], [94mLoss[0m : 3.59657
[1mStep[0m  [28/42], [94mLoss[0m : 3.86365
[1mStep[0m  [32/42], [94mLoss[0m : 3.35957
[1mStep[0m  [36/42], [94mLoss[0m : 3.91737
[1mStep[0m  [40/42], [94mLoss[0m : 3.48762

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.566, [92mTest[0m: 3.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15866
[1mStep[0m  [4/42], [94mLoss[0m : 3.53286
[1mStep[0m  [8/42], [94mLoss[0m : 3.66917
[1mStep[0m  [12/42], [94mLoss[0m : 3.17168
[1mStep[0m  [16/42], [94mLoss[0m : 3.36024
[1mStep[0m  [20/42], [94mLoss[0m : 3.37617
[1mStep[0m  [24/42], [94mLoss[0m : 3.25895
[1mStep[0m  [28/42], [94mLoss[0m : 3.36761
[1mStep[0m  [32/42], [94mLoss[0m : 3.66008
[1mStep[0m  [36/42], [94mLoss[0m : 3.49059
[1mStep[0m  [40/42], [94mLoss[0m : 3.72471

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.520, [92mTest[0m: 3.496, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.61088
[1mStep[0m  [4/42], [94mLoss[0m : 3.68159
[1mStep[0m  [8/42], [94mLoss[0m : 3.47858
[1mStep[0m  [12/42], [94mLoss[0m : 3.16702
[1mStep[0m  [16/42], [94mLoss[0m : 3.11212
[1mStep[0m  [20/42], [94mLoss[0m : 3.63498
[1mStep[0m  [24/42], [94mLoss[0m : 3.61102
[1mStep[0m  [28/42], [94mLoss[0m : 3.67332
[1mStep[0m  [32/42], [94mLoss[0m : 3.53464
[1mStep[0m  [36/42], [94mLoss[0m : 3.45432
[1mStep[0m  [40/42], [94mLoss[0m : 3.45987

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.441, [92mTest[0m: 3.419, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.49359
[1mStep[0m  [4/42], [94mLoss[0m : 3.16873
[1mStep[0m  [8/42], [94mLoss[0m : 3.31172
[1mStep[0m  [12/42], [94mLoss[0m : 3.66566
[1mStep[0m  [16/42], [94mLoss[0m : 3.27140
[1mStep[0m  [20/42], [94mLoss[0m : 3.23422
[1mStep[0m  [24/42], [94mLoss[0m : 3.46392
[1mStep[0m  [28/42], [94mLoss[0m : 3.29971
[1mStep[0m  [32/42], [94mLoss[0m : 3.58135
[1mStep[0m  [36/42], [94mLoss[0m : 3.33636
[1mStep[0m  [40/42], [94mLoss[0m : 3.56463

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.376, [92mTest[0m: 3.353, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.10437
[1mStep[0m  [4/42], [94mLoss[0m : 3.53002
[1mStep[0m  [8/42], [94mLoss[0m : 3.17278
[1mStep[0m  [12/42], [94mLoss[0m : 3.48783
[1mStep[0m  [16/42], [94mLoss[0m : 3.33403
[1mStep[0m  [20/42], [94mLoss[0m : 3.29416
[1mStep[0m  [24/42], [94mLoss[0m : 3.23128
[1mStep[0m  [28/42], [94mLoss[0m : 3.19336
[1mStep[0m  [32/42], [94mLoss[0m : 3.16525
[1mStep[0m  [36/42], [94mLoss[0m : 3.40393
[1mStep[0m  [40/42], [94mLoss[0m : 3.11525

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.332, [92mTest[0m: 3.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.02630
[1mStep[0m  [4/42], [94mLoss[0m : 3.43749
[1mStep[0m  [8/42], [94mLoss[0m : 3.49867
[1mStep[0m  [12/42], [94mLoss[0m : 3.49411
[1mStep[0m  [16/42], [94mLoss[0m : 3.18731
[1mStep[0m  [20/42], [94mLoss[0m : 3.50883
[1mStep[0m  [24/42], [94mLoss[0m : 3.19264
[1mStep[0m  [28/42], [94mLoss[0m : 3.21823
[1mStep[0m  [32/42], [94mLoss[0m : 3.24881
[1mStep[0m  [36/42], [94mLoss[0m : 3.03982
[1mStep[0m  [40/42], [94mLoss[0m : 3.23311

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.283, [92mTest[0m: 3.242, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.16632
[1mStep[0m  [4/42], [94mLoss[0m : 3.35585
[1mStep[0m  [8/42], [94mLoss[0m : 3.17924
[1mStep[0m  [12/42], [94mLoss[0m : 3.42538
[1mStep[0m  [16/42], [94mLoss[0m : 3.20655
[1mStep[0m  [20/42], [94mLoss[0m : 3.27022
[1mStep[0m  [24/42], [94mLoss[0m : 3.13848
[1mStep[0m  [28/42], [94mLoss[0m : 3.18769
[1mStep[0m  [32/42], [94mLoss[0m : 3.27223
[1mStep[0m  [36/42], [94mLoss[0m : 3.26021
[1mStep[0m  [40/42], [94mLoss[0m : 3.23056

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.234, [92mTest[0m: 3.182, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92002
[1mStep[0m  [4/42], [94mLoss[0m : 3.01994
[1mStep[0m  [8/42], [94mLoss[0m : 3.28060
[1mStep[0m  [12/42], [94mLoss[0m : 3.20373
[1mStep[0m  [16/42], [94mLoss[0m : 3.01231
[1mStep[0m  [20/42], [94mLoss[0m : 3.17491
[1mStep[0m  [24/42], [94mLoss[0m : 3.12771
[1mStep[0m  [28/42], [94mLoss[0m : 3.19437
[1mStep[0m  [32/42], [94mLoss[0m : 3.26576
[1mStep[0m  [36/42], [94mLoss[0m : 3.18651
[1mStep[0m  [40/42], [94mLoss[0m : 3.27431

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.194, [92mTest[0m: 3.129, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.26227
[1mStep[0m  [4/42], [94mLoss[0m : 3.04704
[1mStep[0m  [8/42], [94mLoss[0m : 3.05962
[1mStep[0m  [12/42], [94mLoss[0m : 3.26606
[1mStep[0m  [16/42], [94mLoss[0m : 3.22041
[1mStep[0m  [20/42], [94mLoss[0m : 3.14066
[1mStep[0m  [24/42], [94mLoss[0m : 3.29852
[1mStep[0m  [28/42], [94mLoss[0m : 2.96770
[1mStep[0m  [32/42], [94mLoss[0m : 2.91155
[1mStep[0m  [36/42], [94mLoss[0m : 3.18521
[1mStep[0m  [40/42], [94mLoss[0m : 3.30856

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.157, [92mTest[0m: 3.094, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.13269
[1mStep[0m  [4/42], [94mLoss[0m : 3.47358
[1mStep[0m  [8/42], [94mLoss[0m : 3.07410
[1mStep[0m  [12/42], [94mLoss[0m : 3.12040
[1mStep[0m  [16/42], [94mLoss[0m : 2.86115
[1mStep[0m  [20/42], [94mLoss[0m : 2.93242
[1mStep[0m  [24/42], [94mLoss[0m : 2.96142
[1mStep[0m  [28/42], [94mLoss[0m : 2.80624
[1mStep[0m  [32/42], [94mLoss[0m : 2.99203
[1mStep[0m  [36/42], [94mLoss[0m : 3.02024
[1mStep[0m  [40/42], [94mLoss[0m : 2.91995

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.104, [92mTest[0m: 3.048, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.34878
[1mStep[0m  [4/42], [94mLoss[0m : 2.86768
[1mStep[0m  [8/42], [94mLoss[0m : 2.92435
[1mStep[0m  [12/42], [94mLoss[0m : 2.89811
[1mStep[0m  [16/42], [94mLoss[0m : 2.91912
[1mStep[0m  [20/42], [94mLoss[0m : 3.14923
[1mStep[0m  [24/42], [94mLoss[0m : 3.14569
[1mStep[0m  [28/42], [94mLoss[0m : 2.86633
[1mStep[0m  [32/42], [94mLoss[0m : 3.08114
[1mStep[0m  [36/42], [94mLoss[0m : 3.20674
[1mStep[0m  [40/42], [94mLoss[0m : 3.17651

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.090, [92mTest[0m: 3.015, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.87376
[1mStep[0m  [4/42], [94mLoss[0m : 3.09783
[1mStep[0m  [8/42], [94mLoss[0m : 3.27339
[1mStep[0m  [12/42], [94mLoss[0m : 3.01943
[1mStep[0m  [16/42], [94mLoss[0m : 2.81009
[1mStep[0m  [20/42], [94mLoss[0m : 3.08415
[1mStep[0m  [24/42], [94mLoss[0m : 2.86473
[1mStep[0m  [28/42], [94mLoss[0m : 3.14228
[1mStep[0m  [32/42], [94mLoss[0m : 3.09245
[1mStep[0m  [36/42], [94mLoss[0m : 2.91481
[1mStep[0m  [40/42], [94mLoss[0m : 3.14743

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.050, [92mTest[0m: 2.975, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.08271
[1mStep[0m  [4/42], [94mLoss[0m : 2.87845
[1mStep[0m  [8/42], [94mLoss[0m : 2.96591
[1mStep[0m  [12/42], [94mLoss[0m : 3.29221
[1mStep[0m  [16/42], [94mLoss[0m : 2.88668
[1mStep[0m  [20/42], [94mLoss[0m : 2.94721
[1mStep[0m  [24/42], [94mLoss[0m : 3.06855
[1mStep[0m  [28/42], [94mLoss[0m : 3.13793
[1mStep[0m  [32/42], [94mLoss[0m : 3.19055
[1mStep[0m  [36/42], [94mLoss[0m : 2.86811
[1mStep[0m  [40/42], [94mLoss[0m : 2.97463

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.028, [92mTest[0m: 2.939, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.16134
[1mStep[0m  [4/42], [94mLoss[0m : 3.13986
[1mStep[0m  [8/42], [94mLoss[0m : 3.19883
[1mStep[0m  [12/42], [94mLoss[0m : 2.76886
[1mStep[0m  [16/42], [94mLoss[0m : 3.06567
[1mStep[0m  [20/42], [94mLoss[0m : 3.30198
[1mStep[0m  [24/42], [94mLoss[0m : 2.80696
[1mStep[0m  [28/42], [94mLoss[0m : 3.05364
[1mStep[0m  [32/42], [94mLoss[0m : 3.01937
[1mStep[0m  [36/42], [94mLoss[0m : 3.00763
[1mStep[0m  [40/42], [94mLoss[0m : 2.95204

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.000, [92mTest[0m: 2.920, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.07224
[1mStep[0m  [4/42], [94mLoss[0m : 2.95309
[1mStep[0m  [8/42], [94mLoss[0m : 3.36837
[1mStep[0m  [12/42], [94mLoss[0m : 3.04578
[1mStep[0m  [16/42], [94mLoss[0m : 2.95649
[1mStep[0m  [20/42], [94mLoss[0m : 2.93575
[1mStep[0m  [24/42], [94mLoss[0m : 2.94918
[1mStep[0m  [28/42], [94mLoss[0m : 2.88073
[1mStep[0m  [32/42], [94mLoss[0m : 3.10988
[1mStep[0m  [36/42], [94mLoss[0m : 2.89431
[1mStep[0m  [40/42], [94mLoss[0m : 3.00593

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.975, [92mTest[0m: 2.894, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04812
[1mStep[0m  [4/42], [94mLoss[0m : 2.65341
[1mStep[0m  [8/42], [94mLoss[0m : 3.18031
[1mStep[0m  [12/42], [94mLoss[0m : 2.85977
[1mStep[0m  [16/42], [94mLoss[0m : 2.94256
[1mStep[0m  [20/42], [94mLoss[0m : 2.58847
[1mStep[0m  [24/42], [94mLoss[0m : 3.00751
[1mStep[0m  [28/42], [94mLoss[0m : 3.04031
[1mStep[0m  [32/42], [94mLoss[0m : 2.86060
[1mStep[0m  [36/42], [94mLoss[0m : 3.02251
[1mStep[0m  [40/42], [94mLoss[0m : 2.90159

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.952, [92mTest[0m: 2.865, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80959
[1mStep[0m  [4/42], [94mLoss[0m : 2.92148
[1mStep[0m  [8/42], [94mLoss[0m : 2.84546
[1mStep[0m  [12/42], [94mLoss[0m : 2.75127
[1mStep[0m  [16/42], [94mLoss[0m : 2.55870
[1mStep[0m  [20/42], [94mLoss[0m : 3.33902
[1mStep[0m  [24/42], [94mLoss[0m : 3.14221
[1mStep[0m  [28/42], [94mLoss[0m : 3.04870
[1mStep[0m  [32/42], [94mLoss[0m : 2.73842
[1mStep[0m  [36/42], [94mLoss[0m : 2.95862
[1mStep[0m  [40/42], [94mLoss[0m : 2.92202

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.946, [92mTest[0m: 2.842, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.01398
[1mStep[0m  [4/42], [94mLoss[0m : 2.91329
[1mStep[0m  [8/42], [94mLoss[0m : 2.75607
[1mStep[0m  [12/42], [94mLoss[0m : 2.88554
[1mStep[0m  [16/42], [94mLoss[0m : 3.21250
[1mStep[0m  [20/42], [94mLoss[0m : 2.70839
[1mStep[0m  [24/42], [94mLoss[0m : 3.06338
[1mStep[0m  [28/42], [94mLoss[0m : 2.95978
[1mStep[0m  [32/42], [94mLoss[0m : 2.68410
[1mStep[0m  [36/42], [94mLoss[0m : 2.87885
[1mStep[0m  [40/42], [94mLoss[0m : 2.83804

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.907, [92mTest[0m: 2.809, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.99406
[1mStep[0m  [4/42], [94mLoss[0m : 2.94546
[1mStep[0m  [8/42], [94mLoss[0m : 2.99254
[1mStep[0m  [12/42], [94mLoss[0m : 2.63891
[1mStep[0m  [16/42], [94mLoss[0m : 2.76847
[1mStep[0m  [20/42], [94mLoss[0m : 2.96002
[1mStep[0m  [24/42], [94mLoss[0m : 2.80428
[1mStep[0m  [28/42], [94mLoss[0m : 3.02450
[1mStep[0m  [32/42], [94mLoss[0m : 2.95308
[1mStep[0m  [36/42], [94mLoss[0m : 2.96051
[1mStep[0m  [40/42], [94mLoss[0m : 2.98858

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.903, [92mTest[0m: 2.795, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.77244
[1mStep[0m  [4/42], [94mLoss[0m : 2.76023
[1mStep[0m  [8/42], [94mLoss[0m : 2.85656
[1mStep[0m  [12/42], [94mLoss[0m : 2.98901
[1mStep[0m  [16/42], [94mLoss[0m : 2.89535
[1mStep[0m  [20/42], [94mLoss[0m : 3.24535
[1mStep[0m  [24/42], [94mLoss[0m : 3.02544
[1mStep[0m  [28/42], [94mLoss[0m : 2.92274
[1mStep[0m  [32/42], [94mLoss[0m : 2.68073
[1mStep[0m  [36/42], [94mLoss[0m : 2.79354
[1mStep[0m  [40/42], [94mLoss[0m : 2.93898

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.876, [92mTest[0m: 2.774, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62398
[1mStep[0m  [4/42], [94mLoss[0m : 2.78656
[1mStep[0m  [8/42], [94mLoss[0m : 2.70627
[1mStep[0m  [12/42], [94mLoss[0m : 2.80267
[1mStep[0m  [16/42], [94mLoss[0m : 2.68236
[1mStep[0m  [20/42], [94mLoss[0m : 3.11784
[1mStep[0m  [24/42], [94mLoss[0m : 2.90614
[1mStep[0m  [28/42], [94mLoss[0m : 2.92902
[1mStep[0m  [32/42], [94mLoss[0m : 3.23723
[1mStep[0m  [36/42], [94mLoss[0m : 2.90574
[1mStep[0m  [40/42], [94mLoss[0m : 2.84384

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.868, [92mTest[0m: 2.751, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.743
====================================

Phase 2 - Evaluation MAE:  2.742701836994716
MAE score P1        4.615614
MAE score P2        2.742702
loss                2.867533
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay           0.001
Name: 13, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 10.56816
[1mStep[0m  [4/42], [94mLoss[0m : 10.39998
[1mStep[0m  [8/42], [94mLoss[0m : 10.47944
[1mStep[0m  [12/42], [94mLoss[0m : 10.42676
[1mStep[0m  [16/42], [94mLoss[0m : 10.20291
[1mStep[0m  [20/42], [94mLoss[0m : 9.87511
[1mStep[0m  [24/42], [94mLoss[0m : 10.29548
[1mStep[0m  [28/42], [94mLoss[0m : 10.19458
[1mStep[0m  [32/42], [94mLoss[0m : 10.18162
[1mStep[0m  [36/42], [94mLoss[0m : 10.05368
[1mStep[0m  [40/42], [94mLoss[0m : 9.84033

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.294, [92mTest[0m: 10.749, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.83234
[1mStep[0m  [4/42], [94mLoss[0m : 10.15542
[1mStep[0m  [8/42], [94mLoss[0m : 10.25509
[1mStep[0m  [12/42], [94mLoss[0m : 10.15494
[1mStep[0m  [16/42], [94mLoss[0m : 9.71119
[1mStep[0m  [20/42], [94mLoss[0m : 9.52642
[1mStep[0m  [24/42], [94mLoss[0m : 9.90891
[1mStep[0m  [28/42], [94mLoss[0m : 9.86667
[1mStep[0m  [32/42], [94mLoss[0m : 9.49306
[1mStep[0m  [36/42], [94mLoss[0m : 9.04323
[1mStep[0m  [40/42], [94mLoss[0m : 9.61171

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.653, [92mTest[0m: 10.172, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.45146
[1mStep[0m  [4/42], [94mLoss[0m : 9.25727
[1mStep[0m  [8/42], [94mLoss[0m : 9.18858
[1mStep[0m  [12/42], [94mLoss[0m : 9.55755
[1mStep[0m  [16/42], [94mLoss[0m : 8.72910
[1mStep[0m  [20/42], [94mLoss[0m : 8.43402
[1mStep[0m  [24/42], [94mLoss[0m : 9.01711
[1mStep[0m  [28/42], [94mLoss[0m : 8.64256
[1mStep[0m  [32/42], [94mLoss[0m : 8.97622
[1mStep[0m  [36/42], [94mLoss[0m : 8.65331
[1mStep[0m  [40/42], [94mLoss[0m : 8.62033

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.976, [92mTest[0m: 9.578, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.85994
[1mStep[0m  [4/42], [94mLoss[0m : 8.52642
[1mStep[0m  [8/42], [94mLoss[0m : 8.79590
[1mStep[0m  [12/42], [94mLoss[0m : 8.72627
[1mStep[0m  [16/42], [94mLoss[0m : 8.11474
[1mStep[0m  [20/42], [94mLoss[0m : 8.28425
[1mStep[0m  [24/42], [94mLoss[0m : 7.94570
[1mStep[0m  [28/42], [94mLoss[0m : 8.16951
[1mStep[0m  [32/42], [94mLoss[0m : 7.87439
[1mStep[0m  [36/42], [94mLoss[0m : 7.96982
[1mStep[0m  [40/42], [94mLoss[0m : 7.82954

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.300, [92mTest[0m: 8.997, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.02946
[1mStep[0m  [4/42], [94mLoss[0m : 7.67231
[1mStep[0m  [8/42], [94mLoss[0m : 8.23562
[1mStep[0m  [12/42], [94mLoss[0m : 7.85166
[1mStep[0m  [16/42], [94mLoss[0m : 7.81015
[1mStep[0m  [20/42], [94mLoss[0m : 7.41782
[1mStep[0m  [24/42], [94mLoss[0m : 7.29157
[1mStep[0m  [28/42], [94mLoss[0m : 7.43635
[1mStep[0m  [32/42], [94mLoss[0m : 7.55527
[1mStep[0m  [36/42], [94mLoss[0m : 7.67083
[1mStep[0m  [40/42], [94mLoss[0m : 7.34175

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.628, [92mTest[0m: 8.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.67683
[1mStep[0m  [4/42], [94mLoss[0m : 7.36246
[1mStep[0m  [8/42], [94mLoss[0m : 6.91813
[1mStep[0m  [12/42], [94mLoss[0m : 7.14390
[1mStep[0m  [16/42], [94mLoss[0m : 7.32771
[1mStep[0m  [20/42], [94mLoss[0m : 6.94410
[1mStep[0m  [24/42], [94mLoss[0m : 6.87517
[1mStep[0m  [28/42], [94mLoss[0m : 7.22048
[1mStep[0m  [32/42], [94mLoss[0m : 7.03547
[1mStep[0m  [36/42], [94mLoss[0m : 6.77564
[1mStep[0m  [40/42], [94mLoss[0m : 6.32562

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.987, [92mTest[0m: 7.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.76553
[1mStep[0m  [4/42], [94mLoss[0m : 6.40219
[1mStep[0m  [8/42], [94mLoss[0m : 6.58663
[1mStep[0m  [12/42], [94mLoss[0m : 6.68720
[1mStep[0m  [16/42], [94mLoss[0m : 6.76577
[1mStep[0m  [20/42], [94mLoss[0m : 6.67616
[1mStep[0m  [24/42], [94mLoss[0m : 6.54189
[1mStep[0m  [28/42], [94mLoss[0m : 6.12559
[1mStep[0m  [32/42], [94mLoss[0m : 6.30609
[1mStep[0m  [36/42], [94mLoss[0m : 6.08793
[1mStep[0m  [40/42], [94mLoss[0m : 6.30917

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 6.364, [92mTest[0m: 7.346, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.99256
[1mStep[0m  [4/42], [94mLoss[0m : 5.55879
[1mStep[0m  [8/42], [94mLoss[0m : 5.68817
[1mStep[0m  [12/42], [94mLoss[0m : 6.02687
[1mStep[0m  [16/42], [94mLoss[0m : 5.86212
[1mStep[0m  [20/42], [94mLoss[0m : 5.73078
[1mStep[0m  [24/42], [94mLoss[0m : 5.90101
[1mStep[0m  [28/42], [94mLoss[0m : 5.44499
[1mStep[0m  [32/42], [94mLoss[0m : 5.67821
[1mStep[0m  [36/42], [94mLoss[0m : 5.71276
[1mStep[0m  [40/42], [94mLoss[0m : 5.73418

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 5.750, [92mTest[0m: 6.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.51069
[1mStep[0m  [4/42], [94mLoss[0m : 5.65993
[1mStep[0m  [8/42], [94mLoss[0m : 5.09151
[1mStep[0m  [12/42], [94mLoss[0m : 4.68765
[1mStep[0m  [16/42], [94mLoss[0m : 5.05797
[1mStep[0m  [20/42], [94mLoss[0m : 5.42710
[1mStep[0m  [24/42], [94mLoss[0m : 4.85163
[1mStep[0m  [28/42], [94mLoss[0m : 5.21714
[1mStep[0m  [32/42], [94mLoss[0m : 4.86925
[1mStep[0m  [36/42], [94mLoss[0m : 4.41560
[1mStep[0m  [40/42], [94mLoss[0m : 4.84158

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 5.182, [92mTest[0m: 6.179, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.04345
[1mStep[0m  [4/42], [94mLoss[0m : 4.76578
[1mStep[0m  [8/42], [94mLoss[0m : 4.49067
[1mStep[0m  [12/42], [94mLoss[0m : 4.92525
[1mStep[0m  [16/42], [94mLoss[0m : 4.72903
[1mStep[0m  [20/42], [94mLoss[0m : 4.62562
[1mStep[0m  [24/42], [94mLoss[0m : 4.43454
[1mStep[0m  [28/42], [94mLoss[0m : 4.62221
[1mStep[0m  [32/42], [94mLoss[0m : 4.62608
[1mStep[0m  [36/42], [94mLoss[0m : 4.93681
[1mStep[0m  [40/42], [94mLoss[0m : 4.37514

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.637, [92mTest[0m: 5.557, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.27717
[1mStep[0m  [4/42], [94mLoss[0m : 4.21014
[1mStep[0m  [8/42], [94mLoss[0m : 4.26628
[1mStep[0m  [12/42], [94mLoss[0m : 4.58918
[1mStep[0m  [16/42], [94mLoss[0m : 4.30239
[1mStep[0m  [20/42], [94mLoss[0m : 4.21076
[1mStep[0m  [24/42], [94mLoss[0m : 4.33997
[1mStep[0m  [28/42], [94mLoss[0m : 4.37834
[1mStep[0m  [32/42], [94mLoss[0m : 4.02004
[1mStep[0m  [36/42], [94mLoss[0m : 3.71505
[1mStep[0m  [40/42], [94mLoss[0m : 3.87022

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.164, [92mTest[0m: 4.951, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.64829
[1mStep[0m  [4/42], [94mLoss[0m : 3.70253
[1mStep[0m  [8/42], [94mLoss[0m : 3.96558
[1mStep[0m  [12/42], [94mLoss[0m : 3.85462
[1mStep[0m  [16/42], [94mLoss[0m : 3.80546
[1mStep[0m  [20/42], [94mLoss[0m : 3.78858
[1mStep[0m  [24/42], [94mLoss[0m : 3.92972
[1mStep[0m  [28/42], [94mLoss[0m : 3.63843
[1mStep[0m  [32/42], [94mLoss[0m : 3.71149
[1mStep[0m  [36/42], [94mLoss[0m : 3.41572
[1mStep[0m  [40/42], [94mLoss[0m : 3.42661

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.780, [92mTest[0m: 4.408, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.50743
[1mStep[0m  [4/42], [94mLoss[0m : 3.44499
[1mStep[0m  [8/42], [94mLoss[0m : 3.28011
[1mStep[0m  [12/42], [94mLoss[0m : 3.49500
[1mStep[0m  [16/42], [94mLoss[0m : 3.36184
[1mStep[0m  [20/42], [94mLoss[0m : 3.61654
[1mStep[0m  [24/42], [94mLoss[0m : 3.71813
[1mStep[0m  [28/42], [94mLoss[0m : 3.55415
[1mStep[0m  [32/42], [94mLoss[0m : 3.36922
[1mStep[0m  [36/42], [94mLoss[0m : 3.31462
[1mStep[0m  [40/42], [94mLoss[0m : 3.26072

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.462, [92mTest[0m: 3.982, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.19853
[1mStep[0m  [4/42], [94mLoss[0m : 3.28938
[1mStep[0m  [8/42], [94mLoss[0m : 3.16790
[1mStep[0m  [12/42], [94mLoss[0m : 3.05679
[1mStep[0m  [16/42], [94mLoss[0m : 3.01321
[1mStep[0m  [20/42], [94mLoss[0m : 3.20295
[1mStep[0m  [24/42], [94mLoss[0m : 3.29441
[1mStep[0m  [28/42], [94mLoss[0m : 3.34772
[1mStep[0m  [32/42], [94mLoss[0m : 2.94125
[1mStep[0m  [36/42], [94mLoss[0m : 3.14740
[1mStep[0m  [40/42], [94mLoss[0m : 3.18122

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.216, [92mTest[0m: 3.634, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.15290
[1mStep[0m  [4/42], [94mLoss[0m : 3.26250
[1mStep[0m  [8/42], [94mLoss[0m : 3.10596
[1mStep[0m  [12/42], [94mLoss[0m : 3.07992
[1mStep[0m  [16/42], [94mLoss[0m : 3.18528
[1mStep[0m  [20/42], [94mLoss[0m : 2.79366
[1mStep[0m  [24/42], [94mLoss[0m : 3.04403
[1mStep[0m  [28/42], [94mLoss[0m : 3.13938
[1mStep[0m  [32/42], [94mLoss[0m : 3.25631
[1mStep[0m  [36/42], [94mLoss[0m : 3.19932
[1mStep[0m  [40/42], [94mLoss[0m : 2.97335

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.094, [92mTest[0m: 3.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.02087
[1mStep[0m  [4/42], [94mLoss[0m : 2.92049
[1mStep[0m  [8/42], [94mLoss[0m : 2.79917
[1mStep[0m  [12/42], [94mLoss[0m : 2.85266
[1mStep[0m  [16/42], [94mLoss[0m : 2.95714
[1mStep[0m  [20/42], [94mLoss[0m : 2.77600
[1mStep[0m  [24/42], [94mLoss[0m : 2.98955
[1mStep[0m  [28/42], [94mLoss[0m : 2.93721
[1mStep[0m  [32/42], [94mLoss[0m : 3.03951
[1mStep[0m  [36/42], [94mLoss[0m : 2.97835
[1mStep[0m  [40/42], [94mLoss[0m : 2.70912

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.963, [92mTest[0m: 3.207, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.04879
[1mStep[0m  [4/42], [94mLoss[0m : 2.88458
[1mStep[0m  [8/42], [94mLoss[0m : 2.90479
[1mStep[0m  [12/42], [94mLoss[0m : 2.87574
[1mStep[0m  [16/42], [94mLoss[0m : 2.83315
[1mStep[0m  [20/42], [94mLoss[0m : 3.07717
[1mStep[0m  [24/42], [94mLoss[0m : 2.72346
[1mStep[0m  [28/42], [94mLoss[0m : 2.72673
[1mStep[0m  [32/42], [94mLoss[0m : 2.56012
[1mStep[0m  [36/42], [94mLoss[0m : 2.97231
[1mStep[0m  [40/42], [94mLoss[0m : 2.78618

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.869, [92mTest[0m: 3.054, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.53695
[1mStep[0m  [4/42], [94mLoss[0m : 2.93670
[1mStep[0m  [8/42], [94mLoss[0m : 2.82144
[1mStep[0m  [12/42], [94mLoss[0m : 2.91661
[1mStep[0m  [16/42], [94mLoss[0m : 2.63080
[1mStep[0m  [20/42], [94mLoss[0m : 2.76811
[1mStep[0m  [24/42], [94mLoss[0m : 2.63233
[1mStep[0m  [28/42], [94mLoss[0m : 2.79941
[1mStep[0m  [32/42], [94mLoss[0m : 2.58518
[1mStep[0m  [36/42], [94mLoss[0m : 2.82011
[1mStep[0m  [40/42], [94mLoss[0m : 2.81007

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.803, [92mTest[0m: 2.941, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.90429
[1mStep[0m  [4/42], [94mLoss[0m : 2.75785
[1mStep[0m  [8/42], [94mLoss[0m : 2.68843
[1mStep[0m  [12/42], [94mLoss[0m : 2.56626
[1mStep[0m  [16/42], [94mLoss[0m : 2.93507
[1mStep[0m  [20/42], [94mLoss[0m : 2.73518
[1mStep[0m  [24/42], [94mLoss[0m : 2.80531
[1mStep[0m  [28/42], [94mLoss[0m : 2.62424
[1mStep[0m  [32/42], [94mLoss[0m : 2.80180
[1mStep[0m  [36/42], [94mLoss[0m : 2.63068
[1mStep[0m  [40/42], [94mLoss[0m : 2.63417

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.751, [92mTest[0m: 2.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.76818
[1mStep[0m  [4/42], [94mLoss[0m : 2.87656
[1mStep[0m  [8/42], [94mLoss[0m : 2.66422
[1mStep[0m  [12/42], [94mLoss[0m : 2.75888
[1mStep[0m  [16/42], [94mLoss[0m : 2.82468
[1mStep[0m  [20/42], [94mLoss[0m : 2.76227
[1mStep[0m  [24/42], [94mLoss[0m : 2.74218
[1mStep[0m  [28/42], [94mLoss[0m : 2.49679
[1mStep[0m  [32/42], [94mLoss[0m : 2.54916
[1mStep[0m  [36/42], [94mLoss[0m : 2.80748
[1mStep[0m  [40/42], [94mLoss[0m : 2.70135

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.741, [92mTest[0m: 2.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.80341
[1mStep[0m  [4/42], [94mLoss[0m : 2.46599
[1mStep[0m  [8/42], [94mLoss[0m : 2.80060
[1mStep[0m  [12/42], [94mLoss[0m : 2.64656
[1mStep[0m  [16/42], [94mLoss[0m : 3.00132
[1mStep[0m  [20/42], [94mLoss[0m : 2.77767
[1mStep[0m  [24/42], [94mLoss[0m : 2.76474
[1mStep[0m  [28/42], [94mLoss[0m : 2.46553
[1mStep[0m  [32/42], [94mLoss[0m : 2.82421
[1mStep[0m  [36/42], [94mLoss[0m : 2.76527
[1mStep[0m  [40/42], [94mLoss[0m : 2.57304

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.691, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72578
[1mStep[0m  [4/42], [94mLoss[0m : 2.65021
[1mStep[0m  [8/42], [94mLoss[0m : 2.88624
[1mStep[0m  [12/42], [94mLoss[0m : 2.92011
[1mStep[0m  [16/42], [94mLoss[0m : 2.65637
[1mStep[0m  [20/42], [94mLoss[0m : 2.52449
[1mStep[0m  [24/42], [94mLoss[0m : 2.91899
[1mStep[0m  [28/42], [94mLoss[0m : 2.65144
[1mStep[0m  [32/42], [94mLoss[0m : 2.73918
[1mStep[0m  [36/42], [94mLoss[0m : 2.71206
[1mStep[0m  [40/42], [94mLoss[0m : 2.86653

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.716, [92mTest[0m: 2.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57161
[1mStep[0m  [4/42], [94mLoss[0m : 2.69807
[1mStep[0m  [8/42], [94mLoss[0m : 2.75940
[1mStep[0m  [12/42], [94mLoss[0m : 2.38347
[1mStep[0m  [16/42], [94mLoss[0m : 2.68943
[1mStep[0m  [20/42], [94mLoss[0m : 2.62311
[1mStep[0m  [24/42], [94mLoss[0m : 2.85849
[1mStep[0m  [28/42], [94mLoss[0m : 2.63796
[1mStep[0m  [32/42], [94mLoss[0m : 2.65824
[1mStep[0m  [36/42], [94mLoss[0m : 2.55726
[1mStep[0m  [40/42], [94mLoss[0m : 2.83770

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.668, [92mTest[0m: 2.650, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74767
[1mStep[0m  [4/42], [94mLoss[0m : 2.47809
[1mStep[0m  [8/42], [94mLoss[0m : 2.60208
[1mStep[0m  [12/42], [94mLoss[0m : 2.57164
[1mStep[0m  [16/42], [94mLoss[0m : 2.66738
[1mStep[0m  [20/42], [94mLoss[0m : 2.57742
[1mStep[0m  [24/42], [94mLoss[0m : 2.63468
[1mStep[0m  [28/42], [94mLoss[0m : 2.53127
[1mStep[0m  [32/42], [94mLoss[0m : 2.44089
[1mStep[0m  [36/42], [94mLoss[0m : 2.65191
[1mStep[0m  [40/42], [94mLoss[0m : 2.73963

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.642, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44853
[1mStep[0m  [4/42], [94mLoss[0m : 2.61392
[1mStep[0m  [8/42], [94mLoss[0m : 2.67444
[1mStep[0m  [12/42], [94mLoss[0m : 2.79716
[1mStep[0m  [16/42], [94mLoss[0m : 2.54012
[1mStep[0m  [20/42], [94mLoss[0m : 2.43982
[1mStep[0m  [24/42], [94mLoss[0m : 2.66820
[1mStep[0m  [28/42], [94mLoss[0m : 2.86747
[1mStep[0m  [32/42], [94mLoss[0m : 3.03148
[1mStep[0m  [36/42], [94mLoss[0m : 2.58110
[1mStep[0m  [40/42], [94mLoss[0m : 2.79180

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.623, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58639
[1mStep[0m  [4/42], [94mLoss[0m : 2.64141
[1mStep[0m  [8/42], [94mLoss[0m : 2.55825
[1mStep[0m  [12/42], [94mLoss[0m : 2.59274
[1mStep[0m  [16/42], [94mLoss[0m : 2.95223
[1mStep[0m  [20/42], [94mLoss[0m : 2.58400
[1mStep[0m  [24/42], [94mLoss[0m : 2.44532
[1mStep[0m  [28/42], [94mLoss[0m : 2.71184
[1mStep[0m  [32/42], [94mLoss[0m : 2.69190
[1mStep[0m  [36/42], [94mLoss[0m : 2.53508
[1mStep[0m  [40/42], [94mLoss[0m : 2.42426

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.598, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61157
[1mStep[0m  [4/42], [94mLoss[0m : 2.62752
[1mStep[0m  [8/42], [94mLoss[0m : 2.89292
[1mStep[0m  [12/42], [94mLoss[0m : 2.57027
[1mStep[0m  [16/42], [94mLoss[0m : 2.66352
[1mStep[0m  [20/42], [94mLoss[0m : 2.72374
[1mStep[0m  [24/42], [94mLoss[0m : 2.70036
[1mStep[0m  [28/42], [94mLoss[0m : 2.73563
[1mStep[0m  [32/42], [94mLoss[0m : 2.87370
[1mStep[0m  [36/42], [94mLoss[0m : 2.47503
[1mStep[0m  [40/42], [94mLoss[0m : 2.67552

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.578, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.79495
[1mStep[0m  [4/42], [94mLoss[0m : 2.87047
[1mStep[0m  [8/42], [94mLoss[0m : 2.53679
[1mStep[0m  [12/42], [94mLoss[0m : 2.51597
[1mStep[0m  [16/42], [94mLoss[0m : 2.63310
[1mStep[0m  [20/42], [94mLoss[0m : 2.32588
[1mStep[0m  [24/42], [94mLoss[0m : 2.51583
[1mStep[0m  [28/42], [94mLoss[0m : 2.48104
[1mStep[0m  [32/42], [94mLoss[0m : 2.78899
[1mStep[0m  [36/42], [94mLoss[0m : 2.75954
[1mStep[0m  [40/42], [94mLoss[0m : 2.78181

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.564, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66540
[1mStep[0m  [4/42], [94mLoss[0m : 2.62349
[1mStep[0m  [8/42], [94mLoss[0m : 2.52682
[1mStep[0m  [12/42], [94mLoss[0m : 2.40777
[1mStep[0m  [16/42], [94mLoss[0m : 2.52287
[1mStep[0m  [20/42], [94mLoss[0m : 2.67645
[1mStep[0m  [24/42], [94mLoss[0m : 2.65673
[1mStep[0m  [28/42], [94mLoss[0m : 2.58164
[1mStep[0m  [32/42], [94mLoss[0m : 2.52547
[1mStep[0m  [36/42], [94mLoss[0m : 2.66878
[1mStep[0m  [40/42], [94mLoss[0m : 2.67960

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.566, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54747
[1mStep[0m  [4/42], [94mLoss[0m : 2.51204
[1mStep[0m  [8/42], [94mLoss[0m : 2.91783
[1mStep[0m  [12/42], [94mLoss[0m : 2.61647
[1mStep[0m  [16/42], [94mLoss[0m : 2.55361
[1mStep[0m  [20/42], [94mLoss[0m : 2.72319
[1mStep[0m  [24/42], [94mLoss[0m : 2.63106
[1mStep[0m  [28/42], [94mLoss[0m : 2.58432
[1mStep[0m  [32/42], [94mLoss[0m : 2.77612
[1mStep[0m  [36/42], [94mLoss[0m : 2.59145
[1mStep[0m  [40/42], [94mLoss[0m : 2.57677

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.550, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.536
====================================

Phase 1 - Evaluation MAE:  2.5358061620167325
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 2.74739
[1mStep[0m  [4/42], [94mLoss[0m : 2.43110
[1mStep[0m  [8/42], [94mLoss[0m : 2.63012
[1mStep[0m  [12/42], [94mLoss[0m : 2.76505
[1mStep[0m  [16/42], [94mLoss[0m : 2.89484
[1mStep[0m  [20/42], [94mLoss[0m : 2.50774
[1mStep[0m  [24/42], [94mLoss[0m : 2.61225
[1mStep[0m  [28/42], [94mLoss[0m : 2.60076
[1mStep[0m  [32/42], [94mLoss[0m : 2.50642
[1mStep[0m  [36/42], [94mLoss[0m : 2.50056
[1mStep[0m  [40/42], [94mLoss[0m : 2.56126

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.536, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55437
[1mStep[0m  [4/42], [94mLoss[0m : 2.75257
[1mStep[0m  [8/42], [94mLoss[0m : 2.67058
[1mStep[0m  [12/42], [94mLoss[0m : 2.91879
[1mStep[0m  [16/42], [94mLoss[0m : 2.53510
[1mStep[0m  [20/42], [94mLoss[0m : 2.52268
[1mStep[0m  [24/42], [94mLoss[0m : 2.68779
[1mStep[0m  [28/42], [94mLoss[0m : 2.53679
[1mStep[0m  [32/42], [94mLoss[0m : 2.70607
[1mStep[0m  [36/42], [94mLoss[0m : 2.58549
[1mStep[0m  [40/42], [94mLoss[0m : 2.66155

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40461
[1mStep[0m  [4/42], [94mLoss[0m : 2.63597
[1mStep[0m  [8/42], [94mLoss[0m : 2.77439
[1mStep[0m  [12/42], [94mLoss[0m : 2.58885
[1mStep[0m  [16/42], [94mLoss[0m : 2.59369
[1mStep[0m  [20/42], [94mLoss[0m : 2.84899
[1mStep[0m  [24/42], [94mLoss[0m : 2.49559
[1mStep[0m  [28/42], [94mLoss[0m : 2.45954
[1mStep[0m  [32/42], [94mLoss[0m : 2.52051
[1mStep[0m  [36/42], [94mLoss[0m : 2.59989
[1mStep[0m  [40/42], [94mLoss[0m : 2.50744

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.500, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.64853
[1mStep[0m  [4/42], [94mLoss[0m : 2.59109
[1mStep[0m  [8/42], [94mLoss[0m : 2.71352
[1mStep[0m  [12/42], [94mLoss[0m : 2.60158
[1mStep[0m  [16/42], [94mLoss[0m : 2.46487
[1mStep[0m  [20/42], [94mLoss[0m : 2.52507
[1mStep[0m  [24/42], [94mLoss[0m : 2.42822
[1mStep[0m  [28/42], [94mLoss[0m : 2.76341
[1mStep[0m  [32/42], [94mLoss[0m : 2.33270
[1mStep[0m  [36/42], [94mLoss[0m : 2.61473
[1mStep[0m  [40/42], [94mLoss[0m : 2.63752

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.507, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68153
[1mStep[0m  [4/42], [94mLoss[0m : 2.40591
[1mStep[0m  [8/42], [94mLoss[0m : 2.70966
[1mStep[0m  [12/42], [94mLoss[0m : 2.45705
[1mStep[0m  [16/42], [94mLoss[0m : 2.71128
[1mStep[0m  [20/42], [94mLoss[0m : 2.65377
[1mStep[0m  [24/42], [94mLoss[0m : 2.83697
[1mStep[0m  [28/42], [94mLoss[0m : 2.47000
[1mStep[0m  [32/42], [94mLoss[0m : 2.43753
[1mStep[0m  [36/42], [94mLoss[0m : 2.64069
[1mStep[0m  [40/42], [94mLoss[0m : 2.37466

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.620, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57493
[1mStep[0m  [4/42], [94mLoss[0m : 2.44429
[1mStep[0m  [8/42], [94mLoss[0m : 2.46487
[1mStep[0m  [12/42], [94mLoss[0m : 2.67103
[1mStep[0m  [16/42], [94mLoss[0m : 2.68176
[1mStep[0m  [20/42], [94mLoss[0m : 2.53581
[1mStep[0m  [24/42], [94mLoss[0m : 2.62688
[1mStep[0m  [28/42], [94mLoss[0m : 2.62100
[1mStep[0m  [32/42], [94mLoss[0m : 2.58154
[1mStep[0m  [36/42], [94mLoss[0m : 2.61854
[1mStep[0m  [40/42], [94mLoss[0m : 2.73375

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43815
[1mStep[0m  [4/42], [94mLoss[0m : 2.46799
[1mStep[0m  [8/42], [94mLoss[0m : 2.78672
[1mStep[0m  [12/42], [94mLoss[0m : 2.58228
[1mStep[0m  [16/42], [94mLoss[0m : 2.78918
[1mStep[0m  [20/42], [94mLoss[0m : 2.79594
[1mStep[0m  [24/42], [94mLoss[0m : 2.52501
[1mStep[0m  [28/42], [94mLoss[0m : 2.46846
[1mStep[0m  [32/42], [94mLoss[0m : 2.50903
[1mStep[0m  [36/42], [94mLoss[0m : 2.47149
[1mStep[0m  [40/42], [94mLoss[0m : 2.73512

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43001
[1mStep[0m  [4/42], [94mLoss[0m : 2.48308
[1mStep[0m  [8/42], [94mLoss[0m : 2.46703
[1mStep[0m  [12/42], [94mLoss[0m : 2.43138
[1mStep[0m  [16/42], [94mLoss[0m : 2.67567
[1mStep[0m  [20/42], [94mLoss[0m : 2.54007
[1mStep[0m  [24/42], [94mLoss[0m : 2.51324
[1mStep[0m  [28/42], [94mLoss[0m : 2.64747
[1mStep[0m  [32/42], [94mLoss[0m : 2.75407
[1mStep[0m  [36/42], [94mLoss[0m : 2.47735
[1mStep[0m  [40/42], [94mLoss[0m : 2.61912

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.69807
[1mStep[0m  [4/42], [94mLoss[0m : 2.69570
[1mStep[0m  [8/42], [94mLoss[0m : 2.46854
[1mStep[0m  [12/42], [94mLoss[0m : 2.53085
[1mStep[0m  [16/42], [94mLoss[0m : 2.36063
[1mStep[0m  [20/42], [94mLoss[0m : 2.59422
[1mStep[0m  [24/42], [94mLoss[0m : 2.72168
[1mStep[0m  [28/42], [94mLoss[0m : 2.53027
[1mStep[0m  [32/42], [94mLoss[0m : 2.73161
[1mStep[0m  [36/42], [94mLoss[0m : 2.40035
[1mStep[0m  [40/42], [94mLoss[0m : 2.64768

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59199
[1mStep[0m  [4/42], [94mLoss[0m : 2.37384
[1mStep[0m  [8/42], [94mLoss[0m : 2.55229
[1mStep[0m  [12/42], [94mLoss[0m : 2.43295
[1mStep[0m  [16/42], [94mLoss[0m : 2.40181
[1mStep[0m  [20/42], [94mLoss[0m : 2.46908
[1mStep[0m  [24/42], [94mLoss[0m : 2.43625
[1mStep[0m  [28/42], [94mLoss[0m : 2.75601
[1mStep[0m  [32/42], [94mLoss[0m : 2.45038
[1mStep[0m  [36/42], [94mLoss[0m : 2.36295
[1mStep[0m  [40/42], [94mLoss[0m : 2.57360

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.39587
[1mStep[0m  [4/42], [94mLoss[0m : 2.58870
[1mStep[0m  [8/42], [94mLoss[0m : 2.37682
[1mStep[0m  [12/42], [94mLoss[0m : 2.46009
[1mStep[0m  [16/42], [94mLoss[0m : 2.62266
[1mStep[0m  [20/42], [94mLoss[0m : 2.71005
[1mStep[0m  [24/42], [94mLoss[0m : 2.74370
[1mStep[0m  [28/42], [94mLoss[0m : 2.55133
[1mStep[0m  [32/42], [94mLoss[0m : 2.44884
[1mStep[0m  [36/42], [94mLoss[0m : 2.78915
[1mStep[0m  [40/42], [94mLoss[0m : 2.71396

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.785, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.56319
[1mStep[0m  [4/42], [94mLoss[0m : 2.91169
[1mStep[0m  [8/42], [94mLoss[0m : 2.43781
[1mStep[0m  [12/42], [94mLoss[0m : 2.24337
[1mStep[0m  [16/42], [94mLoss[0m : 2.46411
[1mStep[0m  [20/42], [94mLoss[0m : 2.48355
[1mStep[0m  [24/42], [94mLoss[0m : 2.69640
[1mStep[0m  [28/42], [94mLoss[0m : 2.39335
[1mStep[0m  [32/42], [94mLoss[0m : 2.49321
[1mStep[0m  [36/42], [94mLoss[0m : 2.51607
[1mStep[0m  [40/42], [94mLoss[0m : 2.74461

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.692, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74564
[1mStep[0m  [4/42], [94mLoss[0m : 2.59914
[1mStep[0m  [8/42], [94mLoss[0m : 2.48415
[1mStep[0m  [12/42], [94mLoss[0m : 2.48407
[1mStep[0m  [16/42], [94mLoss[0m : 2.53424
[1mStep[0m  [20/42], [94mLoss[0m : 2.67214
[1mStep[0m  [24/42], [94mLoss[0m : 2.43896
[1mStep[0m  [28/42], [94mLoss[0m : 2.79619
[1mStep[0m  [32/42], [94mLoss[0m : 2.52333
[1mStep[0m  [36/42], [94mLoss[0m : 2.41772
[1mStep[0m  [40/42], [94mLoss[0m : 2.80974

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.811, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.65208
[1mStep[0m  [4/42], [94mLoss[0m : 2.36199
[1mStep[0m  [8/42], [94mLoss[0m : 2.59461
[1mStep[0m  [12/42], [94mLoss[0m : 2.70728
[1mStep[0m  [16/42], [94mLoss[0m : 2.40179
[1mStep[0m  [20/42], [94mLoss[0m : 2.41667
[1mStep[0m  [24/42], [94mLoss[0m : 2.37879
[1mStep[0m  [28/42], [94mLoss[0m : 2.65441
[1mStep[0m  [32/42], [94mLoss[0m : 2.66797
[1mStep[0m  [36/42], [94mLoss[0m : 2.58053
[1mStep[0m  [40/42], [94mLoss[0m : 2.75163

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.774, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58078
[1mStep[0m  [4/42], [94mLoss[0m : 2.36039
[1mStep[0m  [8/42], [94mLoss[0m : 2.37404
[1mStep[0m  [12/42], [94mLoss[0m : 2.45078
[1mStep[0m  [16/42], [94mLoss[0m : 2.57866
[1mStep[0m  [20/42], [94mLoss[0m : 2.44531
[1mStep[0m  [24/42], [94mLoss[0m : 2.38529
[1mStep[0m  [28/42], [94mLoss[0m : 2.49623
[1mStep[0m  [32/42], [94mLoss[0m : 2.57103
[1mStep[0m  [36/42], [94mLoss[0m : 2.55752
[1mStep[0m  [40/42], [94mLoss[0m : 2.46532

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.722, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41889
[1mStep[0m  [4/42], [94mLoss[0m : 2.52969
[1mStep[0m  [8/42], [94mLoss[0m : 2.70231
[1mStep[0m  [12/42], [94mLoss[0m : 2.30422
[1mStep[0m  [16/42], [94mLoss[0m : 2.51996
[1mStep[0m  [20/42], [94mLoss[0m : 2.52478
[1mStep[0m  [24/42], [94mLoss[0m : 2.65860
[1mStep[0m  [28/42], [94mLoss[0m : 2.48092
[1mStep[0m  [32/42], [94mLoss[0m : 2.32834
[1mStep[0m  [36/42], [94mLoss[0m : 2.72172
[1mStep[0m  [40/42], [94mLoss[0m : 2.53907

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.782, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44566
[1mStep[0m  [4/42], [94mLoss[0m : 2.44787
[1mStep[0m  [8/42], [94mLoss[0m : 2.58524
[1mStep[0m  [12/42], [94mLoss[0m : 2.64798
[1mStep[0m  [16/42], [94mLoss[0m : 2.48128
[1mStep[0m  [20/42], [94mLoss[0m : 2.29348
[1mStep[0m  [24/42], [94mLoss[0m : 2.64611
[1mStep[0m  [28/42], [94mLoss[0m : 2.38057
[1mStep[0m  [32/42], [94mLoss[0m : 2.45667
[1mStep[0m  [36/42], [94mLoss[0m : 2.44501
[1mStep[0m  [40/42], [94mLoss[0m : 2.43649

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.766, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51232
[1mStep[0m  [4/42], [94mLoss[0m : 2.46283
[1mStep[0m  [8/42], [94mLoss[0m : 2.37818
[1mStep[0m  [12/42], [94mLoss[0m : 2.44392
[1mStep[0m  [16/42], [94mLoss[0m : 2.35907
[1mStep[0m  [20/42], [94mLoss[0m : 2.37182
[1mStep[0m  [24/42], [94mLoss[0m : 2.39024
[1mStep[0m  [28/42], [94mLoss[0m : 2.36893
[1mStep[0m  [32/42], [94mLoss[0m : 2.42553
[1mStep[0m  [36/42], [94mLoss[0m : 2.83098
[1mStep[0m  [40/42], [94mLoss[0m : 2.24687

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.691, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43548
[1mStep[0m  [4/42], [94mLoss[0m : 2.63782
[1mStep[0m  [8/42], [94mLoss[0m : 2.51719
[1mStep[0m  [12/42], [94mLoss[0m : 2.47577
[1mStep[0m  [16/42], [94mLoss[0m : 2.53935
[1mStep[0m  [20/42], [94mLoss[0m : 2.42517
[1mStep[0m  [24/42], [94mLoss[0m : 2.43617
[1mStep[0m  [28/42], [94mLoss[0m : 2.56151
[1mStep[0m  [32/42], [94mLoss[0m : 2.44391
[1mStep[0m  [36/42], [94mLoss[0m : 2.64478
[1mStep[0m  [40/42], [94mLoss[0m : 2.41642

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.741, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.38079
[1mStep[0m  [4/42], [94mLoss[0m : 2.61886
[1mStep[0m  [8/42], [94mLoss[0m : 2.44253
[1mStep[0m  [12/42], [94mLoss[0m : 2.60561
[1mStep[0m  [16/42], [94mLoss[0m : 2.31078
[1mStep[0m  [20/42], [94mLoss[0m : 2.36664
[1mStep[0m  [24/42], [94mLoss[0m : 2.50348
[1mStep[0m  [28/42], [94mLoss[0m : 2.72753
[1mStep[0m  [32/42], [94mLoss[0m : 2.37548
[1mStep[0m  [36/42], [94mLoss[0m : 2.43533
[1mStep[0m  [40/42], [94mLoss[0m : 2.47261

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.776, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.43505
[1mStep[0m  [4/42], [94mLoss[0m : 2.37975
[1mStep[0m  [8/42], [94mLoss[0m : 2.25645
[1mStep[0m  [12/42], [94mLoss[0m : 2.35974
[1mStep[0m  [16/42], [94mLoss[0m : 2.36671
[1mStep[0m  [20/42], [94mLoss[0m : 2.59929
[1mStep[0m  [24/42], [94mLoss[0m : 2.30602
[1mStep[0m  [28/42], [94mLoss[0m : 2.37764
[1mStep[0m  [32/42], [94mLoss[0m : 2.46950
[1mStep[0m  [36/42], [94mLoss[0m : 2.39265
[1mStep[0m  [40/42], [94mLoss[0m : 2.61351

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.674, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55379
[1mStep[0m  [4/42], [94mLoss[0m : 2.45880
[1mStep[0m  [8/42], [94mLoss[0m : 2.34667
[1mStep[0m  [12/42], [94mLoss[0m : 2.49646
[1mStep[0m  [16/42], [94mLoss[0m : 2.42292
[1mStep[0m  [20/42], [94mLoss[0m : 2.45780
[1mStep[0m  [24/42], [94mLoss[0m : 2.27963
[1mStep[0m  [28/42], [94mLoss[0m : 2.49560
[1mStep[0m  [32/42], [94mLoss[0m : 2.59608
[1mStep[0m  [36/42], [94mLoss[0m : 2.44171
[1mStep[0m  [40/42], [94mLoss[0m : 2.44016

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.670, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59233
[1mStep[0m  [4/42], [94mLoss[0m : 2.53016
[1mStep[0m  [8/42], [94mLoss[0m : 2.74356
[1mStep[0m  [12/42], [94mLoss[0m : 2.33892
[1mStep[0m  [16/42], [94mLoss[0m : 2.51854
[1mStep[0m  [20/42], [94mLoss[0m : 2.52600
[1mStep[0m  [24/42], [94mLoss[0m : 2.42673
[1mStep[0m  [28/42], [94mLoss[0m : 2.34960
[1mStep[0m  [32/42], [94mLoss[0m : 2.68715
[1mStep[0m  [36/42], [94mLoss[0m : 2.36921
[1mStep[0m  [40/42], [94mLoss[0m : 2.30780

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.635, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.27409
[1mStep[0m  [4/42], [94mLoss[0m : 2.33069
[1mStep[0m  [8/42], [94mLoss[0m : 2.41377
[1mStep[0m  [12/42], [94mLoss[0m : 2.29399
[1mStep[0m  [16/42], [94mLoss[0m : 2.29712
[1mStep[0m  [20/42], [94mLoss[0m : 2.34059
[1mStep[0m  [24/42], [94mLoss[0m : 2.41296
[1mStep[0m  [28/42], [94mLoss[0m : 2.50313
[1mStep[0m  [32/42], [94mLoss[0m : 2.53967
[1mStep[0m  [36/42], [94mLoss[0m : 2.34043
[1mStep[0m  [40/42], [94mLoss[0m : 2.31830

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.641, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.21462
[1mStep[0m  [4/42], [94mLoss[0m : 2.46211
[1mStep[0m  [8/42], [94mLoss[0m : 2.17678
[1mStep[0m  [12/42], [94mLoss[0m : 2.44525
[1mStep[0m  [16/42], [94mLoss[0m : 2.48100
[1mStep[0m  [20/42], [94mLoss[0m : 2.70601
[1mStep[0m  [24/42], [94mLoss[0m : 2.41130
[1mStep[0m  [28/42], [94mLoss[0m : 2.60756
[1mStep[0m  [32/42], [94mLoss[0m : 2.59166
[1mStep[0m  [36/42], [94mLoss[0m : 2.30965
[1mStep[0m  [40/42], [94mLoss[0m : 2.44880

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.645, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40606
[1mStep[0m  [4/42], [94mLoss[0m : 2.41567
[1mStep[0m  [8/42], [94mLoss[0m : 2.36990
[1mStep[0m  [12/42], [94mLoss[0m : 2.51727
[1mStep[0m  [16/42], [94mLoss[0m : 2.42121
[1mStep[0m  [20/42], [94mLoss[0m : 2.35136
[1mStep[0m  [24/42], [94mLoss[0m : 2.41327
[1mStep[0m  [28/42], [94mLoss[0m : 2.48021
[1mStep[0m  [32/42], [94mLoss[0m : 2.52046
[1mStep[0m  [36/42], [94mLoss[0m : 2.58057
[1mStep[0m  [40/42], [94mLoss[0m : 2.34276

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.610, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.50299
[1mStep[0m  [4/42], [94mLoss[0m : 2.41890
[1mStep[0m  [8/42], [94mLoss[0m : 2.54793
[1mStep[0m  [12/42], [94mLoss[0m : 2.32123
[1mStep[0m  [16/42], [94mLoss[0m : 2.40569
[1mStep[0m  [20/42], [94mLoss[0m : 2.53991
[1mStep[0m  [24/42], [94mLoss[0m : 2.38433
[1mStep[0m  [28/42], [94mLoss[0m : 2.67933
[1mStep[0m  [32/42], [94mLoss[0m : 2.49455
[1mStep[0m  [36/42], [94mLoss[0m : 2.41324
[1mStep[0m  [40/42], [94mLoss[0m : 2.42525

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.637, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52124
[1mStep[0m  [4/42], [94mLoss[0m : 2.40561
[1mStep[0m  [8/42], [94mLoss[0m : 2.25894
[1mStep[0m  [12/42], [94mLoss[0m : 2.36665
[1mStep[0m  [16/42], [94mLoss[0m : 2.27295
[1mStep[0m  [20/42], [94mLoss[0m : 2.39980
[1mStep[0m  [24/42], [94mLoss[0m : 2.49261
[1mStep[0m  [28/42], [94mLoss[0m : 2.57645
[1mStep[0m  [32/42], [94mLoss[0m : 2.37595
[1mStep[0m  [36/42], [94mLoss[0m : 2.47114
[1mStep[0m  [40/42], [94mLoss[0m : 2.41269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.617, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46304
[1mStep[0m  [4/42], [94mLoss[0m : 2.48434
[1mStep[0m  [8/42], [94mLoss[0m : 2.49653
[1mStep[0m  [12/42], [94mLoss[0m : 2.57913
[1mStep[0m  [16/42], [94mLoss[0m : 2.50177
[1mStep[0m  [20/42], [94mLoss[0m : 2.20784
[1mStep[0m  [24/42], [94mLoss[0m : 2.50627
[1mStep[0m  [28/42], [94mLoss[0m : 2.52675
[1mStep[0m  [32/42], [94mLoss[0m : 2.53001
[1mStep[0m  [36/42], [94mLoss[0m : 2.46612
[1mStep[0m  [40/42], [94mLoss[0m : 2.46632

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.579, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40391
[1mStep[0m  [4/42], [94mLoss[0m : 2.29246
[1mStep[0m  [8/42], [94mLoss[0m : 2.35211
[1mStep[0m  [12/42], [94mLoss[0m : 2.32725
[1mStep[0m  [16/42], [94mLoss[0m : 2.46183
[1mStep[0m  [20/42], [94mLoss[0m : 2.53944
[1mStep[0m  [24/42], [94mLoss[0m : 2.52948
[1mStep[0m  [28/42], [94mLoss[0m : 2.35197
[1mStep[0m  [32/42], [94mLoss[0m : 2.38769
[1mStep[0m  [36/42], [94mLoss[0m : 2.40667
[1mStep[0m  [40/42], [94mLoss[0m : 2.78786

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.589, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.547
====================================

Phase 2 - Evaluation MAE:  2.5467017889022827
MAE score P1      2.535806
MAE score P2      2.546702
loss              2.417291
learning_rate       0.0001
batch_size             256
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay         0.001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.85544
[1mStep[0m  [2/21], [94mLoss[0m : 10.89965
[1mStep[0m  [4/21], [94mLoss[0m : 10.75648
[1mStep[0m  [6/21], [94mLoss[0m : 11.14683
[1mStep[0m  [8/21], [94mLoss[0m : 10.97500
[1mStep[0m  [10/21], [94mLoss[0m : 10.83747
[1mStep[0m  [12/21], [94mLoss[0m : 10.93578
[1mStep[0m  [14/21], [94mLoss[0m : 10.52176
[1mStep[0m  [16/21], [94mLoss[0m : 10.86113
[1mStep[0m  [18/21], [94mLoss[0m : 10.81116
[1mStep[0m  [20/21], [94mLoss[0m : 11.05013

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.855, [92mTest[0m: 10.954, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.85848
[1mStep[0m  [2/21], [94mLoss[0m : 10.67495
[1mStep[0m  [4/21], [94mLoss[0m : 10.76275
[1mStep[0m  [6/21], [94mLoss[0m : 10.86387
[1mStep[0m  [8/21], [94mLoss[0m : 10.76686
[1mStep[0m  [10/21], [94mLoss[0m : 10.92394
[1mStep[0m  [12/21], [94mLoss[0m : 10.90442
[1mStep[0m  [14/21], [94mLoss[0m : 10.89301
[1mStep[0m  [16/21], [94mLoss[0m : 10.98084
[1mStep[0m  [18/21], [94mLoss[0m : 11.07993
[1mStep[0m  [20/21], [94mLoss[0m : 10.88850

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.832, [92mTest[0m: 10.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.64978
[1mStep[0m  [2/21], [94mLoss[0m : 10.96657
[1mStep[0m  [4/21], [94mLoss[0m : 11.00862
[1mStep[0m  [6/21], [94mLoss[0m : 10.59585
[1mStep[0m  [8/21], [94mLoss[0m : 10.79265
[1mStep[0m  [10/21], [94mLoss[0m : 10.54349
[1mStep[0m  [12/21], [94mLoss[0m : 10.72475
[1mStep[0m  [14/21], [94mLoss[0m : 10.82049
[1mStep[0m  [16/21], [94mLoss[0m : 10.85636
[1mStep[0m  [18/21], [94mLoss[0m : 10.81533
[1mStep[0m  [20/21], [94mLoss[0m : 10.74302

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.812, [92mTest[0m: 10.892, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54132
[1mStep[0m  [2/21], [94mLoss[0m : 10.86402
[1mStep[0m  [4/21], [94mLoss[0m : 11.04194
[1mStep[0m  [6/21], [94mLoss[0m : 10.72883
[1mStep[0m  [8/21], [94mLoss[0m : 10.55737
[1mStep[0m  [10/21], [94mLoss[0m : 10.60499
[1mStep[0m  [12/21], [94mLoss[0m : 10.86200
[1mStep[0m  [14/21], [94mLoss[0m : 10.92399
[1mStep[0m  [16/21], [94mLoss[0m : 10.79087
[1mStep[0m  [18/21], [94mLoss[0m : 10.58334
[1mStep[0m  [20/21], [94mLoss[0m : 10.74765

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.802, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.80774
[1mStep[0m  [2/21], [94mLoss[0m : 10.80641
[1mStep[0m  [4/21], [94mLoss[0m : 10.65358
[1mStep[0m  [6/21], [94mLoss[0m : 10.68970
[1mStep[0m  [8/21], [94mLoss[0m : 10.45819
[1mStep[0m  [10/21], [94mLoss[0m : 10.95783
[1mStep[0m  [12/21], [94mLoss[0m : 10.92844
[1mStep[0m  [14/21], [94mLoss[0m : 10.76422
[1mStep[0m  [16/21], [94mLoss[0m : 10.79946
[1mStep[0m  [18/21], [94mLoss[0m : 10.94223
[1mStep[0m  [20/21], [94mLoss[0m : 10.49584

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.844, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39816
[1mStep[0m  [2/21], [94mLoss[0m : 10.97129
[1mStep[0m  [4/21], [94mLoss[0m : 10.75654
[1mStep[0m  [6/21], [94mLoss[0m : 10.46083
[1mStep[0m  [8/21], [94mLoss[0m : 10.76433
[1mStep[0m  [10/21], [94mLoss[0m : 10.62590
[1mStep[0m  [12/21], [94mLoss[0m : 10.55483
[1mStep[0m  [14/21], [94mLoss[0m : 10.82626
[1mStep[0m  [16/21], [94mLoss[0m : 10.92934
[1mStep[0m  [18/21], [94mLoss[0m : 10.75776
[1mStep[0m  [20/21], [94mLoss[0m : 10.62229

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.753, [92mTest[0m: 10.826, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93972
[1mStep[0m  [2/21], [94mLoss[0m : 10.95043
[1mStep[0m  [4/21], [94mLoss[0m : 10.47096
[1mStep[0m  [6/21], [94mLoss[0m : 10.76678
[1mStep[0m  [8/21], [94mLoss[0m : 10.87965
[1mStep[0m  [10/21], [94mLoss[0m : 10.84215
[1mStep[0m  [12/21], [94mLoss[0m : 10.60732
[1mStep[0m  [14/21], [94mLoss[0m : 10.73348
[1mStep[0m  [16/21], [94mLoss[0m : 10.82686
[1mStep[0m  [18/21], [94mLoss[0m : 10.57159
[1mStep[0m  [20/21], [94mLoss[0m : 10.52267

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.735, [92mTest[0m: 10.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71515
[1mStep[0m  [2/21], [94mLoss[0m : 10.69548
[1mStep[0m  [4/21], [94mLoss[0m : 10.58628
[1mStep[0m  [6/21], [94mLoss[0m : 10.59438
[1mStep[0m  [8/21], [94mLoss[0m : 11.01738
[1mStep[0m  [10/21], [94mLoss[0m : 10.57810
[1mStep[0m  [12/21], [94mLoss[0m : 10.58070
[1mStep[0m  [14/21], [94mLoss[0m : 10.83578
[1mStep[0m  [16/21], [94mLoss[0m : 10.86449
[1mStep[0m  [18/21], [94mLoss[0m : 10.92255
[1mStep[0m  [20/21], [94mLoss[0m : 10.93494

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.793, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70551
[1mStep[0m  [2/21], [94mLoss[0m : 10.73878
[1mStep[0m  [4/21], [94mLoss[0m : 10.34917
[1mStep[0m  [6/21], [94mLoss[0m : 10.64568
[1mStep[0m  [8/21], [94mLoss[0m : 10.86450
[1mStep[0m  [10/21], [94mLoss[0m : 10.55376
[1mStep[0m  [12/21], [94mLoss[0m : 10.92057
[1mStep[0m  [14/21], [94mLoss[0m : 10.79699
[1mStep[0m  [16/21], [94mLoss[0m : 10.55411
[1mStep[0m  [18/21], [94mLoss[0m : 10.83851
[1mStep[0m  [20/21], [94mLoss[0m : 10.42586

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.698, [92mTest[0m: 10.786, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55182
[1mStep[0m  [2/21], [94mLoss[0m : 10.46454
[1mStep[0m  [4/21], [94mLoss[0m : 10.81554
[1mStep[0m  [6/21], [94mLoss[0m : 10.74278
[1mStep[0m  [8/21], [94mLoss[0m : 10.64682
[1mStep[0m  [10/21], [94mLoss[0m : 10.68478
[1mStep[0m  [12/21], [94mLoss[0m : 10.68045
[1mStep[0m  [14/21], [94mLoss[0m : 10.71207
[1mStep[0m  [16/21], [94mLoss[0m : 10.41633
[1mStep[0m  [18/21], [94mLoss[0m : 10.85242
[1mStep[0m  [20/21], [94mLoss[0m : 10.57144

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.673, [92mTest[0m: 10.761, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.82528
[1mStep[0m  [2/21], [94mLoss[0m : 10.59256
[1mStep[0m  [4/21], [94mLoss[0m : 10.74257
[1mStep[0m  [6/21], [94mLoss[0m : 11.03725
[1mStep[0m  [8/21], [94mLoss[0m : 10.66896
[1mStep[0m  [10/21], [94mLoss[0m : 10.49287
[1mStep[0m  [12/21], [94mLoss[0m : 10.85730
[1mStep[0m  [14/21], [94mLoss[0m : 10.83051
[1mStep[0m  [16/21], [94mLoss[0m : 10.43808
[1mStep[0m  [18/21], [94mLoss[0m : 10.43940
[1mStep[0m  [20/21], [94mLoss[0m : 10.57701

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.755, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73235
[1mStep[0m  [2/21], [94mLoss[0m : 10.30604
[1mStep[0m  [4/21], [94mLoss[0m : 10.43199
[1mStep[0m  [6/21], [94mLoss[0m : 10.51940
[1mStep[0m  [8/21], [94mLoss[0m : 10.59189
[1mStep[0m  [10/21], [94mLoss[0m : 10.46794
[1mStep[0m  [12/21], [94mLoss[0m : 10.56672
[1mStep[0m  [14/21], [94mLoss[0m : 10.68963
[1mStep[0m  [16/21], [94mLoss[0m : 10.74582
[1mStep[0m  [18/21], [94mLoss[0m : 10.40387
[1mStep[0m  [20/21], [94mLoss[0m : 10.78166

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.738, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48474
[1mStep[0m  [2/21], [94mLoss[0m : 10.47478
[1mStep[0m  [4/21], [94mLoss[0m : 10.60023
[1mStep[0m  [6/21], [94mLoss[0m : 10.73029
[1mStep[0m  [8/21], [94mLoss[0m : 10.62360
[1mStep[0m  [10/21], [94mLoss[0m : 10.55408
[1mStep[0m  [12/21], [94mLoss[0m : 10.61871
[1mStep[0m  [14/21], [94mLoss[0m : 10.72342
[1mStep[0m  [16/21], [94mLoss[0m : 10.41940
[1mStep[0m  [18/21], [94mLoss[0m : 10.62200
[1mStep[0m  [20/21], [94mLoss[0m : 10.61493

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.717, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50092
[1mStep[0m  [2/21], [94mLoss[0m : 10.91008
[1mStep[0m  [4/21], [94mLoss[0m : 10.77180
[1mStep[0m  [6/21], [94mLoss[0m : 10.53192
[1mStep[0m  [8/21], [94mLoss[0m : 10.82699
[1mStep[0m  [10/21], [94mLoss[0m : 10.52970
[1mStep[0m  [12/21], [94mLoss[0m : 10.54352
[1mStep[0m  [14/21], [94mLoss[0m : 10.65102
[1mStep[0m  [16/21], [94mLoss[0m : 10.64213
[1mStep[0m  [18/21], [94mLoss[0m : 10.65086
[1mStep[0m  [20/21], [94mLoss[0m : 10.38007

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.592, [92mTest[0m: 10.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.66166
[1mStep[0m  [2/21], [94mLoss[0m : 10.38841
[1mStep[0m  [4/21], [94mLoss[0m : 10.78922
[1mStep[0m  [6/21], [94mLoss[0m : 10.40712
[1mStep[0m  [8/21], [94mLoss[0m : 10.56256
[1mStep[0m  [10/21], [94mLoss[0m : 10.45007
[1mStep[0m  [12/21], [94mLoss[0m : 10.95123
[1mStep[0m  [14/21], [94mLoss[0m : 10.53412
[1mStep[0m  [16/21], [94mLoss[0m : 10.87176
[1mStep[0m  [18/21], [94mLoss[0m : 10.52324
[1mStep[0m  [20/21], [94mLoss[0m : 10.46303

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.581, [92mTest[0m: 10.695, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69370
[1mStep[0m  [2/21], [94mLoss[0m : 10.54767
[1mStep[0m  [4/21], [94mLoss[0m : 10.47130
[1mStep[0m  [6/21], [94mLoss[0m : 10.47352
[1mStep[0m  [8/21], [94mLoss[0m : 10.75421
[1mStep[0m  [10/21], [94mLoss[0m : 10.49882
[1mStep[0m  [12/21], [94mLoss[0m : 10.56250
[1mStep[0m  [14/21], [94mLoss[0m : 10.37965
[1mStep[0m  [16/21], [94mLoss[0m : 10.72769
[1mStep[0m  [18/21], [94mLoss[0m : 10.77907
[1mStep[0m  [20/21], [94mLoss[0m : 10.36188

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.554, [92mTest[0m: 10.660, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59480
[1mStep[0m  [2/21], [94mLoss[0m : 10.31695
[1mStep[0m  [4/21], [94mLoss[0m : 10.68211
[1mStep[0m  [6/21], [94mLoss[0m : 10.43965
[1mStep[0m  [8/21], [94mLoss[0m : 10.69794
[1mStep[0m  [10/21], [94mLoss[0m : 10.75627
[1mStep[0m  [12/21], [94mLoss[0m : 10.66943
[1mStep[0m  [14/21], [94mLoss[0m : 10.32557
[1mStep[0m  [16/21], [94mLoss[0m : 10.29261
[1mStep[0m  [18/21], [94mLoss[0m : 10.62471
[1mStep[0m  [20/21], [94mLoss[0m : 10.32849

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.533, [92mTest[0m: 10.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49579
[1mStep[0m  [2/21], [94mLoss[0m : 10.57960
[1mStep[0m  [4/21], [94mLoss[0m : 10.56224
[1mStep[0m  [6/21], [94mLoss[0m : 10.66199
[1mStep[0m  [8/21], [94mLoss[0m : 10.39522
[1mStep[0m  [10/21], [94mLoss[0m : 10.72431
[1mStep[0m  [12/21], [94mLoss[0m : 10.64129
[1mStep[0m  [14/21], [94mLoss[0m : 10.80486
[1mStep[0m  [16/21], [94mLoss[0m : 10.22214
[1mStep[0m  [18/21], [94mLoss[0m : 10.71819
[1mStep[0m  [20/21], [94mLoss[0m : 10.58242

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.513, [92mTest[0m: 10.656, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48923
[1mStep[0m  [2/21], [94mLoss[0m : 10.59478
[1mStep[0m  [4/21], [94mLoss[0m : 10.59631
[1mStep[0m  [6/21], [94mLoss[0m : 10.39208
[1mStep[0m  [8/21], [94mLoss[0m : 10.50369
[1mStep[0m  [10/21], [94mLoss[0m : 10.40506
[1mStep[0m  [12/21], [94mLoss[0m : 10.55291
[1mStep[0m  [14/21], [94mLoss[0m : 10.35687
[1mStep[0m  [16/21], [94mLoss[0m : 10.33974
[1mStep[0m  [18/21], [94mLoss[0m : 10.43942
[1mStep[0m  [20/21], [94mLoss[0m : 10.32582

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.499, [92mTest[0m: 10.652, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62692
[1mStep[0m  [2/21], [94mLoss[0m : 10.45264
[1mStep[0m  [4/21], [94mLoss[0m : 10.18066
[1mStep[0m  [6/21], [94mLoss[0m : 10.39669
[1mStep[0m  [8/21], [94mLoss[0m : 10.54427
[1mStep[0m  [10/21], [94mLoss[0m : 10.51108
[1mStep[0m  [12/21], [94mLoss[0m : 10.39409
[1mStep[0m  [14/21], [94mLoss[0m : 10.37351
[1mStep[0m  [16/21], [94mLoss[0m : 10.59687
[1mStep[0m  [18/21], [94mLoss[0m : 10.69572
[1mStep[0m  [20/21], [94mLoss[0m : 10.68839

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.469, [92mTest[0m: 10.621, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51322
[1mStep[0m  [2/21], [94mLoss[0m : 10.54573
[1mStep[0m  [4/21], [94mLoss[0m : 10.57826
[1mStep[0m  [6/21], [94mLoss[0m : 10.27151
[1mStep[0m  [8/21], [94mLoss[0m : 10.64860
[1mStep[0m  [10/21], [94mLoss[0m : 10.34833
[1mStep[0m  [12/21], [94mLoss[0m : 11.06629
[1mStep[0m  [14/21], [94mLoss[0m : 10.46014
[1mStep[0m  [16/21], [94mLoss[0m : 10.38088
[1mStep[0m  [18/21], [94mLoss[0m : 10.31063
[1mStep[0m  [20/21], [94mLoss[0m : 10.49669

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.444, [92mTest[0m: 10.590, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38783
[1mStep[0m  [2/21], [94mLoss[0m : 10.28684
[1mStep[0m  [4/21], [94mLoss[0m : 10.27497
[1mStep[0m  [6/21], [94mLoss[0m : 10.59175
[1mStep[0m  [8/21], [94mLoss[0m : 10.54082
[1mStep[0m  [10/21], [94mLoss[0m : 10.63913
[1mStep[0m  [12/21], [94mLoss[0m : 10.38257
[1mStep[0m  [14/21], [94mLoss[0m : 10.38475
[1mStep[0m  [16/21], [94mLoss[0m : 10.59344
[1mStep[0m  [18/21], [94mLoss[0m : 10.57801
[1mStep[0m  [20/21], [94mLoss[0m : 10.48452

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.596, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42562
[1mStep[0m  [2/21], [94mLoss[0m : 10.31047
[1mStep[0m  [4/21], [94mLoss[0m : 10.84826
[1mStep[0m  [6/21], [94mLoss[0m : 10.44541
[1mStep[0m  [8/21], [94mLoss[0m : 10.20294
[1mStep[0m  [10/21], [94mLoss[0m : 10.20078
[1mStep[0m  [12/21], [94mLoss[0m : 10.31229
[1mStep[0m  [14/21], [94mLoss[0m : 10.49130
[1mStep[0m  [16/21], [94mLoss[0m : 10.33540
[1mStep[0m  [18/21], [94mLoss[0m : 10.63206
[1mStep[0m  [20/21], [94mLoss[0m : 10.77738

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.424, [92mTest[0m: 10.590, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49462
[1mStep[0m  [2/21], [94mLoss[0m : 10.42900
[1mStep[0m  [4/21], [94mLoss[0m : 10.40412
[1mStep[0m  [6/21], [94mLoss[0m : 10.36729
[1mStep[0m  [8/21], [94mLoss[0m : 10.36216
[1mStep[0m  [10/21], [94mLoss[0m : 10.39376
[1mStep[0m  [12/21], [94mLoss[0m : 10.37601
[1mStep[0m  [14/21], [94mLoss[0m : 10.34785
[1mStep[0m  [16/21], [94mLoss[0m : 10.36774
[1mStep[0m  [18/21], [94mLoss[0m : 10.06550
[1mStep[0m  [20/21], [94mLoss[0m : 10.48301

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.399, [92mTest[0m: 10.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38498
[1mStep[0m  [2/21], [94mLoss[0m : 10.41707
[1mStep[0m  [4/21], [94mLoss[0m : 10.67795
[1mStep[0m  [6/21], [94mLoss[0m : 10.42849
[1mStep[0m  [8/21], [94mLoss[0m : 10.71074
[1mStep[0m  [10/21], [94mLoss[0m : 10.41712
[1mStep[0m  [12/21], [94mLoss[0m : 10.31316
[1mStep[0m  [14/21], [94mLoss[0m : 10.52948
[1mStep[0m  [16/21], [94mLoss[0m : 10.19959
[1mStep[0m  [18/21], [94mLoss[0m : 10.26410
[1mStep[0m  [20/21], [94mLoss[0m : 10.27156

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.380, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42859
[1mStep[0m  [2/21], [94mLoss[0m : 10.41304
[1mStep[0m  [4/21], [94mLoss[0m : 9.90639
[1mStep[0m  [6/21], [94mLoss[0m : 10.30190
[1mStep[0m  [8/21], [94mLoss[0m : 10.25812
[1mStep[0m  [10/21], [94mLoss[0m : 10.35748
[1mStep[0m  [12/21], [94mLoss[0m : 10.17997
[1mStep[0m  [14/21], [94mLoss[0m : 10.65349
[1mStep[0m  [16/21], [94mLoss[0m : 10.61293
[1mStep[0m  [18/21], [94mLoss[0m : 10.20587
[1mStep[0m  [20/21], [94mLoss[0m : 9.99867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.365, [92mTest[0m: 10.539, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.26189
[1mStep[0m  [2/21], [94mLoss[0m : 10.50271
[1mStep[0m  [4/21], [94mLoss[0m : 10.39446
[1mStep[0m  [6/21], [94mLoss[0m : 10.31089
[1mStep[0m  [8/21], [94mLoss[0m : 10.60842
[1mStep[0m  [10/21], [94mLoss[0m : 10.60892
[1mStep[0m  [12/21], [94mLoss[0m : 10.62379
[1mStep[0m  [14/21], [94mLoss[0m : 10.41314
[1mStep[0m  [16/21], [94mLoss[0m : 10.09791
[1mStep[0m  [18/21], [94mLoss[0m : 10.45565
[1mStep[0m  [20/21], [94mLoss[0m : 10.18859

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.349, [92mTest[0m: 10.539, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.53070
[1mStep[0m  [2/21], [94mLoss[0m : 10.04481
[1mStep[0m  [4/21], [94mLoss[0m : 10.36427
[1mStep[0m  [6/21], [94mLoss[0m : 10.50115
[1mStep[0m  [8/21], [94mLoss[0m : 10.35854
[1mStep[0m  [10/21], [94mLoss[0m : 10.06889
[1mStep[0m  [12/21], [94mLoss[0m : 10.14069
[1mStep[0m  [14/21], [94mLoss[0m : 10.37562
[1mStep[0m  [16/21], [94mLoss[0m : 10.41853
[1mStep[0m  [18/21], [94mLoss[0m : 10.31366
[1mStep[0m  [20/21], [94mLoss[0m : 10.51269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.335, [92mTest[0m: 10.501, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56322
[1mStep[0m  [2/21], [94mLoss[0m : 10.29260
[1mStep[0m  [4/21], [94mLoss[0m : 10.35502
[1mStep[0m  [6/21], [94mLoss[0m : 10.06586
[1mStep[0m  [8/21], [94mLoss[0m : 10.33996
[1mStep[0m  [10/21], [94mLoss[0m : 10.20729
[1mStep[0m  [12/21], [94mLoss[0m : 10.37592
[1mStep[0m  [14/21], [94mLoss[0m : 10.05480
[1mStep[0m  [16/21], [94mLoss[0m : 10.30108
[1mStep[0m  [18/21], [94mLoss[0m : 10.51686
[1mStep[0m  [20/21], [94mLoss[0m : 10.39019

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.311, [92mTest[0m: 10.502, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33798
[1mStep[0m  [2/21], [94mLoss[0m : 10.15321
[1mStep[0m  [4/21], [94mLoss[0m : 10.33039
[1mStep[0m  [6/21], [94mLoss[0m : 10.17432
[1mStep[0m  [8/21], [94mLoss[0m : 10.18946
[1mStep[0m  [10/21], [94mLoss[0m : 10.43377
[1mStep[0m  [12/21], [94mLoss[0m : 10.16735
[1mStep[0m  [14/21], [94mLoss[0m : 10.36543
[1mStep[0m  [16/21], [94mLoss[0m : 10.40635
[1mStep[0m  [18/21], [94mLoss[0m : 10.20692
[1mStep[0m  [20/21], [94mLoss[0m : 10.21187

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.291, [92mTest[0m: 10.497, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.484
====================================

Phase 1 - Evaluation MAE:  10.484128679547991
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.51862
[1mStep[0m  [2/21], [94mLoss[0m : 10.35561
[1mStep[0m  [4/21], [94mLoss[0m : 10.50525
[1mStep[0m  [6/21], [94mLoss[0m : 10.23973
[1mStep[0m  [8/21], [94mLoss[0m : 10.23597
[1mStep[0m  [10/21], [94mLoss[0m : 10.10783
[1mStep[0m  [12/21], [94mLoss[0m : 10.41771
[1mStep[0m  [14/21], [94mLoss[0m : 10.21881
[1mStep[0m  [16/21], [94mLoss[0m : 10.14207
[1mStep[0m  [18/21], [94mLoss[0m : 9.99031
[1mStep[0m  [20/21], [94mLoss[0m : 9.89666

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.274, [92mTest[0m: 10.474, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.34413
[1mStep[0m  [2/21], [94mLoss[0m : 10.31664
[1mStep[0m  [4/21], [94mLoss[0m : 10.27763
[1mStep[0m  [6/21], [94mLoss[0m : 10.28880
[1mStep[0m  [8/21], [94mLoss[0m : 10.37864
[1mStep[0m  [10/21], [94mLoss[0m : 10.31184
[1mStep[0m  [12/21], [94mLoss[0m : 10.33079
[1mStep[0m  [14/21], [94mLoss[0m : 10.31239
[1mStep[0m  [16/21], [94mLoss[0m : 10.23516
[1mStep[0m  [18/21], [94mLoss[0m : 10.15929
[1mStep[0m  [20/21], [94mLoss[0m : 10.13230

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.262, [92mTest[0m: 10.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69764
[1mStep[0m  [2/21], [94mLoss[0m : 10.00450
[1mStep[0m  [4/21], [94mLoss[0m : 9.96648
[1mStep[0m  [6/21], [94mLoss[0m : 10.18555
[1mStep[0m  [8/21], [94mLoss[0m : 10.28917
[1mStep[0m  [10/21], [94mLoss[0m : 10.14511
[1mStep[0m  [12/21], [94mLoss[0m : 10.15244
[1mStep[0m  [14/21], [94mLoss[0m : 10.35598
[1mStep[0m  [16/21], [94mLoss[0m : 10.10102
[1mStep[0m  [18/21], [94mLoss[0m : 10.45381
[1mStep[0m  [20/21], [94mLoss[0m : 9.93094

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.231, [92mTest[0m: 10.436, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.03257
[1mStep[0m  [2/21], [94mLoss[0m : 10.32124
[1mStep[0m  [4/21], [94mLoss[0m : 10.08149
[1mStep[0m  [6/21], [94mLoss[0m : 10.45226
[1mStep[0m  [8/21], [94mLoss[0m : 10.21230
[1mStep[0m  [10/21], [94mLoss[0m : 10.18862
[1mStep[0m  [12/21], [94mLoss[0m : 10.22368
[1mStep[0m  [14/21], [94mLoss[0m : 10.08957
[1mStep[0m  [16/21], [94mLoss[0m : 10.05465
[1mStep[0m  [18/21], [94mLoss[0m : 10.34771
[1mStep[0m  [20/21], [94mLoss[0m : 10.23351

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.212, [92mTest[0m: 10.436, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.20733
[1mStep[0m  [2/21], [94mLoss[0m : 10.21807
[1mStep[0m  [4/21], [94mLoss[0m : 9.95527
[1mStep[0m  [6/21], [94mLoss[0m : 10.18001
[1mStep[0m  [8/21], [94mLoss[0m : 10.51976
[1mStep[0m  [10/21], [94mLoss[0m : 10.02597
[1mStep[0m  [12/21], [94mLoss[0m : 10.24502
[1mStep[0m  [14/21], [94mLoss[0m : 9.96314
[1mStep[0m  [16/21], [94mLoss[0m : 10.07714
[1mStep[0m  [18/21], [94mLoss[0m : 10.17740
[1mStep[0m  [20/21], [94mLoss[0m : 10.26953

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.194, [92mTest[0m: 10.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.21840
[1mStep[0m  [2/21], [94mLoss[0m : 10.40943
[1mStep[0m  [4/21], [94mLoss[0m : 10.14690
[1mStep[0m  [6/21], [94mLoss[0m : 10.10087
[1mStep[0m  [8/21], [94mLoss[0m : 10.19443
[1mStep[0m  [10/21], [94mLoss[0m : 10.09405
[1mStep[0m  [12/21], [94mLoss[0m : 9.75712
[1mStep[0m  [14/21], [94mLoss[0m : 10.17078
[1mStep[0m  [16/21], [94mLoss[0m : 10.09254
[1mStep[0m  [18/21], [94mLoss[0m : 10.23010
[1mStep[0m  [20/21], [94mLoss[0m : 9.98110

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.173, [92mTest[0m: 10.398, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.18776
[1mStep[0m  [2/21], [94mLoss[0m : 10.03475
[1mStep[0m  [4/21], [94mLoss[0m : 10.29275
[1mStep[0m  [6/21], [94mLoss[0m : 10.05281
[1mStep[0m  [8/21], [94mLoss[0m : 10.45752
[1mStep[0m  [10/21], [94mLoss[0m : 10.33295
[1mStep[0m  [12/21], [94mLoss[0m : 10.09518
[1mStep[0m  [14/21], [94mLoss[0m : 10.05115
[1mStep[0m  [16/21], [94mLoss[0m : 9.95526
[1mStep[0m  [18/21], [94mLoss[0m : 10.06658
[1mStep[0m  [20/21], [94mLoss[0m : 9.93629

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.151, [92mTest[0m: 10.387, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.16416
[1mStep[0m  [2/21], [94mLoss[0m : 10.00666
[1mStep[0m  [4/21], [94mLoss[0m : 10.23780
[1mStep[0m  [6/21], [94mLoss[0m : 10.06770
[1mStep[0m  [8/21], [94mLoss[0m : 9.87214
[1mStep[0m  [10/21], [94mLoss[0m : 10.29024
[1mStep[0m  [12/21], [94mLoss[0m : 10.22297
[1mStep[0m  [14/21], [94mLoss[0m : 10.10420
[1mStep[0m  [16/21], [94mLoss[0m : 10.15871
[1mStep[0m  [18/21], [94mLoss[0m : 10.16887
[1mStep[0m  [20/21], [94mLoss[0m : 10.19495

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.131, [92mTest[0m: 10.356, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.11240
[1mStep[0m  [2/21], [94mLoss[0m : 10.15940
[1mStep[0m  [4/21], [94mLoss[0m : 10.19822
[1mStep[0m  [6/21], [94mLoss[0m : 9.82258
[1mStep[0m  [8/21], [94mLoss[0m : 10.32815
[1mStep[0m  [10/21], [94mLoss[0m : 10.19612
[1mStep[0m  [12/21], [94mLoss[0m : 9.96198
[1mStep[0m  [14/21], [94mLoss[0m : 10.24499
[1mStep[0m  [16/21], [94mLoss[0m : 9.99381
[1mStep[0m  [18/21], [94mLoss[0m : 10.35766
[1mStep[0m  [20/21], [94mLoss[0m : 10.20655

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.114, [92mTest[0m: 10.348, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12078
[1mStep[0m  [2/21], [94mLoss[0m : 10.40697
[1mStep[0m  [4/21], [94mLoss[0m : 10.35920
[1mStep[0m  [6/21], [94mLoss[0m : 9.89353
[1mStep[0m  [8/21], [94mLoss[0m : 10.13623
[1mStep[0m  [10/21], [94mLoss[0m : 10.17216
[1mStep[0m  [12/21], [94mLoss[0m : 10.28064
[1mStep[0m  [14/21], [94mLoss[0m : 10.34121
[1mStep[0m  [16/21], [94mLoss[0m : 10.04530
[1mStep[0m  [18/21], [94mLoss[0m : 9.86022
[1mStep[0m  [20/21], [94mLoss[0m : 10.26941

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.095, [92mTest[0m: 10.337, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.02774
[1mStep[0m  [2/21], [94mLoss[0m : 9.99087
[1mStep[0m  [4/21], [94mLoss[0m : 10.17303
[1mStep[0m  [6/21], [94mLoss[0m : 10.14613
[1mStep[0m  [8/21], [94mLoss[0m : 9.89874
[1mStep[0m  [10/21], [94mLoss[0m : 10.05516
[1mStep[0m  [12/21], [94mLoss[0m : 10.21515
[1mStep[0m  [14/21], [94mLoss[0m : 10.49723
[1mStep[0m  [16/21], [94mLoss[0m : 9.92536
[1mStep[0m  [18/21], [94mLoss[0m : 10.04713
[1mStep[0m  [20/21], [94mLoss[0m : 10.09179

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.071, [92mTest[0m: 10.322, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12411
[1mStep[0m  [2/21], [94mLoss[0m : 9.96479
[1mStep[0m  [4/21], [94mLoss[0m : 10.13324
[1mStep[0m  [6/21], [94mLoss[0m : 9.93301
[1mStep[0m  [8/21], [94mLoss[0m : 10.21125
[1mStep[0m  [10/21], [94mLoss[0m : 10.30569
[1mStep[0m  [12/21], [94mLoss[0m : 9.97315
[1mStep[0m  [14/21], [94mLoss[0m : 9.94816
[1mStep[0m  [16/21], [94mLoss[0m : 10.26649
[1mStep[0m  [18/21], [94mLoss[0m : 10.04194
[1mStep[0m  [20/21], [94mLoss[0m : 10.28251

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.050, [92mTest[0m: 10.309, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12804
[1mStep[0m  [2/21], [94mLoss[0m : 9.89732
[1mStep[0m  [4/21], [94mLoss[0m : 9.73784
[1mStep[0m  [6/21], [94mLoss[0m : 10.37294
[1mStep[0m  [8/21], [94mLoss[0m : 9.67552
[1mStep[0m  [10/21], [94mLoss[0m : 9.85082
[1mStep[0m  [12/21], [94mLoss[0m : 10.04905
[1mStep[0m  [14/21], [94mLoss[0m : 10.15752
[1mStep[0m  [16/21], [94mLoss[0m : 10.22475
[1mStep[0m  [18/21], [94mLoss[0m : 10.06612
[1mStep[0m  [20/21], [94mLoss[0m : 10.15559

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.035, [92mTest[0m: 10.284, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.03966
[1mStep[0m  [2/21], [94mLoss[0m : 9.84909
[1mStep[0m  [4/21], [94mLoss[0m : 9.91828
[1mStep[0m  [6/21], [94mLoss[0m : 10.11730
[1mStep[0m  [8/21], [94mLoss[0m : 10.01319
[1mStep[0m  [10/21], [94mLoss[0m : 10.10531
[1mStep[0m  [12/21], [94mLoss[0m : 10.07945
[1mStep[0m  [14/21], [94mLoss[0m : 10.01849
[1mStep[0m  [16/21], [94mLoss[0m : 9.78046
[1mStep[0m  [18/21], [94mLoss[0m : 10.12303
[1mStep[0m  [20/21], [94mLoss[0m : 9.95276

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.011, [92mTest[0m: 10.264, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.99259
[1mStep[0m  [2/21], [94mLoss[0m : 10.20959
[1mStep[0m  [4/21], [94mLoss[0m : 9.87197
[1mStep[0m  [6/21], [94mLoss[0m : 10.09596
[1mStep[0m  [8/21], [94mLoss[0m : 10.03543
[1mStep[0m  [10/21], [94mLoss[0m : 10.15690
[1mStep[0m  [12/21], [94mLoss[0m : 10.16527
[1mStep[0m  [14/21], [94mLoss[0m : 9.81290
[1mStep[0m  [16/21], [94mLoss[0m : 9.88510
[1mStep[0m  [18/21], [94mLoss[0m : 9.92446
[1mStep[0m  [20/21], [94mLoss[0m : 9.90981

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.995, [92mTest[0m: 10.249, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.98458
[1mStep[0m  [2/21], [94mLoss[0m : 9.74164
[1mStep[0m  [4/21], [94mLoss[0m : 9.79714
[1mStep[0m  [6/21], [94mLoss[0m : 9.99935
[1mStep[0m  [8/21], [94mLoss[0m : 9.89127
[1mStep[0m  [10/21], [94mLoss[0m : 10.03776
[1mStep[0m  [12/21], [94mLoss[0m : 10.17017
[1mStep[0m  [14/21], [94mLoss[0m : 9.83726
[1mStep[0m  [16/21], [94mLoss[0m : 10.32340
[1mStep[0m  [18/21], [94mLoss[0m : 10.22105
[1mStep[0m  [20/21], [94mLoss[0m : 10.10445

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.968, [92mTest[0m: 10.252, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.92882
[1mStep[0m  [2/21], [94mLoss[0m : 9.76065
[1mStep[0m  [4/21], [94mLoss[0m : 10.09717
[1mStep[0m  [6/21], [94mLoss[0m : 9.69195
[1mStep[0m  [8/21], [94mLoss[0m : 10.05680
[1mStep[0m  [10/21], [94mLoss[0m : 10.03452
[1mStep[0m  [12/21], [94mLoss[0m : 9.93849
[1mStep[0m  [14/21], [94mLoss[0m : 9.67106
[1mStep[0m  [16/21], [94mLoss[0m : 10.06363
[1mStep[0m  [18/21], [94mLoss[0m : 10.08353
[1mStep[0m  [20/21], [94mLoss[0m : 10.12657

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.950, [92mTest[0m: 10.223, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.01344
[1mStep[0m  [2/21], [94mLoss[0m : 10.07987
[1mStep[0m  [4/21], [94mLoss[0m : 9.90352
[1mStep[0m  [6/21], [94mLoss[0m : 9.78833
[1mStep[0m  [8/21], [94mLoss[0m : 9.74317
[1mStep[0m  [10/21], [94mLoss[0m : 9.99133
[1mStep[0m  [12/21], [94mLoss[0m : 10.01841
[1mStep[0m  [14/21], [94mLoss[0m : 9.87989
[1mStep[0m  [16/21], [94mLoss[0m : 10.02960
[1mStep[0m  [18/21], [94mLoss[0m : 9.85040
[1mStep[0m  [20/21], [94mLoss[0m : 9.57262

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.928, [92mTest[0m: 10.222, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.96532
[1mStep[0m  [2/21], [94mLoss[0m : 10.06691
[1mStep[0m  [4/21], [94mLoss[0m : 9.98177
[1mStep[0m  [6/21], [94mLoss[0m : 9.85611
[1mStep[0m  [8/21], [94mLoss[0m : 10.02791
[1mStep[0m  [10/21], [94mLoss[0m : 9.95880
[1mStep[0m  [12/21], [94mLoss[0m : 9.72265
[1mStep[0m  [14/21], [94mLoss[0m : 9.90154
[1mStep[0m  [16/21], [94mLoss[0m : 9.82573
[1mStep[0m  [18/21], [94mLoss[0m : 9.83211
[1mStep[0m  [20/21], [94mLoss[0m : 10.04576

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.915, [92mTest[0m: 10.190, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.91779
[1mStep[0m  [2/21], [94mLoss[0m : 10.16265
[1mStep[0m  [4/21], [94mLoss[0m : 9.88124
[1mStep[0m  [6/21], [94mLoss[0m : 9.80579
[1mStep[0m  [8/21], [94mLoss[0m : 9.96708
[1mStep[0m  [10/21], [94mLoss[0m : 9.91689
[1mStep[0m  [12/21], [94mLoss[0m : 10.15569
[1mStep[0m  [14/21], [94mLoss[0m : 9.67599
[1mStep[0m  [16/21], [94mLoss[0m : 9.72613
[1mStep[0m  [18/21], [94mLoss[0m : 9.99832
[1mStep[0m  [20/21], [94mLoss[0m : 9.73817

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.888, [92mTest[0m: 10.185, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.56433
[1mStep[0m  [2/21], [94mLoss[0m : 10.18258
[1mStep[0m  [4/21], [94mLoss[0m : 10.06995
[1mStep[0m  [6/21], [94mLoss[0m : 9.78465
[1mStep[0m  [8/21], [94mLoss[0m : 9.66938
[1mStep[0m  [10/21], [94mLoss[0m : 9.90461
[1mStep[0m  [12/21], [94mLoss[0m : 9.85769
[1mStep[0m  [14/21], [94mLoss[0m : 9.90511
[1mStep[0m  [16/21], [94mLoss[0m : 9.87954
[1mStep[0m  [18/21], [94mLoss[0m : 9.62869
[1mStep[0m  [20/21], [94mLoss[0m : 9.52113

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.870, [92mTest[0m: 10.171, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.92937
[1mStep[0m  [2/21], [94mLoss[0m : 9.76524
[1mStep[0m  [4/21], [94mLoss[0m : 9.53053
[1mStep[0m  [6/21], [94mLoss[0m : 9.57029
[1mStep[0m  [8/21], [94mLoss[0m : 9.94769
[1mStep[0m  [10/21], [94mLoss[0m : 10.06766
[1mStep[0m  [12/21], [94mLoss[0m : 9.84838
[1mStep[0m  [14/21], [94mLoss[0m : 9.71771
[1mStep[0m  [16/21], [94mLoss[0m : 9.68285
[1mStep[0m  [18/21], [94mLoss[0m : 9.90341
[1mStep[0m  [20/21], [94mLoss[0m : 10.08840

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.847, [92mTest[0m: 10.151, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.76495
[1mStep[0m  [2/21], [94mLoss[0m : 9.98415
[1mStep[0m  [4/21], [94mLoss[0m : 9.81076
[1mStep[0m  [6/21], [94mLoss[0m : 10.05202
[1mStep[0m  [8/21], [94mLoss[0m : 9.85138
[1mStep[0m  [10/21], [94mLoss[0m : 9.85977
[1mStep[0m  [12/21], [94mLoss[0m : 9.73576
[1mStep[0m  [14/21], [94mLoss[0m : 10.11618
[1mStep[0m  [16/21], [94mLoss[0m : 9.94437
[1mStep[0m  [18/21], [94mLoss[0m : 9.81057
[1mStep[0m  [20/21], [94mLoss[0m : 9.65360

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.834, [92mTest[0m: 10.129, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.12944
[1mStep[0m  [2/21], [94mLoss[0m : 9.91129
[1mStep[0m  [4/21], [94mLoss[0m : 9.75453
[1mStep[0m  [6/21], [94mLoss[0m : 9.84231
[1mStep[0m  [8/21], [94mLoss[0m : 9.60901
[1mStep[0m  [10/21], [94mLoss[0m : 9.86445
[1mStep[0m  [12/21], [94mLoss[0m : 9.70645
[1mStep[0m  [14/21], [94mLoss[0m : 9.84614
[1mStep[0m  [16/21], [94mLoss[0m : 9.61516
[1mStep[0m  [18/21], [94mLoss[0m : 9.94663
[1mStep[0m  [20/21], [94mLoss[0m : 9.81259

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.815, [92mTest[0m: 10.125, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.36321
[1mStep[0m  [2/21], [94mLoss[0m : 9.75195
[1mStep[0m  [4/21], [94mLoss[0m : 9.73168
[1mStep[0m  [6/21], [94mLoss[0m : 9.84631
[1mStep[0m  [8/21], [94mLoss[0m : 9.83886
[1mStep[0m  [10/21], [94mLoss[0m : 9.89054
[1mStep[0m  [12/21], [94mLoss[0m : 9.63541
[1mStep[0m  [14/21], [94mLoss[0m : 9.83723
[1mStep[0m  [16/21], [94mLoss[0m : 9.88861
[1mStep[0m  [18/21], [94mLoss[0m : 9.99054
[1mStep[0m  [20/21], [94mLoss[0m : 9.85331

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.789, [92mTest[0m: 10.101, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.57275
[1mStep[0m  [2/21], [94mLoss[0m : 9.89156
[1mStep[0m  [4/21], [94mLoss[0m : 9.83262
[1mStep[0m  [6/21], [94mLoss[0m : 9.76880
[1mStep[0m  [8/21], [94mLoss[0m : 9.76790
[1mStep[0m  [10/21], [94mLoss[0m : 9.87058
[1mStep[0m  [12/21], [94mLoss[0m : 9.89684
[1mStep[0m  [14/21], [94mLoss[0m : 9.87017
[1mStep[0m  [16/21], [94mLoss[0m : 9.86117
[1mStep[0m  [18/21], [94mLoss[0m : 9.75804
[1mStep[0m  [20/21], [94mLoss[0m : 9.76042

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.783, [92mTest[0m: 10.090, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.86124
[1mStep[0m  [2/21], [94mLoss[0m : 9.76402
[1mStep[0m  [4/21], [94mLoss[0m : 9.87136
[1mStep[0m  [6/21], [94mLoss[0m : 9.69795
[1mStep[0m  [8/21], [94mLoss[0m : 9.57903
[1mStep[0m  [10/21], [94mLoss[0m : 9.75540
[1mStep[0m  [12/21], [94mLoss[0m : 9.76082
[1mStep[0m  [14/21], [94mLoss[0m : 9.90822
[1mStep[0m  [16/21], [94mLoss[0m : 10.06294
[1mStep[0m  [18/21], [94mLoss[0m : 9.26036
[1mStep[0m  [20/21], [94mLoss[0m : 9.94359

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.756, [92mTest[0m: 10.074, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.64848
[1mStep[0m  [2/21], [94mLoss[0m : 9.78836
[1mStep[0m  [4/21], [94mLoss[0m : 9.76572
[1mStep[0m  [6/21], [94mLoss[0m : 9.91491
[1mStep[0m  [8/21], [94mLoss[0m : 9.71219
[1mStep[0m  [10/21], [94mLoss[0m : 9.64843
[1mStep[0m  [12/21], [94mLoss[0m : 9.56553
[1mStep[0m  [14/21], [94mLoss[0m : 9.65429
[1mStep[0m  [16/21], [94mLoss[0m : 9.62719
[1mStep[0m  [18/21], [94mLoss[0m : 9.96106
[1mStep[0m  [20/21], [94mLoss[0m : 9.83337

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.739, [92mTest[0m: 10.062, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.72950
[1mStep[0m  [2/21], [94mLoss[0m : 9.57134
[1mStep[0m  [4/21], [94mLoss[0m : 9.85593
[1mStep[0m  [6/21], [94mLoss[0m : 10.09433
[1mStep[0m  [8/21], [94mLoss[0m : 9.80365
[1mStep[0m  [10/21], [94mLoss[0m : 9.64398
[1mStep[0m  [12/21], [94mLoss[0m : 9.82166
[1mStep[0m  [14/21], [94mLoss[0m : 9.71068
[1mStep[0m  [16/21], [94mLoss[0m : 9.61265
[1mStep[0m  [18/21], [94mLoss[0m : 9.53710
[1mStep[0m  [20/21], [94mLoss[0m : 9.61491

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.730, [92mTest[0m: 10.050, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.64469
[1mStep[0m  [2/21], [94mLoss[0m : 9.55217
[1mStep[0m  [4/21], [94mLoss[0m : 9.58945
[1mStep[0m  [6/21], [94mLoss[0m : 9.78346
[1mStep[0m  [8/21], [94mLoss[0m : 9.66952
[1mStep[0m  [10/21], [94mLoss[0m : 9.74938
[1mStep[0m  [12/21], [94mLoss[0m : 9.47065
[1mStep[0m  [14/21], [94mLoss[0m : 9.62626
[1mStep[0m  [16/21], [94mLoss[0m : 9.89289
[1mStep[0m  [18/21], [94mLoss[0m : 9.71708
[1mStep[0m  [20/21], [94mLoss[0m : 9.81400

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.704, [92mTest[0m: 10.033, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.028
====================================

Phase 2 - Evaluation MAE:  10.028155735560826
MAE score P1      10.484129
MAE score P2      10.028156
loss               9.703972
learning_rate        0.0001
batch_size              512
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay         0.0001
Name: 15, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 11.09812
[1mStep[0m  [4/42], [94mLoss[0m : 10.83007
[1mStep[0m  [8/42], [94mLoss[0m : 10.59650
[1mStep[0m  [12/42], [94mLoss[0m : 10.63613
[1mStep[0m  [16/42], [94mLoss[0m : 10.91133
[1mStep[0m  [20/42], [94mLoss[0m : 10.66819
[1mStep[0m  [24/42], [94mLoss[0m : 11.11251
[1mStep[0m  [28/42], [94mLoss[0m : 11.00615
[1mStep[0m  [32/42], [94mLoss[0m : 10.81480
[1mStep[0m  [36/42], [94mLoss[0m : 10.98125
[1mStep[0m  [40/42], [94mLoss[0m : 11.00685

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.797, [92mTest[0m: 10.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.96878
[1mStep[0m  [4/42], [94mLoss[0m : 10.64500
[1mStep[0m  [8/42], [94mLoss[0m : 10.86745
[1mStep[0m  [12/42], [94mLoss[0m : 10.15519
[1mStep[0m  [16/42], [94mLoss[0m : 11.15183
[1mStep[0m  [20/42], [94mLoss[0m : 11.09213
[1mStep[0m  [24/42], [94mLoss[0m : 10.81284
[1mStep[0m  [28/42], [94mLoss[0m : 10.90266
[1mStep[0m  [32/42], [94mLoss[0m : 11.32300
[1mStep[0m  [36/42], [94mLoss[0m : 10.30333
[1mStep[0m  [40/42], [94mLoss[0m : 10.67817

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.782, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68336
[1mStep[0m  [4/42], [94mLoss[0m : 10.91947
[1mStep[0m  [8/42], [94mLoss[0m : 10.59996
[1mStep[0m  [12/42], [94mLoss[0m : 10.83839
[1mStep[0m  [16/42], [94mLoss[0m : 10.88067
[1mStep[0m  [20/42], [94mLoss[0m : 10.58953
[1mStep[0m  [24/42], [94mLoss[0m : 11.19920
[1mStep[0m  [28/42], [94mLoss[0m : 10.71207
[1mStep[0m  [32/42], [94mLoss[0m : 10.50085
[1mStep[0m  [36/42], [94mLoss[0m : 10.80603
[1mStep[0m  [40/42], [94mLoss[0m : 11.11483

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.775, [92mTest[0m: 10.783, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87232
[1mStep[0m  [4/42], [94mLoss[0m : 10.62156
[1mStep[0m  [8/42], [94mLoss[0m : 11.24263
[1mStep[0m  [12/42], [94mLoss[0m : 10.83134
[1mStep[0m  [16/42], [94mLoss[0m : 11.00046
[1mStep[0m  [20/42], [94mLoss[0m : 10.55009
[1mStep[0m  [24/42], [94mLoss[0m : 10.53691
[1mStep[0m  [28/42], [94mLoss[0m : 10.73249
[1mStep[0m  [32/42], [94mLoss[0m : 10.53654
[1mStep[0m  [36/42], [94mLoss[0m : 10.41748
[1mStep[0m  [40/42], [94mLoss[0m : 10.97970

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.769, [92mTest[0m: 10.776, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.26293
[1mStep[0m  [4/42], [94mLoss[0m : 10.62080
[1mStep[0m  [8/42], [94mLoss[0m : 10.70439
[1mStep[0m  [12/42], [94mLoss[0m : 10.71064
[1mStep[0m  [16/42], [94mLoss[0m : 10.73553
[1mStep[0m  [20/42], [94mLoss[0m : 10.47544
[1mStep[0m  [24/42], [94mLoss[0m : 10.89674
[1mStep[0m  [28/42], [94mLoss[0m : 10.98372
[1mStep[0m  [32/42], [94mLoss[0m : 10.88946
[1mStep[0m  [36/42], [94mLoss[0m : 10.67675
[1mStep[0m  [40/42], [94mLoss[0m : 10.95351

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.746, [92mTest[0m: 10.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.86738
[1mStep[0m  [4/42], [94mLoss[0m : 10.78749
[1mStep[0m  [8/42], [94mLoss[0m : 10.42562
[1mStep[0m  [12/42], [94mLoss[0m : 10.75017
[1mStep[0m  [16/42], [94mLoss[0m : 11.02794
[1mStep[0m  [20/42], [94mLoss[0m : 10.95021
[1mStep[0m  [24/42], [94mLoss[0m : 10.63666
[1mStep[0m  [28/42], [94mLoss[0m : 10.30179
[1mStep[0m  [32/42], [94mLoss[0m : 10.16071
[1mStep[0m  [36/42], [94mLoss[0m : 10.81349
[1mStep[0m  [40/42], [94mLoss[0m : 10.68455

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.747, [92mTest[0m: 10.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.93324
[1mStep[0m  [4/42], [94mLoss[0m : 10.68566
[1mStep[0m  [8/42], [94mLoss[0m : 10.77045
[1mStep[0m  [12/42], [94mLoss[0m : 10.92581
[1mStep[0m  [16/42], [94mLoss[0m : 10.76992
[1mStep[0m  [20/42], [94mLoss[0m : 10.62531
[1mStep[0m  [24/42], [94mLoss[0m : 10.83173
[1mStep[0m  [28/42], [94mLoss[0m : 11.03581
[1mStep[0m  [32/42], [94mLoss[0m : 10.13155
[1mStep[0m  [36/42], [94mLoss[0m : 10.66688
[1mStep[0m  [40/42], [94mLoss[0m : 10.75683

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.731, [92mTest[0m: 10.723, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.71594
[1mStep[0m  [4/42], [94mLoss[0m : 10.61028
[1mStep[0m  [8/42], [94mLoss[0m : 10.75141
[1mStep[0m  [12/42], [94mLoss[0m : 10.81775
[1mStep[0m  [16/42], [94mLoss[0m : 10.88198
[1mStep[0m  [20/42], [94mLoss[0m : 10.72916
[1mStep[0m  [24/42], [94mLoss[0m : 10.40277
[1mStep[0m  [28/42], [94mLoss[0m : 10.86230
[1mStep[0m  [32/42], [94mLoss[0m : 10.46609
[1mStep[0m  [36/42], [94mLoss[0m : 11.14707
[1mStep[0m  [40/42], [94mLoss[0m : 11.22623

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.727, [92mTest[0m: 10.708, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.78326
[1mStep[0m  [4/42], [94mLoss[0m : 10.70382
[1mStep[0m  [8/42], [94mLoss[0m : 11.03102
[1mStep[0m  [12/42], [94mLoss[0m : 10.73313
[1mStep[0m  [16/42], [94mLoss[0m : 10.50530
[1mStep[0m  [20/42], [94mLoss[0m : 10.53727
[1mStep[0m  [24/42], [94mLoss[0m : 10.64206
[1mStep[0m  [28/42], [94mLoss[0m : 10.96455
[1mStep[0m  [32/42], [94mLoss[0m : 10.45689
[1mStep[0m  [36/42], [94mLoss[0m : 10.80702
[1mStep[0m  [40/42], [94mLoss[0m : 10.54140

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.715, [92mTest[0m: 10.698, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.45989
[1mStep[0m  [4/42], [94mLoss[0m : 11.01884
[1mStep[0m  [8/42], [94mLoss[0m : 10.76535
[1mStep[0m  [12/42], [94mLoss[0m : 10.55476
[1mStep[0m  [16/42], [94mLoss[0m : 10.72657
[1mStep[0m  [20/42], [94mLoss[0m : 10.60746
[1mStep[0m  [24/42], [94mLoss[0m : 10.65313
[1mStep[0m  [28/42], [94mLoss[0m : 10.77123
[1mStep[0m  [32/42], [94mLoss[0m : 11.04042
[1mStep[0m  [36/42], [94mLoss[0m : 10.55627
[1mStep[0m  [40/42], [94mLoss[0m : 10.45248

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.706, [92mTest[0m: 10.676, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.19740
[1mStep[0m  [4/42], [94mLoss[0m : 10.53821
[1mStep[0m  [8/42], [94mLoss[0m : 10.50403
[1mStep[0m  [12/42], [94mLoss[0m : 10.79042
[1mStep[0m  [16/42], [94mLoss[0m : 10.74036
[1mStep[0m  [20/42], [94mLoss[0m : 10.54931
[1mStep[0m  [24/42], [94mLoss[0m : 11.03810
[1mStep[0m  [28/42], [94mLoss[0m : 10.47702
[1mStep[0m  [32/42], [94mLoss[0m : 10.34385
[1mStep[0m  [36/42], [94mLoss[0m : 10.61027
[1mStep[0m  [40/42], [94mLoss[0m : 10.84114

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.698, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87382
[1mStep[0m  [4/42], [94mLoss[0m : 11.05296
[1mStep[0m  [8/42], [94mLoss[0m : 10.84857
[1mStep[0m  [12/42], [94mLoss[0m : 10.49312
[1mStep[0m  [16/42], [94mLoss[0m : 10.48460
[1mStep[0m  [20/42], [94mLoss[0m : 10.51826
[1mStep[0m  [24/42], [94mLoss[0m : 10.76179
[1mStep[0m  [28/42], [94mLoss[0m : 10.68988
[1mStep[0m  [32/42], [94mLoss[0m : 10.89005
[1mStep[0m  [36/42], [94mLoss[0m : 10.59086
[1mStep[0m  [40/42], [94mLoss[0m : 10.64384

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.682, [92mTest[0m: 10.667, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.27008
[1mStep[0m  [4/42], [94mLoss[0m : 10.65969
[1mStep[0m  [8/42], [94mLoss[0m : 10.60461
[1mStep[0m  [12/42], [94mLoss[0m : 10.75570
[1mStep[0m  [16/42], [94mLoss[0m : 10.70611
[1mStep[0m  [20/42], [94mLoss[0m : 10.61771
[1mStep[0m  [24/42], [94mLoss[0m : 10.83909
[1mStep[0m  [28/42], [94mLoss[0m : 10.26931
[1mStep[0m  [32/42], [94mLoss[0m : 10.97088
[1mStep[0m  [36/42], [94mLoss[0m : 11.00021
[1mStep[0m  [40/42], [94mLoss[0m : 10.71772

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.676, [92mTest[0m: 10.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.02755
[1mStep[0m  [4/42], [94mLoss[0m : 10.77289
[1mStep[0m  [8/42], [94mLoss[0m : 10.76126
[1mStep[0m  [12/42], [94mLoss[0m : 10.98487
[1mStep[0m  [16/42], [94mLoss[0m : 10.80556
[1mStep[0m  [20/42], [94mLoss[0m : 10.30028
[1mStep[0m  [24/42], [94mLoss[0m : 10.27805
[1mStep[0m  [28/42], [94mLoss[0m : 10.04267
[1mStep[0m  [32/42], [94mLoss[0m : 10.87477
[1mStep[0m  [36/42], [94mLoss[0m : 11.04045
[1mStep[0m  [40/42], [94mLoss[0m : 10.59484

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.635, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.35808
[1mStep[0m  [4/42], [94mLoss[0m : 10.31478
[1mStep[0m  [8/42], [94mLoss[0m : 10.70710
[1mStep[0m  [12/42], [94mLoss[0m : 10.73037
[1mStep[0m  [16/42], [94mLoss[0m : 10.79380
[1mStep[0m  [20/42], [94mLoss[0m : 10.65142
[1mStep[0m  [24/42], [94mLoss[0m : 10.87093
[1mStep[0m  [28/42], [94mLoss[0m : 10.47126
[1mStep[0m  [32/42], [94mLoss[0m : 10.79951
[1mStep[0m  [36/42], [94mLoss[0m : 10.73876
[1mStep[0m  [40/42], [94mLoss[0m : 10.53805

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.652, [92mTest[0m: 10.618, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.40650
[1mStep[0m  [4/42], [94mLoss[0m : 10.67480
[1mStep[0m  [8/42], [94mLoss[0m : 10.37489
[1mStep[0m  [12/42], [94mLoss[0m : 10.69351
[1mStep[0m  [16/42], [94mLoss[0m : 10.62838
[1mStep[0m  [20/42], [94mLoss[0m : 10.81192
[1mStep[0m  [24/42], [94mLoss[0m : 11.01495
[1mStep[0m  [28/42], [94mLoss[0m : 10.51709
[1mStep[0m  [32/42], [94mLoss[0m : 10.48916
[1mStep[0m  [36/42], [94mLoss[0m : 11.09043
[1mStep[0m  [40/42], [94mLoss[0m : 11.17511

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.649, [92mTest[0m: 10.613, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.59919
[1mStep[0m  [4/42], [94mLoss[0m : 10.59896
[1mStep[0m  [8/42], [94mLoss[0m : 10.29855
[1mStep[0m  [12/42], [94mLoss[0m : 10.63521
[1mStep[0m  [16/42], [94mLoss[0m : 10.60605
[1mStep[0m  [20/42], [94mLoss[0m : 10.56716
[1mStep[0m  [24/42], [94mLoss[0m : 10.68092
[1mStep[0m  [28/42], [94mLoss[0m : 10.42003
[1mStep[0m  [32/42], [94mLoss[0m : 10.77576
[1mStep[0m  [36/42], [94mLoss[0m : 10.83388
[1mStep[0m  [40/42], [94mLoss[0m : 10.43462

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.624, [92mTest[0m: 10.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.78377
[1mStep[0m  [4/42], [94mLoss[0m : 10.76747
[1mStep[0m  [8/42], [94mLoss[0m : 10.90254
[1mStep[0m  [12/42], [94mLoss[0m : 10.09419
[1mStep[0m  [16/42], [94mLoss[0m : 10.31381
[1mStep[0m  [20/42], [94mLoss[0m : 10.89168
[1mStep[0m  [24/42], [94mLoss[0m : 10.90259
[1mStep[0m  [28/42], [94mLoss[0m : 10.57897
[1mStep[0m  [32/42], [94mLoss[0m : 10.65681
[1mStep[0m  [36/42], [94mLoss[0m : 10.86053
[1mStep[0m  [40/42], [94mLoss[0m : 10.60820

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.625, [92mTest[0m: 10.592, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.62084
[1mStep[0m  [4/42], [94mLoss[0m : 10.50706
[1mStep[0m  [8/42], [94mLoss[0m : 10.59724
[1mStep[0m  [12/42], [94mLoss[0m : 10.65635
[1mStep[0m  [16/42], [94mLoss[0m : 10.91965
[1mStep[0m  [20/42], [94mLoss[0m : 10.56200
[1mStep[0m  [24/42], [94mLoss[0m : 10.67013
[1mStep[0m  [28/42], [94mLoss[0m : 10.58110
[1mStep[0m  [32/42], [94mLoss[0m : 10.50840
[1mStep[0m  [36/42], [94mLoss[0m : 10.94072
[1mStep[0m  [40/42], [94mLoss[0m : 10.47951

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.617, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.50483
[1mStep[0m  [4/42], [94mLoss[0m : 10.57801
[1mStep[0m  [8/42], [94mLoss[0m : 10.55875
[1mStep[0m  [12/42], [94mLoss[0m : 10.68531
[1mStep[0m  [16/42], [94mLoss[0m : 10.65511
[1mStep[0m  [20/42], [94mLoss[0m : 10.31361
[1mStep[0m  [24/42], [94mLoss[0m : 10.80280
[1mStep[0m  [28/42], [94mLoss[0m : 10.55790
[1mStep[0m  [32/42], [94mLoss[0m : 10.33385
[1mStep[0m  [36/42], [94mLoss[0m : 10.65796
[1mStep[0m  [40/42], [94mLoss[0m : 10.80729

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.602, [92mTest[0m: 10.558, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.55321
[1mStep[0m  [4/42], [94mLoss[0m : 10.58512
[1mStep[0m  [8/42], [94mLoss[0m : 10.48604
[1mStep[0m  [12/42], [94mLoss[0m : 10.71070
[1mStep[0m  [16/42], [94mLoss[0m : 10.82338
[1mStep[0m  [20/42], [94mLoss[0m : 10.27906
[1mStep[0m  [24/42], [94mLoss[0m : 10.63499
[1mStep[0m  [28/42], [94mLoss[0m : 10.20333
[1mStep[0m  [32/42], [94mLoss[0m : 10.69474
[1mStep[0m  [36/42], [94mLoss[0m : 10.74452
[1mStep[0m  [40/42], [94mLoss[0m : 10.92777

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.585, [92mTest[0m: 10.553, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26087
[1mStep[0m  [4/42], [94mLoss[0m : 10.80974
[1mStep[0m  [8/42], [94mLoss[0m : 10.60258
[1mStep[0m  [12/42], [94mLoss[0m : 10.33277
[1mStep[0m  [16/42], [94mLoss[0m : 10.61181
[1mStep[0m  [20/42], [94mLoss[0m : 10.65877
[1mStep[0m  [24/42], [94mLoss[0m : 10.40141
[1mStep[0m  [28/42], [94mLoss[0m : 10.37055
[1mStep[0m  [32/42], [94mLoss[0m : 10.82607
[1mStep[0m  [36/42], [94mLoss[0m : 10.85595
[1mStep[0m  [40/42], [94mLoss[0m : 10.56708

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.588, [92mTest[0m: 10.520, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.45955
[1mStep[0m  [4/42], [94mLoss[0m : 10.38024
[1mStep[0m  [8/42], [94mLoss[0m : 10.51328
[1mStep[0m  [12/42], [94mLoss[0m : 10.40187
[1mStep[0m  [16/42], [94mLoss[0m : 10.93972
[1mStep[0m  [20/42], [94mLoss[0m : 10.25049
[1mStep[0m  [24/42], [94mLoss[0m : 10.86192
[1mStep[0m  [28/42], [94mLoss[0m : 11.05920
[1mStep[0m  [32/42], [94mLoss[0m : 10.62309
[1mStep[0m  [36/42], [94mLoss[0m : 10.57272
[1mStep[0m  [40/42], [94mLoss[0m : 10.84162

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.580, [92mTest[0m: 10.531, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76592
[1mStep[0m  [4/42], [94mLoss[0m : 11.12722
[1mStep[0m  [8/42], [94mLoss[0m : 10.58664
[1mStep[0m  [12/42], [94mLoss[0m : 10.81048
[1mStep[0m  [16/42], [94mLoss[0m : 10.88314
[1mStep[0m  [20/42], [94mLoss[0m : 10.68482
[1mStep[0m  [24/42], [94mLoss[0m : 10.49254
[1mStep[0m  [28/42], [94mLoss[0m : 10.62770
[1mStep[0m  [32/42], [94mLoss[0m : 10.32243
[1mStep[0m  [36/42], [94mLoss[0m : 10.43943
[1mStep[0m  [40/42], [94mLoss[0m : 10.30111

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.559, [92mTest[0m: 10.498, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53713
[1mStep[0m  [4/42], [94mLoss[0m : 10.52101
[1mStep[0m  [8/42], [94mLoss[0m : 10.50755
[1mStep[0m  [12/42], [94mLoss[0m : 10.69008
[1mStep[0m  [16/42], [94mLoss[0m : 10.54617
[1mStep[0m  [20/42], [94mLoss[0m : 10.41992
[1mStep[0m  [24/42], [94mLoss[0m : 10.58759
[1mStep[0m  [28/42], [94mLoss[0m : 10.73346
[1mStep[0m  [32/42], [94mLoss[0m : 10.61013
[1mStep[0m  [36/42], [94mLoss[0m : 10.32809
[1mStep[0m  [40/42], [94mLoss[0m : 10.29924

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.506, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51276
[1mStep[0m  [4/42], [94mLoss[0m : 11.06312
[1mStep[0m  [8/42], [94mLoss[0m : 10.36140
[1mStep[0m  [12/42], [94mLoss[0m : 10.34530
[1mStep[0m  [16/42], [94mLoss[0m : 10.92306
[1mStep[0m  [20/42], [94mLoss[0m : 10.56403
[1mStep[0m  [24/42], [94mLoss[0m : 10.51594
[1mStep[0m  [28/42], [94mLoss[0m : 10.63792
[1mStep[0m  [32/42], [94mLoss[0m : 10.36569
[1mStep[0m  [36/42], [94mLoss[0m : 10.67861
[1mStep[0m  [40/42], [94mLoss[0m : 11.02169

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.542, [92mTest[0m: 10.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54086
[1mStep[0m  [4/42], [94mLoss[0m : 10.60825
[1mStep[0m  [8/42], [94mLoss[0m : 10.69214
[1mStep[0m  [12/42], [94mLoss[0m : 10.09037
[1mStep[0m  [16/42], [94mLoss[0m : 10.54634
[1mStep[0m  [20/42], [94mLoss[0m : 10.20034
[1mStep[0m  [24/42], [94mLoss[0m : 10.56488
[1mStep[0m  [28/42], [94mLoss[0m : 10.41771
[1mStep[0m  [32/42], [94mLoss[0m : 10.79374
[1mStep[0m  [36/42], [94mLoss[0m : 10.61919
[1mStep[0m  [40/42], [94mLoss[0m : 10.54751

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.537, [92mTest[0m: 10.485, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.53942
[1mStep[0m  [4/42], [94mLoss[0m : 10.68014
[1mStep[0m  [8/42], [94mLoss[0m : 10.23712
[1mStep[0m  [12/42], [94mLoss[0m : 10.50492
[1mStep[0m  [16/42], [94mLoss[0m : 10.40967
[1mStep[0m  [20/42], [94mLoss[0m : 10.73245
[1mStep[0m  [24/42], [94mLoss[0m : 10.77305
[1mStep[0m  [28/42], [94mLoss[0m : 10.28434
[1mStep[0m  [32/42], [94mLoss[0m : 10.59093
[1mStep[0m  [36/42], [94mLoss[0m : 10.41841
[1mStep[0m  [40/42], [94mLoss[0m : 10.65109

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.530, [92mTest[0m: 10.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.78720
[1mStep[0m  [4/42], [94mLoss[0m : 10.53325
[1mStep[0m  [8/42], [94mLoss[0m : 10.38339
[1mStep[0m  [12/42], [94mLoss[0m : 10.54523
[1mStep[0m  [16/42], [94mLoss[0m : 10.62259
[1mStep[0m  [20/42], [94mLoss[0m : 10.30175
[1mStep[0m  [24/42], [94mLoss[0m : 10.16471
[1mStep[0m  [28/42], [94mLoss[0m : 10.55606
[1mStep[0m  [32/42], [94mLoss[0m : 10.81153
[1mStep[0m  [36/42], [94mLoss[0m : 10.64352
[1mStep[0m  [40/42], [94mLoss[0m : 10.53837

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.462, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.68322
[1mStep[0m  [4/42], [94mLoss[0m : 10.81984
[1mStep[0m  [8/42], [94mLoss[0m : 10.41878
[1mStep[0m  [12/42], [94mLoss[0m : 10.63510
[1mStep[0m  [16/42], [94mLoss[0m : 10.25074
[1mStep[0m  [20/42], [94mLoss[0m : 10.53626
[1mStep[0m  [24/42], [94mLoss[0m : 10.66845
[1mStep[0m  [28/42], [94mLoss[0m : 10.60297
[1mStep[0m  [32/42], [94mLoss[0m : 10.04259
[1mStep[0m  [36/42], [94mLoss[0m : 10.46575
[1mStep[0m  [40/42], [94mLoss[0m : 10.33464

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.510, [92mTest[0m: 10.435, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.416
====================================

Phase 1 - Evaluation MAE:  10.416478565761022
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.32186
[1mStep[0m  [4/42], [94mLoss[0m : 10.39234
[1mStep[0m  [8/42], [94mLoss[0m : 10.35279
[1mStep[0m  [12/42], [94mLoss[0m : 10.94282
[1mStep[0m  [16/42], [94mLoss[0m : 10.46167
[1mStep[0m  [20/42], [94mLoss[0m : 10.91210
[1mStep[0m  [24/42], [94mLoss[0m : 10.69596
[1mStep[0m  [28/42], [94mLoss[0m : 10.68242
[1mStep[0m  [32/42], [94mLoss[0m : 10.75980
[1mStep[0m  [36/42], [94mLoss[0m : 10.47122
[1mStep[0m  [40/42], [94mLoss[0m : 10.70047

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.491, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32129
[1mStep[0m  [4/42], [94mLoss[0m : 10.62697
[1mStep[0m  [8/42], [94mLoss[0m : 10.45027
[1mStep[0m  [12/42], [94mLoss[0m : 10.57724
[1mStep[0m  [16/42], [94mLoss[0m : 10.43890
[1mStep[0m  [20/42], [94mLoss[0m : 10.25801
[1mStep[0m  [24/42], [94mLoss[0m : 10.56516
[1mStep[0m  [28/42], [94mLoss[0m : 10.64999
[1mStep[0m  [32/42], [94mLoss[0m : 10.54061
[1mStep[0m  [36/42], [94mLoss[0m : 10.17347
[1mStep[0m  [40/42], [94mLoss[0m : 10.74884

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.482, [92mTest[0m: 10.414, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.50060
[1mStep[0m  [4/42], [94mLoss[0m : 10.26891
[1mStep[0m  [8/42], [94mLoss[0m : 10.70855
[1mStep[0m  [12/42], [94mLoss[0m : 10.41664
[1mStep[0m  [16/42], [94mLoss[0m : 10.14138
[1mStep[0m  [20/42], [94mLoss[0m : 10.78862
[1mStep[0m  [24/42], [94mLoss[0m : 10.62770
[1mStep[0m  [28/42], [94mLoss[0m : 10.43362
[1mStep[0m  [32/42], [94mLoss[0m : 10.47282
[1mStep[0m  [36/42], [94mLoss[0m : 10.38351
[1mStep[0m  [40/42], [94mLoss[0m : 10.62647

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.470, [92mTest[0m: 10.387, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.33502
[1mStep[0m  [4/42], [94mLoss[0m : 10.36697
[1mStep[0m  [8/42], [94mLoss[0m : 10.02602
[1mStep[0m  [12/42], [94mLoss[0m : 10.58808
[1mStep[0m  [16/42], [94mLoss[0m : 10.47253
[1mStep[0m  [20/42], [94mLoss[0m : 10.48158
[1mStep[0m  [24/42], [94mLoss[0m : 10.62977
[1mStep[0m  [28/42], [94mLoss[0m : 10.46420
[1mStep[0m  [32/42], [94mLoss[0m : 10.23407
[1mStep[0m  [36/42], [94mLoss[0m : 10.65271
[1mStep[0m  [40/42], [94mLoss[0m : 10.50205

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.461, [92mTest[0m: 10.386, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.60275
[1mStep[0m  [4/42], [94mLoss[0m : 10.54091
[1mStep[0m  [8/42], [94mLoss[0m : 10.51871
[1mStep[0m  [12/42], [94mLoss[0m : 10.28699
[1mStep[0m  [16/42], [94mLoss[0m : 10.51801
[1mStep[0m  [20/42], [94mLoss[0m : 10.26040
[1mStep[0m  [24/42], [94mLoss[0m : 10.34641
[1mStep[0m  [28/42], [94mLoss[0m : 10.59252
[1mStep[0m  [32/42], [94mLoss[0m : 10.25178
[1mStep[0m  [36/42], [94mLoss[0m : 10.52970
[1mStep[0m  [40/42], [94mLoss[0m : 10.50277

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.455, [92mTest[0m: 10.366, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.44060
[1mStep[0m  [4/42], [94mLoss[0m : 10.19614
[1mStep[0m  [8/42], [94mLoss[0m : 10.65936
[1mStep[0m  [12/42], [94mLoss[0m : 11.04409
[1mStep[0m  [16/42], [94mLoss[0m : 10.38680
[1mStep[0m  [20/42], [94mLoss[0m : 10.50044
[1mStep[0m  [24/42], [94mLoss[0m : 10.29980
[1mStep[0m  [28/42], [94mLoss[0m : 10.67122
[1mStep[0m  [32/42], [94mLoss[0m : 10.50617
[1mStep[0m  [36/42], [94mLoss[0m : 10.50998
[1mStep[0m  [40/42], [94mLoss[0m : 10.14611

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.352, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.23618
[1mStep[0m  [4/42], [94mLoss[0m : 10.60585
[1mStep[0m  [8/42], [94mLoss[0m : 10.09158
[1mStep[0m  [12/42], [94mLoss[0m : 10.58584
[1mStep[0m  [16/42], [94mLoss[0m : 10.03947
[1mStep[0m  [20/42], [94mLoss[0m : 10.57828
[1mStep[0m  [24/42], [94mLoss[0m : 10.12802
[1mStep[0m  [28/42], [94mLoss[0m : 10.29488
[1mStep[0m  [32/42], [94mLoss[0m : 10.85885
[1mStep[0m  [36/42], [94mLoss[0m : 10.27637
[1mStep[0m  [40/42], [94mLoss[0m : 10.48935

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.434, [92mTest[0m: 10.333, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.18075
[1mStep[0m  [4/42], [94mLoss[0m : 10.25756
[1mStep[0m  [8/42], [94mLoss[0m : 10.23331
[1mStep[0m  [12/42], [94mLoss[0m : 10.60614
[1mStep[0m  [16/42], [94mLoss[0m : 10.47236
[1mStep[0m  [20/42], [94mLoss[0m : 10.63133
[1mStep[0m  [24/42], [94mLoss[0m : 10.39124
[1mStep[0m  [28/42], [94mLoss[0m : 10.65074
[1mStep[0m  [32/42], [94mLoss[0m : 10.64332
[1mStep[0m  [36/42], [94mLoss[0m : 10.34192
[1mStep[0m  [40/42], [94mLoss[0m : 10.35588

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.416, [92mTest[0m: 10.339, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72208
[1mStep[0m  [4/42], [94mLoss[0m : 10.09691
[1mStep[0m  [8/42], [94mLoss[0m : 10.74749
[1mStep[0m  [12/42], [94mLoss[0m : 9.95729
[1mStep[0m  [16/42], [94mLoss[0m : 10.60144
[1mStep[0m  [20/42], [94mLoss[0m : 10.20190
[1mStep[0m  [24/42], [94mLoss[0m : 10.14824
[1mStep[0m  [28/42], [94mLoss[0m : 10.06950
[1mStep[0m  [32/42], [94mLoss[0m : 10.36540
[1mStep[0m  [36/42], [94mLoss[0m : 10.33282
[1mStep[0m  [40/42], [94mLoss[0m : 10.87592

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.400, [92mTest[0m: 10.311, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26308
[1mStep[0m  [4/42], [94mLoss[0m : 10.76197
[1mStep[0m  [8/42], [94mLoss[0m : 10.37526
[1mStep[0m  [12/42], [94mLoss[0m : 10.55948
[1mStep[0m  [16/42], [94mLoss[0m : 10.35204
[1mStep[0m  [20/42], [94mLoss[0m : 10.31616
[1mStep[0m  [24/42], [94mLoss[0m : 10.72841
[1mStep[0m  [28/42], [94mLoss[0m : 10.29913
[1mStep[0m  [32/42], [94mLoss[0m : 10.20950
[1mStep[0m  [36/42], [94mLoss[0m : 10.35212
[1mStep[0m  [40/42], [94mLoss[0m : 10.24658

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.402, [92mTest[0m: 10.283, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52088
[1mStep[0m  [4/42], [94mLoss[0m : 10.46571
[1mStep[0m  [8/42], [94mLoss[0m : 10.72827
[1mStep[0m  [12/42], [94mLoss[0m : 10.54677
[1mStep[0m  [16/42], [94mLoss[0m : 10.00608
[1mStep[0m  [20/42], [94mLoss[0m : 10.37028
[1mStep[0m  [24/42], [94mLoss[0m : 10.23429
[1mStep[0m  [28/42], [94mLoss[0m : 10.33249
[1mStep[0m  [32/42], [94mLoss[0m : 10.30194
[1mStep[0m  [36/42], [94mLoss[0m : 10.21098
[1mStep[0m  [40/42], [94mLoss[0m : 10.21460

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.375, [92mTest[0m: 10.272, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.47192
[1mStep[0m  [4/42], [94mLoss[0m : 10.66390
[1mStep[0m  [8/42], [94mLoss[0m : 10.40899
[1mStep[0m  [12/42], [94mLoss[0m : 10.58085
[1mStep[0m  [16/42], [94mLoss[0m : 10.50282
[1mStep[0m  [20/42], [94mLoss[0m : 9.95573
[1mStep[0m  [24/42], [94mLoss[0m : 10.04165
[1mStep[0m  [28/42], [94mLoss[0m : 10.05645
[1mStep[0m  [32/42], [94mLoss[0m : 10.11499
[1mStep[0m  [36/42], [94mLoss[0m : 10.50805
[1mStep[0m  [40/42], [94mLoss[0m : 10.37583

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.369, [92mTest[0m: 10.251, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.03617
[1mStep[0m  [4/42], [94mLoss[0m : 10.55145
[1mStep[0m  [8/42], [94mLoss[0m : 10.44444
[1mStep[0m  [12/42], [94mLoss[0m : 9.79952
[1mStep[0m  [16/42], [94mLoss[0m : 10.38081
[1mStep[0m  [20/42], [94mLoss[0m : 10.02297
[1mStep[0m  [24/42], [94mLoss[0m : 10.19317
[1mStep[0m  [28/42], [94mLoss[0m : 10.07585
[1mStep[0m  [32/42], [94mLoss[0m : 10.13400
[1mStep[0m  [36/42], [94mLoss[0m : 10.37474
[1mStep[0m  [40/42], [94mLoss[0m : 10.47138

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.356, [92mTest[0m: 10.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.90647
[1mStep[0m  [4/42], [94mLoss[0m : 10.56133
[1mStep[0m  [8/42], [94mLoss[0m : 10.15433
[1mStep[0m  [12/42], [94mLoss[0m : 10.50497
[1mStep[0m  [16/42], [94mLoss[0m : 10.66882
[1mStep[0m  [20/42], [94mLoss[0m : 10.60251
[1mStep[0m  [24/42], [94mLoss[0m : 10.19534
[1mStep[0m  [28/42], [94mLoss[0m : 10.04385
[1mStep[0m  [32/42], [94mLoss[0m : 10.38054
[1mStep[0m  [36/42], [94mLoss[0m : 10.47593
[1mStep[0m  [40/42], [94mLoss[0m : 10.53429

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.350, [92mTest[0m: 10.260, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.29333
[1mStep[0m  [4/42], [94mLoss[0m : 10.32112
[1mStep[0m  [8/42], [94mLoss[0m : 10.26755
[1mStep[0m  [12/42], [94mLoss[0m : 10.91789
[1mStep[0m  [16/42], [94mLoss[0m : 10.51614
[1mStep[0m  [20/42], [94mLoss[0m : 10.46633
[1mStep[0m  [24/42], [94mLoss[0m : 10.72862
[1mStep[0m  [28/42], [94mLoss[0m : 10.47908
[1mStep[0m  [32/42], [94mLoss[0m : 10.81773
[1mStep[0m  [36/42], [94mLoss[0m : 10.47575
[1mStep[0m  [40/42], [94mLoss[0m : 10.51167

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.334, [92mTest[0m: 10.214, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.56762
[1mStep[0m  [4/42], [94mLoss[0m : 10.20051
[1mStep[0m  [8/42], [94mLoss[0m : 10.23773
[1mStep[0m  [12/42], [94mLoss[0m : 10.18060
[1mStep[0m  [16/42], [94mLoss[0m : 9.98652
[1mStep[0m  [20/42], [94mLoss[0m : 10.51243
[1mStep[0m  [24/42], [94mLoss[0m : 10.32921
[1mStep[0m  [28/42], [94mLoss[0m : 10.37014
[1mStep[0m  [32/42], [94mLoss[0m : 10.42705
[1mStep[0m  [36/42], [94mLoss[0m : 9.91902
[1mStep[0m  [40/42], [94mLoss[0m : 10.36784

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.311, [92mTest[0m: 10.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.52915
[1mStep[0m  [4/42], [94mLoss[0m : 10.90599
[1mStep[0m  [8/42], [94mLoss[0m : 10.03590
[1mStep[0m  [12/42], [94mLoss[0m : 10.39411
[1mStep[0m  [16/42], [94mLoss[0m : 10.21405
[1mStep[0m  [20/42], [94mLoss[0m : 9.86092
[1mStep[0m  [24/42], [94mLoss[0m : 10.33828
[1mStep[0m  [28/42], [94mLoss[0m : 10.41451
[1mStep[0m  [32/42], [94mLoss[0m : 10.17211
[1mStep[0m  [36/42], [94mLoss[0m : 9.85989
[1mStep[0m  [40/42], [94mLoss[0m : 10.45239

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.312, [92mTest[0m: 10.185, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25411
[1mStep[0m  [4/42], [94mLoss[0m : 10.34533
[1mStep[0m  [8/42], [94mLoss[0m : 10.67190
[1mStep[0m  [12/42], [94mLoss[0m : 10.50831
[1mStep[0m  [16/42], [94mLoss[0m : 10.35017
[1mStep[0m  [20/42], [94mLoss[0m : 10.12328
[1mStep[0m  [24/42], [94mLoss[0m : 10.39362
[1mStep[0m  [28/42], [94mLoss[0m : 10.12912
[1mStep[0m  [32/42], [94mLoss[0m : 10.03566
[1mStep[0m  [36/42], [94mLoss[0m : 10.31927
[1mStep[0m  [40/42], [94mLoss[0m : 10.52480

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.299, [92mTest[0m: 10.176, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.51074
[1mStep[0m  [4/42], [94mLoss[0m : 10.46497
[1mStep[0m  [8/42], [94mLoss[0m : 10.20963
[1mStep[0m  [12/42], [94mLoss[0m : 10.16787
[1mStep[0m  [16/42], [94mLoss[0m : 9.98561
[1mStep[0m  [20/42], [94mLoss[0m : 10.16008
[1mStep[0m  [24/42], [94mLoss[0m : 10.58963
[1mStep[0m  [28/42], [94mLoss[0m : 10.25636
[1mStep[0m  [32/42], [94mLoss[0m : 10.14011
[1mStep[0m  [36/42], [94mLoss[0m : 10.42018
[1mStep[0m  [40/42], [94mLoss[0m : 10.12192

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.294, [92mTest[0m: 10.157, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.24453
[1mStep[0m  [4/42], [94mLoss[0m : 10.35809
[1mStep[0m  [8/42], [94mLoss[0m : 10.41756
[1mStep[0m  [12/42], [94mLoss[0m : 10.17158
[1mStep[0m  [16/42], [94mLoss[0m : 10.33631
[1mStep[0m  [20/42], [94mLoss[0m : 10.52489
[1mStep[0m  [24/42], [94mLoss[0m : 10.48748
[1mStep[0m  [28/42], [94mLoss[0m : 10.14543
[1mStep[0m  [32/42], [94mLoss[0m : 9.89186
[1mStep[0m  [36/42], [94mLoss[0m : 10.05890
[1mStep[0m  [40/42], [94mLoss[0m : 10.47692

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.274, [92mTest[0m: 10.158, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.10482
[1mStep[0m  [4/42], [94mLoss[0m : 10.59855
[1mStep[0m  [8/42], [94mLoss[0m : 9.63974
[1mStep[0m  [12/42], [94mLoss[0m : 10.53816
[1mStep[0m  [16/42], [94mLoss[0m : 10.44421
[1mStep[0m  [20/42], [94mLoss[0m : 10.39806
[1mStep[0m  [24/42], [94mLoss[0m : 10.72526
[1mStep[0m  [28/42], [94mLoss[0m : 10.01512
[1mStep[0m  [32/42], [94mLoss[0m : 10.19071
[1mStep[0m  [36/42], [94mLoss[0m : 10.30682
[1mStep[0m  [40/42], [94mLoss[0m : 9.98238

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.262, [92mTest[0m: 10.140, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25933
[1mStep[0m  [4/42], [94mLoss[0m : 10.40361
[1mStep[0m  [8/42], [94mLoss[0m : 10.23064
[1mStep[0m  [12/42], [94mLoss[0m : 10.45112
[1mStep[0m  [16/42], [94mLoss[0m : 9.83556
[1mStep[0m  [20/42], [94mLoss[0m : 10.05390
[1mStep[0m  [24/42], [94mLoss[0m : 10.44331
[1mStep[0m  [28/42], [94mLoss[0m : 9.74726
[1mStep[0m  [32/42], [94mLoss[0m : 10.35190
[1mStep[0m  [36/42], [94mLoss[0m : 10.57375
[1mStep[0m  [40/42], [94mLoss[0m : 10.26432

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.257, [92mTest[0m: 10.111, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15551
[1mStep[0m  [4/42], [94mLoss[0m : 10.09189
[1mStep[0m  [8/42], [94mLoss[0m : 10.32470
[1mStep[0m  [12/42], [94mLoss[0m : 10.00650
[1mStep[0m  [16/42], [94mLoss[0m : 10.42591
[1mStep[0m  [20/42], [94mLoss[0m : 10.53958
[1mStep[0m  [24/42], [94mLoss[0m : 10.26190
[1mStep[0m  [28/42], [94mLoss[0m : 10.39121
[1mStep[0m  [32/42], [94mLoss[0m : 10.30620
[1mStep[0m  [36/42], [94mLoss[0m : 10.35291
[1mStep[0m  [40/42], [94mLoss[0m : 10.08566

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.241, [92mTest[0m: 10.130, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.03860
[1mStep[0m  [4/42], [94mLoss[0m : 10.08008
[1mStep[0m  [8/42], [94mLoss[0m : 10.27286
[1mStep[0m  [12/42], [94mLoss[0m : 10.22398
[1mStep[0m  [16/42], [94mLoss[0m : 10.32411
[1mStep[0m  [20/42], [94mLoss[0m : 10.01376
[1mStep[0m  [24/42], [94mLoss[0m : 10.37833
[1mStep[0m  [28/42], [94mLoss[0m : 10.17131
[1mStep[0m  [32/42], [94mLoss[0m : 10.18044
[1mStep[0m  [36/42], [94mLoss[0m : 9.65768
[1mStep[0m  [40/42], [94mLoss[0m : 10.21639

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.228, [92mTest[0m: 10.109, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.13267
[1mStep[0m  [4/42], [94mLoss[0m : 10.03499
[1mStep[0m  [8/42], [94mLoss[0m : 10.31574
[1mStep[0m  [12/42], [94mLoss[0m : 10.21547
[1mStep[0m  [16/42], [94mLoss[0m : 10.16060
[1mStep[0m  [20/42], [94mLoss[0m : 10.11870
[1mStep[0m  [24/42], [94mLoss[0m : 10.08535
[1mStep[0m  [28/42], [94mLoss[0m : 10.70481
[1mStep[0m  [32/42], [94mLoss[0m : 10.32756
[1mStep[0m  [36/42], [94mLoss[0m : 10.01128
[1mStep[0m  [40/42], [94mLoss[0m : 10.30338

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.215, [92mTest[0m: 10.073, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.19579
[1mStep[0m  [4/42], [94mLoss[0m : 10.33120
[1mStep[0m  [8/42], [94mLoss[0m : 10.22693
[1mStep[0m  [12/42], [94mLoss[0m : 10.21353
[1mStep[0m  [16/42], [94mLoss[0m : 10.24292
[1mStep[0m  [20/42], [94mLoss[0m : 10.05459
[1mStep[0m  [24/42], [94mLoss[0m : 10.05330
[1mStep[0m  [28/42], [94mLoss[0m : 10.28141
[1mStep[0m  [32/42], [94mLoss[0m : 10.44089
[1mStep[0m  [36/42], [94mLoss[0m : 9.98679
[1mStep[0m  [40/42], [94mLoss[0m : 10.46867

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.204, [92mTest[0m: 10.073, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22525
[1mStep[0m  [4/42], [94mLoss[0m : 9.92825
[1mStep[0m  [8/42], [94mLoss[0m : 10.12118
[1mStep[0m  [12/42], [94mLoss[0m : 10.19251
[1mStep[0m  [16/42], [94mLoss[0m : 10.47531
[1mStep[0m  [20/42], [94mLoss[0m : 10.32152
[1mStep[0m  [24/42], [94mLoss[0m : 10.35197
[1mStep[0m  [28/42], [94mLoss[0m : 10.12390
[1mStep[0m  [32/42], [94mLoss[0m : 10.28720
[1mStep[0m  [36/42], [94mLoss[0m : 9.97341
[1mStep[0m  [40/42], [94mLoss[0m : 9.88856

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.197, [92mTest[0m: 10.076, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.05948
[1mStep[0m  [4/42], [94mLoss[0m : 10.30900
[1mStep[0m  [8/42], [94mLoss[0m : 9.94775
[1mStep[0m  [12/42], [94mLoss[0m : 10.42315
[1mStep[0m  [16/42], [94mLoss[0m : 9.81177
[1mStep[0m  [20/42], [94mLoss[0m : 10.48989
[1mStep[0m  [24/42], [94mLoss[0m : 9.80722
[1mStep[0m  [28/42], [94mLoss[0m : 10.48579
[1mStep[0m  [32/42], [94mLoss[0m : 10.22764
[1mStep[0m  [36/42], [94mLoss[0m : 10.09142
[1mStep[0m  [40/42], [94mLoss[0m : 10.14269

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.189, [92mTest[0m: 10.020, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.17152
[1mStep[0m  [4/42], [94mLoss[0m : 10.15382
[1mStep[0m  [8/42], [94mLoss[0m : 10.26442
[1mStep[0m  [12/42], [94mLoss[0m : 10.42690
[1mStep[0m  [16/42], [94mLoss[0m : 10.35002
[1mStep[0m  [20/42], [94mLoss[0m : 10.37246
[1mStep[0m  [24/42], [94mLoss[0m : 10.31180
[1mStep[0m  [28/42], [94mLoss[0m : 10.34151
[1mStep[0m  [32/42], [94mLoss[0m : 10.27589
[1mStep[0m  [36/42], [94mLoss[0m : 9.71089
[1mStep[0m  [40/42], [94mLoss[0m : 10.25076

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.178, [92mTest[0m: 10.024, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.18311
[1mStep[0m  [4/42], [94mLoss[0m : 10.23726
[1mStep[0m  [8/42], [94mLoss[0m : 10.39525
[1mStep[0m  [12/42], [94mLoss[0m : 10.82451
[1mStep[0m  [16/42], [94mLoss[0m : 10.11610
[1mStep[0m  [20/42], [94mLoss[0m : 10.25854
[1mStep[0m  [24/42], [94mLoss[0m : 10.35788
[1mStep[0m  [28/42], [94mLoss[0m : 10.15218
[1mStep[0m  [32/42], [94mLoss[0m : 10.33988
[1mStep[0m  [36/42], [94mLoss[0m : 9.95815
[1mStep[0m  [40/42], [94mLoss[0m : 10.37588

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.173, [92mTest[0m: 10.012, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.988
====================================

Phase 2 - Evaluation MAE:  9.988354751041957
MAE score P1      10.416479
MAE score P2       9.988355
loss              10.173102
learning_rate        0.0001
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 16, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.83970
[1mStep[0m  [2/21], [94mLoss[0m : 10.94673
[1mStep[0m  [4/21], [94mLoss[0m : 10.95460
[1mStep[0m  [6/21], [94mLoss[0m : 10.60866
[1mStep[0m  [8/21], [94mLoss[0m : 11.11099
[1mStep[0m  [10/21], [94mLoss[0m : 10.61011
[1mStep[0m  [12/21], [94mLoss[0m : 10.61989
[1mStep[0m  [14/21], [94mLoss[0m : 10.70973
[1mStep[0m  [16/21], [94mLoss[0m : 10.50160
[1mStep[0m  [18/21], [94mLoss[0m : 10.77757
[1mStep[0m  [20/21], [94mLoss[0m : 10.93547

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.781, [92mTest[0m: 10.949, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72890
[1mStep[0m  [2/21], [94mLoss[0m : 10.55577
[1mStep[0m  [4/21], [94mLoss[0m : 10.50447
[1mStep[0m  [6/21], [94mLoss[0m : 10.22087
[1mStep[0m  [8/21], [94mLoss[0m : 10.46555
[1mStep[0m  [10/21], [94mLoss[0m : 10.22016
[1mStep[0m  [12/21], [94mLoss[0m : 10.60970
[1mStep[0m  [14/21], [94mLoss[0m : 10.40114
[1mStep[0m  [16/21], [94mLoss[0m : 10.42540
[1mStep[0m  [18/21], [94mLoss[0m : 10.33807
[1mStep[0m  [20/21], [94mLoss[0m : 10.57266

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.508, [92mTest[0m: 10.643, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36183
[1mStep[0m  [2/21], [94mLoss[0m : 10.33489
[1mStep[0m  [4/21], [94mLoss[0m : 10.12133
[1mStep[0m  [6/21], [94mLoss[0m : 10.34801
[1mStep[0m  [8/21], [94mLoss[0m : 10.14775
[1mStep[0m  [10/21], [94mLoss[0m : 10.30070
[1mStep[0m  [12/21], [94mLoss[0m : 10.24764
[1mStep[0m  [14/21], [94mLoss[0m : 10.17907
[1mStep[0m  [16/21], [94mLoss[0m : 10.01943
[1mStep[0m  [18/21], [94mLoss[0m : 10.27472
[1mStep[0m  [20/21], [94mLoss[0m : 10.11674

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.246, [92mTest[0m: 10.352, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.07468
[1mStep[0m  [2/21], [94mLoss[0m : 10.16856
[1mStep[0m  [4/21], [94mLoss[0m : 10.08488
[1mStep[0m  [6/21], [94mLoss[0m : 9.98744
[1mStep[0m  [8/21], [94mLoss[0m : 9.90770
[1mStep[0m  [10/21], [94mLoss[0m : 9.95677
[1mStep[0m  [12/21], [94mLoss[0m : 10.00373
[1mStep[0m  [14/21], [94mLoss[0m : 9.67364
[1mStep[0m  [16/21], [94mLoss[0m : 10.08056
[1mStep[0m  [18/21], [94mLoss[0m : 9.86728
[1mStep[0m  [20/21], [94mLoss[0m : 9.92359

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.979, [92mTest[0m: 10.082, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.71484
[1mStep[0m  [2/21], [94mLoss[0m : 9.65815
[1mStep[0m  [4/21], [94mLoss[0m : 9.85457
[1mStep[0m  [6/21], [94mLoss[0m : 9.80468
[1mStep[0m  [8/21], [94mLoss[0m : 9.75334
[1mStep[0m  [10/21], [94mLoss[0m : 9.87250
[1mStep[0m  [12/21], [94mLoss[0m : 9.71323
[1mStep[0m  [14/21], [94mLoss[0m : 9.69985
[1mStep[0m  [16/21], [94mLoss[0m : 9.68190
[1mStep[0m  [18/21], [94mLoss[0m : 9.56364
[1mStep[0m  [20/21], [94mLoss[0m : 9.48711

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.707, [92mTest[0m: 9.819, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.73059
[1mStep[0m  [2/21], [94mLoss[0m : 9.58142
[1mStep[0m  [4/21], [94mLoss[0m : 9.56139
[1mStep[0m  [6/21], [94mLoss[0m : 9.48616
[1mStep[0m  [8/21], [94mLoss[0m : 9.55185
[1mStep[0m  [10/21], [94mLoss[0m : 9.39781
[1mStep[0m  [12/21], [94mLoss[0m : 9.62723
[1mStep[0m  [14/21], [94mLoss[0m : 9.42399
[1mStep[0m  [16/21], [94mLoss[0m : 9.24854
[1mStep[0m  [18/21], [94mLoss[0m : 9.16970
[1mStep[0m  [20/21], [94mLoss[0m : 9.31004

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.448, [92mTest[0m: 9.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.15733
[1mStep[0m  [2/21], [94mLoss[0m : 9.51663
[1mStep[0m  [4/21], [94mLoss[0m : 9.48838
[1mStep[0m  [6/21], [94mLoss[0m : 8.93169
[1mStep[0m  [8/21], [94mLoss[0m : 9.19510
[1mStep[0m  [10/21], [94mLoss[0m : 9.16026
[1mStep[0m  [12/21], [94mLoss[0m : 9.23024
[1mStep[0m  [14/21], [94mLoss[0m : 9.38251
[1mStep[0m  [16/21], [94mLoss[0m : 9.24340
[1mStep[0m  [18/21], [94mLoss[0m : 8.94588
[1mStep[0m  [20/21], [94mLoss[0m : 9.05720

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.181, [92mTest[0m: 9.297, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.22178
[1mStep[0m  [2/21], [94mLoss[0m : 9.13348
[1mStep[0m  [4/21], [94mLoss[0m : 9.14124
[1mStep[0m  [6/21], [94mLoss[0m : 9.10672
[1mStep[0m  [8/21], [94mLoss[0m : 8.84292
[1mStep[0m  [10/21], [94mLoss[0m : 8.71867
[1mStep[0m  [12/21], [94mLoss[0m : 8.80118
[1mStep[0m  [14/21], [94mLoss[0m : 9.30333
[1mStep[0m  [16/21], [94mLoss[0m : 8.80406
[1mStep[0m  [18/21], [94mLoss[0m : 8.64905
[1mStep[0m  [20/21], [94mLoss[0m : 8.61063

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.916, [92mTest[0m: 9.031, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.60077
[1mStep[0m  [2/21], [94mLoss[0m : 8.70681
[1mStep[0m  [4/21], [94mLoss[0m : 8.66197
[1mStep[0m  [6/21], [94mLoss[0m : 8.49876
[1mStep[0m  [8/21], [94mLoss[0m : 8.58046
[1mStep[0m  [10/21], [94mLoss[0m : 8.77602
[1mStep[0m  [12/21], [94mLoss[0m : 8.76839
[1mStep[0m  [14/21], [94mLoss[0m : 8.63723
[1mStep[0m  [16/21], [94mLoss[0m : 8.76389
[1mStep[0m  [18/21], [94mLoss[0m : 8.38155
[1mStep[0m  [20/21], [94mLoss[0m : 8.54532

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.643, [92mTest[0m: 8.755, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.79038
[1mStep[0m  [2/21], [94mLoss[0m : 8.35822
[1mStep[0m  [4/21], [94mLoss[0m : 8.35750
[1mStep[0m  [6/21], [94mLoss[0m : 8.20618
[1mStep[0m  [8/21], [94mLoss[0m : 8.38103
[1mStep[0m  [10/21], [94mLoss[0m : 8.30074
[1mStep[0m  [12/21], [94mLoss[0m : 8.23348
[1mStep[0m  [14/21], [94mLoss[0m : 8.40283
[1mStep[0m  [16/21], [94mLoss[0m : 8.49956
[1mStep[0m  [18/21], [94mLoss[0m : 8.17098
[1mStep[0m  [20/21], [94mLoss[0m : 8.35451

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.379, [92mTest[0m: 8.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.98136
[1mStep[0m  [2/21], [94mLoss[0m : 8.34203
[1mStep[0m  [4/21], [94mLoss[0m : 8.19240
[1mStep[0m  [6/21], [94mLoss[0m : 8.25116
[1mStep[0m  [8/21], [94mLoss[0m : 8.38604
[1mStep[0m  [10/21], [94mLoss[0m : 7.80985
[1mStep[0m  [12/21], [94mLoss[0m : 8.01700
[1mStep[0m  [14/21], [94mLoss[0m : 7.92858
[1mStep[0m  [16/21], [94mLoss[0m : 8.12954
[1mStep[0m  [18/21], [94mLoss[0m : 8.09323
[1mStep[0m  [20/21], [94mLoss[0m : 7.81031

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.109, [92mTest[0m: 8.235, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.09074
[1mStep[0m  [2/21], [94mLoss[0m : 7.70109
[1mStep[0m  [4/21], [94mLoss[0m : 7.80462
[1mStep[0m  [6/21], [94mLoss[0m : 7.86838
[1mStep[0m  [8/21], [94mLoss[0m : 8.08366
[1mStep[0m  [10/21], [94mLoss[0m : 7.87473
[1mStep[0m  [12/21], [94mLoss[0m : 7.90544
[1mStep[0m  [14/21], [94mLoss[0m : 7.79815
[1mStep[0m  [16/21], [94mLoss[0m : 7.80640
[1mStep[0m  [18/21], [94mLoss[0m : 7.77063
[1mStep[0m  [20/21], [94mLoss[0m : 7.47308

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.844, [92mTest[0m: 7.959, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.57406
[1mStep[0m  [2/21], [94mLoss[0m : 7.69666
[1mStep[0m  [4/21], [94mLoss[0m : 7.57939
[1mStep[0m  [6/21], [94mLoss[0m : 7.61484
[1mStep[0m  [8/21], [94mLoss[0m : 7.53498
[1mStep[0m  [10/21], [94mLoss[0m : 7.80901
[1mStep[0m  [12/21], [94mLoss[0m : 7.61781
[1mStep[0m  [14/21], [94mLoss[0m : 7.56658
[1mStep[0m  [16/21], [94mLoss[0m : 7.66053
[1mStep[0m  [18/21], [94mLoss[0m : 7.40269
[1mStep[0m  [20/21], [94mLoss[0m : 7.47079

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.582, [92mTest[0m: 7.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.44823
[1mStep[0m  [2/21], [94mLoss[0m : 7.51129
[1mStep[0m  [4/21], [94mLoss[0m : 7.32444
[1mStep[0m  [6/21], [94mLoss[0m : 7.19649
[1mStep[0m  [8/21], [94mLoss[0m : 7.06168
[1mStep[0m  [10/21], [94mLoss[0m : 7.33632
[1mStep[0m  [12/21], [94mLoss[0m : 7.42531
[1mStep[0m  [14/21], [94mLoss[0m : 7.34174
[1mStep[0m  [16/21], [94mLoss[0m : 7.24612
[1mStep[0m  [18/21], [94mLoss[0m : 7.03265
[1mStep[0m  [20/21], [94mLoss[0m : 7.27491

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.312, [92mTest[0m: 7.430, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.21969
[1mStep[0m  [2/21], [94mLoss[0m : 7.03916
[1mStep[0m  [4/21], [94mLoss[0m : 7.28704
[1mStep[0m  [6/21], [94mLoss[0m : 7.24366
[1mStep[0m  [8/21], [94mLoss[0m : 7.09851
[1mStep[0m  [10/21], [94mLoss[0m : 7.13942
[1mStep[0m  [12/21], [94mLoss[0m : 7.07437
[1mStep[0m  [14/21], [94mLoss[0m : 7.16629
[1mStep[0m  [16/21], [94mLoss[0m : 7.03010
[1mStep[0m  [18/21], [94mLoss[0m : 7.01656
[1mStep[0m  [20/21], [94mLoss[0m : 6.81824

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.039, [92mTest[0m: 7.169, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.49186
[1mStep[0m  [2/21], [94mLoss[0m : 6.95347
[1mStep[0m  [4/21], [94mLoss[0m : 6.79605
[1mStep[0m  [6/21], [94mLoss[0m : 6.81504
[1mStep[0m  [8/21], [94mLoss[0m : 6.82324
[1mStep[0m  [10/21], [94mLoss[0m : 7.02820
[1mStep[0m  [12/21], [94mLoss[0m : 6.54815
[1mStep[0m  [14/21], [94mLoss[0m : 7.04426
[1mStep[0m  [16/21], [94mLoss[0m : 6.62608
[1mStep[0m  [18/21], [94mLoss[0m : 6.54478
[1mStep[0m  [20/21], [94mLoss[0m : 6.71628

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.781, [92mTest[0m: 6.898, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.65258
[1mStep[0m  [2/21], [94mLoss[0m : 6.43431
[1mStep[0m  [4/21], [94mLoss[0m : 6.72561
[1mStep[0m  [6/21], [94mLoss[0m : 6.65316
[1mStep[0m  [8/21], [94mLoss[0m : 6.36257
[1mStep[0m  [10/21], [94mLoss[0m : 6.50223
[1mStep[0m  [12/21], [94mLoss[0m : 6.43504
[1mStep[0m  [14/21], [94mLoss[0m : 6.40952
[1mStep[0m  [16/21], [94mLoss[0m : 6.57429
[1mStep[0m  [18/21], [94mLoss[0m : 6.25795
[1mStep[0m  [20/21], [94mLoss[0m : 6.32173

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 6.511, [92mTest[0m: 6.630, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.33029
[1mStep[0m  [2/21], [94mLoss[0m : 6.30659
[1mStep[0m  [4/21], [94mLoss[0m : 6.18442
[1mStep[0m  [6/21], [94mLoss[0m : 6.35274
[1mStep[0m  [8/21], [94mLoss[0m : 6.36683
[1mStep[0m  [10/21], [94mLoss[0m : 6.06925
[1mStep[0m  [12/21], [94mLoss[0m : 6.29371
[1mStep[0m  [14/21], [94mLoss[0m : 6.16157
[1mStep[0m  [16/21], [94mLoss[0m : 6.18021
[1mStep[0m  [18/21], [94mLoss[0m : 6.21864
[1mStep[0m  [20/21], [94mLoss[0m : 6.32873

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 6.245, [92mTest[0m: 6.362, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.08798
[1mStep[0m  [2/21], [94mLoss[0m : 6.04401
[1mStep[0m  [4/21], [94mLoss[0m : 5.92969
[1mStep[0m  [6/21], [94mLoss[0m : 6.16075
[1mStep[0m  [8/21], [94mLoss[0m : 5.79351
[1mStep[0m  [10/21], [94mLoss[0m : 6.07682
[1mStep[0m  [12/21], [94mLoss[0m : 6.06297
[1mStep[0m  [14/21], [94mLoss[0m : 5.92118
[1mStep[0m  [16/21], [94mLoss[0m : 5.98676
[1mStep[0m  [18/21], [94mLoss[0m : 5.99058
[1mStep[0m  [20/21], [94mLoss[0m : 5.93145

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.978, [92mTest[0m: 6.095, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.87687
[1mStep[0m  [2/21], [94mLoss[0m : 5.66654
[1mStep[0m  [4/21], [94mLoss[0m : 5.67840
[1mStep[0m  [6/21], [94mLoss[0m : 6.00676
[1mStep[0m  [8/21], [94mLoss[0m : 5.73220
[1mStep[0m  [10/21], [94mLoss[0m : 5.86329
[1mStep[0m  [12/21], [94mLoss[0m : 5.89025
[1mStep[0m  [14/21], [94mLoss[0m : 5.37356
[1mStep[0m  [16/21], [94mLoss[0m : 5.62968
[1mStep[0m  [18/21], [94mLoss[0m : 5.64952
[1mStep[0m  [20/21], [94mLoss[0m : 5.59327

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.721, [92mTest[0m: 5.830, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.81239
[1mStep[0m  [2/21], [94mLoss[0m : 5.48472
[1mStep[0m  [4/21], [94mLoss[0m : 5.74076
[1mStep[0m  [6/21], [94mLoss[0m : 5.38818
[1mStep[0m  [8/21], [94mLoss[0m : 5.21042
[1mStep[0m  [10/21], [94mLoss[0m : 5.45901
[1mStep[0m  [12/21], [94mLoss[0m : 5.38576
[1mStep[0m  [14/21], [94mLoss[0m : 5.25310
[1mStep[0m  [16/21], [94mLoss[0m : 5.39036
[1mStep[0m  [18/21], [94mLoss[0m : 5.60276
[1mStep[0m  [20/21], [94mLoss[0m : 5.35264

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.475, [92mTest[0m: 5.569, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.21684
[1mStep[0m  [2/21], [94mLoss[0m : 5.26509
[1mStep[0m  [4/21], [94mLoss[0m : 5.44617
[1mStep[0m  [6/21], [94mLoss[0m : 5.28275
[1mStep[0m  [8/21], [94mLoss[0m : 5.04810
[1mStep[0m  [10/21], [94mLoss[0m : 5.29588
[1mStep[0m  [12/21], [94mLoss[0m : 5.55129
[1mStep[0m  [14/21], [94mLoss[0m : 5.36260
[1mStep[0m  [16/21], [94mLoss[0m : 5.06309
[1mStep[0m  [18/21], [94mLoss[0m : 5.14659
[1mStep[0m  [20/21], [94mLoss[0m : 5.13803

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.257, [92mTest[0m: 5.366, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.06234
[1mStep[0m  [2/21], [94mLoss[0m : 5.06847
[1mStep[0m  [4/21], [94mLoss[0m : 5.28786
[1mStep[0m  [6/21], [94mLoss[0m : 4.88969
[1mStep[0m  [8/21], [94mLoss[0m : 5.18228
[1mStep[0m  [10/21], [94mLoss[0m : 5.17639
[1mStep[0m  [12/21], [94mLoss[0m : 5.22554
[1mStep[0m  [14/21], [94mLoss[0m : 5.08217
[1mStep[0m  [16/21], [94mLoss[0m : 4.76549
[1mStep[0m  [18/21], [94mLoss[0m : 4.83578
[1mStep[0m  [20/21], [94mLoss[0m : 4.86419

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.042, [92mTest[0m: 5.141, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.08539
[1mStep[0m  [2/21], [94mLoss[0m : 4.94319
[1mStep[0m  [4/21], [94mLoss[0m : 4.87034
[1mStep[0m  [6/21], [94mLoss[0m : 4.90948
[1mStep[0m  [8/21], [94mLoss[0m : 4.86164
[1mStep[0m  [10/21], [94mLoss[0m : 4.93684
[1mStep[0m  [12/21], [94mLoss[0m : 5.17562
[1mStep[0m  [14/21], [94mLoss[0m : 4.64978
[1mStep[0m  [16/21], [94mLoss[0m : 4.79506
[1mStep[0m  [18/21], [94mLoss[0m : 4.87141
[1mStep[0m  [20/21], [94mLoss[0m : 4.50210

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.846, [92mTest[0m: 4.945, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.70415
[1mStep[0m  [2/21], [94mLoss[0m : 4.65023
[1mStep[0m  [4/21], [94mLoss[0m : 4.73787
[1mStep[0m  [6/21], [94mLoss[0m : 5.04759
[1mStep[0m  [8/21], [94mLoss[0m : 4.45452
[1mStep[0m  [10/21], [94mLoss[0m : 4.40585
[1mStep[0m  [12/21], [94mLoss[0m : 4.81832
[1mStep[0m  [14/21], [94mLoss[0m : 4.79081
[1mStep[0m  [16/21], [94mLoss[0m : 4.52767
[1mStep[0m  [18/21], [94mLoss[0m : 4.76001
[1mStep[0m  [20/21], [94mLoss[0m : 4.70525

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 4.666, [92mTest[0m: 4.759, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.44519
[1mStep[0m  [2/21], [94mLoss[0m : 4.56435
[1mStep[0m  [4/21], [94mLoss[0m : 4.57561
[1mStep[0m  [6/21], [94mLoss[0m : 4.53265
[1mStep[0m  [8/21], [94mLoss[0m : 4.51347
[1mStep[0m  [10/21], [94mLoss[0m : 4.43995
[1mStep[0m  [12/21], [94mLoss[0m : 4.11115
[1mStep[0m  [14/21], [94mLoss[0m : 4.73703
[1mStep[0m  [16/21], [94mLoss[0m : 4.32542
[1mStep[0m  [18/21], [94mLoss[0m : 4.59997
[1mStep[0m  [20/21], [94mLoss[0m : 4.54758

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 4.492, [92mTest[0m: 4.580, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.33432
[1mStep[0m  [2/21], [94mLoss[0m : 4.65105
[1mStep[0m  [4/21], [94mLoss[0m : 4.47629
[1mStep[0m  [6/21], [94mLoss[0m : 4.62222
[1mStep[0m  [8/21], [94mLoss[0m : 4.22859
[1mStep[0m  [10/21], [94mLoss[0m : 4.51597
[1mStep[0m  [12/21], [94mLoss[0m : 4.14744
[1mStep[0m  [14/21], [94mLoss[0m : 4.19107
[1mStep[0m  [16/21], [94mLoss[0m : 4.32846
[1mStep[0m  [18/21], [94mLoss[0m : 4.07081
[1mStep[0m  [20/21], [94mLoss[0m : 4.57252

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 4.333, [92mTest[0m: 4.419, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.28495
[1mStep[0m  [2/21], [94mLoss[0m : 4.30451
[1mStep[0m  [4/21], [94mLoss[0m : 4.16441
[1mStep[0m  [6/21], [94mLoss[0m : 4.35229
[1mStep[0m  [8/21], [94mLoss[0m : 4.20482
[1mStep[0m  [10/21], [94mLoss[0m : 4.18025
[1mStep[0m  [12/21], [94mLoss[0m : 4.19816
[1mStep[0m  [14/21], [94mLoss[0m : 4.25068
[1mStep[0m  [16/21], [94mLoss[0m : 4.09276
[1mStep[0m  [18/21], [94mLoss[0m : 3.85813
[1mStep[0m  [20/21], [94mLoss[0m : 3.91331

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.194, [92mTest[0m: 4.262, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.24822
[1mStep[0m  [2/21], [94mLoss[0m : 4.08059
[1mStep[0m  [4/21], [94mLoss[0m : 4.13765
[1mStep[0m  [6/21], [94mLoss[0m : 4.07893
[1mStep[0m  [8/21], [94mLoss[0m : 4.23772
[1mStep[0m  [10/21], [94mLoss[0m : 3.91413
[1mStep[0m  [12/21], [94mLoss[0m : 4.02396
[1mStep[0m  [14/21], [94mLoss[0m : 3.90923
[1mStep[0m  [16/21], [94mLoss[0m : 4.02595
[1mStep[0m  [18/21], [94mLoss[0m : 4.06054
[1mStep[0m  [20/21], [94mLoss[0m : 3.95744

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.061, [92mTest[0m: 4.132, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.12940
[1mStep[0m  [2/21], [94mLoss[0m : 4.03174
[1mStep[0m  [4/21], [94mLoss[0m : 3.85173
[1mStep[0m  [6/21], [94mLoss[0m : 3.86541
[1mStep[0m  [8/21], [94mLoss[0m : 3.88811
[1mStep[0m  [10/21], [94mLoss[0m : 3.74190
[1mStep[0m  [12/21], [94mLoss[0m : 3.81527
[1mStep[0m  [14/21], [94mLoss[0m : 4.13316
[1mStep[0m  [16/21], [94mLoss[0m : 3.71673
[1mStep[0m  [18/21], [94mLoss[0m : 3.72669
[1mStep[0m  [20/21], [94mLoss[0m : 3.79676

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.944, [92mTest[0m: 4.004, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.888
====================================

Phase 1 - Evaluation MAE:  3.8881117956978932
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 3.90919
[1mStep[0m  [2/21], [94mLoss[0m : 3.77521
[1mStep[0m  [4/21], [94mLoss[0m : 3.82894
[1mStep[0m  [6/21], [94mLoss[0m : 3.53884
[1mStep[0m  [8/21], [94mLoss[0m : 3.84244
[1mStep[0m  [10/21], [94mLoss[0m : 3.68592
[1mStep[0m  [12/21], [94mLoss[0m : 3.90226
[1mStep[0m  [14/21], [94mLoss[0m : 3.71583
[1mStep[0m  [16/21], [94mLoss[0m : 4.00840
[1mStep[0m  [18/21], [94mLoss[0m : 3.85270
[1mStep[0m  [20/21], [94mLoss[0m : 3.96982

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.827, [92mTest[0m: 3.893, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.95810
[1mStep[0m  [2/21], [94mLoss[0m : 3.78355
[1mStep[0m  [4/21], [94mLoss[0m : 3.67565
[1mStep[0m  [6/21], [94mLoss[0m : 3.76391
[1mStep[0m  [8/21], [94mLoss[0m : 3.69692
[1mStep[0m  [10/21], [94mLoss[0m : 3.75760
[1mStep[0m  [12/21], [94mLoss[0m : 3.58035
[1mStep[0m  [14/21], [94mLoss[0m : 3.86735
[1mStep[0m  [16/21], [94mLoss[0m : 3.80076
[1mStep[0m  [18/21], [94mLoss[0m : 3.46505
[1mStep[0m  [20/21], [94mLoss[0m : 3.76656

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.716, [92mTest[0m: 3.768, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.53638
[1mStep[0m  [2/21], [94mLoss[0m : 3.50250
[1mStep[0m  [4/21], [94mLoss[0m : 3.85308
[1mStep[0m  [6/21], [94mLoss[0m : 3.78820
[1mStep[0m  [8/21], [94mLoss[0m : 3.54750
[1mStep[0m  [10/21], [94mLoss[0m : 3.73739
[1mStep[0m  [12/21], [94mLoss[0m : 3.39707
[1mStep[0m  [14/21], [94mLoss[0m : 3.50690
[1mStep[0m  [16/21], [94mLoss[0m : 3.79727
[1mStep[0m  [18/21], [94mLoss[0m : 3.71761
[1mStep[0m  [20/21], [94mLoss[0m : 3.60684

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.620, [92mTest[0m: 3.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.83396
[1mStep[0m  [2/21], [94mLoss[0m : 3.39147
[1mStep[0m  [4/21], [94mLoss[0m : 3.78702
[1mStep[0m  [6/21], [94mLoss[0m : 3.40823
[1mStep[0m  [8/21], [94mLoss[0m : 3.59611
[1mStep[0m  [10/21], [94mLoss[0m : 3.21857
[1mStep[0m  [12/21], [94mLoss[0m : 3.48328
[1mStep[0m  [14/21], [94mLoss[0m : 3.39549
[1mStep[0m  [16/21], [94mLoss[0m : 3.48733
[1mStep[0m  [18/21], [94mLoss[0m : 3.48650
[1mStep[0m  [20/21], [94mLoss[0m : 3.72740

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.522, [92mTest[0m: 3.575, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.51917
[1mStep[0m  [2/21], [94mLoss[0m : 3.49546
[1mStep[0m  [4/21], [94mLoss[0m : 3.30432
[1mStep[0m  [6/21], [94mLoss[0m : 3.58879
[1mStep[0m  [8/21], [94mLoss[0m : 3.33126
[1mStep[0m  [10/21], [94mLoss[0m : 3.53297
[1mStep[0m  [12/21], [94mLoss[0m : 3.28438
[1mStep[0m  [14/21], [94mLoss[0m : 3.54377
[1mStep[0m  [16/21], [94mLoss[0m : 3.29282
[1mStep[0m  [18/21], [94mLoss[0m : 3.27950
[1mStep[0m  [20/21], [94mLoss[0m : 3.29062

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.437, [92mTest[0m: 3.475, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.33358
[1mStep[0m  [2/21], [94mLoss[0m : 3.47139
[1mStep[0m  [4/21], [94mLoss[0m : 3.43253
[1mStep[0m  [6/21], [94mLoss[0m : 3.32770
[1mStep[0m  [8/21], [94mLoss[0m : 3.30630
[1mStep[0m  [10/21], [94mLoss[0m : 3.18154
[1mStep[0m  [12/21], [94mLoss[0m : 3.46565
[1mStep[0m  [14/21], [94mLoss[0m : 3.41226
[1mStep[0m  [16/21], [94mLoss[0m : 3.35301
[1mStep[0m  [18/21], [94mLoss[0m : 3.38675
[1mStep[0m  [20/21], [94mLoss[0m : 3.42721

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.367, [92mTest[0m: 3.398, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.30259
[1mStep[0m  [2/21], [94mLoss[0m : 3.11749
[1mStep[0m  [4/21], [94mLoss[0m : 3.38318
[1mStep[0m  [6/21], [94mLoss[0m : 3.35739
[1mStep[0m  [8/21], [94mLoss[0m : 3.28993
[1mStep[0m  [10/21], [94mLoss[0m : 3.31868
[1mStep[0m  [12/21], [94mLoss[0m : 3.30332
[1mStep[0m  [14/21], [94mLoss[0m : 3.52959
[1mStep[0m  [16/21], [94mLoss[0m : 3.33659
[1mStep[0m  [18/21], [94mLoss[0m : 3.29728
[1mStep[0m  [20/21], [94mLoss[0m : 3.32150

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.296, [92mTest[0m: 3.329, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.07919
[1mStep[0m  [2/21], [94mLoss[0m : 3.24473
[1mStep[0m  [4/21], [94mLoss[0m : 3.07721
[1mStep[0m  [6/21], [94mLoss[0m : 3.34188
[1mStep[0m  [8/21], [94mLoss[0m : 3.09968
[1mStep[0m  [10/21], [94mLoss[0m : 3.32830
[1mStep[0m  [12/21], [94mLoss[0m : 3.47357
[1mStep[0m  [14/21], [94mLoss[0m : 3.31012
[1mStep[0m  [16/21], [94mLoss[0m : 3.24219
[1mStep[0m  [18/21], [94mLoss[0m : 3.30360
[1mStep[0m  [20/21], [94mLoss[0m : 3.16997

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.245, [92mTest[0m: 3.257, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.39692
[1mStep[0m  [2/21], [94mLoss[0m : 3.06126
[1mStep[0m  [4/21], [94mLoss[0m : 3.23545
[1mStep[0m  [6/21], [94mLoss[0m : 3.25239
[1mStep[0m  [8/21], [94mLoss[0m : 3.15803
[1mStep[0m  [10/21], [94mLoss[0m : 3.18682
[1mStep[0m  [12/21], [94mLoss[0m : 3.25458
[1mStep[0m  [14/21], [94mLoss[0m : 3.10379
[1mStep[0m  [16/21], [94mLoss[0m : 3.32791
[1mStep[0m  [18/21], [94mLoss[0m : 3.25142
[1mStep[0m  [20/21], [94mLoss[0m : 3.20148

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.186, [92mTest[0m: 3.209, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.25940
[1mStep[0m  [2/21], [94mLoss[0m : 3.03619
[1mStep[0m  [4/21], [94mLoss[0m : 3.23303
[1mStep[0m  [6/21], [94mLoss[0m : 3.18926
[1mStep[0m  [8/21], [94mLoss[0m : 3.19183
[1mStep[0m  [10/21], [94mLoss[0m : 3.19020
[1mStep[0m  [12/21], [94mLoss[0m : 2.94478
[1mStep[0m  [14/21], [94mLoss[0m : 3.20831
[1mStep[0m  [16/21], [94mLoss[0m : 3.04528
[1mStep[0m  [18/21], [94mLoss[0m : 3.20806
[1mStep[0m  [20/21], [94mLoss[0m : 3.12208

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.147, [92mTest[0m: 3.146, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.29051
[1mStep[0m  [2/21], [94mLoss[0m : 2.96229
[1mStep[0m  [4/21], [94mLoss[0m : 2.97212
[1mStep[0m  [6/21], [94mLoss[0m : 3.10682
[1mStep[0m  [8/21], [94mLoss[0m : 3.22145
[1mStep[0m  [10/21], [94mLoss[0m : 3.11200
[1mStep[0m  [12/21], [94mLoss[0m : 3.21709
[1mStep[0m  [14/21], [94mLoss[0m : 3.08621
[1mStep[0m  [16/21], [94mLoss[0m : 3.10135
[1mStep[0m  [18/21], [94mLoss[0m : 3.11951
[1mStep[0m  [20/21], [94mLoss[0m : 2.87847

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 3.107, [92mTest[0m: 3.114, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.27338
[1mStep[0m  [2/21], [94mLoss[0m : 3.03909
[1mStep[0m  [4/21], [94mLoss[0m : 2.97414
[1mStep[0m  [6/21], [94mLoss[0m : 3.29204
[1mStep[0m  [8/21], [94mLoss[0m : 3.11653
[1mStep[0m  [10/21], [94mLoss[0m : 3.06884
[1mStep[0m  [12/21], [94mLoss[0m : 3.17387
[1mStep[0m  [14/21], [94mLoss[0m : 2.85957
[1mStep[0m  [16/21], [94mLoss[0m : 3.08844
[1mStep[0m  [18/21], [94mLoss[0m : 3.09080
[1mStep[0m  [20/21], [94mLoss[0m : 3.10282

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 3.058, [92mTest[0m: 3.066, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.10288
[1mStep[0m  [2/21], [94mLoss[0m : 2.99522
[1mStep[0m  [4/21], [94mLoss[0m : 3.02583
[1mStep[0m  [6/21], [94mLoss[0m : 3.04060
[1mStep[0m  [8/21], [94mLoss[0m : 2.82740
[1mStep[0m  [10/21], [94mLoss[0m : 2.96761
[1mStep[0m  [12/21], [94mLoss[0m : 3.21009
[1mStep[0m  [14/21], [94mLoss[0m : 3.06036
[1mStep[0m  [16/21], [94mLoss[0m : 3.25891
[1mStep[0m  [18/21], [94mLoss[0m : 2.99969
[1mStep[0m  [20/21], [94mLoss[0m : 3.06487

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.021, [92mTest[0m: 3.020, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.96832
[1mStep[0m  [2/21], [94mLoss[0m : 3.14309
[1mStep[0m  [4/21], [94mLoss[0m : 2.97805
[1mStep[0m  [6/21], [94mLoss[0m : 2.90498
[1mStep[0m  [8/21], [94mLoss[0m : 3.10478
[1mStep[0m  [10/21], [94mLoss[0m : 3.03017
[1mStep[0m  [12/21], [94mLoss[0m : 2.90864
[1mStep[0m  [14/21], [94mLoss[0m : 2.86833
[1mStep[0m  [16/21], [94mLoss[0m : 2.91957
[1mStep[0m  [18/21], [94mLoss[0m : 3.04000
[1mStep[0m  [20/21], [94mLoss[0m : 2.96575

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.990, [92mTest[0m: 2.991, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.02586
[1mStep[0m  [2/21], [94mLoss[0m : 3.00142
[1mStep[0m  [4/21], [94mLoss[0m : 3.07792
[1mStep[0m  [6/21], [94mLoss[0m : 2.97679
[1mStep[0m  [8/21], [94mLoss[0m : 2.94562
[1mStep[0m  [10/21], [94mLoss[0m : 2.87068
[1mStep[0m  [12/21], [94mLoss[0m : 3.06915
[1mStep[0m  [14/21], [94mLoss[0m : 2.95116
[1mStep[0m  [16/21], [94mLoss[0m : 3.03370
[1mStep[0m  [18/21], [94mLoss[0m : 2.98633
[1mStep[0m  [20/21], [94mLoss[0m : 2.87419

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.960, [92mTest[0m: 2.957, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.06429
[1mStep[0m  [2/21], [94mLoss[0m : 3.02768
[1mStep[0m  [4/21], [94mLoss[0m : 2.95611
[1mStep[0m  [6/21], [94mLoss[0m : 2.99702
[1mStep[0m  [8/21], [94mLoss[0m : 2.85957
[1mStep[0m  [10/21], [94mLoss[0m : 3.06696
[1mStep[0m  [12/21], [94mLoss[0m : 2.80821
[1mStep[0m  [14/21], [94mLoss[0m : 2.83944
[1mStep[0m  [16/21], [94mLoss[0m : 2.95717
[1mStep[0m  [18/21], [94mLoss[0m : 3.03051
[1mStep[0m  [20/21], [94mLoss[0m : 2.87097

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.929, [92mTest[0m: 2.925, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76498
[1mStep[0m  [2/21], [94mLoss[0m : 2.89653
[1mStep[0m  [4/21], [94mLoss[0m : 2.73997
[1mStep[0m  [6/21], [94mLoss[0m : 2.95267
[1mStep[0m  [8/21], [94mLoss[0m : 2.79127
[1mStep[0m  [10/21], [94mLoss[0m : 3.02299
[1mStep[0m  [12/21], [94mLoss[0m : 3.10058
[1mStep[0m  [14/21], [94mLoss[0m : 3.06589
[1mStep[0m  [16/21], [94mLoss[0m : 2.91611
[1mStep[0m  [18/21], [94mLoss[0m : 2.83452
[1mStep[0m  [20/21], [94mLoss[0m : 2.86784

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.913, [92mTest[0m: 2.900, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.05289
[1mStep[0m  [2/21], [94mLoss[0m : 2.69734
[1mStep[0m  [4/21], [94mLoss[0m : 2.56292
[1mStep[0m  [6/21], [94mLoss[0m : 2.87869
[1mStep[0m  [8/21], [94mLoss[0m : 2.99284
[1mStep[0m  [10/21], [94mLoss[0m : 2.83542
[1mStep[0m  [12/21], [94mLoss[0m : 2.94295
[1mStep[0m  [14/21], [94mLoss[0m : 3.09826
[1mStep[0m  [16/21], [94mLoss[0m : 3.07579
[1mStep[0m  [18/21], [94mLoss[0m : 2.82426
[1mStep[0m  [20/21], [94mLoss[0m : 3.03429

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.882, [92mTest[0m: 2.874, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66345
[1mStep[0m  [2/21], [94mLoss[0m : 2.83796
[1mStep[0m  [4/21], [94mLoss[0m : 2.83307
[1mStep[0m  [6/21], [94mLoss[0m : 2.69651
[1mStep[0m  [8/21], [94mLoss[0m : 2.99337
[1mStep[0m  [10/21], [94mLoss[0m : 2.83641
[1mStep[0m  [12/21], [94mLoss[0m : 2.88945
[1mStep[0m  [14/21], [94mLoss[0m : 2.87651
[1mStep[0m  [16/21], [94mLoss[0m : 2.92452
[1mStep[0m  [18/21], [94mLoss[0m : 2.87697
[1mStep[0m  [20/21], [94mLoss[0m : 3.00141

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.857, [92mTest[0m: 2.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82936
[1mStep[0m  [2/21], [94mLoss[0m : 2.81589
[1mStep[0m  [4/21], [94mLoss[0m : 2.83407
[1mStep[0m  [6/21], [94mLoss[0m : 2.81843
[1mStep[0m  [8/21], [94mLoss[0m : 3.08760
[1mStep[0m  [10/21], [94mLoss[0m : 2.92450
[1mStep[0m  [12/21], [94mLoss[0m : 3.05516
[1mStep[0m  [14/21], [94mLoss[0m : 2.86420
[1mStep[0m  [16/21], [94mLoss[0m : 2.79325
[1mStep[0m  [18/21], [94mLoss[0m : 2.82845
[1mStep[0m  [20/21], [94mLoss[0m : 2.86919

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.843, [92mTest[0m: 2.824, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76099
[1mStep[0m  [2/21], [94mLoss[0m : 2.89779
[1mStep[0m  [4/21], [94mLoss[0m : 2.90638
[1mStep[0m  [6/21], [94mLoss[0m : 2.74635
[1mStep[0m  [8/21], [94mLoss[0m : 2.82356
[1mStep[0m  [10/21], [94mLoss[0m : 2.68288
[1mStep[0m  [12/21], [94mLoss[0m : 2.63018
[1mStep[0m  [14/21], [94mLoss[0m : 2.86605
[1mStep[0m  [16/21], [94mLoss[0m : 2.87662
[1mStep[0m  [18/21], [94mLoss[0m : 2.94232
[1mStep[0m  [20/21], [94mLoss[0m : 2.82532

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.817, [92mTest[0m: 2.802, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.85564
[1mStep[0m  [2/21], [94mLoss[0m : 2.72805
[1mStep[0m  [4/21], [94mLoss[0m : 2.78539
[1mStep[0m  [6/21], [94mLoss[0m : 2.56686
[1mStep[0m  [8/21], [94mLoss[0m : 2.92436
[1mStep[0m  [10/21], [94mLoss[0m : 2.74015
[1mStep[0m  [12/21], [94mLoss[0m : 2.79182
[1mStep[0m  [14/21], [94mLoss[0m : 2.91601
[1mStep[0m  [16/21], [94mLoss[0m : 2.99236
[1mStep[0m  [18/21], [94mLoss[0m : 2.82444
[1mStep[0m  [20/21], [94mLoss[0m : 2.79546

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.795, [92mTest[0m: 2.780, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68525
[1mStep[0m  [2/21], [94mLoss[0m : 2.83731
[1mStep[0m  [4/21], [94mLoss[0m : 2.78561
[1mStep[0m  [6/21], [94mLoss[0m : 2.91550
[1mStep[0m  [8/21], [94mLoss[0m : 2.78507
[1mStep[0m  [10/21], [94mLoss[0m : 2.77145
[1mStep[0m  [12/21], [94mLoss[0m : 2.72499
[1mStep[0m  [14/21], [94mLoss[0m : 2.77973
[1mStep[0m  [16/21], [94mLoss[0m : 2.69485
[1mStep[0m  [18/21], [94mLoss[0m : 2.82683
[1mStep[0m  [20/21], [94mLoss[0m : 2.80920

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.779, [92mTest[0m: 2.763, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.70959
[1mStep[0m  [2/21], [94mLoss[0m : 2.88759
[1mStep[0m  [4/21], [94mLoss[0m : 2.82272
[1mStep[0m  [6/21], [94mLoss[0m : 2.80547
[1mStep[0m  [8/21], [94mLoss[0m : 2.87759
[1mStep[0m  [10/21], [94mLoss[0m : 2.72007
[1mStep[0m  [12/21], [94mLoss[0m : 2.69100
[1mStep[0m  [14/21], [94mLoss[0m : 2.50950
[1mStep[0m  [16/21], [94mLoss[0m : 2.91167
[1mStep[0m  [18/21], [94mLoss[0m : 2.77919
[1mStep[0m  [20/21], [94mLoss[0m : 2.67629

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.775, [92mTest[0m: 2.748, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83277
[1mStep[0m  [2/21], [94mLoss[0m : 2.56100
[1mStep[0m  [4/21], [94mLoss[0m : 2.69218
[1mStep[0m  [6/21], [94mLoss[0m : 2.77245
[1mStep[0m  [8/21], [94mLoss[0m : 2.82131
[1mStep[0m  [10/21], [94mLoss[0m : 2.82572
[1mStep[0m  [12/21], [94mLoss[0m : 2.88064
[1mStep[0m  [14/21], [94mLoss[0m : 2.67856
[1mStep[0m  [16/21], [94mLoss[0m : 2.77365
[1mStep[0m  [18/21], [94mLoss[0m : 2.76576
[1mStep[0m  [20/21], [94mLoss[0m : 2.75832

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.750, [92mTest[0m: 2.726, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80989
[1mStep[0m  [2/21], [94mLoss[0m : 2.65281
[1mStep[0m  [4/21], [94mLoss[0m : 2.78037
[1mStep[0m  [6/21], [94mLoss[0m : 2.80002
[1mStep[0m  [8/21], [94mLoss[0m : 2.61638
[1mStep[0m  [10/21], [94mLoss[0m : 2.61551
[1mStep[0m  [12/21], [94mLoss[0m : 2.85244
[1mStep[0m  [14/21], [94mLoss[0m : 2.80233
[1mStep[0m  [16/21], [94mLoss[0m : 2.84522
[1mStep[0m  [18/21], [94mLoss[0m : 2.68775
[1mStep[0m  [20/21], [94mLoss[0m : 2.62317

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.746, [92mTest[0m: 2.720, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73859
[1mStep[0m  [2/21], [94mLoss[0m : 2.78397
[1mStep[0m  [4/21], [94mLoss[0m : 2.79383
[1mStep[0m  [6/21], [94mLoss[0m : 2.64408
[1mStep[0m  [8/21], [94mLoss[0m : 2.72340
[1mStep[0m  [10/21], [94mLoss[0m : 2.73745
[1mStep[0m  [12/21], [94mLoss[0m : 2.75845
[1mStep[0m  [14/21], [94mLoss[0m : 2.73978
[1mStep[0m  [16/21], [94mLoss[0m : 2.76065
[1mStep[0m  [18/21], [94mLoss[0m : 2.72633
[1mStep[0m  [20/21], [94mLoss[0m : 2.69970

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.728, [92mTest[0m: 2.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64983
[1mStep[0m  [2/21], [94mLoss[0m : 2.90298
[1mStep[0m  [4/21], [94mLoss[0m : 2.62338
[1mStep[0m  [6/21], [94mLoss[0m : 2.71926
[1mStep[0m  [8/21], [94mLoss[0m : 2.71671
[1mStep[0m  [10/21], [94mLoss[0m : 2.72739
[1mStep[0m  [12/21], [94mLoss[0m : 2.84442
[1mStep[0m  [14/21], [94mLoss[0m : 2.72832
[1mStep[0m  [16/21], [94mLoss[0m : 2.54066
[1mStep[0m  [18/21], [94mLoss[0m : 2.76227
[1mStep[0m  [20/21], [94mLoss[0m : 2.70413

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.688, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82474
[1mStep[0m  [2/21], [94mLoss[0m : 2.79205
[1mStep[0m  [4/21], [94mLoss[0m : 2.72571
[1mStep[0m  [6/21], [94mLoss[0m : 2.73003
[1mStep[0m  [8/21], [94mLoss[0m : 2.65943
[1mStep[0m  [10/21], [94mLoss[0m : 2.59534
[1mStep[0m  [12/21], [94mLoss[0m : 2.66902
[1mStep[0m  [14/21], [94mLoss[0m : 2.62455
[1mStep[0m  [16/21], [94mLoss[0m : 2.69153
[1mStep[0m  [18/21], [94mLoss[0m : 2.76464
[1mStep[0m  [20/21], [94mLoss[0m : 2.71968

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.708, [92mTest[0m: 2.671, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.78578
[1mStep[0m  [2/21], [94mLoss[0m : 2.84350
[1mStep[0m  [4/21], [94mLoss[0m : 2.86137
[1mStep[0m  [6/21], [94mLoss[0m : 2.58048
[1mStep[0m  [8/21], [94mLoss[0m : 2.64482
[1mStep[0m  [10/21], [94mLoss[0m : 2.70837
[1mStep[0m  [12/21], [94mLoss[0m : 2.57243
[1mStep[0m  [14/21], [94mLoss[0m : 2.50063
[1mStep[0m  [16/21], [94mLoss[0m : 2.67267
[1mStep[0m  [18/21], [94mLoss[0m : 2.72500
[1mStep[0m  [20/21], [94mLoss[0m : 2.73234

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.662, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.645
====================================

Phase 2 - Evaluation MAE:  2.644707373210362
MAE score P1      3.888112
MAE score P2      2.644707
loss               2.70067
learning_rate       0.0001
batch_size             512
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 17, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 10.68735
[1mStep[0m  [2/21], [94mLoss[0m : 10.72806
[1mStep[0m  [4/21], [94mLoss[0m : 10.47248
[1mStep[0m  [6/21], [94mLoss[0m : 10.40433
[1mStep[0m  [8/21], [94mLoss[0m : 10.71980
[1mStep[0m  [10/21], [94mLoss[0m : 10.48452
[1mStep[0m  [12/21], [94mLoss[0m : 10.62019
[1mStep[0m  [14/21], [94mLoss[0m : 10.93626
[1mStep[0m  [16/21], [94mLoss[0m : 10.63029
[1mStep[0m  [18/21], [94mLoss[0m : 10.33118
[1mStep[0m  [20/21], [94mLoss[0m : 10.39502

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.629, [92mTest[0m: 10.779, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38936
[1mStep[0m  [2/21], [94mLoss[0m : 10.58832
[1mStep[0m  [4/21], [94mLoss[0m : 10.48080
[1mStep[0m  [6/21], [94mLoss[0m : 10.56330
[1mStep[0m  [8/21], [94mLoss[0m : 10.33027
[1mStep[0m  [10/21], [94mLoss[0m : 10.30202
[1mStep[0m  [12/21], [94mLoss[0m : 10.10246
[1mStep[0m  [14/21], [94mLoss[0m : 10.29825
[1mStep[0m  [16/21], [94mLoss[0m : 10.18380
[1mStep[0m  [18/21], [94mLoss[0m : 10.06389
[1mStep[0m  [20/21], [94mLoss[0m : 9.89913

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.316, [92mTest[0m: 10.441, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.88451
[1mStep[0m  [2/21], [94mLoss[0m : 9.75972
[1mStep[0m  [4/21], [94mLoss[0m : 10.15017
[1mStep[0m  [6/21], [94mLoss[0m : 10.01222
[1mStep[0m  [8/21], [94mLoss[0m : 10.20732
[1mStep[0m  [10/21], [94mLoss[0m : 10.12274
[1mStep[0m  [12/21], [94mLoss[0m : 10.17387
[1mStep[0m  [14/21], [94mLoss[0m : 10.12005
[1mStep[0m  [16/21], [94mLoss[0m : 9.82277
[1mStep[0m  [18/21], [94mLoss[0m : 9.77615
[1mStep[0m  [20/21], [94mLoss[0m : 9.67972

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.992, [92mTest[0m: 10.113, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.80838
[1mStep[0m  [2/21], [94mLoss[0m : 9.89417
[1mStep[0m  [4/21], [94mLoss[0m : 9.89132
[1mStep[0m  [6/21], [94mLoss[0m : 9.63535
[1mStep[0m  [8/21], [94mLoss[0m : 9.65947
[1mStep[0m  [10/21], [94mLoss[0m : 9.34608
[1mStep[0m  [12/21], [94mLoss[0m : 9.79374
[1mStep[0m  [14/21], [94mLoss[0m : 9.45376
[1mStep[0m  [16/21], [94mLoss[0m : 9.75299
[1mStep[0m  [18/21], [94mLoss[0m : 9.45936
[1mStep[0m  [20/21], [94mLoss[0m : 9.42153

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.676, [92mTest[0m: 9.792, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.56330
[1mStep[0m  [2/21], [94mLoss[0m : 9.43552
[1mStep[0m  [4/21], [94mLoss[0m : 9.25115
[1mStep[0m  [6/21], [94mLoss[0m : 9.22427
[1mStep[0m  [8/21], [94mLoss[0m : 9.15701
[1mStep[0m  [10/21], [94mLoss[0m : 9.62886
[1mStep[0m  [12/21], [94mLoss[0m : 9.53068
[1mStep[0m  [14/21], [94mLoss[0m : 9.41213
[1mStep[0m  [16/21], [94mLoss[0m : 9.31484
[1mStep[0m  [18/21], [94mLoss[0m : 9.01220
[1mStep[0m  [20/21], [94mLoss[0m : 9.15189

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.353, [92mTest[0m: 9.462, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.07055
[1mStep[0m  [2/21], [94mLoss[0m : 9.12738
[1mStep[0m  [4/21], [94mLoss[0m : 9.46187
[1mStep[0m  [6/21], [94mLoss[0m : 8.93324
[1mStep[0m  [8/21], [94mLoss[0m : 8.75529
[1mStep[0m  [10/21], [94mLoss[0m : 8.94368
[1mStep[0m  [12/21], [94mLoss[0m : 9.14371
[1mStep[0m  [14/21], [94mLoss[0m : 9.23087
[1mStep[0m  [16/21], [94mLoss[0m : 8.78059
[1mStep[0m  [18/21], [94mLoss[0m : 9.23711
[1mStep[0m  [20/21], [94mLoss[0m : 8.92706

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.033, [92mTest[0m: 9.162, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.85701
[1mStep[0m  [2/21], [94mLoss[0m : 8.74345
[1mStep[0m  [4/21], [94mLoss[0m : 8.96342
[1mStep[0m  [6/21], [94mLoss[0m : 8.38099
[1mStep[0m  [8/21], [94mLoss[0m : 8.73123
[1mStep[0m  [10/21], [94mLoss[0m : 8.84811
[1mStep[0m  [12/21], [94mLoss[0m : 8.62461
[1mStep[0m  [14/21], [94mLoss[0m : 8.66223
[1mStep[0m  [16/21], [94mLoss[0m : 8.74056
[1mStep[0m  [18/21], [94mLoss[0m : 8.50608
[1mStep[0m  [20/21], [94mLoss[0m : 8.43444

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.709, [92mTest[0m: 8.815, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.48421
[1mStep[0m  [2/21], [94mLoss[0m : 8.32767
[1mStep[0m  [4/21], [94mLoss[0m : 8.49008
[1mStep[0m  [6/21], [94mLoss[0m : 8.35820
[1mStep[0m  [8/21], [94mLoss[0m : 8.29021
[1mStep[0m  [10/21], [94mLoss[0m : 8.67474
[1mStep[0m  [12/21], [94mLoss[0m : 8.25037
[1mStep[0m  [14/21], [94mLoss[0m : 8.39825
[1mStep[0m  [16/21], [94mLoss[0m : 8.28875
[1mStep[0m  [18/21], [94mLoss[0m : 8.25445
[1mStep[0m  [20/21], [94mLoss[0m : 8.35541

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.391, [92mTest[0m: 8.518, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.55591
[1mStep[0m  [2/21], [94mLoss[0m : 8.16986
[1mStep[0m  [4/21], [94mLoss[0m : 8.34093
[1mStep[0m  [6/21], [94mLoss[0m : 7.93938
[1mStep[0m  [8/21], [94mLoss[0m : 7.90076
[1mStep[0m  [10/21], [94mLoss[0m : 7.93731
[1mStep[0m  [12/21], [94mLoss[0m : 8.18120
[1mStep[0m  [14/21], [94mLoss[0m : 7.99103
[1mStep[0m  [16/21], [94mLoss[0m : 7.91140
[1mStep[0m  [18/21], [94mLoss[0m : 7.80370
[1mStep[0m  [20/21], [94mLoss[0m : 7.89940

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.072, [92mTest[0m: 8.180, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.01729
[1mStep[0m  [2/21], [94mLoss[0m : 7.86042
[1mStep[0m  [4/21], [94mLoss[0m : 7.78777
[1mStep[0m  [6/21], [94mLoss[0m : 7.97309
[1mStep[0m  [8/21], [94mLoss[0m : 7.85999
[1mStep[0m  [10/21], [94mLoss[0m : 7.80793
[1mStep[0m  [12/21], [94mLoss[0m : 7.82523
[1mStep[0m  [14/21], [94mLoss[0m : 7.48774
[1mStep[0m  [16/21], [94mLoss[0m : 7.80672
[1mStep[0m  [18/21], [94mLoss[0m : 7.19573
[1mStep[0m  [20/21], [94mLoss[0m : 7.75289

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.755, [92mTest[0m: 7.877, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.57726
[1mStep[0m  [2/21], [94mLoss[0m : 7.56870
[1mStep[0m  [4/21], [94mLoss[0m : 7.40484
[1mStep[0m  [6/21], [94mLoss[0m : 7.38644
[1mStep[0m  [8/21], [94mLoss[0m : 7.10642
[1mStep[0m  [10/21], [94mLoss[0m : 7.40660
[1mStep[0m  [12/21], [94mLoss[0m : 7.71022
[1mStep[0m  [14/21], [94mLoss[0m : 7.40706
[1mStep[0m  [16/21], [94mLoss[0m : 7.36392
[1mStep[0m  [18/21], [94mLoss[0m : 7.39239
[1mStep[0m  [20/21], [94mLoss[0m : 7.40154

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.440, [92mTest[0m: 7.548, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.26364
[1mStep[0m  [2/21], [94mLoss[0m : 7.37722
[1mStep[0m  [4/21], [94mLoss[0m : 7.06383
[1mStep[0m  [6/21], [94mLoss[0m : 7.31067
[1mStep[0m  [8/21], [94mLoss[0m : 6.96095
[1mStep[0m  [10/21], [94mLoss[0m : 6.86353
[1mStep[0m  [12/21], [94mLoss[0m : 6.91177
[1mStep[0m  [14/21], [94mLoss[0m : 7.14155
[1mStep[0m  [16/21], [94mLoss[0m : 7.25416
[1mStep[0m  [18/21], [94mLoss[0m : 7.07832
[1mStep[0m  [20/21], [94mLoss[0m : 7.15442

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.118, [92mTest[0m: 7.236, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.85859
[1mStep[0m  [2/21], [94mLoss[0m : 6.79516
[1mStep[0m  [4/21], [94mLoss[0m : 6.84019
[1mStep[0m  [6/21], [94mLoss[0m : 7.26670
[1mStep[0m  [8/21], [94mLoss[0m : 6.76042
[1mStep[0m  [10/21], [94mLoss[0m : 6.86788
[1mStep[0m  [12/21], [94mLoss[0m : 6.68384
[1mStep[0m  [14/21], [94mLoss[0m : 6.68840
[1mStep[0m  [16/21], [94mLoss[0m : 6.56603
[1mStep[0m  [18/21], [94mLoss[0m : 6.58963
[1mStep[0m  [20/21], [94mLoss[0m : 6.63449

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.786, [92mTest[0m: 6.914, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.59102
[1mStep[0m  [2/21], [94mLoss[0m : 6.58611
[1mStep[0m  [4/21], [94mLoss[0m : 6.55638
[1mStep[0m  [6/21], [94mLoss[0m : 6.78840
[1mStep[0m  [8/21], [94mLoss[0m : 6.58761
[1mStep[0m  [10/21], [94mLoss[0m : 6.38468
[1mStep[0m  [12/21], [94mLoss[0m : 6.38355
[1mStep[0m  [14/21], [94mLoss[0m : 6.32855
[1mStep[0m  [16/21], [94mLoss[0m : 6.48252
[1mStep[0m  [18/21], [94mLoss[0m : 6.23569
[1mStep[0m  [20/21], [94mLoss[0m : 6.34102

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.469, [92mTest[0m: 6.586, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.64145
[1mStep[0m  [2/21], [94mLoss[0m : 6.12423
[1mStep[0m  [4/21], [94mLoss[0m : 6.23976
[1mStep[0m  [6/21], [94mLoss[0m : 6.16145
[1mStep[0m  [8/21], [94mLoss[0m : 6.29472
[1mStep[0m  [10/21], [94mLoss[0m : 6.20529
[1mStep[0m  [12/21], [94mLoss[0m : 6.00581
[1mStep[0m  [14/21], [94mLoss[0m : 6.19498
[1mStep[0m  [16/21], [94mLoss[0m : 5.72818
[1mStep[0m  [18/21], [94mLoss[0m : 6.17503
[1mStep[0m  [20/21], [94mLoss[0m : 6.12667

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.157, [92mTest[0m: 6.272, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.89525
[1mStep[0m  [2/21], [94mLoss[0m : 6.01120
[1mStep[0m  [4/21], [94mLoss[0m : 6.01674
[1mStep[0m  [6/21], [94mLoss[0m : 5.79613
[1mStep[0m  [8/21], [94mLoss[0m : 5.91593
[1mStep[0m  [10/21], [94mLoss[0m : 5.80247
[1mStep[0m  [12/21], [94mLoss[0m : 5.92673
[1mStep[0m  [14/21], [94mLoss[0m : 5.53637
[1mStep[0m  [16/21], [94mLoss[0m : 5.53508
[1mStep[0m  [18/21], [94mLoss[0m : 5.75450
[1mStep[0m  [20/21], [94mLoss[0m : 5.73107

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 5.841, [92mTest[0m: 5.948, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.58596
[1mStep[0m  [2/21], [94mLoss[0m : 5.51836
[1mStep[0m  [4/21], [94mLoss[0m : 5.61735
[1mStep[0m  [6/21], [94mLoss[0m : 5.45525
[1mStep[0m  [8/21], [94mLoss[0m : 5.33269
[1mStep[0m  [10/21], [94mLoss[0m : 5.74366
[1mStep[0m  [12/21], [94mLoss[0m : 5.52939
[1mStep[0m  [14/21], [94mLoss[0m : 5.44641
[1mStep[0m  [16/21], [94mLoss[0m : 5.42819
[1mStep[0m  [18/21], [94mLoss[0m : 5.61638
[1mStep[0m  [20/21], [94mLoss[0m : 5.38459

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.520, [92mTest[0m: 5.636, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.35044
[1mStep[0m  [2/21], [94mLoss[0m : 5.45491
[1mStep[0m  [4/21], [94mLoss[0m : 5.12730
[1mStep[0m  [6/21], [94mLoss[0m : 5.32449
[1mStep[0m  [8/21], [94mLoss[0m : 5.15327
[1mStep[0m  [10/21], [94mLoss[0m : 5.14390
[1mStep[0m  [12/21], [94mLoss[0m : 5.25745
[1mStep[0m  [14/21], [94mLoss[0m : 5.20463
[1mStep[0m  [16/21], [94mLoss[0m : 5.08247
[1mStep[0m  [18/21], [94mLoss[0m : 5.36478
[1mStep[0m  [20/21], [94mLoss[0m : 5.23866

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.226, [92mTest[0m: 5.333, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.17875
[1mStep[0m  [2/21], [94mLoss[0m : 4.93951
[1mStep[0m  [4/21], [94mLoss[0m : 5.15188
[1mStep[0m  [6/21], [94mLoss[0m : 5.35139
[1mStep[0m  [8/21], [94mLoss[0m : 5.00356
[1mStep[0m  [10/21], [94mLoss[0m : 4.93216
[1mStep[0m  [12/21], [94mLoss[0m : 4.94843
[1mStep[0m  [14/21], [94mLoss[0m : 4.87180
[1mStep[0m  [16/21], [94mLoss[0m : 4.81564
[1mStep[0m  [18/21], [94mLoss[0m : 4.88787
[1mStep[0m  [20/21], [94mLoss[0m : 4.84908

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 4.955, [92mTest[0m: 5.042, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.74854
[1mStep[0m  [2/21], [94mLoss[0m : 4.75891
[1mStep[0m  [4/21], [94mLoss[0m : 4.68963
[1mStep[0m  [6/21], [94mLoss[0m : 4.62712
[1mStep[0m  [8/21], [94mLoss[0m : 4.91996
[1mStep[0m  [10/21], [94mLoss[0m : 4.63559
[1mStep[0m  [12/21], [94mLoss[0m : 4.68485
[1mStep[0m  [14/21], [94mLoss[0m : 4.42989
[1mStep[0m  [16/21], [94mLoss[0m : 4.68141
[1mStep[0m  [18/21], [94mLoss[0m : 4.59015
[1mStep[0m  [20/21], [94mLoss[0m : 4.67503

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.694, [92mTest[0m: 4.787, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.57471
[1mStep[0m  [2/21], [94mLoss[0m : 4.23057
[1mStep[0m  [4/21], [94mLoss[0m : 4.38168
[1mStep[0m  [6/21], [94mLoss[0m : 4.64157
[1mStep[0m  [8/21], [94mLoss[0m : 4.47628
[1mStep[0m  [10/21], [94mLoss[0m : 4.34285
[1mStep[0m  [12/21], [94mLoss[0m : 4.36582
[1mStep[0m  [14/21], [94mLoss[0m : 4.49560
[1mStep[0m  [16/21], [94mLoss[0m : 4.40330
[1mStep[0m  [18/21], [94mLoss[0m : 4.34620
[1mStep[0m  [20/21], [94mLoss[0m : 4.59803

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.470, [92mTest[0m: 4.543, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.44480
[1mStep[0m  [2/21], [94mLoss[0m : 4.37185
[1mStep[0m  [4/21], [94mLoss[0m : 4.40493
[1mStep[0m  [6/21], [94mLoss[0m : 3.97087
[1mStep[0m  [8/21], [94mLoss[0m : 4.25681
[1mStep[0m  [10/21], [94mLoss[0m : 4.25978
[1mStep[0m  [12/21], [94mLoss[0m : 4.34011
[1mStep[0m  [14/21], [94mLoss[0m : 4.32285
[1mStep[0m  [16/21], [94mLoss[0m : 4.07017
[1mStep[0m  [18/21], [94mLoss[0m : 4.16675
[1mStep[0m  [20/21], [94mLoss[0m : 4.07638

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.274, [92mTest[0m: 4.348, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.18748
[1mStep[0m  [2/21], [94mLoss[0m : 4.23998
[1mStep[0m  [4/21], [94mLoss[0m : 4.02451
[1mStep[0m  [6/21], [94mLoss[0m : 4.29303
[1mStep[0m  [8/21], [94mLoss[0m : 4.29173
[1mStep[0m  [10/21], [94mLoss[0m : 4.16483
[1mStep[0m  [12/21], [94mLoss[0m : 3.96139
[1mStep[0m  [14/21], [94mLoss[0m : 4.11418
[1mStep[0m  [16/21], [94mLoss[0m : 3.98267
[1mStep[0m  [18/21], [94mLoss[0m : 4.15589
[1mStep[0m  [20/21], [94mLoss[0m : 4.11361

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.107, [92mTest[0m: 4.170, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.98323
[1mStep[0m  [2/21], [94mLoss[0m : 3.86154
[1mStep[0m  [4/21], [94mLoss[0m : 3.85301
[1mStep[0m  [6/21], [94mLoss[0m : 4.14994
[1mStep[0m  [8/21], [94mLoss[0m : 3.88728
[1mStep[0m  [10/21], [94mLoss[0m : 3.83545
[1mStep[0m  [12/21], [94mLoss[0m : 4.07526
[1mStep[0m  [14/21], [94mLoss[0m : 3.85478
[1mStep[0m  [16/21], [94mLoss[0m : 4.13454
[1mStep[0m  [18/21], [94mLoss[0m : 3.90814
[1mStep[0m  [20/21], [94mLoss[0m : 4.06631

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.953, [92mTest[0m: 3.999, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.91813
[1mStep[0m  [2/21], [94mLoss[0m : 4.04247
[1mStep[0m  [4/21], [94mLoss[0m : 3.77472
[1mStep[0m  [6/21], [94mLoss[0m : 3.68216
[1mStep[0m  [8/21], [94mLoss[0m : 3.98972
[1mStep[0m  [10/21], [94mLoss[0m : 3.61550
[1mStep[0m  [12/21], [94mLoss[0m : 4.03748
[1mStep[0m  [14/21], [94mLoss[0m : 3.74764
[1mStep[0m  [16/21], [94mLoss[0m : 3.84148
[1mStep[0m  [18/21], [94mLoss[0m : 3.55735
[1mStep[0m  [20/21], [94mLoss[0m : 3.54926

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.819, [92mTest[0m: 3.860, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.87280
[1mStep[0m  [2/21], [94mLoss[0m : 3.77984
[1mStep[0m  [4/21], [94mLoss[0m : 3.94400
[1mStep[0m  [6/21], [94mLoss[0m : 3.46368
[1mStep[0m  [8/21], [94mLoss[0m : 3.54563
[1mStep[0m  [10/21], [94mLoss[0m : 3.84228
[1mStep[0m  [12/21], [94mLoss[0m : 3.93504
[1mStep[0m  [14/21], [94mLoss[0m : 3.76327
[1mStep[0m  [16/21], [94mLoss[0m : 3.76398
[1mStep[0m  [18/21], [94mLoss[0m : 3.51597
[1mStep[0m  [20/21], [94mLoss[0m : 3.54978

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.703, [92mTest[0m: 3.732, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.57870
[1mStep[0m  [2/21], [94mLoss[0m : 3.68192
[1mStep[0m  [4/21], [94mLoss[0m : 3.52428
[1mStep[0m  [6/21], [94mLoss[0m : 3.61517
[1mStep[0m  [8/21], [94mLoss[0m : 3.78114
[1mStep[0m  [10/21], [94mLoss[0m : 3.65097
[1mStep[0m  [12/21], [94mLoss[0m : 3.68851
[1mStep[0m  [14/21], [94mLoss[0m : 3.83594
[1mStep[0m  [16/21], [94mLoss[0m : 3.64291
[1mStep[0m  [18/21], [94mLoss[0m : 3.46874
[1mStep[0m  [20/21], [94mLoss[0m : 3.40894

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.590, [92mTest[0m: 3.618, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.43458
[1mStep[0m  [2/21], [94mLoss[0m : 3.37932
[1mStep[0m  [4/21], [94mLoss[0m : 3.47227
[1mStep[0m  [6/21], [94mLoss[0m : 3.40745
[1mStep[0m  [8/21], [94mLoss[0m : 3.54756
[1mStep[0m  [10/21], [94mLoss[0m : 3.38199
[1mStep[0m  [12/21], [94mLoss[0m : 3.52697
[1mStep[0m  [14/21], [94mLoss[0m : 3.35169
[1mStep[0m  [16/21], [94mLoss[0m : 3.62993
[1mStep[0m  [18/21], [94mLoss[0m : 3.65893
[1mStep[0m  [20/21], [94mLoss[0m : 3.37453

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.490, [92mTest[0m: 3.516, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.53170
[1mStep[0m  [2/21], [94mLoss[0m : 3.58272
[1mStep[0m  [4/21], [94mLoss[0m : 3.54611
[1mStep[0m  [6/21], [94mLoss[0m : 3.37306
[1mStep[0m  [8/21], [94mLoss[0m : 3.29799
[1mStep[0m  [10/21], [94mLoss[0m : 3.51376
[1mStep[0m  [12/21], [94mLoss[0m : 3.67762
[1mStep[0m  [14/21], [94mLoss[0m : 3.17201
[1mStep[0m  [16/21], [94mLoss[0m : 3.37922
[1mStep[0m  [18/21], [94mLoss[0m : 3.11876
[1mStep[0m  [20/21], [94mLoss[0m : 3.39833

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.412, [92mTest[0m: 3.423, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55815
[1mStep[0m  [2/21], [94mLoss[0m : 3.36837
[1mStep[0m  [4/21], [94mLoss[0m : 3.50265
[1mStep[0m  [6/21], [94mLoss[0m : 3.30341
[1mStep[0m  [8/21], [94mLoss[0m : 3.25004
[1mStep[0m  [10/21], [94mLoss[0m : 3.46672
[1mStep[0m  [12/21], [94mLoss[0m : 3.28612
[1mStep[0m  [14/21], [94mLoss[0m : 3.24413
[1mStep[0m  [16/21], [94mLoss[0m : 3.57366
[1mStep[0m  [18/21], [94mLoss[0m : 3.23560
[1mStep[0m  [20/21], [94mLoss[0m : 3.32897

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.338, [92mTest[0m: 3.344, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.279
====================================

Phase 1 - Evaluation MAE:  3.278801849910191
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 3.23186
[1mStep[0m  [2/21], [94mLoss[0m : 3.33138
[1mStep[0m  [4/21], [94mLoss[0m : 3.29585
[1mStep[0m  [6/21], [94mLoss[0m : 3.27364
[1mStep[0m  [8/21], [94mLoss[0m : 3.34076
[1mStep[0m  [10/21], [94mLoss[0m : 3.13827
[1mStep[0m  [12/21], [94mLoss[0m : 3.38606
[1mStep[0m  [14/21], [94mLoss[0m : 3.48397
[1mStep[0m  [16/21], [94mLoss[0m : 3.17341
[1mStep[0m  [18/21], [94mLoss[0m : 3.28426
[1mStep[0m  [20/21], [94mLoss[0m : 3.28612

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.281, [92mTest[0m: 3.271, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.21896
[1mStep[0m  [2/21], [94mLoss[0m : 3.09432
[1mStep[0m  [4/21], [94mLoss[0m : 3.13132
[1mStep[0m  [6/21], [94mLoss[0m : 3.22547
[1mStep[0m  [8/21], [94mLoss[0m : 3.57721
[1mStep[0m  [10/21], [94mLoss[0m : 3.11358
[1mStep[0m  [12/21], [94mLoss[0m : 3.09009
[1mStep[0m  [14/21], [94mLoss[0m : 3.11916
[1mStep[0m  [16/21], [94mLoss[0m : 3.27104
[1mStep[0m  [18/21], [94mLoss[0m : 3.28529
[1mStep[0m  [20/21], [94mLoss[0m : 2.99144

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.211, [92mTest[0m: 3.217, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.17303
[1mStep[0m  [2/21], [94mLoss[0m : 3.25610
[1mStep[0m  [4/21], [94mLoss[0m : 3.13879
[1mStep[0m  [6/21], [94mLoss[0m : 3.00581
[1mStep[0m  [8/21], [94mLoss[0m : 3.16855
[1mStep[0m  [10/21], [94mLoss[0m : 3.22049
[1mStep[0m  [12/21], [94mLoss[0m : 3.21781
[1mStep[0m  [14/21], [94mLoss[0m : 3.07937
[1mStep[0m  [16/21], [94mLoss[0m : 3.08746
[1mStep[0m  [18/21], [94mLoss[0m : 3.20174
[1mStep[0m  [20/21], [94mLoss[0m : 3.21235

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.142, [92mTest[0m: 3.151, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.04854
[1mStep[0m  [2/21], [94mLoss[0m : 3.27614
[1mStep[0m  [4/21], [94mLoss[0m : 3.14030
[1mStep[0m  [6/21], [94mLoss[0m : 2.98728
[1mStep[0m  [8/21], [94mLoss[0m : 3.14339
[1mStep[0m  [10/21], [94mLoss[0m : 2.97319
[1mStep[0m  [12/21], [94mLoss[0m : 3.01542
[1mStep[0m  [14/21], [94mLoss[0m : 3.08115
[1mStep[0m  [16/21], [94mLoss[0m : 3.03094
[1mStep[0m  [18/21], [94mLoss[0m : 3.05593
[1mStep[0m  [20/21], [94mLoss[0m : 2.95302

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.101, [92mTest[0m: 3.087, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.14710
[1mStep[0m  [2/21], [94mLoss[0m : 2.88639
[1mStep[0m  [4/21], [94mLoss[0m : 3.01497
[1mStep[0m  [6/21], [94mLoss[0m : 3.21190
[1mStep[0m  [8/21], [94mLoss[0m : 3.09183
[1mStep[0m  [10/21], [94mLoss[0m : 3.04256
[1mStep[0m  [12/21], [94mLoss[0m : 3.16821
[1mStep[0m  [14/21], [94mLoss[0m : 2.85583
[1mStep[0m  [16/21], [94mLoss[0m : 2.98112
[1mStep[0m  [18/21], [94mLoss[0m : 2.95847
[1mStep[0m  [20/21], [94mLoss[0m : 3.03814

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.052, [92mTest[0m: 3.034, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.92317
[1mStep[0m  [2/21], [94mLoss[0m : 3.01598
[1mStep[0m  [4/21], [94mLoss[0m : 2.91579
[1mStep[0m  [6/21], [94mLoss[0m : 3.18826
[1mStep[0m  [8/21], [94mLoss[0m : 3.01111
[1mStep[0m  [10/21], [94mLoss[0m : 2.99638
[1mStep[0m  [12/21], [94mLoss[0m : 2.89342
[1mStep[0m  [14/21], [94mLoss[0m : 3.14922
[1mStep[0m  [16/21], [94mLoss[0m : 2.94680
[1mStep[0m  [18/21], [94mLoss[0m : 2.96798
[1mStep[0m  [20/21], [94mLoss[0m : 2.99870

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 3.010, [92mTest[0m: 3.000, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.84063
[1mStep[0m  [2/21], [94mLoss[0m : 3.04207
[1mStep[0m  [4/21], [94mLoss[0m : 2.78435
[1mStep[0m  [6/21], [94mLoss[0m : 2.95318
[1mStep[0m  [8/21], [94mLoss[0m : 3.08832
[1mStep[0m  [10/21], [94mLoss[0m : 3.12941
[1mStep[0m  [12/21], [94mLoss[0m : 3.01675
[1mStep[0m  [14/21], [94mLoss[0m : 2.93784
[1mStep[0m  [16/21], [94mLoss[0m : 2.86276
[1mStep[0m  [18/21], [94mLoss[0m : 3.00681
[1mStep[0m  [20/21], [94mLoss[0m : 3.13696

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.971, [92mTest[0m: 2.960, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.21180
[1mStep[0m  [2/21], [94mLoss[0m : 2.98331
[1mStep[0m  [4/21], [94mLoss[0m : 2.86654
[1mStep[0m  [6/21], [94mLoss[0m : 2.93316
[1mStep[0m  [8/21], [94mLoss[0m : 2.85266
[1mStep[0m  [10/21], [94mLoss[0m : 2.79733
[1mStep[0m  [12/21], [94mLoss[0m : 2.96674
[1mStep[0m  [14/21], [94mLoss[0m : 3.08619
[1mStep[0m  [16/21], [94mLoss[0m : 2.83643
[1mStep[0m  [18/21], [94mLoss[0m : 2.79625
[1mStep[0m  [20/21], [94mLoss[0m : 2.86707

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.934, [92mTest[0m: 2.923, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.04367
[1mStep[0m  [2/21], [94mLoss[0m : 2.94006
[1mStep[0m  [4/21], [94mLoss[0m : 2.83936
[1mStep[0m  [6/21], [94mLoss[0m : 2.78873
[1mStep[0m  [8/21], [94mLoss[0m : 2.84455
[1mStep[0m  [10/21], [94mLoss[0m : 2.92048
[1mStep[0m  [12/21], [94mLoss[0m : 2.88336
[1mStep[0m  [14/21], [94mLoss[0m : 3.03082
[1mStep[0m  [16/21], [94mLoss[0m : 2.85348
[1mStep[0m  [18/21], [94mLoss[0m : 2.91179
[1mStep[0m  [20/21], [94mLoss[0m : 3.00573

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.903, [92mTest[0m: 2.886, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83823
[1mStep[0m  [2/21], [94mLoss[0m : 2.98772
[1mStep[0m  [4/21], [94mLoss[0m : 2.85797
[1mStep[0m  [6/21], [94mLoss[0m : 3.01415
[1mStep[0m  [8/21], [94mLoss[0m : 2.99336
[1mStep[0m  [10/21], [94mLoss[0m : 2.88848
[1mStep[0m  [12/21], [94mLoss[0m : 3.06546
[1mStep[0m  [14/21], [94mLoss[0m : 3.07589
[1mStep[0m  [16/21], [94mLoss[0m : 2.88344
[1mStep[0m  [18/21], [94mLoss[0m : 2.90719
[1mStep[0m  [20/21], [94mLoss[0m : 2.82610

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.877, [92mTest[0m: 2.866, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76207
[1mStep[0m  [2/21], [94mLoss[0m : 2.55663
[1mStep[0m  [4/21], [94mLoss[0m : 2.90049
[1mStep[0m  [6/21], [94mLoss[0m : 2.89368
[1mStep[0m  [8/21], [94mLoss[0m : 2.76250
[1mStep[0m  [10/21], [94mLoss[0m : 2.78232
[1mStep[0m  [12/21], [94mLoss[0m : 2.81901
[1mStep[0m  [14/21], [94mLoss[0m : 2.98106
[1mStep[0m  [16/21], [94mLoss[0m : 2.84503
[1mStep[0m  [18/21], [94mLoss[0m : 2.90601
[1mStep[0m  [20/21], [94mLoss[0m : 2.89575

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.846, [92mTest[0m: 2.838, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.95390
[1mStep[0m  [2/21], [94mLoss[0m : 2.84010
[1mStep[0m  [4/21], [94mLoss[0m : 2.90709
[1mStep[0m  [6/21], [94mLoss[0m : 2.91287
[1mStep[0m  [8/21], [94mLoss[0m : 3.04816
[1mStep[0m  [10/21], [94mLoss[0m : 2.83899
[1mStep[0m  [12/21], [94mLoss[0m : 2.71724
[1mStep[0m  [14/21], [94mLoss[0m : 2.66189
[1mStep[0m  [16/21], [94mLoss[0m : 2.64392
[1mStep[0m  [18/21], [94mLoss[0m : 2.83385
[1mStep[0m  [20/21], [94mLoss[0m : 2.79403

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.831, [92mTest[0m: 2.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79112
[1mStep[0m  [2/21], [94mLoss[0m : 2.88099
[1mStep[0m  [4/21], [94mLoss[0m : 2.77331
[1mStep[0m  [6/21], [94mLoss[0m : 2.77296
[1mStep[0m  [8/21], [94mLoss[0m : 2.57398
[1mStep[0m  [10/21], [94mLoss[0m : 2.92541
[1mStep[0m  [12/21], [94mLoss[0m : 2.73022
[1mStep[0m  [14/21], [94mLoss[0m : 2.78710
[1mStep[0m  [16/21], [94mLoss[0m : 2.83548
[1mStep[0m  [18/21], [94mLoss[0m : 2.97601
[1mStep[0m  [20/21], [94mLoss[0m : 2.78166

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.807, [92mTest[0m: 2.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77397
[1mStep[0m  [2/21], [94mLoss[0m : 2.74523
[1mStep[0m  [4/21], [94mLoss[0m : 2.78247
[1mStep[0m  [6/21], [94mLoss[0m : 2.72399
[1mStep[0m  [8/21], [94mLoss[0m : 2.92217
[1mStep[0m  [10/21], [94mLoss[0m : 2.88831
[1mStep[0m  [12/21], [94mLoss[0m : 2.79627
[1mStep[0m  [14/21], [94mLoss[0m : 2.81684
[1mStep[0m  [16/21], [94mLoss[0m : 2.67855
[1mStep[0m  [18/21], [94mLoss[0m : 2.72177
[1mStep[0m  [20/21], [94mLoss[0m : 2.81454

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.792, [92mTest[0m: 2.757, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.77159
[1mStep[0m  [2/21], [94mLoss[0m : 2.79071
[1mStep[0m  [4/21], [94mLoss[0m : 2.58620
[1mStep[0m  [6/21], [94mLoss[0m : 2.72254
[1mStep[0m  [8/21], [94mLoss[0m : 2.59180
[1mStep[0m  [10/21], [94mLoss[0m : 2.74392
[1mStep[0m  [12/21], [94mLoss[0m : 2.88095
[1mStep[0m  [14/21], [94mLoss[0m : 2.93683
[1mStep[0m  [16/21], [94mLoss[0m : 2.82636
[1mStep[0m  [18/21], [94mLoss[0m : 2.90211
[1mStep[0m  [20/21], [94mLoss[0m : 2.74743

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.769, [92mTest[0m: 2.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.91944
[1mStep[0m  [2/21], [94mLoss[0m : 2.84171
[1mStep[0m  [4/21], [94mLoss[0m : 2.75563
[1mStep[0m  [6/21], [94mLoss[0m : 2.80697
[1mStep[0m  [8/21], [94mLoss[0m : 2.75124
[1mStep[0m  [10/21], [94mLoss[0m : 2.76013
[1mStep[0m  [12/21], [94mLoss[0m : 2.71093
[1mStep[0m  [14/21], [94mLoss[0m : 2.87718
[1mStep[0m  [16/21], [94mLoss[0m : 2.67084
[1mStep[0m  [18/21], [94mLoss[0m : 2.74566
[1mStep[0m  [20/21], [94mLoss[0m : 2.53655

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.745, [92mTest[0m: 2.730, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71214
[1mStep[0m  [2/21], [94mLoss[0m : 2.73839
[1mStep[0m  [4/21], [94mLoss[0m : 2.79380
[1mStep[0m  [6/21], [94mLoss[0m : 2.74696
[1mStep[0m  [8/21], [94mLoss[0m : 2.71877
[1mStep[0m  [10/21], [94mLoss[0m : 2.68670
[1mStep[0m  [12/21], [94mLoss[0m : 2.83155
[1mStep[0m  [14/21], [94mLoss[0m : 2.74597
[1mStep[0m  [16/21], [94mLoss[0m : 2.68661
[1mStep[0m  [18/21], [94mLoss[0m : 2.94725
[1mStep[0m  [20/21], [94mLoss[0m : 2.88565

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.743, [92mTest[0m: 2.709, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72557
[1mStep[0m  [2/21], [94mLoss[0m : 2.67884
[1mStep[0m  [4/21], [94mLoss[0m : 2.66358
[1mStep[0m  [6/21], [94mLoss[0m : 2.52950
[1mStep[0m  [8/21], [94mLoss[0m : 2.78904
[1mStep[0m  [10/21], [94mLoss[0m : 2.75195
[1mStep[0m  [12/21], [94mLoss[0m : 2.76494
[1mStep[0m  [14/21], [94mLoss[0m : 2.78865
[1mStep[0m  [16/21], [94mLoss[0m : 2.48082
[1mStep[0m  [18/21], [94mLoss[0m : 2.69577
[1mStep[0m  [20/21], [94mLoss[0m : 2.83397

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.717, [92mTest[0m: 2.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67647
[1mStep[0m  [2/21], [94mLoss[0m : 2.56978
[1mStep[0m  [4/21], [94mLoss[0m : 2.71015
[1mStep[0m  [6/21], [94mLoss[0m : 2.75066
[1mStep[0m  [8/21], [94mLoss[0m : 2.65144
[1mStep[0m  [10/21], [94mLoss[0m : 2.61627
[1mStep[0m  [12/21], [94mLoss[0m : 2.84699
[1mStep[0m  [14/21], [94mLoss[0m : 2.85782
[1mStep[0m  [16/21], [94mLoss[0m : 2.71465
[1mStep[0m  [18/21], [94mLoss[0m : 2.78831
[1mStep[0m  [20/21], [94mLoss[0m : 2.70992

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.672, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68176
[1mStep[0m  [2/21], [94mLoss[0m : 2.72657
[1mStep[0m  [4/21], [94mLoss[0m : 2.71364
[1mStep[0m  [6/21], [94mLoss[0m : 2.57568
[1mStep[0m  [8/21], [94mLoss[0m : 2.60249
[1mStep[0m  [10/21], [94mLoss[0m : 2.64248
[1mStep[0m  [12/21], [94mLoss[0m : 2.72340
[1mStep[0m  [14/21], [94mLoss[0m : 2.74380
[1mStep[0m  [16/21], [94mLoss[0m : 2.72463
[1mStep[0m  [18/21], [94mLoss[0m : 2.82529
[1mStep[0m  [20/21], [94mLoss[0m : 2.73169

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.698, [92mTest[0m: 2.660, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67105
[1mStep[0m  [2/21], [94mLoss[0m : 2.60257
[1mStep[0m  [4/21], [94mLoss[0m : 2.88196
[1mStep[0m  [6/21], [94mLoss[0m : 2.67259
[1mStep[0m  [8/21], [94mLoss[0m : 2.57437
[1mStep[0m  [10/21], [94mLoss[0m : 2.80911
[1mStep[0m  [12/21], [94mLoss[0m : 2.74349
[1mStep[0m  [14/21], [94mLoss[0m : 2.72242
[1mStep[0m  [16/21], [94mLoss[0m : 2.58168
[1mStep[0m  [18/21], [94mLoss[0m : 2.53822
[1mStep[0m  [20/21], [94mLoss[0m : 2.69293

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.652, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73736
[1mStep[0m  [2/21], [94mLoss[0m : 2.73511
[1mStep[0m  [4/21], [94mLoss[0m : 2.70243
[1mStep[0m  [6/21], [94mLoss[0m : 2.36414
[1mStep[0m  [8/21], [94mLoss[0m : 2.66914
[1mStep[0m  [10/21], [94mLoss[0m : 2.74049
[1mStep[0m  [12/21], [94mLoss[0m : 2.60514
[1mStep[0m  [14/21], [94mLoss[0m : 2.65787
[1mStep[0m  [16/21], [94mLoss[0m : 2.69126
[1mStep[0m  [18/21], [94mLoss[0m : 2.65286
[1mStep[0m  [20/21], [94mLoss[0m : 2.71533

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.643, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.73617
[1mStep[0m  [2/21], [94mLoss[0m : 2.75016
[1mStep[0m  [4/21], [94mLoss[0m : 2.79244
[1mStep[0m  [6/21], [94mLoss[0m : 2.77103
[1mStep[0m  [8/21], [94mLoss[0m : 2.69815
[1mStep[0m  [10/21], [94mLoss[0m : 2.70274
[1mStep[0m  [12/21], [94mLoss[0m : 2.68208
[1mStep[0m  [14/21], [94mLoss[0m : 2.58856
[1mStep[0m  [16/21], [94mLoss[0m : 2.84059
[1mStep[0m  [18/21], [94mLoss[0m : 2.52112
[1mStep[0m  [20/21], [94mLoss[0m : 2.73951

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.633, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83120
[1mStep[0m  [2/21], [94mLoss[0m : 2.70407
[1mStep[0m  [4/21], [94mLoss[0m : 2.52032
[1mStep[0m  [6/21], [94mLoss[0m : 2.60692
[1mStep[0m  [8/21], [94mLoss[0m : 2.66743
[1mStep[0m  [10/21], [94mLoss[0m : 2.38791
[1mStep[0m  [12/21], [94mLoss[0m : 2.65048
[1mStep[0m  [14/21], [94mLoss[0m : 2.75289
[1mStep[0m  [16/21], [94mLoss[0m : 2.75824
[1mStep[0m  [18/21], [94mLoss[0m : 2.82010
[1mStep[0m  [20/21], [94mLoss[0m : 2.62425

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.663, [92mTest[0m: 2.625, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.44648
[1mStep[0m  [2/21], [94mLoss[0m : 2.71999
[1mStep[0m  [4/21], [94mLoss[0m : 2.52592
[1mStep[0m  [6/21], [94mLoss[0m : 2.68414
[1mStep[0m  [8/21], [94mLoss[0m : 2.79637
[1mStep[0m  [10/21], [94mLoss[0m : 2.71400
[1mStep[0m  [12/21], [94mLoss[0m : 2.72465
[1mStep[0m  [14/21], [94mLoss[0m : 2.54703
[1mStep[0m  [16/21], [94mLoss[0m : 2.79399
[1mStep[0m  [18/21], [94mLoss[0m : 2.57893
[1mStep[0m  [20/21], [94mLoss[0m : 2.46459

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.652, [92mTest[0m: 2.615, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50608
[1mStep[0m  [2/21], [94mLoss[0m : 2.69199
[1mStep[0m  [4/21], [94mLoss[0m : 2.71664
[1mStep[0m  [6/21], [94mLoss[0m : 2.85879
[1mStep[0m  [8/21], [94mLoss[0m : 2.58497
[1mStep[0m  [10/21], [94mLoss[0m : 2.71867
[1mStep[0m  [12/21], [94mLoss[0m : 2.50048
[1mStep[0m  [14/21], [94mLoss[0m : 2.70970
[1mStep[0m  [16/21], [94mLoss[0m : 2.66201
[1mStep[0m  [18/21], [94mLoss[0m : 2.56199
[1mStep[0m  [20/21], [94mLoss[0m : 2.68913

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.600, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54355
[1mStep[0m  [2/21], [94mLoss[0m : 2.61630
[1mStep[0m  [4/21], [94mLoss[0m : 2.74746
[1mStep[0m  [6/21], [94mLoss[0m : 2.62977
[1mStep[0m  [8/21], [94mLoss[0m : 2.74201
[1mStep[0m  [10/21], [94mLoss[0m : 2.60343
[1mStep[0m  [12/21], [94mLoss[0m : 2.72020
[1mStep[0m  [14/21], [94mLoss[0m : 2.82304
[1mStep[0m  [16/21], [94mLoss[0m : 2.68691
[1mStep[0m  [18/21], [94mLoss[0m : 2.50556
[1mStep[0m  [20/21], [94mLoss[0m : 2.60459

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.593, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53347
[1mStep[0m  [2/21], [94mLoss[0m : 2.48318
[1mStep[0m  [4/21], [94mLoss[0m : 2.76592
[1mStep[0m  [6/21], [94mLoss[0m : 2.80904
[1mStep[0m  [8/21], [94mLoss[0m : 2.63431
[1mStep[0m  [10/21], [94mLoss[0m : 2.67211
[1mStep[0m  [12/21], [94mLoss[0m : 2.61469
[1mStep[0m  [14/21], [94mLoss[0m : 2.63858
[1mStep[0m  [16/21], [94mLoss[0m : 2.64199
[1mStep[0m  [18/21], [94mLoss[0m : 2.58114
[1mStep[0m  [20/21], [94mLoss[0m : 2.64543

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.596, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.46678
[1mStep[0m  [2/21], [94mLoss[0m : 2.74816
[1mStep[0m  [4/21], [94mLoss[0m : 2.57218
[1mStep[0m  [6/21], [94mLoss[0m : 2.55703
[1mStep[0m  [8/21], [94mLoss[0m : 2.65014
[1mStep[0m  [10/21], [94mLoss[0m : 2.75726
[1mStep[0m  [12/21], [94mLoss[0m : 2.75521
[1mStep[0m  [14/21], [94mLoss[0m : 2.54895
[1mStep[0m  [16/21], [94mLoss[0m : 2.46601
[1mStep[0m  [18/21], [94mLoss[0m : 2.69474
[1mStep[0m  [20/21], [94mLoss[0m : 2.58631

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.589, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65669
[1mStep[0m  [2/21], [94mLoss[0m : 2.65862
[1mStep[0m  [4/21], [94mLoss[0m : 2.57015
[1mStep[0m  [6/21], [94mLoss[0m : 2.72494
[1mStep[0m  [8/21], [94mLoss[0m : 2.46307
[1mStep[0m  [10/21], [94mLoss[0m : 2.56457
[1mStep[0m  [12/21], [94mLoss[0m : 2.64127
[1mStep[0m  [14/21], [94mLoss[0m : 2.69208
[1mStep[0m  [16/21], [94mLoss[0m : 2.58648
[1mStep[0m  [18/21], [94mLoss[0m : 2.58489
[1mStep[0m  [20/21], [94mLoss[0m : 2.66491

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.573, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.572
====================================

Phase 2 - Evaluation MAE:  2.5722883428846086
MAE score P1      3.278802
MAE score P2      2.572288
loss              2.619554
learning_rate       0.0001
batch_size             512
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 18, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 10.63079
[1mStep[0m  [2/21], [94mLoss[0m : 10.71453
[1mStep[0m  [4/21], [94mLoss[0m : 10.65426
[1mStep[0m  [6/21], [94mLoss[0m : 11.00525
[1mStep[0m  [8/21], [94mLoss[0m : 10.79379
[1mStep[0m  [10/21], [94mLoss[0m : 10.63057
[1mStep[0m  [12/21], [94mLoss[0m : 10.61143
[1mStep[0m  [14/21], [94mLoss[0m : 10.31621
[1mStep[0m  [16/21], [94mLoss[0m : 10.70733
[1mStep[0m  [18/21], [94mLoss[0m : 10.64694
[1mStep[0m  [20/21], [94mLoss[0m : 10.69793

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.697, [92mTest[0m: 10.850, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.51721
[1mStep[0m  [2/21], [94mLoss[0m : 10.39404
[1mStep[0m  [4/21], [94mLoss[0m : 10.61108
[1mStep[0m  [6/21], [94mLoss[0m : 10.44917
[1mStep[0m  [8/21], [94mLoss[0m : 10.74985
[1mStep[0m  [10/21], [94mLoss[0m : 10.43861
[1mStep[0m  [12/21], [94mLoss[0m : 10.55745
[1mStep[0m  [14/21], [94mLoss[0m : 10.47060
[1mStep[0m  [16/21], [94mLoss[0m : 10.53369
[1mStep[0m  [18/21], [94mLoss[0m : 10.53386
[1mStep[0m  [20/21], [94mLoss[0m : 10.44921

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.512, [92mTest[0m: 10.765, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.31455
[1mStep[0m  [2/21], [94mLoss[0m : 10.47296
[1mStep[0m  [4/21], [94mLoss[0m : 10.25585
[1mStep[0m  [6/21], [94mLoss[0m : 10.37917
[1mStep[0m  [8/21], [94mLoss[0m : 10.43811
[1mStep[0m  [10/21], [94mLoss[0m : 10.42896
[1mStep[0m  [12/21], [94mLoss[0m : 10.23127
[1mStep[0m  [14/21], [94mLoss[0m : 10.03674
[1mStep[0m  [16/21], [94mLoss[0m : 10.35406
[1mStep[0m  [18/21], [94mLoss[0m : 10.18637
[1mStep[0m  [20/21], [94mLoss[0m : 10.12708

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.321, [92mTest[0m: 10.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.33153
[1mStep[0m  [2/21], [94mLoss[0m : 10.18026
[1mStep[0m  [4/21], [94mLoss[0m : 9.92199
[1mStep[0m  [6/21], [94mLoss[0m : 9.94798
[1mStep[0m  [8/21], [94mLoss[0m : 10.09013
[1mStep[0m  [10/21], [94mLoss[0m : 10.27132
[1mStep[0m  [12/21], [94mLoss[0m : 10.11473
[1mStep[0m  [14/21], [94mLoss[0m : 9.98861
[1mStep[0m  [16/21], [94mLoss[0m : 9.90070
[1mStep[0m  [18/21], [94mLoss[0m : 9.74569
[1mStep[0m  [20/21], [94mLoss[0m : 10.30937

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.130, [92mTest[0m: 10.375, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.94887
[1mStep[0m  [2/21], [94mLoss[0m : 9.93601
[1mStep[0m  [4/21], [94mLoss[0m : 9.85164
[1mStep[0m  [6/21], [94mLoss[0m : 9.73628
[1mStep[0m  [8/21], [94mLoss[0m : 9.94934
[1mStep[0m  [10/21], [94mLoss[0m : 9.90722
[1mStep[0m  [12/21], [94mLoss[0m : 9.87203
[1mStep[0m  [14/21], [94mLoss[0m : 9.67175
[1mStep[0m  [16/21], [94mLoss[0m : 9.94319
[1mStep[0m  [18/21], [94mLoss[0m : 9.82182
[1mStep[0m  [20/21], [94mLoss[0m : 9.84575

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.938, [92mTest[0m: 10.213, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.77111
[1mStep[0m  [2/21], [94mLoss[0m : 10.29307
[1mStep[0m  [4/21], [94mLoss[0m : 9.70523
[1mStep[0m  [6/21], [94mLoss[0m : 9.61955
[1mStep[0m  [8/21], [94mLoss[0m : 9.47621
[1mStep[0m  [10/21], [94mLoss[0m : 9.90155
[1mStep[0m  [12/21], [94mLoss[0m : 9.73762
[1mStep[0m  [14/21], [94mLoss[0m : 9.65267
[1mStep[0m  [16/21], [94mLoss[0m : 9.63929
[1mStep[0m  [18/21], [94mLoss[0m : 9.35325
[1mStep[0m  [20/21], [94mLoss[0m : 9.76286

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.743, [92mTest[0m: 10.057, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.56729
[1mStep[0m  [2/21], [94mLoss[0m : 9.79885
[1mStep[0m  [4/21], [94mLoss[0m : 9.73983
[1mStep[0m  [6/21], [94mLoss[0m : 9.42021
[1mStep[0m  [8/21], [94mLoss[0m : 9.87498
[1mStep[0m  [10/21], [94mLoss[0m : 9.67717
[1mStep[0m  [12/21], [94mLoss[0m : 9.32270
[1mStep[0m  [14/21], [94mLoss[0m : 9.12067
[1mStep[0m  [16/21], [94mLoss[0m : 9.35395
[1mStep[0m  [18/21], [94mLoss[0m : 9.44233
[1mStep[0m  [20/21], [94mLoss[0m : 9.54690

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.548, [92mTest[0m: 9.906, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.33760
[1mStep[0m  [2/21], [94mLoss[0m : 9.42499
[1mStep[0m  [4/21], [94mLoss[0m : 9.32862
[1mStep[0m  [6/21], [94mLoss[0m : 9.21007
[1mStep[0m  [8/21], [94mLoss[0m : 9.27805
[1mStep[0m  [10/21], [94mLoss[0m : 9.62813
[1mStep[0m  [12/21], [94mLoss[0m : 9.22726
[1mStep[0m  [14/21], [94mLoss[0m : 9.27057
[1mStep[0m  [16/21], [94mLoss[0m : 9.48856
[1mStep[0m  [18/21], [94mLoss[0m : 9.48280
[1mStep[0m  [20/21], [94mLoss[0m : 9.20658

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 9.356, [92mTest[0m: 9.732, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.19756
[1mStep[0m  [2/21], [94mLoss[0m : 9.42548
[1mStep[0m  [4/21], [94mLoss[0m : 9.02922
[1mStep[0m  [6/21], [94mLoss[0m : 9.10053
[1mStep[0m  [8/21], [94mLoss[0m : 8.91908
[1mStep[0m  [10/21], [94mLoss[0m : 9.20215
[1mStep[0m  [12/21], [94mLoss[0m : 9.12188
[1mStep[0m  [14/21], [94mLoss[0m : 9.23663
[1mStep[0m  [16/21], [94mLoss[0m : 8.94616
[1mStep[0m  [18/21], [94mLoss[0m : 9.30471
[1mStep[0m  [20/21], [94mLoss[0m : 9.16185

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 9.173, [92mTest[0m: 9.588, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.99867
[1mStep[0m  [2/21], [94mLoss[0m : 9.27126
[1mStep[0m  [4/21], [94mLoss[0m : 9.04729
[1mStep[0m  [6/21], [94mLoss[0m : 8.94285
[1mStep[0m  [8/21], [94mLoss[0m : 8.97663
[1mStep[0m  [10/21], [94mLoss[0m : 8.84497
[1mStep[0m  [12/21], [94mLoss[0m : 8.81021
[1mStep[0m  [14/21], [94mLoss[0m : 8.90350
[1mStep[0m  [16/21], [94mLoss[0m : 8.98690
[1mStep[0m  [18/21], [94mLoss[0m : 8.79583
[1mStep[0m  [20/21], [94mLoss[0m : 8.92010

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.980, [92mTest[0m: 9.426, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.65177
[1mStep[0m  [2/21], [94mLoss[0m : 8.84549
[1mStep[0m  [4/21], [94mLoss[0m : 8.78653
[1mStep[0m  [6/21], [94mLoss[0m : 8.42715
[1mStep[0m  [8/21], [94mLoss[0m : 8.81570
[1mStep[0m  [10/21], [94mLoss[0m : 8.62021
[1mStep[0m  [12/21], [94mLoss[0m : 9.00188
[1mStep[0m  [14/21], [94mLoss[0m : 8.93381
[1mStep[0m  [16/21], [94mLoss[0m : 8.89746
[1mStep[0m  [18/21], [94mLoss[0m : 8.62108
[1mStep[0m  [20/21], [94mLoss[0m : 8.53892

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.786, [92mTest[0m: 9.275, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.84319
[1mStep[0m  [2/21], [94mLoss[0m : 8.97310
[1mStep[0m  [4/21], [94mLoss[0m : 8.90285
[1mStep[0m  [6/21], [94mLoss[0m : 8.50351
[1mStep[0m  [8/21], [94mLoss[0m : 8.43605
[1mStep[0m  [10/21], [94mLoss[0m : 8.76930
[1mStep[0m  [12/21], [94mLoss[0m : 8.57859
[1mStep[0m  [14/21], [94mLoss[0m : 8.39712
[1mStep[0m  [16/21], [94mLoss[0m : 8.52531
[1mStep[0m  [18/21], [94mLoss[0m : 8.53149
[1mStep[0m  [20/21], [94mLoss[0m : 8.74928

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.607, [92mTest[0m: 9.119, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.47547
[1mStep[0m  [2/21], [94mLoss[0m : 8.56267
[1mStep[0m  [4/21], [94mLoss[0m : 8.55289
[1mStep[0m  [6/21], [94mLoss[0m : 8.44635
[1mStep[0m  [8/21], [94mLoss[0m : 8.47260
[1mStep[0m  [10/21], [94mLoss[0m : 8.40269
[1mStep[0m  [12/21], [94mLoss[0m : 8.31589
[1mStep[0m  [14/21], [94mLoss[0m : 8.44604
[1mStep[0m  [16/21], [94mLoss[0m : 8.17431
[1mStep[0m  [18/21], [94mLoss[0m : 8.26436
[1mStep[0m  [20/21], [94mLoss[0m : 8.45104

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.413, [92mTest[0m: 8.959, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.22381
[1mStep[0m  [2/21], [94mLoss[0m : 8.54780
[1mStep[0m  [4/21], [94mLoss[0m : 8.15971
[1mStep[0m  [6/21], [94mLoss[0m : 8.31261
[1mStep[0m  [8/21], [94mLoss[0m : 8.22355
[1mStep[0m  [10/21], [94mLoss[0m : 8.33523
[1mStep[0m  [12/21], [94mLoss[0m : 8.18941
[1mStep[0m  [14/21], [94mLoss[0m : 8.00082
[1mStep[0m  [16/21], [94mLoss[0m : 8.18547
[1mStep[0m  [18/21], [94mLoss[0m : 8.16490
[1mStep[0m  [20/21], [94mLoss[0m : 8.05125

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.232, [92mTest[0m: 8.810, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.33928
[1mStep[0m  [2/21], [94mLoss[0m : 8.00844
[1mStep[0m  [4/21], [94mLoss[0m : 8.00489
[1mStep[0m  [6/21], [94mLoss[0m : 8.29693
[1mStep[0m  [8/21], [94mLoss[0m : 8.24401
[1mStep[0m  [10/21], [94mLoss[0m : 7.87468
[1mStep[0m  [12/21], [94mLoss[0m : 8.01534
[1mStep[0m  [14/21], [94mLoss[0m : 7.75505
[1mStep[0m  [16/21], [94mLoss[0m : 8.08525
[1mStep[0m  [18/21], [94mLoss[0m : 7.91057
[1mStep[0m  [20/21], [94mLoss[0m : 7.71288

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.042, [92mTest[0m: 8.653, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.88975
[1mStep[0m  [2/21], [94mLoss[0m : 8.03273
[1mStep[0m  [4/21], [94mLoss[0m : 7.72185
[1mStep[0m  [6/21], [94mLoss[0m : 7.89825
[1mStep[0m  [8/21], [94mLoss[0m : 7.78146
[1mStep[0m  [10/21], [94mLoss[0m : 8.06932
[1mStep[0m  [12/21], [94mLoss[0m : 8.00243
[1mStep[0m  [14/21], [94mLoss[0m : 7.82587
[1mStep[0m  [16/21], [94mLoss[0m : 7.74893
[1mStep[0m  [18/21], [94mLoss[0m : 8.09643
[1mStep[0m  [20/21], [94mLoss[0m : 7.51177

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.853, [92mTest[0m: 8.510, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.89934
[1mStep[0m  [2/21], [94mLoss[0m : 7.63302
[1mStep[0m  [4/21], [94mLoss[0m : 7.83842
[1mStep[0m  [6/21], [94mLoss[0m : 7.59526
[1mStep[0m  [8/21], [94mLoss[0m : 7.73814
[1mStep[0m  [10/21], [94mLoss[0m : 7.94500
[1mStep[0m  [12/21], [94mLoss[0m : 7.69916
[1mStep[0m  [14/21], [94mLoss[0m : 8.04748
[1mStep[0m  [16/21], [94mLoss[0m : 7.69932
[1mStep[0m  [18/21], [94mLoss[0m : 7.35478
[1mStep[0m  [20/21], [94mLoss[0m : 7.46769

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.675, [92mTest[0m: 8.342, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.41510
[1mStep[0m  [2/21], [94mLoss[0m : 7.52269
[1mStep[0m  [4/21], [94mLoss[0m : 7.64166
[1mStep[0m  [6/21], [94mLoss[0m : 7.65462
[1mStep[0m  [8/21], [94mLoss[0m : 7.35324
[1mStep[0m  [10/21], [94mLoss[0m : 7.51073
[1mStep[0m  [12/21], [94mLoss[0m : 7.43159
[1mStep[0m  [14/21], [94mLoss[0m : 7.73273
[1mStep[0m  [16/21], [94mLoss[0m : 7.78215
[1mStep[0m  [18/21], [94mLoss[0m : 7.59492
[1mStep[0m  [20/21], [94mLoss[0m : 7.39926

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.506, [92mTest[0m: 8.177, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.21028
[1mStep[0m  [2/21], [94mLoss[0m : 7.78720
[1mStep[0m  [4/21], [94mLoss[0m : 7.47585
[1mStep[0m  [6/21], [94mLoss[0m : 7.42504
[1mStep[0m  [8/21], [94mLoss[0m : 7.27548
[1mStep[0m  [10/21], [94mLoss[0m : 7.29333
[1mStep[0m  [12/21], [94mLoss[0m : 7.34809
[1mStep[0m  [14/21], [94mLoss[0m : 7.09662
[1mStep[0m  [16/21], [94mLoss[0m : 7.41693
[1mStep[0m  [18/21], [94mLoss[0m : 7.15039
[1mStep[0m  [20/21], [94mLoss[0m : 7.12675

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.326, [92mTest[0m: 8.035, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.16778
[1mStep[0m  [2/21], [94mLoss[0m : 7.37488
[1mStep[0m  [4/21], [94mLoss[0m : 7.16927
[1mStep[0m  [6/21], [94mLoss[0m : 7.00055
[1mStep[0m  [8/21], [94mLoss[0m : 7.22857
[1mStep[0m  [10/21], [94mLoss[0m : 7.14058
[1mStep[0m  [12/21], [94mLoss[0m : 7.04115
[1mStep[0m  [14/21], [94mLoss[0m : 7.18004
[1mStep[0m  [16/21], [94mLoss[0m : 7.08716
[1mStep[0m  [18/21], [94mLoss[0m : 7.35564
[1mStep[0m  [20/21], [94mLoss[0m : 7.26464

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.165, [92mTest[0m: 7.908, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.37813
[1mStep[0m  [2/21], [94mLoss[0m : 7.03888
[1mStep[0m  [4/21], [94mLoss[0m : 6.80434
[1mStep[0m  [6/21], [94mLoss[0m : 7.24586
[1mStep[0m  [8/21], [94mLoss[0m : 7.13414
[1mStep[0m  [10/21], [94mLoss[0m : 6.96317
[1mStep[0m  [12/21], [94mLoss[0m : 7.23041
[1mStep[0m  [14/21], [94mLoss[0m : 6.84934
[1mStep[0m  [16/21], [94mLoss[0m : 6.96514
[1mStep[0m  [18/21], [94mLoss[0m : 7.01543
[1mStep[0m  [20/21], [94mLoss[0m : 6.82798

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.009, [92mTest[0m: 7.746, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.21965
[1mStep[0m  [2/21], [94mLoss[0m : 6.56840
[1mStep[0m  [4/21], [94mLoss[0m : 7.10589
[1mStep[0m  [6/21], [94mLoss[0m : 7.14662
[1mStep[0m  [8/21], [94mLoss[0m : 6.98123
[1mStep[0m  [10/21], [94mLoss[0m : 6.86534
[1mStep[0m  [12/21], [94mLoss[0m : 6.93024
[1mStep[0m  [14/21], [94mLoss[0m : 6.83073
[1mStep[0m  [16/21], [94mLoss[0m : 7.01943
[1mStep[0m  [18/21], [94mLoss[0m : 6.54858
[1mStep[0m  [20/21], [94mLoss[0m : 6.76864

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.855, [92mTest[0m: 7.629, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.78540
[1mStep[0m  [2/21], [94mLoss[0m : 6.73735
[1mStep[0m  [4/21], [94mLoss[0m : 6.76916
[1mStep[0m  [6/21], [94mLoss[0m : 6.89344
[1mStep[0m  [8/21], [94mLoss[0m : 6.58934
[1mStep[0m  [10/21], [94mLoss[0m : 6.72159
[1mStep[0m  [12/21], [94mLoss[0m : 6.71423
[1mStep[0m  [14/21], [94mLoss[0m : 6.64325
[1mStep[0m  [16/21], [94mLoss[0m : 6.59600
[1mStep[0m  [18/21], [94mLoss[0m : 6.61442
[1mStep[0m  [20/21], [94mLoss[0m : 6.87125

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.709, [92mTest[0m: 7.469, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.78206
[1mStep[0m  [2/21], [94mLoss[0m : 6.77824
[1mStep[0m  [4/21], [94mLoss[0m : 6.74780
[1mStep[0m  [6/21], [94mLoss[0m : 6.38676
[1mStep[0m  [8/21], [94mLoss[0m : 6.72462
[1mStep[0m  [10/21], [94mLoss[0m : 6.26007
[1mStep[0m  [12/21], [94mLoss[0m : 6.65127
[1mStep[0m  [14/21], [94mLoss[0m : 6.34514
[1mStep[0m  [16/21], [94mLoss[0m : 6.66327
[1mStep[0m  [18/21], [94mLoss[0m : 6.55885
[1mStep[0m  [20/21], [94mLoss[0m : 6.51566

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.580, [92mTest[0m: 7.374, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.42281
[1mStep[0m  [2/21], [94mLoss[0m : 6.52217
[1mStep[0m  [4/21], [94mLoss[0m : 6.21549
[1mStep[0m  [6/21], [94mLoss[0m : 6.49477
[1mStep[0m  [8/21], [94mLoss[0m : 6.54786
[1mStep[0m  [10/21], [94mLoss[0m : 6.33908
[1mStep[0m  [12/21], [94mLoss[0m : 6.30330
[1mStep[0m  [14/21], [94mLoss[0m : 6.64874
[1mStep[0m  [16/21], [94mLoss[0m : 6.65930
[1mStep[0m  [18/21], [94mLoss[0m : 6.38848
[1mStep[0m  [20/21], [94mLoss[0m : 6.44738

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.448, [92mTest[0m: 7.250, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.51265
[1mStep[0m  [2/21], [94mLoss[0m : 6.50702
[1mStep[0m  [4/21], [94mLoss[0m : 6.24781
[1mStep[0m  [6/21], [94mLoss[0m : 6.34320
[1mStep[0m  [8/21], [94mLoss[0m : 6.56347
[1mStep[0m  [10/21], [94mLoss[0m : 6.31430
[1mStep[0m  [12/21], [94mLoss[0m : 6.23451
[1mStep[0m  [14/21], [94mLoss[0m : 6.00549
[1mStep[0m  [16/21], [94mLoss[0m : 6.29545
[1mStep[0m  [18/21], [94mLoss[0m : 6.43174
[1mStep[0m  [20/21], [94mLoss[0m : 6.21431

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.315, [92mTest[0m: 7.125, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.18011
[1mStep[0m  [2/21], [94mLoss[0m : 6.37629
[1mStep[0m  [4/21], [94mLoss[0m : 6.41255
[1mStep[0m  [6/21], [94mLoss[0m : 6.11597
[1mStep[0m  [8/21], [94mLoss[0m : 6.18072
[1mStep[0m  [10/21], [94mLoss[0m : 5.93799
[1mStep[0m  [12/21], [94mLoss[0m : 6.09373
[1mStep[0m  [14/21], [94mLoss[0m : 6.30698
[1mStep[0m  [16/21], [94mLoss[0m : 6.22386
[1mStep[0m  [18/21], [94mLoss[0m : 6.44016
[1mStep[0m  [20/21], [94mLoss[0m : 6.21750

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.194, [92mTest[0m: 7.001, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.30476
[1mStep[0m  [2/21], [94mLoss[0m : 6.29580
[1mStep[0m  [4/21], [94mLoss[0m : 6.06418
[1mStep[0m  [6/21], [94mLoss[0m : 6.07747
[1mStep[0m  [8/21], [94mLoss[0m : 6.13306
[1mStep[0m  [10/21], [94mLoss[0m : 6.29731
[1mStep[0m  [12/21], [94mLoss[0m : 6.15094
[1mStep[0m  [14/21], [94mLoss[0m : 5.82878
[1mStep[0m  [16/21], [94mLoss[0m : 5.98024
[1mStep[0m  [18/21], [94mLoss[0m : 5.85699
[1mStep[0m  [20/21], [94mLoss[0m : 5.94293

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.064, [92mTest[0m: 6.884, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.00437
[1mStep[0m  [2/21], [94mLoss[0m : 5.97399
[1mStep[0m  [4/21], [94mLoss[0m : 6.02081
[1mStep[0m  [6/21], [94mLoss[0m : 5.85359
[1mStep[0m  [8/21], [94mLoss[0m : 5.82823
[1mStep[0m  [10/21], [94mLoss[0m : 5.88041
[1mStep[0m  [12/21], [94mLoss[0m : 6.09511
[1mStep[0m  [14/21], [94mLoss[0m : 5.81944
[1mStep[0m  [16/21], [94mLoss[0m : 5.96932
[1mStep[0m  [18/21], [94mLoss[0m : 6.01939
[1mStep[0m  [20/21], [94mLoss[0m : 5.97671

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 5.936, [92mTest[0m: 6.781, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.16343
[1mStep[0m  [2/21], [94mLoss[0m : 5.86402
[1mStep[0m  [4/21], [94mLoss[0m : 5.81030
[1mStep[0m  [6/21], [94mLoss[0m : 5.69436
[1mStep[0m  [8/21], [94mLoss[0m : 5.82287
[1mStep[0m  [10/21], [94mLoss[0m : 5.71758
[1mStep[0m  [12/21], [94mLoss[0m : 5.66418
[1mStep[0m  [14/21], [94mLoss[0m : 5.60301
[1mStep[0m  [16/21], [94mLoss[0m : 5.89156
[1mStep[0m  [18/21], [94mLoss[0m : 5.88333
[1mStep[0m  [20/21], [94mLoss[0m : 5.79981

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 5.827, [92mTest[0m: 6.673, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 6.548
====================================

Phase 1 - Evaluation MAE:  6.548373699188232
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/21], [94mLoss[0m : 5.54980
[1mStep[0m  [2/21], [94mLoss[0m : 5.95568
[1mStep[0m  [4/21], [94mLoss[0m : 5.91425
[1mStep[0m  [6/21], [94mLoss[0m : 5.75448
[1mStep[0m  [8/21], [94mLoss[0m : 5.52830
[1mStep[0m  [10/21], [94mLoss[0m : 5.65907
[1mStep[0m  [12/21], [94mLoss[0m : 5.77662
[1mStep[0m  [14/21], [94mLoss[0m : 5.57751
[1mStep[0m  [16/21], [94mLoss[0m : 5.53320
[1mStep[0m  [18/21], [94mLoss[0m : 5.46710
[1mStep[0m  [20/21], [94mLoss[0m : 5.83134

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.698, [92mTest[0m: 6.535, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.59265
[1mStep[0m  [2/21], [94mLoss[0m : 5.74851
[1mStep[0m  [4/21], [94mLoss[0m : 5.66871
[1mStep[0m  [6/21], [94mLoss[0m : 5.65995
[1mStep[0m  [8/21], [94mLoss[0m : 5.26202
[1mStep[0m  [10/21], [94mLoss[0m : 5.47572
[1mStep[0m  [12/21], [94mLoss[0m : 5.41524
[1mStep[0m  [14/21], [94mLoss[0m : 5.35475
[1mStep[0m  [16/21], [94mLoss[0m : 5.71245
[1mStep[0m  [18/21], [94mLoss[0m : 5.51004
[1mStep[0m  [20/21], [94mLoss[0m : 5.63066

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 5.534, [92mTest[0m: 6.403, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.67800
[1mStep[0m  [2/21], [94mLoss[0m : 5.28296
[1mStep[0m  [4/21], [94mLoss[0m : 5.46140
[1mStep[0m  [6/21], [94mLoss[0m : 5.41557
[1mStep[0m  [8/21], [94mLoss[0m : 5.53785
[1mStep[0m  [10/21], [94mLoss[0m : 5.33069
[1mStep[0m  [12/21], [94mLoss[0m : 5.09923
[1mStep[0m  [14/21], [94mLoss[0m : 5.47259
[1mStep[0m  [16/21], [94mLoss[0m : 5.13240
[1mStep[0m  [18/21], [94mLoss[0m : 5.10684
[1mStep[0m  [20/21], [94mLoss[0m : 5.24757

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.384, [92mTest[0m: 6.269, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.27227
[1mStep[0m  [2/21], [94mLoss[0m : 5.19715
[1mStep[0m  [4/21], [94mLoss[0m : 5.55546
[1mStep[0m  [6/21], [94mLoss[0m : 5.27742
[1mStep[0m  [8/21], [94mLoss[0m : 5.30023
[1mStep[0m  [10/21], [94mLoss[0m : 5.21263
[1mStep[0m  [12/21], [94mLoss[0m : 5.27652
[1mStep[0m  [14/21], [94mLoss[0m : 5.31548
[1mStep[0m  [16/21], [94mLoss[0m : 5.59042
[1mStep[0m  [18/21], [94mLoss[0m : 5.32998
[1mStep[0m  [20/21], [94mLoss[0m : 5.28232

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 5.232, [92mTest[0m: 6.123, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.29067
[1mStep[0m  [2/21], [94mLoss[0m : 5.03750
[1mStep[0m  [4/21], [94mLoss[0m : 5.28586
[1mStep[0m  [6/21], [94mLoss[0m : 5.02666
[1mStep[0m  [8/21], [94mLoss[0m : 5.11896
[1mStep[0m  [10/21], [94mLoss[0m : 5.20493
[1mStep[0m  [12/21], [94mLoss[0m : 5.16345
[1mStep[0m  [14/21], [94mLoss[0m : 4.77183
[1mStep[0m  [16/21], [94mLoss[0m : 4.84753
[1mStep[0m  [18/21], [94mLoss[0m : 5.22411
[1mStep[0m  [20/21], [94mLoss[0m : 5.36267

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 5.098, [92mTest[0m: 5.987, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.01601
[1mStep[0m  [2/21], [94mLoss[0m : 5.00293
[1mStep[0m  [4/21], [94mLoss[0m : 4.95523
[1mStep[0m  [6/21], [94mLoss[0m : 5.01304
[1mStep[0m  [8/21], [94mLoss[0m : 4.93041
[1mStep[0m  [10/21], [94mLoss[0m : 4.81557
[1mStep[0m  [12/21], [94mLoss[0m : 4.92939
[1mStep[0m  [14/21], [94mLoss[0m : 4.89341
[1mStep[0m  [16/21], [94mLoss[0m : 4.91323
[1mStep[0m  [18/21], [94mLoss[0m : 4.97405
[1mStep[0m  [20/21], [94mLoss[0m : 4.93012

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.969, [92mTest[0m: 5.841, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.84041
[1mStep[0m  [2/21], [94mLoss[0m : 4.81208
[1mStep[0m  [4/21], [94mLoss[0m : 5.09397
[1mStep[0m  [6/21], [94mLoss[0m : 4.99510
[1mStep[0m  [8/21], [94mLoss[0m : 4.94382
[1mStep[0m  [10/21], [94mLoss[0m : 4.84079
[1mStep[0m  [12/21], [94mLoss[0m : 4.78262
[1mStep[0m  [14/21], [94mLoss[0m : 4.63338
[1mStep[0m  [16/21], [94mLoss[0m : 4.87781
[1mStep[0m  [18/21], [94mLoss[0m : 4.63289
[1mStep[0m  [20/21], [94mLoss[0m : 4.64885

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 4.839, [92mTest[0m: 5.689, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.79026
[1mStep[0m  [2/21], [94mLoss[0m : 4.83472
[1mStep[0m  [4/21], [94mLoss[0m : 4.61051
[1mStep[0m  [6/21], [94mLoss[0m : 4.57946
[1mStep[0m  [8/21], [94mLoss[0m : 4.85722
[1mStep[0m  [10/21], [94mLoss[0m : 4.70792
[1mStep[0m  [12/21], [94mLoss[0m : 4.64182
[1mStep[0m  [14/21], [94mLoss[0m : 4.70232
[1mStep[0m  [16/21], [94mLoss[0m : 4.59276
[1mStep[0m  [18/21], [94mLoss[0m : 4.32256
[1mStep[0m  [20/21], [94mLoss[0m : 4.65898

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.705, [92mTest[0m: 5.561, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.59348
[1mStep[0m  [2/21], [94mLoss[0m : 4.69782
[1mStep[0m  [4/21], [94mLoss[0m : 4.82803
[1mStep[0m  [6/21], [94mLoss[0m : 4.60245
[1mStep[0m  [8/21], [94mLoss[0m : 4.76951
[1mStep[0m  [10/21], [94mLoss[0m : 4.64525
[1mStep[0m  [12/21], [94mLoss[0m : 4.26055
[1mStep[0m  [14/21], [94mLoss[0m : 4.54070
[1mStep[0m  [16/21], [94mLoss[0m : 4.41063
[1mStep[0m  [18/21], [94mLoss[0m : 4.74576
[1mStep[0m  [20/21], [94mLoss[0m : 4.71882

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 4.592, [92mTest[0m: 5.421, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.71967
[1mStep[0m  [2/21], [94mLoss[0m : 4.46955
[1mStep[0m  [4/21], [94mLoss[0m : 4.61960
[1mStep[0m  [6/21], [94mLoss[0m : 4.61285
[1mStep[0m  [8/21], [94mLoss[0m : 4.19764
[1mStep[0m  [10/21], [94mLoss[0m : 4.69937
[1mStep[0m  [12/21], [94mLoss[0m : 4.49553
[1mStep[0m  [14/21], [94mLoss[0m : 4.60803
[1mStep[0m  [16/21], [94mLoss[0m : 4.37250
[1mStep[0m  [18/21], [94mLoss[0m : 4.42141
[1mStep[0m  [20/21], [94mLoss[0m : 4.45683

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 4.482, [92mTest[0m: 5.268, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.36580
[1mStep[0m  [2/21], [94mLoss[0m : 4.39223
[1mStep[0m  [4/21], [94mLoss[0m : 4.43938
[1mStep[0m  [6/21], [94mLoss[0m : 4.25227
[1mStep[0m  [8/21], [94mLoss[0m : 4.34324
[1mStep[0m  [10/21], [94mLoss[0m : 4.52915
[1mStep[0m  [12/21], [94mLoss[0m : 4.19669
[1mStep[0m  [14/21], [94mLoss[0m : 4.30248
[1mStep[0m  [16/21], [94mLoss[0m : 4.38434
[1mStep[0m  [18/21], [94mLoss[0m : 4.27353
[1mStep[0m  [20/21], [94mLoss[0m : 4.33892

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.363, [92mTest[0m: 5.153, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.37405
[1mStep[0m  [2/21], [94mLoss[0m : 4.28442
[1mStep[0m  [4/21], [94mLoss[0m : 4.23508
[1mStep[0m  [6/21], [94mLoss[0m : 4.06958
[1mStep[0m  [8/21], [94mLoss[0m : 4.35526
[1mStep[0m  [10/21], [94mLoss[0m : 3.93406
[1mStep[0m  [12/21], [94mLoss[0m : 4.25167
[1mStep[0m  [14/21], [94mLoss[0m : 4.43819
[1mStep[0m  [16/21], [94mLoss[0m : 4.40786
[1mStep[0m  [18/21], [94mLoss[0m : 4.47265
[1mStep[0m  [20/21], [94mLoss[0m : 4.48748

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.262, [92mTest[0m: 5.016, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.28122
[1mStep[0m  [2/21], [94mLoss[0m : 4.32280
[1mStep[0m  [4/21], [94mLoss[0m : 4.07190
[1mStep[0m  [6/21], [94mLoss[0m : 4.11704
[1mStep[0m  [8/21], [94mLoss[0m : 4.15168
[1mStep[0m  [10/21], [94mLoss[0m : 4.13995
[1mStep[0m  [12/21], [94mLoss[0m : 4.06667
[1mStep[0m  [14/21], [94mLoss[0m : 3.94114
[1mStep[0m  [16/21], [94mLoss[0m : 3.96098
[1mStep[0m  [18/21], [94mLoss[0m : 4.06041
[1mStep[0m  [20/21], [94mLoss[0m : 3.98240

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.138, [92mTest[0m: 4.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.45365
[1mStep[0m  [2/21], [94mLoss[0m : 4.21093
[1mStep[0m  [4/21], [94mLoss[0m : 3.92798
[1mStep[0m  [6/21], [94mLoss[0m : 4.09975
[1mStep[0m  [8/21], [94mLoss[0m : 4.02756
[1mStep[0m  [10/21], [94mLoss[0m : 4.00968
[1mStep[0m  [12/21], [94mLoss[0m : 4.05927
[1mStep[0m  [14/21], [94mLoss[0m : 4.12863
[1mStep[0m  [16/21], [94mLoss[0m : 3.86598
[1mStep[0m  [18/21], [94mLoss[0m : 4.00107
[1mStep[0m  [20/21], [94mLoss[0m : 3.95136

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.042, [92mTest[0m: 4.799, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.88964
[1mStep[0m  [2/21], [94mLoss[0m : 3.99927
[1mStep[0m  [4/21], [94mLoss[0m : 3.94688
[1mStep[0m  [6/21], [94mLoss[0m : 3.81222
[1mStep[0m  [8/21], [94mLoss[0m : 3.79143
[1mStep[0m  [10/21], [94mLoss[0m : 4.12824
[1mStep[0m  [12/21], [94mLoss[0m : 3.90176
[1mStep[0m  [14/21], [94mLoss[0m : 4.03141
[1mStep[0m  [16/21], [94mLoss[0m : 4.15139
[1mStep[0m  [18/21], [94mLoss[0m : 3.95315
[1mStep[0m  [20/21], [94mLoss[0m : 3.79632

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.972, [92mTest[0m: 4.666, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.85649
[1mStep[0m  [2/21], [94mLoss[0m : 3.93654
[1mStep[0m  [4/21], [94mLoss[0m : 3.79969
[1mStep[0m  [6/21], [94mLoss[0m : 3.80023
[1mStep[0m  [8/21], [94mLoss[0m : 3.83234
[1mStep[0m  [10/21], [94mLoss[0m : 3.86082
[1mStep[0m  [12/21], [94mLoss[0m : 3.98382
[1mStep[0m  [14/21], [94mLoss[0m : 3.72147
[1mStep[0m  [16/21], [94mLoss[0m : 3.86633
[1mStep[0m  [18/21], [94mLoss[0m : 4.14097
[1mStep[0m  [20/21], [94mLoss[0m : 3.63363

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.863, [92mTest[0m: 4.570, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.76077
[1mStep[0m  [2/21], [94mLoss[0m : 3.92942
[1mStep[0m  [4/21], [94mLoss[0m : 3.89976
[1mStep[0m  [6/21], [94mLoss[0m : 3.66659
[1mStep[0m  [8/21], [94mLoss[0m : 3.84619
[1mStep[0m  [10/21], [94mLoss[0m : 3.57900
[1mStep[0m  [12/21], [94mLoss[0m : 3.79319
[1mStep[0m  [14/21], [94mLoss[0m : 3.77062
[1mStep[0m  [16/21], [94mLoss[0m : 3.72854
[1mStep[0m  [18/21], [94mLoss[0m : 3.65996
[1mStep[0m  [20/21], [94mLoss[0m : 3.84563

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.779, [92mTest[0m: 4.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.72274
[1mStep[0m  [2/21], [94mLoss[0m : 3.72911
[1mStep[0m  [4/21], [94mLoss[0m : 3.55089
[1mStep[0m  [6/21], [94mLoss[0m : 3.72157
[1mStep[0m  [8/21], [94mLoss[0m : 3.70083
[1mStep[0m  [10/21], [94mLoss[0m : 3.60797
[1mStep[0m  [12/21], [94mLoss[0m : 3.59954
[1mStep[0m  [14/21], [94mLoss[0m : 3.60361
[1mStep[0m  [16/21], [94mLoss[0m : 3.52572
[1mStep[0m  [18/21], [94mLoss[0m : 3.74873
[1mStep[0m  [20/21], [94mLoss[0m : 3.75546

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.698, [92mTest[0m: 4.326, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.80378
[1mStep[0m  [2/21], [94mLoss[0m : 3.54581
[1mStep[0m  [4/21], [94mLoss[0m : 3.66809
[1mStep[0m  [6/21], [94mLoss[0m : 4.09788
[1mStep[0m  [8/21], [94mLoss[0m : 3.56702
[1mStep[0m  [10/21], [94mLoss[0m : 3.48202
[1mStep[0m  [12/21], [94mLoss[0m : 3.85789
[1mStep[0m  [14/21], [94mLoss[0m : 3.54254
[1mStep[0m  [16/21], [94mLoss[0m : 3.58691
[1mStep[0m  [18/21], [94mLoss[0m : 3.36183
[1mStep[0m  [20/21], [94mLoss[0m : 3.71651

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.622, [92mTest[0m: 4.207, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.54126
[1mStep[0m  [2/21], [94mLoss[0m : 3.45722
[1mStep[0m  [4/21], [94mLoss[0m : 3.65184
[1mStep[0m  [6/21], [94mLoss[0m : 3.52524
[1mStep[0m  [8/21], [94mLoss[0m : 3.35394
[1mStep[0m  [10/21], [94mLoss[0m : 3.59990
[1mStep[0m  [12/21], [94mLoss[0m : 3.58752
[1mStep[0m  [14/21], [94mLoss[0m : 3.38156
[1mStep[0m  [16/21], [94mLoss[0m : 3.62153
[1mStep[0m  [18/21], [94mLoss[0m : 3.41069
[1mStep[0m  [20/21], [94mLoss[0m : 3.45178

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.530, [92mTest[0m: 4.101, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.42891
[1mStep[0m  [2/21], [94mLoss[0m : 3.40949
[1mStep[0m  [4/21], [94mLoss[0m : 3.40985
[1mStep[0m  [6/21], [94mLoss[0m : 3.52037
[1mStep[0m  [8/21], [94mLoss[0m : 3.46061
[1mStep[0m  [10/21], [94mLoss[0m : 3.46555
[1mStep[0m  [12/21], [94mLoss[0m : 3.20624
[1mStep[0m  [14/21], [94mLoss[0m : 3.54220
[1mStep[0m  [16/21], [94mLoss[0m : 3.60484
[1mStep[0m  [18/21], [94mLoss[0m : 3.34407
[1mStep[0m  [20/21], [94mLoss[0m : 3.48698

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.475, [92mTest[0m: 4.010, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.64486
[1mStep[0m  [2/21], [94mLoss[0m : 3.47324
[1mStep[0m  [4/21], [94mLoss[0m : 3.45689
[1mStep[0m  [6/21], [94mLoss[0m : 3.26390
[1mStep[0m  [8/21], [94mLoss[0m : 3.42571
[1mStep[0m  [10/21], [94mLoss[0m : 3.51619
[1mStep[0m  [12/21], [94mLoss[0m : 3.65485
[1mStep[0m  [14/21], [94mLoss[0m : 3.32534
[1mStep[0m  [16/21], [94mLoss[0m : 3.36498
[1mStep[0m  [18/21], [94mLoss[0m : 3.51007
[1mStep[0m  [20/21], [94mLoss[0m : 3.45315

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 3.413, [92mTest[0m: 3.914, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.37934
[1mStep[0m  [2/21], [94mLoss[0m : 3.33788
[1mStep[0m  [4/21], [94mLoss[0m : 3.57585
[1mStep[0m  [6/21], [94mLoss[0m : 3.30768
[1mStep[0m  [8/21], [94mLoss[0m : 3.54096
[1mStep[0m  [10/21], [94mLoss[0m : 3.20110
[1mStep[0m  [12/21], [94mLoss[0m : 3.47297
[1mStep[0m  [14/21], [94mLoss[0m : 3.33089
[1mStep[0m  [16/21], [94mLoss[0m : 3.38904
[1mStep[0m  [18/21], [94mLoss[0m : 3.48138
[1mStep[0m  [20/21], [94mLoss[0m : 3.29757

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 3.351, [92mTest[0m: 3.843, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.14226
[1mStep[0m  [2/21], [94mLoss[0m : 3.08827
[1mStep[0m  [4/21], [94mLoss[0m : 3.38676
[1mStep[0m  [6/21], [94mLoss[0m : 3.43132
[1mStep[0m  [8/21], [94mLoss[0m : 3.23234
[1mStep[0m  [10/21], [94mLoss[0m : 3.18232
[1mStep[0m  [12/21], [94mLoss[0m : 3.26125
[1mStep[0m  [14/21], [94mLoss[0m : 3.27303
[1mStep[0m  [16/21], [94mLoss[0m : 3.37502
[1mStep[0m  [18/21], [94mLoss[0m : 3.49681
[1mStep[0m  [20/21], [94mLoss[0m : 3.04304

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 3.292, [92mTest[0m: 3.752, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.00113
[1mStep[0m  [2/21], [94mLoss[0m : 3.26235
[1mStep[0m  [4/21], [94mLoss[0m : 3.12655
[1mStep[0m  [6/21], [94mLoss[0m : 3.29788
[1mStep[0m  [8/21], [94mLoss[0m : 3.11509
[1mStep[0m  [10/21], [94mLoss[0m : 3.37572
[1mStep[0m  [12/21], [94mLoss[0m : 3.10637
[1mStep[0m  [14/21], [94mLoss[0m : 3.18174
[1mStep[0m  [16/21], [94mLoss[0m : 3.33266
[1mStep[0m  [18/21], [94mLoss[0m : 3.17361
[1mStep[0m  [20/21], [94mLoss[0m : 3.49750

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.234, [92mTest[0m: 3.705, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.11385
[1mStep[0m  [2/21], [94mLoss[0m : 3.21510
[1mStep[0m  [4/21], [94mLoss[0m : 3.15695
[1mStep[0m  [6/21], [94mLoss[0m : 3.06927
[1mStep[0m  [8/21], [94mLoss[0m : 3.05249
[1mStep[0m  [10/21], [94mLoss[0m : 3.18442
[1mStep[0m  [12/21], [94mLoss[0m : 3.71062
[1mStep[0m  [14/21], [94mLoss[0m : 3.21826
[1mStep[0m  [16/21], [94mLoss[0m : 3.17669
[1mStep[0m  [18/21], [94mLoss[0m : 3.18265
[1mStep[0m  [20/21], [94mLoss[0m : 3.08336

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.185, [92mTest[0m: 3.624, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.04061
[1mStep[0m  [2/21], [94mLoss[0m : 3.00261
[1mStep[0m  [4/21], [94mLoss[0m : 3.25268
[1mStep[0m  [6/21], [94mLoss[0m : 3.27311
[1mStep[0m  [8/21], [94mLoss[0m : 3.19826
[1mStep[0m  [10/21], [94mLoss[0m : 3.20619
[1mStep[0m  [12/21], [94mLoss[0m : 3.31527
[1mStep[0m  [14/21], [94mLoss[0m : 3.08969
[1mStep[0m  [16/21], [94mLoss[0m : 3.02302
[1mStep[0m  [18/21], [94mLoss[0m : 3.14873
[1mStep[0m  [20/21], [94mLoss[0m : 2.92888

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.141, [92mTest[0m: 3.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.11203
[1mStep[0m  [2/21], [94mLoss[0m : 3.13107
[1mStep[0m  [4/21], [94mLoss[0m : 3.18835
[1mStep[0m  [6/21], [94mLoss[0m : 3.22663
[1mStep[0m  [8/21], [94mLoss[0m : 2.94373
[1mStep[0m  [10/21], [94mLoss[0m : 3.09647
[1mStep[0m  [12/21], [94mLoss[0m : 3.00208
[1mStep[0m  [14/21], [94mLoss[0m : 3.39680
[1mStep[0m  [16/21], [94mLoss[0m : 3.02325
[1mStep[0m  [18/21], [94mLoss[0m : 3.03421
[1mStep[0m  [20/21], [94mLoss[0m : 3.37070

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.092, [92mTest[0m: 3.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.10834
[1mStep[0m  [2/21], [94mLoss[0m : 2.95752
[1mStep[0m  [4/21], [94mLoss[0m : 3.12974
[1mStep[0m  [6/21], [94mLoss[0m : 2.85612
[1mStep[0m  [8/21], [94mLoss[0m : 2.73272
[1mStep[0m  [10/21], [94mLoss[0m : 2.99933
[1mStep[0m  [12/21], [94mLoss[0m : 2.95181
[1mStep[0m  [14/21], [94mLoss[0m : 3.19072
[1mStep[0m  [16/21], [94mLoss[0m : 3.12299
[1mStep[0m  [18/21], [94mLoss[0m : 3.11647
[1mStep[0m  [20/21], [94mLoss[0m : 2.89959

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.051, [92mTest[0m: 3.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.03113
[1mStep[0m  [2/21], [94mLoss[0m : 2.82359
[1mStep[0m  [4/21], [94mLoss[0m : 3.10376
[1mStep[0m  [6/21], [94mLoss[0m : 3.22476
[1mStep[0m  [8/21], [94mLoss[0m : 3.03090
[1mStep[0m  [10/21], [94mLoss[0m : 3.12352
[1mStep[0m  [12/21], [94mLoss[0m : 2.92657
[1mStep[0m  [14/21], [94mLoss[0m : 2.96698
[1mStep[0m  [16/21], [94mLoss[0m : 3.00847
[1mStep[0m  [18/21], [94mLoss[0m : 3.00051
[1mStep[0m  [20/21], [94mLoss[0m : 3.00204

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.005, [92mTest[0m: 3.379, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.311
====================================

Phase 2 - Evaluation MAE:  3.310971907206944
MAE score P1      6.548374
MAE score P2      3.310972
loss              3.005309
learning_rate       0.0001
batch_size             512
hidden_sizes         [300]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.5
weight_decay        0.0001
Name: 19, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 11.15843
[1mStep[0m  [4/42], [94mLoss[0m : 10.96434
[1mStep[0m  [8/42], [94mLoss[0m : 10.99117
[1mStep[0m  [12/42], [94mLoss[0m : 11.22337
[1mStep[0m  [16/42], [94mLoss[0m : 11.02979
[1mStep[0m  [20/42], [94mLoss[0m : 10.98286
[1mStep[0m  [24/42], [94mLoss[0m : 11.22227
[1mStep[0m  [28/42], [94mLoss[0m : 10.68479
[1mStep[0m  [32/42], [94mLoss[0m : 11.49554
[1mStep[0m  [36/42], [94mLoss[0m : 11.19809
[1mStep[0m  [40/42], [94mLoss[0m : 11.29838

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.036, [92mTest[0m: 11.044, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.31126
[1mStep[0m  [4/42], [94mLoss[0m : 11.08877
[1mStep[0m  [8/42], [94mLoss[0m : 11.07763
[1mStep[0m  [12/42], [94mLoss[0m : 11.15195
[1mStep[0m  [16/42], [94mLoss[0m : 10.97608
[1mStep[0m  [20/42], [94mLoss[0m : 11.02355
[1mStep[0m  [24/42], [94mLoss[0m : 11.19324
[1mStep[0m  [28/42], [94mLoss[0m : 10.87684
[1mStep[0m  [32/42], [94mLoss[0m : 11.03927
[1mStep[0m  [36/42], [94mLoss[0m : 10.71484
[1mStep[0m  [40/42], [94mLoss[0m : 10.76467

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.974, [92mTest[0m: 10.985, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.94231
[1mStep[0m  [4/42], [94mLoss[0m : 10.70826
[1mStep[0m  [8/42], [94mLoss[0m : 11.09510
[1mStep[0m  [12/42], [94mLoss[0m : 10.91424
[1mStep[0m  [16/42], [94mLoss[0m : 10.98114
[1mStep[0m  [20/42], [94mLoss[0m : 10.46763
[1mStep[0m  [24/42], [94mLoss[0m : 11.10732
[1mStep[0m  [28/42], [94mLoss[0m : 11.55272
[1mStep[0m  [32/42], [94mLoss[0m : 10.68625
[1mStep[0m  [36/42], [94mLoss[0m : 10.97296
[1mStep[0m  [40/42], [94mLoss[0m : 10.81510

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.907, [92mTest[0m: 10.927, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74937
[1mStep[0m  [4/42], [94mLoss[0m : 11.18099
[1mStep[0m  [8/42], [94mLoss[0m : 10.41586
[1mStep[0m  [12/42], [94mLoss[0m : 11.31061
[1mStep[0m  [16/42], [94mLoss[0m : 10.57076
[1mStep[0m  [20/42], [94mLoss[0m : 10.98251
[1mStep[0m  [24/42], [94mLoss[0m : 11.10701
[1mStep[0m  [28/42], [94mLoss[0m : 10.88675
[1mStep[0m  [32/42], [94mLoss[0m : 10.33820
[1mStep[0m  [36/42], [94mLoss[0m : 10.89669
[1mStep[0m  [40/42], [94mLoss[0m : 10.96644

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.847, [92mTest[0m: 10.851, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.85653
[1mStep[0m  [4/42], [94mLoss[0m : 10.83032
[1mStep[0m  [8/42], [94mLoss[0m : 11.20676
[1mStep[0m  [12/42], [94mLoss[0m : 10.80880
[1mStep[0m  [16/42], [94mLoss[0m : 11.08660
[1mStep[0m  [20/42], [94mLoss[0m : 11.10214
[1mStep[0m  [24/42], [94mLoss[0m : 10.90317
[1mStep[0m  [28/42], [94mLoss[0m : 10.86265
[1mStep[0m  [32/42], [94mLoss[0m : 10.99074
[1mStep[0m  [36/42], [94mLoss[0m : 10.69919
[1mStep[0m  [40/42], [94mLoss[0m : 10.91635

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.785, [92mTest[0m: 10.803, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.00234
[1mStep[0m  [4/42], [94mLoss[0m : 10.95349
[1mStep[0m  [8/42], [94mLoss[0m : 10.60168
[1mStep[0m  [12/42], [94mLoss[0m : 10.81198
[1mStep[0m  [16/42], [94mLoss[0m : 10.92771
[1mStep[0m  [20/42], [94mLoss[0m : 10.72463
[1mStep[0m  [24/42], [94mLoss[0m : 10.69175
[1mStep[0m  [28/42], [94mLoss[0m : 11.06635
[1mStep[0m  [32/42], [94mLoss[0m : 10.52984
[1mStep[0m  [36/42], [94mLoss[0m : 11.19070
[1mStep[0m  [40/42], [94mLoss[0m : 10.83788

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.722, [92mTest[0m: 10.732, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.40772
[1mStep[0m  [4/42], [94mLoss[0m : 10.35245
[1mStep[0m  [8/42], [94mLoss[0m : 10.67490
[1mStep[0m  [12/42], [94mLoss[0m : 10.98510
[1mStep[0m  [16/42], [94mLoss[0m : 10.86968
[1mStep[0m  [20/42], [94mLoss[0m : 10.49815
[1mStep[0m  [24/42], [94mLoss[0m : 10.92438
[1mStep[0m  [28/42], [94mLoss[0m : 10.27840
[1mStep[0m  [32/42], [94mLoss[0m : 10.72603
[1mStep[0m  [36/42], [94mLoss[0m : 10.60162
[1mStep[0m  [40/42], [94mLoss[0m : 11.02510

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.651, [92mTest[0m: 10.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.73309
[1mStep[0m  [4/42], [94mLoss[0m : 10.50678
[1mStep[0m  [8/42], [94mLoss[0m : 10.81349
[1mStep[0m  [12/42], [94mLoss[0m : 10.59051
[1mStep[0m  [16/42], [94mLoss[0m : 10.58827
[1mStep[0m  [20/42], [94mLoss[0m : 10.18541
[1mStep[0m  [24/42], [94mLoss[0m : 10.31068
[1mStep[0m  [28/42], [94mLoss[0m : 10.83995
[1mStep[0m  [32/42], [94mLoss[0m : 10.68264
[1mStep[0m  [36/42], [94mLoss[0m : 10.87405
[1mStep[0m  [40/42], [94mLoss[0m : 10.39566

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.590, [92mTest[0m: 10.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.49195
[1mStep[0m  [4/42], [94mLoss[0m : 10.31966
[1mStep[0m  [8/42], [94mLoss[0m : 10.64537
[1mStep[0m  [12/42], [94mLoss[0m : 10.38327
[1mStep[0m  [16/42], [94mLoss[0m : 10.76196
[1mStep[0m  [20/42], [94mLoss[0m : 10.80192
[1mStep[0m  [24/42], [94mLoss[0m : 10.55350
[1mStep[0m  [28/42], [94mLoss[0m : 10.91325
[1mStep[0m  [32/42], [94mLoss[0m : 10.57035
[1mStep[0m  [36/42], [94mLoss[0m : 10.23671
[1mStep[0m  [40/42], [94mLoss[0m : 10.53666

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.530, [92mTest[0m: 10.550, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.13276
[1mStep[0m  [4/42], [94mLoss[0m : 10.13766
[1mStep[0m  [8/42], [94mLoss[0m : 10.34839
[1mStep[0m  [12/42], [94mLoss[0m : 10.68307
[1mStep[0m  [16/42], [94mLoss[0m : 10.70631
[1mStep[0m  [20/42], [94mLoss[0m : 10.12892
[1mStep[0m  [24/42], [94mLoss[0m : 10.51989
[1mStep[0m  [28/42], [94mLoss[0m : 10.21164
[1mStep[0m  [32/42], [94mLoss[0m : 10.38755
[1mStep[0m  [36/42], [94mLoss[0m : 10.68675
[1mStep[0m  [40/42], [94mLoss[0m : 10.38257

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.472, [92mTest[0m: 10.475, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37377
[1mStep[0m  [4/42], [94mLoss[0m : 10.21891
[1mStep[0m  [8/42], [94mLoss[0m : 10.77692
[1mStep[0m  [12/42], [94mLoss[0m : 10.98844
[1mStep[0m  [16/42], [94mLoss[0m : 10.47425
[1mStep[0m  [20/42], [94mLoss[0m : 10.21339
[1mStep[0m  [24/42], [94mLoss[0m : 10.29350
[1mStep[0m  [28/42], [94mLoss[0m : 10.39489
[1mStep[0m  [32/42], [94mLoss[0m : 10.56159
[1mStep[0m  [36/42], [94mLoss[0m : 10.58876
[1mStep[0m  [40/42], [94mLoss[0m : 10.26329

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.403, [92mTest[0m: 10.406, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.38035
[1mStep[0m  [4/42], [94mLoss[0m : 10.23130
[1mStep[0m  [8/42], [94mLoss[0m : 10.69986
[1mStep[0m  [12/42], [94mLoss[0m : 10.46033
[1mStep[0m  [16/42], [94mLoss[0m : 10.46771
[1mStep[0m  [20/42], [94mLoss[0m : 10.57264
[1mStep[0m  [24/42], [94mLoss[0m : 10.28050
[1mStep[0m  [28/42], [94mLoss[0m : 10.00979
[1mStep[0m  [32/42], [94mLoss[0m : 10.01837
[1mStep[0m  [36/42], [94mLoss[0m : 10.29344
[1mStep[0m  [40/42], [94mLoss[0m : 9.93514

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.341, [92mTest[0m: 10.350, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.35403
[1mStep[0m  [4/42], [94mLoss[0m : 10.15291
[1mStep[0m  [8/42], [94mLoss[0m : 10.67545
[1mStep[0m  [12/42], [94mLoss[0m : 9.66547
[1mStep[0m  [16/42], [94mLoss[0m : 10.21536
[1mStep[0m  [20/42], [94mLoss[0m : 10.04503
[1mStep[0m  [24/42], [94mLoss[0m : 10.31421
[1mStep[0m  [28/42], [94mLoss[0m : 10.19871
[1mStep[0m  [32/42], [94mLoss[0m : 10.38641
[1mStep[0m  [36/42], [94mLoss[0m : 10.31275
[1mStep[0m  [40/42], [94mLoss[0m : 10.21555

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.276, [92mTest[0m: 10.282, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.05145
[1mStep[0m  [4/42], [94mLoss[0m : 10.54502
[1mStep[0m  [8/42], [94mLoss[0m : 10.04735
[1mStep[0m  [12/42], [94mLoss[0m : 10.29078
[1mStep[0m  [16/42], [94mLoss[0m : 9.99173
[1mStep[0m  [20/42], [94mLoss[0m : 10.67027
[1mStep[0m  [24/42], [94mLoss[0m : 10.36559
[1mStep[0m  [28/42], [94mLoss[0m : 10.04317
[1mStep[0m  [32/42], [94mLoss[0m : 9.99283
[1mStep[0m  [36/42], [94mLoss[0m : 10.50554
[1mStep[0m  [40/42], [94mLoss[0m : 10.18448

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.216, [92mTest[0m: 10.228, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.26644
[1mStep[0m  [4/42], [94mLoss[0m : 10.16111
[1mStep[0m  [8/42], [94mLoss[0m : 9.75887
[1mStep[0m  [12/42], [94mLoss[0m : 9.52486
[1mStep[0m  [16/42], [94mLoss[0m : 10.22851
[1mStep[0m  [20/42], [94mLoss[0m : 10.08745
[1mStep[0m  [24/42], [94mLoss[0m : 10.01209
[1mStep[0m  [28/42], [94mLoss[0m : 10.59748
[1mStep[0m  [32/42], [94mLoss[0m : 10.22878
[1mStep[0m  [36/42], [94mLoss[0m : 9.68722
[1mStep[0m  [40/42], [94mLoss[0m : 9.93359

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.157, [92mTest[0m: 10.167, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.41070
[1mStep[0m  [4/42], [94mLoss[0m : 10.02889
[1mStep[0m  [8/42], [94mLoss[0m : 10.01246
[1mStep[0m  [12/42], [94mLoss[0m : 10.07643
[1mStep[0m  [16/42], [94mLoss[0m : 10.34729
[1mStep[0m  [20/42], [94mLoss[0m : 9.85077
[1mStep[0m  [24/42], [94mLoss[0m : 10.30703
[1mStep[0m  [28/42], [94mLoss[0m : 10.32729
[1mStep[0m  [32/42], [94mLoss[0m : 9.88059
[1mStep[0m  [36/42], [94mLoss[0m : 10.53196
[1mStep[0m  [40/42], [94mLoss[0m : 9.70545

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.087, [92mTest[0m: 10.106, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.25670
[1mStep[0m  [4/42], [94mLoss[0m : 9.92714
[1mStep[0m  [8/42], [94mLoss[0m : 9.73815
[1mStep[0m  [12/42], [94mLoss[0m : 9.94006
[1mStep[0m  [16/42], [94mLoss[0m : 9.91164
[1mStep[0m  [20/42], [94mLoss[0m : 10.22306
[1mStep[0m  [24/42], [94mLoss[0m : 9.87359
[1mStep[0m  [28/42], [94mLoss[0m : 10.00070
[1mStep[0m  [32/42], [94mLoss[0m : 10.25081
[1mStep[0m  [36/42], [94mLoss[0m : 10.13026
[1mStep[0m  [40/42], [94mLoss[0m : 10.00867

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.023, [92mTest[0m: 10.048, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.83804
[1mStep[0m  [4/42], [94mLoss[0m : 10.04106
[1mStep[0m  [8/42], [94mLoss[0m : 9.49470
[1mStep[0m  [12/42], [94mLoss[0m : 10.12223
[1mStep[0m  [16/42], [94mLoss[0m : 10.10314
[1mStep[0m  [20/42], [94mLoss[0m : 10.05946
[1mStep[0m  [24/42], [94mLoss[0m : 9.73592
[1mStep[0m  [28/42], [94mLoss[0m : 9.81489
[1mStep[0m  [32/42], [94mLoss[0m : 10.30306
[1mStep[0m  [36/42], [94mLoss[0m : 9.78416
[1mStep[0m  [40/42], [94mLoss[0m : 10.41617

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.959, [92mTest[0m: 9.977, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.45091
[1mStep[0m  [4/42], [94mLoss[0m : 9.68090
[1mStep[0m  [8/42], [94mLoss[0m : 9.76185
[1mStep[0m  [12/42], [94mLoss[0m : 10.13075
[1mStep[0m  [16/42], [94mLoss[0m : 9.95168
[1mStep[0m  [20/42], [94mLoss[0m : 10.09038
[1mStep[0m  [24/42], [94mLoss[0m : 10.10358
[1mStep[0m  [28/42], [94mLoss[0m : 9.67845
[1mStep[0m  [32/42], [94mLoss[0m : 9.27551
[1mStep[0m  [36/42], [94mLoss[0m : 9.81532
[1mStep[0m  [40/42], [94mLoss[0m : 9.84935

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.890, [92mTest[0m: 9.909, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.82548
[1mStep[0m  [4/42], [94mLoss[0m : 9.89165
[1mStep[0m  [8/42], [94mLoss[0m : 9.81813
[1mStep[0m  [12/42], [94mLoss[0m : 9.82475
[1mStep[0m  [16/42], [94mLoss[0m : 9.76956
[1mStep[0m  [20/42], [94mLoss[0m : 9.91372
[1mStep[0m  [24/42], [94mLoss[0m : 9.84453
[1mStep[0m  [28/42], [94mLoss[0m : 9.84611
[1mStep[0m  [32/42], [94mLoss[0m : 9.92317
[1mStep[0m  [36/42], [94mLoss[0m : 10.03650
[1mStep[0m  [40/42], [94mLoss[0m : 9.80622

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.832, [92mTest[0m: 9.845, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.38465
[1mStep[0m  [4/42], [94mLoss[0m : 9.79740
[1mStep[0m  [8/42], [94mLoss[0m : 10.26559
[1mStep[0m  [12/42], [94mLoss[0m : 9.68823
[1mStep[0m  [16/42], [94mLoss[0m : 9.74614
[1mStep[0m  [20/42], [94mLoss[0m : 9.63765
[1mStep[0m  [24/42], [94mLoss[0m : 9.47544
[1mStep[0m  [28/42], [94mLoss[0m : 9.70753
[1mStep[0m  [32/42], [94mLoss[0m : 9.55332
[1mStep[0m  [36/42], [94mLoss[0m : 10.14912
[1mStep[0m  [40/42], [94mLoss[0m : 9.61832

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.771, [92mTest[0m: 9.787, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.77869
[1mStep[0m  [4/42], [94mLoss[0m : 9.43069
[1mStep[0m  [8/42], [94mLoss[0m : 9.53134
[1mStep[0m  [12/42], [94mLoss[0m : 9.42397
[1mStep[0m  [16/42], [94mLoss[0m : 9.76328
[1mStep[0m  [20/42], [94mLoss[0m : 9.82543
[1mStep[0m  [24/42], [94mLoss[0m : 9.75220
[1mStep[0m  [28/42], [94mLoss[0m : 9.90118
[1mStep[0m  [32/42], [94mLoss[0m : 9.46719
[1mStep[0m  [36/42], [94mLoss[0m : 9.45094
[1mStep[0m  [40/42], [94mLoss[0m : 9.46101

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.720, [92mTest[0m: 9.738, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.79321
[1mStep[0m  [4/42], [94mLoss[0m : 9.41895
[1mStep[0m  [8/42], [94mLoss[0m : 9.95313
[1mStep[0m  [12/42], [94mLoss[0m : 9.34638
[1mStep[0m  [16/42], [94mLoss[0m : 9.78821
[1mStep[0m  [20/42], [94mLoss[0m : 9.81622
[1mStep[0m  [24/42], [94mLoss[0m : 9.73649
[1mStep[0m  [28/42], [94mLoss[0m : 9.58304
[1mStep[0m  [32/42], [94mLoss[0m : 9.53685
[1mStep[0m  [36/42], [94mLoss[0m : 9.57171
[1mStep[0m  [40/42], [94mLoss[0m : 9.63374

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.664, [92mTest[0m: 9.669, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.63574
[1mStep[0m  [4/42], [94mLoss[0m : 9.95575
[1mStep[0m  [8/42], [94mLoss[0m : 9.92555
[1mStep[0m  [12/42], [94mLoss[0m : 9.89542
[1mStep[0m  [16/42], [94mLoss[0m : 9.30859
[1mStep[0m  [20/42], [94mLoss[0m : 9.67336
[1mStep[0m  [24/42], [94mLoss[0m : 9.53424
[1mStep[0m  [28/42], [94mLoss[0m : 9.31659
[1mStep[0m  [32/42], [94mLoss[0m : 9.73100
[1mStep[0m  [36/42], [94mLoss[0m : 9.21144
[1mStep[0m  [40/42], [94mLoss[0m : 10.13379

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.607, [92mTest[0m: 9.612, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.93102
[1mStep[0m  [4/42], [94mLoss[0m : 9.32863
[1mStep[0m  [8/42], [94mLoss[0m : 9.37263
[1mStep[0m  [12/42], [94mLoss[0m : 9.21768
[1mStep[0m  [16/42], [94mLoss[0m : 9.45488
[1mStep[0m  [20/42], [94mLoss[0m : 9.13148
[1mStep[0m  [24/42], [94mLoss[0m : 9.00727
[1mStep[0m  [28/42], [94mLoss[0m : 9.53733
[1mStep[0m  [32/42], [94mLoss[0m : 9.82756
[1mStep[0m  [36/42], [94mLoss[0m : 9.89930
[1mStep[0m  [40/42], [94mLoss[0m : 9.44727

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.547, [92mTest[0m: 9.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.39239
[1mStep[0m  [4/42], [94mLoss[0m : 9.67435
[1mStep[0m  [8/42], [94mLoss[0m : 9.57425
[1mStep[0m  [12/42], [94mLoss[0m : 9.95502
[1mStep[0m  [16/42], [94mLoss[0m : 9.34925
[1mStep[0m  [20/42], [94mLoss[0m : 9.80024
[1mStep[0m  [24/42], [94mLoss[0m : 9.70898
[1mStep[0m  [28/42], [94mLoss[0m : 9.16782
[1mStep[0m  [32/42], [94mLoss[0m : 9.47317
[1mStep[0m  [36/42], [94mLoss[0m : 9.56931
[1mStep[0m  [40/42], [94mLoss[0m : 9.34878

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.502, [92mTest[0m: 9.496, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.54265
[1mStep[0m  [4/42], [94mLoss[0m : 9.47868
[1mStep[0m  [8/42], [94mLoss[0m : 8.92163
[1mStep[0m  [12/42], [94mLoss[0m : 9.03782
[1mStep[0m  [16/42], [94mLoss[0m : 9.37498
[1mStep[0m  [20/42], [94mLoss[0m : 9.59473
[1mStep[0m  [24/42], [94mLoss[0m : 9.41012
[1mStep[0m  [28/42], [94mLoss[0m : 9.76935
[1mStep[0m  [32/42], [94mLoss[0m : 9.60868
[1mStep[0m  [36/42], [94mLoss[0m : 9.45107
[1mStep[0m  [40/42], [94mLoss[0m : 9.62536

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.430, [92mTest[0m: 9.447, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.39607
[1mStep[0m  [4/42], [94mLoss[0m : 9.28977
[1mStep[0m  [8/42], [94mLoss[0m : 9.47508
[1mStep[0m  [12/42], [94mLoss[0m : 8.98470
[1mStep[0m  [16/42], [94mLoss[0m : 9.67004
[1mStep[0m  [20/42], [94mLoss[0m : 9.44162
[1mStep[0m  [24/42], [94mLoss[0m : 9.11838
[1mStep[0m  [28/42], [94mLoss[0m : 9.18610
[1mStep[0m  [32/42], [94mLoss[0m : 9.27709
[1mStep[0m  [36/42], [94mLoss[0m : 9.20312
[1mStep[0m  [40/42], [94mLoss[0m : 9.36814

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.376, [92mTest[0m: 9.391, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.47748
[1mStep[0m  [4/42], [94mLoss[0m : 9.65072
[1mStep[0m  [8/42], [94mLoss[0m : 8.84706
[1mStep[0m  [12/42], [94mLoss[0m : 9.24096
[1mStep[0m  [16/42], [94mLoss[0m : 9.14227
[1mStep[0m  [20/42], [94mLoss[0m : 9.26373
[1mStep[0m  [24/42], [94mLoss[0m : 9.15341
[1mStep[0m  [28/42], [94mLoss[0m : 9.74553
[1mStep[0m  [32/42], [94mLoss[0m : 9.75517
[1mStep[0m  [36/42], [94mLoss[0m : 9.35215
[1mStep[0m  [40/42], [94mLoss[0m : 9.37559

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.321, [92mTest[0m: 9.336, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.53707
[1mStep[0m  [4/42], [94mLoss[0m : 9.11029
[1mStep[0m  [8/42], [94mLoss[0m : 9.06891
[1mStep[0m  [12/42], [94mLoss[0m : 9.16041
[1mStep[0m  [16/42], [94mLoss[0m : 9.33878
[1mStep[0m  [20/42], [94mLoss[0m : 9.26761
[1mStep[0m  [24/42], [94mLoss[0m : 9.03993
[1mStep[0m  [28/42], [94mLoss[0m : 9.06409
[1mStep[0m  [32/42], [94mLoss[0m : 9.32711
[1mStep[0m  [36/42], [94mLoss[0m : 9.94003
[1mStep[0m  [40/42], [94mLoss[0m : 9.47015

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.260, [92mTest[0m: 9.278, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 9.223
====================================

Phase 1 - Evaluation MAE:  9.223139354160853
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/42], [94mLoss[0m : 9.39695
[1mStep[0m  [4/42], [94mLoss[0m : 8.99814
[1mStep[0m  [8/42], [94mLoss[0m : 8.98873
[1mStep[0m  [12/42], [94mLoss[0m : 9.19468
[1mStep[0m  [16/42], [94mLoss[0m : 8.75358
[1mStep[0m  [20/42], [94mLoss[0m : 8.95801
[1mStep[0m  [24/42], [94mLoss[0m : 9.63686
[1mStep[0m  [28/42], [94mLoss[0m : 9.32714
[1mStep[0m  [32/42], [94mLoss[0m : 8.77392
[1mStep[0m  [36/42], [94mLoss[0m : 9.39208
[1mStep[0m  [40/42], [94mLoss[0m : 9.03278

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.201, [92mTest[0m: 9.216, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.49066
[1mStep[0m  [4/42], [94mLoss[0m : 9.28498
[1mStep[0m  [8/42], [94mLoss[0m : 9.35304
[1mStep[0m  [12/42], [94mLoss[0m : 9.23332
[1mStep[0m  [16/42], [94mLoss[0m : 9.16385
[1mStep[0m  [20/42], [94mLoss[0m : 9.32077
[1mStep[0m  [24/42], [94mLoss[0m : 9.04666
[1mStep[0m  [28/42], [94mLoss[0m : 8.46160
[1mStep[0m  [32/42], [94mLoss[0m : 9.17676
[1mStep[0m  [36/42], [94mLoss[0m : 9.19295
[1mStep[0m  [40/42], [94mLoss[0m : 9.49187

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.141, [92mTest[0m: 9.156, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.32348
[1mStep[0m  [4/42], [94mLoss[0m : 8.79993
[1mStep[0m  [8/42], [94mLoss[0m : 8.83621
[1mStep[0m  [12/42], [94mLoss[0m : 9.33004
[1mStep[0m  [16/42], [94mLoss[0m : 9.37600
[1mStep[0m  [20/42], [94mLoss[0m : 8.60247
[1mStep[0m  [24/42], [94mLoss[0m : 9.10508
[1mStep[0m  [28/42], [94mLoss[0m : 8.97308
[1mStep[0m  [32/42], [94mLoss[0m : 9.43149
[1mStep[0m  [36/42], [94mLoss[0m : 9.46852
[1mStep[0m  [40/42], [94mLoss[0m : 9.26290

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.082, [92mTest[0m: 9.089, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.10993
[1mStep[0m  [4/42], [94mLoss[0m : 9.18786
[1mStep[0m  [8/42], [94mLoss[0m : 9.08819
[1mStep[0m  [12/42], [94mLoss[0m : 9.04223
[1mStep[0m  [16/42], [94mLoss[0m : 9.16073
[1mStep[0m  [20/42], [94mLoss[0m : 9.28478
[1mStep[0m  [24/42], [94mLoss[0m : 8.70393
[1mStep[0m  [28/42], [94mLoss[0m : 9.08507
[1mStep[0m  [32/42], [94mLoss[0m : 8.74524
[1mStep[0m  [36/42], [94mLoss[0m : 9.26035
[1mStep[0m  [40/42], [94mLoss[0m : 8.74795

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.015, [92mTest[0m: 9.026, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.63968
[1mStep[0m  [4/42], [94mLoss[0m : 8.76983
[1mStep[0m  [8/42], [94mLoss[0m : 9.00884
[1mStep[0m  [12/42], [94mLoss[0m : 9.00552
[1mStep[0m  [16/42], [94mLoss[0m : 9.15624
[1mStep[0m  [20/42], [94mLoss[0m : 8.57513
[1mStep[0m  [24/42], [94mLoss[0m : 8.94320
[1mStep[0m  [28/42], [94mLoss[0m : 9.39978
[1mStep[0m  [32/42], [94mLoss[0m : 8.96834
[1mStep[0m  [36/42], [94mLoss[0m : 8.64357
[1mStep[0m  [40/42], [94mLoss[0m : 9.02248

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.953, [92mTest[0m: 8.955, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.92552
[1mStep[0m  [4/42], [94mLoss[0m : 8.86153
[1mStep[0m  [8/42], [94mLoss[0m : 9.18695
[1mStep[0m  [12/42], [94mLoss[0m : 9.24105
[1mStep[0m  [16/42], [94mLoss[0m : 8.58644
[1mStep[0m  [20/42], [94mLoss[0m : 8.65052
[1mStep[0m  [24/42], [94mLoss[0m : 8.93356
[1mStep[0m  [28/42], [94mLoss[0m : 8.94227
[1mStep[0m  [32/42], [94mLoss[0m : 8.81432
[1mStep[0m  [36/42], [94mLoss[0m : 8.38403
[1mStep[0m  [40/42], [94mLoss[0m : 8.50928

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.895, [92mTest[0m: 8.891, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.83310
[1mStep[0m  [4/42], [94mLoss[0m : 8.47957
[1mStep[0m  [8/42], [94mLoss[0m : 8.96544
[1mStep[0m  [12/42], [94mLoss[0m : 9.00402
[1mStep[0m  [16/42], [94mLoss[0m : 8.77453
[1mStep[0m  [20/42], [94mLoss[0m : 8.49099
[1mStep[0m  [24/42], [94mLoss[0m : 8.58880
[1mStep[0m  [28/42], [94mLoss[0m : 9.06799
[1mStep[0m  [32/42], [94mLoss[0m : 8.82601
[1mStep[0m  [36/42], [94mLoss[0m : 8.75883
[1mStep[0m  [40/42], [94mLoss[0m : 8.99106

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.826, [92mTest[0m: 8.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.80891
[1mStep[0m  [4/42], [94mLoss[0m : 8.56361
[1mStep[0m  [8/42], [94mLoss[0m : 8.89580
[1mStep[0m  [12/42], [94mLoss[0m : 8.46969
[1mStep[0m  [16/42], [94mLoss[0m : 8.78615
[1mStep[0m  [20/42], [94mLoss[0m : 8.81849
[1mStep[0m  [24/42], [94mLoss[0m : 8.88621
[1mStep[0m  [28/42], [94mLoss[0m : 8.69980
[1mStep[0m  [32/42], [94mLoss[0m : 8.57775
[1mStep[0m  [36/42], [94mLoss[0m : 9.10549
[1mStep[0m  [40/42], [94mLoss[0m : 8.61622

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.764, [92mTest[0m: 8.770, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.69929
[1mStep[0m  [4/42], [94mLoss[0m : 8.88692
[1mStep[0m  [8/42], [94mLoss[0m : 8.93915
[1mStep[0m  [12/42], [94mLoss[0m : 8.42900
[1mStep[0m  [16/42], [94mLoss[0m : 8.75286
[1mStep[0m  [20/42], [94mLoss[0m : 9.09344
[1mStep[0m  [24/42], [94mLoss[0m : 8.56384
[1mStep[0m  [28/42], [94mLoss[0m : 9.05777
[1mStep[0m  [32/42], [94mLoss[0m : 8.74097
[1mStep[0m  [36/42], [94mLoss[0m : 8.35332
[1mStep[0m  [40/42], [94mLoss[0m : 8.91914

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.698, [92mTest[0m: 8.722, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.63120
[1mStep[0m  [4/42], [94mLoss[0m : 8.23490
[1mStep[0m  [8/42], [94mLoss[0m : 8.76028
[1mStep[0m  [12/42], [94mLoss[0m : 8.41364
[1mStep[0m  [16/42], [94mLoss[0m : 8.82273
[1mStep[0m  [20/42], [94mLoss[0m : 8.39278
[1mStep[0m  [24/42], [94mLoss[0m : 8.32763
[1mStep[0m  [28/42], [94mLoss[0m : 8.84899
[1mStep[0m  [32/42], [94mLoss[0m : 8.61605
[1mStep[0m  [36/42], [94mLoss[0m : 8.90754
[1mStep[0m  [40/42], [94mLoss[0m : 8.41984

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.638, [92mTest[0m: 8.655, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.53618
[1mStep[0m  [4/42], [94mLoss[0m : 8.37416
[1mStep[0m  [8/42], [94mLoss[0m : 8.28153
[1mStep[0m  [12/42], [94mLoss[0m : 8.51530
[1mStep[0m  [16/42], [94mLoss[0m : 8.37591
[1mStep[0m  [20/42], [94mLoss[0m : 8.29588
[1mStep[0m  [24/42], [94mLoss[0m : 8.46482
[1mStep[0m  [28/42], [94mLoss[0m : 8.76582
[1mStep[0m  [32/42], [94mLoss[0m : 8.82246
[1mStep[0m  [36/42], [94mLoss[0m : 8.86496
[1mStep[0m  [40/42], [94mLoss[0m : 8.49076

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.577, [92mTest[0m: 8.591, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.58676
[1mStep[0m  [4/42], [94mLoss[0m : 8.29775
[1mStep[0m  [8/42], [94mLoss[0m : 8.74310
[1mStep[0m  [12/42], [94mLoss[0m : 8.48266
[1mStep[0m  [16/42], [94mLoss[0m : 8.46458
[1mStep[0m  [20/42], [94mLoss[0m : 8.49662
[1mStep[0m  [24/42], [94mLoss[0m : 8.38193
[1mStep[0m  [28/42], [94mLoss[0m : 8.37271
[1mStep[0m  [32/42], [94mLoss[0m : 8.58835
[1mStep[0m  [36/42], [94mLoss[0m : 8.28378
[1mStep[0m  [40/42], [94mLoss[0m : 8.58463

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.507, [92mTest[0m: 8.527, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.45653
[1mStep[0m  [4/42], [94mLoss[0m : 8.39906
[1mStep[0m  [8/42], [94mLoss[0m : 8.43882
[1mStep[0m  [12/42], [94mLoss[0m : 8.32204
[1mStep[0m  [16/42], [94mLoss[0m : 8.62770
[1mStep[0m  [20/42], [94mLoss[0m : 8.54723
[1mStep[0m  [24/42], [94mLoss[0m : 8.28362
[1mStep[0m  [28/42], [94mLoss[0m : 7.94139
[1mStep[0m  [32/42], [94mLoss[0m : 8.20834
[1mStep[0m  [36/42], [94mLoss[0m : 8.30088
[1mStep[0m  [40/42], [94mLoss[0m : 8.75590

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 8.451, [92mTest[0m: 8.457, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.34524
[1mStep[0m  [4/42], [94mLoss[0m : 8.45761
[1mStep[0m  [8/42], [94mLoss[0m : 8.58039
[1mStep[0m  [12/42], [94mLoss[0m : 8.20196
[1mStep[0m  [16/42], [94mLoss[0m : 8.60569
[1mStep[0m  [20/42], [94mLoss[0m : 8.08269
[1mStep[0m  [24/42], [94mLoss[0m : 8.44758
[1mStep[0m  [28/42], [94mLoss[0m : 8.19774
[1mStep[0m  [32/42], [94mLoss[0m : 8.23219
[1mStep[0m  [36/42], [94mLoss[0m : 8.36906
[1mStep[0m  [40/42], [94mLoss[0m : 8.21039

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 8.381, [92mTest[0m: 8.385, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.29424
[1mStep[0m  [4/42], [94mLoss[0m : 8.15046
[1mStep[0m  [8/42], [94mLoss[0m : 7.92963
[1mStep[0m  [12/42], [94mLoss[0m : 8.51035
[1mStep[0m  [16/42], [94mLoss[0m : 8.43201
[1mStep[0m  [20/42], [94mLoss[0m : 8.42652
[1mStep[0m  [24/42], [94mLoss[0m : 8.49909
[1mStep[0m  [28/42], [94mLoss[0m : 8.40716
[1mStep[0m  [32/42], [94mLoss[0m : 8.50233
[1mStep[0m  [36/42], [94mLoss[0m : 8.54633
[1mStep[0m  [40/42], [94mLoss[0m : 8.21690

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 8.318, [92mTest[0m: 8.325, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.07578
[1mStep[0m  [4/42], [94mLoss[0m : 8.21383
[1mStep[0m  [8/42], [94mLoss[0m : 8.30091
[1mStep[0m  [12/42], [94mLoss[0m : 7.99955
[1mStep[0m  [16/42], [94mLoss[0m : 8.64231
[1mStep[0m  [20/42], [94mLoss[0m : 8.25763
[1mStep[0m  [24/42], [94mLoss[0m : 8.12373
[1mStep[0m  [28/42], [94mLoss[0m : 8.17389
[1mStep[0m  [32/42], [94mLoss[0m : 8.55199
[1mStep[0m  [36/42], [94mLoss[0m : 8.09949
[1mStep[0m  [40/42], [94mLoss[0m : 8.22801

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 8.263, [92mTest[0m: 8.253, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.84078
[1mStep[0m  [4/42], [94mLoss[0m : 8.03211
[1mStep[0m  [8/42], [94mLoss[0m : 8.05303
[1mStep[0m  [12/42], [94mLoss[0m : 8.69509
[1mStep[0m  [16/42], [94mLoss[0m : 7.94927
[1mStep[0m  [20/42], [94mLoss[0m : 8.04288
[1mStep[0m  [24/42], [94mLoss[0m : 8.27925
[1mStep[0m  [28/42], [94mLoss[0m : 8.33875
[1mStep[0m  [32/42], [94mLoss[0m : 8.35036
[1mStep[0m  [36/42], [94mLoss[0m : 7.98438
[1mStep[0m  [40/42], [94mLoss[0m : 8.27207

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 8.203, [92mTest[0m: 8.208, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.21797
[1mStep[0m  [4/42], [94mLoss[0m : 7.94082
[1mStep[0m  [8/42], [94mLoss[0m : 8.15948
[1mStep[0m  [12/42], [94mLoss[0m : 8.30735
[1mStep[0m  [16/42], [94mLoss[0m : 8.12527
[1mStep[0m  [20/42], [94mLoss[0m : 8.31871
[1mStep[0m  [24/42], [94mLoss[0m : 8.22465
[1mStep[0m  [28/42], [94mLoss[0m : 8.01210
[1mStep[0m  [32/42], [94mLoss[0m : 8.03772
[1mStep[0m  [36/42], [94mLoss[0m : 7.97441
[1mStep[0m  [40/42], [94mLoss[0m : 8.11841

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 8.135, [92mTest[0m: 8.141, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.80072
[1mStep[0m  [4/42], [94mLoss[0m : 7.89088
[1mStep[0m  [8/42], [94mLoss[0m : 8.13744
[1mStep[0m  [12/42], [94mLoss[0m : 8.44940
[1mStep[0m  [16/42], [94mLoss[0m : 8.35302
[1mStep[0m  [20/42], [94mLoss[0m : 7.70485
[1mStep[0m  [24/42], [94mLoss[0m : 7.61852
[1mStep[0m  [28/42], [94mLoss[0m : 8.02614
[1mStep[0m  [32/42], [94mLoss[0m : 7.99014
[1mStep[0m  [36/42], [94mLoss[0m : 8.50478
[1mStep[0m  [40/42], [94mLoss[0m : 8.37141

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 8.066, [92mTest[0m: 8.063, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.16025
[1mStep[0m  [4/42], [94mLoss[0m : 7.91878
[1mStep[0m  [8/42], [94mLoss[0m : 8.06377
[1mStep[0m  [12/42], [94mLoss[0m : 8.01820
[1mStep[0m  [16/42], [94mLoss[0m : 8.40067
[1mStep[0m  [20/42], [94mLoss[0m : 7.98089
[1mStep[0m  [24/42], [94mLoss[0m : 8.00529
[1mStep[0m  [28/42], [94mLoss[0m : 7.69011
[1mStep[0m  [32/42], [94mLoss[0m : 7.67601
[1mStep[0m  [36/42], [94mLoss[0m : 8.06114
[1mStep[0m  [40/42], [94mLoss[0m : 7.80473

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.008, [92mTest[0m: 8.016, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.23358
[1mStep[0m  [4/42], [94mLoss[0m : 7.81432
[1mStep[0m  [8/42], [94mLoss[0m : 8.05738
[1mStep[0m  [12/42], [94mLoss[0m : 7.91848
[1mStep[0m  [16/42], [94mLoss[0m : 8.33401
[1mStep[0m  [20/42], [94mLoss[0m : 7.99921
[1mStep[0m  [24/42], [94mLoss[0m : 7.90504
[1mStep[0m  [28/42], [94mLoss[0m : 8.07752
[1mStep[0m  [32/42], [94mLoss[0m : 8.33705
[1mStep[0m  [36/42], [94mLoss[0m : 7.68544
[1mStep[0m  [40/42], [94mLoss[0m : 8.00049

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.948, [92mTest[0m: 7.938, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.44567
[1mStep[0m  [4/42], [94mLoss[0m : 8.06641
[1mStep[0m  [8/42], [94mLoss[0m : 7.84742
[1mStep[0m  [12/42], [94mLoss[0m : 7.88100
[1mStep[0m  [16/42], [94mLoss[0m : 7.60724
[1mStep[0m  [20/42], [94mLoss[0m : 7.78543
[1mStep[0m  [24/42], [94mLoss[0m : 7.88569
[1mStep[0m  [28/42], [94mLoss[0m : 8.06956
[1mStep[0m  [32/42], [94mLoss[0m : 7.74656
[1mStep[0m  [36/42], [94mLoss[0m : 7.96481
[1mStep[0m  [40/42], [94mLoss[0m : 7.68534

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 7.884, [92mTest[0m: 7.892, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.15723
[1mStep[0m  [4/42], [94mLoss[0m : 8.01291
[1mStep[0m  [8/42], [94mLoss[0m : 7.94427
[1mStep[0m  [12/42], [94mLoss[0m : 7.74730
[1mStep[0m  [16/42], [94mLoss[0m : 7.64671
[1mStep[0m  [20/42], [94mLoss[0m : 8.29918
[1mStep[0m  [24/42], [94mLoss[0m : 7.64919
[1mStep[0m  [28/42], [94mLoss[0m : 7.99007
[1mStep[0m  [32/42], [94mLoss[0m : 7.72906
[1mStep[0m  [36/42], [94mLoss[0m : 7.97206
[1mStep[0m  [40/42], [94mLoss[0m : 8.05208

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 7.832, [92mTest[0m: 7.835, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.46839
[1mStep[0m  [4/42], [94mLoss[0m : 8.02643
[1mStep[0m  [8/42], [94mLoss[0m : 7.91738
[1mStep[0m  [12/42], [94mLoss[0m : 7.85814
[1mStep[0m  [16/42], [94mLoss[0m : 7.62032
[1mStep[0m  [20/42], [94mLoss[0m : 8.01722
[1mStep[0m  [24/42], [94mLoss[0m : 7.94518
[1mStep[0m  [28/42], [94mLoss[0m : 7.86240
[1mStep[0m  [32/42], [94mLoss[0m : 8.05076
[1mStep[0m  [36/42], [94mLoss[0m : 7.92432
[1mStep[0m  [40/42], [94mLoss[0m : 7.94343

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 7.777, [92mTest[0m: 7.775, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.54240
[1mStep[0m  [4/42], [94mLoss[0m : 7.97649
[1mStep[0m  [8/42], [94mLoss[0m : 7.53950
[1mStep[0m  [12/42], [94mLoss[0m : 7.75578
[1mStep[0m  [16/42], [94mLoss[0m : 7.77179
[1mStep[0m  [20/42], [94mLoss[0m : 7.84356
[1mStep[0m  [24/42], [94mLoss[0m : 7.48313
[1mStep[0m  [28/42], [94mLoss[0m : 7.87342
[1mStep[0m  [32/42], [94mLoss[0m : 7.49602
[1mStep[0m  [36/42], [94mLoss[0m : 7.78312
[1mStep[0m  [40/42], [94mLoss[0m : 7.69209

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 7.716, [92mTest[0m: 7.718, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.98902
[1mStep[0m  [4/42], [94mLoss[0m : 8.15061
[1mStep[0m  [8/42], [94mLoss[0m : 7.72514
[1mStep[0m  [12/42], [94mLoss[0m : 7.09965
[1mStep[0m  [16/42], [94mLoss[0m : 8.01198
[1mStep[0m  [20/42], [94mLoss[0m : 7.32062
[1mStep[0m  [24/42], [94mLoss[0m : 7.81113
[1mStep[0m  [28/42], [94mLoss[0m : 7.45356
[1mStep[0m  [32/42], [94mLoss[0m : 7.62605
[1mStep[0m  [36/42], [94mLoss[0m : 7.76001
[1mStep[0m  [40/42], [94mLoss[0m : 7.72973

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 7.659, [92mTest[0m: 7.668, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.04776
[1mStep[0m  [4/42], [94mLoss[0m : 7.80823
[1mStep[0m  [8/42], [94mLoss[0m : 7.66474
[1mStep[0m  [12/42], [94mLoss[0m : 7.33863
[1mStep[0m  [16/42], [94mLoss[0m : 7.74554
[1mStep[0m  [20/42], [94mLoss[0m : 7.30943
[1mStep[0m  [24/42], [94mLoss[0m : 7.62339
[1mStep[0m  [28/42], [94mLoss[0m : 7.53397
[1mStep[0m  [32/42], [94mLoss[0m : 7.96721
[1mStep[0m  [36/42], [94mLoss[0m : 7.63036
[1mStep[0m  [40/42], [94mLoss[0m : 7.68835

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 7.605, [92mTest[0m: 7.594, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.33041
[1mStep[0m  [4/42], [94mLoss[0m : 7.58784
[1mStep[0m  [8/42], [94mLoss[0m : 7.37825
[1mStep[0m  [12/42], [94mLoss[0m : 7.71662
[1mStep[0m  [16/42], [94mLoss[0m : 7.38446
[1mStep[0m  [20/42], [94mLoss[0m : 7.71519
[1mStep[0m  [24/42], [94mLoss[0m : 7.30294
[1mStep[0m  [28/42], [94mLoss[0m : 7.40710
[1mStep[0m  [32/42], [94mLoss[0m : 7.50774
[1mStep[0m  [36/42], [94mLoss[0m : 7.34239
[1mStep[0m  [40/42], [94mLoss[0m : 7.78457

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 7.545, [92mTest[0m: 7.555, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.12251
[1mStep[0m  [4/42], [94mLoss[0m : 7.56123
[1mStep[0m  [8/42], [94mLoss[0m : 7.49419
[1mStep[0m  [12/42], [94mLoss[0m : 7.44439
[1mStep[0m  [16/42], [94mLoss[0m : 7.38940
[1mStep[0m  [20/42], [94mLoss[0m : 7.54877
[1mStep[0m  [24/42], [94mLoss[0m : 7.45768
[1mStep[0m  [28/42], [94mLoss[0m : 7.63966
[1mStep[0m  [32/42], [94mLoss[0m : 7.28636
[1mStep[0m  [36/42], [94mLoss[0m : 7.52149
[1mStep[0m  [40/42], [94mLoss[0m : 7.32433

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.491, [92mTest[0m: 7.490, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.64115
[1mStep[0m  [4/42], [94mLoss[0m : 7.58312
[1mStep[0m  [8/42], [94mLoss[0m : 7.28849
[1mStep[0m  [12/42], [94mLoss[0m : 7.21074
[1mStep[0m  [16/42], [94mLoss[0m : 7.41044
[1mStep[0m  [20/42], [94mLoss[0m : 7.26167
[1mStep[0m  [24/42], [94mLoss[0m : 7.56787
[1mStep[0m  [28/42], [94mLoss[0m : 7.65804
[1mStep[0m  [32/42], [94mLoss[0m : 7.95286
[1mStep[0m  [36/42], [94mLoss[0m : 7.24287
[1mStep[0m  [40/42], [94mLoss[0m : 7.38786

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.434, [92mTest[0m: 7.441, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.364
====================================

Phase 2 - Evaluation MAE:  7.364365305219378
MAE score P1       9.223139
MAE score P2       7.364365
loss               7.434051
learning_rate        0.0001
batch_size              256
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay          0.001
Name: 20, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.09544
[1mStep[0m  [2/21], [94mLoss[0m : 11.02600
[1mStep[0m  [4/21], [94mLoss[0m : 11.11825
[1mStep[0m  [6/21], [94mLoss[0m : 11.07823
[1mStep[0m  [8/21], [94mLoss[0m : 10.91843
[1mStep[0m  [10/21], [94mLoss[0m : 10.99676
[1mStep[0m  [12/21], [94mLoss[0m : 10.99964
[1mStep[0m  [14/21], [94mLoss[0m : 10.96896
[1mStep[0m  [16/21], [94mLoss[0m : 10.63193
[1mStep[0m  [18/21], [94mLoss[0m : 10.94862
[1mStep[0m  [20/21], [94mLoss[0m : 10.88468

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.957, [92mTest[0m: 10.906, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.93025
[1mStep[0m  [2/21], [94mLoss[0m : 10.75857
[1mStep[0m  [4/21], [94mLoss[0m : 10.74809
[1mStep[0m  [6/21], [94mLoss[0m : 10.71880
[1mStep[0m  [8/21], [94mLoss[0m : 10.69208
[1mStep[0m  [10/21], [94mLoss[0m : 10.66243
[1mStep[0m  [12/21], [94mLoss[0m : 10.79652
[1mStep[0m  [14/21], [94mLoss[0m : 10.54698
[1mStep[0m  [16/21], [94mLoss[0m : 10.51014
[1mStep[0m  [18/21], [94mLoss[0m : 10.57683
[1mStep[0m  [20/21], [94mLoss[0m : 10.50571

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.668, [92mTest[0m: 10.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54609
[1mStep[0m  [2/21], [94mLoss[0m : 10.39564
[1mStep[0m  [4/21], [94mLoss[0m : 10.43336
[1mStep[0m  [6/21], [94mLoss[0m : 10.41223
[1mStep[0m  [8/21], [94mLoss[0m : 10.12769
[1mStep[0m  [10/21], [94mLoss[0m : 10.32598
[1mStep[0m  [12/21], [94mLoss[0m : 10.27665
[1mStep[0m  [14/21], [94mLoss[0m : 10.30307
[1mStep[0m  [16/21], [94mLoss[0m : 10.45988
[1mStep[0m  [18/21], [94mLoss[0m : 10.18428
[1mStep[0m  [20/21], [94mLoss[0m : 10.03209

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.345, [92mTest[0m: 10.538, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.18155
[1mStep[0m  [2/21], [94mLoss[0m : 9.95045
[1mStep[0m  [4/21], [94mLoss[0m : 10.21341
[1mStep[0m  [6/21], [94mLoss[0m : 9.96923
[1mStep[0m  [8/21], [94mLoss[0m : 9.92587
[1mStep[0m  [10/21], [94mLoss[0m : 10.06974
[1mStep[0m  [12/21], [94mLoss[0m : 10.10293
[1mStep[0m  [14/21], [94mLoss[0m : 9.67886
[1mStep[0m  [16/21], [94mLoss[0m : 9.70769
[1mStep[0m  [18/21], [94mLoss[0m : 9.79838
[1mStep[0m  [20/21], [94mLoss[0m : 9.69542

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.011, [92mTest[0m: 10.248, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.75527
[1mStep[0m  [2/21], [94mLoss[0m : 9.98487
[1mStep[0m  [4/21], [94mLoss[0m : 9.51116
[1mStep[0m  [6/21], [94mLoss[0m : 9.76600
[1mStep[0m  [8/21], [94mLoss[0m : 9.56119
[1mStep[0m  [10/21], [94mLoss[0m : 9.70176
[1mStep[0m  [12/21], [94mLoss[0m : 9.54421
[1mStep[0m  [14/21], [94mLoss[0m : 9.76399
[1mStep[0m  [16/21], [94mLoss[0m : 9.58148
[1mStep[0m  [18/21], [94mLoss[0m : 9.69549
[1mStep[0m  [20/21], [94mLoss[0m : 9.63817

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.674, [92mTest[0m: 9.965, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.54452
[1mStep[0m  [2/21], [94mLoss[0m : 9.60273
[1mStep[0m  [4/21], [94mLoss[0m : 9.36051
[1mStep[0m  [6/21], [94mLoss[0m : 9.25385
[1mStep[0m  [8/21], [94mLoss[0m : 9.24718
[1mStep[0m  [10/21], [94mLoss[0m : 9.26291
[1mStep[0m  [12/21], [94mLoss[0m : 9.49345
[1mStep[0m  [14/21], [94mLoss[0m : 9.43640
[1mStep[0m  [16/21], [94mLoss[0m : 9.30134
[1mStep[0m  [18/21], [94mLoss[0m : 9.08177
[1mStep[0m  [20/21], [94mLoss[0m : 9.13318

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.336, [92mTest[0m: 9.701, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.16000
[1mStep[0m  [2/21], [94mLoss[0m : 9.02183
[1mStep[0m  [4/21], [94mLoss[0m : 9.10506
[1mStep[0m  [6/21], [94mLoss[0m : 9.00608
[1mStep[0m  [8/21], [94mLoss[0m : 9.10913
[1mStep[0m  [10/21], [94mLoss[0m : 9.04653
[1mStep[0m  [12/21], [94mLoss[0m : 9.02003
[1mStep[0m  [14/21], [94mLoss[0m : 8.95439
[1mStep[0m  [16/21], [94mLoss[0m : 8.89449
[1mStep[0m  [18/21], [94mLoss[0m : 8.73170
[1mStep[0m  [20/21], [94mLoss[0m : 9.02329

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.003, [92mTest[0m: 9.429, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.91135
[1mStep[0m  [2/21], [94mLoss[0m : 8.75136
[1mStep[0m  [4/21], [94mLoss[0m : 8.92898
[1mStep[0m  [6/21], [94mLoss[0m : 8.77996
[1mStep[0m  [8/21], [94mLoss[0m : 8.47965
[1mStep[0m  [10/21], [94mLoss[0m : 8.72710
[1mStep[0m  [12/21], [94mLoss[0m : 8.66161
[1mStep[0m  [14/21], [94mLoss[0m : 8.68071
[1mStep[0m  [16/21], [94mLoss[0m : 8.75153
[1mStep[0m  [18/21], [94mLoss[0m : 8.56355
[1mStep[0m  [20/21], [94mLoss[0m : 8.37435

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.671, [92mTest[0m: 9.138, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.31315
[1mStep[0m  [2/21], [94mLoss[0m : 8.40324
[1mStep[0m  [4/21], [94mLoss[0m : 8.43487
[1mStep[0m  [6/21], [94mLoss[0m : 8.43323
[1mStep[0m  [8/21], [94mLoss[0m : 8.40786
[1mStep[0m  [10/21], [94mLoss[0m : 8.67019
[1mStep[0m  [12/21], [94mLoss[0m : 8.47972
[1mStep[0m  [14/21], [94mLoss[0m : 8.73625
[1mStep[0m  [16/21], [94mLoss[0m : 8.02229
[1mStep[0m  [18/21], [94mLoss[0m : 8.08743
[1mStep[0m  [20/21], [94mLoss[0m : 8.04094

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.320, [92mTest[0m: 8.868, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.15011
[1mStep[0m  [2/21], [94mLoss[0m : 8.03163
[1mStep[0m  [4/21], [94mLoss[0m : 8.16180
[1mStep[0m  [6/21], [94mLoss[0m : 7.75181
[1mStep[0m  [8/21], [94mLoss[0m : 7.94292
[1mStep[0m  [10/21], [94mLoss[0m : 8.03925
[1mStep[0m  [12/21], [94mLoss[0m : 7.83527
[1mStep[0m  [14/21], [94mLoss[0m : 8.02745
[1mStep[0m  [16/21], [94mLoss[0m : 7.90911
[1mStep[0m  [18/21], [94mLoss[0m : 7.92879
[1mStep[0m  [20/21], [94mLoss[0m : 7.78044

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 7.990, [92mTest[0m: 8.575, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.75559
[1mStep[0m  [2/21], [94mLoss[0m : 7.84152
[1mStep[0m  [4/21], [94mLoss[0m : 7.68332
[1mStep[0m  [6/21], [94mLoss[0m : 7.80734
[1mStep[0m  [8/21], [94mLoss[0m : 7.88412
[1mStep[0m  [10/21], [94mLoss[0m : 7.66263
[1mStep[0m  [12/21], [94mLoss[0m : 7.65374
[1mStep[0m  [14/21], [94mLoss[0m : 7.37866
[1mStep[0m  [16/21], [94mLoss[0m : 7.57449
[1mStep[0m  [18/21], [94mLoss[0m : 7.40978
[1mStep[0m  [20/21], [94mLoss[0m : 7.45188

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.663, [92mTest[0m: 8.288, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.40022
[1mStep[0m  [2/21], [94mLoss[0m : 7.54230
[1mStep[0m  [4/21], [94mLoss[0m : 7.28671
[1mStep[0m  [6/21], [94mLoss[0m : 7.62324
[1mStep[0m  [8/21], [94mLoss[0m : 7.38874
[1mStep[0m  [10/21], [94mLoss[0m : 7.37419
[1mStep[0m  [12/21], [94mLoss[0m : 7.43894
[1mStep[0m  [14/21], [94mLoss[0m : 7.40539
[1mStep[0m  [16/21], [94mLoss[0m : 7.27590
[1mStep[0m  [18/21], [94mLoss[0m : 7.24932
[1mStep[0m  [20/21], [94mLoss[0m : 7.04791

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.346, [92mTest[0m: 8.029, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.90408
[1mStep[0m  [2/21], [94mLoss[0m : 6.86607
[1mStep[0m  [4/21], [94mLoss[0m : 6.91069
[1mStep[0m  [6/21], [94mLoss[0m : 7.25877
[1mStep[0m  [8/21], [94mLoss[0m : 6.93776
[1mStep[0m  [10/21], [94mLoss[0m : 7.21088
[1mStep[0m  [12/21], [94mLoss[0m : 7.23040
[1mStep[0m  [14/21], [94mLoss[0m : 7.03020
[1mStep[0m  [16/21], [94mLoss[0m : 6.94631
[1mStep[0m  [18/21], [94mLoss[0m : 6.76024
[1mStep[0m  [20/21], [94mLoss[0m : 7.09569

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.033, [92mTest[0m: 7.748, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.04649
[1mStep[0m  [2/21], [94mLoss[0m : 6.63093
[1mStep[0m  [4/21], [94mLoss[0m : 6.55336
[1mStep[0m  [6/21], [94mLoss[0m : 6.81461
[1mStep[0m  [8/21], [94mLoss[0m : 6.90970
[1mStep[0m  [10/21], [94mLoss[0m : 6.73267
[1mStep[0m  [12/21], [94mLoss[0m : 6.54414
[1mStep[0m  [14/21], [94mLoss[0m : 6.90879
[1mStep[0m  [16/21], [94mLoss[0m : 6.56154
[1mStep[0m  [18/21], [94mLoss[0m : 6.80084
[1mStep[0m  [20/21], [94mLoss[0m : 6.59257

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.703, [92mTest[0m: 7.471, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.52596
[1mStep[0m  [2/21], [94mLoss[0m : 6.84032
[1mStep[0m  [4/21], [94mLoss[0m : 6.58381
[1mStep[0m  [6/21], [94mLoss[0m : 6.22559
[1mStep[0m  [8/21], [94mLoss[0m : 6.36448
[1mStep[0m  [10/21], [94mLoss[0m : 6.51417
[1mStep[0m  [12/21], [94mLoss[0m : 6.43472
[1mStep[0m  [14/21], [94mLoss[0m : 6.25735
[1mStep[0m  [16/21], [94mLoss[0m : 6.21016
[1mStep[0m  [18/21], [94mLoss[0m : 6.31650
[1mStep[0m  [20/21], [94mLoss[0m : 6.19027

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.398, [92mTest[0m: 7.194, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.32425
[1mStep[0m  [2/21], [94mLoss[0m : 6.16691
[1mStep[0m  [4/21], [94mLoss[0m : 6.40556
[1mStep[0m  [6/21], [94mLoss[0m : 6.04535
[1mStep[0m  [8/21], [94mLoss[0m : 6.01028
[1mStep[0m  [10/21], [94mLoss[0m : 5.98909
[1mStep[0m  [12/21], [94mLoss[0m : 6.02911
[1mStep[0m  [14/21], [94mLoss[0m : 5.96982
[1mStep[0m  [16/21], [94mLoss[0m : 6.11587
[1mStep[0m  [18/21], [94mLoss[0m : 5.98758
[1mStep[0m  [20/21], [94mLoss[0m : 5.73469

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.088, [92mTest[0m: 6.907, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.71008
[1mStep[0m  [2/21], [94mLoss[0m : 6.01266
[1mStep[0m  [4/21], [94mLoss[0m : 5.73842
[1mStep[0m  [6/21], [94mLoss[0m : 6.01972
[1mStep[0m  [8/21], [94mLoss[0m : 5.99303
[1mStep[0m  [10/21], [94mLoss[0m : 5.86117
[1mStep[0m  [12/21], [94mLoss[0m : 5.84615
[1mStep[0m  [14/21], [94mLoss[0m : 5.69615
[1mStep[0m  [16/21], [94mLoss[0m : 5.60603
[1mStep[0m  [18/21], [94mLoss[0m : 5.50273
[1mStep[0m  [20/21], [94mLoss[0m : 5.72176

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.787, [92mTest[0m: 6.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.69045
[1mStep[0m  [2/21], [94mLoss[0m : 5.53733
[1mStep[0m  [4/21], [94mLoss[0m : 5.91219
[1mStep[0m  [6/21], [94mLoss[0m : 5.65440
[1mStep[0m  [8/21], [94mLoss[0m : 5.46899
[1mStep[0m  [10/21], [94mLoss[0m : 5.74068
[1mStep[0m  [12/21], [94mLoss[0m : 5.32321
[1mStep[0m  [14/21], [94mLoss[0m : 5.32952
[1mStep[0m  [16/21], [94mLoss[0m : 5.29738
[1mStep[0m  [18/21], [94mLoss[0m : 5.38654
[1mStep[0m  [20/21], [94mLoss[0m : 5.29803

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.515, [92mTest[0m: 6.302, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.65766
[1mStep[0m  [2/21], [94mLoss[0m : 5.56457
[1mStep[0m  [4/21], [94mLoss[0m : 5.34384
[1mStep[0m  [6/21], [94mLoss[0m : 5.32194
[1mStep[0m  [8/21], [94mLoss[0m : 5.35331
[1mStep[0m  [10/21], [94mLoss[0m : 5.00415
[1mStep[0m  [12/21], [94mLoss[0m : 4.94854
[1mStep[0m  [14/21], [94mLoss[0m : 4.94374
[1mStep[0m  [16/21], [94mLoss[0m : 5.04456
[1mStep[0m  [18/21], [94mLoss[0m : 4.93641
[1mStep[0m  [20/21], [94mLoss[0m : 5.27271

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.200, [92mTest[0m: 6.018, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.86030
[1mStep[0m  [2/21], [94mLoss[0m : 5.05415
[1mStep[0m  [4/21], [94mLoss[0m : 4.60375
[1mStep[0m  [6/21], [94mLoss[0m : 4.95205
[1mStep[0m  [8/21], [94mLoss[0m : 5.10613
[1mStep[0m  [10/21], [94mLoss[0m : 4.87247
[1mStep[0m  [12/21], [94mLoss[0m : 5.13938
[1mStep[0m  [14/21], [94mLoss[0m : 4.99123
[1mStep[0m  [16/21], [94mLoss[0m : 4.75275
[1mStep[0m  [18/21], [94mLoss[0m : 4.78715
[1mStep[0m  [20/21], [94mLoss[0m : 4.87992

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 4.930, [92mTest[0m: 5.721, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.84934
[1mStep[0m  [2/21], [94mLoss[0m : 4.53431
[1mStep[0m  [4/21], [94mLoss[0m : 4.59022
[1mStep[0m  [6/21], [94mLoss[0m : 4.88389
[1mStep[0m  [8/21], [94mLoss[0m : 4.65209
[1mStep[0m  [10/21], [94mLoss[0m : 4.99752
[1mStep[0m  [12/21], [94mLoss[0m : 4.69188
[1mStep[0m  [14/21], [94mLoss[0m : 4.60765
[1mStep[0m  [16/21], [94mLoss[0m : 4.54047
[1mStep[0m  [18/21], [94mLoss[0m : 4.63346
[1mStep[0m  [20/21], [94mLoss[0m : 4.75289

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 4.704, [92mTest[0m: 5.414, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.64166
[1mStep[0m  [2/21], [94mLoss[0m : 4.40645
[1mStep[0m  [4/21], [94mLoss[0m : 4.69142
[1mStep[0m  [6/21], [94mLoss[0m : 4.43269
[1mStep[0m  [8/21], [94mLoss[0m : 4.26712
[1mStep[0m  [10/21], [94mLoss[0m : 4.40717
[1mStep[0m  [12/21], [94mLoss[0m : 4.26066
[1mStep[0m  [14/21], [94mLoss[0m : 4.56432
[1mStep[0m  [16/21], [94mLoss[0m : 4.10515
[1mStep[0m  [18/21], [94mLoss[0m : 4.62156
[1mStep[0m  [20/21], [94mLoss[0m : 4.35995

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 4.487, [92mTest[0m: 5.148, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.58510
[1mStep[0m  [2/21], [94mLoss[0m : 4.45327
[1mStep[0m  [4/21], [94mLoss[0m : 4.32061
[1mStep[0m  [6/21], [94mLoss[0m : 4.16354
[1mStep[0m  [8/21], [94mLoss[0m : 4.28482
[1mStep[0m  [10/21], [94mLoss[0m : 4.14302
[1mStep[0m  [12/21], [94mLoss[0m : 4.05282
[1mStep[0m  [14/21], [94mLoss[0m : 4.09594
[1mStep[0m  [16/21], [94mLoss[0m : 4.28925
[1mStep[0m  [18/21], [94mLoss[0m : 4.33613
[1mStep[0m  [20/21], [94mLoss[0m : 4.11409

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 4.248, [92mTest[0m: 4.916, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.40488
[1mStep[0m  [2/21], [94mLoss[0m : 4.21664
[1mStep[0m  [4/21], [94mLoss[0m : 4.22419
[1mStep[0m  [6/21], [94mLoss[0m : 4.05394
[1mStep[0m  [8/21], [94mLoss[0m : 4.02425
[1mStep[0m  [10/21], [94mLoss[0m : 4.13543
[1mStep[0m  [12/21], [94mLoss[0m : 4.08896
[1mStep[0m  [14/21], [94mLoss[0m : 3.94923
[1mStep[0m  [16/21], [94mLoss[0m : 3.79489
[1mStep[0m  [18/21], [94mLoss[0m : 4.00070
[1mStep[0m  [20/21], [94mLoss[0m : 4.05358

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 4.067, [92mTest[0m: 4.690, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.97109
[1mStep[0m  [2/21], [94mLoss[0m : 4.00178
[1mStep[0m  [4/21], [94mLoss[0m : 4.17421
[1mStep[0m  [6/21], [94mLoss[0m : 3.98223
[1mStep[0m  [8/21], [94mLoss[0m : 4.00400
[1mStep[0m  [10/21], [94mLoss[0m : 4.11104
[1mStep[0m  [12/21], [94mLoss[0m : 3.96387
[1mStep[0m  [14/21], [94mLoss[0m : 3.82432
[1mStep[0m  [16/21], [94mLoss[0m : 4.00486
[1mStep[0m  [18/21], [94mLoss[0m : 3.80215
[1mStep[0m  [20/21], [94mLoss[0m : 3.62319

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 3.918, [92mTest[0m: 4.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.07664
[1mStep[0m  [2/21], [94mLoss[0m : 3.63449
[1mStep[0m  [4/21], [94mLoss[0m : 3.52420
[1mStep[0m  [6/21], [94mLoss[0m : 3.52319
[1mStep[0m  [8/21], [94mLoss[0m : 3.91191
[1mStep[0m  [10/21], [94mLoss[0m : 3.65430
[1mStep[0m  [12/21], [94mLoss[0m : 3.58560
[1mStep[0m  [14/21], [94mLoss[0m : 3.92413
[1mStep[0m  [16/21], [94mLoss[0m : 3.80354
[1mStep[0m  [18/21], [94mLoss[0m : 3.80324
[1mStep[0m  [20/21], [94mLoss[0m : 3.95600

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 3.738, [92mTest[0m: 4.238, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.55301
[1mStep[0m  [2/21], [94mLoss[0m : 3.68393
[1mStep[0m  [4/21], [94mLoss[0m : 3.55500
[1mStep[0m  [6/21], [94mLoss[0m : 3.38923
[1mStep[0m  [8/21], [94mLoss[0m : 3.52648
[1mStep[0m  [10/21], [94mLoss[0m : 3.62625
[1mStep[0m  [12/21], [94mLoss[0m : 3.54396
[1mStep[0m  [14/21], [94mLoss[0m : 3.60063
[1mStep[0m  [16/21], [94mLoss[0m : 3.68559
[1mStep[0m  [18/21], [94mLoss[0m : 3.74349
[1mStep[0m  [20/21], [94mLoss[0m : 3.59614

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 3.614, [92mTest[0m: 4.069, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.65269
[1mStep[0m  [2/21], [94mLoss[0m : 3.51585
[1mStep[0m  [4/21], [94mLoss[0m : 3.36172
[1mStep[0m  [6/21], [94mLoss[0m : 3.62670
[1mStep[0m  [8/21], [94mLoss[0m : 3.58921
[1mStep[0m  [10/21], [94mLoss[0m : 3.34294
[1mStep[0m  [12/21], [94mLoss[0m : 3.31320
[1mStep[0m  [14/21], [94mLoss[0m : 3.45325
[1mStep[0m  [16/21], [94mLoss[0m : 3.38625
[1mStep[0m  [18/21], [94mLoss[0m : 3.35274
[1mStep[0m  [20/21], [94mLoss[0m : 3.63630

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 3.497, [92mTest[0m: 3.915, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.53291
[1mStep[0m  [2/21], [94mLoss[0m : 3.51428
[1mStep[0m  [4/21], [94mLoss[0m : 3.42791
[1mStep[0m  [6/21], [94mLoss[0m : 3.52297
[1mStep[0m  [8/21], [94mLoss[0m : 3.44204
[1mStep[0m  [10/21], [94mLoss[0m : 3.24578
[1mStep[0m  [12/21], [94mLoss[0m : 3.51843
[1mStep[0m  [14/21], [94mLoss[0m : 3.45241
[1mStep[0m  [16/21], [94mLoss[0m : 3.26590
[1mStep[0m  [18/21], [94mLoss[0m : 3.23711
[1mStep[0m  [20/21], [94mLoss[0m : 3.28465

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 3.385, [92mTest[0m: 3.779, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.20430
[1mStep[0m  [2/21], [94mLoss[0m : 3.43068
[1mStep[0m  [4/21], [94mLoss[0m : 3.36575
[1mStep[0m  [6/21], [94mLoss[0m : 3.49014
[1mStep[0m  [8/21], [94mLoss[0m : 3.22365
[1mStep[0m  [10/21], [94mLoss[0m : 3.40537
[1mStep[0m  [12/21], [94mLoss[0m : 3.15561
[1mStep[0m  [14/21], [94mLoss[0m : 3.37421
[1mStep[0m  [16/21], [94mLoss[0m : 3.19705
[1mStep[0m  [18/21], [94mLoss[0m : 3.10659
[1mStep[0m  [20/21], [94mLoss[0m : 3.29380

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 3.272, [92mTest[0m: 3.614, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 3.526
====================================

Phase 1 - Evaluation MAE:  3.525876658303397
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 3.24304
[1mStep[0m  [2/21], [94mLoss[0m : 3.26101
[1mStep[0m  [4/21], [94mLoss[0m : 3.03978
[1mStep[0m  [6/21], [94mLoss[0m : 3.22355
[1mStep[0m  [8/21], [94mLoss[0m : 3.32892
[1mStep[0m  [10/21], [94mLoss[0m : 3.08898
[1mStep[0m  [12/21], [94mLoss[0m : 3.14270
[1mStep[0m  [14/21], [94mLoss[0m : 3.44257
[1mStep[0m  [16/21], [94mLoss[0m : 3.23423
[1mStep[0m  [18/21], [94mLoss[0m : 3.15928
[1mStep[0m  [20/21], [94mLoss[0m : 3.27868

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.216, [92mTest[0m: 3.523, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.14438
[1mStep[0m  [2/21], [94mLoss[0m : 3.15923
[1mStep[0m  [4/21], [94mLoss[0m : 3.23544
[1mStep[0m  [6/21], [94mLoss[0m : 3.01099
[1mStep[0m  [8/21], [94mLoss[0m : 3.06748
[1mStep[0m  [10/21], [94mLoss[0m : 3.08159
[1mStep[0m  [12/21], [94mLoss[0m : 2.95650
[1mStep[0m  [14/21], [94mLoss[0m : 3.05258
[1mStep[0m  [16/21], [94mLoss[0m : 3.05155
[1mStep[0m  [18/21], [94mLoss[0m : 2.83633
[1mStep[0m  [20/21], [94mLoss[0m : 3.04072

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.097, [92mTest[0m: 3.603, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.10410
[1mStep[0m  [2/21], [94mLoss[0m : 2.97454
[1mStep[0m  [4/21], [94mLoss[0m : 3.12678
[1mStep[0m  [6/21], [94mLoss[0m : 3.01728
[1mStep[0m  [8/21], [94mLoss[0m : 3.10832
[1mStep[0m  [10/21], [94mLoss[0m : 2.98746
[1mStep[0m  [12/21], [94mLoss[0m : 2.79168
[1mStep[0m  [14/21], [94mLoss[0m : 2.90515
[1mStep[0m  [16/21], [94mLoss[0m : 2.68446
[1mStep[0m  [18/21], [94mLoss[0m : 2.89283
[1mStep[0m  [20/21], [94mLoss[0m : 3.17128

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.019, [92mTest[0m: 3.357, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.87792
[1mStep[0m  [2/21], [94mLoss[0m : 2.79168
[1mStep[0m  [4/21], [94mLoss[0m : 3.03611
[1mStep[0m  [6/21], [94mLoss[0m : 2.95086
[1mStep[0m  [8/21], [94mLoss[0m : 3.20758
[1mStep[0m  [10/21], [94mLoss[0m : 2.95452
[1mStep[0m  [12/21], [94mLoss[0m : 2.98603
[1mStep[0m  [14/21], [94mLoss[0m : 2.72925
[1mStep[0m  [16/21], [94mLoss[0m : 3.00586
[1mStep[0m  [18/21], [94mLoss[0m : 2.86248
[1mStep[0m  [20/21], [94mLoss[0m : 2.94104

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.947, [92mTest[0m: 3.125, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.95442
[1mStep[0m  [2/21], [94mLoss[0m : 2.91736
[1mStep[0m  [4/21], [94mLoss[0m : 2.87961
[1mStep[0m  [6/21], [94mLoss[0m : 3.00393
[1mStep[0m  [8/21], [94mLoss[0m : 2.79579
[1mStep[0m  [10/21], [94mLoss[0m : 2.77203
[1mStep[0m  [12/21], [94mLoss[0m : 2.89240
[1mStep[0m  [14/21], [94mLoss[0m : 2.86858
[1mStep[0m  [16/21], [94mLoss[0m : 2.85113
[1mStep[0m  [18/21], [94mLoss[0m : 3.07205
[1mStep[0m  [20/21], [94mLoss[0m : 2.85706

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.894, [92mTest[0m: 2.913, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.91988
[1mStep[0m  [2/21], [94mLoss[0m : 2.76277
[1mStep[0m  [4/21], [94mLoss[0m : 2.92718
[1mStep[0m  [6/21], [94mLoss[0m : 2.91656
[1mStep[0m  [8/21], [94mLoss[0m : 3.00622
[1mStep[0m  [10/21], [94mLoss[0m : 2.75984
[1mStep[0m  [12/21], [94mLoss[0m : 2.94476
[1mStep[0m  [14/21], [94mLoss[0m : 2.77622
[1mStep[0m  [16/21], [94mLoss[0m : 2.75748
[1mStep[0m  [18/21], [94mLoss[0m : 2.86105
[1mStep[0m  [20/21], [94mLoss[0m : 2.83826

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.862, [92mTest[0m: 2.745, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.80911
[1mStep[0m  [2/21], [94mLoss[0m : 2.79642
[1mStep[0m  [4/21], [94mLoss[0m : 2.80958
[1mStep[0m  [6/21], [94mLoss[0m : 2.96604
[1mStep[0m  [8/21], [94mLoss[0m : 2.66033
[1mStep[0m  [10/21], [94mLoss[0m : 2.74953
[1mStep[0m  [12/21], [94mLoss[0m : 2.82667
[1mStep[0m  [14/21], [94mLoss[0m : 2.76566
[1mStep[0m  [16/21], [94mLoss[0m : 2.63627
[1mStep[0m  [18/21], [94mLoss[0m : 2.78609
[1mStep[0m  [20/21], [94mLoss[0m : 2.70663

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.798, [92mTest[0m: 2.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.82227
[1mStep[0m  [2/21], [94mLoss[0m : 2.73674
[1mStep[0m  [4/21], [94mLoss[0m : 2.83807
[1mStep[0m  [6/21], [94mLoss[0m : 2.87835
[1mStep[0m  [8/21], [94mLoss[0m : 2.74283
[1mStep[0m  [10/21], [94mLoss[0m : 2.90614
[1mStep[0m  [12/21], [94mLoss[0m : 2.63408
[1mStep[0m  [14/21], [94mLoss[0m : 2.73097
[1mStep[0m  [16/21], [94mLoss[0m : 2.73740
[1mStep[0m  [18/21], [94mLoss[0m : 3.04685
[1mStep[0m  [20/21], [94mLoss[0m : 2.68926

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.775, [92mTest[0m: 2.607, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74240
[1mStep[0m  [2/21], [94mLoss[0m : 2.96537
[1mStep[0m  [4/21], [94mLoss[0m : 2.67496
[1mStep[0m  [6/21], [94mLoss[0m : 2.99455
[1mStep[0m  [8/21], [94mLoss[0m : 2.74040
[1mStep[0m  [10/21], [94mLoss[0m : 2.82157
[1mStep[0m  [12/21], [94mLoss[0m : 2.60168
[1mStep[0m  [14/21], [94mLoss[0m : 2.86503
[1mStep[0m  [16/21], [94mLoss[0m : 2.63136
[1mStep[0m  [18/21], [94mLoss[0m : 2.63318
[1mStep[0m  [20/21], [94mLoss[0m : 2.85984

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.738, [92mTest[0m: 2.581, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.74158
[1mStep[0m  [2/21], [94mLoss[0m : 2.75051
[1mStep[0m  [4/21], [94mLoss[0m : 2.74036
[1mStep[0m  [6/21], [94mLoss[0m : 2.68377
[1mStep[0m  [8/21], [94mLoss[0m : 2.68614
[1mStep[0m  [10/21], [94mLoss[0m : 2.61842
[1mStep[0m  [12/21], [94mLoss[0m : 2.67985
[1mStep[0m  [14/21], [94mLoss[0m : 2.79989
[1mStep[0m  [16/21], [94mLoss[0m : 2.57226
[1mStep[0m  [18/21], [94mLoss[0m : 2.69624
[1mStep[0m  [20/21], [94mLoss[0m : 2.78718

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.711, [92mTest[0m: 2.590, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81237
[1mStep[0m  [2/21], [94mLoss[0m : 2.60200
[1mStep[0m  [4/21], [94mLoss[0m : 2.86328
[1mStep[0m  [6/21], [94mLoss[0m : 2.52979
[1mStep[0m  [8/21], [94mLoss[0m : 2.73293
[1mStep[0m  [10/21], [94mLoss[0m : 2.76685
[1mStep[0m  [12/21], [94mLoss[0m : 2.68095
[1mStep[0m  [14/21], [94mLoss[0m : 2.66447
[1mStep[0m  [16/21], [94mLoss[0m : 2.74326
[1mStep[0m  [18/21], [94mLoss[0m : 2.58402
[1mStep[0m  [20/21], [94mLoss[0m : 2.55917

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.704, [92mTest[0m: 2.551, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79990
[1mStep[0m  [2/21], [94mLoss[0m : 2.71229
[1mStep[0m  [4/21], [94mLoss[0m : 2.69555
[1mStep[0m  [6/21], [94mLoss[0m : 2.64261
[1mStep[0m  [8/21], [94mLoss[0m : 2.77910
[1mStep[0m  [10/21], [94mLoss[0m : 2.68338
[1mStep[0m  [12/21], [94mLoss[0m : 2.82966
[1mStep[0m  [14/21], [94mLoss[0m : 2.90466
[1mStep[0m  [16/21], [94mLoss[0m : 2.72439
[1mStep[0m  [18/21], [94mLoss[0m : 2.65361
[1mStep[0m  [20/21], [94mLoss[0m : 2.76856

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.587, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.67735
[1mStep[0m  [2/21], [94mLoss[0m : 2.65672
[1mStep[0m  [4/21], [94mLoss[0m : 2.69824
[1mStep[0m  [6/21], [94mLoss[0m : 2.66668
[1mStep[0m  [8/21], [94mLoss[0m : 2.79398
[1mStep[0m  [10/21], [94mLoss[0m : 2.66854
[1mStep[0m  [12/21], [94mLoss[0m : 2.64669
[1mStep[0m  [14/21], [94mLoss[0m : 2.82738
[1mStep[0m  [16/21], [94mLoss[0m : 2.67515
[1mStep[0m  [18/21], [94mLoss[0m : 2.73297
[1mStep[0m  [20/21], [94mLoss[0m : 2.57296

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.560, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.83302
[1mStep[0m  [2/21], [94mLoss[0m : 2.56687
[1mStep[0m  [4/21], [94mLoss[0m : 2.68757
[1mStep[0m  [6/21], [94mLoss[0m : 2.64484
[1mStep[0m  [8/21], [94mLoss[0m : 2.41392
[1mStep[0m  [10/21], [94mLoss[0m : 2.48068
[1mStep[0m  [12/21], [94mLoss[0m : 2.54714
[1mStep[0m  [14/21], [94mLoss[0m : 2.65362
[1mStep[0m  [16/21], [94mLoss[0m : 2.80959
[1mStep[0m  [18/21], [94mLoss[0m : 2.63549
[1mStep[0m  [20/21], [94mLoss[0m : 2.84849

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.600, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65259
[1mStep[0m  [2/21], [94mLoss[0m : 2.73988
[1mStep[0m  [4/21], [94mLoss[0m : 2.51916
[1mStep[0m  [6/21], [94mLoss[0m : 2.77257
[1mStep[0m  [8/21], [94mLoss[0m : 2.48777
[1mStep[0m  [10/21], [94mLoss[0m : 2.78841
[1mStep[0m  [12/21], [94mLoss[0m : 2.63963
[1mStep[0m  [14/21], [94mLoss[0m : 2.69544
[1mStep[0m  [16/21], [94mLoss[0m : 2.63455
[1mStep[0m  [18/21], [94mLoss[0m : 2.58284
[1mStep[0m  [20/21], [94mLoss[0m : 2.52120

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.606, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.60889
[1mStep[0m  [2/21], [94mLoss[0m : 2.63535
[1mStep[0m  [4/21], [94mLoss[0m : 2.75283
[1mStep[0m  [6/21], [94mLoss[0m : 2.45847
[1mStep[0m  [8/21], [94mLoss[0m : 2.49820
[1mStep[0m  [10/21], [94mLoss[0m : 2.63707
[1mStep[0m  [12/21], [94mLoss[0m : 2.65184
[1mStep[0m  [14/21], [94mLoss[0m : 2.66183
[1mStep[0m  [16/21], [94mLoss[0m : 2.45797
[1mStep[0m  [18/21], [94mLoss[0m : 2.53819
[1mStep[0m  [20/21], [94mLoss[0m : 2.51206

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.589, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.66298
[1mStep[0m  [2/21], [94mLoss[0m : 2.41838
[1mStep[0m  [4/21], [94mLoss[0m : 2.64363
[1mStep[0m  [6/21], [94mLoss[0m : 2.78102
[1mStep[0m  [8/21], [94mLoss[0m : 2.53127
[1mStep[0m  [10/21], [94mLoss[0m : 2.76486
[1mStep[0m  [12/21], [94mLoss[0m : 2.55131
[1mStep[0m  [14/21], [94mLoss[0m : 2.73788
[1mStep[0m  [16/21], [94mLoss[0m : 2.62326
[1mStep[0m  [18/21], [94mLoss[0m : 2.58554
[1mStep[0m  [20/21], [94mLoss[0m : 2.64074

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.611, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.58867
[1mStep[0m  [2/21], [94mLoss[0m : 2.53066
[1mStep[0m  [4/21], [94mLoss[0m : 2.62484
[1mStep[0m  [6/21], [94mLoss[0m : 2.66775
[1mStep[0m  [8/21], [94mLoss[0m : 2.65210
[1mStep[0m  [10/21], [94mLoss[0m : 2.65452
[1mStep[0m  [12/21], [94mLoss[0m : 2.61953
[1mStep[0m  [14/21], [94mLoss[0m : 2.63331
[1mStep[0m  [16/21], [94mLoss[0m : 2.70388
[1mStep[0m  [18/21], [94mLoss[0m : 2.59755
[1mStep[0m  [20/21], [94mLoss[0m : 2.52968

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.614, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.76261
[1mStep[0m  [2/21], [94mLoss[0m : 2.68562
[1mStep[0m  [4/21], [94mLoss[0m : 2.68805
[1mStep[0m  [6/21], [94mLoss[0m : 2.65341
[1mStep[0m  [8/21], [94mLoss[0m : 2.59674
[1mStep[0m  [10/21], [94mLoss[0m : 2.64111
[1mStep[0m  [12/21], [94mLoss[0m : 2.60862
[1mStep[0m  [14/21], [94mLoss[0m : 2.72655
[1mStep[0m  [16/21], [94mLoss[0m : 2.72615
[1mStep[0m  [18/21], [94mLoss[0m : 2.60443
[1mStep[0m  [20/21], [94mLoss[0m : 2.70876

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.569, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.65607
[1mStep[0m  [2/21], [94mLoss[0m : 2.53433
[1mStep[0m  [4/21], [94mLoss[0m : 2.56455
[1mStep[0m  [6/21], [94mLoss[0m : 2.44791
[1mStep[0m  [8/21], [94mLoss[0m : 2.78084
[1mStep[0m  [10/21], [94mLoss[0m : 2.58969
[1mStep[0m  [12/21], [94mLoss[0m : 2.64336
[1mStep[0m  [14/21], [94mLoss[0m : 2.55635
[1mStep[0m  [16/21], [94mLoss[0m : 2.55753
[1mStep[0m  [18/21], [94mLoss[0m : 2.57259
[1mStep[0m  [20/21], [94mLoss[0m : 2.62076

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.598, [92mTest[0m: 2.560, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62047
[1mStep[0m  [2/21], [94mLoss[0m : 2.49832
[1mStep[0m  [4/21], [94mLoss[0m : 2.52399
[1mStep[0m  [6/21], [94mLoss[0m : 2.54634
[1mStep[0m  [8/21], [94mLoss[0m : 2.59026
[1mStep[0m  [10/21], [94mLoss[0m : 2.51934
[1mStep[0m  [12/21], [94mLoss[0m : 2.53369
[1mStep[0m  [14/21], [94mLoss[0m : 2.67636
[1mStep[0m  [16/21], [94mLoss[0m : 2.63275
[1mStep[0m  [18/21], [94mLoss[0m : 2.44251
[1mStep[0m  [20/21], [94mLoss[0m : 2.53560

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.584, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39123
[1mStep[0m  [2/21], [94mLoss[0m : 2.53466
[1mStep[0m  [4/21], [94mLoss[0m : 2.42395
[1mStep[0m  [6/21], [94mLoss[0m : 2.59113
[1mStep[0m  [8/21], [94mLoss[0m : 2.80091
[1mStep[0m  [10/21], [94mLoss[0m : 2.61272
[1mStep[0m  [12/21], [94mLoss[0m : 2.62115
[1mStep[0m  [14/21], [94mLoss[0m : 2.63825
[1mStep[0m  [16/21], [94mLoss[0m : 2.61738
[1mStep[0m  [18/21], [94mLoss[0m : 2.54962
[1mStep[0m  [20/21], [94mLoss[0m : 2.52416

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.658, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.61495
[1mStep[0m  [2/21], [94mLoss[0m : 2.58565
[1mStep[0m  [4/21], [94mLoss[0m : 2.56899
[1mStep[0m  [6/21], [94mLoss[0m : 2.60126
[1mStep[0m  [8/21], [94mLoss[0m : 2.38700
[1mStep[0m  [10/21], [94mLoss[0m : 2.54379
[1mStep[0m  [12/21], [94mLoss[0m : 2.73229
[1mStep[0m  [14/21], [94mLoss[0m : 2.61713
[1mStep[0m  [16/21], [94mLoss[0m : 2.62305
[1mStep[0m  [18/21], [94mLoss[0m : 2.55565
[1mStep[0m  [20/21], [94mLoss[0m : 2.51031

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.599, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.42217
[1mStep[0m  [2/21], [94mLoss[0m : 2.59258
[1mStep[0m  [4/21], [94mLoss[0m : 2.61970
[1mStep[0m  [6/21], [94mLoss[0m : 2.64136
[1mStep[0m  [8/21], [94mLoss[0m : 2.71928
[1mStep[0m  [10/21], [94mLoss[0m : 2.57260
[1mStep[0m  [12/21], [94mLoss[0m : 2.58243
[1mStep[0m  [14/21], [94mLoss[0m : 2.57578
[1mStep[0m  [16/21], [94mLoss[0m : 2.59560
[1mStep[0m  [18/21], [94mLoss[0m : 2.50871
[1mStep[0m  [20/21], [94mLoss[0m : 2.49072

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.575, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54545
[1mStep[0m  [2/21], [94mLoss[0m : 2.44374
[1mStep[0m  [4/21], [94mLoss[0m : 2.61293
[1mStep[0m  [6/21], [94mLoss[0m : 2.60211
[1mStep[0m  [8/21], [94mLoss[0m : 2.49406
[1mStep[0m  [10/21], [94mLoss[0m : 2.50218
[1mStep[0m  [12/21], [94mLoss[0m : 2.48710
[1mStep[0m  [14/21], [94mLoss[0m : 2.65021
[1mStep[0m  [16/21], [94mLoss[0m : 2.49112
[1mStep[0m  [18/21], [94mLoss[0m : 2.53775
[1mStep[0m  [20/21], [94mLoss[0m : 2.46308

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.564, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54124
[1mStep[0m  [2/21], [94mLoss[0m : 2.58039
[1mStep[0m  [4/21], [94mLoss[0m : 2.41223
[1mStep[0m  [6/21], [94mLoss[0m : 2.47153
[1mStep[0m  [8/21], [94mLoss[0m : 2.53630
[1mStep[0m  [10/21], [94mLoss[0m : 2.65195
[1mStep[0m  [12/21], [94mLoss[0m : 2.48693
[1mStep[0m  [14/21], [94mLoss[0m : 2.61223
[1mStep[0m  [16/21], [94mLoss[0m : 2.47989
[1mStep[0m  [18/21], [94mLoss[0m : 2.47177
[1mStep[0m  [20/21], [94mLoss[0m : 2.46246

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.586, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.68017
[1mStep[0m  [2/21], [94mLoss[0m : 2.39238
[1mStep[0m  [4/21], [94mLoss[0m : 2.46118
[1mStep[0m  [6/21], [94mLoss[0m : 2.46800
[1mStep[0m  [8/21], [94mLoss[0m : 2.60877
[1mStep[0m  [10/21], [94mLoss[0m : 2.78812
[1mStep[0m  [12/21], [94mLoss[0m : 2.50412
[1mStep[0m  [14/21], [94mLoss[0m : 2.55467
[1mStep[0m  [16/21], [94mLoss[0m : 2.50880
[1mStep[0m  [18/21], [94mLoss[0m : 2.35615
[1mStep[0m  [20/21], [94mLoss[0m : 2.66259

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.567, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45746
[1mStep[0m  [2/21], [94mLoss[0m : 2.58850
[1mStep[0m  [4/21], [94mLoss[0m : 2.46297
[1mStep[0m  [6/21], [94mLoss[0m : 2.65492
[1mStep[0m  [8/21], [94mLoss[0m : 2.52183
[1mStep[0m  [10/21], [94mLoss[0m : 2.62444
[1mStep[0m  [12/21], [94mLoss[0m : 2.47298
[1mStep[0m  [14/21], [94mLoss[0m : 2.52855
[1mStep[0m  [16/21], [94mLoss[0m : 2.47479
[1mStep[0m  [18/21], [94mLoss[0m : 2.44056
[1mStep[0m  [20/21], [94mLoss[0m : 2.70402

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.554, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.50284
[1mStep[0m  [2/21], [94mLoss[0m : 2.59041
[1mStep[0m  [4/21], [94mLoss[0m : 2.36561
[1mStep[0m  [6/21], [94mLoss[0m : 2.53180
[1mStep[0m  [8/21], [94mLoss[0m : 2.62295
[1mStep[0m  [10/21], [94mLoss[0m : 2.84538
[1mStep[0m  [12/21], [94mLoss[0m : 2.49672
[1mStep[0m  [14/21], [94mLoss[0m : 2.62681
[1mStep[0m  [16/21], [94mLoss[0m : 2.61504
[1mStep[0m  [18/21], [94mLoss[0m : 2.50368
[1mStep[0m  [20/21], [94mLoss[0m : 2.50416

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.547, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.64094
[1mStep[0m  [2/21], [94mLoss[0m : 2.39672
[1mStep[0m  [4/21], [94mLoss[0m : 2.43086
[1mStep[0m  [6/21], [94mLoss[0m : 2.58995
[1mStep[0m  [8/21], [94mLoss[0m : 2.63432
[1mStep[0m  [10/21], [94mLoss[0m : 2.53933
[1mStep[0m  [12/21], [94mLoss[0m : 2.55038
[1mStep[0m  [14/21], [94mLoss[0m : 2.66882
[1mStep[0m  [16/21], [94mLoss[0m : 2.56011
[1mStep[0m  [18/21], [94mLoss[0m : 2.53581
[1mStep[0m  [20/21], [94mLoss[0m : 2.64373

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.485, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.560
====================================

Phase 2 - Evaluation MAE:  2.559526171003069
MAE score P1      3.525877
MAE score P2      2.559526
loss              2.542727
learning_rate       0.0001
batch_size             512
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 21, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.85477
[1mStep[0m  [2/21], [94mLoss[0m : 10.79800
[1mStep[0m  [4/21], [94mLoss[0m : 11.07094
[1mStep[0m  [6/21], [94mLoss[0m : 10.84289
[1mStep[0m  [8/21], [94mLoss[0m : 10.82418
[1mStep[0m  [10/21], [94mLoss[0m : 11.19837
[1mStep[0m  [12/21], [94mLoss[0m : 10.67592
[1mStep[0m  [14/21], [94mLoss[0m : 11.04652
[1mStep[0m  [16/21], [94mLoss[0m : 10.90193
[1mStep[0m  [18/21], [94mLoss[0m : 10.72558
[1mStep[0m  [20/21], [94mLoss[0m : 10.88968

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.917, [92mTest[0m: 10.983, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.80842
[1mStep[0m  [2/21], [94mLoss[0m : 10.96868
[1mStep[0m  [4/21], [94mLoss[0m : 10.88305
[1mStep[0m  [6/21], [94mLoss[0m : 10.91596
[1mStep[0m  [8/21], [94mLoss[0m : 10.86523
[1mStep[0m  [10/21], [94mLoss[0m : 10.82472
[1mStep[0m  [12/21], [94mLoss[0m : 11.17810
[1mStep[0m  [14/21], [94mLoss[0m : 11.06360
[1mStep[0m  [16/21], [94mLoss[0m : 10.68899
[1mStep[0m  [18/21], [94mLoss[0m : 10.61488
[1mStep[0m  [20/21], [94mLoss[0m : 10.91730

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.916, [92mTest[0m: 10.916, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.57308
[1mStep[0m  [2/21], [94mLoss[0m : 10.96867
[1mStep[0m  [4/21], [94mLoss[0m : 10.81129
[1mStep[0m  [6/21], [94mLoss[0m : 10.78530
[1mStep[0m  [8/21], [94mLoss[0m : 10.89890
[1mStep[0m  [10/21], [94mLoss[0m : 10.91901
[1mStep[0m  [12/21], [94mLoss[0m : 11.05565
[1mStep[0m  [14/21], [94mLoss[0m : 10.87366
[1mStep[0m  [16/21], [94mLoss[0m : 10.98296
[1mStep[0m  [18/21], [94mLoss[0m : 10.85822
[1mStep[0m  [20/21], [94mLoss[0m : 10.93875

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.907, [92mTest[0m: 10.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.21562
[1mStep[0m  [2/21], [94mLoss[0m : 11.01188
[1mStep[0m  [4/21], [94mLoss[0m : 11.02985
[1mStep[0m  [6/21], [94mLoss[0m : 10.86928
[1mStep[0m  [8/21], [94mLoss[0m : 10.92364
[1mStep[0m  [10/21], [94mLoss[0m : 10.73065
[1mStep[0m  [12/21], [94mLoss[0m : 10.93472
[1mStep[0m  [14/21], [94mLoss[0m : 10.79417
[1mStep[0m  [16/21], [94mLoss[0m : 10.91748
[1mStep[0m  [18/21], [94mLoss[0m : 10.73330
[1mStep[0m  [20/21], [94mLoss[0m : 10.78189

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.895, [92mTest[0m: 10.881, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.95331
[1mStep[0m  [2/21], [94mLoss[0m : 10.75362
[1mStep[0m  [4/21], [94mLoss[0m : 11.11733
[1mStep[0m  [6/21], [94mLoss[0m : 10.88760
[1mStep[0m  [8/21], [94mLoss[0m : 11.08059
[1mStep[0m  [10/21], [94mLoss[0m : 10.88836
[1mStep[0m  [12/21], [94mLoss[0m : 10.65175
[1mStep[0m  [14/21], [94mLoss[0m : 10.79763
[1mStep[0m  [16/21], [94mLoss[0m : 10.89240
[1mStep[0m  [18/21], [94mLoss[0m : 10.96197
[1mStep[0m  [20/21], [94mLoss[0m : 10.90063

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.897, [92mTest[0m: 10.875, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.80841
[1mStep[0m  [2/21], [94mLoss[0m : 10.74107
[1mStep[0m  [4/21], [94mLoss[0m : 10.95305
[1mStep[0m  [6/21], [94mLoss[0m : 10.79064
[1mStep[0m  [8/21], [94mLoss[0m : 10.81157
[1mStep[0m  [10/21], [94mLoss[0m : 10.91334
[1mStep[0m  [12/21], [94mLoss[0m : 10.61182
[1mStep[0m  [14/21], [94mLoss[0m : 11.21734
[1mStep[0m  [16/21], [94mLoss[0m : 10.82805
[1mStep[0m  [18/21], [94mLoss[0m : 10.78926
[1mStep[0m  [20/21], [94mLoss[0m : 10.70455

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.890, [92mTest[0m: 10.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.03498
[1mStep[0m  [2/21], [94mLoss[0m : 11.11969
[1mStep[0m  [4/21], [94mLoss[0m : 10.72408
[1mStep[0m  [6/21], [94mLoss[0m : 10.78848
[1mStep[0m  [8/21], [94mLoss[0m : 10.93176
[1mStep[0m  [10/21], [94mLoss[0m : 10.88080
[1mStep[0m  [12/21], [94mLoss[0m : 10.86359
[1mStep[0m  [14/21], [94mLoss[0m : 10.70081
[1mStep[0m  [16/21], [94mLoss[0m : 10.78360
[1mStep[0m  [18/21], [94mLoss[0m : 10.85330
[1mStep[0m  [20/21], [94mLoss[0m : 11.03510

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.885, [92mTest[0m: 10.870, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79209
[1mStep[0m  [2/21], [94mLoss[0m : 11.06273
[1mStep[0m  [4/21], [94mLoss[0m : 10.80062
[1mStep[0m  [6/21], [94mLoss[0m : 10.79718
[1mStep[0m  [8/21], [94mLoss[0m : 10.65504
[1mStep[0m  [10/21], [94mLoss[0m : 10.96325
[1mStep[0m  [12/21], [94mLoss[0m : 11.22540
[1mStep[0m  [14/21], [94mLoss[0m : 10.85282
[1mStep[0m  [16/21], [94mLoss[0m : 10.72265
[1mStep[0m  [18/21], [94mLoss[0m : 10.69708
[1mStep[0m  [20/21], [94mLoss[0m : 11.06068

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.879, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79319
[1mStep[0m  [2/21], [94mLoss[0m : 10.95897
[1mStep[0m  [4/21], [94mLoss[0m : 10.88087
[1mStep[0m  [6/21], [94mLoss[0m : 11.01012
[1mStep[0m  [8/21], [94mLoss[0m : 10.76458
[1mStep[0m  [10/21], [94mLoss[0m : 10.83461
[1mStep[0m  [12/21], [94mLoss[0m : 10.98600
[1mStep[0m  [14/21], [94mLoss[0m : 10.97961
[1mStep[0m  [16/21], [94mLoss[0m : 10.71485
[1mStep[0m  [18/21], [94mLoss[0m : 10.84695
[1mStep[0m  [20/21], [94mLoss[0m : 10.74576

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.868, [92mTest[0m: 10.842, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78522
[1mStep[0m  [2/21], [94mLoss[0m : 10.60246
[1mStep[0m  [4/21], [94mLoss[0m : 11.02198
[1mStep[0m  [6/21], [94mLoss[0m : 10.90987
[1mStep[0m  [8/21], [94mLoss[0m : 11.13652
[1mStep[0m  [10/21], [94mLoss[0m : 10.71998
[1mStep[0m  [12/21], [94mLoss[0m : 11.15671
[1mStep[0m  [14/21], [94mLoss[0m : 10.84465
[1mStep[0m  [16/21], [94mLoss[0m : 10.92866
[1mStep[0m  [18/21], [94mLoss[0m : 10.80017
[1mStep[0m  [20/21], [94mLoss[0m : 10.90763

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.869, [92mTest[0m: 10.843, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.84820
[1mStep[0m  [2/21], [94mLoss[0m : 11.00166
[1mStep[0m  [4/21], [94mLoss[0m : 10.92624
[1mStep[0m  [6/21], [94mLoss[0m : 10.78678
[1mStep[0m  [8/21], [94mLoss[0m : 10.98346
[1mStep[0m  [10/21], [94mLoss[0m : 10.85190
[1mStep[0m  [12/21], [94mLoss[0m : 10.69973
[1mStep[0m  [14/21], [94mLoss[0m : 10.74784
[1mStep[0m  [16/21], [94mLoss[0m : 10.73967
[1mStep[0m  [18/21], [94mLoss[0m : 10.74816
[1mStep[0m  [20/21], [94mLoss[0m : 10.87734

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.861, [92mTest[0m: 10.822, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96930
[1mStep[0m  [2/21], [94mLoss[0m : 10.88308
[1mStep[0m  [4/21], [94mLoss[0m : 11.30030
[1mStep[0m  [6/21], [94mLoss[0m : 10.82570
[1mStep[0m  [8/21], [94mLoss[0m : 10.82709
[1mStep[0m  [10/21], [94mLoss[0m : 10.90911
[1mStep[0m  [12/21], [94mLoss[0m : 10.98226
[1mStep[0m  [14/21], [94mLoss[0m : 10.92559
[1mStep[0m  [16/21], [94mLoss[0m : 10.77043
[1mStep[0m  [18/21], [94mLoss[0m : 10.73749
[1mStep[0m  [20/21], [94mLoss[0m : 10.64458

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.852, [92mTest[0m: 10.818, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75121
[1mStep[0m  [2/21], [94mLoss[0m : 10.78194
[1mStep[0m  [4/21], [94mLoss[0m : 10.95135
[1mStep[0m  [6/21], [94mLoss[0m : 11.02260
[1mStep[0m  [8/21], [94mLoss[0m : 10.86680
[1mStep[0m  [10/21], [94mLoss[0m : 10.83375
[1mStep[0m  [12/21], [94mLoss[0m : 10.80396
[1mStep[0m  [14/21], [94mLoss[0m : 10.90539
[1mStep[0m  [16/21], [94mLoss[0m : 10.71976
[1mStep[0m  [18/21], [94mLoss[0m : 10.95925
[1mStep[0m  [20/21], [94mLoss[0m : 10.85716

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.846, [92mTest[0m: 10.818, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.94429
[1mStep[0m  [2/21], [94mLoss[0m : 10.58704
[1mStep[0m  [4/21], [94mLoss[0m : 10.68105
[1mStep[0m  [6/21], [94mLoss[0m : 10.77586
[1mStep[0m  [8/21], [94mLoss[0m : 10.95468
[1mStep[0m  [10/21], [94mLoss[0m : 10.79725
[1mStep[0m  [12/21], [94mLoss[0m : 10.83880
[1mStep[0m  [14/21], [94mLoss[0m : 10.74694
[1mStep[0m  [16/21], [94mLoss[0m : 10.78053
[1mStep[0m  [18/21], [94mLoss[0m : 11.04725
[1mStep[0m  [20/21], [94mLoss[0m : 11.10529

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.842, [92mTest[0m: 10.812, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.92343
[1mStep[0m  [2/21], [94mLoss[0m : 10.79675
[1mStep[0m  [4/21], [94mLoss[0m : 10.83419
[1mStep[0m  [6/21], [94mLoss[0m : 10.75835
[1mStep[0m  [8/21], [94mLoss[0m : 10.92279
[1mStep[0m  [10/21], [94mLoss[0m : 10.95347
[1mStep[0m  [12/21], [94mLoss[0m : 10.87217
[1mStep[0m  [14/21], [94mLoss[0m : 10.89029
[1mStep[0m  [16/21], [94mLoss[0m : 10.72465
[1mStep[0m  [18/21], [94mLoss[0m : 11.04829
[1mStep[0m  [20/21], [94mLoss[0m : 10.57154

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.840, [92mTest[0m: 10.790, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90789
[1mStep[0m  [2/21], [94mLoss[0m : 10.76864
[1mStep[0m  [4/21], [94mLoss[0m : 10.87123
[1mStep[0m  [6/21], [94mLoss[0m : 10.53186
[1mStep[0m  [8/21], [94mLoss[0m : 10.89753
[1mStep[0m  [10/21], [94mLoss[0m : 10.78936
[1mStep[0m  [12/21], [94mLoss[0m : 10.62022
[1mStep[0m  [14/21], [94mLoss[0m : 10.52666
[1mStep[0m  [16/21], [94mLoss[0m : 10.95530
[1mStep[0m  [18/21], [94mLoss[0m : 10.86316
[1mStep[0m  [20/21], [94mLoss[0m : 11.16659

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.830, [92mTest[0m: 10.793, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.89396
[1mStep[0m  [2/21], [94mLoss[0m : 10.68674
[1mStep[0m  [4/21], [94mLoss[0m : 10.51311
[1mStep[0m  [6/21], [94mLoss[0m : 10.94632
[1mStep[0m  [8/21], [94mLoss[0m : 10.70318
[1mStep[0m  [10/21], [94mLoss[0m : 10.70641
[1mStep[0m  [12/21], [94mLoss[0m : 10.59283
[1mStep[0m  [14/21], [94mLoss[0m : 10.64084
[1mStep[0m  [16/21], [94mLoss[0m : 10.60635
[1mStep[0m  [18/21], [94mLoss[0m : 10.96459
[1mStep[0m  [20/21], [94mLoss[0m : 11.10204

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.817, [92mTest[0m: 10.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54200
[1mStep[0m  [2/21], [94mLoss[0m : 10.99009
[1mStep[0m  [4/21], [94mLoss[0m : 10.64039
[1mStep[0m  [6/21], [94mLoss[0m : 10.96987
[1mStep[0m  [8/21], [94mLoss[0m : 10.74015
[1mStep[0m  [10/21], [94mLoss[0m : 10.76803
[1mStep[0m  [12/21], [94mLoss[0m : 10.86749
[1mStep[0m  [14/21], [94mLoss[0m : 10.96955
[1mStep[0m  [16/21], [94mLoss[0m : 10.90030
[1mStep[0m  [18/21], [94mLoss[0m : 10.64909
[1mStep[0m  [20/21], [94mLoss[0m : 10.60109

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.818, [92mTest[0m: 10.758, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99169
[1mStep[0m  [2/21], [94mLoss[0m : 10.78606
[1mStep[0m  [4/21], [94mLoss[0m : 10.94439
[1mStep[0m  [6/21], [94mLoss[0m : 10.82655
[1mStep[0m  [8/21], [94mLoss[0m : 10.54959
[1mStep[0m  [10/21], [94mLoss[0m : 10.99046
[1mStep[0m  [12/21], [94mLoss[0m : 10.74837
[1mStep[0m  [14/21], [94mLoss[0m : 10.91896
[1mStep[0m  [16/21], [94mLoss[0m : 10.70619
[1mStep[0m  [18/21], [94mLoss[0m : 10.62435
[1mStep[0m  [20/21], [94mLoss[0m : 10.87844

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68453
[1mStep[0m  [2/21], [94mLoss[0m : 10.79691
[1mStep[0m  [4/21], [94mLoss[0m : 10.70176
[1mStep[0m  [6/21], [94mLoss[0m : 10.91220
[1mStep[0m  [8/21], [94mLoss[0m : 10.81876
[1mStep[0m  [10/21], [94mLoss[0m : 10.75560
[1mStep[0m  [12/21], [94mLoss[0m : 10.76091
[1mStep[0m  [14/21], [94mLoss[0m : 10.72438
[1mStep[0m  [16/21], [94mLoss[0m : 10.79693
[1mStep[0m  [18/21], [94mLoss[0m : 10.72827
[1mStep[0m  [20/21], [94mLoss[0m : 10.88208

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.799, [92mTest[0m: 10.761, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.83394
[1mStep[0m  [2/21], [94mLoss[0m : 10.59408
[1mStep[0m  [4/21], [94mLoss[0m : 10.85289
[1mStep[0m  [6/21], [94mLoss[0m : 10.79159
[1mStep[0m  [8/21], [94mLoss[0m : 10.94608
[1mStep[0m  [10/21], [94mLoss[0m : 10.96788
[1mStep[0m  [12/21], [94mLoss[0m : 10.84071
[1mStep[0m  [14/21], [94mLoss[0m : 10.97426
[1mStep[0m  [16/21], [94mLoss[0m : 10.76719
[1mStep[0m  [18/21], [94mLoss[0m : 10.81106
[1mStep[0m  [20/21], [94mLoss[0m : 10.80297

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.799, [92mTest[0m: 10.761, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.89998
[1mStep[0m  [2/21], [94mLoss[0m : 10.79683
[1mStep[0m  [4/21], [94mLoss[0m : 10.93744
[1mStep[0m  [6/21], [94mLoss[0m : 10.81930
[1mStep[0m  [8/21], [94mLoss[0m : 10.75390
[1mStep[0m  [10/21], [94mLoss[0m : 10.99134
[1mStep[0m  [12/21], [94mLoss[0m : 10.47863
[1mStep[0m  [14/21], [94mLoss[0m : 10.87006
[1mStep[0m  [16/21], [94mLoss[0m : 10.72349
[1mStep[0m  [18/21], [94mLoss[0m : 10.79836
[1mStep[0m  [20/21], [94mLoss[0m : 10.59389

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.789, [92mTest[0m: 10.734, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69526
[1mStep[0m  [2/21], [94mLoss[0m : 10.97410
[1mStep[0m  [4/21], [94mLoss[0m : 10.65312
[1mStep[0m  [6/21], [94mLoss[0m : 10.81308
[1mStep[0m  [8/21], [94mLoss[0m : 10.56079
[1mStep[0m  [10/21], [94mLoss[0m : 10.68073
[1mStep[0m  [12/21], [94mLoss[0m : 10.70635
[1mStep[0m  [14/21], [94mLoss[0m : 11.00821
[1mStep[0m  [16/21], [94mLoss[0m : 10.43139
[1mStep[0m  [18/21], [94mLoss[0m : 10.69716
[1mStep[0m  [20/21], [94mLoss[0m : 11.05458

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.785, [92mTest[0m: 10.736, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.95887
[1mStep[0m  [2/21], [94mLoss[0m : 10.68507
[1mStep[0m  [4/21], [94mLoss[0m : 10.75957
[1mStep[0m  [6/21], [94mLoss[0m : 10.84209
[1mStep[0m  [8/21], [94mLoss[0m : 10.74201
[1mStep[0m  [10/21], [94mLoss[0m : 10.78738
[1mStep[0m  [12/21], [94mLoss[0m : 10.50864
[1mStep[0m  [14/21], [94mLoss[0m : 11.08909
[1mStep[0m  [16/21], [94mLoss[0m : 10.94645
[1mStep[0m  [18/21], [94mLoss[0m : 10.74010
[1mStep[0m  [20/21], [94mLoss[0m : 10.67092

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.787, [92mTest[0m: 10.722, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55379
[1mStep[0m  [2/21], [94mLoss[0m : 10.83515
[1mStep[0m  [4/21], [94mLoss[0m : 10.77594
[1mStep[0m  [6/21], [94mLoss[0m : 10.56239
[1mStep[0m  [8/21], [94mLoss[0m : 10.80937
[1mStep[0m  [10/21], [94mLoss[0m : 10.97249
[1mStep[0m  [12/21], [94mLoss[0m : 10.68683
[1mStep[0m  [14/21], [94mLoss[0m : 10.99546
[1mStep[0m  [16/21], [94mLoss[0m : 10.91656
[1mStep[0m  [18/21], [94mLoss[0m : 10.56918
[1mStep[0m  [20/21], [94mLoss[0m : 10.72812

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.776, [92mTest[0m: 10.721, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60157
[1mStep[0m  [2/21], [94mLoss[0m : 11.21253
[1mStep[0m  [4/21], [94mLoss[0m : 10.67624
[1mStep[0m  [6/21], [94mLoss[0m : 10.98073
[1mStep[0m  [8/21], [94mLoss[0m : 10.86921
[1mStep[0m  [10/21], [94mLoss[0m : 10.64378
[1mStep[0m  [12/21], [94mLoss[0m : 10.53779
[1mStep[0m  [14/21], [94mLoss[0m : 10.51226
[1mStep[0m  [16/21], [94mLoss[0m : 10.72012
[1mStep[0m  [18/21], [94mLoss[0m : 10.83520
[1mStep[0m  [20/21], [94mLoss[0m : 10.80646

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.768, [92mTest[0m: 10.714, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74658
[1mStep[0m  [2/21], [94mLoss[0m : 10.77428
[1mStep[0m  [4/21], [94mLoss[0m : 10.65000
[1mStep[0m  [6/21], [94mLoss[0m : 10.94382
[1mStep[0m  [8/21], [94mLoss[0m : 10.54205
[1mStep[0m  [10/21], [94mLoss[0m : 10.54749
[1mStep[0m  [12/21], [94mLoss[0m : 10.95692
[1mStep[0m  [14/21], [94mLoss[0m : 10.50439
[1mStep[0m  [16/21], [94mLoss[0m : 10.86523
[1mStep[0m  [18/21], [94mLoss[0m : 10.77579
[1mStep[0m  [20/21], [94mLoss[0m : 10.88802

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.758, [92mTest[0m: 10.694, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.01149
[1mStep[0m  [2/21], [94mLoss[0m : 10.58295
[1mStep[0m  [4/21], [94mLoss[0m : 10.69469
[1mStep[0m  [6/21], [94mLoss[0m : 10.99789
[1mStep[0m  [8/21], [94mLoss[0m : 10.70470
[1mStep[0m  [10/21], [94mLoss[0m : 10.61933
[1mStep[0m  [12/21], [94mLoss[0m : 11.05285
[1mStep[0m  [14/21], [94mLoss[0m : 10.48571
[1mStep[0m  [16/21], [94mLoss[0m : 10.57056
[1mStep[0m  [18/21], [94mLoss[0m : 10.51961
[1mStep[0m  [20/21], [94mLoss[0m : 10.63761

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.750, [92mTest[0m: 10.699, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86459
[1mStep[0m  [2/21], [94mLoss[0m : 10.59851
[1mStep[0m  [4/21], [94mLoss[0m : 10.58352
[1mStep[0m  [6/21], [94mLoss[0m : 10.66902
[1mStep[0m  [8/21], [94mLoss[0m : 10.84800
[1mStep[0m  [10/21], [94mLoss[0m : 10.64304
[1mStep[0m  [12/21], [94mLoss[0m : 10.77346
[1mStep[0m  [14/21], [94mLoss[0m : 10.92082
[1mStep[0m  [16/21], [94mLoss[0m : 10.62642
[1mStep[0m  [18/21], [94mLoss[0m : 10.73832
[1mStep[0m  [20/21], [94mLoss[0m : 10.78938

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.753, [92mTest[0m: 10.693, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.88405
[1mStep[0m  [2/21], [94mLoss[0m : 10.82295
[1mStep[0m  [4/21], [94mLoss[0m : 10.47151
[1mStep[0m  [6/21], [94mLoss[0m : 10.97972
[1mStep[0m  [8/21], [94mLoss[0m : 10.57004
[1mStep[0m  [10/21], [94mLoss[0m : 11.01618
[1mStep[0m  [12/21], [94mLoss[0m : 10.58581
[1mStep[0m  [14/21], [94mLoss[0m : 10.48037
[1mStep[0m  [16/21], [94mLoss[0m : 10.68181
[1mStep[0m  [18/21], [94mLoss[0m : 10.62000
[1mStep[0m  [20/21], [94mLoss[0m : 10.71120

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.746, [92mTest[0m: 10.678, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.667
====================================

Phase 1 - Evaluation MAE:  10.667331286839076
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.75825
[1mStep[0m  [2/21], [94mLoss[0m : 10.88883
[1mStep[0m  [4/21], [94mLoss[0m : 10.81916
[1mStep[0m  [6/21], [94mLoss[0m : 10.58154
[1mStep[0m  [8/21], [94mLoss[0m : 10.79160
[1mStep[0m  [10/21], [94mLoss[0m : 10.90612
[1mStep[0m  [12/21], [94mLoss[0m : 10.63313
[1mStep[0m  [14/21], [94mLoss[0m : 10.82463
[1mStep[0m  [16/21], [94mLoss[0m : 10.70250
[1mStep[0m  [18/21], [94mLoss[0m : 10.55304
[1mStep[0m  [20/21], [94mLoss[0m : 10.56450

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.732, [92mTest[0m: 10.668, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.91974
[1mStep[0m  [2/21], [94mLoss[0m : 10.78738
[1mStep[0m  [4/21], [94mLoss[0m : 10.58110
[1mStep[0m  [6/21], [94mLoss[0m : 10.87184
[1mStep[0m  [8/21], [94mLoss[0m : 10.56848
[1mStep[0m  [10/21], [94mLoss[0m : 10.73726
[1mStep[0m  [12/21], [94mLoss[0m : 10.84963
[1mStep[0m  [14/21], [94mLoss[0m : 10.74587
[1mStep[0m  [16/21], [94mLoss[0m : 10.69650
[1mStep[0m  [18/21], [94mLoss[0m : 10.67939
[1mStep[0m  [20/21], [94mLoss[0m : 10.93169

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.728, [92mTest[0m: 10.665, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59376
[1mStep[0m  [2/21], [94mLoss[0m : 10.62475
[1mStep[0m  [4/21], [94mLoss[0m : 10.80566
[1mStep[0m  [6/21], [94mLoss[0m : 10.97433
[1mStep[0m  [8/21], [94mLoss[0m : 10.98628
[1mStep[0m  [10/21], [94mLoss[0m : 10.53853
[1mStep[0m  [12/21], [94mLoss[0m : 10.63251
[1mStep[0m  [14/21], [94mLoss[0m : 10.80398
[1mStep[0m  [16/21], [94mLoss[0m : 10.66919
[1mStep[0m  [18/21], [94mLoss[0m : 10.57944
[1mStep[0m  [20/21], [94mLoss[0m : 10.86640

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.727, [92mTest[0m: 10.663, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.21393
[1mStep[0m  [2/21], [94mLoss[0m : 10.67397
[1mStep[0m  [4/21], [94mLoss[0m : 10.80567
[1mStep[0m  [6/21], [94mLoss[0m : 10.65547
[1mStep[0m  [8/21], [94mLoss[0m : 10.72754
[1mStep[0m  [10/21], [94mLoss[0m : 10.90856
[1mStep[0m  [12/21], [94mLoss[0m : 10.79729
[1mStep[0m  [14/21], [94mLoss[0m : 10.56082
[1mStep[0m  [16/21], [94mLoss[0m : 10.80777
[1mStep[0m  [18/21], [94mLoss[0m : 10.57360
[1mStep[0m  [20/21], [94mLoss[0m : 10.74036

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.713, [92mTest[0m: 10.653, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65767
[1mStep[0m  [2/21], [94mLoss[0m : 10.71230
[1mStep[0m  [4/21], [94mLoss[0m : 10.76196
[1mStep[0m  [6/21], [94mLoss[0m : 10.81811
[1mStep[0m  [8/21], [94mLoss[0m : 10.41080
[1mStep[0m  [10/21], [94mLoss[0m : 11.00152
[1mStep[0m  [12/21], [94mLoss[0m : 10.86185
[1mStep[0m  [14/21], [94mLoss[0m : 10.71467
[1mStep[0m  [16/21], [94mLoss[0m : 10.49532
[1mStep[0m  [18/21], [94mLoss[0m : 10.91780
[1mStep[0m  [20/21], [94mLoss[0m : 10.62194

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.707, [92mTest[0m: 10.640, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.68761
[1mStep[0m  [2/21], [94mLoss[0m : 10.55176
[1mStep[0m  [4/21], [94mLoss[0m : 10.68747
[1mStep[0m  [6/21], [94mLoss[0m : 10.38647
[1mStep[0m  [8/21], [94mLoss[0m : 10.78491
[1mStep[0m  [10/21], [94mLoss[0m : 10.58865
[1mStep[0m  [12/21], [94mLoss[0m : 10.75759
[1mStep[0m  [14/21], [94mLoss[0m : 10.70512
[1mStep[0m  [16/21], [94mLoss[0m : 10.75106
[1mStep[0m  [18/21], [94mLoss[0m : 10.66965
[1mStep[0m  [20/21], [94mLoss[0m : 10.86427

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.701, [92mTest[0m: 10.639, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.87891
[1mStep[0m  [2/21], [94mLoss[0m : 10.60985
[1mStep[0m  [4/21], [94mLoss[0m : 11.11316
[1mStep[0m  [6/21], [94mLoss[0m : 10.70443
[1mStep[0m  [8/21], [94mLoss[0m : 10.56967
[1mStep[0m  [10/21], [94mLoss[0m : 10.80225
[1mStep[0m  [12/21], [94mLoss[0m : 10.67313
[1mStep[0m  [14/21], [94mLoss[0m : 10.44898
[1mStep[0m  [16/21], [94mLoss[0m : 10.66228
[1mStep[0m  [18/21], [94mLoss[0m : 10.42262
[1mStep[0m  [20/21], [94mLoss[0m : 10.95850

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.697, [92mTest[0m: 10.616, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78147
[1mStep[0m  [2/21], [94mLoss[0m : 10.74848
[1mStep[0m  [4/21], [94mLoss[0m : 10.61739
[1mStep[0m  [6/21], [94mLoss[0m : 10.67785
[1mStep[0m  [8/21], [94mLoss[0m : 10.61225
[1mStep[0m  [10/21], [94mLoss[0m : 10.84936
[1mStep[0m  [12/21], [94mLoss[0m : 10.62988
[1mStep[0m  [14/21], [94mLoss[0m : 10.27769
[1mStep[0m  [16/21], [94mLoss[0m : 10.61730
[1mStep[0m  [18/21], [94mLoss[0m : 10.90609
[1mStep[0m  [20/21], [94mLoss[0m : 10.78230

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.681, [92mTest[0m: 10.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.83682
[1mStep[0m  [2/21], [94mLoss[0m : 10.82929
[1mStep[0m  [4/21], [94mLoss[0m : 10.72450
[1mStep[0m  [6/21], [94mLoss[0m : 10.86908
[1mStep[0m  [8/21], [94mLoss[0m : 10.70589
[1mStep[0m  [10/21], [94mLoss[0m : 10.50651
[1mStep[0m  [12/21], [94mLoss[0m : 10.82222
[1mStep[0m  [14/21], [94mLoss[0m : 10.76369
[1mStep[0m  [16/21], [94mLoss[0m : 10.85157
[1mStep[0m  [18/21], [94mLoss[0m : 10.56418
[1mStep[0m  [20/21], [94mLoss[0m : 10.60853

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.671, [92mTest[0m: 10.585, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75800
[1mStep[0m  [2/21], [94mLoss[0m : 10.66984
[1mStep[0m  [4/21], [94mLoss[0m : 10.78602
[1mStep[0m  [6/21], [94mLoss[0m : 10.66704
[1mStep[0m  [8/21], [94mLoss[0m : 10.69430
[1mStep[0m  [10/21], [94mLoss[0m : 10.66056
[1mStep[0m  [12/21], [94mLoss[0m : 10.44389
[1mStep[0m  [14/21], [94mLoss[0m : 10.43686
[1mStep[0m  [16/21], [94mLoss[0m : 10.86161
[1mStep[0m  [18/21], [94mLoss[0m : 10.63584
[1mStep[0m  [20/21], [94mLoss[0m : 10.59646

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.666, [92mTest[0m: 10.578, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48377
[1mStep[0m  [2/21], [94mLoss[0m : 10.87132
[1mStep[0m  [4/21], [94mLoss[0m : 10.75273
[1mStep[0m  [6/21], [94mLoss[0m : 10.80504
[1mStep[0m  [8/21], [94mLoss[0m : 10.78702
[1mStep[0m  [10/21], [94mLoss[0m : 10.49032
[1mStep[0m  [12/21], [94mLoss[0m : 10.45050
[1mStep[0m  [14/21], [94mLoss[0m : 10.91504
[1mStep[0m  [16/21], [94mLoss[0m : 10.78843
[1mStep[0m  [18/21], [94mLoss[0m : 10.41174
[1mStep[0m  [20/21], [94mLoss[0m : 10.40767

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.657, [92mTest[0m: 10.582, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73004
[1mStep[0m  [2/21], [94mLoss[0m : 10.80933
[1mStep[0m  [4/21], [94mLoss[0m : 10.46759
[1mStep[0m  [6/21], [94mLoss[0m : 10.61134
[1mStep[0m  [8/21], [94mLoss[0m : 10.49129
[1mStep[0m  [10/21], [94mLoss[0m : 10.18473
[1mStep[0m  [12/21], [94mLoss[0m : 10.87708
[1mStep[0m  [14/21], [94mLoss[0m : 10.71643
[1mStep[0m  [16/21], [94mLoss[0m : 10.53279
[1mStep[0m  [18/21], [94mLoss[0m : 10.62385
[1mStep[0m  [20/21], [94mLoss[0m : 10.77472

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.650, [92mTest[0m: 10.564, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67622
[1mStep[0m  [2/21], [94mLoss[0m : 10.43482
[1mStep[0m  [4/21], [94mLoss[0m : 10.63476
[1mStep[0m  [6/21], [94mLoss[0m : 10.49880
[1mStep[0m  [8/21], [94mLoss[0m : 10.79716
[1mStep[0m  [10/21], [94mLoss[0m : 10.55757
[1mStep[0m  [12/21], [94mLoss[0m : 10.63266
[1mStep[0m  [14/21], [94mLoss[0m : 10.60038
[1mStep[0m  [16/21], [94mLoss[0m : 10.86454
[1mStep[0m  [18/21], [94mLoss[0m : 10.59130
[1mStep[0m  [20/21], [94mLoss[0m : 10.79582

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.637, [92mTest[0m: 10.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.53552
[1mStep[0m  [2/21], [94mLoss[0m : 10.67850
[1mStep[0m  [4/21], [94mLoss[0m : 10.49547
[1mStep[0m  [6/21], [94mLoss[0m : 10.71481
[1mStep[0m  [8/21], [94mLoss[0m : 10.75904
[1mStep[0m  [10/21], [94mLoss[0m : 10.68623
[1mStep[0m  [12/21], [94mLoss[0m : 10.90760
[1mStep[0m  [14/21], [94mLoss[0m : 10.66209
[1mStep[0m  [16/21], [94mLoss[0m : 10.45576
[1mStep[0m  [18/21], [94mLoss[0m : 10.54705
[1mStep[0m  [20/21], [94mLoss[0m : 10.77424

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.634, [92mTest[0m: 10.552, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.66141
[1mStep[0m  [2/21], [94mLoss[0m : 10.26522
[1mStep[0m  [4/21], [94mLoss[0m : 10.62887
[1mStep[0m  [6/21], [94mLoss[0m : 10.83699
[1mStep[0m  [8/21], [94mLoss[0m : 10.32638
[1mStep[0m  [10/21], [94mLoss[0m : 10.70501
[1mStep[0m  [12/21], [94mLoss[0m : 10.64070
[1mStep[0m  [14/21], [94mLoss[0m : 10.84842
[1mStep[0m  [16/21], [94mLoss[0m : 10.62567
[1mStep[0m  [18/21], [94mLoss[0m : 10.46500
[1mStep[0m  [20/21], [94mLoss[0m : 10.66406

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.619, [92mTest[0m: 10.534, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54691
[1mStep[0m  [2/21], [94mLoss[0m : 10.66302
[1mStep[0m  [4/21], [94mLoss[0m : 10.60381
[1mStep[0m  [6/21], [94mLoss[0m : 10.71275
[1mStep[0m  [8/21], [94mLoss[0m : 10.31252
[1mStep[0m  [10/21], [94mLoss[0m : 10.42514
[1mStep[0m  [12/21], [94mLoss[0m : 10.62597
[1mStep[0m  [14/21], [94mLoss[0m : 10.83462
[1mStep[0m  [16/21], [94mLoss[0m : 10.44898
[1mStep[0m  [18/21], [94mLoss[0m : 10.65752
[1mStep[0m  [20/21], [94mLoss[0m : 10.54300

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.613, [92mTest[0m: 10.532, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.52256
[1mStep[0m  [2/21], [94mLoss[0m : 10.79039
[1mStep[0m  [4/21], [94mLoss[0m : 10.38744
[1mStep[0m  [6/21], [94mLoss[0m : 10.63953
[1mStep[0m  [8/21], [94mLoss[0m : 10.42715
[1mStep[0m  [10/21], [94mLoss[0m : 10.65598
[1mStep[0m  [12/21], [94mLoss[0m : 10.71848
[1mStep[0m  [14/21], [94mLoss[0m : 10.73224
[1mStep[0m  [16/21], [94mLoss[0m : 10.59137
[1mStep[0m  [18/21], [94mLoss[0m : 10.65912
[1mStep[0m  [20/21], [94mLoss[0m : 10.58955

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.607, [92mTest[0m: 10.528, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42085
[1mStep[0m  [2/21], [94mLoss[0m : 10.46160
[1mStep[0m  [4/21], [94mLoss[0m : 10.65574
[1mStep[0m  [6/21], [94mLoss[0m : 10.44392
[1mStep[0m  [8/21], [94mLoss[0m : 10.52986
[1mStep[0m  [10/21], [94mLoss[0m : 10.82283
[1mStep[0m  [12/21], [94mLoss[0m : 10.52861
[1mStep[0m  [14/21], [94mLoss[0m : 10.64005
[1mStep[0m  [16/21], [94mLoss[0m : 10.51136
[1mStep[0m  [18/21], [94mLoss[0m : 10.54204
[1mStep[0m  [20/21], [94mLoss[0m : 10.69973

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.598, [92mTest[0m: 10.514, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.36178
[1mStep[0m  [2/21], [94mLoss[0m : 10.53396
[1mStep[0m  [4/21], [94mLoss[0m : 10.46166
[1mStep[0m  [6/21], [94mLoss[0m : 10.36872
[1mStep[0m  [8/21], [94mLoss[0m : 10.34630
[1mStep[0m  [10/21], [94mLoss[0m : 10.58928
[1mStep[0m  [12/21], [94mLoss[0m : 10.84111
[1mStep[0m  [14/21], [94mLoss[0m : 10.65693
[1mStep[0m  [16/21], [94mLoss[0m : 10.68809
[1mStep[0m  [18/21], [94mLoss[0m : 10.46213
[1mStep[0m  [20/21], [94mLoss[0m : 10.55783

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.589, [92mTest[0m: 10.494, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56370
[1mStep[0m  [2/21], [94mLoss[0m : 10.55506
[1mStep[0m  [4/21], [94mLoss[0m : 10.87579
[1mStep[0m  [6/21], [94mLoss[0m : 10.17156
[1mStep[0m  [8/21], [94mLoss[0m : 10.81642
[1mStep[0m  [10/21], [94mLoss[0m : 10.55150
[1mStep[0m  [12/21], [94mLoss[0m : 10.82585
[1mStep[0m  [14/21], [94mLoss[0m : 10.64423
[1mStep[0m  [16/21], [94mLoss[0m : 10.39551
[1mStep[0m  [18/21], [94mLoss[0m : 10.48918
[1mStep[0m  [20/21], [94mLoss[0m : 10.60086

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.581, [92mTest[0m: 10.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86016
[1mStep[0m  [2/21], [94mLoss[0m : 10.66279
[1mStep[0m  [4/21], [94mLoss[0m : 10.58309
[1mStep[0m  [6/21], [94mLoss[0m : 10.39737
[1mStep[0m  [8/21], [94mLoss[0m : 10.84739
[1mStep[0m  [10/21], [94mLoss[0m : 10.66776
[1mStep[0m  [12/21], [94mLoss[0m : 10.39594
[1mStep[0m  [14/21], [94mLoss[0m : 10.55485
[1mStep[0m  [16/21], [94mLoss[0m : 10.50318
[1mStep[0m  [18/21], [94mLoss[0m : 10.75828
[1mStep[0m  [20/21], [94mLoss[0m : 10.39547

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.569, [92mTest[0m: 10.487, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56454
[1mStep[0m  [2/21], [94mLoss[0m : 10.48738
[1mStep[0m  [4/21], [94mLoss[0m : 10.14070
[1mStep[0m  [6/21], [94mLoss[0m : 10.59215
[1mStep[0m  [8/21], [94mLoss[0m : 10.65684
[1mStep[0m  [10/21], [94mLoss[0m : 10.24138
[1mStep[0m  [12/21], [94mLoss[0m : 10.49443
[1mStep[0m  [14/21], [94mLoss[0m : 10.66523
[1mStep[0m  [16/21], [94mLoss[0m : 10.57037
[1mStep[0m  [18/21], [94mLoss[0m : 10.43735
[1mStep[0m  [20/21], [94mLoss[0m : 10.56328

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.560, [92mTest[0m: 10.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58245
[1mStep[0m  [2/21], [94mLoss[0m : 10.57190
[1mStep[0m  [4/21], [94mLoss[0m : 10.78203
[1mStep[0m  [6/21], [94mLoss[0m : 10.42344
[1mStep[0m  [8/21], [94mLoss[0m : 10.70234
[1mStep[0m  [10/21], [94mLoss[0m : 10.60578
[1mStep[0m  [12/21], [94mLoss[0m : 10.55988
[1mStep[0m  [14/21], [94mLoss[0m : 10.44200
[1mStep[0m  [16/21], [94mLoss[0m : 10.44792
[1mStep[0m  [18/21], [94mLoss[0m : 10.61445
[1mStep[0m  [20/21], [94mLoss[0m : 10.28144

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.554, [92mTest[0m: 10.466, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.35028
[1mStep[0m  [2/21], [94mLoss[0m : 10.58038
[1mStep[0m  [4/21], [94mLoss[0m : 10.51015
[1mStep[0m  [6/21], [94mLoss[0m : 10.52008
[1mStep[0m  [8/21], [94mLoss[0m : 10.66316
[1mStep[0m  [10/21], [94mLoss[0m : 10.53674
[1mStep[0m  [12/21], [94mLoss[0m : 10.52366
[1mStep[0m  [14/21], [94mLoss[0m : 10.40974
[1mStep[0m  [16/21], [94mLoss[0m : 10.36067
[1mStep[0m  [18/21], [94mLoss[0m : 10.42289
[1mStep[0m  [20/21], [94mLoss[0m : 10.51958

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.545, [92mTest[0m: 10.455, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.26865
[1mStep[0m  [2/21], [94mLoss[0m : 10.36887
[1mStep[0m  [4/21], [94mLoss[0m : 10.63472
[1mStep[0m  [6/21], [94mLoss[0m : 10.55164
[1mStep[0m  [8/21], [94mLoss[0m : 10.45745
[1mStep[0m  [10/21], [94mLoss[0m : 10.42809
[1mStep[0m  [12/21], [94mLoss[0m : 10.72788
[1mStep[0m  [14/21], [94mLoss[0m : 10.38875
[1mStep[0m  [16/21], [94mLoss[0m : 10.48163
[1mStep[0m  [18/21], [94mLoss[0m : 10.50276
[1mStep[0m  [20/21], [94mLoss[0m : 10.44127

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.537, [92mTest[0m: 10.445, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50310
[1mStep[0m  [2/21], [94mLoss[0m : 10.25933
[1mStep[0m  [4/21], [94mLoss[0m : 10.47183
[1mStep[0m  [6/21], [94mLoss[0m : 10.48885
[1mStep[0m  [8/21], [94mLoss[0m : 10.77198
[1mStep[0m  [10/21], [94mLoss[0m : 10.56453
[1mStep[0m  [12/21], [94mLoss[0m : 10.46045
[1mStep[0m  [14/21], [94mLoss[0m : 10.61154
[1mStep[0m  [16/21], [94mLoss[0m : 10.87556
[1mStep[0m  [18/21], [94mLoss[0m : 10.60374
[1mStep[0m  [20/21], [94mLoss[0m : 10.50990

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.527, [92mTest[0m: 10.452, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.54078
[1mStep[0m  [2/21], [94mLoss[0m : 10.66946
[1mStep[0m  [4/21], [94mLoss[0m : 10.63960
[1mStep[0m  [6/21], [94mLoss[0m : 10.49139
[1mStep[0m  [8/21], [94mLoss[0m : 10.50014
[1mStep[0m  [10/21], [94mLoss[0m : 10.60243
[1mStep[0m  [12/21], [94mLoss[0m : 10.43752
[1mStep[0m  [14/21], [94mLoss[0m : 10.47544
[1mStep[0m  [16/21], [94mLoss[0m : 10.57179
[1mStep[0m  [18/21], [94mLoss[0m : 10.56595
[1mStep[0m  [20/21], [94mLoss[0m : 10.40672

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.516, [92mTest[0m: 10.430, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39276
[1mStep[0m  [2/21], [94mLoss[0m : 10.39859
[1mStep[0m  [4/21], [94mLoss[0m : 10.43302
[1mStep[0m  [6/21], [94mLoss[0m : 10.44834
[1mStep[0m  [8/21], [94mLoss[0m : 10.55101
[1mStep[0m  [10/21], [94mLoss[0m : 10.73682
[1mStep[0m  [12/21], [94mLoss[0m : 10.42218
[1mStep[0m  [14/21], [94mLoss[0m : 10.34087
[1mStep[0m  [16/21], [94mLoss[0m : 10.68491
[1mStep[0m  [18/21], [94mLoss[0m : 10.63415
[1mStep[0m  [20/21], [94mLoss[0m : 10.62415

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.503, [92mTest[0m: 10.421, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.26282
[1mStep[0m  [2/21], [94mLoss[0m : 10.43742
[1mStep[0m  [4/21], [94mLoss[0m : 10.64595
[1mStep[0m  [6/21], [94mLoss[0m : 10.41954
[1mStep[0m  [8/21], [94mLoss[0m : 10.53944
[1mStep[0m  [10/21], [94mLoss[0m : 10.64926
[1mStep[0m  [12/21], [94mLoss[0m : 10.46075
[1mStep[0m  [14/21], [94mLoss[0m : 10.41043
[1mStep[0m  [16/21], [94mLoss[0m : 10.44127
[1mStep[0m  [18/21], [94mLoss[0m : 10.67830
[1mStep[0m  [20/21], [94mLoss[0m : 10.53967

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.492, [92mTest[0m: 10.407, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58375
[1mStep[0m  [2/21], [94mLoss[0m : 10.26854
[1mStep[0m  [4/21], [94mLoss[0m : 10.33935
[1mStep[0m  [6/21], [94mLoss[0m : 10.31146
[1mStep[0m  [8/21], [94mLoss[0m : 10.78553
[1mStep[0m  [10/21], [94mLoss[0m : 10.58948
[1mStep[0m  [12/21], [94mLoss[0m : 10.38298
[1mStep[0m  [14/21], [94mLoss[0m : 10.41441
[1mStep[0m  [16/21], [94mLoss[0m : 10.32363
[1mStep[0m  [18/21], [94mLoss[0m : 10.29567
[1mStep[0m  [20/21], [94mLoss[0m : 10.53324

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.486, [92mTest[0m: 10.392, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.401
====================================

Phase 2 - Evaluation MAE:  10.400589670453753
MAE score P1      10.667331
MAE score P2       10.40059
loss              10.486459
learning_rate        0.0001
batch_size              512
hidden_sizes          [100]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay           0.01
Name: 22, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 11.13317
[1mStep[0m  [2/21], [94mLoss[0m : 11.13156
[1mStep[0m  [4/21], [94mLoss[0m : 10.88276
[1mStep[0m  [6/21], [94mLoss[0m : 10.70439
[1mStep[0m  [8/21], [94mLoss[0m : 10.79169
[1mStep[0m  [10/21], [94mLoss[0m : 10.93914
[1mStep[0m  [12/21], [94mLoss[0m : 10.77350
[1mStep[0m  [14/21], [94mLoss[0m : 10.96341
[1mStep[0m  [16/21], [94mLoss[0m : 10.88772
[1mStep[0m  [18/21], [94mLoss[0m : 10.76733
[1mStep[0m  [20/21], [94mLoss[0m : 10.61879

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.864, [92mTest[0m: 10.995, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86897
[1mStep[0m  [2/21], [94mLoss[0m : 10.68782
[1mStep[0m  [4/21], [94mLoss[0m : 10.81211
[1mStep[0m  [6/21], [94mLoss[0m : 10.88682
[1mStep[0m  [8/21], [94mLoss[0m : 11.02151
[1mStep[0m  [10/21], [94mLoss[0m : 10.90284
[1mStep[0m  [12/21], [94mLoss[0m : 10.83280
[1mStep[0m  [14/21], [94mLoss[0m : 10.75264
[1mStep[0m  [16/21], [94mLoss[0m : 11.07556
[1mStep[0m  [18/21], [94mLoss[0m : 10.68051
[1mStep[0m  [20/21], [94mLoss[0m : 11.10255

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.858, [92mTest[0m: 10.922, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.75340
[1mStep[0m  [2/21], [94mLoss[0m : 10.71780
[1mStep[0m  [4/21], [94mLoss[0m : 10.92280
[1mStep[0m  [6/21], [94mLoss[0m : 10.75135
[1mStep[0m  [8/21], [94mLoss[0m : 11.22217
[1mStep[0m  [10/21], [94mLoss[0m : 10.91616
[1mStep[0m  [12/21], [94mLoss[0m : 10.87346
[1mStep[0m  [14/21], [94mLoss[0m : 11.10183
[1mStep[0m  [16/21], [94mLoss[0m : 10.85078
[1mStep[0m  [18/21], [94mLoss[0m : 10.80832
[1mStep[0m  [20/21], [94mLoss[0m : 10.96776

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.859, [92mTest[0m: 10.882, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79589
[1mStep[0m  [2/21], [94mLoss[0m : 10.67272
[1mStep[0m  [4/21], [94mLoss[0m : 10.88017
[1mStep[0m  [6/21], [94mLoss[0m : 10.70138
[1mStep[0m  [8/21], [94mLoss[0m : 10.88494
[1mStep[0m  [10/21], [94mLoss[0m : 10.75296
[1mStep[0m  [12/21], [94mLoss[0m : 10.55929
[1mStep[0m  [14/21], [94mLoss[0m : 10.71803
[1mStep[0m  [16/21], [94mLoss[0m : 10.93670
[1mStep[0m  [18/21], [94mLoss[0m : 10.58717
[1mStep[0m  [20/21], [94mLoss[0m : 10.95962

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.842, [92mTest[0m: 10.878, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65150
[1mStep[0m  [2/21], [94mLoss[0m : 10.81479
[1mStep[0m  [4/21], [94mLoss[0m : 10.60938
[1mStep[0m  [6/21], [94mLoss[0m : 10.89460
[1mStep[0m  [8/21], [94mLoss[0m : 10.73970
[1mStep[0m  [10/21], [94mLoss[0m : 10.67836
[1mStep[0m  [12/21], [94mLoss[0m : 10.97089
[1mStep[0m  [14/21], [94mLoss[0m : 10.80416
[1mStep[0m  [16/21], [94mLoss[0m : 10.96875
[1mStep[0m  [18/21], [94mLoss[0m : 10.78997
[1mStep[0m  [20/21], [94mLoss[0m : 10.93337

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.841, [92mTest[0m: 10.875, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73048
[1mStep[0m  [2/21], [94mLoss[0m : 10.77504
[1mStep[0m  [4/21], [94mLoss[0m : 10.65971
[1mStep[0m  [6/21], [94mLoss[0m : 10.99113
[1mStep[0m  [8/21], [94mLoss[0m : 10.83733
[1mStep[0m  [10/21], [94mLoss[0m : 11.04978
[1mStep[0m  [12/21], [94mLoss[0m : 10.69491
[1mStep[0m  [14/21], [94mLoss[0m : 10.91204
[1mStep[0m  [16/21], [94mLoss[0m : 10.84219
[1mStep[0m  [18/21], [94mLoss[0m : 11.06906
[1mStep[0m  [20/21], [94mLoss[0m : 10.79832

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.838, [92mTest[0m: 10.865, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79592
[1mStep[0m  [2/21], [94mLoss[0m : 10.62723
[1mStep[0m  [4/21], [94mLoss[0m : 10.87957
[1mStep[0m  [6/21], [94mLoss[0m : 10.50514
[1mStep[0m  [8/21], [94mLoss[0m : 10.78802
[1mStep[0m  [10/21], [94mLoss[0m : 11.13770
[1mStep[0m  [12/21], [94mLoss[0m : 10.75760
[1mStep[0m  [14/21], [94mLoss[0m : 10.96153
[1mStep[0m  [16/21], [94mLoss[0m : 10.82938
[1mStep[0m  [18/21], [94mLoss[0m : 10.97509
[1mStep[0m  [20/21], [94mLoss[0m : 10.86533

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.831, [92mTest[0m: 10.855, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.76868
[1mStep[0m  [2/21], [94mLoss[0m : 10.69831
[1mStep[0m  [4/21], [94mLoss[0m : 10.86265
[1mStep[0m  [6/21], [94mLoss[0m : 10.65890
[1mStep[0m  [8/21], [94mLoss[0m : 10.86060
[1mStep[0m  [10/21], [94mLoss[0m : 10.86063
[1mStep[0m  [12/21], [94mLoss[0m : 10.86017
[1mStep[0m  [14/21], [94mLoss[0m : 10.73211
[1mStep[0m  [16/21], [94mLoss[0m : 10.95898
[1mStep[0m  [18/21], [94mLoss[0m : 10.79539
[1mStep[0m  [20/21], [94mLoss[0m : 10.88715

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.819, [92mTest[0m: 10.841, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 11.16498
[1mStep[0m  [2/21], [94mLoss[0m : 11.05712
[1mStep[0m  [4/21], [94mLoss[0m : 11.00898
[1mStep[0m  [6/21], [94mLoss[0m : 10.71435
[1mStep[0m  [8/21], [94mLoss[0m : 10.79903
[1mStep[0m  [10/21], [94mLoss[0m : 10.92095
[1mStep[0m  [12/21], [94mLoss[0m : 10.89167
[1mStep[0m  [14/21], [94mLoss[0m : 10.73354
[1mStep[0m  [16/21], [94mLoss[0m : 10.69235
[1mStep[0m  [18/21], [94mLoss[0m : 10.81616
[1mStep[0m  [20/21], [94mLoss[0m : 10.63678

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.812, [92mTest[0m: 10.831, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.96017
[1mStep[0m  [2/21], [94mLoss[0m : 10.80360
[1mStep[0m  [4/21], [94mLoss[0m : 11.11052
[1mStep[0m  [6/21], [94mLoss[0m : 10.87139
[1mStep[0m  [8/21], [94mLoss[0m : 10.82773
[1mStep[0m  [10/21], [94mLoss[0m : 10.75877
[1mStep[0m  [12/21], [94mLoss[0m : 10.53529
[1mStep[0m  [14/21], [94mLoss[0m : 10.85361
[1mStep[0m  [16/21], [94mLoss[0m : 10.52661
[1mStep[0m  [18/21], [94mLoss[0m : 10.64213
[1mStep[0m  [20/21], [94mLoss[0m : 10.69082

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.807, [92mTest[0m: 10.811, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.94052
[1mStep[0m  [2/21], [94mLoss[0m : 10.81844
[1mStep[0m  [4/21], [94mLoss[0m : 10.68038
[1mStep[0m  [6/21], [94mLoss[0m : 10.93888
[1mStep[0m  [8/21], [94mLoss[0m : 10.70295
[1mStep[0m  [10/21], [94mLoss[0m : 10.75972
[1mStep[0m  [12/21], [94mLoss[0m : 10.85288
[1mStep[0m  [14/21], [94mLoss[0m : 10.62780
[1mStep[0m  [16/21], [94mLoss[0m : 10.75914
[1mStep[0m  [18/21], [94mLoss[0m : 10.64973
[1mStep[0m  [20/21], [94mLoss[0m : 10.67225

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.792, [92mTest[0m: 10.824, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.86872
[1mStep[0m  [2/21], [94mLoss[0m : 10.75949
[1mStep[0m  [4/21], [94mLoss[0m : 10.55727
[1mStep[0m  [6/21], [94mLoss[0m : 10.69189
[1mStep[0m  [8/21], [94mLoss[0m : 10.93917
[1mStep[0m  [10/21], [94mLoss[0m : 10.86980
[1mStep[0m  [12/21], [94mLoss[0m : 10.67005
[1mStep[0m  [14/21], [94mLoss[0m : 10.74895
[1mStep[0m  [16/21], [94mLoss[0m : 10.83357
[1mStep[0m  [18/21], [94mLoss[0m : 10.74834
[1mStep[0m  [20/21], [94mLoss[0m : 10.91793

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.792, [92mTest[0m: 10.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90974
[1mStep[0m  [2/21], [94mLoss[0m : 10.36973
[1mStep[0m  [4/21], [94mLoss[0m : 10.89429
[1mStep[0m  [6/21], [94mLoss[0m : 10.61171
[1mStep[0m  [8/21], [94mLoss[0m : 10.73026
[1mStep[0m  [10/21], [94mLoss[0m : 10.51050
[1mStep[0m  [12/21], [94mLoss[0m : 10.79884
[1mStep[0m  [14/21], [94mLoss[0m : 10.78918
[1mStep[0m  [16/21], [94mLoss[0m : 10.63555
[1mStep[0m  [18/21], [94mLoss[0m : 10.71906
[1mStep[0m  [20/21], [94mLoss[0m : 10.90075

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.785, [92mTest[0m: 10.798, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.91062
[1mStep[0m  [2/21], [94mLoss[0m : 10.76222
[1mStep[0m  [4/21], [94mLoss[0m : 10.64254
[1mStep[0m  [6/21], [94mLoss[0m : 10.60766
[1mStep[0m  [8/21], [94mLoss[0m : 10.91778
[1mStep[0m  [10/21], [94mLoss[0m : 10.96592
[1mStep[0m  [12/21], [94mLoss[0m : 10.84034
[1mStep[0m  [14/21], [94mLoss[0m : 10.68873
[1mStep[0m  [16/21], [94mLoss[0m : 10.81604
[1mStep[0m  [18/21], [94mLoss[0m : 10.84808
[1mStep[0m  [20/21], [94mLoss[0m : 10.68991

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.784, [92mTest[0m: 10.788, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.81351
[1mStep[0m  [2/21], [94mLoss[0m : 10.77080
[1mStep[0m  [4/21], [94mLoss[0m : 10.90158
[1mStep[0m  [6/21], [94mLoss[0m : 10.93206
[1mStep[0m  [8/21], [94mLoss[0m : 10.93942
[1mStep[0m  [10/21], [94mLoss[0m : 10.68705
[1mStep[0m  [12/21], [94mLoss[0m : 10.83261
[1mStep[0m  [14/21], [94mLoss[0m : 10.77941
[1mStep[0m  [16/21], [94mLoss[0m : 10.73417
[1mStep[0m  [18/21], [94mLoss[0m : 10.88074
[1mStep[0m  [20/21], [94mLoss[0m : 10.61226

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.772, [92mTest[0m: 10.762, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.94789
[1mStep[0m  [2/21], [94mLoss[0m : 10.82132
[1mStep[0m  [4/21], [94mLoss[0m : 10.77546
[1mStep[0m  [6/21], [94mLoss[0m : 10.98517
[1mStep[0m  [8/21], [94mLoss[0m : 10.69998
[1mStep[0m  [10/21], [94mLoss[0m : 10.83994
[1mStep[0m  [12/21], [94mLoss[0m : 10.82872
[1mStep[0m  [14/21], [94mLoss[0m : 10.67823
[1mStep[0m  [16/21], [94mLoss[0m : 10.79634
[1mStep[0m  [18/21], [94mLoss[0m : 10.67360
[1mStep[0m  [20/21], [94mLoss[0m : 10.74100

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.770, [92mTest[0m: 10.753, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.55132
[1mStep[0m  [2/21], [94mLoss[0m : 10.82421
[1mStep[0m  [4/21], [94mLoss[0m : 10.67365
[1mStep[0m  [6/21], [94mLoss[0m : 10.69151
[1mStep[0m  [8/21], [94mLoss[0m : 10.61704
[1mStep[0m  [10/21], [94mLoss[0m : 10.90926
[1mStep[0m  [12/21], [94mLoss[0m : 10.66348
[1mStep[0m  [14/21], [94mLoss[0m : 10.68328
[1mStep[0m  [16/21], [94mLoss[0m : 10.66961
[1mStep[0m  [18/21], [94mLoss[0m : 10.82026
[1mStep[0m  [20/21], [94mLoss[0m : 10.73495

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.754, [92mTest[0m: 10.738, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.85686
[1mStep[0m  [2/21], [94mLoss[0m : 10.45668
[1mStep[0m  [4/21], [94mLoss[0m : 10.74129
[1mStep[0m  [6/21], [94mLoss[0m : 10.89348
[1mStep[0m  [8/21], [94mLoss[0m : 10.60926
[1mStep[0m  [10/21], [94mLoss[0m : 10.66299
[1mStep[0m  [12/21], [94mLoss[0m : 10.94147
[1mStep[0m  [14/21], [94mLoss[0m : 10.63418
[1mStep[0m  [16/21], [94mLoss[0m : 10.28311
[1mStep[0m  [18/21], [94mLoss[0m : 10.88001
[1mStep[0m  [20/21], [94mLoss[0m : 10.71542

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.744, [92mTest[0m: 10.745, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.71491
[1mStep[0m  [2/21], [94mLoss[0m : 10.67594
[1mStep[0m  [4/21], [94mLoss[0m : 10.60191
[1mStep[0m  [6/21], [94mLoss[0m : 10.90244
[1mStep[0m  [8/21], [94mLoss[0m : 10.37015
[1mStep[0m  [10/21], [94mLoss[0m : 10.67083
[1mStep[0m  [12/21], [94mLoss[0m : 11.06058
[1mStep[0m  [14/21], [94mLoss[0m : 10.89975
[1mStep[0m  [16/21], [94mLoss[0m : 10.68053
[1mStep[0m  [18/21], [94mLoss[0m : 10.91529
[1mStep[0m  [20/21], [94mLoss[0m : 10.72317

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.737, [92mTest[0m: 10.728, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.60761
[1mStep[0m  [2/21], [94mLoss[0m : 10.73519
[1mStep[0m  [4/21], [94mLoss[0m : 10.81465
[1mStep[0m  [6/21], [94mLoss[0m : 10.75855
[1mStep[0m  [8/21], [94mLoss[0m : 10.61623
[1mStep[0m  [10/21], [94mLoss[0m : 10.73956
[1mStep[0m  [12/21], [94mLoss[0m : 10.78614
[1mStep[0m  [14/21], [94mLoss[0m : 10.64641
[1mStep[0m  [16/21], [94mLoss[0m : 10.82118
[1mStep[0m  [18/21], [94mLoss[0m : 10.56831
[1mStep[0m  [20/21], [94mLoss[0m : 10.99041

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.741, [92mTest[0m: 10.725, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.79908
[1mStep[0m  [2/21], [94mLoss[0m : 10.83253
[1mStep[0m  [4/21], [94mLoss[0m : 10.76304
[1mStep[0m  [6/21], [94mLoss[0m : 10.63492
[1mStep[0m  [8/21], [94mLoss[0m : 10.44925
[1mStep[0m  [10/21], [94mLoss[0m : 10.73105
[1mStep[0m  [12/21], [94mLoss[0m : 10.66984
[1mStep[0m  [14/21], [94mLoss[0m : 10.55885
[1mStep[0m  [16/21], [94mLoss[0m : 11.02884
[1mStep[0m  [18/21], [94mLoss[0m : 10.87817
[1mStep[0m  [20/21], [94mLoss[0m : 10.63481

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.715, [92mTest[0m: 10.693, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.59354
[1mStep[0m  [2/21], [94mLoss[0m : 10.48181
[1mStep[0m  [4/21], [94mLoss[0m : 10.72883
[1mStep[0m  [6/21], [94mLoss[0m : 10.85224
[1mStep[0m  [8/21], [94mLoss[0m : 10.71805
[1mStep[0m  [10/21], [94mLoss[0m : 10.68657
[1mStep[0m  [12/21], [94mLoss[0m : 10.70113
[1mStep[0m  [14/21], [94mLoss[0m : 10.85533
[1mStep[0m  [16/21], [94mLoss[0m : 10.46177
[1mStep[0m  [18/21], [94mLoss[0m : 10.77981
[1mStep[0m  [20/21], [94mLoss[0m : 10.82181

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.715, [92mTest[0m: 10.694, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.97491
[1mStep[0m  [2/21], [94mLoss[0m : 10.32211
[1mStep[0m  [4/21], [94mLoss[0m : 10.47385
[1mStep[0m  [6/21], [94mLoss[0m : 11.23543
[1mStep[0m  [8/21], [94mLoss[0m : 10.80770
[1mStep[0m  [10/21], [94mLoss[0m : 10.65453
[1mStep[0m  [12/21], [94mLoss[0m : 10.85339
[1mStep[0m  [14/21], [94mLoss[0m : 10.88672
[1mStep[0m  [16/21], [94mLoss[0m : 10.51517
[1mStep[0m  [18/21], [94mLoss[0m : 10.96178
[1mStep[0m  [20/21], [94mLoss[0m : 10.92233

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.717, [92mTest[0m: 10.671, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.65640
[1mStep[0m  [2/21], [94mLoss[0m : 10.60339
[1mStep[0m  [4/21], [94mLoss[0m : 10.93044
[1mStep[0m  [6/21], [94mLoss[0m : 10.70828
[1mStep[0m  [8/21], [94mLoss[0m : 10.64803
[1mStep[0m  [10/21], [94mLoss[0m : 10.56189
[1mStep[0m  [12/21], [94mLoss[0m : 10.57905
[1mStep[0m  [14/21], [94mLoss[0m : 10.89571
[1mStep[0m  [16/21], [94mLoss[0m : 10.84387
[1mStep[0m  [18/21], [94mLoss[0m : 10.55673
[1mStep[0m  [20/21], [94mLoss[0m : 10.69936

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.710, [92mTest[0m: 10.671, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70412
[1mStep[0m  [2/21], [94mLoss[0m : 10.69004
[1mStep[0m  [4/21], [94mLoss[0m : 10.70130
[1mStep[0m  [6/21], [94mLoss[0m : 10.76186
[1mStep[0m  [8/21], [94mLoss[0m : 10.75519
[1mStep[0m  [10/21], [94mLoss[0m : 10.68233
[1mStep[0m  [12/21], [94mLoss[0m : 10.65559
[1mStep[0m  [14/21], [94mLoss[0m : 10.66770
[1mStep[0m  [16/21], [94mLoss[0m : 10.56900
[1mStep[0m  [18/21], [94mLoss[0m : 10.69869
[1mStep[0m  [20/21], [94mLoss[0m : 10.75994

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.700, [92mTest[0m: 10.668, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.48621
[1mStep[0m  [2/21], [94mLoss[0m : 10.83213
[1mStep[0m  [4/21], [94mLoss[0m : 10.80710
[1mStep[0m  [6/21], [94mLoss[0m : 10.52066
[1mStep[0m  [8/21], [94mLoss[0m : 10.30556
[1mStep[0m  [10/21], [94mLoss[0m : 10.89444
[1mStep[0m  [12/21], [94mLoss[0m : 10.52206
[1mStep[0m  [14/21], [94mLoss[0m : 10.86642
[1mStep[0m  [16/21], [94mLoss[0m : 10.89060
[1mStep[0m  [18/21], [94mLoss[0m : 10.83322
[1mStep[0m  [20/21], [94mLoss[0m : 10.68246

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.690, [92mTest[0m: 10.678, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70451
[1mStep[0m  [2/21], [94mLoss[0m : 10.74375
[1mStep[0m  [4/21], [94mLoss[0m : 10.59832
[1mStep[0m  [6/21], [94mLoss[0m : 10.80263
[1mStep[0m  [8/21], [94mLoss[0m : 10.60082
[1mStep[0m  [10/21], [94mLoss[0m : 10.60490
[1mStep[0m  [12/21], [94mLoss[0m : 10.74352
[1mStep[0m  [14/21], [94mLoss[0m : 10.83646
[1mStep[0m  [16/21], [94mLoss[0m : 10.79023
[1mStep[0m  [18/21], [94mLoss[0m : 10.45730
[1mStep[0m  [20/21], [94mLoss[0m : 10.75843

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.691, [92mTest[0m: 10.653, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.66335
[1mStep[0m  [2/21], [94mLoss[0m : 10.73649
[1mStep[0m  [4/21], [94mLoss[0m : 10.55472
[1mStep[0m  [6/21], [94mLoss[0m : 10.74829
[1mStep[0m  [8/21], [94mLoss[0m : 10.68505
[1mStep[0m  [10/21], [94mLoss[0m : 10.68043
[1mStep[0m  [12/21], [94mLoss[0m : 10.84642
[1mStep[0m  [14/21], [94mLoss[0m : 10.52047
[1mStep[0m  [16/21], [94mLoss[0m : 10.48503
[1mStep[0m  [18/21], [94mLoss[0m : 10.57291
[1mStep[0m  [20/21], [94mLoss[0m : 10.70981

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.686, [92mTest[0m: 10.645, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.99741
[1mStep[0m  [2/21], [94mLoss[0m : 10.75921
[1mStep[0m  [4/21], [94mLoss[0m : 10.85605
[1mStep[0m  [6/21], [94mLoss[0m : 10.69737
[1mStep[0m  [8/21], [94mLoss[0m : 10.43790
[1mStep[0m  [10/21], [94mLoss[0m : 10.82377
[1mStep[0m  [12/21], [94mLoss[0m : 10.54632
[1mStep[0m  [14/21], [94mLoss[0m : 10.61339
[1mStep[0m  [16/21], [94mLoss[0m : 10.54114
[1mStep[0m  [18/21], [94mLoss[0m : 10.64944
[1mStep[0m  [20/21], [94mLoss[0m : 10.31177

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.671, [92mTest[0m: 10.631, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.72327
[1mStep[0m  [2/21], [94mLoss[0m : 10.69637
[1mStep[0m  [4/21], [94mLoss[0m : 10.74123
[1mStep[0m  [6/21], [94mLoss[0m : 10.57390
[1mStep[0m  [8/21], [94mLoss[0m : 10.66748
[1mStep[0m  [10/21], [94mLoss[0m : 10.45469
[1mStep[0m  [12/21], [94mLoss[0m : 10.78569
[1mStep[0m  [14/21], [94mLoss[0m : 10.94075
[1mStep[0m  [16/21], [94mLoss[0m : 10.72647
[1mStep[0m  [18/21], [94mLoss[0m : 10.51296
[1mStep[0m  [20/21], [94mLoss[0m : 10.57474

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.671, [92mTest[0m: 10.619, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.620
====================================

Phase 1 - Evaluation MAE:  10.619508607046944
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/21], [94mLoss[0m : 10.70294
[1mStep[0m  [2/21], [94mLoss[0m : 10.86528
[1mStep[0m  [4/21], [94mLoss[0m : 10.83306
[1mStep[0m  [6/21], [94mLoss[0m : 10.70205
[1mStep[0m  [8/21], [94mLoss[0m : 10.60910
[1mStep[0m  [10/21], [94mLoss[0m : 10.69750
[1mStep[0m  [12/21], [94mLoss[0m : 10.71030
[1mStep[0m  [14/21], [94mLoss[0m : 10.48067
[1mStep[0m  [16/21], [94mLoss[0m : 10.57815
[1mStep[0m  [18/21], [94mLoss[0m : 10.90802
[1mStep[0m  [20/21], [94mLoss[0m : 10.67967

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.658, [92mTest[0m: 10.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.63214
[1mStep[0m  [2/21], [94mLoss[0m : 10.47293
[1mStep[0m  [4/21], [94mLoss[0m : 10.73797
[1mStep[0m  [6/21], [94mLoss[0m : 10.73077
[1mStep[0m  [8/21], [94mLoss[0m : 10.88439
[1mStep[0m  [10/21], [94mLoss[0m : 10.54433
[1mStep[0m  [12/21], [94mLoss[0m : 10.67377
[1mStep[0m  [14/21], [94mLoss[0m : 10.84368
[1mStep[0m  [16/21], [94mLoss[0m : 10.64386
[1mStep[0m  [18/21], [94mLoss[0m : 10.49473
[1mStep[0m  [20/21], [94mLoss[0m : 10.32560

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.654, [92mTest[0m: 10.607, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.90254
[1mStep[0m  [2/21], [94mLoss[0m : 10.73450
[1mStep[0m  [4/21], [94mLoss[0m : 10.77007
[1mStep[0m  [6/21], [94mLoss[0m : 10.90671
[1mStep[0m  [8/21], [94mLoss[0m : 10.81708
[1mStep[0m  [10/21], [94mLoss[0m : 10.53881
[1mStep[0m  [12/21], [94mLoss[0m : 10.41593
[1mStep[0m  [14/21], [94mLoss[0m : 10.56755
[1mStep[0m  [16/21], [94mLoss[0m : 10.34152
[1mStep[0m  [18/21], [94mLoss[0m : 10.76529
[1mStep[0m  [20/21], [94mLoss[0m : 10.65488

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.641, [92mTest[0m: 10.601, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70258
[1mStep[0m  [2/21], [94mLoss[0m : 10.70799
[1mStep[0m  [4/21], [94mLoss[0m : 10.70426
[1mStep[0m  [6/21], [94mLoss[0m : 10.83484
[1mStep[0m  [8/21], [94mLoss[0m : 10.49637
[1mStep[0m  [10/21], [94mLoss[0m : 10.64716
[1mStep[0m  [12/21], [94mLoss[0m : 10.74364
[1mStep[0m  [14/21], [94mLoss[0m : 10.77032
[1mStep[0m  [16/21], [94mLoss[0m : 10.58706
[1mStep[0m  [18/21], [94mLoss[0m : 10.55472
[1mStep[0m  [20/21], [94mLoss[0m : 10.60255

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.631, [92mTest[0m: 10.580, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.67876
[1mStep[0m  [2/21], [94mLoss[0m : 10.75603
[1mStep[0m  [4/21], [94mLoss[0m : 10.46201
[1mStep[0m  [6/21], [94mLoss[0m : 10.69166
[1mStep[0m  [8/21], [94mLoss[0m : 10.30100
[1mStep[0m  [10/21], [94mLoss[0m : 10.74366
[1mStep[0m  [12/21], [94mLoss[0m : 10.72884
[1mStep[0m  [14/21], [94mLoss[0m : 10.89439
[1mStep[0m  [16/21], [94mLoss[0m : 10.36156
[1mStep[0m  [18/21], [94mLoss[0m : 10.57765
[1mStep[0m  [20/21], [94mLoss[0m : 10.50974

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.624, [92mTest[0m: 10.577, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70066
[1mStep[0m  [2/21], [94mLoss[0m : 10.49071
[1mStep[0m  [4/21], [94mLoss[0m : 10.80501
[1mStep[0m  [6/21], [94mLoss[0m : 10.25105
[1mStep[0m  [8/21], [94mLoss[0m : 10.44668
[1mStep[0m  [10/21], [94mLoss[0m : 10.64095
[1mStep[0m  [12/21], [94mLoss[0m : 10.65545
[1mStep[0m  [14/21], [94mLoss[0m : 10.52255
[1mStep[0m  [16/21], [94mLoss[0m : 10.53898
[1mStep[0m  [18/21], [94mLoss[0m : 10.68063
[1mStep[0m  [20/21], [94mLoss[0m : 10.65735

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.615, [92mTest[0m: 10.550, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.38005
[1mStep[0m  [2/21], [94mLoss[0m : 10.61641
[1mStep[0m  [4/21], [94mLoss[0m : 10.61276
[1mStep[0m  [6/21], [94mLoss[0m : 10.83278
[1mStep[0m  [8/21], [94mLoss[0m : 10.89232
[1mStep[0m  [10/21], [94mLoss[0m : 10.73841
[1mStep[0m  [12/21], [94mLoss[0m : 10.56225
[1mStep[0m  [14/21], [94mLoss[0m : 10.38912
[1mStep[0m  [16/21], [94mLoss[0m : 10.52957
[1mStep[0m  [18/21], [94mLoss[0m : 10.69867
[1mStep[0m  [20/21], [94mLoss[0m : 10.32476

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.605, [92mTest[0m: 10.543, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.73059
[1mStep[0m  [2/21], [94mLoss[0m : 10.66924
[1mStep[0m  [4/21], [94mLoss[0m : 10.60569
[1mStep[0m  [6/21], [94mLoss[0m : 10.33571
[1mStep[0m  [8/21], [94mLoss[0m : 10.73614
[1mStep[0m  [10/21], [94mLoss[0m : 10.42007
[1mStep[0m  [12/21], [94mLoss[0m : 10.77348
[1mStep[0m  [14/21], [94mLoss[0m : 10.56837
[1mStep[0m  [16/21], [94mLoss[0m : 10.48705
[1mStep[0m  [18/21], [94mLoss[0m : 10.46923
[1mStep[0m  [20/21], [94mLoss[0m : 10.54062

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.590, [92mTest[0m: 10.533, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.78829
[1mStep[0m  [2/21], [94mLoss[0m : 10.72823
[1mStep[0m  [4/21], [94mLoss[0m : 10.50236
[1mStep[0m  [6/21], [94mLoss[0m : 10.60669
[1mStep[0m  [8/21], [94mLoss[0m : 10.59067
[1mStep[0m  [10/21], [94mLoss[0m : 10.72074
[1mStep[0m  [12/21], [94mLoss[0m : 10.51152
[1mStep[0m  [14/21], [94mLoss[0m : 10.60544
[1mStep[0m  [16/21], [94mLoss[0m : 10.38790
[1mStep[0m  [18/21], [94mLoss[0m : 10.64643
[1mStep[0m  [20/21], [94mLoss[0m : 10.50694

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.582, [92mTest[0m: 10.509, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.58827
[1mStep[0m  [2/21], [94mLoss[0m : 10.18680
[1mStep[0m  [4/21], [94mLoss[0m : 10.59089
[1mStep[0m  [6/21], [94mLoss[0m : 10.50378
[1mStep[0m  [8/21], [94mLoss[0m : 10.53228
[1mStep[0m  [10/21], [94mLoss[0m : 10.47375
[1mStep[0m  [12/21], [94mLoss[0m : 10.48660
[1mStep[0m  [14/21], [94mLoss[0m : 10.78243
[1mStep[0m  [16/21], [94mLoss[0m : 10.70497
[1mStep[0m  [18/21], [94mLoss[0m : 10.80486
[1mStep[0m  [20/21], [94mLoss[0m : 10.63218

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.565, [92mTest[0m: 10.490, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.39175
[1mStep[0m  [2/21], [94mLoss[0m : 10.55870
[1mStep[0m  [4/21], [94mLoss[0m : 10.59284
[1mStep[0m  [6/21], [94mLoss[0m : 10.42270
[1mStep[0m  [8/21], [94mLoss[0m : 10.64627
[1mStep[0m  [10/21], [94mLoss[0m : 10.55216
[1mStep[0m  [12/21], [94mLoss[0m : 10.73209
[1mStep[0m  [14/21], [94mLoss[0m : 10.62415
[1mStep[0m  [16/21], [94mLoss[0m : 10.64963
[1mStep[0m  [18/21], [94mLoss[0m : 10.54878
[1mStep[0m  [20/21], [94mLoss[0m : 10.66630

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.548, [92mTest[0m: 10.479, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.44654
[1mStep[0m  [2/21], [94mLoss[0m : 10.38588
[1mStep[0m  [4/21], [94mLoss[0m : 10.75703
[1mStep[0m  [6/21], [94mLoss[0m : 10.28255
[1mStep[0m  [8/21], [94mLoss[0m : 10.48728
[1mStep[0m  [10/21], [94mLoss[0m : 10.45425
[1mStep[0m  [12/21], [94mLoss[0m : 10.36002
[1mStep[0m  [14/21], [94mLoss[0m : 10.59654
[1mStep[0m  [16/21], [94mLoss[0m : 10.74252
[1mStep[0m  [18/21], [94mLoss[0m : 10.52291
[1mStep[0m  [20/21], [94mLoss[0m : 10.76171

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.537, [92mTest[0m: 10.459, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.76758
[1mStep[0m  [2/21], [94mLoss[0m : 10.47503
[1mStep[0m  [4/21], [94mLoss[0m : 10.63345
[1mStep[0m  [6/21], [94mLoss[0m : 10.66293
[1mStep[0m  [8/21], [94mLoss[0m : 10.28374
[1mStep[0m  [10/21], [94mLoss[0m : 10.48178
[1mStep[0m  [12/21], [94mLoss[0m : 10.67509
[1mStep[0m  [14/21], [94mLoss[0m : 10.62584
[1mStep[0m  [16/21], [94mLoss[0m : 10.36050
[1mStep[0m  [18/21], [94mLoss[0m : 10.64005
[1mStep[0m  [20/21], [94mLoss[0m : 10.23279

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.528, [92mTest[0m: 10.473, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.46565
[1mStep[0m  [2/21], [94mLoss[0m : 10.45438
[1mStep[0m  [4/21], [94mLoss[0m : 10.47989
[1mStep[0m  [6/21], [94mLoss[0m : 10.44722
[1mStep[0m  [8/21], [94mLoss[0m : 10.87023
[1mStep[0m  [10/21], [94mLoss[0m : 10.56524
[1mStep[0m  [12/21], [94mLoss[0m : 10.40525
[1mStep[0m  [14/21], [94mLoss[0m : 10.56659
[1mStep[0m  [16/21], [94mLoss[0m : 10.37275
[1mStep[0m  [18/21], [94mLoss[0m : 10.53403
[1mStep[0m  [20/21], [94mLoss[0m : 10.37182

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.520, [92mTest[0m: 10.444, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62679
[1mStep[0m  [2/21], [94mLoss[0m : 10.57538
[1mStep[0m  [4/21], [94mLoss[0m : 10.15529
[1mStep[0m  [6/21], [94mLoss[0m : 10.46186
[1mStep[0m  [8/21], [94mLoss[0m : 10.38930
[1mStep[0m  [10/21], [94mLoss[0m : 10.39992
[1mStep[0m  [12/21], [94mLoss[0m : 10.81022
[1mStep[0m  [14/21], [94mLoss[0m : 10.62130
[1mStep[0m  [16/21], [94mLoss[0m : 10.55034
[1mStep[0m  [18/21], [94mLoss[0m : 10.33535
[1mStep[0m  [20/21], [94mLoss[0m : 10.83446

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.505, [92mTest[0m: 10.434, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.41910
[1mStep[0m  [2/21], [94mLoss[0m : 10.51826
[1mStep[0m  [4/21], [94mLoss[0m : 10.48277
[1mStep[0m  [6/21], [94mLoss[0m : 10.43013
[1mStep[0m  [8/21], [94mLoss[0m : 10.30585
[1mStep[0m  [10/21], [94mLoss[0m : 10.50082
[1mStep[0m  [12/21], [94mLoss[0m : 10.42635
[1mStep[0m  [14/21], [94mLoss[0m : 10.64019
[1mStep[0m  [16/21], [94mLoss[0m : 10.51493
[1mStep[0m  [18/21], [94mLoss[0m : 10.52471
[1mStep[0m  [20/21], [94mLoss[0m : 10.66510

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.488, [92mTest[0m: 10.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.70293
[1mStep[0m  [2/21], [94mLoss[0m : 10.70961
[1mStep[0m  [4/21], [94mLoss[0m : 10.70662
[1mStep[0m  [6/21], [94mLoss[0m : 10.89947
[1mStep[0m  [8/21], [94mLoss[0m : 10.34802
[1mStep[0m  [10/21], [94mLoss[0m : 10.40101
[1mStep[0m  [12/21], [94mLoss[0m : 10.29340
[1mStep[0m  [14/21], [94mLoss[0m : 10.31811
[1mStep[0m  [16/21], [94mLoss[0m : 10.66950
[1mStep[0m  [18/21], [94mLoss[0m : 10.24371
[1mStep[0m  [20/21], [94mLoss[0m : 10.30340

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.482, [92mTest[0m: 10.412, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.62783
[1mStep[0m  [2/21], [94mLoss[0m : 10.24259
[1mStep[0m  [4/21], [94mLoss[0m : 10.47439
[1mStep[0m  [6/21], [94mLoss[0m : 10.58415
[1mStep[0m  [8/21], [94mLoss[0m : 10.51101
[1mStep[0m  [10/21], [94mLoss[0m : 10.61760
[1mStep[0m  [12/21], [94mLoss[0m : 10.53382
[1mStep[0m  [14/21], [94mLoss[0m : 10.57901
[1mStep[0m  [16/21], [94mLoss[0m : 10.39054
[1mStep[0m  [18/21], [94mLoss[0m : 10.27448
[1mStep[0m  [20/21], [94mLoss[0m : 10.49117

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.461, [92mTest[0m: 10.377, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.28497
[1mStep[0m  [2/21], [94mLoss[0m : 10.54006
[1mStep[0m  [4/21], [94mLoss[0m : 10.52734
[1mStep[0m  [6/21], [94mLoss[0m : 10.39536
[1mStep[0m  [8/21], [94mLoss[0m : 10.54337
[1mStep[0m  [10/21], [94mLoss[0m : 10.65922
[1mStep[0m  [12/21], [94mLoss[0m : 10.48601
[1mStep[0m  [14/21], [94mLoss[0m : 10.52357
[1mStep[0m  [16/21], [94mLoss[0m : 10.66084
[1mStep[0m  [18/21], [94mLoss[0m : 10.56066
[1mStep[0m  [20/21], [94mLoss[0m : 10.24924

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 10.447, [92mTest[0m: 10.372, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.47438
[1mStep[0m  [2/21], [94mLoss[0m : 10.64320
[1mStep[0m  [4/21], [94mLoss[0m : 10.53298
[1mStep[0m  [6/21], [94mLoss[0m : 10.45896
[1mStep[0m  [8/21], [94mLoss[0m : 10.53005
[1mStep[0m  [10/21], [94mLoss[0m : 10.44075
[1mStep[0m  [12/21], [94mLoss[0m : 10.53636
[1mStep[0m  [14/21], [94mLoss[0m : 10.67301
[1mStep[0m  [16/21], [94mLoss[0m : 10.04057
[1mStep[0m  [18/21], [94mLoss[0m : 10.50455
[1mStep[0m  [20/21], [94mLoss[0m : 10.26036

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 10.436, [92mTest[0m: 10.363, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.27437
[1mStep[0m  [2/21], [94mLoss[0m : 10.34578
[1mStep[0m  [4/21], [94mLoss[0m : 10.38728
[1mStep[0m  [6/21], [94mLoss[0m : 10.47920
[1mStep[0m  [8/21], [94mLoss[0m : 10.27745
[1mStep[0m  [10/21], [94mLoss[0m : 10.43516
[1mStep[0m  [12/21], [94mLoss[0m : 10.68858
[1mStep[0m  [14/21], [94mLoss[0m : 10.24429
[1mStep[0m  [16/21], [94mLoss[0m : 10.18138
[1mStep[0m  [18/21], [94mLoss[0m : 10.26043
[1mStep[0m  [20/21], [94mLoss[0m : 10.37460

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 10.421, [92mTest[0m: 10.356, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.52406
[1mStep[0m  [2/21], [94mLoss[0m : 10.42790
[1mStep[0m  [4/21], [94mLoss[0m : 10.36815
[1mStep[0m  [6/21], [94mLoss[0m : 10.53005
[1mStep[0m  [8/21], [94mLoss[0m : 10.36510
[1mStep[0m  [10/21], [94mLoss[0m : 10.23124
[1mStep[0m  [12/21], [94mLoss[0m : 10.41035
[1mStep[0m  [14/21], [94mLoss[0m : 10.35982
[1mStep[0m  [16/21], [94mLoss[0m : 10.40652
[1mStep[0m  [18/21], [94mLoss[0m : 10.46694
[1mStep[0m  [20/21], [94mLoss[0m : 10.40295

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 10.399, [92mTest[0m: 10.333, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.43144
[1mStep[0m  [2/21], [94mLoss[0m : 10.81965
[1mStep[0m  [4/21], [94mLoss[0m : 10.35692
[1mStep[0m  [6/21], [94mLoss[0m : 10.27613
[1mStep[0m  [8/21], [94mLoss[0m : 10.28199
[1mStep[0m  [10/21], [94mLoss[0m : 10.42855
[1mStep[0m  [12/21], [94mLoss[0m : 10.53981
[1mStep[0m  [14/21], [94mLoss[0m : 10.31104
[1mStep[0m  [16/21], [94mLoss[0m : 10.18269
[1mStep[0m  [18/21], [94mLoss[0m : 10.55277
[1mStep[0m  [20/21], [94mLoss[0m : 10.62280

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 10.387, [92mTest[0m: 10.325, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.49102
[1mStep[0m  [2/21], [94mLoss[0m : 10.18282
[1mStep[0m  [4/21], [94mLoss[0m : 10.35324
[1mStep[0m  [6/21], [94mLoss[0m : 10.35864
[1mStep[0m  [8/21], [94mLoss[0m : 10.54168
[1mStep[0m  [10/21], [94mLoss[0m : 10.49146
[1mStep[0m  [12/21], [94mLoss[0m : 10.24154
[1mStep[0m  [14/21], [94mLoss[0m : 10.18284
[1mStep[0m  [16/21], [94mLoss[0m : 10.32886
[1mStep[0m  [18/21], [94mLoss[0m : 10.39548
[1mStep[0m  [20/21], [94mLoss[0m : 10.43694

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 10.377, [92mTest[0m: 10.301, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.41804
[1mStep[0m  [2/21], [94mLoss[0m : 10.35376
[1mStep[0m  [4/21], [94mLoss[0m : 10.18550
[1mStep[0m  [6/21], [94mLoss[0m : 10.34386
[1mStep[0m  [8/21], [94mLoss[0m : 10.63691
[1mStep[0m  [10/21], [94mLoss[0m : 10.12473
[1mStep[0m  [12/21], [94mLoss[0m : 10.19600
[1mStep[0m  [14/21], [94mLoss[0m : 10.23001
[1mStep[0m  [16/21], [94mLoss[0m : 10.21031
[1mStep[0m  [18/21], [94mLoss[0m : 10.11963
[1mStep[0m  [20/21], [94mLoss[0m : 10.28941

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 10.361, [92mTest[0m: 10.289, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.34360
[1mStep[0m  [2/21], [94mLoss[0m : 10.26371
[1mStep[0m  [4/21], [94mLoss[0m : 10.25662
[1mStep[0m  [6/21], [94mLoss[0m : 10.23461
[1mStep[0m  [8/21], [94mLoss[0m : 10.40158
[1mStep[0m  [10/21], [94mLoss[0m : 10.36985
[1mStep[0m  [12/21], [94mLoss[0m : 10.44370
[1mStep[0m  [14/21], [94mLoss[0m : 10.35199
[1mStep[0m  [16/21], [94mLoss[0m : 10.44180
[1mStep[0m  [18/21], [94mLoss[0m : 10.29779
[1mStep[0m  [20/21], [94mLoss[0m : 10.43685

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 10.347, [92mTest[0m: 10.251, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.45391
[1mStep[0m  [2/21], [94mLoss[0m : 10.32928
[1mStep[0m  [4/21], [94mLoss[0m : 10.53011
[1mStep[0m  [6/21], [94mLoss[0m : 10.23400
[1mStep[0m  [8/21], [94mLoss[0m : 10.56804
[1mStep[0m  [10/21], [94mLoss[0m : 10.36333
[1mStep[0m  [12/21], [94mLoss[0m : 10.17292
[1mStep[0m  [14/21], [94mLoss[0m : 10.47389
[1mStep[0m  [16/21], [94mLoss[0m : 10.04959
[1mStep[0m  [18/21], [94mLoss[0m : 10.54101
[1mStep[0m  [20/21], [94mLoss[0m : 10.40223

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 10.335, [92mTest[0m: 10.242, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.56680
[1mStep[0m  [2/21], [94mLoss[0m : 10.36055
[1mStep[0m  [4/21], [94mLoss[0m : 10.39078
[1mStep[0m  [6/21], [94mLoss[0m : 10.03417
[1mStep[0m  [8/21], [94mLoss[0m : 10.17653
[1mStep[0m  [10/21], [94mLoss[0m : 10.37258
[1mStep[0m  [12/21], [94mLoss[0m : 10.39654
[1mStep[0m  [14/21], [94mLoss[0m : 10.28204
[1mStep[0m  [16/21], [94mLoss[0m : 10.40838
[1mStep[0m  [18/21], [94mLoss[0m : 10.09798
[1mStep[0m  [20/21], [94mLoss[0m : 10.20824

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 10.313, [92mTest[0m: 10.242, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.69602
[1mStep[0m  [2/21], [94mLoss[0m : 10.38807
[1mStep[0m  [4/21], [94mLoss[0m : 10.37354
[1mStep[0m  [6/21], [94mLoss[0m : 10.30219
[1mStep[0m  [8/21], [94mLoss[0m : 10.42391
[1mStep[0m  [10/21], [94mLoss[0m : 10.54827
[1mStep[0m  [12/21], [94mLoss[0m : 10.27284
[1mStep[0m  [14/21], [94mLoss[0m : 10.39319
[1mStep[0m  [16/21], [94mLoss[0m : 10.20541
[1mStep[0m  [18/21], [94mLoss[0m : 9.94916
[1mStep[0m  [20/21], [94mLoss[0m : 10.24512

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 10.308, [92mTest[0m: 10.229, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.42346
[1mStep[0m  [2/21], [94mLoss[0m : 10.15542
[1mStep[0m  [4/21], [94mLoss[0m : 10.24961
[1mStep[0m  [6/21], [94mLoss[0m : 10.27004
[1mStep[0m  [8/21], [94mLoss[0m : 9.89905
[1mStep[0m  [10/21], [94mLoss[0m : 10.28995
[1mStep[0m  [12/21], [94mLoss[0m : 10.60687
[1mStep[0m  [14/21], [94mLoss[0m : 10.28175
[1mStep[0m  [16/21], [94mLoss[0m : 10.34013
[1mStep[0m  [18/21], [94mLoss[0m : 10.35416
[1mStep[0m  [20/21], [94mLoss[0m : 10.42083

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 10.294, [92mTest[0m: 10.195, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 10.180
====================================

Phase 2 - Evaluation MAE:  10.180475779942103
MAE score P1      10.619509
MAE score P2      10.180476
loss              10.293762
learning_rate        0.0001
batch_size              512
hidden_sizes          [300]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay           0.01
Name: 23, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.13005
[1mStep[0m  [2/21], [94mLoss[0m : 11.25318
[1mStep[0m  [4/21], [94mLoss[0m : 11.12587
[1mStep[0m  [6/21], [94mLoss[0m : 11.15243
[1mStep[0m  [8/21], [94mLoss[0m : 10.80124
[1mStep[0m  [10/21], [94mLoss[0m : 11.21866
[1mStep[0m  [12/21], [94mLoss[0m : 10.85146
[1mStep[0m  [14/21], [94mLoss[0m : 11.19290
[1mStep[0m  [16/21], [94mLoss[0m : 10.79075
[1mStep[0m  [18/21], [94mLoss[0m : 10.71434
[1mStep[0m  [20/21], [94mLoss[0m : 10.96667

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.986, [92mTest[0m: 10.931, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.74127
[1mStep[0m  [2/21], [94mLoss[0m : 10.67723
[1mStep[0m  [4/21], [94mLoss[0m : 10.73824
[1mStep[0m  [6/21], [94mLoss[0m : 10.68422
[1mStep[0m  [8/21], [94mLoss[0m : 10.80882
[1mStep[0m  [10/21], [94mLoss[0m : 10.80486
[1mStep[0m  [12/21], [94mLoss[0m : 10.83336
[1mStep[0m  [14/21], [94mLoss[0m : 10.51114
[1mStep[0m  [16/21], [94mLoss[0m : 10.65422
[1mStep[0m  [18/21], [94mLoss[0m : 10.54122
[1mStep[0m  [20/21], [94mLoss[0m : 10.74471

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.717, [92mTest[0m: 10.828, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.43987
[1mStep[0m  [2/21], [94mLoss[0m : 10.60291
[1mStep[0m  [4/21], [94mLoss[0m : 10.43727
[1mStep[0m  [6/21], [94mLoss[0m : 10.52087
[1mStep[0m  [8/21], [94mLoss[0m : 10.33782
[1mStep[0m  [10/21], [94mLoss[0m : 10.62107
[1mStep[0m  [12/21], [94mLoss[0m : 10.03976
[1mStep[0m  [14/21], [94mLoss[0m : 10.02217
[1mStep[0m  [16/21], [94mLoss[0m : 10.29092
[1mStep[0m  [18/21], [94mLoss[0m : 10.40017
[1mStep[0m  [20/21], [94mLoss[0m : 10.34222

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.396, [92mTest[0m: 10.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.05951
[1mStep[0m  [2/21], [94mLoss[0m : 10.23904
[1mStep[0m  [4/21], [94mLoss[0m : 10.29088
[1mStep[0m  [6/21], [94mLoss[0m : 10.16138
[1mStep[0m  [8/21], [94mLoss[0m : 10.20312
[1mStep[0m  [10/21], [94mLoss[0m : 9.98228
[1mStep[0m  [12/21], [94mLoss[0m : 10.14417
[1mStep[0m  [14/21], [94mLoss[0m : 10.07392
[1mStep[0m  [16/21], [94mLoss[0m : 9.98211
[1mStep[0m  [18/21], [94mLoss[0m : 9.85380
[1mStep[0m  [20/21], [94mLoss[0m : 10.01410

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.071, [92mTest[0m: 10.456, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.81655
[1mStep[0m  [2/21], [94mLoss[0m : 9.97337
[1mStep[0m  [4/21], [94mLoss[0m : 9.94191
[1mStep[0m  [6/21], [94mLoss[0m : 9.95076
[1mStep[0m  [8/21], [94mLoss[0m : 9.86685
[1mStep[0m  [10/21], [94mLoss[0m : 9.73120
[1mStep[0m  [12/21], [94mLoss[0m : 9.42300
[1mStep[0m  [14/21], [94mLoss[0m : 9.38974
[1mStep[0m  [16/21], [94mLoss[0m : 9.71673
[1mStep[0m  [18/21], [94mLoss[0m : 9.56367
[1mStep[0m  [20/21], [94mLoss[0m : 9.60552

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.739, [92mTest[0m: 10.276, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.90545
[1mStep[0m  [2/21], [94mLoss[0m : 9.77827
[1mStep[0m  [4/21], [94mLoss[0m : 9.38890
[1mStep[0m  [6/21], [94mLoss[0m : 9.30585
[1mStep[0m  [8/21], [94mLoss[0m : 9.28106
[1mStep[0m  [10/21], [94mLoss[0m : 9.42652
[1mStep[0m  [12/21], [94mLoss[0m : 9.04202
[1mStep[0m  [14/21], [94mLoss[0m : 9.18049
[1mStep[0m  [16/21], [94mLoss[0m : 9.19177
[1mStep[0m  [18/21], [94mLoss[0m : 9.10137
[1mStep[0m  [20/21], [94mLoss[0m : 9.25729

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.407, [92mTest[0m: 10.051, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.99329
[1mStep[0m  [2/21], [94mLoss[0m : 9.48260
[1mStep[0m  [4/21], [94mLoss[0m : 9.50043
[1mStep[0m  [6/21], [94mLoss[0m : 9.24454
[1mStep[0m  [8/21], [94mLoss[0m : 9.01161
[1mStep[0m  [10/21], [94mLoss[0m : 9.16624
[1mStep[0m  [12/21], [94mLoss[0m : 9.11152
[1mStep[0m  [14/21], [94mLoss[0m : 8.74576
[1mStep[0m  [16/21], [94mLoss[0m : 9.09204
[1mStep[0m  [18/21], [94mLoss[0m : 9.12136
[1mStep[0m  [20/21], [94mLoss[0m : 9.02430

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 9.078, [92mTest[0m: 9.871, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.75180
[1mStep[0m  [2/21], [94mLoss[0m : 9.01276
[1mStep[0m  [4/21], [94mLoss[0m : 8.72944
[1mStep[0m  [6/21], [94mLoss[0m : 8.67799
[1mStep[0m  [8/21], [94mLoss[0m : 8.68473
[1mStep[0m  [10/21], [94mLoss[0m : 8.80134
[1mStep[0m  [12/21], [94mLoss[0m : 8.74234
[1mStep[0m  [14/21], [94mLoss[0m : 8.66587
[1mStep[0m  [16/21], [94mLoss[0m : 8.75192
[1mStep[0m  [18/21], [94mLoss[0m : 8.35310
[1mStep[0m  [20/21], [94mLoss[0m : 8.75070

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.735, [92mTest[0m: 9.649, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.47048
[1mStep[0m  [2/21], [94mLoss[0m : 8.51848
[1mStep[0m  [4/21], [94mLoss[0m : 8.36075
[1mStep[0m  [6/21], [94mLoss[0m : 8.34652
[1mStep[0m  [8/21], [94mLoss[0m : 8.35921
[1mStep[0m  [10/21], [94mLoss[0m : 8.83076
[1mStep[0m  [12/21], [94mLoss[0m : 8.37499
[1mStep[0m  [14/21], [94mLoss[0m : 8.32917
[1mStep[0m  [16/21], [94mLoss[0m : 8.18283
[1mStep[0m  [18/21], [94mLoss[0m : 8.39973
[1mStep[0m  [20/21], [94mLoss[0m : 8.15248

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.414, [92mTest[0m: 9.445, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.27453
[1mStep[0m  [2/21], [94mLoss[0m : 8.19207
[1mStep[0m  [4/21], [94mLoss[0m : 8.15171
[1mStep[0m  [6/21], [94mLoss[0m : 8.37527
[1mStep[0m  [8/21], [94mLoss[0m : 7.91172
[1mStep[0m  [10/21], [94mLoss[0m : 7.95713
[1mStep[0m  [12/21], [94mLoss[0m : 8.13974
[1mStep[0m  [14/21], [94mLoss[0m : 7.86151
[1mStep[0m  [16/21], [94mLoss[0m : 8.05355
[1mStep[0m  [18/21], [94mLoss[0m : 8.02891
[1mStep[0m  [20/21], [94mLoss[0m : 8.23781

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.081, [92mTest[0m: 9.245, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.14912
[1mStep[0m  [2/21], [94mLoss[0m : 7.87113
[1mStep[0m  [4/21], [94mLoss[0m : 7.64997
[1mStep[0m  [6/21], [94mLoss[0m : 7.72459
[1mStep[0m  [8/21], [94mLoss[0m : 7.70169
[1mStep[0m  [10/21], [94mLoss[0m : 7.69413
[1mStep[0m  [12/21], [94mLoss[0m : 7.72117
[1mStep[0m  [14/21], [94mLoss[0m : 7.43869
[1mStep[0m  [16/21], [94mLoss[0m : 7.42949
[1mStep[0m  [18/21], [94mLoss[0m : 7.84305
[1mStep[0m  [20/21], [94mLoss[0m : 7.95789

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 7.756, [92mTest[0m: 9.035, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.81920
[1mStep[0m  [2/21], [94mLoss[0m : 7.41079
[1mStep[0m  [4/21], [94mLoss[0m : 7.60579
[1mStep[0m  [6/21], [94mLoss[0m : 7.38855
[1mStep[0m  [8/21], [94mLoss[0m : 7.42491
[1mStep[0m  [10/21], [94mLoss[0m : 7.38233
[1mStep[0m  [12/21], [94mLoss[0m : 7.42573
[1mStep[0m  [14/21], [94mLoss[0m : 7.66598
[1mStep[0m  [16/21], [94mLoss[0m : 7.16281
[1mStep[0m  [18/21], [94mLoss[0m : 7.26891
[1mStep[0m  [20/21], [94mLoss[0m : 7.59219

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 7.430, [92mTest[0m: 8.837, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.21775
[1mStep[0m  [2/21], [94mLoss[0m : 6.85555
[1mStep[0m  [4/21], [94mLoss[0m : 7.51644
[1mStep[0m  [6/21], [94mLoss[0m : 7.57435
[1mStep[0m  [8/21], [94mLoss[0m : 7.04822
[1mStep[0m  [10/21], [94mLoss[0m : 6.94680
[1mStep[0m  [12/21], [94mLoss[0m : 7.11622
[1mStep[0m  [14/21], [94mLoss[0m : 7.24973
[1mStep[0m  [16/21], [94mLoss[0m : 7.20565
[1mStep[0m  [18/21], [94mLoss[0m : 6.76806
[1mStep[0m  [20/21], [94mLoss[0m : 7.04412

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.104, [92mTest[0m: 8.621, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.03614
[1mStep[0m  [2/21], [94mLoss[0m : 6.96422
[1mStep[0m  [4/21], [94mLoss[0m : 6.85824
[1mStep[0m  [6/21], [94mLoss[0m : 7.01394
[1mStep[0m  [8/21], [94mLoss[0m : 6.74244
[1mStep[0m  [10/21], [94mLoss[0m : 6.75454
[1mStep[0m  [12/21], [94mLoss[0m : 6.80378
[1mStep[0m  [14/21], [94mLoss[0m : 6.60251
[1mStep[0m  [16/21], [94mLoss[0m : 6.45562
[1mStep[0m  [18/21], [94mLoss[0m : 6.67509
[1mStep[0m  [20/21], [94mLoss[0m : 6.65253

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.811, [92mTest[0m: 8.430, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 13 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.202
====================================

Phase 1 - Evaluation MAE:  8.201925277709961
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 6.55918
[1mStep[0m  [2/21], [94mLoss[0m : 6.49894
[1mStep[0m  [4/21], [94mLoss[0m : 6.73752
[1mStep[0m  [6/21], [94mLoss[0m : 6.58624
[1mStep[0m  [8/21], [94mLoss[0m : 6.64231
[1mStep[0m  [10/21], [94mLoss[0m : 6.64910
[1mStep[0m  [12/21], [94mLoss[0m : 6.40603
[1mStep[0m  [14/21], [94mLoss[0m : 6.20983
[1mStep[0m  [16/21], [94mLoss[0m : 6.74689
[1mStep[0m  [18/21], [94mLoss[0m : 6.74397
[1mStep[0m  [20/21], [94mLoss[0m : 6.42680

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.566, [92mTest[0m: 8.207, [96mlr[0m: 0.0001
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 0 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.048
====================================

Phase 2 - Evaluation MAE:  8.047539779118129
MAE score P1        8.201925
MAE score P2         8.04754
loss                6.566472
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.4
momentum                 0.9
weight_decay           0.001
Name: 24, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 11.09082
[1mStep[0m  [2/21], [94mLoss[0m : 10.76099
[1mStep[0m  [4/21], [94mLoss[0m : 10.74739
[1mStep[0m  [6/21], [94mLoss[0m : 11.11528
[1mStep[0m  [8/21], [94mLoss[0m : 10.63926
[1mStep[0m  [10/21], [94mLoss[0m : 10.72618
[1mStep[0m  [12/21], [94mLoss[0m : 10.66495
[1mStep[0m  [14/21], [94mLoss[0m : 10.94468
[1mStep[0m  [16/21], [94mLoss[0m : 10.90171
[1mStep[0m  [18/21], [94mLoss[0m : 10.63655
[1mStep[0m  [20/21], [94mLoss[0m : 10.64449

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.771, [92mTest[0m: 10.868, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.50794
[1mStep[0m  [2/21], [94mLoss[0m : 10.30476
[1mStep[0m  [4/21], [94mLoss[0m : 10.50452
[1mStep[0m  [6/21], [94mLoss[0m : 10.51322
[1mStep[0m  [8/21], [94mLoss[0m : 10.28191
[1mStep[0m  [10/21], [94mLoss[0m : 10.52415
[1mStep[0m  [12/21], [94mLoss[0m : 10.22807
[1mStep[0m  [14/21], [94mLoss[0m : 10.33531
[1mStep[0m  [16/21], [94mLoss[0m : 10.00751
[1mStep[0m  [18/21], [94mLoss[0m : 10.25207
[1mStep[0m  [20/21], [94mLoss[0m : 10.04471

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.313, [92mTest[0m: 10.537, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 10.28709
[1mStep[0m  [2/21], [94mLoss[0m : 10.02899
[1mStep[0m  [4/21], [94mLoss[0m : 9.96511
[1mStep[0m  [6/21], [94mLoss[0m : 9.91989
[1mStep[0m  [8/21], [94mLoss[0m : 9.77566
[1mStep[0m  [10/21], [94mLoss[0m : 9.61929
[1mStep[0m  [12/21], [94mLoss[0m : 9.90761
[1mStep[0m  [14/21], [94mLoss[0m : 9.75850
[1mStep[0m  [16/21], [94mLoss[0m : 9.63651
[1mStep[0m  [18/21], [94mLoss[0m : 9.58989
[1mStep[0m  [20/21], [94mLoss[0m : 9.61985

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.770, [92mTest[0m: 10.031, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 9.50506
[1mStep[0m  [2/21], [94mLoss[0m : 9.25331
[1mStep[0m  [4/21], [94mLoss[0m : 9.71750
[1mStep[0m  [6/21], [94mLoss[0m : 9.42903
[1mStep[0m  [8/21], [94mLoss[0m : 9.55332
[1mStep[0m  [10/21], [94mLoss[0m : 9.22384
[1mStep[0m  [12/21], [94mLoss[0m : 9.13230
[1mStep[0m  [14/21], [94mLoss[0m : 8.98546
[1mStep[0m  [16/21], [94mLoss[0m : 8.91455
[1mStep[0m  [18/21], [94mLoss[0m : 8.97033
[1mStep[0m  [20/21], [94mLoss[0m : 8.80733

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.229, [92mTest[0m: 9.484, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.97334
[1mStep[0m  [2/21], [94mLoss[0m : 9.19171
[1mStep[0m  [4/21], [94mLoss[0m : 8.81973
[1mStep[0m  [6/21], [94mLoss[0m : 8.92938
[1mStep[0m  [8/21], [94mLoss[0m : 8.81380
[1mStep[0m  [10/21], [94mLoss[0m : 8.85696
[1mStep[0m  [12/21], [94mLoss[0m : 8.65247
[1mStep[0m  [14/21], [94mLoss[0m : 8.89902
[1mStep[0m  [16/21], [94mLoss[0m : 8.18891
[1mStep[0m  [18/21], [94mLoss[0m : 8.34424
[1mStep[0m  [20/21], [94mLoss[0m : 8.46937

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.683, [92mTest[0m: 8.932, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.49928
[1mStep[0m  [2/21], [94mLoss[0m : 8.29633
[1mStep[0m  [4/21], [94mLoss[0m : 8.45021
[1mStep[0m  [6/21], [94mLoss[0m : 8.05919
[1mStep[0m  [8/21], [94mLoss[0m : 8.05165
[1mStep[0m  [10/21], [94mLoss[0m : 7.99940
[1mStep[0m  [12/21], [94mLoss[0m : 8.24896
[1mStep[0m  [14/21], [94mLoss[0m : 8.03040
[1mStep[0m  [16/21], [94mLoss[0m : 7.78856
[1mStep[0m  [18/21], [94mLoss[0m : 8.05797
[1mStep[0m  [20/21], [94mLoss[0m : 8.18750

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.137, [92mTest[0m: 8.397, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 8.02861
[1mStep[0m  [2/21], [94mLoss[0m : 7.58523
[1mStep[0m  [4/21], [94mLoss[0m : 7.77123
[1mStep[0m  [6/21], [94mLoss[0m : 7.53955
[1mStep[0m  [8/21], [94mLoss[0m : 7.47487
[1mStep[0m  [10/21], [94mLoss[0m : 7.73184
[1mStep[0m  [12/21], [94mLoss[0m : 7.51102
[1mStep[0m  [14/21], [94mLoss[0m : 7.70786
[1mStep[0m  [16/21], [94mLoss[0m : 7.34511
[1mStep[0m  [18/21], [94mLoss[0m : 7.26237
[1mStep[0m  [20/21], [94mLoss[0m : 7.36235

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.592, [92mTest[0m: 7.845, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 7.24720
[1mStep[0m  [2/21], [94mLoss[0m : 7.35940
[1mStep[0m  [4/21], [94mLoss[0m : 7.31020
[1mStep[0m  [6/21], [94mLoss[0m : 7.01154
[1mStep[0m  [8/21], [94mLoss[0m : 7.21157
[1mStep[0m  [10/21], [94mLoss[0m : 6.95602
[1mStep[0m  [12/21], [94mLoss[0m : 7.17085
[1mStep[0m  [14/21], [94mLoss[0m : 7.17347
[1mStep[0m  [16/21], [94mLoss[0m : 6.86586
[1mStep[0m  [18/21], [94mLoss[0m : 6.85410
[1mStep[0m  [20/21], [94mLoss[0m : 6.79307

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.042, [92mTest[0m: 7.294, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.68474
[1mStep[0m  [2/21], [94mLoss[0m : 6.38711
[1mStep[0m  [4/21], [94mLoss[0m : 6.68948
[1mStep[0m  [6/21], [94mLoss[0m : 6.48648
[1mStep[0m  [8/21], [94mLoss[0m : 6.63103
[1mStep[0m  [10/21], [94mLoss[0m : 6.22431
[1mStep[0m  [12/21], [94mLoss[0m : 6.41903
[1mStep[0m  [14/21], [94mLoss[0m : 6.57962
[1mStep[0m  [16/21], [94mLoss[0m : 6.46187
[1mStep[0m  [18/21], [94mLoss[0m : 6.13122
[1mStep[0m  [20/21], [94mLoss[0m : 6.16901

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.499, [92mTest[0m: 6.750, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 6.19564
[1mStep[0m  [2/21], [94mLoss[0m : 6.03364
[1mStep[0m  [4/21], [94mLoss[0m : 6.19605
[1mStep[0m  [6/21], [94mLoss[0m : 6.17103
[1mStep[0m  [8/21], [94mLoss[0m : 6.09523
[1mStep[0m  [10/21], [94mLoss[0m : 6.13922
[1mStep[0m  [12/21], [94mLoss[0m : 5.65697
[1mStep[0m  [14/21], [94mLoss[0m : 5.69298
[1mStep[0m  [16/21], [94mLoss[0m : 5.89759
[1mStep[0m  [18/21], [94mLoss[0m : 5.87733
[1mStep[0m  [20/21], [94mLoss[0m : 5.67750

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.950, [92mTest[0m: 6.207, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.62708
[1mStep[0m  [2/21], [94mLoss[0m : 5.74944
[1mStep[0m  [4/21], [94mLoss[0m : 5.67047
[1mStep[0m  [6/21], [94mLoss[0m : 5.21538
[1mStep[0m  [8/21], [94mLoss[0m : 5.73982
[1mStep[0m  [10/21], [94mLoss[0m : 5.33357
[1mStep[0m  [12/21], [94mLoss[0m : 5.20838
[1mStep[0m  [14/21], [94mLoss[0m : 5.37704
[1mStep[0m  [16/21], [94mLoss[0m : 5.18376
[1mStep[0m  [18/21], [94mLoss[0m : 5.11901
[1mStep[0m  [20/21], [94mLoss[0m : 5.11008

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.429, [92mTest[0m: 5.670, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 5.25616
[1mStep[0m  [2/21], [94mLoss[0m : 4.91508
[1mStep[0m  [4/21], [94mLoss[0m : 5.17988
[1mStep[0m  [6/21], [94mLoss[0m : 4.84410
[1mStep[0m  [8/21], [94mLoss[0m : 4.91572
[1mStep[0m  [10/21], [94mLoss[0m : 4.97152
[1mStep[0m  [12/21], [94mLoss[0m : 4.98624
[1mStep[0m  [14/21], [94mLoss[0m : 5.13212
[1mStep[0m  [16/21], [94mLoss[0m : 4.90681
[1mStep[0m  [18/21], [94mLoss[0m : 4.78645
[1mStep[0m  [20/21], [94mLoss[0m : 4.65908

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.936, [92mTest[0m: 5.160, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.86213
[1mStep[0m  [2/21], [94mLoss[0m : 4.85746
[1mStep[0m  [4/21], [94mLoss[0m : 4.49173
[1mStep[0m  [6/21], [94mLoss[0m : 4.56430
[1mStep[0m  [8/21], [94mLoss[0m : 4.46127
[1mStep[0m  [10/21], [94mLoss[0m : 4.80951
[1mStep[0m  [12/21], [94mLoss[0m : 4.03685
[1mStep[0m  [14/21], [94mLoss[0m : 4.52360
[1mStep[0m  [16/21], [94mLoss[0m : 4.30986
[1mStep[0m  [18/21], [94mLoss[0m : 4.16527
[1mStep[0m  [20/21], [94mLoss[0m : 4.28944

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.522, [92mTest[0m: 4.710, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.32662
[1mStep[0m  [2/21], [94mLoss[0m : 4.24862
[1mStep[0m  [4/21], [94mLoss[0m : 4.42230
[1mStep[0m  [6/21], [94mLoss[0m : 4.06531
[1mStep[0m  [8/21], [94mLoss[0m : 4.07688
[1mStep[0m  [10/21], [94mLoss[0m : 4.30715
[1mStep[0m  [12/21], [94mLoss[0m : 4.10723
[1mStep[0m  [14/21], [94mLoss[0m : 4.22691
[1mStep[0m  [16/21], [94mLoss[0m : 4.31743
[1mStep[0m  [18/21], [94mLoss[0m : 3.91870
[1mStep[0m  [20/21], [94mLoss[0m : 4.15685

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.171, [92mTest[0m: 4.332, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 4.21961
[1mStep[0m  [2/21], [94mLoss[0m : 4.06684
[1mStep[0m  [4/21], [94mLoss[0m : 4.06349
[1mStep[0m  [6/21], [94mLoss[0m : 4.05235
[1mStep[0m  [8/21], [94mLoss[0m : 3.87330
[1mStep[0m  [10/21], [94mLoss[0m : 3.84081
[1mStep[0m  [12/21], [94mLoss[0m : 3.94910
[1mStep[0m  [14/21], [94mLoss[0m : 3.57599
[1mStep[0m  [16/21], [94mLoss[0m : 3.75090
[1mStep[0m  [18/21], [94mLoss[0m : 3.45091
[1mStep[0m  [20/21], [94mLoss[0m : 3.80689

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.896, [92mTest[0m: 3.999, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.60735
[1mStep[0m  [2/21], [94mLoss[0m : 3.75157
[1mStep[0m  [4/21], [94mLoss[0m : 3.76420
[1mStep[0m  [6/21], [94mLoss[0m : 3.37221
[1mStep[0m  [8/21], [94mLoss[0m : 3.71011
[1mStep[0m  [10/21], [94mLoss[0m : 3.67396
[1mStep[0m  [12/21], [94mLoss[0m : 3.61670
[1mStep[0m  [14/21], [94mLoss[0m : 3.90237
[1mStep[0m  [16/21], [94mLoss[0m : 3.90544
[1mStep[0m  [18/21], [94mLoss[0m : 3.64919
[1mStep[0m  [20/21], [94mLoss[0m : 3.63186

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.641, [92mTest[0m: 3.743, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.46841
[1mStep[0m  [2/21], [94mLoss[0m : 3.58894
[1mStep[0m  [4/21], [94mLoss[0m : 3.67727
[1mStep[0m  [6/21], [94mLoss[0m : 3.54041
[1mStep[0m  [8/21], [94mLoss[0m : 3.30214
[1mStep[0m  [10/21], [94mLoss[0m : 3.48484
[1mStep[0m  [12/21], [94mLoss[0m : 3.57971
[1mStep[0m  [14/21], [94mLoss[0m : 3.49615
[1mStep[0m  [16/21], [94mLoss[0m : 3.46682
[1mStep[0m  [18/21], [94mLoss[0m : 3.17822
[1mStep[0m  [20/21], [94mLoss[0m : 3.47512

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.462, [92mTest[0m: 3.510, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.28622
[1mStep[0m  [2/21], [94mLoss[0m : 3.15529
[1mStep[0m  [4/21], [94mLoss[0m : 3.35321
[1mStep[0m  [6/21], [94mLoss[0m : 3.41367
[1mStep[0m  [8/21], [94mLoss[0m : 3.26762
[1mStep[0m  [10/21], [94mLoss[0m : 3.37689
[1mStep[0m  [12/21], [94mLoss[0m : 3.36781
[1mStep[0m  [14/21], [94mLoss[0m : 3.25678
[1mStep[0m  [16/21], [94mLoss[0m : 3.07729
[1mStep[0m  [18/21], [94mLoss[0m : 3.60010
[1mStep[0m  [20/21], [94mLoss[0m : 3.35520

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.316, [92mTest[0m: 3.335, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.30263
[1mStep[0m  [2/21], [94mLoss[0m : 3.31074
[1mStep[0m  [4/21], [94mLoss[0m : 3.39404
[1mStep[0m  [6/21], [94mLoss[0m : 3.34814
[1mStep[0m  [8/21], [94mLoss[0m : 3.06488
[1mStep[0m  [10/21], [94mLoss[0m : 3.07274
[1mStep[0m  [12/21], [94mLoss[0m : 3.09097
[1mStep[0m  [14/21], [94mLoss[0m : 3.15152
[1mStep[0m  [16/21], [94mLoss[0m : 3.18587
[1mStep[0m  [18/21], [94mLoss[0m : 3.27640
[1mStep[0m  [20/21], [94mLoss[0m : 3.03576

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 3.194, [92mTest[0m: 3.197, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 3.09608
[1mStep[0m  [2/21], [94mLoss[0m : 3.12696
[1mStep[0m  [4/21], [94mLoss[0m : 3.07946
[1mStep[0m  [6/21], [94mLoss[0m : 3.03200
[1mStep[0m  [8/21], [94mLoss[0m : 2.99079
[1mStep[0m  [10/21], [94mLoss[0m : 2.92574
[1mStep[0m  [12/21], [94mLoss[0m : 2.83902
[1mStep[0m  [14/21], [94mLoss[0m : 3.26402
[1mStep[0m  [16/21], [94mLoss[0m : 2.89255
[1mStep[0m  [18/21], [94mLoss[0m : 3.11379
[1mStep[0m  [20/21], [94mLoss[0m : 2.90031

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 3.108, [92mTest[0m: 3.066, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.90013
[1mStep[0m  [2/21], [94mLoss[0m : 3.23378
[1mStep[0m  [4/21], [94mLoss[0m : 3.03287
[1mStep[0m  [6/21], [94mLoss[0m : 2.96330
[1mStep[0m  [8/21], [94mLoss[0m : 3.23483
[1mStep[0m  [10/21], [94mLoss[0m : 2.92453
[1mStep[0m  [12/21], [94mLoss[0m : 2.99108
[1mStep[0m  [14/21], [94mLoss[0m : 3.15553
[1mStep[0m  [16/21], [94mLoss[0m : 2.91486
[1mStep[0m  [18/21], [94mLoss[0m : 2.90383
[1mStep[0m  [20/21], [94mLoss[0m : 3.02268

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 3.023, [92mTest[0m: 2.980, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.98263
[1mStep[0m  [2/21], [94mLoss[0m : 2.88835
[1mStep[0m  [4/21], [94mLoss[0m : 3.10405
[1mStep[0m  [6/21], [94mLoss[0m : 2.92540
[1mStep[0m  [8/21], [94mLoss[0m : 3.02445
[1mStep[0m  [10/21], [94mLoss[0m : 2.99097
[1mStep[0m  [12/21], [94mLoss[0m : 2.88308
[1mStep[0m  [14/21], [94mLoss[0m : 2.86795
[1mStep[0m  [16/21], [94mLoss[0m : 3.03997
[1mStep[0m  [18/21], [94mLoss[0m : 2.91977
[1mStep[0m  [20/21], [94mLoss[0m : 2.85141

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.956, [92mTest[0m: 2.909, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.93947
[1mStep[0m  [2/21], [94mLoss[0m : 2.84752
[1mStep[0m  [4/21], [94mLoss[0m : 2.88019
[1mStep[0m  [6/21], [94mLoss[0m : 3.01608
[1mStep[0m  [8/21], [94mLoss[0m : 2.87103
[1mStep[0m  [10/21], [94mLoss[0m : 2.77124
[1mStep[0m  [12/21], [94mLoss[0m : 2.72196
[1mStep[0m  [14/21], [94mLoss[0m : 2.94172
[1mStep[0m  [16/21], [94mLoss[0m : 2.95679
[1mStep[0m  [18/21], [94mLoss[0m : 2.92791
[1mStep[0m  [20/21], [94mLoss[0m : 3.06874

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.912, [92mTest[0m: 2.843, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.90079
[1mStep[0m  [2/21], [94mLoss[0m : 2.71279
[1mStep[0m  [4/21], [94mLoss[0m : 3.03464
[1mStep[0m  [6/21], [94mLoss[0m : 2.81614
[1mStep[0m  [8/21], [94mLoss[0m : 2.84325
[1mStep[0m  [10/21], [94mLoss[0m : 2.88113
[1mStep[0m  [12/21], [94mLoss[0m : 2.94744
[1mStep[0m  [14/21], [94mLoss[0m : 2.66842
[1mStep[0m  [16/21], [94mLoss[0m : 2.88634
[1mStep[0m  [18/21], [94mLoss[0m : 2.92261
[1mStep[0m  [20/21], [94mLoss[0m : 2.77096

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.854, [92mTest[0m: 2.783, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.79151
[1mStep[0m  [2/21], [94mLoss[0m : 2.97905
[1mStep[0m  [4/21], [94mLoss[0m : 3.02828
[1mStep[0m  [6/21], [94mLoss[0m : 2.69851
[1mStep[0m  [8/21], [94mLoss[0m : 2.71537
[1mStep[0m  [10/21], [94mLoss[0m : 2.95818
[1mStep[0m  [12/21], [94mLoss[0m : 2.83829
[1mStep[0m  [14/21], [94mLoss[0m : 2.88701
[1mStep[0m  [16/21], [94mLoss[0m : 2.86447
[1mStep[0m  [18/21], [94mLoss[0m : 2.77725
[1mStep[0m  [20/21], [94mLoss[0m : 2.76998

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.820, [92mTest[0m: 2.738, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71723
[1mStep[0m  [2/21], [94mLoss[0m : 2.64174
[1mStep[0m  [4/21], [94mLoss[0m : 2.86153
[1mStep[0m  [6/21], [94mLoss[0m : 2.77237
[1mStep[0m  [8/21], [94mLoss[0m : 2.93684
[1mStep[0m  [10/21], [94mLoss[0m : 2.88914
[1mStep[0m  [12/21], [94mLoss[0m : 2.83107
[1mStep[0m  [14/21], [94mLoss[0m : 2.85619
[1mStep[0m  [16/21], [94mLoss[0m : 2.91409
[1mStep[0m  [18/21], [94mLoss[0m : 2.87239
[1mStep[0m  [20/21], [94mLoss[0m : 2.60523

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.780, [92mTest[0m: 2.692, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.87380
[1mStep[0m  [2/21], [94mLoss[0m : 2.79170
[1mStep[0m  [4/21], [94mLoss[0m : 2.84705
[1mStep[0m  [6/21], [94mLoss[0m : 2.75987
[1mStep[0m  [8/21], [94mLoss[0m : 2.79186
[1mStep[0m  [10/21], [94mLoss[0m : 2.84510
[1mStep[0m  [12/21], [94mLoss[0m : 2.82548
[1mStep[0m  [14/21], [94mLoss[0m : 2.84843
[1mStep[0m  [16/21], [94mLoss[0m : 2.64828
[1mStep[0m  [18/21], [94mLoss[0m : 2.92814
[1mStep[0m  [20/21], [94mLoss[0m : 2.52186

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.654, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81303
[1mStep[0m  [2/21], [94mLoss[0m : 2.69385
[1mStep[0m  [4/21], [94mLoss[0m : 2.82344
[1mStep[0m  [6/21], [94mLoss[0m : 2.76482
[1mStep[0m  [8/21], [94mLoss[0m : 2.68730
[1mStep[0m  [10/21], [94mLoss[0m : 2.81347
[1mStep[0m  [12/21], [94mLoss[0m : 2.57029
[1mStep[0m  [14/21], [94mLoss[0m : 2.71394
[1mStep[0m  [16/21], [94mLoss[0m : 2.70280
[1mStep[0m  [18/21], [94mLoss[0m : 2.61614
[1mStep[0m  [20/21], [94mLoss[0m : 2.76161

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.713, [92mTest[0m: 2.622, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.81285
[1mStep[0m  [2/21], [94mLoss[0m : 2.65436
[1mStep[0m  [4/21], [94mLoss[0m : 2.50252
[1mStep[0m  [6/21], [94mLoss[0m : 2.82804
[1mStep[0m  [8/21], [94mLoss[0m : 2.78813
[1mStep[0m  [10/21], [94mLoss[0m : 2.65490
[1mStep[0m  [12/21], [94mLoss[0m : 2.84668
[1mStep[0m  [14/21], [94mLoss[0m : 2.73658
[1mStep[0m  [16/21], [94mLoss[0m : 2.63950
[1mStep[0m  [18/21], [94mLoss[0m : 2.71587
[1mStep[0m  [20/21], [94mLoss[0m : 2.59115

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.700, [92mTest[0m: 2.585, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.71932
[1mStep[0m  [2/21], [94mLoss[0m : 2.68284
[1mStep[0m  [4/21], [94mLoss[0m : 2.67250
[1mStep[0m  [6/21], [94mLoss[0m : 2.60205
[1mStep[0m  [8/21], [94mLoss[0m : 2.66083
[1mStep[0m  [10/21], [94mLoss[0m : 2.65789
[1mStep[0m  [12/21], [94mLoss[0m : 2.63788
[1mStep[0m  [14/21], [94mLoss[0m : 2.63388
[1mStep[0m  [16/21], [94mLoss[0m : 2.65218
[1mStep[0m  [18/21], [94mLoss[0m : 2.71001
[1mStep[0m  [20/21], [94mLoss[0m : 2.65273

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.571, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.542
====================================

Phase 1 - Evaluation MAE:  2.541583844593593
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/21], [94mLoss[0m : 2.74258
[1mStep[0m  [2/21], [94mLoss[0m : 2.63904
[1mStep[0m  [4/21], [94mLoss[0m : 2.63843
[1mStep[0m  [6/21], [94mLoss[0m : 2.56512
[1mStep[0m  [8/21], [94mLoss[0m : 2.71668
[1mStep[0m  [10/21], [94mLoss[0m : 2.68380
[1mStep[0m  [12/21], [94mLoss[0m : 2.72996
[1mStep[0m  [14/21], [94mLoss[0m : 2.66136
[1mStep[0m  [16/21], [94mLoss[0m : 2.60873
[1mStep[0m  [18/21], [94mLoss[0m : 2.63713
[1mStep[0m  [20/21], [94mLoss[0m : 2.64523

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.69427
[1mStep[0m  [2/21], [94mLoss[0m : 2.63344
[1mStep[0m  [4/21], [94mLoss[0m : 2.49789
[1mStep[0m  [6/21], [94mLoss[0m : 2.45365
[1mStep[0m  [8/21], [94mLoss[0m : 2.62391
[1mStep[0m  [10/21], [94mLoss[0m : 2.72658
[1mStep[0m  [12/21], [94mLoss[0m : 2.66678
[1mStep[0m  [14/21], [94mLoss[0m : 2.57446
[1mStep[0m  [16/21], [94mLoss[0m : 2.58312
[1mStep[0m  [18/21], [94mLoss[0m : 2.72621
[1mStep[0m  [20/21], [94mLoss[0m : 2.61808

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.523, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.72462
[1mStep[0m  [2/21], [94mLoss[0m : 2.57850
[1mStep[0m  [4/21], [94mLoss[0m : 2.55841
[1mStep[0m  [6/21], [94mLoss[0m : 2.53171
[1mStep[0m  [8/21], [94mLoss[0m : 2.63964
[1mStep[0m  [10/21], [94mLoss[0m : 2.48180
[1mStep[0m  [12/21], [94mLoss[0m : 2.65056
[1mStep[0m  [14/21], [94mLoss[0m : 2.74366
[1mStep[0m  [16/21], [94mLoss[0m : 2.70122
[1mStep[0m  [18/21], [94mLoss[0m : 2.51835
[1mStep[0m  [20/21], [94mLoss[0m : 2.62642

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.507, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59952
[1mStep[0m  [2/21], [94mLoss[0m : 2.47838
[1mStep[0m  [4/21], [94mLoss[0m : 2.54282
[1mStep[0m  [6/21], [94mLoss[0m : 2.60685
[1mStep[0m  [8/21], [94mLoss[0m : 2.54881
[1mStep[0m  [10/21], [94mLoss[0m : 2.65142
[1mStep[0m  [12/21], [94mLoss[0m : 2.57459
[1mStep[0m  [14/21], [94mLoss[0m : 2.48714
[1mStep[0m  [16/21], [94mLoss[0m : 2.61133
[1mStep[0m  [18/21], [94mLoss[0m : 2.61778
[1mStep[0m  [20/21], [94mLoss[0m : 2.47928

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.485, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59346
[1mStep[0m  [2/21], [94mLoss[0m : 2.53467
[1mStep[0m  [4/21], [94mLoss[0m : 2.57354
[1mStep[0m  [6/21], [94mLoss[0m : 2.64285
[1mStep[0m  [8/21], [94mLoss[0m : 2.55592
[1mStep[0m  [10/21], [94mLoss[0m : 2.42582
[1mStep[0m  [12/21], [94mLoss[0m : 2.53676
[1mStep[0m  [14/21], [94mLoss[0m : 2.56079
[1mStep[0m  [16/21], [94mLoss[0m : 2.47549
[1mStep[0m  [18/21], [94mLoss[0m : 2.50961
[1mStep[0m  [20/21], [94mLoss[0m : 2.66530

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52182
[1mStep[0m  [2/21], [94mLoss[0m : 2.56666
[1mStep[0m  [4/21], [94mLoss[0m : 2.62301
[1mStep[0m  [6/21], [94mLoss[0m : 2.48618
[1mStep[0m  [8/21], [94mLoss[0m : 2.55953
[1mStep[0m  [10/21], [94mLoss[0m : 2.53004
[1mStep[0m  [12/21], [94mLoss[0m : 2.48023
[1mStep[0m  [14/21], [94mLoss[0m : 2.58964
[1mStep[0m  [16/21], [94mLoss[0m : 2.63845
[1mStep[0m  [18/21], [94mLoss[0m : 2.61938
[1mStep[0m  [20/21], [94mLoss[0m : 2.65569

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.463, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.59261
[1mStep[0m  [2/21], [94mLoss[0m : 2.61184
[1mStep[0m  [4/21], [94mLoss[0m : 2.45545
[1mStep[0m  [6/21], [94mLoss[0m : 2.53350
[1mStep[0m  [8/21], [94mLoss[0m : 2.50081
[1mStep[0m  [10/21], [94mLoss[0m : 2.65433
[1mStep[0m  [12/21], [94mLoss[0m : 2.41744
[1mStep[0m  [14/21], [94mLoss[0m : 2.72528
[1mStep[0m  [16/21], [94mLoss[0m : 2.45776
[1mStep[0m  [18/21], [94mLoss[0m : 2.58292
[1mStep[0m  [20/21], [94mLoss[0m : 2.59409

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.452, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45039
[1mStep[0m  [2/21], [94mLoss[0m : 2.70895
[1mStep[0m  [4/21], [94mLoss[0m : 2.47768
[1mStep[0m  [6/21], [94mLoss[0m : 2.47711
[1mStep[0m  [8/21], [94mLoss[0m : 2.72285
[1mStep[0m  [10/21], [94mLoss[0m : 2.44116
[1mStep[0m  [12/21], [94mLoss[0m : 2.43238
[1mStep[0m  [14/21], [94mLoss[0m : 2.56354
[1mStep[0m  [16/21], [94mLoss[0m : 2.42006
[1mStep[0m  [18/21], [94mLoss[0m : 2.69045
[1mStep[0m  [20/21], [94mLoss[0m : 2.37989

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.439, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43403
[1mStep[0m  [2/21], [94mLoss[0m : 2.53553
[1mStep[0m  [4/21], [94mLoss[0m : 2.42363
[1mStep[0m  [6/21], [94mLoss[0m : 2.54144
[1mStep[0m  [8/21], [94mLoss[0m : 2.64546
[1mStep[0m  [10/21], [94mLoss[0m : 2.56275
[1mStep[0m  [12/21], [94mLoss[0m : 2.47965
[1mStep[0m  [14/21], [94mLoss[0m : 2.56385
[1mStep[0m  [16/21], [94mLoss[0m : 2.57794
[1mStep[0m  [18/21], [94mLoss[0m : 2.64561
[1mStep[0m  [20/21], [94mLoss[0m : 2.53575

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.440, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43583
[1mStep[0m  [2/21], [94mLoss[0m : 2.54663
[1mStep[0m  [4/21], [94mLoss[0m : 2.57836
[1mStep[0m  [6/21], [94mLoss[0m : 2.45768
[1mStep[0m  [8/21], [94mLoss[0m : 2.63962
[1mStep[0m  [10/21], [94mLoss[0m : 2.50300
[1mStep[0m  [12/21], [94mLoss[0m : 2.47357
[1mStep[0m  [14/21], [94mLoss[0m : 2.50056
[1mStep[0m  [16/21], [94mLoss[0m : 2.52859
[1mStep[0m  [18/21], [94mLoss[0m : 2.48405
[1mStep[0m  [20/21], [94mLoss[0m : 2.59042

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.437, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52026
[1mStep[0m  [2/21], [94mLoss[0m : 2.53200
[1mStep[0m  [4/21], [94mLoss[0m : 2.53604
[1mStep[0m  [6/21], [94mLoss[0m : 2.59752
[1mStep[0m  [8/21], [94mLoss[0m : 2.70261
[1mStep[0m  [10/21], [94mLoss[0m : 2.44188
[1mStep[0m  [12/21], [94mLoss[0m : 2.39004
[1mStep[0m  [14/21], [94mLoss[0m : 2.47877
[1mStep[0m  [16/21], [94mLoss[0m : 2.44931
[1mStep[0m  [18/21], [94mLoss[0m : 2.52680
[1mStep[0m  [20/21], [94mLoss[0m : 2.50551

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.427, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55444
[1mStep[0m  [2/21], [94mLoss[0m : 2.38987
[1mStep[0m  [4/21], [94mLoss[0m : 2.48097
[1mStep[0m  [6/21], [94mLoss[0m : 2.57930
[1mStep[0m  [8/21], [94mLoss[0m : 2.52871
[1mStep[0m  [10/21], [94mLoss[0m : 2.54510
[1mStep[0m  [12/21], [94mLoss[0m : 2.74667
[1mStep[0m  [14/21], [94mLoss[0m : 2.68636
[1mStep[0m  [16/21], [94mLoss[0m : 2.47792
[1mStep[0m  [18/21], [94mLoss[0m : 2.54610
[1mStep[0m  [20/21], [94mLoss[0m : 2.53135

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.423, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40729
[1mStep[0m  [2/21], [94mLoss[0m : 2.58261
[1mStep[0m  [4/21], [94mLoss[0m : 2.56520
[1mStep[0m  [6/21], [94mLoss[0m : 2.65953
[1mStep[0m  [8/21], [94mLoss[0m : 2.44400
[1mStep[0m  [10/21], [94mLoss[0m : 2.50277
[1mStep[0m  [12/21], [94mLoss[0m : 2.45578
[1mStep[0m  [14/21], [94mLoss[0m : 2.55962
[1mStep[0m  [16/21], [94mLoss[0m : 2.53515
[1mStep[0m  [18/21], [94mLoss[0m : 2.66353
[1mStep[0m  [20/21], [94mLoss[0m : 2.69231

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.424, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.53173
[1mStep[0m  [2/21], [94mLoss[0m : 2.40612
[1mStep[0m  [4/21], [94mLoss[0m : 2.47545
[1mStep[0m  [6/21], [94mLoss[0m : 2.45751
[1mStep[0m  [8/21], [94mLoss[0m : 2.48468
[1mStep[0m  [10/21], [94mLoss[0m : 2.59728
[1mStep[0m  [12/21], [94mLoss[0m : 2.39450
[1mStep[0m  [14/21], [94mLoss[0m : 2.61126
[1mStep[0m  [16/21], [94mLoss[0m : 2.38757
[1mStep[0m  [18/21], [94mLoss[0m : 2.51097
[1mStep[0m  [20/21], [94mLoss[0m : 2.41714

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55185
[1mStep[0m  [2/21], [94mLoss[0m : 2.52214
[1mStep[0m  [4/21], [94mLoss[0m : 2.61111
[1mStep[0m  [6/21], [94mLoss[0m : 2.51829
[1mStep[0m  [8/21], [94mLoss[0m : 2.57615
[1mStep[0m  [10/21], [94mLoss[0m : 2.59308
[1mStep[0m  [12/21], [94mLoss[0m : 2.44211
[1mStep[0m  [14/21], [94mLoss[0m : 2.39319
[1mStep[0m  [16/21], [94mLoss[0m : 2.56606
[1mStep[0m  [18/21], [94mLoss[0m : 2.46957
[1mStep[0m  [20/21], [94mLoss[0m : 2.58224

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.414, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54792
[1mStep[0m  [2/21], [94mLoss[0m : 2.43168
[1mStep[0m  [4/21], [94mLoss[0m : 2.35312
[1mStep[0m  [6/21], [94mLoss[0m : 2.50982
[1mStep[0m  [8/21], [94mLoss[0m : 2.49579
[1mStep[0m  [10/21], [94mLoss[0m : 2.57358
[1mStep[0m  [12/21], [94mLoss[0m : 2.52912
[1mStep[0m  [14/21], [94mLoss[0m : 2.54584
[1mStep[0m  [16/21], [94mLoss[0m : 2.64887
[1mStep[0m  [18/21], [94mLoss[0m : 2.40462
[1mStep[0m  [20/21], [94mLoss[0m : 2.35103

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62959
[1mStep[0m  [2/21], [94mLoss[0m : 2.49275
[1mStep[0m  [4/21], [94mLoss[0m : 2.52034
[1mStep[0m  [6/21], [94mLoss[0m : 2.52876
[1mStep[0m  [8/21], [94mLoss[0m : 2.44504
[1mStep[0m  [10/21], [94mLoss[0m : 2.56852
[1mStep[0m  [12/21], [94mLoss[0m : 2.52779
[1mStep[0m  [14/21], [94mLoss[0m : 2.39439
[1mStep[0m  [16/21], [94mLoss[0m : 2.43162
[1mStep[0m  [18/21], [94mLoss[0m : 2.48606
[1mStep[0m  [20/21], [94mLoss[0m : 2.44138

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.400, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.52502
[1mStep[0m  [2/21], [94mLoss[0m : 2.39235
[1mStep[0m  [4/21], [94mLoss[0m : 2.51839
[1mStep[0m  [6/21], [94mLoss[0m : 2.60048
[1mStep[0m  [8/21], [94mLoss[0m : 2.48096
[1mStep[0m  [10/21], [94mLoss[0m : 2.44780
[1mStep[0m  [12/21], [94mLoss[0m : 2.48119
[1mStep[0m  [14/21], [94mLoss[0m : 2.57684
[1mStep[0m  [16/21], [94mLoss[0m : 2.68517
[1mStep[0m  [18/21], [94mLoss[0m : 2.49406
[1mStep[0m  [20/21], [94mLoss[0m : 2.49602

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.410, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55673
[1mStep[0m  [2/21], [94mLoss[0m : 2.51588
[1mStep[0m  [4/21], [94mLoss[0m : 2.46479
[1mStep[0m  [6/21], [94mLoss[0m : 2.49095
[1mStep[0m  [8/21], [94mLoss[0m : 2.33813
[1mStep[0m  [10/21], [94mLoss[0m : 2.49632
[1mStep[0m  [12/21], [94mLoss[0m : 2.52678
[1mStep[0m  [14/21], [94mLoss[0m : 2.64541
[1mStep[0m  [16/21], [94mLoss[0m : 2.49672
[1mStep[0m  [18/21], [94mLoss[0m : 2.50605
[1mStep[0m  [20/21], [94mLoss[0m : 2.30588

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.403, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.62872
[1mStep[0m  [2/21], [94mLoss[0m : 2.42697
[1mStep[0m  [4/21], [94mLoss[0m : 2.51028
[1mStep[0m  [6/21], [94mLoss[0m : 2.36856
[1mStep[0m  [8/21], [94mLoss[0m : 2.56014
[1mStep[0m  [10/21], [94mLoss[0m : 2.42416
[1mStep[0m  [12/21], [94mLoss[0m : 2.64232
[1mStep[0m  [14/21], [94mLoss[0m : 2.56116
[1mStep[0m  [16/21], [94mLoss[0m : 2.44678
[1mStep[0m  [18/21], [94mLoss[0m : 2.37232
[1mStep[0m  [20/21], [94mLoss[0m : 2.48427

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.409, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.43584
[1mStep[0m  [2/21], [94mLoss[0m : 2.43333
[1mStep[0m  [4/21], [94mLoss[0m : 2.46719
[1mStep[0m  [6/21], [94mLoss[0m : 2.49717
[1mStep[0m  [8/21], [94mLoss[0m : 2.40614
[1mStep[0m  [10/21], [94mLoss[0m : 2.33387
[1mStep[0m  [12/21], [94mLoss[0m : 2.54006
[1mStep[0m  [14/21], [94mLoss[0m : 2.49999
[1mStep[0m  [16/21], [94mLoss[0m : 2.51666
[1mStep[0m  [18/21], [94mLoss[0m : 2.67954
[1mStep[0m  [20/21], [94mLoss[0m : 2.43681

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.409, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.45046
[1mStep[0m  [2/21], [94mLoss[0m : 2.45378
[1mStep[0m  [4/21], [94mLoss[0m : 2.50014
[1mStep[0m  [6/21], [94mLoss[0m : 2.40909
[1mStep[0m  [8/21], [94mLoss[0m : 2.49427
[1mStep[0m  [10/21], [94mLoss[0m : 2.59882
[1mStep[0m  [12/21], [94mLoss[0m : 2.66255
[1mStep[0m  [14/21], [94mLoss[0m : 2.48824
[1mStep[0m  [16/21], [94mLoss[0m : 2.48239
[1mStep[0m  [18/21], [94mLoss[0m : 2.28341
[1mStep[0m  [20/21], [94mLoss[0m : 2.35653

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.399, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37874
[1mStep[0m  [2/21], [94mLoss[0m : 2.28900
[1mStep[0m  [4/21], [94mLoss[0m : 2.62028
[1mStep[0m  [6/21], [94mLoss[0m : 2.48572
[1mStep[0m  [8/21], [94mLoss[0m : 2.52488
[1mStep[0m  [10/21], [94mLoss[0m : 2.49982
[1mStep[0m  [12/21], [94mLoss[0m : 2.43877
[1mStep[0m  [14/21], [94mLoss[0m : 2.54726
[1mStep[0m  [16/21], [94mLoss[0m : 2.61396
[1mStep[0m  [18/21], [94mLoss[0m : 2.45784
[1mStep[0m  [20/21], [94mLoss[0m : 2.40280

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.399, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.31590
[1mStep[0m  [2/21], [94mLoss[0m : 2.62508
[1mStep[0m  [4/21], [94mLoss[0m : 2.36190
[1mStep[0m  [6/21], [94mLoss[0m : 2.51462
[1mStep[0m  [8/21], [94mLoss[0m : 2.56632
[1mStep[0m  [10/21], [94mLoss[0m : 2.37298
[1mStep[0m  [12/21], [94mLoss[0m : 2.66862
[1mStep[0m  [14/21], [94mLoss[0m : 2.62701
[1mStep[0m  [16/21], [94mLoss[0m : 2.47503
[1mStep[0m  [18/21], [94mLoss[0m : 2.42078
[1mStep[0m  [20/21], [94mLoss[0m : 2.49054

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.393, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54400
[1mStep[0m  [2/21], [94mLoss[0m : 2.49688
[1mStep[0m  [4/21], [94mLoss[0m : 2.47483
[1mStep[0m  [6/21], [94mLoss[0m : 2.45282
[1mStep[0m  [8/21], [94mLoss[0m : 2.60348
[1mStep[0m  [10/21], [94mLoss[0m : 2.35886
[1mStep[0m  [12/21], [94mLoss[0m : 2.39107
[1mStep[0m  [14/21], [94mLoss[0m : 2.49699
[1mStep[0m  [16/21], [94mLoss[0m : 2.44437
[1mStep[0m  [18/21], [94mLoss[0m : 2.46329
[1mStep[0m  [20/21], [94mLoss[0m : 2.48595

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.397, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.37884
[1mStep[0m  [2/21], [94mLoss[0m : 2.60199
[1mStep[0m  [4/21], [94mLoss[0m : 2.35116
[1mStep[0m  [6/21], [94mLoss[0m : 2.53455
[1mStep[0m  [8/21], [94mLoss[0m : 2.52660
[1mStep[0m  [10/21], [94mLoss[0m : 2.52284
[1mStep[0m  [12/21], [94mLoss[0m : 2.38315
[1mStep[0m  [14/21], [94mLoss[0m : 2.47258
[1mStep[0m  [16/21], [94mLoss[0m : 2.47143
[1mStep[0m  [18/21], [94mLoss[0m : 2.48856
[1mStep[0m  [20/21], [94mLoss[0m : 2.31694

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.394, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.54312
[1mStep[0m  [2/21], [94mLoss[0m : 2.36618
[1mStep[0m  [4/21], [94mLoss[0m : 2.49636
[1mStep[0m  [6/21], [94mLoss[0m : 2.34448
[1mStep[0m  [8/21], [94mLoss[0m : 2.46847
[1mStep[0m  [10/21], [94mLoss[0m : 2.48220
[1mStep[0m  [12/21], [94mLoss[0m : 2.54369
[1mStep[0m  [14/21], [94mLoss[0m : 2.30924
[1mStep[0m  [16/21], [94mLoss[0m : 2.41145
[1mStep[0m  [18/21], [94mLoss[0m : 2.39254
[1mStep[0m  [20/21], [94mLoss[0m : 2.53281

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.394, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.55472
[1mStep[0m  [2/21], [94mLoss[0m : 2.45003
[1mStep[0m  [4/21], [94mLoss[0m : 2.38759
[1mStep[0m  [6/21], [94mLoss[0m : 2.42039
[1mStep[0m  [8/21], [94mLoss[0m : 2.37710
[1mStep[0m  [10/21], [94mLoss[0m : 2.47035
[1mStep[0m  [12/21], [94mLoss[0m : 2.42085
[1mStep[0m  [14/21], [94mLoss[0m : 2.64987
[1mStep[0m  [16/21], [94mLoss[0m : 2.54842
[1mStep[0m  [18/21], [94mLoss[0m : 2.32196
[1mStep[0m  [20/21], [94mLoss[0m : 2.48991

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.387, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.39222
[1mStep[0m  [2/21], [94mLoss[0m : 2.36429
[1mStep[0m  [4/21], [94mLoss[0m : 2.49315
[1mStep[0m  [6/21], [94mLoss[0m : 2.54557
[1mStep[0m  [8/21], [94mLoss[0m : 2.43461
[1mStep[0m  [10/21], [94mLoss[0m : 2.43526
[1mStep[0m  [12/21], [94mLoss[0m : 2.33512
[1mStep[0m  [14/21], [94mLoss[0m : 2.45450
[1mStep[0m  [16/21], [94mLoss[0m : 2.41610
[1mStep[0m  [18/21], [94mLoss[0m : 2.40495
[1mStep[0m  [20/21], [94mLoss[0m : 2.60719

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.402, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/21], [94mLoss[0m : 2.40987
[1mStep[0m  [2/21], [94mLoss[0m : 2.65947
[1mStep[0m  [4/21], [94mLoss[0m : 2.46280
[1mStep[0m  [6/21], [94mLoss[0m : 2.32905
[1mStep[0m  [8/21], [94mLoss[0m : 2.33550
[1mStep[0m  [10/21], [94mLoss[0m : 2.46146
[1mStep[0m  [12/21], [94mLoss[0m : 2.66378
[1mStep[0m  [14/21], [94mLoss[0m : 2.61418
[1mStep[0m  [16/21], [94mLoss[0m : 2.47118
[1mStep[0m  [18/21], [94mLoss[0m : 2.35945
[1mStep[0m  [20/21], [94mLoss[0m : 2.34549

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.394, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 2 - Evaluation MAE:  2.3927509103502547
MAE score P1        2.541584
MAE score P2        2.392751
loss                2.457326
learning_rate         0.0001
batch_size               512
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.9
weight_decay           0.001
Name: 25, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 11.31191
[1mStep[0m  [4/42], [94mLoss[0m : 11.17436
[1mStep[0m  [8/42], [94mLoss[0m : 11.31670
[1mStep[0m  [12/42], [94mLoss[0m : 11.04025
[1mStep[0m  [16/42], [94mLoss[0m : 11.17084
[1mStep[0m  [20/42], [94mLoss[0m : 10.88567
[1mStep[0m  [24/42], [94mLoss[0m : 11.18758
[1mStep[0m  [28/42], [94mLoss[0m : 10.81704
[1mStep[0m  [32/42], [94mLoss[0m : 11.19830
[1mStep[0m  [36/42], [94mLoss[0m : 10.76225
[1mStep[0m  [40/42], [94mLoss[0m : 10.80665

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 11.065, [92mTest[0m: 11.134, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.74211
[1mStep[0m  [4/42], [94mLoss[0m : 11.46030
[1mStep[0m  [8/42], [94mLoss[0m : 11.02245
[1mStep[0m  [12/42], [94mLoss[0m : 11.22785
[1mStep[0m  [16/42], [94mLoss[0m : 11.14476
[1mStep[0m  [20/42], [94mLoss[0m : 10.82057
[1mStep[0m  [24/42], [94mLoss[0m : 10.82466
[1mStep[0m  [28/42], [94mLoss[0m : 11.01980
[1mStep[0m  [32/42], [94mLoss[0m : 10.81829
[1mStep[0m  [36/42], [94mLoss[0m : 10.97503
[1mStep[0m  [40/42], [94mLoss[0m : 10.85811

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.953, [92mTest[0m: 10.999, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.01996
[1mStep[0m  [4/42], [94mLoss[0m : 10.80213
[1mStep[0m  [8/42], [94mLoss[0m : 10.69253
[1mStep[0m  [12/42], [94mLoss[0m : 11.15232
[1mStep[0m  [16/42], [94mLoss[0m : 10.99691
[1mStep[0m  [20/42], [94mLoss[0m : 10.93278
[1mStep[0m  [24/42], [94mLoss[0m : 10.68954
[1mStep[0m  [28/42], [94mLoss[0m : 10.55791
[1mStep[0m  [32/42], [94mLoss[0m : 10.71885
[1mStep[0m  [36/42], [94mLoss[0m : 10.57185
[1mStep[0m  [40/42], [94mLoss[0m : 10.74383

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.841, [92mTest[0m: 10.899, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.37041
[1mStep[0m  [4/42], [94mLoss[0m : 10.74256
[1mStep[0m  [8/42], [94mLoss[0m : 10.42980
[1mStep[0m  [12/42], [94mLoss[0m : 10.82822
[1mStep[0m  [16/42], [94mLoss[0m : 10.73526
[1mStep[0m  [20/42], [94mLoss[0m : 10.87574
[1mStep[0m  [24/42], [94mLoss[0m : 10.50451
[1mStep[0m  [28/42], [94mLoss[0m : 10.99898
[1mStep[0m  [32/42], [94mLoss[0m : 10.79565
[1mStep[0m  [36/42], [94mLoss[0m : 10.68756
[1mStep[0m  [40/42], [94mLoss[0m : 10.74893

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.733, [92mTest[0m: 10.772, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.87811
[1mStep[0m  [4/42], [94mLoss[0m : 10.92303
[1mStep[0m  [8/42], [94mLoss[0m : 10.61924
[1mStep[0m  [12/42], [94mLoss[0m : 10.62000
[1mStep[0m  [16/42], [94mLoss[0m : 10.93133
[1mStep[0m  [20/42], [94mLoss[0m : 10.36222
[1mStep[0m  [24/42], [94mLoss[0m : 10.45962
[1mStep[0m  [28/42], [94mLoss[0m : 10.63037
[1mStep[0m  [32/42], [94mLoss[0m : 10.36423
[1mStep[0m  [36/42], [94mLoss[0m : 10.41928
[1mStep[0m  [40/42], [94mLoss[0m : 10.46032

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.616, [92mTest[0m: 10.672, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.50989
[1mStep[0m  [4/42], [94mLoss[0m : 10.13246
[1mStep[0m  [8/42], [94mLoss[0m : 10.75034
[1mStep[0m  [12/42], [94mLoss[0m : 10.49538
[1mStep[0m  [16/42], [94mLoss[0m : 10.05533
[1mStep[0m  [20/42], [94mLoss[0m : 10.67140
[1mStep[0m  [24/42], [94mLoss[0m : 10.58702
[1mStep[0m  [28/42], [94mLoss[0m : 10.35571
[1mStep[0m  [32/42], [94mLoss[0m : 10.05904
[1mStep[0m  [36/42], [94mLoss[0m : 10.10394
[1mStep[0m  [40/42], [94mLoss[0m : 10.43830

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.507, [92mTest[0m: 10.563, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.32413
[1mStep[0m  [4/42], [94mLoss[0m : 10.43593
[1mStep[0m  [8/42], [94mLoss[0m : 10.60653
[1mStep[0m  [12/42], [94mLoss[0m : 10.51940
[1mStep[0m  [16/42], [94mLoss[0m : 10.23898
[1mStep[0m  [20/42], [94mLoss[0m : 10.78743
[1mStep[0m  [24/42], [94mLoss[0m : 9.99062
[1mStep[0m  [28/42], [94mLoss[0m : 10.56807
[1mStep[0m  [32/42], [94mLoss[0m : 10.04585
[1mStep[0m  [36/42], [94mLoss[0m : 10.53747
[1mStep[0m  [40/42], [94mLoss[0m : 10.23859

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.389, [92mTest[0m: 10.448, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.08623
[1mStep[0m  [4/42], [94mLoss[0m : 10.23015
[1mStep[0m  [8/42], [94mLoss[0m : 10.44590
[1mStep[0m  [12/42], [94mLoss[0m : 10.01503
[1mStep[0m  [16/42], [94mLoss[0m : 10.28703
[1mStep[0m  [20/42], [94mLoss[0m : 10.90160
[1mStep[0m  [24/42], [94mLoss[0m : 10.12804
[1mStep[0m  [28/42], [94mLoss[0m : 10.33956
[1mStep[0m  [32/42], [94mLoss[0m : 10.41475
[1mStep[0m  [36/42], [94mLoss[0m : 10.39580
[1mStep[0m  [40/42], [94mLoss[0m : 10.03770

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.275, [92mTest[0m: 10.338, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.20408
[1mStep[0m  [4/42], [94mLoss[0m : 10.28160
[1mStep[0m  [8/42], [94mLoss[0m : 10.05423
[1mStep[0m  [12/42], [94mLoss[0m : 10.17572
[1mStep[0m  [16/42], [94mLoss[0m : 10.06699
[1mStep[0m  [20/42], [94mLoss[0m : 9.80838
[1mStep[0m  [24/42], [94mLoss[0m : 10.46118
[1mStep[0m  [28/42], [94mLoss[0m : 10.31621
[1mStep[0m  [32/42], [94mLoss[0m : 10.22509
[1mStep[0m  [36/42], [94mLoss[0m : 10.38605
[1mStep[0m  [40/42], [94mLoss[0m : 9.76050

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.162, [92mTest[0m: 10.207, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.89648
[1mStep[0m  [4/42], [94mLoss[0m : 10.17422
[1mStep[0m  [8/42], [94mLoss[0m : 9.65969
[1mStep[0m  [12/42], [94mLoss[0m : 10.39657
[1mStep[0m  [16/42], [94mLoss[0m : 9.81244
[1mStep[0m  [20/42], [94mLoss[0m : 9.92611
[1mStep[0m  [24/42], [94mLoss[0m : 9.91108
[1mStep[0m  [28/42], [94mLoss[0m : 10.02735
[1mStep[0m  [32/42], [94mLoss[0m : 9.87182
[1mStep[0m  [36/42], [94mLoss[0m : 9.95613
[1mStep[0m  [40/42], [94mLoss[0m : 10.01757

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.047, [92mTest[0m: 10.102, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.15096
[1mStep[0m  [4/42], [94mLoss[0m : 9.83785
[1mStep[0m  [8/42], [94mLoss[0m : 9.94507
[1mStep[0m  [12/42], [94mLoss[0m : 9.85638
[1mStep[0m  [16/42], [94mLoss[0m : 10.03308
[1mStep[0m  [20/42], [94mLoss[0m : 9.62017
[1mStep[0m  [24/42], [94mLoss[0m : 10.30694
[1mStep[0m  [28/42], [94mLoss[0m : 9.95841
[1mStep[0m  [32/42], [94mLoss[0m : 10.34387
[1mStep[0m  [36/42], [94mLoss[0m : 10.00280
[1mStep[0m  [40/42], [94mLoss[0m : 10.20962

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 9.932, [92mTest[0m: 9.994, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.68011
[1mStep[0m  [4/42], [94mLoss[0m : 10.08764
[1mStep[0m  [8/42], [94mLoss[0m : 10.17033
[1mStep[0m  [12/42], [94mLoss[0m : 10.07343
[1mStep[0m  [16/42], [94mLoss[0m : 9.81192
[1mStep[0m  [20/42], [94mLoss[0m : 9.76934
[1mStep[0m  [24/42], [94mLoss[0m : 9.89061
[1mStep[0m  [28/42], [94mLoss[0m : 9.74015
[1mStep[0m  [32/42], [94mLoss[0m : 9.31310
[1mStep[0m  [36/42], [94mLoss[0m : 9.46228
[1mStep[0m  [40/42], [94mLoss[0m : 9.69268

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 9.822, [92mTest[0m: 9.862, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.82535
[1mStep[0m  [4/42], [94mLoss[0m : 9.67148
[1mStep[0m  [8/42], [94mLoss[0m : 10.21997
[1mStep[0m  [12/42], [94mLoss[0m : 9.65556
[1mStep[0m  [16/42], [94mLoss[0m : 9.94880
[1mStep[0m  [20/42], [94mLoss[0m : 9.64202
[1mStep[0m  [24/42], [94mLoss[0m : 9.60011
[1mStep[0m  [28/42], [94mLoss[0m : 9.32936
[1mStep[0m  [32/42], [94mLoss[0m : 9.37285
[1mStep[0m  [36/42], [94mLoss[0m : 9.56591
[1mStep[0m  [40/42], [94mLoss[0m : 9.22321

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 9.702, [92mTest[0m: 9.759, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.57603
[1mStep[0m  [4/42], [94mLoss[0m : 9.41287
[1mStep[0m  [8/42], [94mLoss[0m : 9.84879
[1mStep[0m  [12/42], [94mLoss[0m : 10.13068
[1mStep[0m  [16/42], [94mLoss[0m : 9.64913
[1mStep[0m  [20/42], [94mLoss[0m : 9.64392
[1mStep[0m  [24/42], [94mLoss[0m : 9.31833
[1mStep[0m  [28/42], [94mLoss[0m : 9.04319
[1mStep[0m  [32/42], [94mLoss[0m : 9.40400
[1mStep[0m  [36/42], [94mLoss[0m : 9.21543
[1mStep[0m  [40/42], [94mLoss[0m : 9.45031

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 9.595, [92mTest[0m: 9.653, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.12707
[1mStep[0m  [4/42], [94mLoss[0m : 9.52439
[1mStep[0m  [8/42], [94mLoss[0m : 9.59914
[1mStep[0m  [12/42], [94mLoss[0m : 9.98453
[1mStep[0m  [16/42], [94mLoss[0m : 9.85982
[1mStep[0m  [20/42], [94mLoss[0m : 9.76374
[1mStep[0m  [24/42], [94mLoss[0m : 9.26513
[1mStep[0m  [28/42], [94mLoss[0m : 9.40256
[1mStep[0m  [32/42], [94mLoss[0m : 9.44650
[1mStep[0m  [36/42], [94mLoss[0m : 9.68861
[1mStep[0m  [40/42], [94mLoss[0m : 9.35408

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 9.486, [92mTest[0m: 9.530, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.19768
[1mStep[0m  [4/42], [94mLoss[0m : 9.69894
[1mStep[0m  [8/42], [94mLoss[0m : 9.22004
[1mStep[0m  [12/42], [94mLoss[0m : 8.97181
[1mStep[0m  [16/42], [94mLoss[0m : 9.70960
[1mStep[0m  [20/42], [94mLoss[0m : 9.56239
[1mStep[0m  [24/42], [94mLoss[0m : 9.40715
[1mStep[0m  [28/42], [94mLoss[0m : 9.29239
[1mStep[0m  [32/42], [94mLoss[0m : 8.94273
[1mStep[0m  [36/42], [94mLoss[0m : 9.00350
[1mStep[0m  [40/42], [94mLoss[0m : 9.28489

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 9.362, [92mTest[0m: 9.418, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.64690
[1mStep[0m  [4/42], [94mLoss[0m : 9.50547
[1mStep[0m  [8/42], [94mLoss[0m : 9.51360
[1mStep[0m  [12/42], [94mLoss[0m : 9.37032
[1mStep[0m  [16/42], [94mLoss[0m : 9.47290
[1mStep[0m  [20/42], [94mLoss[0m : 9.33284
[1mStep[0m  [24/42], [94mLoss[0m : 9.38356
[1mStep[0m  [28/42], [94mLoss[0m : 9.19678
[1mStep[0m  [32/42], [94mLoss[0m : 9.15979
[1mStep[0m  [36/42], [94mLoss[0m : 9.12983
[1mStep[0m  [40/42], [94mLoss[0m : 9.29676

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 9.252, [92mTest[0m: 9.291, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.84010
[1mStep[0m  [4/42], [94mLoss[0m : 9.16318
[1mStep[0m  [8/42], [94mLoss[0m : 9.40770
[1mStep[0m  [12/42], [94mLoss[0m : 9.00855
[1mStep[0m  [16/42], [94mLoss[0m : 9.37759
[1mStep[0m  [20/42], [94mLoss[0m : 9.11518
[1mStep[0m  [24/42], [94mLoss[0m : 8.97994
[1mStep[0m  [28/42], [94mLoss[0m : 8.95663
[1mStep[0m  [32/42], [94mLoss[0m : 8.90146
[1mStep[0m  [36/42], [94mLoss[0m : 8.74018
[1mStep[0m  [40/42], [94mLoss[0m : 9.26309

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 9.131, [92mTest[0m: 9.194, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.45691
[1mStep[0m  [4/42], [94mLoss[0m : 8.99804
[1mStep[0m  [8/42], [94mLoss[0m : 9.12659
[1mStep[0m  [12/42], [94mLoss[0m : 9.03965
[1mStep[0m  [16/42], [94mLoss[0m : 9.37166
[1mStep[0m  [20/42], [94mLoss[0m : 9.35303
[1mStep[0m  [24/42], [94mLoss[0m : 9.06747
[1mStep[0m  [28/42], [94mLoss[0m : 9.09672
[1mStep[0m  [32/42], [94mLoss[0m : 9.07731
[1mStep[0m  [36/42], [94mLoss[0m : 8.61095
[1mStep[0m  [40/42], [94mLoss[0m : 9.34513

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.026, [92mTest[0m: 9.080, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.72103
[1mStep[0m  [4/42], [94mLoss[0m : 9.00824
[1mStep[0m  [8/42], [94mLoss[0m : 8.94882
[1mStep[0m  [12/42], [94mLoss[0m : 8.75026
[1mStep[0m  [16/42], [94mLoss[0m : 8.83973
[1mStep[0m  [20/42], [94mLoss[0m : 8.92748
[1mStep[0m  [24/42], [94mLoss[0m : 9.42085
[1mStep[0m  [28/42], [94mLoss[0m : 8.99382
[1mStep[0m  [32/42], [94mLoss[0m : 8.65567
[1mStep[0m  [36/42], [94mLoss[0m : 9.23725
[1mStep[0m  [40/42], [94mLoss[0m : 9.24899

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 8.914, [92mTest[0m: 8.963, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.97342
[1mStep[0m  [4/42], [94mLoss[0m : 8.82135
[1mStep[0m  [8/42], [94mLoss[0m : 8.37589
[1mStep[0m  [12/42], [94mLoss[0m : 8.93071
[1mStep[0m  [16/42], [94mLoss[0m : 9.18268
[1mStep[0m  [20/42], [94mLoss[0m : 8.88908
[1mStep[0m  [24/42], [94mLoss[0m : 8.82141
[1mStep[0m  [28/42], [94mLoss[0m : 8.51922
[1mStep[0m  [32/42], [94mLoss[0m : 8.79188
[1mStep[0m  [36/42], [94mLoss[0m : 8.94262
[1mStep[0m  [40/42], [94mLoss[0m : 8.94388

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 8.803, [92mTest[0m: 8.847, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.79425
[1mStep[0m  [4/42], [94mLoss[0m : 8.68610
[1mStep[0m  [8/42], [94mLoss[0m : 8.73250
[1mStep[0m  [12/42], [94mLoss[0m : 8.83361
[1mStep[0m  [16/42], [94mLoss[0m : 9.19225
[1mStep[0m  [20/42], [94mLoss[0m : 8.22831
[1mStep[0m  [24/42], [94mLoss[0m : 9.07028
[1mStep[0m  [28/42], [94mLoss[0m : 8.94653
[1mStep[0m  [32/42], [94mLoss[0m : 8.87119
[1mStep[0m  [36/42], [94mLoss[0m : 8.50880
[1mStep[0m  [40/42], [94mLoss[0m : 8.94405

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 8.705, [92mTest[0m: 8.742, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.72254
[1mStep[0m  [4/42], [94mLoss[0m : 8.21617
[1mStep[0m  [8/42], [94mLoss[0m : 8.81720
[1mStep[0m  [12/42], [94mLoss[0m : 8.80051
[1mStep[0m  [16/42], [94mLoss[0m : 8.68064
[1mStep[0m  [20/42], [94mLoss[0m : 8.97838
[1mStep[0m  [24/42], [94mLoss[0m : 8.85715
[1mStep[0m  [28/42], [94mLoss[0m : 8.58055
[1mStep[0m  [32/42], [94mLoss[0m : 8.57039
[1mStep[0m  [36/42], [94mLoss[0m : 8.60570
[1mStep[0m  [40/42], [94mLoss[0m : 8.34999

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 8.594, [92mTest[0m: 8.640, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.60871
[1mStep[0m  [4/42], [94mLoss[0m : 8.65221
[1mStep[0m  [8/42], [94mLoss[0m : 8.44271
[1mStep[0m  [12/42], [94mLoss[0m : 8.47916
[1mStep[0m  [16/42], [94mLoss[0m : 8.90097
[1mStep[0m  [20/42], [94mLoss[0m : 9.00996
[1mStep[0m  [24/42], [94mLoss[0m : 8.15620
[1mStep[0m  [28/42], [94mLoss[0m : 8.40569
[1mStep[0m  [32/42], [94mLoss[0m : 8.02191
[1mStep[0m  [36/42], [94mLoss[0m : 8.66103
[1mStep[0m  [40/42], [94mLoss[0m : 8.30897

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 8.501, [92mTest[0m: 8.551, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.64810
[1mStep[0m  [4/42], [94mLoss[0m : 8.70741
[1mStep[0m  [8/42], [94mLoss[0m : 8.40335
[1mStep[0m  [12/42], [94mLoss[0m : 8.27834
[1mStep[0m  [16/42], [94mLoss[0m : 8.41215
[1mStep[0m  [20/42], [94mLoss[0m : 7.99469
[1mStep[0m  [24/42], [94mLoss[0m : 8.53479
[1mStep[0m  [28/42], [94mLoss[0m : 8.41374
[1mStep[0m  [32/42], [94mLoss[0m : 8.38348
[1mStep[0m  [36/42], [94mLoss[0m : 8.63981
[1mStep[0m  [40/42], [94mLoss[0m : 8.25042

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 8.393, [92mTest[0m: 8.443, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.48736
[1mStep[0m  [4/42], [94mLoss[0m : 8.39731
[1mStep[0m  [8/42], [94mLoss[0m : 8.43311
[1mStep[0m  [12/42], [94mLoss[0m : 8.66866
[1mStep[0m  [16/42], [94mLoss[0m : 8.32185
[1mStep[0m  [20/42], [94mLoss[0m : 8.01133
[1mStep[0m  [24/42], [94mLoss[0m : 8.29415
[1mStep[0m  [28/42], [94mLoss[0m : 8.05076
[1mStep[0m  [32/42], [94mLoss[0m : 7.82381
[1mStep[0m  [36/42], [94mLoss[0m : 7.65570
[1mStep[0m  [40/42], [94mLoss[0m : 8.25053

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 8.290, [92mTest[0m: 8.347, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.51486
[1mStep[0m  [4/42], [94mLoss[0m : 7.76932
[1mStep[0m  [8/42], [94mLoss[0m : 8.14148
[1mStep[0m  [12/42], [94mLoss[0m : 8.03107
[1mStep[0m  [16/42], [94mLoss[0m : 8.22912
[1mStep[0m  [20/42], [94mLoss[0m : 8.46996
[1mStep[0m  [24/42], [94mLoss[0m : 8.27473
[1mStep[0m  [28/42], [94mLoss[0m : 8.40716
[1mStep[0m  [32/42], [94mLoss[0m : 8.36353
[1mStep[0m  [36/42], [94mLoss[0m : 8.07539
[1mStep[0m  [40/42], [94mLoss[0m : 8.06529

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 8.194, [92mTest[0m: 8.247, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.93270
[1mStep[0m  [4/42], [94mLoss[0m : 8.40094
[1mStep[0m  [8/42], [94mLoss[0m : 8.40262
[1mStep[0m  [12/42], [94mLoss[0m : 8.48370
[1mStep[0m  [16/42], [94mLoss[0m : 8.36094
[1mStep[0m  [20/42], [94mLoss[0m : 7.79770
[1mStep[0m  [24/42], [94mLoss[0m : 8.33865
[1mStep[0m  [28/42], [94mLoss[0m : 7.83280
[1mStep[0m  [32/42], [94mLoss[0m : 7.72042
[1mStep[0m  [36/42], [94mLoss[0m : 7.76618
[1mStep[0m  [40/42], [94mLoss[0m : 8.25941

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 8.089, [92mTest[0m: 8.123, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.87793
[1mStep[0m  [4/42], [94mLoss[0m : 8.03333
[1mStep[0m  [8/42], [94mLoss[0m : 7.86346
[1mStep[0m  [12/42], [94mLoss[0m : 7.83859
[1mStep[0m  [16/42], [94mLoss[0m : 7.41997
[1mStep[0m  [20/42], [94mLoss[0m : 7.74991
[1mStep[0m  [24/42], [94mLoss[0m : 7.67943
[1mStep[0m  [28/42], [94mLoss[0m : 7.83128
[1mStep[0m  [32/42], [94mLoss[0m : 8.25517
[1mStep[0m  [36/42], [94mLoss[0m : 7.81468
[1mStep[0m  [40/42], [94mLoss[0m : 7.92402

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 7.987, [92mTest[0m: 8.039, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.14385
[1mStep[0m  [4/42], [94mLoss[0m : 7.96238
[1mStep[0m  [8/42], [94mLoss[0m : 7.67399
[1mStep[0m  [12/42], [94mLoss[0m : 8.10327
[1mStep[0m  [16/42], [94mLoss[0m : 8.08570
[1mStep[0m  [20/42], [94mLoss[0m : 7.87504
[1mStep[0m  [24/42], [94mLoss[0m : 7.74317
[1mStep[0m  [28/42], [94mLoss[0m : 8.13244
[1mStep[0m  [32/42], [94mLoss[0m : 8.08918
[1mStep[0m  [36/42], [94mLoss[0m : 7.90821
[1mStep[0m  [40/42], [94mLoss[0m : 7.72554

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 7.875, [92mTest[0m: 7.935, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 7.829
====================================

Phase 1 - Evaluation MAE:  7.828524896076748
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 7.77869
[1mStep[0m  [4/42], [94mLoss[0m : 8.14493
[1mStep[0m  [8/42], [94mLoss[0m : 8.32890
[1mStep[0m  [12/42], [94mLoss[0m : 7.64218
[1mStep[0m  [16/42], [94mLoss[0m : 7.77532
[1mStep[0m  [20/42], [94mLoss[0m : 7.36487
[1mStep[0m  [24/42], [94mLoss[0m : 7.93472
[1mStep[0m  [28/42], [94mLoss[0m : 7.65082
[1mStep[0m  [32/42], [94mLoss[0m : 7.54245
[1mStep[0m  [36/42], [94mLoss[0m : 7.67179
[1mStep[0m  [40/42], [94mLoss[0m : 7.55625

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.773, [92mTest[0m: 7.821, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.99360
[1mStep[0m  [4/42], [94mLoss[0m : 7.62175
[1mStep[0m  [8/42], [94mLoss[0m : 7.52583
[1mStep[0m  [12/42], [94mLoss[0m : 7.92845
[1mStep[0m  [16/42], [94mLoss[0m : 7.95232
[1mStep[0m  [20/42], [94mLoss[0m : 7.85040
[1mStep[0m  [24/42], [94mLoss[0m : 7.43086
[1mStep[0m  [28/42], [94mLoss[0m : 7.40667
[1mStep[0m  [32/42], [94mLoss[0m : 7.53811
[1mStep[0m  [36/42], [94mLoss[0m : 7.43204
[1mStep[0m  [40/42], [94mLoss[0m : 7.66061

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.664, [92mTest[0m: 7.713, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.63432
[1mStep[0m  [4/42], [94mLoss[0m : 7.44340
[1mStep[0m  [8/42], [94mLoss[0m : 7.58499
[1mStep[0m  [12/42], [94mLoss[0m : 7.48230
[1mStep[0m  [16/42], [94mLoss[0m : 7.59168
[1mStep[0m  [20/42], [94mLoss[0m : 7.71424
[1mStep[0m  [24/42], [94mLoss[0m : 7.64624
[1mStep[0m  [28/42], [94mLoss[0m : 7.97776
[1mStep[0m  [32/42], [94mLoss[0m : 7.79799
[1mStep[0m  [36/42], [94mLoss[0m : 7.44645
[1mStep[0m  [40/42], [94mLoss[0m : 7.36161

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 7.547, [92mTest[0m: 7.602, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.09759
[1mStep[0m  [4/42], [94mLoss[0m : 7.01518
[1mStep[0m  [8/42], [94mLoss[0m : 7.02309
[1mStep[0m  [12/42], [94mLoss[0m : 7.64438
[1mStep[0m  [16/42], [94mLoss[0m : 7.59563
[1mStep[0m  [20/42], [94mLoss[0m : 7.60153
[1mStep[0m  [24/42], [94mLoss[0m : 7.76570
[1mStep[0m  [28/42], [94mLoss[0m : 7.22278
[1mStep[0m  [32/42], [94mLoss[0m : 7.35208
[1mStep[0m  [36/42], [94mLoss[0m : 7.77158
[1mStep[0m  [40/42], [94mLoss[0m : 6.98804

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.449, [92mTest[0m: 7.493, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.36305
[1mStep[0m  [4/42], [94mLoss[0m : 7.30810
[1mStep[0m  [8/42], [94mLoss[0m : 7.69363
[1mStep[0m  [12/42], [94mLoss[0m : 7.13247
[1mStep[0m  [16/42], [94mLoss[0m : 7.26136
[1mStep[0m  [20/42], [94mLoss[0m : 7.20200
[1mStep[0m  [24/42], [94mLoss[0m : 7.20329
[1mStep[0m  [28/42], [94mLoss[0m : 6.97960
[1mStep[0m  [32/42], [94mLoss[0m : 7.22534
[1mStep[0m  [36/42], [94mLoss[0m : 7.33923
[1mStep[0m  [40/42], [94mLoss[0m : 7.44923

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.320, [92mTest[0m: 7.371, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.28843
[1mStep[0m  [4/42], [94mLoss[0m : 7.32191
[1mStep[0m  [8/42], [94mLoss[0m : 7.15275
[1mStep[0m  [12/42], [94mLoss[0m : 7.18546
[1mStep[0m  [16/42], [94mLoss[0m : 7.71216
[1mStep[0m  [20/42], [94mLoss[0m : 6.94641
[1mStep[0m  [24/42], [94mLoss[0m : 7.41500
[1mStep[0m  [28/42], [94mLoss[0m : 6.99836
[1mStep[0m  [32/42], [94mLoss[0m : 7.29684
[1mStep[0m  [36/42], [94mLoss[0m : 7.37797
[1mStep[0m  [40/42], [94mLoss[0m : 6.65693

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 7.201, [92mTest[0m: 7.249, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.28543
[1mStep[0m  [4/42], [94mLoss[0m : 7.47880
[1mStep[0m  [8/42], [94mLoss[0m : 6.41527
[1mStep[0m  [12/42], [94mLoss[0m : 7.35602
[1mStep[0m  [16/42], [94mLoss[0m : 7.04861
[1mStep[0m  [20/42], [94mLoss[0m : 7.39240
[1mStep[0m  [24/42], [94mLoss[0m : 6.64717
[1mStep[0m  [28/42], [94mLoss[0m : 6.93567
[1mStep[0m  [32/42], [94mLoss[0m : 7.19351
[1mStep[0m  [36/42], [94mLoss[0m : 7.04020
[1mStep[0m  [40/42], [94mLoss[0m : 6.79132

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.092, [92mTest[0m: 7.142, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.44129
[1mStep[0m  [4/42], [94mLoss[0m : 7.08143
[1mStep[0m  [8/42], [94mLoss[0m : 6.72616
[1mStep[0m  [12/42], [94mLoss[0m : 7.19485
[1mStep[0m  [16/42], [94mLoss[0m : 7.05261
[1mStep[0m  [20/42], [94mLoss[0m : 6.73663
[1mStep[0m  [24/42], [94mLoss[0m : 7.04278
[1mStep[0m  [28/42], [94mLoss[0m : 6.81178
[1mStep[0m  [32/42], [94mLoss[0m : 7.31138
[1mStep[0m  [36/42], [94mLoss[0m : 6.98915
[1mStep[0m  [40/42], [94mLoss[0m : 7.12503

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.978, [92mTest[0m: 7.028, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.96203
[1mStep[0m  [4/42], [94mLoss[0m : 6.82987
[1mStep[0m  [8/42], [94mLoss[0m : 7.04656
[1mStep[0m  [12/42], [94mLoss[0m : 7.12796
[1mStep[0m  [16/42], [94mLoss[0m : 6.78414
[1mStep[0m  [20/42], [94mLoss[0m : 6.97437
[1mStep[0m  [24/42], [94mLoss[0m : 6.78554
[1mStep[0m  [28/42], [94mLoss[0m : 7.26458
[1mStep[0m  [32/42], [94mLoss[0m : 6.76431
[1mStep[0m  [36/42], [94mLoss[0m : 6.58981
[1mStep[0m  [40/42], [94mLoss[0m : 6.80342

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.859, [92mTest[0m: 6.919, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.53235
[1mStep[0m  [4/42], [94mLoss[0m : 6.45868
[1mStep[0m  [8/42], [94mLoss[0m : 7.12313
[1mStep[0m  [12/42], [94mLoss[0m : 6.38508
[1mStep[0m  [16/42], [94mLoss[0m : 6.51379
[1mStep[0m  [20/42], [94mLoss[0m : 6.82816
[1mStep[0m  [24/42], [94mLoss[0m : 6.71491
[1mStep[0m  [28/42], [94mLoss[0m : 6.75216
[1mStep[0m  [32/42], [94mLoss[0m : 6.85132
[1mStep[0m  [36/42], [94mLoss[0m : 6.73095
[1mStep[0m  [40/42], [94mLoss[0m : 6.73204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.753, [92mTest[0m: 6.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.86585
[1mStep[0m  [4/42], [94mLoss[0m : 6.40644
[1mStep[0m  [8/42], [94mLoss[0m : 6.57927
[1mStep[0m  [12/42], [94mLoss[0m : 6.44582
[1mStep[0m  [16/42], [94mLoss[0m : 6.68066
[1mStep[0m  [20/42], [94mLoss[0m : 6.49751
[1mStep[0m  [24/42], [94mLoss[0m : 6.81072
[1mStep[0m  [28/42], [94mLoss[0m : 6.15223
[1mStep[0m  [32/42], [94mLoss[0m : 6.60263
[1mStep[0m  [36/42], [94mLoss[0m : 6.51410
[1mStep[0m  [40/42], [94mLoss[0m : 6.21315

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.650, [92mTest[0m: 6.693, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.41656
[1mStep[0m  [4/42], [94mLoss[0m : 6.29571
[1mStep[0m  [8/42], [94mLoss[0m : 6.92634
[1mStep[0m  [12/42], [94mLoss[0m : 6.37455
[1mStep[0m  [16/42], [94mLoss[0m : 6.63977
[1mStep[0m  [20/42], [94mLoss[0m : 6.31350
[1mStep[0m  [24/42], [94mLoss[0m : 6.46464
[1mStep[0m  [28/42], [94mLoss[0m : 6.13025
[1mStep[0m  [32/42], [94mLoss[0m : 6.46869
[1mStep[0m  [36/42], [94mLoss[0m : 6.50624
[1mStep[0m  [40/42], [94mLoss[0m : 6.64521

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 6.525, [92mTest[0m: 6.568, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.60403
[1mStep[0m  [4/42], [94mLoss[0m : 6.40191
[1mStep[0m  [8/42], [94mLoss[0m : 6.39029
[1mStep[0m  [12/42], [94mLoss[0m : 6.37431
[1mStep[0m  [16/42], [94mLoss[0m : 6.55602
[1mStep[0m  [20/42], [94mLoss[0m : 6.31995
[1mStep[0m  [24/42], [94mLoss[0m : 6.27660
[1mStep[0m  [28/42], [94mLoss[0m : 6.76022
[1mStep[0m  [32/42], [94mLoss[0m : 6.23849
[1mStep[0m  [36/42], [94mLoss[0m : 6.39931
[1mStep[0m  [40/42], [94mLoss[0m : 6.40864

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 6.407, [92mTest[0m: 6.463, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.00425
[1mStep[0m  [4/42], [94mLoss[0m : 6.46691
[1mStep[0m  [8/42], [94mLoss[0m : 6.14503
[1mStep[0m  [12/42], [94mLoss[0m : 6.12527
[1mStep[0m  [16/42], [94mLoss[0m : 6.34424
[1mStep[0m  [20/42], [94mLoss[0m : 6.05004
[1mStep[0m  [24/42], [94mLoss[0m : 6.39510
[1mStep[0m  [28/42], [94mLoss[0m : 6.05040
[1mStep[0m  [32/42], [94mLoss[0m : 6.26033
[1mStep[0m  [36/42], [94mLoss[0m : 6.00071
[1mStep[0m  [40/42], [94mLoss[0m : 6.09773

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 6.300, [92mTest[0m: 6.347, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.62916
[1mStep[0m  [4/42], [94mLoss[0m : 5.80563
[1mStep[0m  [8/42], [94mLoss[0m : 6.13096
[1mStep[0m  [12/42], [94mLoss[0m : 5.84446
[1mStep[0m  [16/42], [94mLoss[0m : 6.51093
[1mStep[0m  [20/42], [94mLoss[0m : 6.18405
[1mStep[0m  [24/42], [94mLoss[0m : 5.81081
[1mStep[0m  [28/42], [94mLoss[0m : 6.35886
[1mStep[0m  [32/42], [94mLoss[0m : 6.75013
[1mStep[0m  [36/42], [94mLoss[0m : 5.97870
[1mStep[0m  [40/42], [94mLoss[0m : 6.10453

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 6.177, [92mTest[0m: 6.223, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.66701
[1mStep[0m  [4/42], [94mLoss[0m : 6.36322
[1mStep[0m  [8/42], [94mLoss[0m : 6.42196
[1mStep[0m  [12/42], [94mLoss[0m : 5.78610
[1mStep[0m  [16/42], [94mLoss[0m : 6.28539
[1mStep[0m  [20/42], [94mLoss[0m : 6.20755
[1mStep[0m  [24/42], [94mLoss[0m : 5.73208
[1mStep[0m  [28/42], [94mLoss[0m : 5.66009
[1mStep[0m  [32/42], [94mLoss[0m : 6.08644
[1mStep[0m  [36/42], [94mLoss[0m : 6.17190
[1mStep[0m  [40/42], [94mLoss[0m : 5.67798

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 6.064, [92mTest[0m: 6.130, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.01014
[1mStep[0m  [4/42], [94mLoss[0m : 5.89982
[1mStep[0m  [8/42], [94mLoss[0m : 5.72744
[1mStep[0m  [12/42], [94mLoss[0m : 6.03153
[1mStep[0m  [16/42], [94mLoss[0m : 5.70822
[1mStep[0m  [20/42], [94mLoss[0m : 5.87388
[1mStep[0m  [24/42], [94mLoss[0m : 5.92595
[1mStep[0m  [28/42], [94mLoss[0m : 5.72854
[1mStep[0m  [32/42], [94mLoss[0m : 6.55717
[1mStep[0m  [36/42], [94mLoss[0m : 6.05885
[1mStep[0m  [40/42], [94mLoss[0m : 6.22462

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 5.971, [92mTest[0m: 6.011, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.82882
[1mStep[0m  [4/42], [94mLoss[0m : 5.45933
[1mStep[0m  [8/42], [94mLoss[0m : 5.83599
[1mStep[0m  [12/42], [94mLoss[0m : 5.83683
[1mStep[0m  [16/42], [94mLoss[0m : 5.64248
[1mStep[0m  [20/42], [94mLoss[0m : 5.76574
[1mStep[0m  [24/42], [94mLoss[0m : 6.01496
[1mStep[0m  [28/42], [94mLoss[0m : 5.55957
[1mStep[0m  [32/42], [94mLoss[0m : 5.94953
[1mStep[0m  [36/42], [94mLoss[0m : 5.66546
[1mStep[0m  [40/42], [94mLoss[0m : 5.53719

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 5.859, [92mTest[0m: 5.890, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.89827
[1mStep[0m  [4/42], [94mLoss[0m : 5.63039
[1mStep[0m  [8/42], [94mLoss[0m : 6.05620
[1mStep[0m  [12/42], [94mLoss[0m : 5.30716
[1mStep[0m  [16/42], [94mLoss[0m : 5.86879
[1mStep[0m  [20/42], [94mLoss[0m : 5.33944
[1mStep[0m  [24/42], [94mLoss[0m : 5.61854
[1mStep[0m  [28/42], [94mLoss[0m : 6.01130
[1mStep[0m  [32/42], [94mLoss[0m : 5.82843
[1mStep[0m  [36/42], [94mLoss[0m : 5.57249
[1mStep[0m  [40/42], [94mLoss[0m : 5.63197

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 5.756, [92mTest[0m: 5.784, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.14180
[1mStep[0m  [4/42], [94mLoss[0m : 5.65488
[1mStep[0m  [8/42], [94mLoss[0m : 5.54225
[1mStep[0m  [12/42], [94mLoss[0m : 5.65780
[1mStep[0m  [16/42], [94mLoss[0m : 5.79042
[1mStep[0m  [20/42], [94mLoss[0m : 5.65431
[1mStep[0m  [24/42], [94mLoss[0m : 5.46793
[1mStep[0m  [28/42], [94mLoss[0m : 5.58761
[1mStep[0m  [32/42], [94mLoss[0m : 5.48951
[1mStep[0m  [36/42], [94mLoss[0m : 5.79776
[1mStep[0m  [40/42], [94mLoss[0m : 5.66829

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 5.648, [92mTest[0m: 5.654, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.79500
[1mStep[0m  [4/42], [94mLoss[0m : 5.40396
[1mStep[0m  [8/42], [94mLoss[0m : 5.41210
[1mStep[0m  [12/42], [94mLoss[0m : 5.89166
[1mStep[0m  [16/42], [94mLoss[0m : 5.51085
[1mStep[0m  [20/42], [94mLoss[0m : 5.58249
[1mStep[0m  [24/42], [94mLoss[0m : 5.54992
[1mStep[0m  [28/42], [94mLoss[0m : 5.26387
[1mStep[0m  [32/42], [94mLoss[0m : 5.21160
[1mStep[0m  [36/42], [94mLoss[0m : 5.76037
[1mStep[0m  [40/42], [94mLoss[0m : 5.54537

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 5.539, [92mTest[0m: 5.573, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.40763
[1mStep[0m  [4/42], [94mLoss[0m : 5.27837
[1mStep[0m  [8/42], [94mLoss[0m : 5.29507
[1mStep[0m  [12/42], [94mLoss[0m : 5.29440
[1mStep[0m  [16/42], [94mLoss[0m : 5.40720
[1mStep[0m  [20/42], [94mLoss[0m : 5.54946
[1mStep[0m  [24/42], [94mLoss[0m : 5.54539
[1mStep[0m  [28/42], [94mLoss[0m : 5.93936
[1mStep[0m  [32/42], [94mLoss[0m : 5.16361
[1mStep[0m  [36/42], [94mLoss[0m : 5.66349
[1mStep[0m  [40/42], [94mLoss[0m : 5.15350

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 5.460, [92mTest[0m: 5.484, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.61534
[1mStep[0m  [4/42], [94mLoss[0m : 4.92703
[1mStep[0m  [8/42], [94mLoss[0m : 5.33015
[1mStep[0m  [12/42], [94mLoss[0m : 5.70949
[1mStep[0m  [16/42], [94mLoss[0m : 5.55665
[1mStep[0m  [20/42], [94mLoss[0m : 5.38135
[1mStep[0m  [24/42], [94mLoss[0m : 5.44569
[1mStep[0m  [28/42], [94mLoss[0m : 5.19469
[1mStep[0m  [32/42], [94mLoss[0m : 5.07064
[1mStep[0m  [36/42], [94mLoss[0m : 5.43359
[1mStep[0m  [40/42], [94mLoss[0m : 5.55163

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 5.367, [92mTest[0m: 5.388, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.37104
[1mStep[0m  [4/42], [94mLoss[0m : 5.30659
[1mStep[0m  [8/42], [94mLoss[0m : 5.02543
[1mStep[0m  [12/42], [94mLoss[0m : 5.21670
[1mStep[0m  [16/42], [94mLoss[0m : 5.35707
[1mStep[0m  [20/42], [94mLoss[0m : 4.92680
[1mStep[0m  [24/42], [94mLoss[0m : 5.20159
[1mStep[0m  [28/42], [94mLoss[0m : 5.00307
[1mStep[0m  [32/42], [94mLoss[0m : 5.25696
[1mStep[0m  [36/42], [94mLoss[0m : 5.85954
[1mStep[0m  [40/42], [94mLoss[0m : 4.96579

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 5.270, [92mTest[0m: 5.298, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.46405
[1mStep[0m  [4/42], [94mLoss[0m : 5.14495
[1mStep[0m  [8/42], [94mLoss[0m : 4.97275
[1mStep[0m  [12/42], [94mLoss[0m : 5.43734
[1mStep[0m  [16/42], [94mLoss[0m : 5.21162
[1mStep[0m  [20/42], [94mLoss[0m : 4.68892
[1mStep[0m  [24/42], [94mLoss[0m : 5.35022
[1mStep[0m  [28/42], [94mLoss[0m : 5.38087
[1mStep[0m  [32/42], [94mLoss[0m : 5.02855
[1mStep[0m  [36/42], [94mLoss[0m : 5.16359
[1mStep[0m  [40/42], [94mLoss[0m : 4.84236

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 5.191, [92mTest[0m: 5.212, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.81083
[1mStep[0m  [4/42], [94mLoss[0m : 5.38934
[1mStep[0m  [8/42], [94mLoss[0m : 5.18081
[1mStep[0m  [12/42], [94mLoss[0m : 5.20290
[1mStep[0m  [16/42], [94mLoss[0m : 4.79776
[1mStep[0m  [20/42], [94mLoss[0m : 5.15293
[1mStep[0m  [24/42], [94mLoss[0m : 5.19070
[1mStep[0m  [28/42], [94mLoss[0m : 4.90352
[1mStep[0m  [32/42], [94mLoss[0m : 5.02479
[1mStep[0m  [36/42], [94mLoss[0m : 4.98891
[1mStep[0m  [40/42], [94mLoss[0m : 4.57607

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 5.108, [92mTest[0m: 5.116, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.90261
[1mStep[0m  [4/42], [94mLoss[0m : 4.97416
[1mStep[0m  [8/42], [94mLoss[0m : 5.09519
[1mStep[0m  [12/42], [94mLoss[0m : 5.19321
[1mStep[0m  [16/42], [94mLoss[0m : 5.09475
[1mStep[0m  [20/42], [94mLoss[0m : 4.78441
[1mStep[0m  [24/42], [94mLoss[0m : 5.35579
[1mStep[0m  [28/42], [94mLoss[0m : 5.08377
[1mStep[0m  [32/42], [94mLoss[0m : 4.78174
[1mStep[0m  [36/42], [94mLoss[0m : 5.12507
[1mStep[0m  [40/42], [94mLoss[0m : 4.83772

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 5.043, [92mTest[0m: 5.038, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.28013
[1mStep[0m  [4/42], [94mLoss[0m : 5.16648
[1mStep[0m  [8/42], [94mLoss[0m : 5.07686
[1mStep[0m  [12/42], [94mLoss[0m : 5.12265
[1mStep[0m  [16/42], [94mLoss[0m : 4.84537
[1mStep[0m  [20/42], [94mLoss[0m : 5.09340
[1mStep[0m  [24/42], [94mLoss[0m : 4.89289
[1mStep[0m  [28/42], [94mLoss[0m : 4.80636
[1mStep[0m  [32/42], [94mLoss[0m : 5.21562
[1mStep[0m  [36/42], [94mLoss[0m : 4.75590
[1mStep[0m  [40/42], [94mLoss[0m : 4.90306

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 4.954, [92mTest[0m: 4.949, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 5.09752
[1mStep[0m  [4/42], [94mLoss[0m : 4.92943
[1mStep[0m  [8/42], [94mLoss[0m : 4.67968
[1mStep[0m  [12/42], [94mLoss[0m : 4.89290
[1mStep[0m  [16/42], [94mLoss[0m : 4.87768
[1mStep[0m  [20/42], [94mLoss[0m : 4.98544
[1mStep[0m  [24/42], [94mLoss[0m : 5.03091
[1mStep[0m  [28/42], [94mLoss[0m : 4.80353
[1mStep[0m  [32/42], [94mLoss[0m : 5.00083
[1mStep[0m  [36/42], [94mLoss[0m : 4.88693
[1mStep[0m  [40/42], [94mLoss[0m : 4.70048

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 4.873, [92mTest[0m: 4.860, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.72200
[1mStep[0m  [4/42], [94mLoss[0m : 4.68614
[1mStep[0m  [8/42], [94mLoss[0m : 4.76900
[1mStep[0m  [12/42], [94mLoss[0m : 4.62023
[1mStep[0m  [16/42], [94mLoss[0m : 4.77380
[1mStep[0m  [20/42], [94mLoss[0m : 4.80872
[1mStep[0m  [24/42], [94mLoss[0m : 5.06573
[1mStep[0m  [28/42], [94mLoss[0m : 4.94699
[1mStep[0m  [32/42], [94mLoss[0m : 5.36036
[1mStep[0m  [36/42], [94mLoss[0m : 4.79977
[1mStep[0m  [40/42], [94mLoss[0m : 5.31618

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 4.790, [92mTest[0m: 4.777, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 4.717
====================================

Phase 2 - Evaluation MAE:  4.716989415032523
MAE score P1       7.828525
MAE score P2       4.716989
loss                4.78983
learning_rate        0.0001
batch_size              256
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 26, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 10.65966
[1mStep[0m  [4/42], [94mLoss[0m : 10.44742
[1mStep[0m  [8/42], [94mLoss[0m : 10.46368
[1mStep[0m  [12/42], [94mLoss[0m : 9.95751
[1mStep[0m  [16/42], [94mLoss[0m : 9.92048
[1mStep[0m  [20/42], [94mLoss[0m : 9.56761
[1mStep[0m  [24/42], [94mLoss[0m : 9.46707
[1mStep[0m  [28/42], [94mLoss[0m : 8.98353
[1mStep[0m  [32/42], [94mLoss[0m : 8.87467
[1mStep[0m  [36/42], [94mLoss[0m : 8.64685
[1mStep[0m  [40/42], [94mLoss[0m : 8.50545

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.509, [92mTest[0m: 10.554, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.95947
[1mStep[0m  [4/42], [94mLoss[0m : 7.80016
[1mStep[0m  [8/42], [94mLoss[0m : 7.22540
[1mStep[0m  [12/42], [94mLoss[0m : 7.25150
[1mStep[0m  [16/42], [94mLoss[0m : 6.64260
[1mStep[0m  [20/42], [94mLoss[0m : 6.49903
[1mStep[0m  [24/42], [94mLoss[0m : 6.62859
[1mStep[0m  [28/42], [94mLoss[0m : 5.92010
[1mStep[0m  [32/42], [94mLoss[0m : 5.78233
[1mStep[0m  [36/42], [94mLoss[0m : 5.78596
[1mStep[0m  [40/42], [94mLoss[0m : 4.91831

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.484, [92mTest[0m: 8.037, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 4.32421
[1mStep[0m  [4/42], [94mLoss[0m : 4.50109
[1mStep[0m  [8/42], [94mLoss[0m : 4.53302
[1mStep[0m  [12/42], [94mLoss[0m : 4.17653
[1mStep[0m  [16/42], [94mLoss[0m : 3.82503
[1mStep[0m  [20/42], [94mLoss[0m : 4.13002
[1mStep[0m  [24/42], [94mLoss[0m : 4.00194
[1mStep[0m  [28/42], [94mLoss[0m : 3.45290
[1mStep[0m  [32/42], [94mLoss[0m : 3.84933
[1mStep[0m  [36/42], [94mLoss[0m : 3.45690
[1mStep[0m  [40/42], [94mLoss[0m : 3.56320

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.962, [92mTest[0m: 4.953, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 3.21301
[1mStep[0m  [4/42], [94mLoss[0m : 3.53498
[1mStep[0m  [8/42], [94mLoss[0m : 3.12461
[1mStep[0m  [12/42], [94mLoss[0m : 3.17942
[1mStep[0m  [16/42], [94mLoss[0m : 2.97847
[1mStep[0m  [20/42], [94mLoss[0m : 3.12712
[1mStep[0m  [24/42], [94mLoss[0m : 3.05065
[1mStep[0m  [28/42], [94mLoss[0m : 2.85762
[1mStep[0m  [32/42], [94mLoss[0m : 3.04737
[1mStep[0m  [36/42], [94mLoss[0m : 2.95050
[1mStep[0m  [40/42], [94mLoss[0m : 2.97147

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.082, [92mTest[0m: 3.290, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.92114
[1mStep[0m  [4/42], [94mLoss[0m : 2.94455
[1mStep[0m  [8/42], [94mLoss[0m : 2.88312
[1mStep[0m  [12/42], [94mLoss[0m : 2.95559
[1mStep[0m  [16/42], [94mLoss[0m : 2.92811
[1mStep[0m  [20/42], [94mLoss[0m : 2.76576
[1mStep[0m  [24/42], [94mLoss[0m : 2.88368
[1mStep[0m  [28/42], [94mLoss[0m : 2.86926
[1mStep[0m  [32/42], [94mLoss[0m : 2.89726
[1mStep[0m  [36/42], [94mLoss[0m : 2.88613
[1mStep[0m  [40/42], [94mLoss[0m : 2.84566

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.889, [92mTest[0m: 2.921, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.89243
[1mStep[0m  [4/42], [94mLoss[0m : 2.72530
[1mStep[0m  [8/42], [94mLoss[0m : 2.94318
[1mStep[0m  [12/42], [94mLoss[0m : 2.83373
[1mStep[0m  [16/42], [94mLoss[0m : 2.68909
[1mStep[0m  [20/42], [94mLoss[0m : 2.80607
[1mStep[0m  [24/42], [94mLoss[0m : 2.96036
[1mStep[0m  [28/42], [94mLoss[0m : 2.91266
[1mStep[0m  [32/42], [94mLoss[0m : 2.88631
[1mStep[0m  [36/42], [94mLoss[0m : 2.96590
[1mStep[0m  [40/42], [94mLoss[0m : 2.79697

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.776, [92mTest[0m: 2.797, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.75324
[1mStep[0m  [4/42], [94mLoss[0m : 2.75385
[1mStep[0m  [8/42], [94mLoss[0m : 2.82749
[1mStep[0m  [12/42], [94mLoss[0m : 2.67378
[1mStep[0m  [16/42], [94mLoss[0m : 2.42103
[1mStep[0m  [20/42], [94mLoss[0m : 2.74725
[1mStep[0m  [24/42], [94mLoss[0m : 2.78889
[1mStep[0m  [28/42], [94mLoss[0m : 2.72043
[1mStep[0m  [32/42], [94mLoss[0m : 2.47869
[1mStep[0m  [36/42], [94mLoss[0m : 2.41557
[1mStep[0m  [40/42], [94mLoss[0m : 3.05839

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.731, [92mTest[0m: 2.718, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.58110
[1mStep[0m  [4/42], [94mLoss[0m : 2.94347
[1mStep[0m  [8/42], [94mLoss[0m : 2.64760
[1mStep[0m  [12/42], [94mLoss[0m : 2.72006
[1mStep[0m  [16/42], [94mLoss[0m : 2.79349
[1mStep[0m  [20/42], [94mLoss[0m : 2.85614
[1mStep[0m  [24/42], [94mLoss[0m : 2.86095
[1mStep[0m  [28/42], [94mLoss[0m : 2.71338
[1mStep[0m  [32/42], [94mLoss[0m : 2.65294
[1mStep[0m  [36/42], [94mLoss[0m : 2.56780
[1mStep[0m  [40/42], [94mLoss[0m : 2.75247

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.659, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.63157
[1mStep[0m  [4/42], [94mLoss[0m : 2.59267
[1mStep[0m  [8/42], [94mLoss[0m : 2.75560
[1mStep[0m  [12/42], [94mLoss[0m : 2.50882
[1mStep[0m  [16/42], [94mLoss[0m : 2.58867
[1mStep[0m  [20/42], [94mLoss[0m : 2.62217
[1mStep[0m  [24/42], [94mLoss[0m : 2.66356
[1mStep[0m  [28/42], [94mLoss[0m : 2.67365
[1mStep[0m  [32/42], [94mLoss[0m : 2.70158
[1mStep[0m  [36/42], [94mLoss[0m : 2.80957
[1mStep[0m  [40/42], [94mLoss[0m : 2.60304

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.612, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.72815
[1mStep[0m  [4/42], [94mLoss[0m : 2.54277
[1mStep[0m  [8/42], [94mLoss[0m : 2.74466
[1mStep[0m  [12/42], [94mLoss[0m : 2.71076
[1mStep[0m  [16/42], [94mLoss[0m : 2.45415
[1mStep[0m  [20/42], [94mLoss[0m : 2.74105
[1mStep[0m  [24/42], [94mLoss[0m : 2.49294
[1mStep[0m  [28/42], [94mLoss[0m : 2.36824
[1mStep[0m  [32/42], [94mLoss[0m : 2.52008
[1mStep[0m  [36/42], [94mLoss[0m : 2.70547
[1mStep[0m  [40/42], [94mLoss[0m : 2.66620

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.588, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.74078
[1mStep[0m  [4/42], [94mLoss[0m : 2.56522
[1mStep[0m  [8/42], [94mLoss[0m : 2.73135
[1mStep[0m  [12/42], [94mLoss[0m : 2.70796
[1mStep[0m  [16/42], [94mLoss[0m : 2.68717
[1mStep[0m  [20/42], [94mLoss[0m : 2.49231
[1mStep[0m  [24/42], [94mLoss[0m : 2.47309
[1mStep[0m  [28/42], [94mLoss[0m : 2.50989
[1mStep[0m  [32/42], [94mLoss[0m : 2.42406
[1mStep[0m  [36/42], [94mLoss[0m : 2.57133
[1mStep[0m  [40/42], [94mLoss[0m : 2.61980

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.565, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62039
[1mStep[0m  [4/42], [94mLoss[0m : 2.67187
[1mStep[0m  [8/42], [94mLoss[0m : 2.58630
[1mStep[0m  [12/42], [94mLoss[0m : 2.64219
[1mStep[0m  [16/42], [94mLoss[0m : 2.78921
[1mStep[0m  [20/42], [94mLoss[0m : 2.45134
[1mStep[0m  [24/42], [94mLoss[0m : 2.69758
[1mStep[0m  [28/42], [94mLoss[0m : 2.66233
[1mStep[0m  [32/42], [94mLoss[0m : 2.37218
[1mStep[0m  [36/42], [94mLoss[0m : 2.70525
[1mStep[0m  [40/42], [94mLoss[0m : 2.82463

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.545, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61180
[1mStep[0m  [4/42], [94mLoss[0m : 2.43118
[1mStep[0m  [8/42], [94mLoss[0m : 2.48983
[1mStep[0m  [12/42], [94mLoss[0m : 2.39670
[1mStep[0m  [16/42], [94mLoss[0m : 2.51379
[1mStep[0m  [20/42], [94mLoss[0m : 2.39045
[1mStep[0m  [24/42], [94mLoss[0m : 2.50336
[1mStep[0m  [28/42], [94mLoss[0m : 2.60514
[1mStep[0m  [32/42], [94mLoss[0m : 2.71320
[1mStep[0m  [36/42], [94mLoss[0m : 2.52306
[1mStep[0m  [40/42], [94mLoss[0m : 2.73573

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.526, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49681
[1mStep[0m  [4/42], [94mLoss[0m : 2.62914
[1mStep[0m  [8/42], [94mLoss[0m : 2.80017
[1mStep[0m  [12/42], [94mLoss[0m : 2.80451
[1mStep[0m  [16/42], [94mLoss[0m : 2.37702
[1mStep[0m  [20/42], [94mLoss[0m : 2.71459
[1mStep[0m  [24/42], [94mLoss[0m : 2.48792
[1mStep[0m  [28/42], [94mLoss[0m : 2.56639
[1mStep[0m  [32/42], [94mLoss[0m : 2.37767
[1mStep[0m  [36/42], [94mLoss[0m : 2.70421
[1mStep[0m  [40/42], [94mLoss[0m : 2.58452

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.503, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.47359
[1mStep[0m  [4/42], [94mLoss[0m : 2.70288
[1mStep[0m  [8/42], [94mLoss[0m : 2.57865
[1mStep[0m  [12/42], [94mLoss[0m : 2.71833
[1mStep[0m  [16/42], [94mLoss[0m : 2.56768
[1mStep[0m  [20/42], [94mLoss[0m : 2.53794
[1mStep[0m  [24/42], [94mLoss[0m : 2.49955
[1mStep[0m  [28/42], [94mLoss[0m : 2.34991
[1mStep[0m  [32/42], [94mLoss[0m : 2.59424
[1mStep[0m  [36/42], [94mLoss[0m : 2.52325
[1mStep[0m  [40/42], [94mLoss[0m : 2.53059

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.497, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41436
[1mStep[0m  [4/42], [94mLoss[0m : 2.51060
[1mStep[0m  [8/42], [94mLoss[0m : 2.43725
[1mStep[0m  [12/42], [94mLoss[0m : 2.76122
[1mStep[0m  [16/42], [94mLoss[0m : 2.40103
[1mStep[0m  [20/42], [94mLoss[0m : 2.45526
[1mStep[0m  [24/42], [94mLoss[0m : 2.65283
[1mStep[0m  [28/42], [94mLoss[0m : 2.73460
[1mStep[0m  [32/42], [94mLoss[0m : 2.44139
[1mStep[0m  [36/42], [94mLoss[0m : 2.27704
[1mStep[0m  [40/42], [94mLoss[0m : 2.46160

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.489, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68979
[1mStep[0m  [4/42], [94mLoss[0m : 2.66069
[1mStep[0m  [8/42], [94mLoss[0m : 2.52126
[1mStep[0m  [12/42], [94mLoss[0m : 2.46811
[1mStep[0m  [16/42], [94mLoss[0m : 2.55931
[1mStep[0m  [20/42], [94mLoss[0m : 2.60453
[1mStep[0m  [24/42], [94mLoss[0m : 2.38936
[1mStep[0m  [28/42], [94mLoss[0m : 2.70052
[1mStep[0m  [32/42], [94mLoss[0m : 2.46118
[1mStep[0m  [36/42], [94mLoss[0m : 2.37462
[1mStep[0m  [40/42], [94mLoss[0m : 2.57628

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.481, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41497
[1mStep[0m  [4/42], [94mLoss[0m : 2.56295
[1mStep[0m  [8/42], [94mLoss[0m : 2.68503
[1mStep[0m  [12/42], [94mLoss[0m : 2.53718
[1mStep[0m  [16/42], [94mLoss[0m : 2.51279
[1mStep[0m  [20/42], [94mLoss[0m : 2.49478
[1mStep[0m  [24/42], [94mLoss[0m : 2.69505
[1mStep[0m  [28/42], [94mLoss[0m : 2.66678
[1mStep[0m  [32/42], [94mLoss[0m : 2.61508
[1mStep[0m  [36/42], [94mLoss[0m : 2.57714
[1mStep[0m  [40/42], [94mLoss[0m : 2.65131

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.471, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42276
[1mStep[0m  [4/42], [94mLoss[0m : 2.61844
[1mStep[0m  [8/42], [94mLoss[0m : 2.50390
[1mStep[0m  [12/42], [94mLoss[0m : 2.29940
[1mStep[0m  [16/42], [94mLoss[0m : 2.67422
[1mStep[0m  [20/42], [94mLoss[0m : 2.46015
[1mStep[0m  [24/42], [94mLoss[0m : 2.49895
[1mStep[0m  [28/42], [94mLoss[0m : 2.63863
[1mStep[0m  [32/42], [94mLoss[0m : 2.51377
[1mStep[0m  [36/42], [94mLoss[0m : 2.47712
[1mStep[0m  [40/42], [94mLoss[0m : 2.48534

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.466, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.68781
[1mStep[0m  [4/42], [94mLoss[0m : 2.51468
[1mStep[0m  [8/42], [94mLoss[0m : 2.49051
[1mStep[0m  [12/42], [94mLoss[0m : 2.54859
[1mStep[0m  [16/42], [94mLoss[0m : 2.55348
[1mStep[0m  [20/42], [94mLoss[0m : 2.67561
[1mStep[0m  [24/42], [94mLoss[0m : 2.52215
[1mStep[0m  [28/42], [94mLoss[0m : 2.39927
[1mStep[0m  [32/42], [94mLoss[0m : 2.47427
[1mStep[0m  [36/42], [94mLoss[0m : 2.48866
[1mStep[0m  [40/42], [94mLoss[0m : 2.57577

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61070
[1mStep[0m  [4/42], [94mLoss[0m : 2.45562
[1mStep[0m  [8/42], [94mLoss[0m : 2.63566
[1mStep[0m  [12/42], [94mLoss[0m : 2.79629
[1mStep[0m  [16/42], [94mLoss[0m : 2.44836
[1mStep[0m  [20/42], [94mLoss[0m : 2.47590
[1mStep[0m  [24/42], [94mLoss[0m : 2.50622
[1mStep[0m  [28/42], [94mLoss[0m : 2.44618
[1mStep[0m  [32/42], [94mLoss[0m : 2.51383
[1mStep[0m  [36/42], [94mLoss[0m : 2.50549
[1mStep[0m  [40/42], [94mLoss[0m : 2.52987

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.456, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55549
[1mStep[0m  [4/42], [94mLoss[0m : 2.50722
[1mStep[0m  [8/42], [94mLoss[0m : 2.40996
[1mStep[0m  [12/42], [94mLoss[0m : 2.40772
[1mStep[0m  [16/42], [94mLoss[0m : 2.44236
[1mStep[0m  [20/42], [94mLoss[0m : 2.76191
[1mStep[0m  [24/42], [94mLoss[0m : 2.54596
[1mStep[0m  [28/42], [94mLoss[0m : 2.50561
[1mStep[0m  [32/42], [94mLoss[0m : 2.44894
[1mStep[0m  [36/42], [94mLoss[0m : 2.48692
[1mStep[0m  [40/42], [94mLoss[0m : 2.53692

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.461, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.66503
[1mStep[0m  [4/42], [94mLoss[0m : 2.50702
[1mStep[0m  [8/42], [94mLoss[0m : 2.74989
[1mStep[0m  [12/42], [94mLoss[0m : 2.55990
[1mStep[0m  [16/42], [94mLoss[0m : 2.52705
[1mStep[0m  [20/42], [94mLoss[0m : 2.46300
[1mStep[0m  [24/42], [94mLoss[0m : 2.52969
[1mStep[0m  [28/42], [94mLoss[0m : 2.46617
[1mStep[0m  [32/42], [94mLoss[0m : 2.52466
[1mStep[0m  [36/42], [94mLoss[0m : 2.41878
[1mStep[0m  [40/42], [94mLoss[0m : 2.43112

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.453, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.55618
[1mStep[0m  [4/42], [94mLoss[0m : 2.67851
[1mStep[0m  [8/42], [94mLoss[0m : 2.64724
[1mStep[0m  [12/42], [94mLoss[0m : 2.60392
[1mStep[0m  [16/42], [94mLoss[0m : 2.54633
[1mStep[0m  [20/42], [94mLoss[0m : 2.41870
[1mStep[0m  [24/42], [94mLoss[0m : 2.57322
[1mStep[0m  [28/42], [94mLoss[0m : 2.54236
[1mStep[0m  [32/42], [94mLoss[0m : 2.60894
[1mStep[0m  [36/42], [94mLoss[0m : 2.64238
[1mStep[0m  [40/42], [94mLoss[0m : 2.47461

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.441, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30263
[1mStep[0m  [4/42], [94mLoss[0m : 2.60403
[1mStep[0m  [8/42], [94mLoss[0m : 2.24446
[1mStep[0m  [12/42], [94mLoss[0m : 2.69884
[1mStep[0m  [16/42], [94mLoss[0m : 2.61990
[1mStep[0m  [20/42], [94mLoss[0m : 2.57117
[1mStep[0m  [24/42], [94mLoss[0m : 2.45209
[1mStep[0m  [28/42], [94mLoss[0m : 2.69135
[1mStep[0m  [32/42], [94mLoss[0m : 2.62276
[1mStep[0m  [36/42], [94mLoss[0m : 2.63410
[1mStep[0m  [40/42], [94mLoss[0m : 2.48967

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.444, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61430
[1mStep[0m  [4/42], [94mLoss[0m : 2.48347
[1mStep[0m  [8/42], [94mLoss[0m : 2.38010
[1mStep[0m  [12/42], [94mLoss[0m : 2.57741
[1mStep[0m  [16/42], [94mLoss[0m : 2.54955
[1mStep[0m  [20/42], [94mLoss[0m : 2.76848
[1mStep[0m  [24/42], [94mLoss[0m : 2.44712
[1mStep[0m  [28/42], [94mLoss[0m : 2.47579
[1mStep[0m  [32/42], [94mLoss[0m : 2.46247
[1mStep[0m  [36/42], [94mLoss[0m : 2.53388
[1mStep[0m  [40/42], [94mLoss[0m : 2.50201

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.439, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.54908
[1mStep[0m  [4/42], [94mLoss[0m : 2.52254
[1mStep[0m  [8/42], [94mLoss[0m : 2.41506
[1mStep[0m  [12/42], [94mLoss[0m : 2.42460
[1mStep[0m  [16/42], [94mLoss[0m : 2.30205
[1mStep[0m  [20/42], [94mLoss[0m : 2.52931
[1mStep[0m  [24/42], [94mLoss[0m : 2.46251
[1mStep[0m  [28/42], [94mLoss[0m : 2.42308
[1mStep[0m  [32/42], [94mLoss[0m : 2.37602
[1mStep[0m  [36/42], [94mLoss[0m : 2.36163
[1mStep[0m  [40/42], [94mLoss[0m : 2.43727

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.442, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.61834
[1mStep[0m  [4/42], [94mLoss[0m : 2.70092
[1mStep[0m  [8/42], [94mLoss[0m : 2.40927
[1mStep[0m  [12/42], [94mLoss[0m : 2.43471
[1mStep[0m  [16/42], [94mLoss[0m : 2.34289
[1mStep[0m  [20/42], [94mLoss[0m : 2.60806
[1mStep[0m  [24/42], [94mLoss[0m : 2.46301
[1mStep[0m  [28/42], [94mLoss[0m : 2.49932
[1mStep[0m  [32/42], [94mLoss[0m : 2.35115
[1mStep[0m  [36/42], [94mLoss[0m : 2.37206
[1mStep[0m  [40/42], [94mLoss[0m : 2.44480

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.436, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.51828
[1mStep[0m  [4/42], [94mLoss[0m : 2.50881
[1mStep[0m  [8/42], [94mLoss[0m : 2.47256
[1mStep[0m  [12/42], [94mLoss[0m : 2.59428
[1mStep[0m  [16/42], [94mLoss[0m : 2.37795
[1mStep[0m  [20/42], [94mLoss[0m : 2.30979
[1mStep[0m  [24/42], [94mLoss[0m : 2.39879
[1mStep[0m  [28/42], [94mLoss[0m : 2.46099
[1mStep[0m  [32/42], [94mLoss[0m : 2.35001
[1mStep[0m  [36/42], [94mLoss[0m : 2.65119
[1mStep[0m  [40/42], [94mLoss[0m : 2.45303

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.431, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42572
[1mStep[0m  [4/42], [94mLoss[0m : 2.43200
[1mStep[0m  [8/42], [94mLoss[0m : 2.62467
[1mStep[0m  [12/42], [94mLoss[0m : 2.40235
[1mStep[0m  [16/42], [94mLoss[0m : 2.38134
[1mStep[0m  [20/42], [94mLoss[0m : 2.46501
[1mStep[0m  [24/42], [94mLoss[0m : 2.49193
[1mStep[0m  [28/42], [94mLoss[0m : 2.49746
[1mStep[0m  [32/42], [94mLoss[0m : 2.65634
[1mStep[0m  [36/42], [94mLoss[0m : 2.52103
[1mStep[0m  [40/42], [94mLoss[0m : 2.34762

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.426, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.430
====================================

Phase 1 - Evaluation MAE:  2.430319138935634
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/42], [94mLoss[0m : 2.44232
[1mStep[0m  [4/42], [94mLoss[0m : 2.50406
[1mStep[0m  [8/42], [94mLoss[0m : 2.47767
[1mStep[0m  [12/42], [94mLoss[0m : 2.61265
[1mStep[0m  [16/42], [94mLoss[0m : 2.47649
[1mStep[0m  [20/42], [94mLoss[0m : 2.49585
[1mStep[0m  [24/42], [94mLoss[0m : 2.39223
[1mStep[0m  [28/42], [94mLoss[0m : 2.51755
[1mStep[0m  [32/42], [94mLoss[0m : 2.61183
[1mStep[0m  [36/42], [94mLoss[0m : 2.50567
[1mStep[0m  [40/42], [94mLoss[0m : 2.55854

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.481, [92mTest[0m: 2.428, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40266
[1mStep[0m  [4/42], [94mLoss[0m : 2.59300
[1mStep[0m  [8/42], [94mLoss[0m : 2.55909
[1mStep[0m  [12/42], [94mLoss[0m : 2.39437
[1mStep[0m  [16/42], [94mLoss[0m : 2.50284
[1mStep[0m  [20/42], [94mLoss[0m : 2.42913
[1mStep[0m  [24/42], [94mLoss[0m : 2.43473
[1mStep[0m  [28/42], [94mLoss[0m : 2.52500
[1mStep[0m  [32/42], [94mLoss[0m : 2.40010
[1mStep[0m  [36/42], [94mLoss[0m : 2.47915
[1mStep[0m  [40/42], [94mLoss[0m : 2.35878

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.420, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.45806
[1mStep[0m  [4/42], [94mLoss[0m : 2.37494
[1mStep[0m  [8/42], [94mLoss[0m : 2.56640
[1mStep[0m  [12/42], [94mLoss[0m : 2.50906
[1mStep[0m  [16/42], [94mLoss[0m : 2.46244
[1mStep[0m  [20/42], [94mLoss[0m : 2.32455
[1mStep[0m  [24/42], [94mLoss[0m : 2.50037
[1mStep[0m  [28/42], [94mLoss[0m : 2.43821
[1mStep[0m  [32/42], [94mLoss[0m : 2.63468
[1mStep[0m  [36/42], [94mLoss[0m : 2.27915
[1mStep[0m  [40/42], [94mLoss[0m : 2.35061

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.415, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.52711
[1mStep[0m  [4/42], [94mLoss[0m : 2.39335
[1mStep[0m  [8/42], [94mLoss[0m : 2.43226
[1mStep[0m  [12/42], [94mLoss[0m : 2.45763
[1mStep[0m  [16/42], [94mLoss[0m : 2.56333
[1mStep[0m  [20/42], [94mLoss[0m : 2.45644
[1mStep[0m  [24/42], [94mLoss[0m : 2.45887
[1mStep[0m  [28/42], [94mLoss[0m : 2.54666
[1mStep[0m  [32/42], [94mLoss[0m : 2.43862
[1mStep[0m  [36/42], [94mLoss[0m : 2.54887
[1mStep[0m  [40/42], [94mLoss[0m : 2.49410

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.417, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.29463
[1mStep[0m  [4/42], [94mLoss[0m : 2.59645
[1mStep[0m  [8/42], [94mLoss[0m : 2.47343
[1mStep[0m  [12/42], [94mLoss[0m : 2.36010
[1mStep[0m  [16/42], [94mLoss[0m : 2.61662
[1mStep[0m  [20/42], [94mLoss[0m : 2.15771
[1mStep[0m  [24/42], [94mLoss[0m : 2.55258
[1mStep[0m  [28/42], [94mLoss[0m : 2.34395
[1mStep[0m  [32/42], [94mLoss[0m : 2.25452
[1mStep[0m  [36/42], [94mLoss[0m : 2.53417
[1mStep[0m  [40/42], [94mLoss[0m : 2.27627

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.405, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34005
[1mStep[0m  [4/42], [94mLoss[0m : 2.47149
[1mStep[0m  [8/42], [94mLoss[0m : 2.68963
[1mStep[0m  [12/42], [94mLoss[0m : 2.24319
[1mStep[0m  [16/42], [94mLoss[0m : 2.22316
[1mStep[0m  [20/42], [94mLoss[0m : 2.31588
[1mStep[0m  [24/42], [94mLoss[0m : 2.63638
[1mStep[0m  [28/42], [94mLoss[0m : 2.55098
[1mStep[0m  [32/42], [94mLoss[0m : 2.40701
[1mStep[0m  [36/42], [94mLoss[0m : 2.44761
[1mStep[0m  [40/42], [94mLoss[0m : 2.44652

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.407, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.60655
[1mStep[0m  [4/42], [94mLoss[0m : 2.30840
[1mStep[0m  [8/42], [94mLoss[0m : 2.46023
[1mStep[0m  [12/42], [94mLoss[0m : 2.41214
[1mStep[0m  [16/42], [94mLoss[0m : 2.43553
[1mStep[0m  [20/42], [94mLoss[0m : 2.49443
[1mStep[0m  [24/42], [94mLoss[0m : 2.59064
[1mStep[0m  [28/42], [94mLoss[0m : 2.44488
[1mStep[0m  [32/42], [94mLoss[0m : 2.64479
[1mStep[0m  [36/42], [94mLoss[0m : 2.63425
[1mStep[0m  [40/42], [94mLoss[0m : 2.45196

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.397, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.62046
[1mStep[0m  [4/42], [94mLoss[0m : 2.62971
[1mStep[0m  [8/42], [94mLoss[0m : 2.38189
[1mStep[0m  [12/42], [94mLoss[0m : 2.46005
[1mStep[0m  [16/42], [94mLoss[0m : 2.58719
[1mStep[0m  [20/42], [94mLoss[0m : 2.29716
[1mStep[0m  [24/42], [94mLoss[0m : 2.49574
[1mStep[0m  [28/42], [94mLoss[0m : 2.43908
[1mStep[0m  [32/42], [94mLoss[0m : 2.54162
[1mStep[0m  [36/42], [94mLoss[0m : 2.34763
[1mStep[0m  [40/42], [94mLoss[0m : 2.61855

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.393, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49091
[1mStep[0m  [4/42], [94mLoss[0m : 2.29953
[1mStep[0m  [8/42], [94mLoss[0m : 2.67996
[1mStep[0m  [12/42], [94mLoss[0m : 2.29113
[1mStep[0m  [16/42], [94mLoss[0m : 2.45237
[1mStep[0m  [20/42], [94mLoss[0m : 2.44841
[1mStep[0m  [24/42], [94mLoss[0m : 2.58771
[1mStep[0m  [28/42], [94mLoss[0m : 2.43878
[1mStep[0m  [32/42], [94mLoss[0m : 2.34181
[1mStep[0m  [36/42], [94mLoss[0m : 2.42243
[1mStep[0m  [40/42], [94mLoss[0m : 2.40427

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.395, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.59192
[1mStep[0m  [4/42], [94mLoss[0m : 2.34781
[1mStep[0m  [8/42], [94mLoss[0m : 2.37200
[1mStep[0m  [12/42], [94mLoss[0m : 2.43253
[1mStep[0m  [16/42], [94mLoss[0m : 2.26975
[1mStep[0m  [20/42], [94mLoss[0m : 2.50526
[1mStep[0m  [24/42], [94mLoss[0m : 2.49150
[1mStep[0m  [28/42], [94mLoss[0m : 2.74479
[1mStep[0m  [32/42], [94mLoss[0m : 2.48718
[1mStep[0m  [36/42], [94mLoss[0m : 2.41179
[1mStep[0m  [40/42], [94mLoss[0m : 2.32236

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.391, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.70749
[1mStep[0m  [4/42], [94mLoss[0m : 2.66412
[1mStep[0m  [8/42], [94mLoss[0m : 2.69993
[1mStep[0m  [12/42], [94mLoss[0m : 2.43769
[1mStep[0m  [16/42], [94mLoss[0m : 2.42075
[1mStep[0m  [20/42], [94mLoss[0m : 2.67877
[1mStep[0m  [24/42], [94mLoss[0m : 2.35437
[1mStep[0m  [28/42], [94mLoss[0m : 2.44842
[1mStep[0m  [32/42], [94mLoss[0m : 2.49414
[1mStep[0m  [36/42], [94mLoss[0m : 2.47075
[1mStep[0m  [40/42], [94mLoss[0m : 2.56697

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.42595
[1mStep[0m  [4/42], [94mLoss[0m : 2.22812
[1mStep[0m  [8/42], [94mLoss[0m : 2.42759
[1mStep[0m  [12/42], [94mLoss[0m : 2.30827
[1mStep[0m  [16/42], [94mLoss[0m : 2.53715
[1mStep[0m  [20/42], [94mLoss[0m : 2.44424
[1mStep[0m  [24/42], [94mLoss[0m : 2.55965
[1mStep[0m  [28/42], [94mLoss[0m : 2.55602
[1mStep[0m  [32/42], [94mLoss[0m : 2.57003
[1mStep[0m  [36/42], [94mLoss[0m : 2.51137
[1mStep[0m  [40/42], [94mLoss[0m : 2.26496

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.391, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.41110
[1mStep[0m  [4/42], [94mLoss[0m : 2.13160
[1mStep[0m  [8/42], [94mLoss[0m : 2.48032
[1mStep[0m  [12/42], [94mLoss[0m : 2.52269
[1mStep[0m  [16/42], [94mLoss[0m : 2.64613
[1mStep[0m  [20/42], [94mLoss[0m : 2.32041
[1mStep[0m  [24/42], [94mLoss[0m : 2.26354
[1mStep[0m  [28/42], [94mLoss[0m : 2.28217
[1mStep[0m  [32/42], [94mLoss[0m : 2.41485
[1mStep[0m  [36/42], [94mLoss[0m : 2.49476
[1mStep[0m  [40/42], [94mLoss[0m : 2.32678

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.394, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.25623
[1mStep[0m  [4/42], [94mLoss[0m : 2.49639
[1mStep[0m  [8/42], [94mLoss[0m : 2.54307
[1mStep[0m  [12/42], [94mLoss[0m : 2.49598
[1mStep[0m  [16/42], [94mLoss[0m : 2.61151
[1mStep[0m  [20/42], [94mLoss[0m : 2.29938
[1mStep[0m  [24/42], [94mLoss[0m : 2.44746
[1mStep[0m  [28/42], [94mLoss[0m : 2.35241
[1mStep[0m  [32/42], [94mLoss[0m : 2.49643
[1mStep[0m  [36/42], [94mLoss[0m : 2.40688
[1mStep[0m  [40/42], [94mLoss[0m : 2.52879

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.392, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33711
[1mStep[0m  [4/42], [94mLoss[0m : 2.49012
[1mStep[0m  [8/42], [94mLoss[0m : 2.62461
[1mStep[0m  [12/42], [94mLoss[0m : 2.34730
[1mStep[0m  [16/42], [94mLoss[0m : 2.59465
[1mStep[0m  [20/42], [94mLoss[0m : 2.24143
[1mStep[0m  [24/42], [94mLoss[0m : 2.46714
[1mStep[0m  [28/42], [94mLoss[0m : 2.21860
[1mStep[0m  [32/42], [94mLoss[0m : 2.64400
[1mStep[0m  [36/42], [94mLoss[0m : 2.71753
[1mStep[0m  [40/42], [94mLoss[0m : 2.30089

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.379, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.24912
[1mStep[0m  [4/42], [94mLoss[0m : 2.53489
[1mStep[0m  [8/42], [94mLoss[0m : 2.33542
[1mStep[0m  [12/42], [94mLoss[0m : 2.32471
[1mStep[0m  [16/42], [94mLoss[0m : 2.40056
[1mStep[0m  [20/42], [94mLoss[0m : 2.64301
[1mStep[0m  [24/42], [94mLoss[0m : 2.27813
[1mStep[0m  [28/42], [94mLoss[0m : 2.35770
[1mStep[0m  [32/42], [94mLoss[0m : 2.44839
[1mStep[0m  [36/42], [94mLoss[0m : 2.36842
[1mStep[0m  [40/42], [94mLoss[0m : 2.47163

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.378, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.33226
[1mStep[0m  [4/42], [94mLoss[0m : 2.49982
[1mStep[0m  [8/42], [94mLoss[0m : 2.53644
[1mStep[0m  [12/42], [94mLoss[0m : 2.31992
[1mStep[0m  [16/42], [94mLoss[0m : 2.49834
[1mStep[0m  [20/42], [94mLoss[0m : 2.84779
[1mStep[0m  [24/42], [94mLoss[0m : 2.39262
[1mStep[0m  [28/42], [94mLoss[0m : 2.44394
[1mStep[0m  [32/42], [94mLoss[0m : 2.32284
[1mStep[0m  [36/42], [94mLoss[0m : 2.37423
[1mStep[0m  [40/42], [94mLoss[0m : 2.32013

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.373, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.36746
[1mStep[0m  [4/42], [94mLoss[0m : 2.40035
[1mStep[0m  [8/42], [94mLoss[0m : 2.30013
[1mStep[0m  [12/42], [94mLoss[0m : 2.36408
[1mStep[0m  [16/42], [94mLoss[0m : 2.59744
[1mStep[0m  [20/42], [94mLoss[0m : 2.35397
[1mStep[0m  [24/42], [94mLoss[0m : 2.38042
[1mStep[0m  [28/42], [94mLoss[0m : 2.38591
[1mStep[0m  [32/42], [94mLoss[0m : 2.13839
[1mStep[0m  [36/42], [94mLoss[0m : 2.21987
[1mStep[0m  [40/42], [94mLoss[0m : 2.56597

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.375, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31335
[1mStep[0m  [4/42], [94mLoss[0m : 2.35300
[1mStep[0m  [8/42], [94mLoss[0m : 2.18188
[1mStep[0m  [12/42], [94mLoss[0m : 2.58293
[1mStep[0m  [16/42], [94mLoss[0m : 2.39510
[1mStep[0m  [20/42], [94mLoss[0m : 2.34849
[1mStep[0m  [24/42], [94mLoss[0m : 2.26538
[1mStep[0m  [28/42], [94mLoss[0m : 2.28711
[1mStep[0m  [32/42], [94mLoss[0m : 2.41546
[1mStep[0m  [36/42], [94mLoss[0m : 2.53343
[1mStep[0m  [40/42], [94mLoss[0m : 2.37117

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.367, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.31764
[1mStep[0m  [4/42], [94mLoss[0m : 2.47205
[1mStep[0m  [8/42], [94mLoss[0m : 2.16421
[1mStep[0m  [12/42], [94mLoss[0m : 2.47571
[1mStep[0m  [16/42], [94mLoss[0m : 2.45290
[1mStep[0m  [20/42], [94mLoss[0m : 2.47212
[1mStep[0m  [24/42], [94mLoss[0m : 2.41280
[1mStep[0m  [28/42], [94mLoss[0m : 2.40623
[1mStep[0m  [32/42], [94mLoss[0m : 2.03583
[1mStep[0m  [36/42], [94mLoss[0m : 2.46205
[1mStep[0m  [40/42], [94mLoss[0m : 2.49275

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.375, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.49286
[1mStep[0m  [4/42], [94mLoss[0m : 2.61320
[1mStep[0m  [8/42], [94mLoss[0m : 2.22277
[1mStep[0m  [12/42], [94mLoss[0m : 2.31518
[1mStep[0m  [16/42], [94mLoss[0m : 2.24916
[1mStep[0m  [20/42], [94mLoss[0m : 2.46840
[1mStep[0m  [24/42], [94mLoss[0m : 2.32791
[1mStep[0m  [28/42], [94mLoss[0m : 2.33210
[1mStep[0m  [32/42], [94mLoss[0m : 2.48572
[1mStep[0m  [36/42], [94mLoss[0m : 2.32670
[1mStep[0m  [40/42], [94mLoss[0m : 2.41775

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.371, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.30589
[1mStep[0m  [4/42], [94mLoss[0m : 2.44488
[1mStep[0m  [8/42], [94mLoss[0m : 2.48856
[1mStep[0m  [12/42], [94mLoss[0m : 2.18786
[1mStep[0m  [16/42], [94mLoss[0m : 2.40225
[1mStep[0m  [20/42], [94mLoss[0m : 2.35660
[1mStep[0m  [24/42], [94mLoss[0m : 2.40555
[1mStep[0m  [28/42], [94mLoss[0m : 2.43148
[1mStep[0m  [32/42], [94mLoss[0m : 2.52497
[1mStep[0m  [36/42], [94mLoss[0m : 2.10736
[1mStep[0m  [40/42], [94mLoss[0m : 2.43562

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.385, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.26847
[1mStep[0m  [4/42], [94mLoss[0m : 2.46983
[1mStep[0m  [8/42], [94mLoss[0m : 2.21311
[1mStep[0m  [12/42], [94mLoss[0m : 2.24659
[1mStep[0m  [16/42], [94mLoss[0m : 2.21563
[1mStep[0m  [20/42], [94mLoss[0m : 2.41816
[1mStep[0m  [24/42], [94mLoss[0m : 2.30632
[1mStep[0m  [28/42], [94mLoss[0m : 2.39886
[1mStep[0m  [32/42], [94mLoss[0m : 2.22562
[1mStep[0m  [36/42], [94mLoss[0m : 2.41918
[1mStep[0m  [40/42], [94mLoss[0m : 2.35538

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.377, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.34054
[1mStep[0m  [4/42], [94mLoss[0m : 2.41462
[1mStep[0m  [8/42], [94mLoss[0m : 2.29590
[1mStep[0m  [12/42], [94mLoss[0m : 2.46571
[1mStep[0m  [16/42], [94mLoss[0m : 2.50876
[1mStep[0m  [20/42], [94mLoss[0m : 2.45548
[1mStep[0m  [24/42], [94mLoss[0m : 2.22416
[1mStep[0m  [28/42], [94mLoss[0m : 2.60163
[1mStep[0m  [32/42], [94mLoss[0m : 2.24429
[1mStep[0m  [36/42], [94mLoss[0m : 2.26937
[1mStep[0m  [40/42], [94mLoss[0m : 2.40313

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.391, [92mTest[0m: 2.368, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.46244
[1mStep[0m  [4/42], [94mLoss[0m : 2.39834
[1mStep[0m  [8/42], [94mLoss[0m : 2.24033
[1mStep[0m  [12/42], [94mLoss[0m : 2.45568
[1mStep[0m  [16/42], [94mLoss[0m : 2.41544
[1mStep[0m  [20/42], [94mLoss[0m : 2.29067
[1mStep[0m  [24/42], [94mLoss[0m : 2.55261
[1mStep[0m  [28/42], [94mLoss[0m : 2.28771
[1mStep[0m  [32/42], [94mLoss[0m : 2.54992
[1mStep[0m  [36/42], [94mLoss[0m : 2.39472
[1mStep[0m  [40/42], [94mLoss[0m : 2.40656

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.379, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.40714
[1mStep[0m  [4/42], [94mLoss[0m : 2.29366
[1mStep[0m  [8/42], [94mLoss[0m : 2.48628
[1mStep[0m  [12/42], [94mLoss[0m : 2.29467
[1mStep[0m  [16/42], [94mLoss[0m : 2.42870
[1mStep[0m  [20/42], [94mLoss[0m : 2.32676
[1mStep[0m  [24/42], [94mLoss[0m : 2.34157
[1mStep[0m  [28/42], [94mLoss[0m : 2.33654
[1mStep[0m  [32/42], [94mLoss[0m : 2.44892
[1mStep[0m  [36/42], [94mLoss[0m : 2.48420
[1mStep[0m  [40/42], [94mLoss[0m : 2.26623

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.382, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.57894
[1mStep[0m  [4/42], [94mLoss[0m : 2.37735
[1mStep[0m  [8/42], [94mLoss[0m : 2.40453
[1mStep[0m  [12/42], [94mLoss[0m : 2.25379
[1mStep[0m  [16/42], [94mLoss[0m : 2.36618
[1mStep[0m  [20/42], [94mLoss[0m : 2.40679
[1mStep[0m  [24/42], [94mLoss[0m : 2.41709
[1mStep[0m  [28/42], [94mLoss[0m : 2.35688
[1mStep[0m  [32/42], [94mLoss[0m : 2.38518
[1mStep[0m  [36/42], [94mLoss[0m : 2.27163
[1mStep[0m  [40/42], [94mLoss[0m : 2.48461

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.375, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.37765
[1mStep[0m  [4/42], [94mLoss[0m : 2.46447
[1mStep[0m  [8/42], [94mLoss[0m : 2.23328
[1mStep[0m  [12/42], [94mLoss[0m : 2.49547
[1mStep[0m  [16/42], [94mLoss[0m : 2.30113
[1mStep[0m  [20/42], [94mLoss[0m : 2.36337
[1mStep[0m  [24/42], [94mLoss[0m : 2.55097
[1mStep[0m  [28/42], [94mLoss[0m : 2.39065
[1mStep[0m  [32/42], [94mLoss[0m : 2.30235
[1mStep[0m  [36/42], [94mLoss[0m : 2.39767
[1mStep[0m  [40/42], [94mLoss[0m : 2.35373

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.355, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.32313
[1mStep[0m  [4/42], [94mLoss[0m : 2.62157
[1mStep[0m  [8/42], [94mLoss[0m : 2.31677
[1mStep[0m  [12/42], [94mLoss[0m : 2.54431
[1mStep[0m  [16/42], [94mLoss[0m : 2.39095
[1mStep[0m  [20/42], [94mLoss[0m : 2.36993
[1mStep[0m  [24/42], [94mLoss[0m : 2.42199
[1mStep[0m  [28/42], [94mLoss[0m : 2.43259
[1mStep[0m  [32/42], [94mLoss[0m : 2.23615
[1mStep[0m  [36/42], [94mLoss[0m : 2.53231
[1mStep[0m  [40/42], [94mLoss[0m : 2.49621

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.376, [92mTest[0m: 2.383, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 2.44265
[1mStep[0m  [4/42], [94mLoss[0m : 2.39620
[1mStep[0m  [8/42], [94mLoss[0m : 2.44372
[1mStep[0m  [12/42], [94mLoss[0m : 2.37609
[1mStep[0m  [16/42], [94mLoss[0m : 2.36114
[1mStep[0m  [20/42], [94mLoss[0m : 2.38812
[1mStep[0m  [24/42], [94mLoss[0m : 2.28665
[1mStep[0m  [28/42], [94mLoss[0m : 2.35874
[1mStep[0m  [32/42], [94mLoss[0m : 2.40319
[1mStep[0m  [36/42], [94mLoss[0m : 2.49251
[1mStep[0m  [40/42], [94mLoss[0m : 2.45226

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.362, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.358
====================================

Phase 2 - Evaluation MAE:  2.3582424436296736
MAE score P1      2.430319
MAE score P2      2.358242
loss               2.37563
learning_rate       0.0001
batch_size             256
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.9
weight_decay          0.01
Name: 27, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 10.78950
[1mStep[0m  [4/42], [94mLoss[0m : 11.29798
[1mStep[0m  [8/42], [94mLoss[0m : 10.85738
[1mStep[0m  [12/42], [94mLoss[0m : 11.14695
[1mStep[0m  [16/42], [94mLoss[0m : 10.99878
[1mStep[0m  [20/42], [94mLoss[0m : 10.68878
[1mStep[0m  [24/42], [94mLoss[0m : 11.13952
[1mStep[0m  [28/42], [94mLoss[0m : 10.31376
[1mStep[0m  [32/42], [94mLoss[0m : 10.94235
[1mStep[0m  [36/42], [94mLoss[0m : 10.73225
[1mStep[0m  [40/42], [94mLoss[0m : 11.36664

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.974, [92mTest[0m: 11.007, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.27169
[1mStep[0m  [4/42], [94mLoss[0m : 10.93781
[1mStep[0m  [8/42], [94mLoss[0m : 11.14798
[1mStep[0m  [12/42], [94mLoss[0m : 10.58852
[1mStep[0m  [16/42], [94mLoss[0m : 10.91402
[1mStep[0m  [20/42], [94mLoss[0m : 11.01946
[1mStep[0m  [24/42], [94mLoss[0m : 10.90344
[1mStep[0m  [28/42], [94mLoss[0m : 10.56744
[1mStep[0m  [32/42], [94mLoss[0m : 10.94199
[1mStep[0m  [36/42], [94mLoss[0m : 11.01735
[1mStep[0m  [40/42], [94mLoss[0m : 10.76471

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.923, [92mTest[0m: 10.877, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 11.01572
[1mStep[0m  [4/42], [94mLoss[0m : 10.79229
[1mStep[0m  [8/42], [94mLoss[0m : 10.39071
[1mStep[0m  [12/42], [94mLoss[0m : 10.73889
[1mStep[0m  [16/42], [94mLoss[0m : 10.63013
[1mStep[0m  [20/42], [94mLoss[0m : 10.92542
[1mStep[0m  [24/42], [94mLoss[0m : 10.75719
[1mStep[0m  [28/42], [94mLoss[0m : 10.91287
[1mStep[0m  [32/42], [94mLoss[0m : 10.92521
[1mStep[0m  [36/42], [94mLoss[0m : 10.71413
[1mStep[0m  [40/42], [94mLoss[0m : 10.54274

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.873, [92mTest[0m: 10.819, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.91766
[1mStep[0m  [4/42], [94mLoss[0m : 10.68225
[1mStep[0m  [8/42], [94mLoss[0m : 10.95398
[1mStep[0m  [12/42], [94mLoss[0m : 11.11674
[1mStep[0m  [16/42], [94mLoss[0m : 11.30194
[1mStep[0m  [20/42], [94mLoss[0m : 10.57749
[1mStep[0m  [24/42], [94mLoss[0m : 10.80756
[1mStep[0m  [28/42], [94mLoss[0m : 10.52647
[1mStep[0m  [32/42], [94mLoss[0m : 10.69916
[1mStep[0m  [36/42], [94mLoss[0m : 10.79705
[1mStep[0m  [40/42], [94mLoss[0m : 10.77641

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 10.821, [92mTest[0m: 10.730, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.97333
[1mStep[0m  [4/42], [94mLoss[0m : 10.82328
[1mStep[0m  [8/42], [94mLoss[0m : 10.73920
[1mStep[0m  [12/42], [94mLoss[0m : 10.44370
[1mStep[0m  [16/42], [94mLoss[0m : 10.31940
[1mStep[0m  [20/42], [94mLoss[0m : 10.81472
[1mStep[0m  [24/42], [94mLoss[0m : 10.83473
[1mStep[0m  [28/42], [94mLoss[0m : 10.87016
[1mStep[0m  [32/42], [94mLoss[0m : 10.89469
[1mStep[0m  [36/42], [94mLoss[0m : 10.38674
[1mStep[0m  [40/42], [94mLoss[0m : 10.90839

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 10.774, [92mTest[0m: 10.675, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.79703
[1mStep[0m  [4/42], [94mLoss[0m : 10.66681
[1mStep[0m  [8/42], [94mLoss[0m : 10.65094
[1mStep[0m  [12/42], [94mLoss[0m : 10.85218
[1mStep[0m  [16/42], [94mLoss[0m : 10.93017
[1mStep[0m  [20/42], [94mLoss[0m : 10.57100
[1mStep[0m  [24/42], [94mLoss[0m : 10.76139
[1mStep[0m  [28/42], [94mLoss[0m : 10.81419
[1mStep[0m  [32/42], [94mLoss[0m : 10.58430
[1mStep[0m  [36/42], [94mLoss[0m : 10.58691
[1mStep[0m  [40/42], [94mLoss[0m : 10.69147

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 10.716, [92mTest[0m: 10.603, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.57697
[1mStep[0m  [4/42], [94mLoss[0m : 10.78986
[1mStep[0m  [8/42], [94mLoss[0m : 10.36173
[1mStep[0m  [12/42], [94mLoss[0m : 10.68069
[1mStep[0m  [16/42], [94mLoss[0m : 10.83161
[1mStep[0m  [20/42], [94mLoss[0m : 11.04092
[1mStep[0m  [24/42], [94mLoss[0m : 10.33637
[1mStep[0m  [28/42], [94mLoss[0m : 10.69371
[1mStep[0m  [32/42], [94mLoss[0m : 10.63057
[1mStep[0m  [36/42], [94mLoss[0m : 10.78452
[1mStep[0m  [40/42], [94mLoss[0m : 10.78613

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 10.662, [92mTest[0m: 10.549, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.65483
[1mStep[0m  [4/42], [94mLoss[0m : 10.60503
[1mStep[0m  [8/42], [94mLoss[0m : 10.61967
[1mStep[0m  [12/42], [94mLoss[0m : 10.32769
[1mStep[0m  [16/42], [94mLoss[0m : 10.41110
[1mStep[0m  [20/42], [94mLoss[0m : 10.50225
[1mStep[0m  [24/42], [94mLoss[0m : 10.78628
[1mStep[0m  [28/42], [94mLoss[0m : 10.18624
[1mStep[0m  [32/42], [94mLoss[0m : 10.45896
[1mStep[0m  [36/42], [94mLoss[0m : 10.86011
[1mStep[0m  [40/42], [94mLoss[0m : 10.50722

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 10.614, [92mTest[0m: 10.477, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.76118
[1mStep[0m  [4/42], [94mLoss[0m : 10.73878
[1mStep[0m  [8/42], [94mLoss[0m : 10.83434
[1mStep[0m  [12/42], [94mLoss[0m : 10.69855
[1mStep[0m  [16/42], [94mLoss[0m : 10.01101
[1mStep[0m  [20/42], [94mLoss[0m : 10.51279
[1mStep[0m  [24/42], [94mLoss[0m : 10.20193
[1mStep[0m  [28/42], [94mLoss[0m : 11.04557
[1mStep[0m  [32/42], [94mLoss[0m : 10.61536
[1mStep[0m  [36/42], [94mLoss[0m : 10.56099
[1mStep[0m  [40/42], [94mLoss[0m : 11.07884

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 10.556, [92mTest[0m: 10.413, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.83778
[1mStep[0m  [4/42], [94mLoss[0m : 10.33990
[1mStep[0m  [8/42], [94mLoss[0m : 10.45218
[1mStep[0m  [12/42], [94mLoss[0m : 10.49744
[1mStep[0m  [16/42], [94mLoss[0m : 10.39007
[1mStep[0m  [20/42], [94mLoss[0m : 10.61348
[1mStep[0m  [24/42], [94mLoss[0m : 10.66342
[1mStep[0m  [28/42], [94mLoss[0m : 10.10705
[1mStep[0m  [32/42], [94mLoss[0m : 10.52031
[1mStep[0m  [36/42], [94mLoss[0m : 10.43426
[1mStep[0m  [40/42], [94mLoss[0m : 11.00267

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 10.507, [92mTest[0m: 10.331, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.90157
[1mStep[0m  [4/42], [94mLoss[0m : 10.26079
[1mStep[0m  [8/42], [94mLoss[0m : 10.67600
[1mStep[0m  [12/42], [94mLoss[0m : 10.28252
[1mStep[0m  [16/42], [94mLoss[0m : 10.67316
[1mStep[0m  [20/42], [94mLoss[0m : 10.41906
[1mStep[0m  [24/42], [94mLoss[0m : 10.28542
[1mStep[0m  [28/42], [94mLoss[0m : 10.60036
[1mStep[0m  [32/42], [94mLoss[0m : 10.52279
[1mStep[0m  [36/42], [94mLoss[0m : 10.57954
[1mStep[0m  [40/42], [94mLoss[0m : 10.68142

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 10.444, [92mTest[0m: 10.257, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.54713
[1mStep[0m  [4/42], [94mLoss[0m : 10.28771
[1mStep[0m  [8/42], [94mLoss[0m : 10.53437
[1mStep[0m  [12/42], [94mLoss[0m : 10.40172
[1mStep[0m  [16/42], [94mLoss[0m : 10.79592
[1mStep[0m  [20/42], [94mLoss[0m : 10.36266
[1mStep[0m  [24/42], [94mLoss[0m : 10.55563
[1mStep[0m  [28/42], [94mLoss[0m : 10.67258
[1mStep[0m  [32/42], [94mLoss[0m : 9.98889
[1mStep[0m  [36/42], [94mLoss[0m : 10.49841
[1mStep[0m  [40/42], [94mLoss[0m : 10.34796

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 10.391, [92mTest[0m: 10.203, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.56345
[1mStep[0m  [4/42], [94mLoss[0m : 10.30662
[1mStep[0m  [8/42], [94mLoss[0m : 10.43382
[1mStep[0m  [12/42], [94mLoss[0m : 10.24580
[1mStep[0m  [16/42], [94mLoss[0m : 10.06327
[1mStep[0m  [20/42], [94mLoss[0m : 10.93859
[1mStep[0m  [24/42], [94mLoss[0m : 9.91588
[1mStep[0m  [28/42], [94mLoss[0m : 10.25565
[1mStep[0m  [32/42], [94mLoss[0m : 10.20336
[1mStep[0m  [36/42], [94mLoss[0m : 10.07259
[1mStep[0m  [40/42], [94mLoss[0m : 10.29947

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 10.342, [92mTest[0m: 10.128, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.72234
[1mStep[0m  [4/42], [94mLoss[0m : 10.23240
[1mStep[0m  [8/42], [94mLoss[0m : 10.49985
[1mStep[0m  [12/42], [94mLoss[0m : 10.26793
[1mStep[0m  [16/42], [94mLoss[0m : 10.18316
[1mStep[0m  [20/42], [94mLoss[0m : 10.19808
[1mStep[0m  [24/42], [94mLoss[0m : 10.71737
[1mStep[0m  [28/42], [94mLoss[0m : 10.07832
[1mStep[0m  [32/42], [94mLoss[0m : 10.29310
[1mStep[0m  [36/42], [94mLoss[0m : 10.34156
[1mStep[0m  [40/42], [94mLoss[0m : 10.18652

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 10.293, [92mTest[0m: 10.068, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.14638
[1mStep[0m  [4/42], [94mLoss[0m : 10.10405
[1mStep[0m  [8/42], [94mLoss[0m : 10.35615
[1mStep[0m  [12/42], [94mLoss[0m : 10.22940
[1mStep[0m  [16/42], [94mLoss[0m : 10.32362
[1mStep[0m  [20/42], [94mLoss[0m : 10.26122
[1mStep[0m  [24/42], [94mLoss[0m : 10.14491
[1mStep[0m  [28/42], [94mLoss[0m : 10.17367
[1mStep[0m  [32/42], [94mLoss[0m : 10.28031
[1mStep[0m  [36/42], [94mLoss[0m : 10.13731
[1mStep[0m  [40/42], [94mLoss[0m : 10.12257

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 10.232, [92mTest[0m: 9.988, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.22470
[1mStep[0m  [4/42], [94mLoss[0m : 10.37218
[1mStep[0m  [8/42], [94mLoss[0m : 10.09925
[1mStep[0m  [12/42], [94mLoss[0m : 10.25128
[1mStep[0m  [16/42], [94mLoss[0m : 10.22666
[1mStep[0m  [20/42], [94mLoss[0m : 10.20143
[1mStep[0m  [24/42], [94mLoss[0m : 10.14626
[1mStep[0m  [28/42], [94mLoss[0m : 10.33497
[1mStep[0m  [32/42], [94mLoss[0m : 10.28083
[1mStep[0m  [36/42], [94mLoss[0m : 10.48653
[1mStep[0m  [40/42], [94mLoss[0m : 10.23729

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 10.174, [92mTest[0m: 9.905, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.21728
[1mStep[0m  [4/42], [94mLoss[0m : 10.35603
[1mStep[0m  [8/42], [94mLoss[0m : 10.29355
[1mStep[0m  [12/42], [94mLoss[0m : 10.13254
[1mStep[0m  [16/42], [94mLoss[0m : 10.08045
[1mStep[0m  [20/42], [94mLoss[0m : 10.06419
[1mStep[0m  [24/42], [94mLoss[0m : 9.69634
[1mStep[0m  [28/42], [94mLoss[0m : 9.90511
[1mStep[0m  [32/42], [94mLoss[0m : 10.43861
[1mStep[0m  [36/42], [94mLoss[0m : 9.89394
[1mStep[0m  [40/42], [94mLoss[0m : 9.91693

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 10.114, [92mTest[0m: 9.840, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.63580
[1mStep[0m  [4/42], [94mLoss[0m : 10.26553
[1mStep[0m  [8/42], [94mLoss[0m : 9.95058
[1mStep[0m  [12/42], [94mLoss[0m : 9.89085
[1mStep[0m  [16/42], [94mLoss[0m : 9.82203
[1mStep[0m  [20/42], [94mLoss[0m : 10.04355
[1mStep[0m  [24/42], [94mLoss[0m : 10.49228
[1mStep[0m  [28/42], [94mLoss[0m : 9.84035
[1mStep[0m  [32/42], [94mLoss[0m : 10.17396
[1mStep[0m  [36/42], [94mLoss[0m : 9.64000
[1mStep[0m  [40/42], [94mLoss[0m : 9.94306

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 10.053, [92mTest[0m: 9.756, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.75762
[1mStep[0m  [4/42], [94mLoss[0m : 10.07456
[1mStep[0m  [8/42], [94mLoss[0m : 10.17594
[1mStep[0m  [12/42], [94mLoss[0m : 9.85541
[1mStep[0m  [16/42], [94mLoss[0m : 9.89597
[1mStep[0m  [20/42], [94mLoss[0m : 9.64457
[1mStep[0m  [24/42], [94mLoss[0m : 10.19712
[1mStep[0m  [28/42], [94mLoss[0m : 9.83660
[1mStep[0m  [32/42], [94mLoss[0m : 9.98914
[1mStep[0m  [36/42], [94mLoss[0m : 9.82447
[1mStep[0m  [40/42], [94mLoss[0m : 9.82014

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 9.994, [92mTest[0m: 9.697, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.71858
[1mStep[0m  [4/42], [94mLoss[0m : 10.08266
[1mStep[0m  [8/42], [94mLoss[0m : 10.05798
[1mStep[0m  [12/42], [94mLoss[0m : 9.97619
[1mStep[0m  [16/42], [94mLoss[0m : 10.02743
[1mStep[0m  [20/42], [94mLoss[0m : 10.37395
[1mStep[0m  [24/42], [94mLoss[0m : 9.83709
[1mStep[0m  [28/42], [94mLoss[0m : 10.18342
[1mStep[0m  [32/42], [94mLoss[0m : 10.07089
[1mStep[0m  [36/42], [94mLoss[0m : 9.88682
[1mStep[0m  [40/42], [94mLoss[0m : 9.66328

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 9.943, [92mTest[0m: 9.636, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.60999
[1mStep[0m  [4/42], [94mLoss[0m : 9.97096
[1mStep[0m  [8/42], [94mLoss[0m : 9.96727
[1mStep[0m  [12/42], [94mLoss[0m : 10.01183
[1mStep[0m  [16/42], [94mLoss[0m : 10.18204
[1mStep[0m  [20/42], [94mLoss[0m : 9.65896
[1mStep[0m  [24/42], [94mLoss[0m : 9.70946
[1mStep[0m  [28/42], [94mLoss[0m : 9.72763
[1mStep[0m  [32/42], [94mLoss[0m : 9.86707
[1mStep[0m  [36/42], [94mLoss[0m : 10.00599
[1mStep[0m  [40/42], [94mLoss[0m : 9.68787

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 9.875, [92mTest[0m: 9.556, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.79485
[1mStep[0m  [4/42], [94mLoss[0m : 9.65482
[1mStep[0m  [8/42], [94mLoss[0m : 9.69363
[1mStep[0m  [12/42], [94mLoss[0m : 9.81800
[1mStep[0m  [16/42], [94mLoss[0m : 9.69180
[1mStep[0m  [20/42], [94mLoss[0m : 10.16217
[1mStep[0m  [24/42], [94mLoss[0m : 9.67599
[1mStep[0m  [28/42], [94mLoss[0m : 9.70352
[1mStep[0m  [32/42], [94mLoss[0m : 9.57436
[1mStep[0m  [36/42], [94mLoss[0m : 9.84164
[1mStep[0m  [40/42], [94mLoss[0m : 9.70315

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 9.821, [92mTest[0m: 9.470, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 10.02317
[1mStep[0m  [4/42], [94mLoss[0m : 9.83506
[1mStep[0m  [8/42], [94mLoss[0m : 9.59684
[1mStep[0m  [12/42], [94mLoss[0m : 9.74530
[1mStep[0m  [16/42], [94mLoss[0m : 10.00188
[1mStep[0m  [20/42], [94mLoss[0m : 9.80632
[1mStep[0m  [24/42], [94mLoss[0m : 10.11862
[1mStep[0m  [28/42], [94mLoss[0m : 10.10812
[1mStep[0m  [32/42], [94mLoss[0m : 9.62888
[1mStep[0m  [36/42], [94mLoss[0m : 9.27198
[1mStep[0m  [40/42], [94mLoss[0m : 9.71037

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 9.778, [92mTest[0m: 9.411, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.95240
[1mStep[0m  [4/42], [94mLoss[0m : 9.55739
[1mStep[0m  [8/42], [94mLoss[0m : 9.80826
[1mStep[0m  [12/42], [94mLoss[0m : 9.75056
[1mStep[0m  [16/42], [94mLoss[0m : 9.48568
[1mStep[0m  [20/42], [94mLoss[0m : 10.19678
[1mStep[0m  [24/42], [94mLoss[0m : 9.64641
[1mStep[0m  [28/42], [94mLoss[0m : 9.81776
[1mStep[0m  [32/42], [94mLoss[0m : 9.52470
[1mStep[0m  [36/42], [94mLoss[0m : 9.71612
[1mStep[0m  [40/42], [94mLoss[0m : 9.96224

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 9.702, [92mTest[0m: 9.325, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.35551
[1mStep[0m  [4/42], [94mLoss[0m : 9.96245
[1mStep[0m  [8/42], [94mLoss[0m : 9.97474
[1mStep[0m  [12/42], [94mLoss[0m : 9.72195
[1mStep[0m  [16/42], [94mLoss[0m : 9.75148
[1mStep[0m  [20/42], [94mLoss[0m : 9.63818
[1mStep[0m  [24/42], [94mLoss[0m : 9.81736
[1mStep[0m  [28/42], [94mLoss[0m : 9.76771
[1mStep[0m  [32/42], [94mLoss[0m : 10.25843
[1mStep[0m  [36/42], [94mLoss[0m : 9.43018
[1mStep[0m  [40/42], [94mLoss[0m : 9.29447

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 9.647, [92mTest[0m: 9.244, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.85051
[1mStep[0m  [4/42], [94mLoss[0m : 9.35899
[1mStep[0m  [8/42], [94mLoss[0m : 9.91288
[1mStep[0m  [12/42], [94mLoss[0m : 9.65203
[1mStep[0m  [16/42], [94mLoss[0m : 9.43149
[1mStep[0m  [20/42], [94mLoss[0m : 9.52459
[1mStep[0m  [24/42], [94mLoss[0m : 9.49599
[1mStep[0m  [28/42], [94mLoss[0m : 9.67025
[1mStep[0m  [32/42], [94mLoss[0m : 9.66123
[1mStep[0m  [36/42], [94mLoss[0m : 9.65437
[1mStep[0m  [40/42], [94mLoss[0m : 9.68571

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 9.595, [92mTest[0m: 9.159, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.62851
[1mStep[0m  [4/42], [94mLoss[0m : 9.99076
[1mStep[0m  [8/42], [94mLoss[0m : 9.06609
[1mStep[0m  [12/42], [94mLoss[0m : 9.56666
[1mStep[0m  [16/42], [94mLoss[0m : 9.90102
[1mStep[0m  [20/42], [94mLoss[0m : 9.68142
[1mStep[0m  [24/42], [94mLoss[0m : 9.74801
[1mStep[0m  [28/42], [94mLoss[0m : 9.47693
[1mStep[0m  [32/42], [94mLoss[0m : 9.56626
[1mStep[0m  [36/42], [94mLoss[0m : 9.64185
[1mStep[0m  [40/42], [94mLoss[0m : 9.88180

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 9.529, [92mTest[0m: 9.085, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.41284
[1mStep[0m  [4/42], [94mLoss[0m : 9.47586
[1mStep[0m  [8/42], [94mLoss[0m : 9.86201
[1mStep[0m  [12/42], [94mLoss[0m : 9.43528
[1mStep[0m  [16/42], [94mLoss[0m : 9.71239
[1mStep[0m  [20/42], [94mLoss[0m : 9.14078
[1mStep[0m  [24/42], [94mLoss[0m : 9.14248
[1mStep[0m  [28/42], [94mLoss[0m : 9.49627
[1mStep[0m  [32/42], [94mLoss[0m : 9.57947
[1mStep[0m  [36/42], [94mLoss[0m : 9.39008
[1mStep[0m  [40/42], [94mLoss[0m : 9.67705

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 9.471, [92mTest[0m: 9.017, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.57457
[1mStep[0m  [4/42], [94mLoss[0m : 9.28778
[1mStep[0m  [8/42], [94mLoss[0m : 9.43549
[1mStep[0m  [12/42], [94mLoss[0m : 9.00662
[1mStep[0m  [16/42], [94mLoss[0m : 9.40951
[1mStep[0m  [20/42], [94mLoss[0m : 9.48278
[1mStep[0m  [24/42], [94mLoss[0m : 9.35427
[1mStep[0m  [28/42], [94mLoss[0m : 9.25101
[1mStep[0m  [32/42], [94mLoss[0m : 9.45843
[1mStep[0m  [36/42], [94mLoss[0m : 9.28656
[1mStep[0m  [40/42], [94mLoss[0m : 9.12064

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 9.408, [92mTest[0m: 8.982, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.26312
[1mStep[0m  [4/42], [94mLoss[0m : 9.94652
[1mStep[0m  [8/42], [94mLoss[0m : 9.33127
[1mStep[0m  [12/42], [94mLoss[0m : 9.26462
[1mStep[0m  [16/42], [94mLoss[0m : 9.57892
[1mStep[0m  [20/42], [94mLoss[0m : 9.69978
[1mStep[0m  [24/42], [94mLoss[0m : 9.33690
[1mStep[0m  [28/42], [94mLoss[0m : 9.58604
[1mStep[0m  [32/42], [94mLoss[0m : 9.24479
[1mStep[0m  [36/42], [94mLoss[0m : 9.45873
[1mStep[0m  [40/42], [94mLoss[0m : 9.25276

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 9.343, [92mTest[0m: 8.855, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 8.747
====================================

Phase 1 - Evaluation MAE:  8.747121197836739
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.0001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/42], [94mLoss[0m : 9.34686
[1mStep[0m  [4/42], [94mLoss[0m : 9.43014
[1mStep[0m  [8/42], [94mLoss[0m : 9.49475
[1mStep[0m  [12/42], [94mLoss[0m : 9.54332
[1mStep[0m  [16/42], [94mLoss[0m : 8.95750
[1mStep[0m  [20/42], [94mLoss[0m : 9.35625
[1mStep[0m  [24/42], [94mLoss[0m : 9.27859
[1mStep[0m  [28/42], [94mLoss[0m : 8.76313
[1mStep[0m  [32/42], [94mLoss[0m : 9.03282
[1mStep[0m  [36/42], [94mLoss[0m : 9.16285
[1mStep[0m  [40/42], [94mLoss[0m : 9.41991

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.263, [92mTest[0m: 8.746, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.55120
[1mStep[0m  [4/42], [94mLoss[0m : 9.18867
[1mStep[0m  [8/42], [94mLoss[0m : 9.05587
[1mStep[0m  [12/42], [94mLoss[0m : 9.55872
[1mStep[0m  [16/42], [94mLoss[0m : 9.52846
[1mStep[0m  [20/42], [94mLoss[0m : 9.09216
[1mStep[0m  [24/42], [94mLoss[0m : 9.18594
[1mStep[0m  [28/42], [94mLoss[0m : 9.31232
[1mStep[0m  [32/42], [94mLoss[0m : 9.14905
[1mStep[0m  [36/42], [94mLoss[0m : 9.30637
[1mStep[0m  [40/42], [94mLoss[0m : 8.91364

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.174, [92mTest[0m: 8.604, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 9.15766
[1mStep[0m  [4/42], [94mLoss[0m : 8.92634
[1mStep[0m  [8/42], [94mLoss[0m : 9.82712
[1mStep[0m  [12/42], [94mLoss[0m : 8.92770
[1mStep[0m  [16/42], [94mLoss[0m : 8.78890
[1mStep[0m  [20/42], [94mLoss[0m : 9.39043
[1mStep[0m  [24/42], [94mLoss[0m : 9.17749
[1mStep[0m  [28/42], [94mLoss[0m : 9.32629
[1mStep[0m  [32/42], [94mLoss[0m : 9.05978
[1mStep[0m  [36/42], [94mLoss[0m : 9.20971
[1mStep[0m  [40/42], [94mLoss[0m : 9.16259

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.073, [92mTest[0m: 8.473, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.88145
[1mStep[0m  [4/42], [94mLoss[0m : 9.03607
[1mStep[0m  [8/42], [94mLoss[0m : 8.88208
[1mStep[0m  [12/42], [94mLoss[0m : 9.14253
[1mStep[0m  [16/42], [94mLoss[0m : 9.07359
[1mStep[0m  [20/42], [94mLoss[0m : 9.30279
[1mStep[0m  [24/42], [94mLoss[0m : 8.78474
[1mStep[0m  [28/42], [94mLoss[0m : 8.90617
[1mStep[0m  [32/42], [94mLoss[0m : 8.56961
[1mStep[0m  [36/42], [94mLoss[0m : 9.29826
[1mStep[0m  [40/42], [94mLoss[0m : 8.93206

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.968, [92mTest[0m: 8.346, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.89084
[1mStep[0m  [4/42], [94mLoss[0m : 8.56737
[1mStep[0m  [8/42], [94mLoss[0m : 8.87290
[1mStep[0m  [12/42], [94mLoss[0m : 9.18623
[1mStep[0m  [16/42], [94mLoss[0m : 8.45600
[1mStep[0m  [20/42], [94mLoss[0m : 8.86397
[1mStep[0m  [24/42], [94mLoss[0m : 9.25884
[1mStep[0m  [28/42], [94mLoss[0m : 8.48745
[1mStep[0m  [32/42], [94mLoss[0m : 8.90346
[1mStep[0m  [36/42], [94mLoss[0m : 8.64762
[1mStep[0m  [40/42], [94mLoss[0m : 8.80164

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.849, [92mTest[0m: 8.381, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.35992
[1mStep[0m  [4/42], [94mLoss[0m : 8.69725
[1mStep[0m  [8/42], [94mLoss[0m : 8.54680
[1mStep[0m  [12/42], [94mLoss[0m : 8.84134
[1mStep[0m  [16/42], [94mLoss[0m : 8.84512
[1mStep[0m  [20/42], [94mLoss[0m : 8.84212
[1mStep[0m  [24/42], [94mLoss[0m : 8.86860
[1mStep[0m  [28/42], [94mLoss[0m : 8.76008
[1mStep[0m  [32/42], [94mLoss[0m : 8.57408
[1mStep[0m  [36/42], [94mLoss[0m : 8.44937
[1mStep[0m  [40/42], [94mLoss[0m : 8.63033

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.745, [92mTest[0m: 8.368, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.35454
[1mStep[0m  [4/42], [94mLoss[0m : 8.92946
[1mStep[0m  [8/42], [94mLoss[0m : 8.48334
[1mStep[0m  [12/42], [94mLoss[0m : 8.72938
[1mStep[0m  [16/42], [94mLoss[0m : 8.81767
[1mStep[0m  [20/42], [94mLoss[0m : 8.02928
[1mStep[0m  [24/42], [94mLoss[0m : 8.87483
[1mStep[0m  [28/42], [94mLoss[0m : 8.70379
[1mStep[0m  [32/42], [94mLoss[0m : 8.97282
[1mStep[0m  [36/42], [94mLoss[0m : 8.82668
[1mStep[0m  [40/42], [94mLoss[0m : 8.71363

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.635, [92mTest[0m: 8.131, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.55783
[1mStep[0m  [4/42], [94mLoss[0m : 9.12696
[1mStep[0m  [8/42], [94mLoss[0m : 8.32946
[1mStep[0m  [12/42], [94mLoss[0m : 8.28886
[1mStep[0m  [16/42], [94mLoss[0m : 8.41101
[1mStep[0m  [20/42], [94mLoss[0m : 8.86484
[1mStep[0m  [24/42], [94mLoss[0m : 8.42999
[1mStep[0m  [28/42], [94mLoss[0m : 8.63319
[1mStep[0m  [32/42], [94mLoss[0m : 8.22575
[1mStep[0m  [36/42], [94mLoss[0m : 8.63400
[1mStep[0m  [40/42], [94mLoss[0m : 8.20187

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.522, [92mTest[0m: 7.978, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.39364
[1mStep[0m  [4/42], [94mLoss[0m : 8.43641
[1mStep[0m  [8/42], [94mLoss[0m : 8.54290
[1mStep[0m  [12/42], [94mLoss[0m : 8.44468
[1mStep[0m  [16/42], [94mLoss[0m : 8.14094
[1mStep[0m  [20/42], [94mLoss[0m : 8.52286
[1mStep[0m  [24/42], [94mLoss[0m : 8.70282
[1mStep[0m  [28/42], [94mLoss[0m : 8.34161
[1mStep[0m  [32/42], [94mLoss[0m : 7.99682
[1mStep[0m  [36/42], [94mLoss[0m : 7.92794
[1mStep[0m  [40/42], [94mLoss[0m : 8.26010

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 8.405, [92mTest[0m: 7.979, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.26842
[1mStep[0m  [4/42], [94mLoss[0m : 8.30189
[1mStep[0m  [8/42], [94mLoss[0m : 8.53279
[1mStep[0m  [12/42], [94mLoss[0m : 8.45933
[1mStep[0m  [16/42], [94mLoss[0m : 8.60494
[1mStep[0m  [20/42], [94mLoss[0m : 8.26515
[1mStep[0m  [24/42], [94mLoss[0m : 8.54571
[1mStep[0m  [28/42], [94mLoss[0m : 8.45939
[1mStep[0m  [32/42], [94mLoss[0m : 8.00519
[1mStep[0m  [36/42], [94mLoss[0m : 8.06261
[1mStep[0m  [40/42], [94mLoss[0m : 8.18270

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 8.281, [92mTest[0m: 7.740, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.52458
[1mStep[0m  [4/42], [94mLoss[0m : 8.23930
[1mStep[0m  [8/42], [94mLoss[0m : 8.34366
[1mStep[0m  [12/42], [94mLoss[0m : 8.34033
[1mStep[0m  [16/42], [94mLoss[0m : 8.13077
[1mStep[0m  [20/42], [94mLoss[0m : 8.42363
[1mStep[0m  [24/42], [94mLoss[0m : 8.22979
[1mStep[0m  [28/42], [94mLoss[0m : 8.20300
[1mStep[0m  [32/42], [94mLoss[0m : 8.15066
[1mStep[0m  [36/42], [94mLoss[0m : 8.39674
[1mStep[0m  [40/42], [94mLoss[0m : 8.01233

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 8.176, [92mTest[0m: 7.789, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 8.15734
[1mStep[0m  [4/42], [94mLoss[0m : 8.37120
[1mStep[0m  [8/42], [94mLoss[0m : 7.95714
[1mStep[0m  [12/42], [94mLoss[0m : 7.74623
[1mStep[0m  [16/42], [94mLoss[0m : 7.85398
[1mStep[0m  [20/42], [94mLoss[0m : 7.75509
[1mStep[0m  [24/42], [94mLoss[0m : 8.09480
[1mStep[0m  [28/42], [94mLoss[0m : 8.26144
[1mStep[0m  [32/42], [94mLoss[0m : 8.30151
[1mStep[0m  [36/42], [94mLoss[0m : 8.14755
[1mStep[0m  [40/42], [94mLoss[0m : 8.37565

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 8.035, [92mTest[0m: 7.450, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.55787
[1mStep[0m  [4/42], [94mLoss[0m : 8.15318
[1mStep[0m  [8/42], [94mLoss[0m : 8.34076
[1mStep[0m  [12/42], [94mLoss[0m : 7.73088
[1mStep[0m  [16/42], [94mLoss[0m : 7.80587
[1mStep[0m  [20/42], [94mLoss[0m : 7.80688
[1mStep[0m  [24/42], [94mLoss[0m : 7.83844
[1mStep[0m  [28/42], [94mLoss[0m : 7.91218
[1mStep[0m  [32/42], [94mLoss[0m : 7.70662
[1mStep[0m  [36/42], [94mLoss[0m : 7.56836
[1mStep[0m  [40/42], [94mLoss[0m : 7.66736

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 7.914, [92mTest[0m: 7.339, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.67680
[1mStep[0m  [4/42], [94mLoss[0m : 7.90397
[1mStep[0m  [8/42], [94mLoss[0m : 7.84460
[1mStep[0m  [12/42], [94mLoss[0m : 7.69184
[1mStep[0m  [16/42], [94mLoss[0m : 7.77935
[1mStep[0m  [20/42], [94mLoss[0m : 7.64726
[1mStep[0m  [24/42], [94mLoss[0m : 7.90057
[1mStep[0m  [28/42], [94mLoss[0m : 7.91363
[1mStep[0m  [32/42], [94mLoss[0m : 7.55299
[1mStep[0m  [36/42], [94mLoss[0m : 7.84865
[1mStep[0m  [40/42], [94mLoss[0m : 7.86360

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 7.797, [92mTest[0m: 7.424, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.34275
[1mStep[0m  [4/42], [94mLoss[0m : 7.74418
[1mStep[0m  [8/42], [94mLoss[0m : 7.53588
[1mStep[0m  [12/42], [94mLoss[0m : 7.36396
[1mStep[0m  [16/42], [94mLoss[0m : 8.05516
[1mStep[0m  [20/42], [94mLoss[0m : 7.71663
[1mStep[0m  [24/42], [94mLoss[0m : 8.29082
[1mStep[0m  [28/42], [94mLoss[0m : 7.65770
[1mStep[0m  [32/42], [94mLoss[0m : 7.51149
[1mStep[0m  [36/42], [94mLoss[0m : 7.72461
[1mStep[0m  [40/42], [94mLoss[0m : 7.69417

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 7.677, [92mTest[0m: 7.073, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.49297
[1mStep[0m  [4/42], [94mLoss[0m : 7.82215
[1mStep[0m  [8/42], [94mLoss[0m : 7.60904
[1mStep[0m  [12/42], [94mLoss[0m : 7.14493
[1mStep[0m  [16/42], [94mLoss[0m : 7.75635
[1mStep[0m  [20/42], [94mLoss[0m : 7.38243
[1mStep[0m  [24/42], [94mLoss[0m : 7.61354
[1mStep[0m  [28/42], [94mLoss[0m : 6.86774
[1mStep[0m  [32/42], [94mLoss[0m : 7.50703
[1mStep[0m  [36/42], [94mLoss[0m : 7.63721
[1mStep[0m  [40/42], [94mLoss[0m : 7.47417

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 7.578, [92mTest[0m: 7.517, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.49871
[1mStep[0m  [4/42], [94mLoss[0m : 7.43133
[1mStep[0m  [8/42], [94mLoss[0m : 7.31683
[1mStep[0m  [12/42], [94mLoss[0m : 7.37932
[1mStep[0m  [16/42], [94mLoss[0m : 7.21935
[1mStep[0m  [20/42], [94mLoss[0m : 7.32323
[1mStep[0m  [24/42], [94mLoss[0m : 7.47317
[1mStep[0m  [28/42], [94mLoss[0m : 7.18229
[1mStep[0m  [32/42], [94mLoss[0m : 7.31830
[1mStep[0m  [36/42], [94mLoss[0m : 7.68772
[1mStep[0m  [40/42], [94mLoss[0m : 6.83653

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 7.453, [92mTest[0m: 6.848, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.53426
[1mStep[0m  [4/42], [94mLoss[0m : 7.13209
[1mStep[0m  [8/42], [94mLoss[0m : 7.29067
[1mStep[0m  [12/42], [94mLoss[0m : 7.61071
[1mStep[0m  [16/42], [94mLoss[0m : 7.51885
[1mStep[0m  [20/42], [94mLoss[0m : 7.27376
[1mStep[0m  [24/42], [94mLoss[0m : 7.38343
[1mStep[0m  [28/42], [94mLoss[0m : 7.62254
[1mStep[0m  [32/42], [94mLoss[0m : 7.40304
[1mStep[0m  [36/42], [94mLoss[0m : 7.21106
[1mStep[0m  [40/42], [94mLoss[0m : 7.23574

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 7.352, [92mTest[0m: 6.469, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.36280
[1mStep[0m  [4/42], [94mLoss[0m : 6.99723
[1mStep[0m  [8/42], [94mLoss[0m : 7.08352
[1mStep[0m  [12/42], [94mLoss[0m : 7.50643
[1mStep[0m  [16/42], [94mLoss[0m : 6.89936
[1mStep[0m  [20/42], [94mLoss[0m : 7.28514
[1mStep[0m  [24/42], [94mLoss[0m : 7.21120
[1mStep[0m  [28/42], [94mLoss[0m : 7.29881
[1mStep[0m  [32/42], [94mLoss[0m : 7.10685
[1mStep[0m  [36/42], [94mLoss[0m : 7.13177
[1mStep[0m  [40/42], [94mLoss[0m : 7.23326

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 7.269, [92mTest[0m: 6.396, [96mlr[0m: 0.0001
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.16557
[1mStep[0m  [4/42], [94mLoss[0m : 7.25365
[1mStep[0m  [8/42], [94mLoss[0m : 7.26157
[1mStep[0m  [12/42], [94mLoss[0m : 7.00311
[1mStep[0m  [16/42], [94mLoss[0m : 7.48467
[1mStep[0m  [20/42], [94mLoss[0m : 7.08689
[1mStep[0m  [24/42], [94mLoss[0m : 6.77521
[1mStep[0m  [28/42], [94mLoss[0m : 7.38852
[1mStep[0m  [32/42], [94mLoss[0m : 6.96614
[1mStep[0m  [36/42], [94mLoss[0m : 7.26902
[1mStep[0m  [40/42], [94mLoss[0m : 7.07895

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 7.165, [92mTest[0m: 6.241, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.29763
[1mStep[0m  [4/42], [94mLoss[0m : 6.98321
[1mStep[0m  [8/42], [94mLoss[0m : 7.29038
[1mStep[0m  [12/42], [94mLoss[0m : 6.90121
[1mStep[0m  [16/42], [94mLoss[0m : 6.97793
[1mStep[0m  [20/42], [94mLoss[0m : 7.11199
[1mStep[0m  [24/42], [94mLoss[0m : 7.00043
[1mStep[0m  [28/42], [94mLoss[0m : 6.96771
[1mStep[0m  [32/42], [94mLoss[0m : 7.09681
[1mStep[0m  [36/42], [94mLoss[0m : 6.81564
[1mStep[0m  [40/42], [94mLoss[0m : 6.94393

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 7.071, [92mTest[0m: 6.557, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.29634
[1mStep[0m  [4/42], [94mLoss[0m : 6.84646
[1mStep[0m  [8/42], [94mLoss[0m : 6.46622
[1mStep[0m  [12/42], [94mLoss[0m : 7.04792
[1mStep[0m  [16/42], [94mLoss[0m : 6.90384
[1mStep[0m  [20/42], [94mLoss[0m : 6.88012
[1mStep[0m  [24/42], [94mLoss[0m : 7.31848
[1mStep[0m  [28/42], [94mLoss[0m : 7.34520
[1mStep[0m  [32/42], [94mLoss[0m : 7.18426
[1mStep[0m  [36/42], [94mLoss[0m : 6.58944
[1mStep[0m  [40/42], [94mLoss[0m : 7.02376

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 6.966, [92mTest[0m: 6.921, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.12164
[1mStep[0m  [4/42], [94mLoss[0m : 6.90192
[1mStep[0m  [8/42], [94mLoss[0m : 6.85663
[1mStep[0m  [12/42], [94mLoss[0m : 6.53474
[1mStep[0m  [16/42], [94mLoss[0m : 7.10791
[1mStep[0m  [20/42], [94mLoss[0m : 6.78905
[1mStep[0m  [24/42], [94mLoss[0m : 6.88904
[1mStep[0m  [28/42], [94mLoss[0m : 6.94283
[1mStep[0m  [32/42], [94mLoss[0m : 6.85355
[1mStep[0m  [36/42], [94mLoss[0m : 7.00694
[1mStep[0m  [40/42], [94mLoss[0m : 7.11279

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 6.883, [92mTest[0m: 6.325, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.67151
[1mStep[0m  [4/42], [94mLoss[0m : 6.72962
[1mStep[0m  [8/42], [94mLoss[0m : 6.47088
[1mStep[0m  [12/42], [94mLoss[0m : 6.54220
[1mStep[0m  [16/42], [94mLoss[0m : 6.57404
[1mStep[0m  [20/42], [94mLoss[0m : 7.16181
[1mStep[0m  [24/42], [94mLoss[0m : 6.69182
[1mStep[0m  [28/42], [94mLoss[0m : 6.78207
[1mStep[0m  [32/42], [94mLoss[0m : 6.91190
[1mStep[0m  [36/42], [94mLoss[0m : 7.00977
[1mStep[0m  [40/42], [94mLoss[0m : 6.87007

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 6.778, [92mTest[0m: 6.166, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.40030
[1mStep[0m  [4/42], [94mLoss[0m : 7.00316
[1mStep[0m  [8/42], [94mLoss[0m : 7.12085
[1mStep[0m  [12/42], [94mLoss[0m : 6.98486
[1mStep[0m  [16/42], [94mLoss[0m : 6.97083
[1mStep[0m  [20/42], [94mLoss[0m : 6.89685
[1mStep[0m  [24/42], [94mLoss[0m : 6.17058
[1mStep[0m  [28/42], [94mLoss[0m : 6.37312
[1mStep[0m  [32/42], [94mLoss[0m : 6.54506
[1mStep[0m  [36/42], [94mLoss[0m : 6.72979
[1mStep[0m  [40/42], [94mLoss[0m : 6.76978

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 6.696, [92mTest[0m: 7.494, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 7.11789
[1mStep[0m  [4/42], [94mLoss[0m : 6.81648
[1mStep[0m  [8/42], [94mLoss[0m : 6.74950
[1mStep[0m  [12/42], [94mLoss[0m : 6.79920
[1mStep[0m  [16/42], [94mLoss[0m : 6.69581
[1mStep[0m  [20/42], [94mLoss[0m : 6.45171
[1mStep[0m  [24/42], [94mLoss[0m : 6.50357
[1mStep[0m  [28/42], [94mLoss[0m : 6.55952
[1mStep[0m  [32/42], [94mLoss[0m : 6.83134
[1mStep[0m  [36/42], [94mLoss[0m : 6.50739
[1mStep[0m  [40/42], [94mLoss[0m : 6.60365

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 6.603, [92mTest[0m: 6.451, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.46929
[1mStep[0m  [4/42], [94mLoss[0m : 6.33968
[1mStep[0m  [8/42], [94mLoss[0m : 6.47976
[1mStep[0m  [12/42], [94mLoss[0m : 6.55162
[1mStep[0m  [16/42], [94mLoss[0m : 6.52119
[1mStep[0m  [20/42], [94mLoss[0m : 6.37910
[1mStep[0m  [24/42], [94mLoss[0m : 6.43732
[1mStep[0m  [28/42], [94mLoss[0m : 6.24022
[1mStep[0m  [32/42], [94mLoss[0m : 6.59394
[1mStep[0m  [36/42], [94mLoss[0m : 6.56285
[1mStep[0m  [40/42], [94mLoss[0m : 6.38751

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 6.482, [92mTest[0m: 5.840, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.59699
[1mStep[0m  [4/42], [94mLoss[0m : 6.27454
[1mStep[0m  [8/42], [94mLoss[0m : 6.83622
[1mStep[0m  [12/42], [94mLoss[0m : 6.65590
[1mStep[0m  [16/42], [94mLoss[0m : 6.44557
[1mStep[0m  [20/42], [94mLoss[0m : 6.41032
[1mStep[0m  [24/42], [94mLoss[0m : 6.25138
[1mStep[0m  [28/42], [94mLoss[0m : 6.66121
[1mStep[0m  [32/42], [94mLoss[0m : 6.15039
[1mStep[0m  [36/42], [94mLoss[0m : 6.06470
[1mStep[0m  [40/42], [94mLoss[0m : 6.11288

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 6.406, [92mTest[0m: 5.630, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.32697
[1mStep[0m  [4/42], [94mLoss[0m : 6.10999
[1mStep[0m  [8/42], [94mLoss[0m : 6.57914
[1mStep[0m  [12/42], [94mLoss[0m : 6.40386
[1mStep[0m  [16/42], [94mLoss[0m : 6.43065
[1mStep[0m  [20/42], [94mLoss[0m : 6.19785
[1mStep[0m  [24/42], [94mLoss[0m : 6.35803
[1mStep[0m  [28/42], [94mLoss[0m : 6.25723
[1mStep[0m  [32/42], [94mLoss[0m : 6.59407
[1mStep[0m  [36/42], [94mLoss[0m : 6.12442
[1mStep[0m  [40/42], [94mLoss[0m : 6.14079

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 6.290, [92mTest[0m: 5.521, [96mlr[0m: 9e-05
====================================

[1mStep[0m  [0/42], [94mLoss[0m : 6.38024
[1mStep[0m  [4/42], [94mLoss[0m : 6.06425
[1mStep[0m  [8/42], [94mLoss[0m : 5.76414
[1mStep[0m  [12/42], [94mLoss[0m : 5.99029
[1mStep[0m  [16/42], [94mLoss[0m : 6.12021
[1mStep[0m  [20/42], [94mLoss[0m : 6.22344
[1mStep[0m  [24/42], [94mLoss[0m : 6.04900
[1mStep[0m  [28/42], [94mLoss[0m : 5.95549
[1mStep[0m  [32/42], [94mLoss[0m : 6.59523
[1mStep[0m  [36/42], [94mLoss[0m : 6.36313
[1mStep[0m  [40/42], [94mLoss[0m : 6.57732

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 6.167, [92mTest[0m: 5.516, [96mlr[0m: 9e-05
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 5.244
====================================

Phase 2 - Evaluation MAE:  5.244368757520403
MAE score P1        8.747121
MAE score P2        5.244369
loss                6.167329
learning_rate         0.0001
batch_size               256
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 28, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'relu', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [100], 'learning_rate': 0.0001, 'batch_size': 512, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.0001, 'batch_size': 256, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
