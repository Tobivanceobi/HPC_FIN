no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  4
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 11.04670
[1mStep[0m  [16/169], [94mLoss[0m : 9.15378
[1mStep[0m  [32/169], [94mLoss[0m : 10.04509
[1mStep[0m  [48/169], [94mLoss[0m : 8.17553
[1mStep[0m  [64/169], [94mLoss[0m : 8.19710
[1mStep[0m  [80/169], [94mLoss[0m : 7.53300
[1mStep[0m  [96/169], [94mLoss[0m : 6.88950
[1mStep[0m  [112/169], [94mLoss[0m : 6.03073
[1mStep[0m  [128/169], [94mLoss[0m : 5.70896
[1mStep[0m  [144/169], [94mLoss[0m : 5.40109
[1mStep[0m  [160/169], [94mLoss[0m : 4.54431

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.542, [92mTest[0m: 10.785, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.91527
[1mStep[0m  [16/169], [94mLoss[0m : 4.70722
[1mStep[0m  [32/169], [94mLoss[0m : 3.62566
[1mStep[0m  [48/169], [94mLoss[0m : 3.56161
[1mStep[0m  [64/169], [94mLoss[0m : 3.82157
[1mStep[0m  [80/169], [94mLoss[0m : 3.20633
[1mStep[0m  [96/169], [94mLoss[0m : 3.91691
[1mStep[0m  [112/169], [94mLoss[0m : 3.33331
[1mStep[0m  [128/169], [94mLoss[0m : 3.43219
[1mStep[0m  [144/169], [94mLoss[0m : 2.82296
[1mStep[0m  [160/169], [94mLoss[0m : 3.06476

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.436, [92mTest[0m: 4.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.11578
[1mStep[0m  [16/169], [94mLoss[0m : 2.43621
[1mStep[0m  [32/169], [94mLoss[0m : 2.65144
[1mStep[0m  [48/169], [94mLoss[0m : 3.16024
[1mStep[0m  [64/169], [94mLoss[0m : 3.60457
[1mStep[0m  [80/169], [94mLoss[0m : 2.68727
[1mStep[0m  [96/169], [94mLoss[0m : 2.85670
[1mStep[0m  [112/169], [94mLoss[0m : 2.70848
[1mStep[0m  [128/169], [94mLoss[0m : 2.51252
[1mStep[0m  [144/169], [94mLoss[0m : 3.31199
[1mStep[0m  [160/169], [94mLoss[0m : 2.73253

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.792, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96389
[1mStep[0m  [16/169], [94mLoss[0m : 2.73342
[1mStep[0m  [32/169], [94mLoss[0m : 2.28198
[1mStep[0m  [48/169], [94mLoss[0m : 2.45646
[1mStep[0m  [64/169], [94mLoss[0m : 2.25713
[1mStep[0m  [80/169], [94mLoss[0m : 2.32775
[1mStep[0m  [96/169], [94mLoss[0m : 2.40082
[1mStep[0m  [112/169], [94mLoss[0m : 2.89325
[1mStep[0m  [128/169], [94mLoss[0m : 2.55110
[1mStep[0m  [144/169], [94mLoss[0m : 2.44861
[1mStep[0m  [160/169], [94mLoss[0m : 2.52085

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.705, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58246
[1mStep[0m  [16/169], [94mLoss[0m : 2.75332
[1mStep[0m  [32/169], [94mLoss[0m : 2.51749
[1mStep[0m  [48/169], [94mLoss[0m : 2.70751
[1mStep[0m  [64/169], [94mLoss[0m : 2.96574
[1mStep[0m  [80/169], [94mLoss[0m : 2.75547
[1mStep[0m  [96/169], [94mLoss[0m : 2.44404
[1mStep[0m  [112/169], [94mLoss[0m : 2.49318
[1mStep[0m  [128/169], [94mLoss[0m : 2.77828
[1mStep[0m  [144/169], [94mLoss[0m : 3.54909
[1mStep[0m  [160/169], [94mLoss[0m : 3.18327

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85359
[1mStep[0m  [16/169], [94mLoss[0m : 2.54586
[1mStep[0m  [32/169], [94mLoss[0m : 2.91642
[1mStep[0m  [48/169], [94mLoss[0m : 2.78887
[1mStep[0m  [64/169], [94mLoss[0m : 3.08415
[1mStep[0m  [80/169], [94mLoss[0m : 2.68321
[1mStep[0m  [96/169], [94mLoss[0m : 2.99330
[1mStep[0m  [112/169], [94mLoss[0m : 2.31027
[1mStep[0m  [128/169], [94mLoss[0m : 2.14661
[1mStep[0m  [144/169], [94mLoss[0m : 2.43519
[1mStep[0m  [160/169], [94mLoss[0m : 2.77259

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.682, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67139
[1mStep[0m  [16/169], [94mLoss[0m : 2.88202
[1mStep[0m  [32/169], [94mLoss[0m : 3.30860
[1mStep[0m  [48/169], [94mLoss[0m : 2.72951
[1mStep[0m  [64/169], [94mLoss[0m : 2.61440
[1mStep[0m  [80/169], [94mLoss[0m : 2.31876
[1mStep[0m  [96/169], [94mLoss[0m : 2.51336
[1mStep[0m  [112/169], [94mLoss[0m : 2.26619
[1mStep[0m  [128/169], [94mLoss[0m : 2.72475
[1mStep[0m  [144/169], [94mLoss[0m : 3.41914
[1mStep[0m  [160/169], [94mLoss[0m : 2.44085

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69794
[1mStep[0m  [16/169], [94mLoss[0m : 3.04475
[1mStep[0m  [32/169], [94mLoss[0m : 2.37409
[1mStep[0m  [48/169], [94mLoss[0m : 3.00037
[1mStep[0m  [64/169], [94mLoss[0m : 2.06929
[1mStep[0m  [80/169], [94mLoss[0m : 2.84966
[1mStep[0m  [96/169], [94mLoss[0m : 2.63991
[1mStep[0m  [112/169], [94mLoss[0m : 2.84678
[1mStep[0m  [128/169], [94mLoss[0m : 2.41522
[1mStep[0m  [144/169], [94mLoss[0m : 2.62397
[1mStep[0m  [160/169], [94mLoss[0m : 2.67604

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50879
[1mStep[0m  [16/169], [94mLoss[0m : 2.65228
[1mStep[0m  [32/169], [94mLoss[0m : 2.56805
[1mStep[0m  [48/169], [94mLoss[0m : 2.79260
[1mStep[0m  [64/169], [94mLoss[0m : 2.93912
[1mStep[0m  [80/169], [94mLoss[0m : 3.05292
[1mStep[0m  [96/169], [94mLoss[0m : 2.35335
[1mStep[0m  [112/169], [94mLoss[0m : 2.34640
[1mStep[0m  [128/169], [94mLoss[0m : 2.83949
[1mStep[0m  [144/169], [94mLoss[0m : 2.70460
[1mStep[0m  [160/169], [94mLoss[0m : 2.94408

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62256
[1mStep[0m  [16/169], [94mLoss[0m : 2.61181
[1mStep[0m  [32/169], [94mLoss[0m : 2.64764
[1mStep[0m  [48/169], [94mLoss[0m : 2.83940
[1mStep[0m  [64/169], [94mLoss[0m : 2.18453
[1mStep[0m  [80/169], [94mLoss[0m : 2.43092
[1mStep[0m  [96/169], [94mLoss[0m : 2.71709
[1mStep[0m  [112/169], [94mLoss[0m : 2.82644
[1mStep[0m  [128/169], [94mLoss[0m : 2.70706
[1mStep[0m  [144/169], [94mLoss[0m : 3.28882
[1mStep[0m  [160/169], [94mLoss[0m : 2.74273

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83199
[1mStep[0m  [16/169], [94mLoss[0m : 2.75192
[1mStep[0m  [32/169], [94mLoss[0m : 2.78122
[1mStep[0m  [48/169], [94mLoss[0m : 2.62573
[1mStep[0m  [64/169], [94mLoss[0m : 2.26467
[1mStep[0m  [80/169], [94mLoss[0m : 2.67915
[1mStep[0m  [96/169], [94mLoss[0m : 2.54533
[1mStep[0m  [112/169], [94mLoss[0m : 2.75123
[1mStep[0m  [128/169], [94mLoss[0m : 2.29342
[1mStep[0m  [144/169], [94mLoss[0m : 2.57923
[1mStep[0m  [160/169], [94mLoss[0m : 2.80509

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43376
[1mStep[0m  [16/169], [94mLoss[0m : 2.43552
[1mStep[0m  [32/169], [94mLoss[0m : 2.84335
[1mStep[0m  [48/169], [94mLoss[0m : 2.45755
[1mStep[0m  [64/169], [94mLoss[0m : 2.25227
[1mStep[0m  [80/169], [94mLoss[0m : 2.27585
[1mStep[0m  [96/169], [94mLoss[0m : 2.19815
[1mStep[0m  [112/169], [94mLoss[0m : 2.78690
[1mStep[0m  [128/169], [94mLoss[0m : 2.77091
[1mStep[0m  [144/169], [94mLoss[0m : 2.75655
[1mStep[0m  [160/169], [94mLoss[0m : 2.67932

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85311
[1mStep[0m  [16/169], [94mLoss[0m : 2.82949
[1mStep[0m  [32/169], [94mLoss[0m : 2.43413
[1mStep[0m  [48/169], [94mLoss[0m : 2.58942
[1mStep[0m  [64/169], [94mLoss[0m : 2.89129
[1mStep[0m  [80/169], [94mLoss[0m : 2.88259
[1mStep[0m  [96/169], [94mLoss[0m : 2.52573
[1mStep[0m  [112/169], [94mLoss[0m : 2.45484
[1mStep[0m  [128/169], [94mLoss[0m : 2.24632
[1mStep[0m  [144/169], [94mLoss[0m : 2.87271
[1mStep[0m  [160/169], [94mLoss[0m : 2.69791

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41074
[1mStep[0m  [16/169], [94mLoss[0m : 2.94115
[1mStep[0m  [32/169], [94mLoss[0m : 2.34642
[1mStep[0m  [48/169], [94mLoss[0m : 2.20002
[1mStep[0m  [64/169], [94mLoss[0m : 2.70535
[1mStep[0m  [80/169], [94mLoss[0m : 2.92201
[1mStep[0m  [96/169], [94mLoss[0m : 2.35128
[1mStep[0m  [112/169], [94mLoss[0m : 2.68653
[1mStep[0m  [128/169], [94mLoss[0m : 2.83761
[1mStep[0m  [144/169], [94mLoss[0m : 2.26910
[1mStep[0m  [160/169], [94mLoss[0m : 3.04153

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60187
[1mStep[0m  [16/169], [94mLoss[0m : 2.61980
[1mStep[0m  [32/169], [94mLoss[0m : 2.50343
[1mStep[0m  [48/169], [94mLoss[0m : 2.59549
[1mStep[0m  [64/169], [94mLoss[0m : 2.29959
[1mStep[0m  [80/169], [94mLoss[0m : 2.96986
[1mStep[0m  [96/169], [94mLoss[0m : 2.54406
[1mStep[0m  [112/169], [94mLoss[0m : 3.00400
[1mStep[0m  [128/169], [94mLoss[0m : 2.89243
[1mStep[0m  [144/169], [94mLoss[0m : 2.35862
[1mStep[0m  [160/169], [94mLoss[0m : 2.68453

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84053
[1mStep[0m  [16/169], [94mLoss[0m : 2.45969
[1mStep[0m  [32/169], [94mLoss[0m : 2.97359
[1mStep[0m  [48/169], [94mLoss[0m : 3.04332
[1mStep[0m  [64/169], [94mLoss[0m : 2.78988
[1mStep[0m  [80/169], [94mLoss[0m : 2.75061
[1mStep[0m  [96/169], [94mLoss[0m : 2.79491
[1mStep[0m  [112/169], [94mLoss[0m : 2.67131
[1mStep[0m  [128/169], [94mLoss[0m : 2.69560
[1mStep[0m  [144/169], [94mLoss[0m : 2.34465
[1mStep[0m  [160/169], [94mLoss[0m : 2.54646

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.644, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82117
[1mStep[0m  [16/169], [94mLoss[0m : 2.29824
[1mStep[0m  [32/169], [94mLoss[0m : 2.24318
[1mStep[0m  [48/169], [94mLoss[0m : 2.76728
[1mStep[0m  [64/169], [94mLoss[0m : 2.77828
[1mStep[0m  [80/169], [94mLoss[0m : 2.75455
[1mStep[0m  [96/169], [94mLoss[0m : 2.12768
[1mStep[0m  [112/169], [94mLoss[0m : 2.91242
[1mStep[0m  [128/169], [94mLoss[0m : 2.80864
[1mStep[0m  [144/169], [94mLoss[0m : 2.47262
[1mStep[0m  [160/169], [94mLoss[0m : 2.64169

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.634, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.37310
[1mStep[0m  [16/169], [94mLoss[0m : 2.74562
[1mStep[0m  [32/169], [94mLoss[0m : 2.62797
[1mStep[0m  [48/169], [94mLoss[0m : 2.30151
[1mStep[0m  [64/169], [94mLoss[0m : 2.93374
[1mStep[0m  [80/169], [94mLoss[0m : 2.36536
[1mStep[0m  [96/169], [94mLoss[0m : 2.45961
[1mStep[0m  [112/169], [94mLoss[0m : 2.97962
[1mStep[0m  [128/169], [94mLoss[0m : 3.00696
[1mStep[0m  [144/169], [94mLoss[0m : 2.81625
[1mStep[0m  [160/169], [94mLoss[0m : 2.55046

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17576
[1mStep[0m  [16/169], [94mLoss[0m : 2.50172
[1mStep[0m  [32/169], [94mLoss[0m : 2.28694
[1mStep[0m  [48/169], [94mLoss[0m : 2.75104
[1mStep[0m  [64/169], [94mLoss[0m : 2.86132
[1mStep[0m  [80/169], [94mLoss[0m : 2.33595
[1mStep[0m  [96/169], [94mLoss[0m : 2.85772
[1mStep[0m  [112/169], [94mLoss[0m : 2.29456
[1mStep[0m  [128/169], [94mLoss[0m : 2.67520
[1mStep[0m  [144/169], [94mLoss[0m : 2.22777
[1mStep[0m  [160/169], [94mLoss[0m : 2.63017

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55928
[1mStep[0m  [16/169], [94mLoss[0m : 2.43189
[1mStep[0m  [32/169], [94mLoss[0m : 2.44057
[1mStep[0m  [48/169], [94mLoss[0m : 2.44870
[1mStep[0m  [64/169], [94mLoss[0m : 2.39296
[1mStep[0m  [80/169], [94mLoss[0m : 2.82124
[1mStep[0m  [96/169], [94mLoss[0m : 2.52701
[1mStep[0m  [112/169], [94mLoss[0m : 2.94305
[1mStep[0m  [128/169], [94mLoss[0m : 2.57253
[1mStep[0m  [144/169], [94mLoss[0m : 2.56549
[1mStep[0m  [160/169], [94mLoss[0m : 2.53410

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45652
[1mStep[0m  [16/169], [94mLoss[0m : 2.35043
[1mStep[0m  [32/169], [94mLoss[0m : 2.83947
[1mStep[0m  [48/169], [94mLoss[0m : 2.38807
[1mStep[0m  [64/169], [94mLoss[0m : 2.20809
[1mStep[0m  [80/169], [94mLoss[0m : 2.69320
[1mStep[0m  [96/169], [94mLoss[0m : 2.39448
[1mStep[0m  [112/169], [94mLoss[0m : 3.43272
[1mStep[0m  [128/169], [94mLoss[0m : 2.94459
[1mStep[0m  [144/169], [94mLoss[0m : 2.60913
[1mStep[0m  [160/169], [94mLoss[0m : 2.74358

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.359, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66799
[1mStep[0m  [16/169], [94mLoss[0m : 2.42389
[1mStep[0m  [32/169], [94mLoss[0m : 2.50769
[1mStep[0m  [48/169], [94mLoss[0m : 2.69802
[1mStep[0m  [64/169], [94mLoss[0m : 2.68197
[1mStep[0m  [80/169], [94mLoss[0m : 2.77706
[1mStep[0m  [96/169], [94mLoss[0m : 2.63813
[1mStep[0m  [112/169], [94mLoss[0m : 2.34908
[1mStep[0m  [128/169], [94mLoss[0m : 2.43392
[1mStep[0m  [144/169], [94mLoss[0m : 2.75643
[1mStep[0m  [160/169], [94mLoss[0m : 2.53408

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.353, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24943
[1mStep[0m  [16/169], [94mLoss[0m : 2.10451
[1mStep[0m  [32/169], [94mLoss[0m : 2.30860
[1mStep[0m  [48/169], [94mLoss[0m : 2.94177
[1mStep[0m  [64/169], [94mLoss[0m : 2.41559
[1mStep[0m  [80/169], [94mLoss[0m : 2.76105
[1mStep[0m  [96/169], [94mLoss[0m : 2.18006
[1mStep[0m  [112/169], [94mLoss[0m : 2.55485
[1mStep[0m  [128/169], [94mLoss[0m : 2.47791
[1mStep[0m  [144/169], [94mLoss[0m : 2.83283
[1mStep[0m  [160/169], [94mLoss[0m : 2.13903

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46477
[1mStep[0m  [16/169], [94mLoss[0m : 2.35072
[1mStep[0m  [32/169], [94mLoss[0m : 2.79274
[1mStep[0m  [48/169], [94mLoss[0m : 2.53265
[1mStep[0m  [64/169], [94mLoss[0m : 2.90597
[1mStep[0m  [80/169], [94mLoss[0m : 2.93011
[1mStep[0m  [96/169], [94mLoss[0m : 2.59302
[1mStep[0m  [112/169], [94mLoss[0m : 2.36981
[1mStep[0m  [128/169], [94mLoss[0m : 2.37997
[1mStep[0m  [144/169], [94mLoss[0m : 2.71817
[1mStep[0m  [160/169], [94mLoss[0m : 2.80520

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.601, [92mTest[0m: 2.355, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78712
[1mStep[0m  [16/169], [94mLoss[0m : 2.85840
[1mStep[0m  [32/169], [94mLoss[0m : 2.73879
[1mStep[0m  [48/169], [94mLoss[0m : 2.68899
[1mStep[0m  [64/169], [94mLoss[0m : 2.68404
[1mStep[0m  [80/169], [94mLoss[0m : 2.19839
[1mStep[0m  [96/169], [94mLoss[0m : 2.98458
[1mStep[0m  [112/169], [94mLoss[0m : 2.90008
[1mStep[0m  [128/169], [94mLoss[0m : 2.47989
[1mStep[0m  [144/169], [94mLoss[0m : 2.52429
[1mStep[0m  [160/169], [94mLoss[0m : 2.27827

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.612, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.97177
[1mStep[0m  [16/169], [94mLoss[0m : 3.00421
[1mStep[0m  [32/169], [94mLoss[0m : 2.15943
[1mStep[0m  [48/169], [94mLoss[0m : 2.65960
[1mStep[0m  [64/169], [94mLoss[0m : 2.65424
[1mStep[0m  [80/169], [94mLoss[0m : 2.84006
[1mStep[0m  [96/169], [94mLoss[0m : 2.67350
[1mStep[0m  [112/169], [94mLoss[0m : 2.71943
[1mStep[0m  [128/169], [94mLoss[0m : 3.01811
[1mStep[0m  [144/169], [94mLoss[0m : 2.99509
[1mStep[0m  [160/169], [94mLoss[0m : 2.34241

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56996
[1mStep[0m  [16/169], [94mLoss[0m : 2.36740
[1mStep[0m  [32/169], [94mLoss[0m : 2.80284
[1mStep[0m  [48/169], [94mLoss[0m : 2.87726
[1mStep[0m  [64/169], [94mLoss[0m : 1.99345
[1mStep[0m  [80/169], [94mLoss[0m : 2.29048
[1mStep[0m  [96/169], [94mLoss[0m : 3.15866
[1mStep[0m  [112/169], [94mLoss[0m : 3.00745
[1mStep[0m  [128/169], [94mLoss[0m : 2.28648
[1mStep[0m  [144/169], [94mLoss[0m : 2.36653
[1mStep[0m  [160/169], [94mLoss[0m : 2.87868

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.88456
[1mStep[0m  [16/169], [94mLoss[0m : 2.66496
[1mStep[0m  [32/169], [94mLoss[0m : 2.30997
[1mStep[0m  [48/169], [94mLoss[0m : 2.70815
[1mStep[0m  [64/169], [94mLoss[0m : 2.83979
[1mStep[0m  [80/169], [94mLoss[0m : 2.52926
[1mStep[0m  [96/169], [94mLoss[0m : 3.16615
[1mStep[0m  [112/169], [94mLoss[0m : 2.61097
[1mStep[0m  [128/169], [94mLoss[0m : 2.48610
[1mStep[0m  [144/169], [94mLoss[0m : 2.23554
[1mStep[0m  [160/169], [94mLoss[0m : 2.42329

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10067
[1mStep[0m  [16/169], [94mLoss[0m : 2.13867
[1mStep[0m  [32/169], [94mLoss[0m : 2.50544
[1mStep[0m  [48/169], [94mLoss[0m : 2.73572
[1mStep[0m  [64/169], [94mLoss[0m : 2.82580
[1mStep[0m  [80/169], [94mLoss[0m : 3.07474
[1mStep[0m  [96/169], [94mLoss[0m : 2.66724
[1mStep[0m  [112/169], [94mLoss[0m : 2.28158
[1mStep[0m  [128/169], [94mLoss[0m : 2.62174
[1mStep[0m  [144/169], [94mLoss[0m : 2.61965
[1mStep[0m  [160/169], [94mLoss[0m : 2.66232

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61402
[1mStep[0m  [16/169], [94mLoss[0m : 2.73029
[1mStep[0m  [32/169], [94mLoss[0m : 3.06866
[1mStep[0m  [48/169], [94mLoss[0m : 2.52844
[1mStep[0m  [64/169], [94mLoss[0m : 2.39845
[1mStep[0m  [80/169], [94mLoss[0m : 2.70138
[1mStep[0m  [96/169], [94mLoss[0m : 2.75987
[1mStep[0m  [112/169], [94mLoss[0m : 2.76610
[1mStep[0m  [128/169], [94mLoss[0m : 2.84395
[1mStep[0m  [144/169], [94mLoss[0m : 2.49900
[1mStep[0m  [160/169], [94mLoss[0m : 2.16477

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.343
====================================

Phase 1 - Evaluation MAE:  2.3426643077816283
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.43852
[1mStep[0m  [16/169], [94mLoss[0m : 3.08706
[1mStep[0m  [32/169], [94mLoss[0m : 2.61058
[1mStep[0m  [48/169], [94mLoss[0m : 2.36695
[1mStep[0m  [64/169], [94mLoss[0m : 3.34147
[1mStep[0m  [80/169], [94mLoss[0m : 2.38882
[1mStep[0m  [96/169], [94mLoss[0m : 2.52105
[1mStep[0m  [112/169], [94mLoss[0m : 2.24615
[1mStep[0m  [128/169], [94mLoss[0m : 2.30075
[1mStep[0m  [144/169], [94mLoss[0m : 2.10158
[1mStep[0m  [160/169], [94mLoss[0m : 2.88419

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62256
[1mStep[0m  [16/169], [94mLoss[0m : 2.57653
[1mStep[0m  [32/169], [94mLoss[0m : 2.78437
[1mStep[0m  [48/169], [94mLoss[0m : 2.43217
[1mStep[0m  [64/169], [94mLoss[0m : 2.84102
[1mStep[0m  [80/169], [94mLoss[0m : 2.74522
[1mStep[0m  [96/169], [94mLoss[0m : 2.46407
[1mStep[0m  [112/169], [94mLoss[0m : 2.67834
[1mStep[0m  [128/169], [94mLoss[0m : 2.86324
[1mStep[0m  [144/169], [94mLoss[0m : 2.55924
[1mStep[0m  [160/169], [94mLoss[0m : 2.71734

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.526, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24292
[1mStep[0m  [16/169], [94mLoss[0m : 2.28468
[1mStep[0m  [32/169], [94mLoss[0m : 2.90162
[1mStep[0m  [48/169], [94mLoss[0m : 2.53191
[1mStep[0m  [64/169], [94mLoss[0m : 2.69401
[1mStep[0m  [80/169], [94mLoss[0m : 2.59542
[1mStep[0m  [96/169], [94mLoss[0m : 2.52756
[1mStep[0m  [112/169], [94mLoss[0m : 2.78243
[1mStep[0m  [128/169], [94mLoss[0m : 2.52627
[1mStep[0m  [144/169], [94mLoss[0m : 3.00382
[1mStep[0m  [160/169], [94mLoss[0m : 2.46644

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.592, [92mTest[0m: 2.571, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65049
[1mStep[0m  [16/169], [94mLoss[0m : 2.30195
[1mStep[0m  [32/169], [94mLoss[0m : 2.08561
[1mStep[0m  [48/169], [94mLoss[0m : 2.46735
[1mStep[0m  [64/169], [94mLoss[0m : 2.42347
[1mStep[0m  [80/169], [94mLoss[0m : 2.13142
[1mStep[0m  [96/169], [94mLoss[0m : 2.94140
[1mStep[0m  [112/169], [94mLoss[0m : 2.35623
[1mStep[0m  [128/169], [94mLoss[0m : 2.63413
[1mStep[0m  [144/169], [94mLoss[0m : 2.94906
[1mStep[0m  [160/169], [94mLoss[0m : 2.90276

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.570, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34381
[1mStep[0m  [16/169], [94mLoss[0m : 2.58602
[1mStep[0m  [32/169], [94mLoss[0m : 3.03836
[1mStep[0m  [48/169], [94mLoss[0m : 2.47251
[1mStep[0m  [64/169], [94mLoss[0m : 2.45614
[1mStep[0m  [80/169], [94mLoss[0m : 2.72827
[1mStep[0m  [96/169], [94mLoss[0m : 2.50829
[1mStep[0m  [112/169], [94mLoss[0m : 2.89554
[1mStep[0m  [128/169], [94mLoss[0m : 2.02497
[1mStep[0m  [144/169], [94mLoss[0m : 2.45236
[1mStep[0m  [160/169], [94mLoss[0m : 2.97053

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.625, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26385
[1mStep[0m  [16/169], [94mLoss[0m : 2.38974
[1mStep[0m  [32/169], [94mLoss[0m : 2.81165
[1mStep[0m  [48/169], [94mLoss[0m : 2.19091
[1mStep[0m  [64/169], [94mLoss[0m : 2.13598
[1mStep[0m  [80/169], [94mLoss[0m : 2.64731
[1mStep[0m  [96/169], [94mLoss[0m : 2.81471
[1mStep[0m  [112/169], [94mLoss[0m : 2.84079
[1mStep[0m  [128/169], [94mLoss[0m : 2.99425
[1mStep[0m  [144/169], [94mLoss[0m : 2.23499
[1mStep[0m  [160/169], [94mLoss[0m : 2.41359

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.571, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69327
[1mStep[0m  [16/169], [94mLoss[0m : 2.41084
[1mStep[0m  [32/169], [94mLoss[0m : 2.50629
[1mStep[0m  [48/169], [94mLoss[0m : 2.38556
[1mStep[0m  [64/169], [94mLoss[0m : 2.51429
[1mStep[0m  [80/169], [94mLoss[0m : 2.60467
[1mStep[0m  [96/169], [94mLoss[0m : 2.48621
[1mStep[0m  [112/169], [94mLoss[0m : 2.71093
[1mStep[0m  [128/169], [94mLoss[0m : 2.81835
[1mStep[0m  [144/169], [94mLoss[0m : 2.16895
[1mStep[0m  [160/169], [94mLoss[0m : 2.62624

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.641, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32779
[1mStep[0m  [16/169], [94mLoss[0m : 3.01552
[1mStep[0m  [32/169], [94mLoss[0m : 2.03917
[1mStep[0m  [48/169], [94mLoss[0m : 2.30149
[1mStep[0m  [64/169], [94mLoss[0m : 2.44911
[1mStep[0m  [80/169], [94mLoss[0m : 2.91745
[1mStep[0m  [96/169], [94mLoss[0m : 2.52016
[1mStep[0m  [112/169], [94mLoss[0m : 2.16784
[1mStep[0m  [128/169], [94mLoss[0m : 2.25157
[1mStep[0m  [144/169], [94mLoss[0m : 2.61153
[1mStep[0m  [160/169], [94mLoss[0m : 2.63025

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.647, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58233
[1mStep[0m  [16/169], [94mLoss[0m : 2.49597
[1mStep[0m  [32/169], [94mLoss[0m : 2.57011
[1mStep[0m  [48/169], [94mLoss[0m : 2.06170
[1mStep[0m  [64/169], [94mLoss[0m : 2.34483
[1mStep[0m  [80/169], [94mLoss[0m : 2.13026
[1mStep[0m  [96/169], [94mLoss[0m : 2.73319
[1mStep[0m  [112/169], [94mLoss[0m : 2.28512
[1mStep[0m  [128/169], [94mLoss[0m : 2.62266
[1mStep[0m  [144/169], [94mLoss[0m : 2.42478
[1mStep[0m  [160/169], [94mLoss[0m : 2.33638

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.617, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.99444
[1mStep[0m  [16/169], [94mLoss[0m : 2.32094
[1mStep[0m  [32/169], [94mLoss[0m : 2.30278
[1mStep[0m  [48/169], [94mLoss[0m : 2.90862
[1mStep[0m  [64/169], [94mLoss[0m : 2.48400
[1mStep[0m  [80/169], [94mLoss[0m : 2.50469
[1mStep[0m  [96/169], [94mLoss[0m : 2.23137
[1mStep[0m  [112/169], [94mLoss[0m : 2.24754
[1mStep[0m  [128/169], [94mLoss[0m : 2.38515
[1mStep[0m  [144/169], [94mLoss[0m : 2.43701
[1mStep[0m  [160/169], [94mLoss[0m : 2.68347

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.608, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43250
[1mStep[0m  [16/169], [94mLoss[0m : 2.13580
[1mStep[0m  [32/169], [94mLoss[0m : 2.62234
[1mStep[0m  [48/169], [94mLoss[0m : 2.68810
[1mStep[0m  [64/169], [94mLoss[0m : 2.43942
[1mStep[0m  [80/169], [94mLoss[0m : 2.14013
[1mStep[0m  [96/169], [94mLoss[0m : 2.31026
[1mStep[0m  [112/169], [94mLoss[0m : 2.37231
[1mStep[0m  [128/169], [94mLoss[0m : 2.36192
[1mStep[0m  [144/169], [94mLoss[0m : 2.47989
[1mStep[0m  [160/169], [94mLoss[0m : 2.61700

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95922
[1mStep[0m  [16/169], [94mLoss[0m : 2.63992
[1mStep[0m  [32/169], [94mLoss[0m : 1.81467
[1mStep[0m  [48/169], [94mLoss[0m : 2.70192
[1mStep[0m  [64/169], [94mLoss[0m : 2.43160
[1mStep[0m  [80/169], [94mLoss[0m : 2.12017
[1mStep[0m  [96/169], [94mLoss[0m : 2.31945
[1mStep[0m  [112/169], [94mLoss[0m : 1.96464
[1mStep[0m  [128/169], [94mLoss[0m : 2.53199
[1mStep[0m  [144/169], [94mLoss[0m : 2.14384
[1mStep[0m  [160/169], [94mLoss[0m : 2.28908

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57687
[1mStep[0m  [16/169], [94mLoss[0m : 2.11489
[1mStep[0m  [32/169], [94mLoss[0m : 2.50511
[1mStep[0m  [48/169], [94mLoss[0m : 2.05352
[1mStep[0m  [64/169], [94mLoss[0m : 2.46119
[1mStep[0m  [80/169], [94mLoss[0m : 2.35929
[1mStep[0m  [96/169], [94mLoss[0m : 2.61320
[1mStep[0m  [112/169], [94mLoss[0m : 2.42772
[1mStep[0m  [128/169], [94mLoss[0m : 2.83248
[1mStep[0m  [144/169], [94mLoss[0m : 2.37274
[1mStep[0m  [160/169], [94mLoss[0m : 2.24872

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.571, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69296
[1mStep[0m  [16/169], [94mLoss[0m : 2.02035
[1mStep[0m  [32/169], [94mLoss[0m : 2.12052
[1mStep[0m  [48/169], [94mLoss[0m : 1.77809
[1mStep[0m  [64/169], [94mLoss[0m : 2.41379
[1mStep[0m  [80/169], [94mLoss[0m : 2.22337
[1mStep[0m  [96/169], [94mLoss[0m : 2.16187
[1mStep[0m  [112/169], [94mLoss[0m : 1.94817
[1mStep[0m  [128/169], [94mLoss[0m : 2.48989
[1mStep[0m  [144/169], [94mLoss[0m : 2.47115
[1mStep[0m  [160/169], [94mLoss[0m : 2.29940

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.355, [92mTest[0m: 2.633, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09220
[1mStep[0m  [16/169], [94mLoss[0m : 2.64640
[1mStep[0m  [32/169], [94mLoss[0m : 2.14141
[1mStep[0m  [48/169], [94mLoss[0m : 2.38268
[1mStep[0m  [64/169], [94mLoss[0m : 2.12726
[1mStep[0m  [80/169], [94mLoss[0m : 2.51802
[1mStep[0m  [96/169], [94mLoss[0m : 2.51895
[1mStep[0m  [112/169], [94mLoss[0m : 2.20848
[1mStep[0m  [128/169], [94mLoss[0m : 2.02934
[1mStep[0m  [144/169], [94mLoss[0m : 2.52935
[1mStep[0m  [160/169], [94mLoss[0m : 2.15493

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.578, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58348
[1mStep[0m  [16/169], [94mLoss[0m : 1.75072
[1mStep[0m  [32/169], [94mLoss[0m : 2.50717
[1mStep[0m  [48/169], [94mLoss[0m : 1.84813
[1mStep[0m  [64/169], [94mLoss[0m : 2.27951
[1mStep[0m  [80/169], [94mLoss[0m : 2.14779
[1mStep[0m  [96/169], [94mLoss[0m : 2.64908
[1mStep[0m  [112/169], [94mLoss[0m : 2.00047
[1mStep[0m  [128/169], [94mLoss[0m : 1.79879
[1mStep[0m  [144/169], [94mLoss[0m : 2.28188
[1mStep[0m  [160/169], [94mLoss[0m : 1.95447

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.307, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04947
[1mStep[0m  [16/169], [94mLoss[0m : 2.30693
[1mStep[0m  [32/169], [94mLoss[0m : 2.06715
[1mStep[0m  [48/169], [94mLoss[0m : 1.71830
[1mStep[0m  [64/169], [94mLoss[0m : 2.53759
[1mStep[0m  [80/169], [94mLoss[0m : 2.15533
[1mStep[0m  [96/169], [94mLoss[0m : 2.34442
[1mStep[0m  [112/169], [94mLoss[0m : 2.03660
[1mStep[0m  [128/169], [94mLoss[0m : 2.31115
[1mStep[0m  [144/169], [94mLoss[0m : 2.16162
[1mStep[0m  [160/169], [94mLoss[0m : 2.33650

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.274, [92mTest[0m: 2.574, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34611
[1mStep[0m  [16/169], [94mLoss[0m : 1.78345
[1mStep[0m  [32/169], [94mLoss[0m : 2.51569
[1mStep[0m  [48/169], [94mLoss[0m : 2.18979
[1mStep[0m  [64/169], [94mLoss[0m : 2.41579
[1mStep[0m  [80/169], [94mLoss[0m : 2.26643
[1mStep[0m  [96/169], [94mLoss[0m : 2.03237
[1mStep[0m  [112/169], [94mLoss[0m : 2.43469
[1mStep[0m  [128/169], [94mLoss[0m : 2.22248
[1mStep[0m  [144/169], [94mLoss[0m : 1.81191
[1mStep[0m  [160/169], [94mLoss[0m : 2.03417

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33040
[1mStep[0m  [16/169], [94mLoss[0m : 2.23930
[1mStep[0m  [32/169], [94mLoss[0m : 2.17035
[1mStep[0m  [48/169], [94mLoss[0m : 2.12998
[1mStep[0m  [64/169], [94mLoss[0m : 2.12444
[1mStep[0m  [80/169], [94mLoss[0m : 2.45638
[1mStep[0m  [96/169], [94mLoss[0m : 2.31853
[1mStep[0m  [112/169], [94mLoss[0m : 2.11813
[1mStep[0m  [128/169], [94mLoss[0m : 2.33893
[1mStep[0m  [144/169], [94mLoss[0m : 2.09375
[1mStep[0m  [160/169], [94mLoss[0m : 2.20545

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.208, [92mTest[0m: 2.521, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36582
[1mStep[0m  [16/169], [94mLoss[0m : 1.87278
[1mStep[0m  [32/169], [94mLoss[0m : 2.29283
[1mStep[0m  [48/169], [94mLoss[0m : 2.22940
[1mStep[0m  [64/169], [94mLoss[0m : 2.10106
[1mStep[0m  [80/169], [94mLoss[0m : 2.36374
[1mStep[0m  [96/169], [94mLoss[0m : 2.42164
[1mStep[0m  [112/169], [94mLoss[0m : 2.09370
[1mStep[0m  [128/169], [94mLoss[0m : 2.27728
[1mStep[0m  [144/169], [94mLoss[0m : 2.16033
[1mStep[0m  [160/169], [94mLoss[0m : 1.92370

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.455, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28253
[1mStep[0m  [16/169], [94mLoss[0m : 2.41449
[1mStep[0m  [32/169], [94mLoss[0m : 1.86330
[1mStep[0m  [48/169], [94mLoss[0m : 2.03155
[1mStep[0m  [64/169], [94mLoss[0m : 2.54866
[1mStep[0m  [80/169], [94mLoss[0m : 2.24953
[1mStep[0m  [96/169], [94mLoss[0m : 2.23120
[1mStep[0m  [112/169], [94mLoss[0m : 2.13551
[1mStep[0m  [128/169], [94mLoss[0m : 2.10050
[1mStep[0m  [144/169], [94mLoss[0m : 2.06139
[1mStep[0m  [160/169], [94mLoss[0m : 2.15559

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.151, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98504
[1mStep[0m  [16/169], [94mLoss[0m : 2.51546
[1mStep[0m  [32/169], [94mLoss[0m : 2.15184
[1mStep[0m  [48/169], [94mLoss[0m : 2.32563
[1mStep[0m  [64/169], [94mLoss[0m : 1.94962
[1mStep[0m  [80/169], [94mLoss[0m : 1.98508
[1mStep[0m  [96/169], [94mLoss[0m : 2.34564
[1mStep[0m  [112/169], [94mLoss[0m : 2.42830
[1mStep[0m  [128/169], [94mLoss[0m : 2.23401
[1mStep[0m  [144/169], [94mLoss[0m : 2.14487
[1mStep[0m  [160/169], [94mLoss[0m : 1.90429

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.120, [92mTest[0m: 2.567, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00696
[1mStep[0m  [16/169], [94mLoss[0m : 2.21866
[1mStep[0m  [32/169], [94mLoss[0m : 2.22883
[1mStep[0m  [48/169], [94mLoss[0m : 2.29004
[1mStep[0m  [64/169], [94mLoss[0m : 2.12492
[1mStep[0m  [80/169], [94mLoss[0m : 2.28907
[1mStep[0m  [96/169], [94mLoss[0m : 2.27976
[1mStep[0m  [112/169], [94mLoss[0m : 2.17914
[1mStep[0m  [128/169], [94mLoss[0m : 2.25997
[1mStep[0m  [144/169], [94mLoss[0m : 2.23044
[1mStep[0m  [160/169], [94mLoss[0m : 2.30508

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20671
[1mStep[0m  [16/169], [94mLoss[0m : 2.36271
[1mStep[0m  [32/169], [94mLoss[0m : 2.57670
[1mStep[0m  [48/169], [94mLoss[0m : 2.33480
[1mStep[0m  [64/169], [94mLoss[0m : 1.99350
[1mStep[0m  [80/169], [94mLoss[0m : 2.29095
[1mStep[0m  [96/169], [94mLoss[0m : 2.10197
[1mStep[0m  [112/169], [94mLoss[0m : 1.91828
[1mStep[0m  [128/169], [94mLoss[0m : 2.39911
[1mStep[0m  [144/169], [94mLoss[0m : 2.36704
[1mStep[0m  [160/169], [94mLoss[0m : 1.98857

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63429
[1mStep[0m  [16/169], [94mLoss[0m : 2.02318
[1mStep[0m  [32/169], [94mLoss[0m : 1.70315
[1mStep[0m  [48/169], [94mLoss[0m : 1.92000
[1mStep[0m  [64/169], [94mLoss[0m : 1.71689
[1mStep[0m  [80/169], [94mLoss[0m : 2.06312
[1mStep[0m  [96/169], [94mLoss[0m : 1.85520
[1mStep[0m  [112/169], [94mLoss[0m : 2.23533
[1mStep[0m  [128/169], [94mLoss[0m : 1.95726
[1mStep[0m  [144/169], [94mLoss[0m : 1.94652
[1mStep[0m  [160/169], [94mLoss[0m : 2.27211

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.571, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98368
[1mStep[0m  [16/169], [94mLoss[0m : 2.37187
[1mStep[0m  [32/169], [94mLoss[0m : 2.37881
[1mStep[0m  [48/169], [94mLoss[0m : 2.26941
[1mStep[0m  [64/169], [94mLoss[0m : 2.09492
[1mStep[0m  [80/169], [94mLoss[0m : 1.75795
[1mStep[0m  [96/169], [94mLoss[0m : 2.10431
[1mStep[0m  [112/169], [94mLoss[0m : 1.78106
[1mStep[0m  [128/169], [94mLoss[0m : 1.93287
[1mStep[0m  [144/169], [94mLoss[0m : 1.72477
[1mStep[0m  [160/169], [94mLoss[0m : 1.65421

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.008, [92mTest[0m: 2.491, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60579
[1mStep[0m  [16/169], [94mLoss[0m : 1.46707
[1mStep[0m  [32/169], [94mLoss[0m : 1.77311
[1mStep[0m  [48/169], [94mLoss[0m : 1.91742
[1mStep[0m  [64/169], [94mLoss[0m : 1.76628
[1mStep[0m  [80/169], [94mLoss[0m : 1.81166
[1mStep[0m  [96/169], [94mLoss[0m : 2.17203
[1mStep[0m  [112/169], [94mLoss[0m : 2.12535
[1mStep[0m  [128/169], [94mLoss[0m : 1.83830
[1mStep[0m  [144/169], [94mLoss[0m : 2.36170
[1mStep[0m  [160/169], [94mLoss[0m : 1.94887

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98828
[1mStep[0m  [16/169], [94mLoss[0m : 1.92876
[1mStep[0m  [32/169], [94mLoss[0m : 1.80350
[1mStep[0m  [48/169], [94mLoss[0m : 1.86898
[1mStep[0m  [64/169], [94mLoss[0m : 1.99011
[1mStep[0m  [80/169], [94mLoss[0m : 1.73695
[1mStep[0m  [96/169], [94mLoss[0m : 2.46719
[1mStep[0m  [112/169], [94mLoss[0m : 1.85764
[1mStep[0m  [128/169], [94mLoss[0m : 1.76372
[1mStep[0m  [144/169], [94mLoss[0m : 2.62900
[1mStep[0m  [160/169], [94mLoss[0m : 1.85604

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.996, [92mTest[0m: 2.532, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68684
[1mStep[0m  [16/169], [94mLoss[0m : 1.97689
[1mStep[0m  [32/169], [94mLoss[0m : 1.99545
[1mStep[0m  [48/169], [94mLoss[0m : 1.85752
[1mStep[0m  [64/169], [94mLoss[0m : 1.78278
[1mStep[0m  [80/169], [94mLoss[0m : 2.04830
[1mStep[0m  [96/169], [94mLoss[0m : 2.04088
[1mStep[0m  [112/169], [94mLoss[0m : 1.73533
[1mStep[0m  [128/169], [94mLoss[0m : 2.08579
[1mStep[0m  [144/169], [94mLoss[0m : 2.10065
[1mStep[0m  [160/169], [94mLoss[0m : 1.98072

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.556, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90291
[1mStep[0m  [16/169], [94mLoss[0m : 1.75941
[1mStep[0m  [32/169], [94mLoss[0m : 2.11886
[1mStep[0m  [48/169], [94mLoss[0m : 1.74753
[1mStep[0m  [64/169], [94mLoss[0m : 1.73834
[1mStep[0m  [80/169], [94mLoss[0m : 1.81220
[1mStep[0m  [96/169], [94mLoss[0m : 1.90144
[1mStep[0m  [112/169], [94mLoss[0m : 1.87339
[1mStep[0m  [128/169], [94mLoss[0m : 1.95123
[1mStep[0m  [144/169], [94mLoss[0m : 1.96970
[1mStep[0m  [160/169], [94mLoss[0m : 2.05936

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.522, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.531
====================================

Phase 2 - Evaluation MAE:  2.530606836080551
MAE score P1       2.342664
MAE score P2       2.530607
loss               1.951512
learning_rate      0.002575
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay           0.01
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.12331
[1mStep[0m  [16/169], [94mLoss[0m : 9.59297
[1mStep[0m  [32/169], [94mLoss[0m : 8.13049
[1mStep[0m  [48/169], [94mLoss[0m : 7.49433
[1mStep[0m  [64/169], [94mLoss[0m : 5.46520
[1mStep[0m  [80/169], [94mLoss[0m : 3.17668
[1mStep[0m  [96/169], [94mLoss[0m : 4.48141
[1mStep[0m  [112/169], [94mLoss[0m : 4.34308
[1mStep[0m  [128/169], [94mLoss[0m : 3.00630
[1mStep[0m  [144/169], [94mLoss[0m : 2.97401
[1mStep[0m  [160/169], [94mLoss[0m : 2.84096

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.369, [92mTest[0m: 10.936, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86762
[1mStep[0m  [16/169], [94mLoss[0m : 2.85687
[1mStep[0m  [32/169], [94mLoss[0m : 2.82733
[1mStep[0m  [48/169], [94mLoss[0m : 3.06567
[1mStep[0m  [64/169], [94mLoss[0m : 2.27372
[1mStep[0m  [80/169], [94mLoss[0m : 2.92709
[1mStep[0m  [96/169], [94mLoss[0m : 2.67760
[1mStep[0m  [112/169], [94mLoss[0m : 3.25869
[1mStep[0m  [128/169], [94mLoss[0m : 2.93698
[1mStep[0m  [144/169], [94mLoss[0m : 2.12964
[1mStep[0m  [160/169], [94mLoss[0m : 2.67468

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.737, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42698
[1mStep[0m  [16/169], [94mLoss[0m : 2.77214
[1mStep[0m  [32/169], [94mLoss[0m : 3.10190
[1mStep[0m  [48/169], [94mLoss[0m : 2.61766
[1mStep[0m  [64/169], [94mLoss[0m : 3.07296
[1mStep[0m  [80/169], [94mLoss[0m : 2.68737
[1mStep[0m  [96/169], [94mLoss[0m : 2.42093
[1mStep[0m  [112/169], [94mLoss[0m : 2.97666
[1mStep[0m  [128/169], [94mLoss[0m : 2.59422
[1mStep[0m  [144/169], [94mLoss[0m : 2.75253
[1mStep[0m  [160/169], [94mLoss[0m : 2.56981

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53885
[1mStep[0m  [16/169], [94mLoss[0m : 2.68851
[1mStep[0m  [32/169], [94mLoss[0m : 3.05881
[1mStep[0m  [48/169], [94mLoss[0m : 2.98045
[1mStep[0m  [64/169], [94mLoss[0m : 2.72718
[1mStep[0m  [80/169], [94mLoss[0m : 2.73028
[1mStep[0m  [96/169], [94mLoss[0m : 2.80457
[1mStep[0m  [112/169], [94mLoss[0m : 2.23249
[1mStep[0m  [128/169], [94mLoss[0m : 2.03408
[1mStep[0m  [144/169], [94mLoss[0m : 2.24233
[1mStep[0m  [160/169], [94mLoss[0m : 2.96931

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02252
[1mStep[0m  [16/169], [94mLoss[0m : 2.16105
[1mStep[0m  [32/169], [94mLoss[0m : 2.27742
[1mStep[0m  [48/169], [94mLoss[0m : 2.72375
[1mStep[0m  [64/169], [94mLoss[0m : 2.58515
[1mStep[0m  [80/169], [94mLoss[0m : 2.65821
[1mStep[0m  [96/169], [94mLoss[0m : 2.18276
[1mStep[0m  [112/169], [94mLoss[0m : 2.59638
[1mStep[0m  [128/169], [94mLoss[0m : 2.72270
[1mStep[0m  [144/169], [94mLoss[0m : 2.47787
[1mStep[0m  [160/169], [94mLoss[0m : 2.45973

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55986
[1mStep[0m  [16/169], [94mLoss[0m : 2.30956
[1mStep[0m  [32/169], [94mLoss[0m : 2.45084
[1mStep[0m  [48/169], [94mLoss[0m : 2.89865
[1mStep[0m  [64/169], [94mLoss[0m : 2.72882
[1mStep[0m  [80/169], [94mLoss[0m : 2.61179
[1mStep[0m  [96/169], [94mLoss[0m : 2.70653
[1mStep[0m  [112/169], [94mLoss[0m : 2.42212
[1mStep[0m  [128/169], [94mLoss[0m : 2.36409
[1mStep[0m  [144/169], [94mLoss[0m : 2.98354
[1mStep[0m  [160/169], [94mLoss[0m : 1.71522

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34140
[1mStep[0m  [16/169], [94mLoss[0m : 2.82489
[1mStep[0m  [32/169], [94mLoss[0m : 2.19754
[1mStep[0m  [48/169], [94mLoss[0m : 2.66046
[1mStep[0m  [64/169], [94mLoss[0m : 2.31815
[1mStep[0m  [80/169], [94mLoss[0m : 2.58056
[1mStep[0m  [96/169], [94mLoss[0m : 2.53386
[1mStep[0m  [112/169], [94mLoss[0m : 2.31109
[1mStep[0m  [128/169], [94mLoss[0m : 2.74346
[1mStep[0m  [144/169], [94mLoss[0m : 2.31702
[1mStep[0m  [160/169], [94mLoss[0m : 2.33639

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.04390
[1mStep[0m  [16/169], [94mLoss[0m : 2.52172
[1mStep[0m  [32/169], [94mLoss[0m : 2.31994
[1mStep[0m  [48/169], [94mLoss[0m : 2.33760
[1mStep[0m  [64/169], [94mLoss[0m : 2.07023
[1mStep[0m  [80/169], [94mLoss[0m : 2.59021
[1mStep[0m  [96/169], [94mLoss[0m : 2.62144
[1mStep[0m  [112/169], [94mLoss[0m : 2.53523
[1mStep[0m  [128/169], [94mLoss[0m : 2.76904
[1mStep[0m  [144/169], [94mLoss[0m : 2.41672
[1mStep[0m  [160/169], [94mLoss[0m : 2.90308

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.15622
[1mStep[0m  [16/169], [94mLoss[0m : 2.92866
[1mStep[0m  [32/169], [94mLoss[0m : 2.71116
[1mStep[0m  [48/169], [94mLoss[0m : 3.10874
[1mStep[0m  [64/169], [94mLoss[0m : 2.93313
[1mStep[0m  [80/169], [94mLoss[0m : 2.94424
[1mStep[0m  [96/169], [94mLoss[0m : 2.35920
[1mStep[0m  [112/169], [94mLoss[0m : 2.33131
[1mStep[0m  [128/169], [94mLoss[0m : 2.58460
[1mStep[0m  [144/169], [94mLoss[0m : 2.50030
[1mStep[0m  [160/169], [94mLoss[0m : 2.08560

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91670
[1mStep[0m  [16/169], [94mLoss[0m : 2.53608
[1mStep[0m  [32/169], [94mLoss[0m : 2.46413
[1mStep[0m  [48/169], [94mLoss[0m : 3.02341
[1mStep[0m  [64/169], [94mLoss[0m : 2.62721
[1mStep[0m  [80/169], [94mLoss[0m : 2.46025
[1mStep[0m  [96/169], [94mLoss[0m : 2.53510
[1mStep[0m  [112/169], [94mLoss[0m : 2.29438
[1mStep[0m  [128/169], [94mLoss[0m : 2.42073
[1mStep[0m  [144/169], [94mLoss[0m : 2.79659
[1mStep[0m  [160/169], [94mLoss[0m : 2.68723

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08104
[1mStep[0m  [16/169], [94mLoss[0m : 2.48400
[1mStep[0m  [32/169], [94mLoss[0m : 2.51536
[1mStep[0m  [48/169], [94mLoss[0m : 2.47627
[1mStep[0m  [64/169], [94mLoss[0m : 3.17751
[1mStep[0m  [80/169], [94mLoss[0m : 2.15713
[1mStep[0m  [96/169], [94mLoss[0m : 2.56501
[1mStep[0m  [112/169], [94mLoss[0m : 2.35291
[1mStep[0m  [128/169], [94mLoss[0m : 2.96911
[1mStep[0m  [144/169], [94mLoss[0m : 2.70354
[1mStep[0m  [160/169], [94mLoss[0m : 2.76233

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70856
[1mStep[0m  [16/169], [94mLoss[0m : 2.23511
[1mStep[0m  [32/169], [94mLoss[0m : 2.44332
[1mStep[0m  [48/169], [94mLoss[0m : 2.34644
[1mStep[0m  [64/169], [94mLoss[0m : 2.64654
[1mStep[0m  [80/169], [94mLoss[0m : 2.86952
[1mStep[0m  [96/169], [94mLoss[0m : 2.69282
[1mStep[0m  [112/169], [94mLoss[0m : 2.31073
[1mStep[0m  [128/169], [94mLoss[0m : 2.84837
[1mStep[0m  [144/169], [94mLoss[0m : 2.59948
[1mStep[0m  [160/169], [94mLoss[0m : 2.88709

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55174
[1mStep[0m  [16/169], [94mLoss[0m : 2.51097
[1mStep[0m  [32/169], [94mLoss[0m : 2.36991
[1mStep[0m  [48/169], [94mLoss[0m : 2.57697
[1mStep[0m  [64/169], [94mLoss[0m : 2.64213
[1mStep[0m  [80/169], [94mLoss[0m : 2.70562
[1mStep[0m  [96/169], [94mLoss[0m : 2.57494
[1mStep[0m  [112/169], [94mLoss[0m : 2.76433
[1mStep[0m  [128/169], [94mLoss[0m : 2.66932
[1mStep[0m  [144/169], [94mLoss[0m : 2.53737
[1mStep[0m  [160/169], [94mLoss[0m : 3.01595

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34613
[1mStep[0m  [16/169], [94mLoss[0m : 2.71528
[1mStep[0m  [32/169], [94mLoss[0m : 2.27644
[1mStep[0m  [48/169], [94mLoss[0m : 2.32197
[1mStep[0m  [64/169], [94mLoss[0m : 2.42758
[1mStep[0m  [80/169], [94mLoss[0m : 2.45425
[1mStep[0m  [96/169], [94mLoss[0m : 2.69419
[1mStep[0m  [112/169], [94mLoss[0m : 2.48965
[1mStep[0m  [128/169], [94mLoss[0m : 2.54524
[1mStep[0m  [144/169], [94mLoss[0m : 2.55337
[1mStep[0m  [160/169], [94mLoss[0m : 2.57232

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.307, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21096
[1mStep[0m  [16/169], [94mLoss[0m : 2.49682
[1mStep[0m  [32/169], [94mLoss[0m : 2.88449
[1mStep[0m  [48/169], [94mLoss[0m : 2.48624
[1mStep[0m  [64/169], [94mLoss[0m : 2.33404
[1mStep[0m  [80/169], [94mLoss[0m : 2.36821
[1mStep[0m  [96/169], [94mLoss[0m : 2.23600
[1mStep[0m  [112/169], [94mLoss[0m : 2.24297
[1mStep[0m  [128/169], [94mLoss[0m : 2.36606
[1mStep[0m  [144/169], [94mLoss[0m : 2.47504
[1mStep[0m  [160/169], [94mLoss[0m : 2.71484

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60985
[1mStep[0m  [16/169], [94mLoss[0m : 2.42221
[1mStep[0m  [32/169], [94mLoss[0m : 2.55180
[1mStep[0m  [48/169], [94mLoss[0m : 2.51512
[1mStep[0m  [64/169], [94mLoss[0m : 2.25191
[1mStep[0m  [80/169], [94mLoss[0m : 2.32664
[1mStep[0m  [96/169], [94mLoss[0m : 2.77232
[1mStep[0m  [112/169], [94mLoss[0m : 2.61557
[1mStep[0m  [128/169], [94mLoss[0m : 2.16303
[1mStep[0m  [144/169], [94mLoss[0m : 2.71367
[1mStep[0m  [160/169], [94mLoss[0m : 2.28306

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46713
[1mStep[0m  [16/169], [94mLoss[0m : 2.54236
[1mStep[0m  [32/169], [94mLoss[0m : 2.58126
[1mStep[0m  [48/169], [94mLoss[0m : 2.21150
[1mStep[0m  [64/169], [94mLoss[0m : 2.15950
[1mStep[0m  [80/169], [94mLoss[0m : 2.40747
[1mStep[0m  [96/169], [94mLoss[0m : 1.83121
[1mStep[0m  [112/169], [94mLoss[0m : 2.91030
[1mStep[0m  [128/169], [94mLoss[0m : 2.17826
[1mStep[0m  [144/169], [94mLoss[0m : 3.05551
[1mStep[0m  [160/169], [94mLoss[0m : 2.28593

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13841
[1mStep[0m  [16/169], [94mLoss[0m : 2.62165
[1mStep[0m  [32/169], [94mLoss[0m : 2.90630
[1mStep[0m  [48/169], [94mLoss[0m : 3.24951
[1mStep[0m  [64/169], [94mLoss[0m : 2.73735
[1mStep[0m  [80/169], [94mLoss[0m : 2.08826
[1mStep[0m  [96/169], [94mLoss[0m : 2.84070
[1mStep[0m  [112/169], [94mLoss[0m : 2.64795
[1mStep[0m  [128/169], [94mLoss[0m : 2.29878
[1mStep[0m  [144/169], [94mLoss[0m : 1.98944
[1mStep[0m  [160/169], [94mLoss[0m : 2.65603

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04907
[1mStep[0m  [16/169], [94mLoss[0m : 2.56156
[1mStep[0m  [32/169], [94mLoss[0m : 2.71280
[1mStep[0m  [48/169], [94mLoss[0m : 2.59226
[1mStep[0m  [64/169], [94mLoss[0m : 2.36095
[1mStep[0m  [80/169], [94mLoss[0m : 2.78216
[1mStep[0m  [96/169], [94mLoss[0m : 2.25292
[1mStep[0m  [112/169], [94mLoss[0m : 2.43398
[1mStep[0m  [128/169], [94mLoss[0m : 2.26266
[1mStep[0m  [144/169], [94mLoss[0m : 2.13076
[1mStep[0m  [160/169], [94mLoss[0m : 2.44775

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74095
[1mStep[0m  [16/169], [94mLoss[0m : 1.93759
[1mStep[0m  [32/169], [94mLoss[0m : 2.66813
[1mStep[0m  [48/169], [94mLoss[0m : 2.13165
[1mStep[0m  [64/169], [94mLoss[0m : 2.43053
[1mStep[0m  [80/169], [94mLoss[0m : 2.58608
[1mStep[0m  [96/169], [94mLoss[0m : 2.93314
[1mStep[0m  [112/169], [94mLoss[0m : 2.60283
[1mStep[0m  [128/169], [94mLoss[0m : 2.58083
[1mStep[0m  [144/169], [94mLoss[0m : 2.47837
[1mStep[0m  [160/169], [94mLoss[0m : 2.53528

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72773
[1mStep[0m  [16/169], [94mLoss[0m : 2.30852
[1mStep[0m  [32/169], [94mLoss[0m : 2.07047
[1mStep[0m  [48/169], [94mLoss[0m : 2.47690
[1mStep[0m  [64/169], [94mLoss[0m : 2.64762
[1mStep[0m  [80/169], [94mLoss[0m : 2.51114
[1mStep[0m  [96/169], [94mLoss[0m : 2.54378
[1mStep[0m  [112/169], [94mLoss[0m : 2.61735
[1mStep[0m  [128/169], [94mLoss[0m : 2.27084
[1mStep[0m  [144/169], [94mLoss[0m : 3.01485
[1mStep[0m  [160/169], [94mLoss[0m : 2.35244

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41269
[1mStep[0m  [16/169], [94mLoss[0m : 2.24525
[1mStep[0m  [32/169], [94mLoss[0m : 2.72013
[1mStep[0m  [48/169], [94mLoss[0m : 2.24353
[1mStep[0m  [64/169], [94mLoss[0m : 2.78365
[1mStep[0m  [80/169], [94mLoss[0m : 2.27749
[1mStep[0m  [96/169], [94mLoss[0m : 2.64275
[1mStep[0m  [112/169], [94mLoss[0m : 2.54593
[1mStep[0m  [128/169], [94mLoss[0m : 2.48777
[1mStep[0m  [144/169], [94mLoss[0m : 2.72431
[1mStep[0m  [160/169], [94mLoss[0m : 2.64552

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58741
[1mStep[0m  [16/169], [94mLoss[0m : 2.55058
[1mStep[0m  [32/169], [94mLoss[0m : 2.42144
[1mStep[0m  [48/169], [94mLoss[0m : 2.86521
[1mStep[0m  [64/169], [94mLoss[0m : 2.57949
[1mStep[0m  [80/169], [94mLoss[0m : 2.32370
[1mStep[0m  [96/169], [94mLoss[0m : 2.46552
[1mStep[0m  [112/169], [94mLoss[0m : 2.47139
[1mStep[0m  [128/169], [94mLoss[0m : 2.33280
[1mStep[0m  [144/169], [94mLoss[0m : 2.81576
[1mStep[0m  [160/169], [94mLoss[0m : 2.59363

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.314, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31604
[1mStep[0m  [16/169], [94mLoss[0m : 2.06742
[1mStep[0m  [32/169], [94mLoss[0m : 2.18098
[1mStep[0m  [48/169], [94mLoss[0m : 2.32718
[1mStep[0m  [64/169], [94mLoss[0m : 2.38856
[1mStep[0m  [80/169], [94mLoss[0m : 2.14454
[1mStep[0m  [96/169], [94mLoss[0m : 2.23575
[1mStep[0m  [112/169], [94mLoss[0m : 2.40586
[1mStep[0m  [128/169], [94mLoss[0m : 2.23679
[1mStep[0m  [144/169], [94mLoss[0m : 2.21473
[1mStep[0m  [160/169], [94mLoss[0m : 2.60092

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.08891
[1mStep[0m  [16/169], [94mLoss[0m : 2.14417
[1mStep[0m  [32/169], [94mLoss[0m : 2.28594
[1mStep[0m  [48/169], [94mLoss[0m : 2.60316
[1mStep[0m  [64/169], [94mLoss[0m : 2.64287
[1mStep[0m  [80/169], [94mLoss[0m : 1.83444
[1mStep[0m  [96/169], [94mLoss[0m : 2.45148
[1mStep[0m  [112/169], [94mLoss[0m : 2.90668
[1mStep[0m  [128/169], [94mLoss[0m : 2.16656
[1mStep[0m  [144/169], [94mLoss[0m : 2.17440
[1mStep[0m  [160/169], [94mLoss[0m : 2.58038

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.320, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14802
[1mStep[0m  [16/169], [94mLoss[0m : 2.64006
[1mStep[0m  [32/169], [94mLoss[0m : 2.55794
[1mStep[0m  [48/169], [94mLoss[0m : 2.23362
[1mStep[0m  [64/169], [94mLoss[0m : 2.45930
[1mStep[0m  [80/169], [94mLoss[0m : 2.48548
[1mStep[0m  [96/169], [94mLoss[0m : 2.76032
[1mStep[0m  [112/169], [94mLoss[0m : 2.21492
[1mStep[0m  [128/169], [94mLoss[0m : 2.59157
[1mStep[0m  [144/169], [94mLoss[0m : 2.28674
[1mStep[0m  [160/169], [94mLoss[0m : 2.39913

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31414
[1mStep[0m  [16/169], [94mLoss[0m : 2.32781
[1mStep[0m  [32/169], [94mLoss[0m : 2.21347
[1mStep[0m  [48/169], [94mLoss[0m : 2.21490
[1mStep[0m  [64/169], [94mLoss[0m : 2.35828
[1mStep[0m  [80/169], [94mLoss[0m : 2.68803
[1mStep[0m  [96/169], [94mLoss[0m : 2.30876
[1mStep[0m  [112/169], [94mLoss[0m : 2.89006
[1mStep[0m  [128/169], [94mLoss[0m : 2.11688
[1mStep[0m  [144/169], [94mLoss[0m : 2.49946
[1mStep[0m  [160/169], [94mLoss[0m : 2.39481

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43168
[1mStep[0m  [16/169], [94mLoss[0m : 2.39478
[1mStep[0m  [32/169], [94mLoss[0m : 2.46642
[1mStep[0m  [48/169], [94mLoss[0m : 2.69854
[1mStep[0m  [64/169], [94mLoss[0m : 2.46920
[1mStep[0m  [80/169], [94mLoss[0m : 2.23036
[1mStep[0m  [96/169], [94mLoss[0m : 2.23646
[1mStep[0m  [112/169], [94mLoss[0m : 2.15256
[1mStep[0m  [128/169], [94mLoss[0m : 2.78636
[1mStep[0m  [144/169], [94mLoss[0m : 2.51015
[1mStep[0m  [160/169], [94mLoss[0m : 2.61502

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14053
[1mStep[0m  [16/169], [94mLoss[0m : 2.05668
[1mStep[0m  [32/169], [94mLoss[0m : 2.24903
[1mStep[0m  [48/169], [94mLoss[0m : 2.01177
[1mStep[0m  [64/169], [94mLoss[0m : 2.65976
[1mStep[0m  [80/169], [94mLoss[0m : 2.16651
[1mStep[0m  [96/169], [94mLoss[0m : 2.59460
[1mStep[0m  [112/169], [94mLoss[0m : 2.61303
[1mStep[0m  [128/169], [94mLoss[0m : 2.46695
[1mStep[0m  [144/169], [94mLoss[0m : 2.24275
[1mStep[0m  [160/169], [94mLoss[0m : 2.84419

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.319, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31067
[1mStep[0m  [16/169], [94mLoss[0m : 1.91642
[1mStep[0m  [32/169], [94mLoss[0m : 2.48664
[1mStep[0m  [48/169], [94mLoss[0m : 2.35201
[1mStep[0m  [64/169], [94mLoss[0m : 2.07033
[1mStep[0m  [80/169], [94mLoss[0m : 2.29262
[1mStep[0m  [96/169], [94mLoss[0m : 2.11716
[1mStep[0m  [112/169], [94mLoss[0m : 2.32521
[1mStep[0m  [128/169], [94mLoss[0m : 2.19656
[1mStep[0m  [144/169], [94mLoss[0m : 2.53883
[1mStep[0m  [160/169], [94mLoss[0m : 2.42681

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.311
====================================

Phase 1 - Evaluation MAE:  2.3114981779030392
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.36074
[1mStep[0m  [16/169], [94mLoss[0m : 2.58622
[1mStep[0m  [32/169], [94mLoss[0m : 2.40206
[1mStep[0m  [48/169], [94mLoss[0m : 2.18917
[1mStep[0m  [64/169], [94mLoss[0m : 2.42480
[1mStep[0m  [80/169], [94mLoss[0m : 2.87651
[1mStep[0m  [96/169], [94mLoss[0m : 2.70784
[1mStep[0m  [112/169], [94mLoss[0m : 2.80075
[1mStep[0m  [128/169], [94mLoss[0m : 2.47781
[1mStep[0m  [144/169], [94mLoss[0m : 2.45261
[1mStep[0m  [160/169], [94mLoss[0m : 2.79919

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.305, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68207
[1mStep[0m  [16/169], [94mLoss[0m : 2.52843
[1mStep[0m  [32/169], [94mLoss[0m : 2.87671
[1mStep[0m  [48/169], [94mLoss[0m : 2.53543
[1mStep[0m  [64/169], [94mLoss[0m : 2.50534
[1mStep[0m  [80/169], [94mLoss[0m : 2.40779
[1mStep[0m  [96/169], [94mLoss[0m : 2.33326
[1mStep[0m  [112/169], [94mLoss[0m : 2.43578
[1mStep[0m  [128/169], [94mLoss[0m : 2.29344
[1mStep[0m  [144/169], [94mLoss[0m : 2.04522
[1mStep[0m  [160/169], [94mLoss[0m : 2.50420

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.943, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40959
[1mStep[0m  [16/169], [94mLoss[0m : 2.66224
[1mStep[0m  [32/169], [94mLoss[0m : 2.53714
[1mStep[0m  [48/169], [94mLoss[0m : 2.44902
[1mStep[0m  [64/169], [94mLoss[0m : 2.23149
[1mStep[0m  [80/169], [94mLoss[0m : 2.31319
[1mStep[0m  [96/169], [94mLoss[0m : 2.75162
[1mStep[0m  [112/169], [94mLoss[0m : 2.29645
[1mStep[0m  [128/169], [94mLoss[0m : 2.16870
[1mStep[0m  [144/169], [94mLoss[0m : 2.32830
[1mStep[0m  [160/169], [94mLoss[0m : 2.30122

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11552
[1mStep[0m  [16/169], [94mLoss[0m : 2.25255
[1mStep[0m  [32/169], [94mLoss[0m : 2.14794
[1mStep[0m  [48/169], [94mLoss[0m : 2.30603
[1mStep[0m  [64/169], [94mLoss[0m : 2.69435
[1mStep[0m  [80/169], [94mLoss[0m : 2.40875
[1mStep[0m  [96/169], [94mLoss[0m : 2.44743
[1mStep[0m  [112/169], [94mLoss[0m : 2.16633
[1mStep[0m  [128/169], [94mLoss[0m : 2.71736
[1mStep[0m  [144/169], [94mLoss[0m : 2.37264
[1mStep[0m  [160/169], [94mLoss[0m : 2.23147

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30521
[1mStep[0m  [16/169], [94mLoss[0m : 2.07864
[1mStep[0m  [32/169], [94mLoss[0m : 2.13682
[1mStep[0m  [48/169], [94mLoss[0m : 2.26765
[1mStep[0m  [64/169], [94mLoss[0m : 2.24103
[1mStep[0m  [80/169], [94mLoss[0m : 2.56066
[1mStep[0m  [96/169], [94mLoss[0m : 2.30278
[1mStep[0m  [112/169], [94mLoss[0m : 2.73360
[1mStep[0m  [128/169], [94mLoss[0m : 2.50804
[1mStep[0m  [144/169], [94mLoss[0m : 2.19108
[1mStep[0m  [160/169], [94mLoss[0m : 2.45411

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59613
[1mStep[0m  [16/169], [94mLoss[0m : 2.28925
[1mStep[0m  [32/169], [94mLoss[0m : 2.40248
[1mStep[0m  [48/169], [94mLoss[0m : 2.25727
[1mStep[0m  [64/169], [94mLoss[0m : 2.14876
[1mStep[0m  [80/169], [94mLoss[0m : 2.54530
[1mStep[0m  [96/169], [94mLoss[0m : 2.16911
[1mStep[0m  [112/169], [94mLoss[0m : 1.80831
[1mStep[0m  [128/169], [94mLoss[0m : 2.17936
[1mStep[0m  [144/169], [94mLoss[0m : 1.73783
[1mStep[0m  [160/169], [94mLoss[0m : 2.03968

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.220, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32863
[1mStep[0m  [16/169], [94mLoss[0m : 2.18280
[1mStep[0m  [32/169], [94mLoss[0m : 2.06323
[1mStep[0m  [48/169], [94mLoss[0m : 2.21327
[1mStep[0m  [64/169], [94mLoss[0m : 2.52890
[1mStep[0m  [80/169], [94mLoss[0m : 2.22872
[1mStep[0m  [96/169], [94mLoss[0m : 2.27427
[1mStep[0m  [112/169], [94mLoss[0m : 1.97816
[1mStep[0m  [128/169], [94mLoss[0m : 1.86924
[1mStep[0m  [144/169], [94mLoss[0m : 2.19943
[1mStep[0m  [160/169], [94mLoss[0m : 2.27369

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.375, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32623
[1mStep[0m  [16/169], [94mLoss[0m : 1.88307
[1mStep[0m  [32/169], [94mLoss[0m : 1.77077
[1mStep[0m  [48/169], [94mLoss[0m : 2.02873
[1mStep[0m  [64/169], [94mLoss[0m : 2.37636
[1mStep[0m  [80/169], [94mLoss[0m : 2.00382
[1mStep[0m  [96/169], [94mLoss[0m : 2.22145
[1mStep[0m  [112/169], [94mLoss[0m : 2.01856
[1mStep[0m  [128/169], [94mLoss[0m : 2.36057
[1mStep[0m  [144/169], [94mLoss[0m : 2.35342
[1mStep[0m  [160/169], [94mLoss[0m : 2.14131

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20424
[1mStep[0m  [16/169], [94mLoss[0m : 1.97858
[1mStep[0m  [32/169], [94mLoss[0m : 1.84460
[1mStep[0m  [48/169], [94mLoss[0m : 2.27327
[1mStep[0m  [64/169], [94mLoss[0m : 1.96375
[1mStep[0m  [80/169], [94mLoss[0m : 1.85796
[1mStep[0m  [96/169], [94mLoss[0m : 2.33292
[1mStep[0m  [112/169], [94mLoss[0m : 1.95071
[1mStep[0m  [128/169], [94mLoss[0m : 1.95594
[1mStep[0m  [144/169], [94mLoss[0m : 1.91781
[1mStep[0m  [160/169], [94mLoss[0m : 1.90894

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23642
[1mStep[0m  [16/169], [94mLoss[0m : 2.38537
[1mStep[0m  [32/169], [94mLoss[0m : 1.92006
[1mStep[0m  [48/169], [94mLoss[0m : 1.63491
[1mStep[0m  [64/169], [94mLoss[0m : 2.22945
[1mStep[0m  [80/169], [94mLoss[0m : 2.06524
[1mStep[0m  [96/169], [94mLoss[0m : 2.15168
[1mStep[0m  [112/169], [94mLoss[0m : 2.20224
[1mStep[0m  [128/169], [94mLoss[0m : 1.79483
[1mStep[0m  [144/169], [94mLoss[0m : 2.06998
[1mStep[0m  [160/169], [94mLoss[0m : 2.00849

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07799
[1mStep[0m  [16/169], [94mLoss[0m : 2.11241
[1mStep[0m  [32/169], [94mLoss[0m : 2.42194
[1mStep[0m  [48/169], [94mLoss[0m : 2.46798
[1mStep[0m  [64/169], [94mLoss[0m : 2.12984
[1mStep[0m  [80/169], [94mLoss[0m : 2.09641
[1mStep[0m  [96/169], [94mLoss[0m : 1.84427
[1mStep[0m  [112/169], [94mLoss[0m : 1.87423
[1mStep[0m  [128/169], [94mLoss[0m : 2.12860
[1mStep[0m  [144/169], [94mLoss[0m : 2.04440
[1mStep[0m  [160/169], [94mLoss[0m : 2.09799

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.027, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32658
[1mStep[0m  [16/169], [94mLoss[0m : 2.17117
[1mStep[0m  [32/169], [94mLoss[0m : 1.85287
[1mStep[0m  [48/169], [94mLoss[0m : 1.56525
[1mStep[0m  [64/169], [94mLoss[0m : 1.85323
[1mStep[0m  [80/169], [94mLoss[0m : 1.68173
[1mStep[0m  [96/169], [94mLoss[0m : 1.96700
[1mStep[0m  [112/169], [94mLoss[0m : 2.25800
[1mStep[0m  [128/169], [94mLoss[0m : 2.01803
[1mStep[0m  [144/169], [94mLoss[0m : 1.58806
[1mStep[0m  [160/169], [94mLoss[0m : 2.15698

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94975
[1mStep[0m  [16/169], [94mLoss[0m : 1.86545
[1mStep[0m  [32/169], [94mLoss[0m : 1.65087
[1mStep[0m  [48/169], [94mLoss[0m : 1.68744
[1mStep[0m  [64/169], [94mLoss[0m : 1.60890
[1mStep[0m  [80/169], [94mLoss[0m : 1.73883
[1mStep[0m  [96/169], [94mLoss[0m : 1.87748
[1mStep[0m  [112/169], [94mLoss[0m : 2.05480
[1mStep[0m  [128/169], [94mLoss[0m : 2.24295
[1mStep[0m  [144/169], [94mLoss[0m : 2.09100
[1mStep[0m  [160/169], [94mLoss[0m : 1.72243

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.928, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76230
[1mStep[0m  [16/169], [94mLoss[0m : 1.85018
[1mStep[0m  [32/169], [94mLoss[0m : 1.75590
[1mStep[0m  [48/169], [94mLoss[0m : 1.74331
[1mStep[0m  [64/169], [94mLoss[0m : 2.35342
[1mStep[0m  [80/169], [94mLoss[0m : 1.95306
[1mStep[0m  [96/169], [94mLoss[0m : 1.79091
[1mStep[0m  [112/169], [94mLoss[0m : 1.87568
[1mStep[0m  [128/169], [94mLoss[0m : 2.09617
[1mStep[0m  [144/169], [94mLoss[0m : 1.82473
[1mStep[0m  [160/169], [94mLoss[0m : 1.75751

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.492, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96732
[1mStep[0m  [16/169], [94mLoss[0m : 1.88106
[1mStep[0m  [32/169], [94mLoss[0m : 1.69958
[1mStep[0m  [48/169], [94mLoss[0m : 1.97377
[1mStep[0m  [64/169], [94mLoss[0m : 1.79720
[1mStep[0m  [80/169], [94mLoss[0m : 1.82360
[1mStep[0m  [96/169], [94mLoss[0m : 1.81313
[1mStep[0m  [112/169], [94mLoss[0m : 1.94304
[1mStep[0m  [128/169], [94mLoss[0m : 1.79329
[1mStep[0m  [144/169], [94mLoss[0m : 1.68923
[1mStep[0m  [160/169], [94mLoss[0m : 1.69923

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54031
[1mStep[0m  [16/169], [94mLoss[0m : 1.91121
[1mStep[0m  [32/169], [94mLoss[0m : 1.60997
[1mStep[0m  [48/169], [94mLoss[0m : 1.68228
[1mStep[0m  [64/169], [94mLoss[0m : 1.68375
[1mStep[0m  [80/169], [94mLoss[0m : 1.51321
[1mStep[0m  [96/169], [94mLoss[0m : 1.46109
[1mStep[0m  [112/169], [94mLoss[0m : 1.86311
[1mStep[0m  [128/169], [94mLoss[0m : 1.77591
[1mStep[0m  [144/169], [94mLoss[0m : 1.71205
[1mStep[0m  [160/169], [94mLoss[0m : 2.32003

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97141
[1mStep[0m  [16/169], [94mLoss[0m : 1.89566
[1mStep[0m  [32/169], [94mLoss[0m : 2.07073
[1mStep[0m  [48/169], [94mLoss[0m : 1.58043
[1mStep[0m  [64/169], [94mLoss[0m : 1.55618
[1mStep[0m  [80/169], [94mLoss[0m : 1.45017
[1mStep[0m  [96/169], [94mLoss[0m : 2.06149
[1mStep[0m  [112/169], [94mLoss[0m : 1.56368
[1mStep[0m  [128/169], [94mLoss[0m : 1.78520
[1mStep[0m  [144/169], [94mLoss[0m : 1.58501
[1mStep[0m  [160/169], [94mLoss[0m : 1.88143

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86161
[1mStep[0m  [16/169], [94mLoss[0m : 2.05021
[1mStep[0m  [32/169], [94mLoss[0m : 1.67673
[1mStep[0m  [48/169], [94mLoss[0m : 1.78523
[1mStep[0m  [64/169], [94mLoss[0m : 1.63770
[1mStep[0m  [80/169], [94mLoss[0m : 1.67926
[1mStep[0m  [96/169], [94mLoss[0m : 1.94020
[1mStep[0m  [112/169], [94mLoss[0m : 1.57029
[1mStep[0m  [128/169], [94mLoss[0m : 1.86475
[1mStep[0m  [144/169], [94mLoss[0m : 1.94489
[1mStep[0m  [160/169], [94mLoss[0m : 1.50153

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19115
[1mStep[0m  [16/169], [94mLoss[0m : 2.20708
[1mStep[0m  [32/169], [94mLoss[0m : 1.55650
[1mStep[0m  [48/169], [94mLoss[0m : 1.39970
[1mStep[0m  [64/169], [94mLoss[0m : 1.52835
[1mStep[0m  [80/169], [94mLoss[0m : 1.54058
[1mStep[0m  [96/169], [94mLoss[0m : 1.98061
[1mStep[0m  [112/169], [94mLoss[0m : 1.73883
[1mStep[0m  [128/169], [94mLoss[0m : 1.56833
[1mStep[0m  [144/169], [94mLoss[0m : 1.56358
[1mStep[0m  [160/169], [94mLoss[0m : 1.82572

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02077
[1mStep[0m  [16/169], [94mLoss[0m : 1.77286
[1mStep[0m  [32/169], [94mLoss[0m : 1.67697
[1mStep[0m  [48/169], [94mLoss[0m : 1.75083
[1mStep[0m  [64/169], [94mLoss[0m : 1.70913
[1mStep[0m  [80/169], [94mLoss[0m : 1.61924
[1mStep[0m  [96/169], [94mLoss[0m : 1.70634
[1mStep[0m  [112/169], [94mLoss[0m : 1.69113
[1mStep[0m  [128/169], [94mLoss[0m : 1.83645
[1mStep[0m  [144/169], [94mLoss[0m : 1.49840
[1mStep[0m  [160/169], [94mLoss[0m : 1.60589

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.730, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62867
[1mStep[0m  [16/169], [94mLoss[0m : 1.73195
[1mStep[0m  [32/169], [94mLoss[0m : 1.30536
[1mStep[0m  [48/169], [94mLoss[0m : 1.52649
[1mStep[0m  [64/169], [94mLoss[0m : 1.61867
[1mStep[0m  [80/169], [94mLoss[0m : 1.71057
[1mStep[0m  [96/169], [94mLoss[0m : 1.86001
[1mStep[0m  [112/169], [94mLoss[0m : 1.57543
[1mStep[0m  [128/169], [94mLoss[0m : 1.62084
[1mStep[0m  [144/169], [94mLoss[0m : 1.94762
[1mStep[0m  [160/169], [94mLoss[0m : 1.83924

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.73697
[1mStep[0m  [16/169], [94mLoss[0m : 1.40784
[1mStep[0m  [32/169], [94mLoss[0m : 1.54323
[1mStep[0m  [48/169], [94mLoss[0m : 1.97078
[1mStep[0m  [64/169], [94mLoss[0m : 1.54451
[1mStep[0m  [80/169], [94mLoss[0m : 1.73155
[1mStep[0m  [96/169], [94mLoss[0m : 1.69954
[1mStep[0m  [112/169], [94mLoss[0m : 1.68072
[1mStep[0m  [128/169], [94mLoss[0m : 1.68444
[1mStep[0m  [144/169], [94mLoss[0m : 1.60559
[1mStep[0m  [160/169], [94mLoss[0m : 1.37487

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.27514
[1mStep[0m  [16/169], [94mLoss[0m : 2.02587
[1mStep[0m  [32/169], [94mLoss[0m : 1.68695
[1mStep[0m  [48/169], [94mLoss[0m : 1.92932
[1mStep[0m  [64/169], [94mLoss[0m : 1.57519
[1mStep[0m  [80/169], [94mLoss[0m : 1.60696
[1mStep[0m  [96/169], [94mLoss[0m : 2.05318
[1mStep[0m  [112/169], [94mLoss[0m : 1.69354
[1mStep[0m  [128/169], [94mLoss[0m : 1.48737
[1mStep[0m  [144/169], [94mLoss[0m : 1.64512
[1mStep[0m  [160/169], [94mLoss[0m : 1.43802

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.441, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46840
[1mStep[0m  [16/169], [94mLoss[0m : 1.34325
[1mStep[0m  [32/169], [94mLoss[0m : 1.78386
[1mStep[0m  [48/169], [94mLoss[0m : 1.81973
[1mStep[0m  [64/169], [94mLoss[0m : 1.75345
[1mStep[0m  [80/169], [94mLoss[0m : 1.96839
[1mStep[0m  [96/169], [94mLoss[0m : 1.76782
[1mStep[0m  [112/169], [94mLoss[0m : 1.54214
[1mStep[0m  [128/169], [94mLoss[0m : 1.65321
[1mStep[0m  [144/169], [94mLoss[0m : 1.76433
[1mStep[0m  [160/169], [94mLoss[0m : 1.70400

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.604, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50829
[1mStep[0m  [16/169], [94mLoss[0m : 1.68457
[1mStep[0m  [32/169], [94mLoss[0m : 1.65561
[1mStep[0m  [48/169], [94mLoss[0m : 1.67302
[1mStep[0m  [64/169], [94mLoss[0m : 1.65564
[1mStep[0m  [80/169], [94mLoss[0m : 1.46436
[1mStep[0m  [96/169], [94mLoss[0m : 1.66123
[1mStep[0m  [112/169], [94mLoss[0m : 1.21139
[1mStep[0m  [128/169], [94mLoss[0m : 1.67246
[1mStep[0m  [144/169], [94mLoss[0m : 1.45260
[1mStep[0m  [160/169], [94mLoss[0m : 1.39562

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92113
[1mStep[0m  [16/169], [94mLoss[0m : 1.53753
[1mStep[0m  [32/169], [94mLoss[0m : 1.39982
[1mStep[0m  [48/169], [94mLoss[0m : 1.31476
[1mStep[0m  [64/169], [94mLoss[0m : 1.74202
[1mStep[0m  [80/169], [94mLoss[0m : 1.82975
[1mStep[0m  [96/169], [94mLoss[0m : 1.44335
[1mStep[0m  [112/169], [94mLoss[0m : 1.66271
[1mStep[0m  [128/169], [94mLoss[0m : 1.55041
[1mStep[0m  [144/169], [94mLoss[0m : 1.59483
[1mStep[0m  [160/169], [94mLoss[0m : 1.75052

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69869
[1mStep[0m  [16/169], [94mLoss[0m : 1.69287
[1mStep[0m  [32/169], [94mLoss[0m : 1.57215
[1mStep[0m  [48/169], [94mLoss[0m : 1.67671
[1mStep[0m  [64/169], [94mLoss[0m : 1.42027
[1mStep[0m  [80/169], [94mLoss[0m : 1.43030
[1mStep[0m  [96/169], [94mLoss[0m : 1.56357
[1mStep[0m  [112/169], [94mLoss[0m : 1.54085
[1mStep[0m  [128/169], [94mLoss[0m : 1.57193
[1mStep[0m  [144/169], [94mLoss[0m : 1.75924
[1mStep[0m  [160/169], [94mLoss[0m : 1.59239

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45857
[1mStep[0m  [16/169], [94mLoss[0m : 1.24415
[1mStep[0m  [32/169], [94mLoss[0m : 1.81710
[1mStep[0m  [48/169], [94mLoss[0m : 1.71437
[1mStep[0m  [64/169], [94mLoss[0m : 1.24303
[1mStep[0m  [80/169], [94mLoss[0m : 1.97350
[1mStep[0m  [96/169], [94mLoss[0m : 1.59194
[1mStep[0m  [112/169], [94mLoss[0m : 1.36841
[1mStep[0m  [128/169], [94mLoss[0m : 1.39527
[1mStep[0m  [144/169], [94mLoss[0m : 1.58062
[1mStep[0m  [160/169], [94mLoss[0m : 1.65002

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.555, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38948
[1mStep[0m  [16/169], [94mLoss[0m : 1.60681
[1mStep[0m  [32/169], [94mLoss[0m : 1.50160
[1mStep[0m  [48/169], [94mLoss[0m : 1.29771
[1mStep[0m  [64/169], [94mLoss[0m : 1.41760
[1mStep[0m  [80/169], [94mLoss[0m : 1.77274
[1mStep[0m  [96/169], [94mLoss[0m : 1.43364
[1mStep[0m  [112/169], [94mLoss[0m : 1.60302
[1mStep[0m  [128/169], [94mLoss[0m : 1.44934
[1mStep[0m  [144/169], [94mLoss[0m : 1.50018
[1mStep[0m  [160/169], [94mLoss[0m : 1.46889

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.532, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60721
[1mStep[0m  [16/169], [94mLoss[0m : 1.26759
[1mStep[0m  [32/169], [94mLoss[0m : 1.57031
[1mStep[0m  [48/169], [94mLoss[0m : 1.45061
[1mStep[0m  [64/169], [94mLoss[0m : 1.69761
[1mStep[0m  [80/169], [94mLoss[0m : 1.64028
[1mStep[0m  [96/169], [94mLoss[0m : 1.36788
[1mStep[0m  [112/169], [94mLoss[0m : 1.60110
[1mStep[0m  [128/169], [94mLoss[0m : 1.52195
[1mStep[0m  [144/169], [94mLoss[0m : 1.19216
[1mStep[0m  [160/169], [94mLoss[0m : 1.54594

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.523, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.446
====================================

Phase 2 - Evaluation MAE:  2.4458174960953847
MAE score P1      2.311498
MAE score P2      2.445817
loss               1.52332
learning_rate     0.002575
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay        0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.16049
[1mStep[0m  [8/84], [94mLoss[0m : 10.24012
[1mStep[0m  [16/84], [94mLoss[0m : 8.05375
[1mStep[0m  [24/84], [94mLoss[0m : 6.59119
[1mStep[0m  [32/84], [94mLoss[0m : 5.23418
[1mStep[0m  [40/84], [94mLoss[0m : 4.35802
[1mStep[0m  [48/84], [94mLoss[0m : 4.28224
[1mStep[0m  [56/84], [94mLoss[0m : 3.12457
[1mStep[0m  [64/84], [94mLoss[0m : 3.25860
[1mStep[0m  [72/84], [94mLoss[0m : 3.29033
[1mStep[0m  [80/84], [94mLoss[0m : 2.69139

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.383, [92mTest[0m: 10.907, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49828
[1mStep[0m  [8/84], [94mLoss[0m : 2.76051
[1mStep[0m  [16/84], [94mLoss[0m : 2.77034
[1mStep[0m  [24/84], [94mLoss[0m : 2.42829
[1mStep[0m  [32/84], [94mLoss[0m : 2.77623
[1mStep[0m  [40/84], [94mLoss[0m : 2.60185
[1mStep[0m  [48/84], [94mLoss[0m : 2.66874
[1mStep[0m  [56/84], [94mLoss[0m : 2.62961
[1mStep[0m  [64/84], [94mLoss[0m : 2.53616
[1mStep[0m  [72/84], [94mLoss[0m : 2.66866
[1mStep[0m  [80/84], [94mLoss[0m : 2.58907

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.604, [92mTest[0m: 2.996, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57745
[1mStep[0m  [8/84], [94mLoss[0m : 2.54164
[1mStep[0m  [16/84], [94mLoss[0m : 2.79345
[1mStep[0m  [24/84], [94mLoss[0m : 2.44065
[1mStep[0m  [32/84], [94mLoss[0m : 2.33514
[1mStep[0m  [40/84], [94mLoss[0m : 2.28572
[1mStep[0m  [48/84], [94mLoss[0m : 2.56395
[1mStep[0m  [56/84], [94mLoss[0m : 2.22337
[1mStep[0m  [64/84], [94mLoss[0m : 2.48786
[1mStep[0m  [72/84], [94mLoss[0m : 2.70590
[1mStep[0m  [80/84], [94mLoss[0m : 2.78609

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39296
[1mStep[0m  [8/84], [94mLoss[0m : 2.51081
[1mStep[0m  [16/84], [94mLoss[0m : 2.39142
[1mStep[0m  [24/84], [94mLoss[0m : 2.32598
[1mStep[0m  [32/84], [94mLoss[0m : 2.51031
[1mStep[0m  [40/84], [94mLoss[0m : 2.64636
[1mStep[0m  [48/84], [94mLoss[0m : 2.25613
[1mStep[0m  [56/84], [94mLoss[0m : 2.40992
[1mStep[0m  [64/84], [94mLoss[0m : 2.66171
[1mStep[0m  [72/84], [94mLoss[0m : 2.69996
[1mStep[0m  [80/84], [94mLoss[0m : 2.38860

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.595, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47800
[1mStep[0m  [8/84], [94mLoss[0m : 2.50771
[1mStep[0m  [16/84], [94mLoss[0m : 2.85183
[1mStep[0m  [24/84], [94mLoss[0m : 2.51624
[1mStep[0m  [32/84], [94mLoss[0m : 2.39868
[1mStep[0m  [40/84], [94mLoss[0m : 2.62478
[1mStep[0m  [48/84], [94mLoss[0m : 2.34941
[1mStep[0m  [56/84], [94mLoss[0m : 2.76747
[1mStep[0m  [64/84], [94mLoss[0m : 2.44540
[1mStep[0m  [72/84], [94mLoss[0m : 2.86378
[1mStep[0m  [80/84], [94mLoss[0m : 2.26930

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60481
[1mStep[0m  [8/84], [94mLoss[0m : 2.22177
[1mStep[0m  [16/84], [94mLoss[0m : 2.43593
[1mStep[0m  [24/84], [94mLoss[0m : 2.25771
[1mStep[0m  [32/84], [94mLoss[0m : 2.14657
[1mStep[0m  [40/84], [94mLoss[0m : 2.31409
[1mStep[0m  [48/84], [94mLoss[0m : 2.30548
[1mStep[0m  [56/84], [94mLoss[0m : 2.36527
[1mStep[0m  [64/84], [94mLoss[0m : 2.58402
[1mStep[0m  [72/84], [94mLoss[0m : 2.33302
[1mStep[0m  [80/84], [94mLoss[0m : 2.36453

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56996
[1mStep[0m  [8/84], [94mLoss[0m : 2.40596
[1mStep[0m  [16/84], [94mLoss[0m : 2.43726
[1mStep[0m  [24/84], [94mLoss[0m : 2.46662
[1mStep[0m  [32/84], [94mLoss[0m : 2.69229
[1mStep[0m  [40/84], [94mLoss[0m : 2.24653
[1mStep[0m  [48/84], [94mLoss[0m : 2.74304
[1mStep[0m  [56/84], [94mLoss[0m : 2.40583
[1mStep[0m  [64/84], [94mLoss[0m : 2.39945
[1mStep[0m  [72/84], [94mLoss[0m : 2.50445
[1mStep[0m  [80/84], [94mLoss[0m : 2.49410

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.549, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79680
[1mStep[0m  [8/84], [94mLoss[0m : 2.62536
[1mStep[0m  [16/84], [94mLoss[0m : 2.39711
[1mStep[0m  [24/84], [94mLoss[0m : 2.50268
[1mStep[0m  [32/84], [94mLoss[0m : 2.48002
[1mStep[0m  [40/84], [94mLoss[0m : 2.38019
[1mStep[0m  [48/84], [94mLoss[0m : 2.30996
[1mStep[0m  [56/84], [94mLoss[0m : 2.15384
[1mStep[0m  [64/84], [94mLoss[0m : 2.62343
[1mStep[0m  [72/84], [94mLoss[0m : 2.35047
[1mStep[0m  [80/84], [94mLoss[0m : 2.43466

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.561, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34600
[1mStep[0m  [8/84], [94mLoss[0m : 2.53059
[1mStep[0m  [16/84], [94mLoss[0m : 2.49344
[1mStep[0m  [24/84], [94mLoss[0m : 2.12258
[1mStep[0m  [32/84], [94mLoss[0m : 2.49760
[1mStep[0m  [40/84], [94mLoss[0m : 2.35737
[1mStep[0m  [48/84], [94mLoss[0m : 2.58182
[1mStep[0m  [56/84], [94mLoss[0m : 2.47335
[1mStep[0m  [64/84], [94mLoss[0m : 2.36115
[1mStep[0m  [72/84], [94mLoss[0m : 2.31285
[1mStep[0m  [80/84], [94mLoss[0m : 2.78562

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32240
[1mStep[0m  [8/84], [94mLoss[0m : 2.23458
[1mStep[0m  [16/84], [94mLoss[0m : 2.45725
[1mStep[0m  [24/84], [94mLoss[0m : 2.30178
[1mStep[0m  [32/84], [94mLoss[0m : 2.19052
[1mStep[0m  [40/84], [94mLoss[0m : 2.43641
[1mStep[0m  [48/84], [94mLoss[0m : 2.60998
[1mStep[0m  [56/84], [94mLoss[0m : 2.32192
[1mStep[0m  [64/84], [94mLoss[0m : 2.63029
[1mStep[0m  [72/84], [94mLoss[0m : 2.04879
[1mStep[0m  [80/84], [94mLoss[0m : 2.51364

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48231
[1mStep[0m  [8/84], [94mLoss[0m : 2.32747
[1mStep[0m  [16/84], [94mLoss[0m : 2.35648
[1mStep[0m  [24/84], [94mLoss[0m : 2.57885
[1mStep[0m  [32/84], [94mLoss[0m : 2.35086
[1mStep[0m  [40/84], [94mLoss[0m : 2.40619
[1mStep[0m  [48/84], [94mLoss[0m : 2.56178
[1mStep[0m  [56/84], [94mLoss[0m : 2.48630
[1mStep[0m  [64/84], [94mLoss[0m : 2.32526
[1mStep[0m  [72/84], [94mLoss[0m : 2.56048
[1mStep[0m  [80/84], [94mLoss[0m : 2.55299

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.568, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19676
[1mStep[0m  [8/84], [94mLoss[0m : 2.51867
[1mStep[0m  [16/84], [94mLoss[0m : 2.45767
[1mStep[0m  [24/84], [94mLoss[0m : 2.49751
[1mStep[0m  [32/84], [94mLoss[0m : 2.07216
[1mStep[0m  [40/84], [94mLoss[0m : 2.37766
[1mStep[0m  [48/84], [94mLoss[0m : 2.64120
[1mStep[0m  [56/84], [94mLoss[0m : 1.99616
[1mStep[0m  [64/84], [94mLoss[0m : 2.67710
[1mStep[0m  [72/84], [94mLoss[0m : 2.44430
[1mStep[0m  [80/84], [94mLoss[0m : 2.38143

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.530, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39979
[1mStep[0m  [8/84], [94mLoss[0m : 2.64660
[1mStep[0m  [16/84], [94mLoss[0m : 2.48451
[1mStep[0m  [24/84], [94mLoss[0m : 2.69199
[1mStep[0m  [32/84], [94mLoss[0m : 2.29056
[1mStep[0m  [40/84], [94mLoss[0m : 2.64964
[1mStep[0m  [48/84], [94mLoss[0m : 2.58447
[1mStep[0m  [56/84], [94mLoss[0m : 2.52222
[1mStep[0m  [64/84], [94mLoss[0m : 2.29260
[1mStep[0m  [72/84], [94mLoss[0m : 2.39629
[1mStep[0m  [80/84], [94mLoss[0m : 2.50244

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44275
[1mStep[0m  [8/84], [94mLoss[0m : 2.28255
[1mStep[0m  [16/84], [94mLoss[0m : 2.48226
[1mStep[0m  [24/84], [94mLoss[0m : 2.48709
[1mStep[0m  [32/84], [94mLoss[0m : 2.51601
[1mStep[0m  [40/84], [94mLoss[0m : 2.20730
[1mStep[0m  [48/84], [94mLoss[0m : 2.48856
[1mStep[0m  [56/84], [94mLoss[0m : 2.13943
[1mStep[0m  [64/84], [94mLoss[0m : 2.52814
[1mStep[0m  [72/84], [94mLoss[0m : 2.40996
[1mStep[0m  [80/84], [94mLoss[0m : 2.37540

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.502, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52864
[1mStep[0m  [8/84], [94mLoss[0m : 2.39992
[1mStep[0m  [16/84], [94mLoss[0m : 2.39235
[1mStep[0m  [24/84], [94mLoss[0m : 2.15265
[1mStep[0m  [32/84], [94mLoss[0m : 2.59333
[1mStep[0m  [40/84], [94mLoss[0m : 2.49495
[1mStep[0m  [48/84], [94mLoss[0m : 2.73070
[1mStep[0m  [56/84], [94mLoss[0m : 2.15361
[1mStep[0m  [64/84], [94mLoss[0m : 2.62497
[1mStep[0m  [72/84], [94mLoss[0m : 2.25839
[1mStep[0m  [80/84], [94mLoss[0m : 2.11185

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55435
[1mStep[0m  [8/84], [94mLoss[0m : 2.40048
[1mStep[0m  [16/84], [94mLoss[0m : 2.23677
[1mStep[0m  [24/84], [94mLoss[0m : 2.62441
[1mStep[0m  [32/84], [94mLoss[0m : 2.44766
[1mStep[0m  [40/84], [94mLoss[0m : 2.16504
[1mStep[0m  [48/84], [94mLoss[0m : 1.91327
[1mStep[0m  [56/84], [94mLoss[0m : 2.55298
[1mStep[0m  [64/84], [94mLoss[0m : 2.36782
[1mStep[0m  [72/84], [94mLoss[0m : 2.19698
[1mStep[0m  [80/84], [94mLoss[0m : 2.12871

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.509, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48804
[1mStep[0m  [8/84], [94mLoss[0m : 2.45386
[1mStep[0m  [16/84], [94mLoss[0m : 2.59924
[1mStep[0m  [24/84], [94mLoss[0m : 2.75098
[1mStep[0m  [32/84], [94mLoss[0m : 2.60101
[1mStep[0m  [40/84], [94mLoss[0m : 2.23828
[1mStep[0m  [48/84], [94mLoss[0m : 2.65973
[1mStep[0m  [56/84], [94mLoss[0m : 2.51665
[1mStep[0m  [64/84], [94mLoss[0m : 2.38144
[1mStep[0m  [72/84], [94mLoss[0m : 2.46258
[1mStep[0m  [80/84], [94mLoss[0m : 2.33570

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21472
[1mStep[0m  [8/84], [94mLoss[0m : 2.25203
[1mStep[0m  [16/84], [94mLoss[0m : 2.56291
[1mStep[0m  [24/84], [94mLoss[0m : 2.81860
[1mStep[0m  [32/84], [94mLoss[0m : 1.99333
[1mStep[0m  [40/84], [94mLoss[0m : 2.37810
[1mStep[0m  [48/84], [94mLoss[0m : 2.21621
[1mStep[0m  [56/84], [94mLoss[0m : 2.50119
[1mStep[0m  [64/84], [94mLoss[0m : 2.19923
[1mStep[0m  [72/84], [94mLoss[0m : 2.29955
[1mStep[0m  [80/84], [94mLoss[0m : 2.19041

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50443
[1mStep[0m  [8/84], [94mLoss[0m : 2.33136
[1mStep[0m  [16/84], [94mLoss[0m : 2.32642
[1mStep[0m  [24/84], [94mLoss[0m : 2.34838
[1mStep[0m  [32/84], [94mLoss[0m : 2.38269
[1mStep[0m  [40/84], [94mLoss[0m : 2.43902
[1mStep[0m  [48/84], [94mLoss[0m : 2.47106
[1mStep[0m  [56/84], [94mLoss[0m : 2.57981
[1mStep[0m  [64/84], [94mLoss[0m : 2.28831
[1mStep[0m  [72/84], [94mLoss[0m : 2.50870
[1mStep[0m  [80/84], [94mLoss[0m : 2.58487

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49075
[1mStep[0m  [8/84], [94mLoss[0m : 2.38942
[1mStep[0m  [16/84], [94mLoss[0m : 2.54398
[1mStep[0m  [24/84], [94mLoss[0m : 2.33253
[1mStep[0m  [32/84], [94mLoss[0m : 2.36919
[1mStep[0m  [40/84], [94mLoss[0m : 2.21777
[1mStep[0m  [48/84], [94mLoss[0m : 2.32596
[1mStep[0m  [56/84], [94mLoss[0m : 2.08867
[1mStep[0m  [64/84], [94mLoss[0m : 2.17477
[1mStep[0m  [72/84], [94mLoss[0m : 2.77526
[1mStep[0m  [80/84], [94mLoss[0m : 2.26874

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36803
[1mStep[0m  [8/84], [94mLoss[0m : 2.56147
[1mStep[0m  [16/84], [94mLoss[0m : 2.34281
[1mStep[0m  [24/84], [94mLoss[0m : 2.29299
[1mStep[0m  [32/84], [94mLoss[0m : 2.13901
[1mStep[0m  [40/84], [94mLoss[0m : 2.28088
[1mStep[0m  [48/84], [94mLoss[0m : 2.37695
[1mStep[0m  [56/84], [94mLoss[0m : 2.49947
[1mStep[0m  [64/84], [94mLoss[0m : 2.35348
[1mStep[0m  [72/84], [94mLoss[0m : 2.29393
[1mStep[0m  [80/84], [94mLoss[0m : 2.54563

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.542, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41783
[1mStep[0m  [8/84], [94mLoss[0m : 2.34220
[1mStep[0m  [16/84], [94mLoss[0m : 2.04565
[1mStep[0m  [24/84], [94mLoss[0m : 1.93746
[1mStep[0m  [32/84], [94mLoss[0m : 2.48951
[1mStep[0m  [40/84], [94mLoss[0m : 1.99956
[1mStep[0m  [48/84], [94mLoss[0m : 2.17948
[1mStep[0m  [56/84], [94mLoss[0m : 2.51038
[1mStep[0m  [64/84], [94mLoss[0m : 2.43226
[1mStep[0m  [72/84], [94mLoss[0m : 2.44970
[1mStep[0m  [80/84], [94mLoss[0m : 2.43866

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41899
[1mStep[0m  [8/84], [94mLoss[0m : 2.61054
[1mStep[0m  [16/84], [94mLoss[0m : 2.24762
[1mStep[0m  [24/84], [94mLoss[0m : 2.34123
[1mStep[0m  [32/84], [94mLoss[0m : 2.03941
[1mStep[0m  [40/84], [94mLoss[0m : 2.38579
[1mStep[0m  [48/84], [94mLoss[0m : 2.45199
[1mStep[0m  [56/84], [94mLoss[0m : 2.25473
[1mStep[0m  [64/84], [94mLoss[0m : 2.51096
[1mStep[0m  [72/84], [94mLoss[0m : 2.77402
[1mStep[0m  [80/84], [94mLoss[0m : 2.63672

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.494, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38611
[1mStep[0m  [8/84], [94mLoss[0m : 2.77541
[1mStep[0m  [16/84], [94mLoss[0m : 2.34571
[1mStep[0m  [24/84], [94mLoss[0m : 2.34381
[1mStep[0m  [32/84], [94mLoss[0m : 2.50839
[1mStep[0m  [40/84], [94mLoss[0m : 2.40873
[1mStep[0m  [48/84], [94mLoss[0m : 2.34827
[1mStep[0m  [56/84], [94mLoss[0m : 1.97063
[1mStep[0m  [64/84], [94mLoss[0m : 2.02871
[1mStep[0m  [72/84], [94mLoss[0m : 2.11677
[1mStep[0m  [80/84], [94mLoss[0m : 2.22868

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42518
[1mStep[0m  [8/84], [94mLoss[0m : 2.68219
[1mStep[0m  [16/84], [94mLoss[0m : 2.18729
[1mStep[0m  [24/84], [94mLoss[0m : 2.16535
[1mStep[0m  [32/84], [94mLoss[0m : 2.31541
[1mStep[0m  [40/84], [94mLoss[0m : 2.49510
[1mStep[0m  [48/84], [94mLoss[0m : 2.62155
[1mStep[0m  [56/84], [94mLoss[0m : 2.40030
[1mStep[0m  [64/84], [94mLoss[0m : 2.40569
[1mStep[0m  [72/84], [94mLoss[0m : 2.26076
[1mStep[0m  [80/84], [94mLoss[0m : 2.14450

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65531
[1mStep[0m  [8/84], [94mLoss[0m : 2.06257
[1mStep[0m  [16/84], [94mLoss[0m : 2.20324
[1mStep[0m  [24/84], [94mLoss[0m : 2.71090
[1mStep[0m  [32/84], [94mLoss[0m : 2.20687
[1mStep[0m  [40/84], [94mLoss[0m : 2.73234
[1mStep[0m  [48/84], [94mLoss[0m : 2.29478
[1mStep[0m  [56/84], [94mLoss[0m : 2.33473
[1mStep[0m  [64/84], [94mLoss[0m : 2.13558
[1mStep[0m  [72/84], [94mLoss[0m : 2.34480
[1mStep[0m  [80/84], [94mLoss[0m : 2.36279

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.357, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31285
[1mStep[0m  [8/84], [94mLoss[0m : 2.18786
[1mStep[0m  [16/84], [94mLoss[0m : 1.99449
[1mStep[0m  [24/84], [94mLoss[0m : 2.26231
[1mStep[0m  [32/84], [94mLoss[0m : 2.31248
[1mStep[0m  [40/84], [94mLoss[0m : 2.51919
[1mStep[0m  [48/84], [94mLoss[0m : 2.52263
[1mStep[0m  [56/84], [94mLoss[0m : 2.26112
[1mStep[0m  [64/84], [94mLoss[0m : 2.56126
[1mStep[0m  [72/84], [94mLoss[0m : 2.49626
[1mStep[0m  [80/84], [94mLoss[0m : 2.32112

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.466, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39028
[1mStep[0m  [8/84], [94mLoss[0m : 2.30600
[1mStep[0m  [16/84], [94mLoss[0m : 2.19414
[1mStep[0m  [24/84], [94mLoss[0m : 2.06175
[1mStep[0m  [32/84], [94mLoss[0m : 2.23680
[1mStep[0m  [40/84], [94mLoss[0m : 2.02911
[1mStep[0m  [48/84], [94mLoss[0m : 2.55736
[1mStep[0m  [56/84], [94mLoss[0m : 2.36118
[1mStep[0m  [64/84], [94mLoss[0m : 2.22057
[1mStep[0m  [72/84], [94mLoss[0m : 2.33192
[1mStep[0m  [80/84], [94mLoss[0m : 2.66587

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.431, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57746
[1mStep[0m  [8/84], [94mLoss[0m : 2.29713
[1mStep[0m  [16/84], [94mLoss[0m : 2.37457
[1mStep[0m  [24/84], [94mLoss[0m : 2.01518
[1mStep[0m  [32/84], [94mLoss[0m : 2.32213
[1mStep[0m  [40/84], [94mLoss[0m : 2.35545
[1mStep[0m  [48/84], [94mLoss[0m : 2.15695
[1mStep[0m  [56/84], [94mLoss[0m : 2.16533
[1mStep[0m  [64/84], [94mLoss[0m : 2.45818
[1mStep[0m  [72/84], [94mLoss[0m : 2.71204
[1mStep[0m  [80/84], [94mLoss[0m : 2.44328

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.345, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37114
[1mStep[0m  [8/84], [94mLoss[0m : 2.46838
[1mStep[0m  [16/84], [94mLoss[0m : 2.38561
[1mStep[0m  [24/84], [94mLoss[0m : 2.48395
[1mStep[0m  [32/84], [94mLoss[0m : 2.67756
[1mStep[0m  [40/84], [94mLoss[0m : 2.25619
[1mStep[0m  [48/84], [94mLoss[0m : 2.17194
[1mStep[0m  [56/84], [94mLoss[0m : 2.11915
[1mStep[0m  [64/84], [94mLoss[0m : 2.48283
[1mStep[0m  [72/84], [94mLoss[0m : 1.96179
[1mStep[0m  [80/84], [94mLoss[0m : 2.30264

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.440
====================================

Phase 1 - Evaluation MAE:  2.4399724262101308
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.21790
[1mStep[0m  [8/84], [94mLoss[0m : 2.05047
[1mStep[0m  [16/84], [94mLoss[0m : 2.30718
[1mStep[0m  [24/84], [94mLoss[0m : 2.64882
[1mStep[0m  [32/84], [94mLoss[0m : 2.71986
[1mStep[0m  [40/84], [94mLoss[0m : 2.35618
[1mStep[0m  [48/84], [94mLoss[0m : 2.24086
[1mStep[0m  [56/84], [94mLoss[0m : 2.51861
[1mStep[0m  [64/84], [94mLoss[0m : 2.66369
[1mStep[0m  [72/84], [94mLoss[0m : 2.23987
[1mStep[0m  [80/84], [94mLoss[0m : 2.39399

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27155
[1mStep[0m  [8/84], [94mLoss[0m : 2.27000
[1mStep[0m  [16/84], [94mLoss[0m : 2.44566
[1mStep[0m  [24/84], [94mLoss[0m : 2.34246
[1mStep[0m  [32/84], [94mLoss[0m : 2.30968
[1mStep[0m  [40/84], [94mLoss[0m : 2.43460
[1mStep[0m  [48/84], [94mLoss[0m : 2.28090
[1mStep[0m  [56/84], [94mLoss[0m : 2.29429
[1mStep[0m  [64/84], [94mLoss[0m : 2.51300
[1mStep[0m  [72/84], [94mLoss[0m : 2.34609
[1mStep[0m  [80/84], [94mLoss[0m : 2.34461

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27763
[1mStep[0m  [8/84], [94mLoss[0m : 2.22284
[1mStep[0m  [16/84], [94mLoss[0m : 2.41458
[1mStep[0m  [24/84], [94mLoss[0m : 2.18355
[1mStep[0m  [32/84], [94mLoss[0m : 2.37304
[1mStep[0m  [40/84], [94mLoss[0m : 2.37675
[1mStep[0m  [48/84], [94mLoss[0m : 2.29941
[1mStep[0m  [56/84], [94mLoss[0m : 2.16656
[1mStep[0m  [64/84], [94mLoss[0m : 2.33397
[1mStep[0m  [72/84], [94mLoss[0m : 2.19091
[1mStep[0m  [80/84], [94mLoss[0m : 2.14251

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.826, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00966
[1mStep[0m  [8/84], [94mLoss[0m : 2.32932
[1mStep[0m  [16/84], [94mLoss[0m : 2.38865
[1mStep[0m  [24/84], [94mLoss[0m : 2.19712
[1mStep[0m  [32/84], [94mLoss[0m : 2.13531
[1mStep[0m  [40/84], [94mLoss[0m : 1.87948
[1mStep[0m  [48/84], [94mLoss[0m : 2.28349
[1mStep[0m  [56/84], [94mLoss[0m : 2.37450
[1mStep[0m  [64/84], [94mLoss[0m : 2.22972
[1mStep[0m  [72/84], [94mLoss[0m : 2.12204
[1mStep[0m  [80/84], [94mLoss[0m : 2.11731

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.285, [92mTest[0m: 2.672, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01809
[1mStep[0m  [8/84], [94mLoss[0m : 2.04914
[1mStep[0m  [16/84], [94mLoss[0m : 2.35914
[1mStep[0m  [24/84], [94mLoss[0m : 2.37905
[1mStep[0m  [32/84], [94mLoss[0m : 2.18940
[1mStep[0m  [40/84], [94mLoss[0m : 2.26612
[1mStep[0m  [48/84], [94mLoss[0m : 2.13296
[1mStep[0m  [56/84], [94mLoss[0m : 2.42275
[1mStep[0m  [64/84], [94mLoss[0m : 1.99086
[1mStep[0m  [72/84], [94mLoss[0m : 2.41678
[1mStep[0m  [80/84], [94mLoss[0m : 2.25243

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08346
[1mStep[0m  [8/84], [94mLoss[0m : 2.41572
[1mStep[0m  [16/84], [94mLoss[0m : 2.43382
[1mStep[0m  [24/84], [94mLoss[0m : 2.09943
[1mStep[0m  [32/84], [94mLoss[0m : 2.15913
[1mStep[0m  [40/84], [94mLoss[0m : 2.06225
[1mStep[0m  [48/84], [94mLoss[0m : 2.26426
[1mStep[0m  [56/84], [94mLoss[0m : 1.95406
[1mStep[0m  [64/84], [94mLoss[0m : 1.92036
[1mStep[0m  [72/84], [94mLoss[0m : 2.29770
[1mStep[0m  [80/84], [94mLoss[0m : 2.21068

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30592
[1mStep[0m  [8/84], [94mLoss[0m : 1.91491
[1mStep[0m  [16/84], [94mLoss[0m : 2.13062
[1mStep[0m  [24/84], [94mLoss[0m : 2.17254
[1mStep[0m  [32/84], [94mLoss[0m : 2.16997
[1mStep[0m  [40/84], [94mLoss[0m : 2.16525
[1mStep[0m  [48/84], [94mLoss[0m : 2.00386
[1mStep[0m  [56/84], [94mLoss[0m : 1.98246
[1mStep[0m  [64/84], [94mLoss[0m : 2.12450
[1mStep[0m  [72/84], [94mLoss[0m : 2.29723
[1mStep[0m  [80/84], [94mLoss[0m : 2.06075

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.144, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.95932
[1mStep[0m  [8/84], [94mLoss[0m : 1.96275
[1mStep[0m  [16/84], [94mLoss[0m : 2.05104
[1mStep[0m  [24/84], [94mLoss[0m : 1.96735
[1mStep[0m  [32/84], [94mLoss[0m : 2.38978
[1mStep[0m  [40/84], [94mLoss[0m : 2.00692
[1mStep[0m  [48/84], [94mLoss[0m : 2.32569
[1mStep[0m  [56/84], [94mLoss[0m : 2.00456
[1mStep[0m  [64/84], [94mLoss[0m : 2.16235
[1mStep[0m  [72/84], [94mLoss[0m : 2.05505
[1mStep[0m  [80/84], [94mLoss[0m : 2.10982

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.080, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05343
[1mStep[0m  [8/84], [94mLoss[0m : 1.82680
[1mStep[0m  [16/84], [94mLoss[0m : 2.00370
[1mStep[0m  [24/84], [94mLoss[0m : 2.06291
[1mStep[0m  [32/84], [94mLoss[0m : 1.98903
[1mStep[0m  [40/84], [94mLoss[0m : 2.01003
[1mStep[0m  [48/84], [94mLoss[0m : 2.15310
[1mStep[0m  [56/84], [94mLoss[0m : 2.14228
[1mStep[0m  [64/84], [94mLoss[0m : 2.04285
[1mStep[0m  [72/84], [94mLoss[0m : 2.02902
[1mStep[0m  [80/84], [94mLoss[0m : 2.35847

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.043, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90015
[1mStep[0m  [8/84], [94mLoss[0m : 1.70551
[1mStep[0m  [16/84], [94mLoss[0m : 2.08854
[1mStep[0m  [24/84], [94mLoss[0m : 2.19011
[1mStep[0m  [32/84], [94mLoss[0m : 1.75292
[1mStep[0m  [40/84], [94mLoss[0m : 2.08720
[1mStep[0m  [48/84], [94mLoss[0m : 1.83481
[1mStep[0m  [56/84], [94mLoss[0m : 2.17697
[1mStep[0m  [64/84], [94mLoss[0m : 1.98729
[1mStep[0m  [72/84], [94mLoss[0m : 2.09976
[1mStep[0m  [80/84], [94mLoss[0m : 1.58155

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83383
[1mStep[0m  [8/84], [94mLoss[0m : 1.89504
[1mStep[0m  [16/84], [94mLoss[0m : 1.91413
[1mStep[0m  [24/84], [94mLoss[0m : 1.80518
[1mStep[0m  [32/84], [94mLoss[0m : 2.01298
[1mStep[0m  [40/84], [94mLoss[0m : 2.18038
[1mStep[0m  [48/84], [94mLoss[0m : 1.83738
[1mStep[0m  [56/84], [94mLoss[0m : 2.03335
[1mStep[0m  [64/84], [94mLoss[0m : 2.14389
[1mStep[0m  [72/84], [94mLoss[0m : 1.88018
[1mStep[0m  [80/84], [94mLoss[0m : 1.92305

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.975, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79515
[1mStep[0m  [8/84], [94mLoss[0m : 1.84409
[1mStep[0m  [16/84], [94mLoss[0m : 1.79513
[1mStep[0m  [24/84], [94mLoss[0m : 2.19749
[1mStep[0m  [32/84], [94mLoss[0m : 1.87257
[1mStep[0m  [40/84], [94mLoss[0m : 1.80923
[1mStep[0m  [48/84], [94mLoss[0m : 2.27762
[1mStep[0m  [56/84], [94mLoss[0m : 1.90279
[1mStep[0m  [64/84], [94mLoss[0m : 2.14164
[1mStep[0m  [72/84], [94mLoss[0m : 1.72454
[1mStep[0m  [80/84], [94mLoss[0m : 1.89886

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85582
[1mStep[0m  [8/84], [94mLoss[0m : 2.14971
[1mStep[0m  [16/84], [94mLoss[0m : 1.81750
[1mStep[0m  [24/84], [94mLoss[0m : 1.66298
[1mStep[0m  [32/84], [94mLoss[0m : 2.14180
[1mStep[0m  [40/84], [94mLoss[0m : 1.78639
[1mStep[0m  [48/84], [94mLoss[0m : 1.80430
[1mStep[0m  [56/84], [94mLoss[0m : 1.58748
[1mStep[0m  [64/84], [94mLoss[0m : 2.05067
[1mStep[0m  [72/84], [94mLoss[0m : 1.81906
[1mStep[0m  [80/84], [94mLoss[0m : 2.02032

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64695
[1mStep[0m  [8/84], [94mLoss[0m : 2.02854
[1mStep[0m  [16/84], [94mLoss[0m : 1.48681
[1mStep[0m  [24/84], [94mLoss[0m : 2.04567
[1mStep[0m  [32/84], [94mLoss[0m : 1.90496
[1mStep[0m  [40/84], [94mLoss[0m : 1.73279
[1mStep[0m  [48/84], [94mLoss[0m : 1.87694
[1mStep[0m  [56/84], [94mLoss[0m : 1.81584
[1mStep[0m  [64/84], [94mLoss[0m : 1.88035
[1mStep[0m  [72/84], [94mLoss[0m : 1.83083
[1mStep[0m  [80/84], [94mLoss[0m : 1.79314

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82884
[1mStep[0m  [8/84], [94mLoss[0m : 1.77114
[1mStep[0m  [16/84], [94mLoss[0m : 1.73793
[1mStep[0m  [24/84], [94mLoss[0m : 1.54060
[1mStep[0m  [32/84], [94mLoss[0m : 1.73840
[1mStep[0m  [40/84], [94mLoss[0m : 1.90817
[1mStep[0m  [48/84], [94mLoss[0m : 1.69034
[1mStep[0m  [56/84], [94mLoss[0m : 1.71720
[1mStep[0m  [64/84], [94mLoss[0m : 1.93302
[1mStep[0m  [72/84], [94mLoss[0m : 2.02339
[1mStep[0m  [80/84], [94mLoss[0m : 1.71218

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.814, [92mTest[0m: 2.546, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80129
[1mStep[0m  [8/84], [94mLoss[0m : 1.62151
[1mStep[0m  [16/84], [94mLoss[0m : 1.72984
[1mStep[0m  [24/84], [94mLoss[0m : 1.76854
[1mStep[0m  [32/84], [94mLoss[0m : 1.74347
[1mStep[0m  [40/84], [94mLoss[0m : 1.94614
[1mStep[0m  [48/84], [94mLoss[0m : 1.69917
[1mStep[0m  [56/84], [94mLoss[0m : 1.94534
[1mStep[0m  [64/84], [94mLoss[0m : 1.58080
[1mStep[0m  [72/84], [94mLoss[0m : 1.97519
[1mStep[0m  [80/84], [94mLoss[0m : 1.84422

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.663, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74261
[1mStep[0m  [8/84], [94mLoss[0m : 1.62215
[1mStep[0m  [16/84], [94mLoss[0m : 1.59572
[1mStep[0m  [24/84], [94mLoss[0m : 1.67704
[1mStep[0m  [32/84], [94mLoss[0m : 1.67451
[1mStep[0m  [40/84], [94mLoss[0m : 1.68505
[1mStep[0m  [48/84], [94mLoss[0m : 1.76199
[1mStep[0m  [56/84], [94mLoss[0m : 1.67751
[1mStep[0m  [64/84], [94mLoss[0m : 1.60737
[1mStep[0m  [72/84], [94mLoss[0m : 1.87698
[1mStep[0m  [80/84], [94mLoss[0m : 1.72047

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.625, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74423
[1mStep[0m  [8/84], [94mLoss[0m : 1.66014
[1mStep[0m  [16/84], [94mLoss[0m : 1.79350
[1mStep[0m  [24/84], [94mLoss[0m : 1.66091
[1mStep[0m  [32/84], [94mLoss[0m : 1.59282
[1mStep[0m  [40/84], [94mLoss[0m : 1.64555
[1mStep[0m  [48/84], [94mLoss[0m : 1.59813
[1mStep[0m  [56/84], [94mLoss[0m : 1.56897
[1mStep[0m  [64/84], [94mLoss[0m : 1.75541
[1mStep[0m  [72/84], [94mLoss[0m : 1.68577
[1mStep[0m  [80/84], [94mLoss[0m : 1.79267

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.690, [92mTest[0m: 2.699, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56721
[1mStep[0m  [8/84], [94mLoss[0m : 1.61329
[1mStep[0m  [16/84], [94mLoss[0m : 1.47099
[1mStep[0m  [24/84], [94mLoss[0m : 1.63900
[1mStep[0m  [32/84], [94mLoss[0m : 1.67655
[1mStep[0m  [40/84], [94mLoss[0m : 1.57012
[1mStep[0m  [48/84], [94mLoss[0m : 1.53550
[1mStep[0m  [56/84], [94mLoss[0m : 1.67747
[1mStep[0m  [64/84], [94mLoss[0m : 1.71810
[1mStep[0m  [72/84], [94mLoss[0m : 1.79753
[1mStep[0m  [80/84], [94mLoss[0m : 1.70027

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.554, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.36577
[1mStep[0m  [8/84], [94mLoss[0m : 1.76649
[1mStep[0m  [16/84], [94mLoss[0m : 1.59787
[1mStep[0m  [24/84], [94mLoss[0m : 1.64080
[1mStep[0m  [32/84], [94mLoss[0m : 1.55643
[1mStep[0m  [40/84], [94mLoss[0m : 2.05222
[1mStep[0m  [48/84], [94mLoss[0m : 1.66129
[1mStep[0m  [56/84], [94mLoss[0m : 1.77773
[1mStep[0m  [64/84], [94mLoss[0m : 1.78171
[1mStep[0m  [72/84], [94mLoss[0m : 1.76409
[1mStep[0m  [80/84], [94mLoss[0m : 1.96435

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.525, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.37004
[1mStep[0m  [8/84], [94mLoss[0m : 1.62286
[1mStep[0m  [16/84], [94mLoss[0m : 1.66964
[1mStep[0m  [24/84], [94mLoss[0m : 1.53895
[1mStep[0m  [32/84], [94mLoss[0m : 1.62983
[1mStep[0m  [40/84], [94mLoss[0m : 1.57556
[1mStep[0m  [48/84], [94mLoss[0m : 1.66107
[1mStep[0m  [56/84], [94mLoss[0m : 1.60677
[1mStep[0m  [64/84], [94mLoss[0m : 1.51103
[1mStep[0m  [72/84], [94mLoss[0m : 1.74610
[1mStep[0m  [80/84], [94mLoss[0m : 1.56719

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.650, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.56573
[1mStep[0m  [8/84], [94mLoss[0m : 1.56571
[1mStep[0m  [16/84], [94mLoss[0m : 1.70577
[1mStep[0m  [24/84], [94mLoss[0m : 1.49098
[1mStep[0m  [32/84], [94mLoss[0m : 1.64894
[1mStep[0m  [40/84], [94mLoss[0m : 1.46450
[1mStep[0m  [48/84], [94mLoss[0m : 1.74174
[1mStep[0m  [56/84], [94mLoss[0m : 1.61675
[1mStep[0m  [64/84], [94mLoss[0m : 1.55980
[1mStep[0m  [72/84], [94mLoss[0m : 1.38353
[1mStep[0m  [80/84], [94mLoss[0m : 1.73447

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.598, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73000
[1mStep[0m  [8/84], [94mLoss[0m : 1.59900
[1mStep[0m  [16/84], [94mLoss[0m : 1.40406
[1mStep[0m  [24/84], [94mLoss[0m : 1.39935
[1mStep[0m  [32/84], [94mLoss[0m : 1.51926
[1mStep[0m  [40/84], [94mLoss[0m : 1.47066
[1mStep[0m  [48/84], [94mLoss[0m : 1.52613
[1mStep[0m  [56/84], [94mLoss[0m : 1.56072
[1mStep[0m  [64/84], [94mLoss[0m : 1.52593
[1mStep[0m  [72/84], [94mLoss[0m : 1.62367
[1mStep[0m  [80/84], [94mLoss[0m : 1.57854

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.626, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.34225
[1mStep[0m  [8/84], [94mLoss[0m : 1.48189
[1mStep[0m  [16/84], [94mLoss[0m : 1.54439
[1mStep[0m  [24/84], [94mLoss[0m : 1.26015
[1mStep[0m  [32/84], [94mLoss[0m : 1.40162
[1mStep[0m  [40/84], [94mLoss[0m : 1.50436
[1mStep[0m  [48/84], [94mLoss[0m : 1.68209
[1mStep[0m  [56/84], [94mLoss[0m : 1.50887
[1mStep[0m  [64/84], [94mLoss[0m : 1.61257
[1mStep[0m  [72/84], [94mLoss[0m : 1.62800
[1mStep[0m  [80/84], [94mLoss[0m : 1.41826

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.606, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.653
====================================

Phase 2 - Evaluation MAE:  2.6529553958347867
MAE score P1      2.439972
MAE score P2      2.652955
loss              1.538093
learning_rate     0.002575
batch_size             128
hidden_sizes         [250]
epochs                  30
activation            relu
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.5
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.31058
[1mStep[0m  [16/169], [94mLoss[0m : 6.59532
[1mStep[0m  [32/169], [94mLoss[0m : 3.12043
[1mStep[0m  [48/169], [94mLoss[0m : 2.97926
[1mStep[0m  [64/169], [94mLoss[0m : 3.02646
[1mStep[0m  [80/169], [94mLoss[0m : 3.31762
[1mStep[0m  [96/169], [94mLoss[0m : 2.68867
[1mStep[0m  [112/169], [94mLoss[0m : 2.65687
[1mStep[0m  [128/169], [94mLoss[0m : 2.47063
[1mStep[0m  [144/169], [94mLoss[0m : 2.88942
[1mStep[0m  [160/169], [94mLoss[0m : 3.01766

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.656, [92mTest[0m: 10.894, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39102
[1mStep[0m  [16/169], [94mLoss[0m : 2.70304
[1mStep[0m  [32/169], [94mLoss[0m : 2.94582
[1mStep[0m  [48/169], [94mLoss[0m : 2.38291
[1mStep[0m  [64/169], [94mLoss[0m : 2.68480
[1mStep[0m  [80/169], [94mLoss[0m : 2.87350
[1mStep[0m  [96/169], [94mLoss[0m : 3.35777
[1mStep[0m  [112/169], [94mLoss[0m : 2.26482
[1mStep[0m  [128/169], [94mLoss[0m : 2.28549
[1mStep[0m  [144/169], [94mLoss[0m : 2.42274
[1mStep[0m  [160/169], [94mLoss[0m : 2.45747

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.89418
[1mStep[0m  [16/169], [94mLoss[0m : 2.57835
[1mStep[0m  [32/169], [94mLoss[0m : 2.48009
[1mStep[0m  [48/169], [94mLoss[0m : 2.62002
[1mStep[0m  [64/169], [94mLoss[0m : 2.55486
[1mStep[0m  [80/169], [94mLoss[0m : 2.48987
[1mStep[0m  [96/169], [94mLoss[0m : 2.32143
[1mStep[0m  [112/169], [94mLoss[0m : 2.25300
[1mStep[0m  [128/169], [94mLoss[0m : 2.35037
[1mStep[0m  [144/169], [94mLoss[0m : 2.99425
[1mStep[0m  [160/169], [94mLoss[0m : 2.46959

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65583
[1mStep[0m  [16/169], [94mLoss[0m : 2.67847
[1mStep[0m  [32/169], [94mLoss[0m : 2.21808
[1mStep[0m  [48/169], [94mLoss[0m : 2.68405
[1mStep[0m  [64/169], [94mLoss[0m : 2.55763
[1mStep[0m  [80/169], [94mLoss[0m : 2.25278
[1mStep[0m  [96/169], [94mLoss[0m : 2.73283
[1mStep[0m  [112/169], [94mLoss[0m : 2.87046
[1mStep[0m  [128/169], [94mLoss[0m : 2.41471
[1mStep[0m  [144/169], [94mLoss[0m : 2.54180
[1mStep[0m  [160/169], [94mLoss[0m : 2.63881

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83901
[1mStep[0m  [16/169], [94mLoss[0m : 2.64864
[1mStep[0m  [32/169], [94mLoss[0m : 2.43274
[1mStep[0m  [48/169], [94mLoss[0m : 2.33914
[1mStep[0m  [64/169], [94mLoss[0m : 2.51344
[1mStep[0m  [80/169], [94mLoss[0m : 2.83182
[1mStep[0m  [96/169], [94mLoss[0m : 2.11001
[1mStep[0m  [112/169], [94mLoss[0m : 2.60323
[1mStep[0m  [128/169], [94mLoss[0m : 2.46694
[1mStep[0m  [144/169], [94mLoss[0m : 2.41856
[1mStep[0m  [160/169], [94mLoss[0m : 2.98832

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48883
[1mStep[0m  [16/169], [94mLoss[0m : 2.36064
[1mStep[0m  [32/169], [94mLoss[0m : 2.81377
[1mStep[0m  [48/169], [94mLoss[0m : 2.72619
[1mStep[0m  [64/169], [94mLoss[0m : 2.38238
[1mStep[0m  [80/169], [94mLoss[0m : 2.14400
[1mStep[0m  [96/169], [94mLoss[0m : 2.68110
[1mStep[0m  [112/169], [94mLoss[0m : 2.40657
[1mStep[0m  [128/169], [94mLoss[0m : 2.71105
[1mStep[0m  [144/169], [94mLoss[0m : 2.68206
[1mStep[0m  [160/169], [94mLoss[0m : 2.36658

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75178
[1mStep[0m  [16/169], [94mLoss[0m : 2.50977
[1mStep[0m  [32/169], [94mLoss[0m : 2.68320
[1mStep[0m  [48/169], [94mLoss[0m : 2.93901
[1mStep[0m  [64/169], [94mLoss[0m : 2.38015
[1mStep[0m  [80/169], [94mLoss[0m : 2.67686
[1mStep[0m  [96/169], [94mLoss[0m : 2.39442
[1mStep[0m  [112/169], [94mLoss[0m : 2.33236
[1mStep[0m  [128/169], [94mLoss[0m : 2.96566
[1mStep[0m  [144/169], [94mLoss[0m : 2.24331
[1mStep[0m  [160/169], [94mLoss[0m : 2.65596

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48048
[1mStep[0m  [16/169], [94mLoss[0m : 2.17126
[1mStep[0m  [32/169], [94mLoss[0m : 2.79063
[1mStep[0m  [48/169], [94mLoss[0m : 3.06859
[1mStep[0m  [64/169], [94mLoss[0m : 2.68834
[1mStep[0m  [80/169], [94mLoss[0m : 2.05971
[1mStep[0m  [96/169], [94mLoss[0m : 2.22781
[1mStep[0m  [112/169], [94mLoss[0m : 2.53716
[1mStep[0m  [128/169], [94mLoss[0m : 2.26593
[1mStep[0m  [144/169], [94mLoss[0m : 2.85617
[1mStep[0m  [160/169], [94mLoss[0m : 2.43467

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15317
[1mStep[0m  [16/169], [94mLoss[0m : 2.21227
[1mStep[0m  [32/169], [94mLoss[0m : 2.55682
[1mStep[0m  [48/169], [94mLoss[0m : 2.55917
[1mStep[0m  [64/169], [94mLoss[0m : 2.40161
[1mStep[0m  [80/169], [94mLoss[0m : 2.64391
[1mStep[0m  [96/169], [94mLoss[0m : 2.39695
[1mStep[0m  [112/169], [94mLoss[0m : 2.74187
[1mStep[0m  [128/169], [94mLoss[0m : 2.35815
[1mStep[0m  [144/169], [94mLoss[0m : 2.48673
[1mStep[0m  [160/169], [94mLoss[0m : 2.28263

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26007
[1mStep[0m  [16/169], [94mLoss[0m : 2.07700
[1mStep[0m  [32/169], [94mLoss[0m : 2.22218
[1mStep[0m  [48/169], [94mLoss[0m : 2.60027
[1mStep[0m  [64/169], [94mLoss[0m : 2.85216
[1mStep[0m  [80/169], [94mLoss[0m : 2.16979
[1mStep[0m  [96/169], [94mLoss[0m : 2.42145
[1mStep[0m  [112/169], [94mLoss[0m : 2.57323
[1mStep[0m  [128/169], [94mLoss[0m : 1.98820
[1mStep[0m  [144/169], [94mLoss[0m : 2.59308
[1mStep[0m  [160/169], [94mLoss[0m : 2.06072

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17507
[1mStep[0m  [16/169], [94mLoss[0m : 2.49274
[1mStep[0m  [32/169], [94mLoss[0m : 1.88695
[1mStep[0m  [48/169], [94mLoss[0m : 2.31089
[1mStep[0m  [64/169], [94mLoss[0m : 2.63340
[1mStep[0m  [80/169], [94mLoss[0m : 2.23849
[1mStep[0m  [96/169], [94mLoss[0m : 2.40002
[1mStep[0m  [112/169], [94mLoss[0m : 2.54912
[1mStep[0m  [128/169], [94mLoss[0m : 2.58727
[1mStep[0m  [144/169], [94mLoss[0m : 2.80351
[1mStep[0m  [160/169], [94mLoss[0m : 2.55338

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.320, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47635
[1mStep[0m  [16/169], [94mLoss[0m : 2.27382
[1mStep[0m  [32/169], [94mLoss[0m : 2.84630
[1mStep[0m  [48/169], [94mLoss[0m : 2.14881
[1mStep[0m  [64/169], [94mLoss[0m : 2.12544
[1mStep[0m  [80/169], [94mLoss[0m : 2.35927
[1mStep[0m  [96/169], [94mLoss[0m : 2.21477
[1mStep[0m  [112/169], [94mLoss[0m : 2.49817
[1mStep[0m  [128/169], [94mLoss[0m : 2.68744
[1mStep[0m  [144/169], [94mLoss[0m : 2.11171
[1mStep[0m  [160/169], [94mLoss[0m : 2.25357

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21722
[1mStep[0m  [16/169], [94mLoss[0m : 2.46775
[1mStep[0m  [32/169], [94mLoss[0m : 2.67149
[1mStep[0m  [48/169], [94mLoss[0m : 2.62713
[1mStep[0m  [64/169], [94mLoss[0m : 2.61911
[1mStep[0m  [80/169], [94mLoss[0m : 2.07235
[1mStep[0m  [96/169], [94mLoss[0m : 1.93524
[1mStep[0m  [112/169], [94mLoss[0m : 2.36637
[1mStep[0m  [128/169], [94mLoss[0m : 2.07284
[1mStep[0m  [144/169], [94mLoss[0m : 2.06065
[1mStep[0m  [160/169], [94mLoss[0m : 2.06295

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66582
[1mStep[0m  [16/169], [94mLoss[0m : 2.23292
[1mStep[0m  [32/169], [94mLoss[0m : 2.27055
[1mStep[0m  [48/169], [94mLoss[0m : 2.24771
[1mStep[0m  [64/169], [94mLoss[0m : 2.80847
[1mStep[0m  [80/169], [94mLoss[0m : 2.26238
[1mStep[0m  [96/169], [94mLoss[0m : 2.50990
[1mStep[0m  [112/169], [94mLoss[0m : 2.39499
[1mStep[0m  [128/169], [94mLoss[0m : 2.18550
[1mStep[0m  [144/169], [94mLoss[0m : 2.52227
[1mStep[0m  [160/169], [94mLoss[0m : 2.25905

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58880
[1mStep[0m  [16/169], [94mLoss[0m : 2.72986
[1mStep[0m  [32/169], [94mLoss[0m : 2.43028
[1mStep[0m  [48/169], [94mLoss[0m : 2.33451
[1mStep[0m  [64/169], [94mLoss[0m : 2.34288
[1mStep[0m  [80/169], [94mLoss[0m : 1.95090
[1mStep[0m  [96/169], [94mLoss[0m : 1.86781
[1mStep[0m  [112/169], [94mLoss[0m : 2.57683
[1mStep[0m  [128/169], [94mLoss[0m : 2.27838
[1mStep[0m  [144/169], [94mLoss[0m : 2.18195
[1mStep[0m  [160/169], [94mLoss[0m : 2.37961

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.296, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71733
[1mStep[0m  [16/169], [94mLoss[0m : 2.38526
[1mStep[0m  [32/169], [94mLoss[0m : 2.02762
[1mStep[0m  [48/169], [94mLoss[0m : 2.59516
[1mStep[0m  [64/169], [94mLoss[0m : 2.58243
[1mStep[0m  [80/169], [94mLoss[0m : 2.07277
[1mStep[0m  [96/169], [94mLoss[0m : 2.66112
[1mStep[0m  [112/169], [94mLoss[0m : 2.39949
[1mStep[0m  [128/169], [94mLoss[0m : 3.00589
[1mStep[0m  [144/169], [94mLoss[0m : 2.62132
[1mStep[0m  [160/169], [94mLoss[0m : 2.26949

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28206
[1mStep[0m  [16/169], [94mLoss[0m : 2.33648
[1mStep[0m  [32/169], [94mLoss[0m : 2.71041
[1mStep[0m  [48/169], [94mLoss[0m : 2.54317
[1mStep[0m  [64/169], [94mLoss[0m : 2.30529
[1mStep[0m  [80/169], [94mLoss[0m : 2.38931
[1mStep[0m  [96/169], [94mLoss[0m : 2.56474
[1mStep[0m  [112/169], [94mLoss[0m : 2.25830
[1mStep[0m  [128/169], [94mLoss[0m : 2.30297
[1mStep[0m  [144/169], [94mLoss[0m : 2.22264
[1mStep[0m  [160/169], [94mLoss[0m : 2.33267

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.322, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47267
[1mStep[0m  [16/169], [94mLoss[0m : 2.48705
[1mStep[0m  [32/169], [94mLoss[0m : 2.46811
[1mStep[0m  [48/169], [94mLoss[0m : 2.46219
[1mStep[0m  [64/169], [94mLoss[0m : 2.60090
[1mStep[0m  [80/169], [94mLoss[0m : 2.19017
[1mStep[0m  [96/169], [94mLoss[0m : 2.56723
[1mStep[0m  [112/169], [94mLoss[0m : 2.78092
[1mStep[0m  [128/169], [94mLoss[0m : 2.18147
[1mStep[0m  [144/169], [94mLoss[0m : 2.75613
[1mStep[0m  [160/169], [94mLoss[0m : 2.16661

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46274
[1mStep[0m  [16/169], [94mLoss[0m : 2.54274
[1mStep[0m  [32/169], [94mLoss[0m : 2.63917
[1mStep[0m  [48/169], [94mLoss[0m : 2.25990
[1mStep[0m  [64/169], [94mLoss[0m : 2.41551
[1mStep[0m  [80/169], [94mLoss[0m : 2.45509
[1mStep[0m  [96/169], [94mLoss[0m : 2.08801
[1mStep[0m  [112/169], [94mLoss[0m : 2.66754
[1mStep[0m  [128/169], [94mLoss[0m : 2.23393
[1mStep[0m  [144/169], [94mLoss[0m : 2.23658
[1mStep[0m  [160/169], [94mLoss[0m : 2.49594

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45704
[1mStep[0m  [16/169], [94mLoss[0m : 1.92335
[1mStep[0m  [32/169], [94mLoss[0m : 2.12952
[1mStep[0m  [48/169], [94mLoss[0m : 2.33755
[1mStep[0m  [64/169], [94mLoss[0m : 2.60191
[1mStep[0m  [80/169], [94mLoss[0m : 2.58936
[1mStep[0m  [96/169], [94mLoss[0m : 2.22787
[1mStep[0m  [112/169], [94mLoss[0m : 2.33001
[1mStep[0m  [128/169], [94mLoss[0m : 1.96578
[1mStep[0m  [144/169], [94mLoss[0m : 2.41396
[1mStep[0m  [160/169], [94mLoss[0m : 2.51925

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.377, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42449
[1mStep[0m  [16/169], [94mLoss[0m : 2.23860
[1mStep[0m  [32/169], [94mLoss[0m : 2.33318
[1mStep[0m  [48/169], [94mLoss[0m : 2.27230
[1mStep[0m  [64/169], [94mLoss[0m : 2.08053
[1mStep[0m  [80/169], [94mLoss[0m : 2.45115
[1mStep[0m  [96/169], [94mLoss[0m : 2.31662
[1mStep[0m  [112/169], [94mLoss[0m : 2.19000
[1mStep[0m  [128/169], [94mLoss[0m : 1.87433
[1mStep[0m  [144/169], [94mLoss[0m : 2.04427
[1mStep[0m  [160/169], [94mLoss[0m : 2.51603

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.313, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56120
[1mStep[0m  [16/169], [94mLoss[0m : 2.52365
[1mStep[0m  [32/169], [94mLoss[0m : 2.00554
[1mStep[0m  [48/169], [94mLoss[0m : 2.17922
[1mStep[0m  [64/169], [94mLoss[0m : 2.05650
[1mStep[0m  [80/169], [94mLoss[0m : 2.23047
[1mStep[0m  [96/169], [94mLoss[0m : 2.02815
[1mStep[0m  [112/169], [94mLoss[0m : 2.42501
[1mStep[0m  [128/169], [94mLoss[0m : 2.22143
[1mStep[0m  [144/169], [94mLoss[0m : 2.34384
[1mStep[0m  [160/169], [94mLoss[0m : 2.09315

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.301, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11035
[1mStep[0m  [16/169], [94mLoss[0m : 1.98603
[1mStep[0m  [32/169], [94mLoss[0m : 2.22320
[1mStep[0m  [48/169], [94mLoss[0m : 2.45052
[1mStep[0m  [64/169], [94mLoss[0m : 2.29511
[1mStep[0m  [80/169], [94mLoss[0m : 1.84892
[1mStep[0m  [96/169], [94mLoss[0m : 2.05047
[1mStep[0m  [112/169], [94mLoss[0m : 2.29983
[1mStep[0m  [128/169], [94mLoss[0m : 2.49562
[1mStep[0m  [144/169], [94mLoss[0m : 1.98441
[1mStep[0m  [160/169], [94mLoss[0m : 2.14585

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.327, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66772
[1mStep[0m  [16/169], [94mLoss[0m : 2.03639
[1mStep[0m  [32/169], [94mLoss[0m : 2.01196
[1mStep[0m  [48/169], [94mLoss[0m : 2.64591
[1mStep[0m  [64/169], [94mLoss[0m : 2.19221
[1mStep[0m  [80/169], [94mLoss[0m : 2.59836
[1mStep[0m  [96/169], [94mLoss[0m : 2.70023
[1mStep[0m  [112/169], [94mLoss[0m : 2.58932
[1mStep[0m  [128/169], [94mLoss[0m : 2.84846
[1mStep[0m  [144/169], [94mLoss[0m : 2.61431
[1mStep[0m  [160/169], [94mLoss[0m : 1.83841

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34138
[1mStep[0m  [16/169], [94mLoss[0m : 2.45279
[1mStep[0m  [32/169], [94mLoss[0m : 2.13324
[1mStep[0m  [48/169], [94mLoss[0m : 2.53139
[1mStep[0m  [64/169], [94mLoss[0m : 2.34554
[1mStep[0m  [80/169], [94mLoss[0m : 1.95601
[1mStep[0m  [96/169], [94mLoss[0m : 2.35194
[1mStep[0m  [112/169], [94mLoss[0m : 2.54497
[1mStep[0m  [128/169], [94mLoss[0m : 2.33855
[1mStep[0m  [144/169], [94mLoss[0m : 2.28892
[1mStep[0m  [160/169], [94mLoss[0m : 2.16994

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.302, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40145
[1mStep[0m  [16/169], [94mLoss[0m : 1.98603
[1mStep[0m  [32/169], [94mLoss[0m : 1.99587
[1mStep[0m  [48/169], [94mLoss[0m : 2.79255
[1mStep[0m  [64/169], [94mLoss[0m : 2.34961
[1mStep[0m  [80/169], [94mLoss[0m : 2.34320
[1mStep[0m  [96/169], [94mLoss[0m : 2.55693
[1mStep[0m  [112/169], [94mLoss[0m : 2.31005
[1mStep[0m  [128/169], [94mLoss[0m : 2.41270
[1mStep[0m  [144/169], [94mLoss[0m : 2.13854
[1mStep[0m  [160/169], [94mLoss[0m : 2.23564

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48874
[1mStep[0m  [16/169], [94mLoss[0m : 2.45225
[1mStep[0m  [32/169], [94mLoss[0m : 1.96770
[1mStep[0m  [48/169], [94mLoss[0m : 2.70545
[1mStep[0m  [64/169], [94mLoss[0m : 1.86765
[1mStep[0m  [80/169], [94mLoss[0m : 1.88998
[1mStep[0m  [96/169], [94mLoss[0m : 2.01652
[1mStep[0m  [112/169], [94mLoss[0m : 2.48099
[1mStep[0m  [128/169], [94mLoss[0m : 2.41271
[1mStep[0m  [144/169], [94mLoss[0m : 2.35323
[1mStep[0m  [160/169], [94mLoss[0m : 2.68124

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.303, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28688
[1mStep[0m  [16/169], [94mLoss[0m : 2.37109
[1mStep[0m  [32/169], [94mLoss[0m : 2.38784
[1mStep[0m  [48/169], [94mLoss[0m : 2.88626
[1mStep[0m  [64/169], [94mLoss[0m : 2.16796
[1mStep[0m  [80/169], [94mLoss[0m : 2.40047
[1mStep[0m  [96/169], [94mLoss[0m : 2.07001
[1mStep[0m  [112/169], [94mLoss[0m : 2.54683
[1mStep[0m  [128/169], [94mLoss[0m : 2.50361
[1mStep[0m  [144/169], [94mLoss[0m : 2.04584
[1mStep[0m  [160/169], [94mLoss[0m : 2.07400

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08879
[1mStep[0m  [16/169], [94mLoss[0m : 1.98244
[1mStep[0m  [32/169], [94mLoss[0m : 2.23084
[1mStep[0m  [48/169], [94mLoss[0m : 2.03702
[1mStep[0m  [64/169], [94mLoss[0m : 2.20402
[1mStep[0m  [80/169], [94mLoss[0m : 2.24908
[1mStep[0m  [96/169], [94mLoss[0m : 2.11144
[1mStep[0m  [112/169], [94mLoss[0m : 1.87047
[1mStep[0m  [128/169], [94mLoss[0m : 2.67552
[1mStep[0m  [144/169], [94mLoss[0m : 2.68132
[1mStep[0m  [160/169], [94mLoss[0m : 2.81400

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.293, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96178
[1mStep[0m  [16/169], [94mLoss[0m : 2.31426
[1mStep[0m  [32/169], [94mLoss[0m : 2.65612
[1mStep[0m  [48/169], [94mLoss[0m : 2.60167
[1mStep[0m  [64/169], [94mLoss[0m : 2.16444
[1mStep[0m  [80/169], [94mLoss[0m : 2.12786
[1mStep[0m  [96/169], [94mLoss[0m : 2.54614
[1mStep[0m  [112/169], [94mLoss[0m : 2.44972
[1mStep[0m  [128/169], [94mLoss[0m : 1.97391
[1mStep[0m  [144/169], [94mLoss[0m : 2.59492
[1mStep[0m  [160/169], [94mLoss[0m : 1.86642

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.304, [92mTest[0m: 2.322, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.313
====================================

Phase 1 - Evaluation MAE:  2.3133135118654797
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.51576
[1mStep[0m  [16/169], [94mLoss[0m : 2.88006
[1mStep[0m  [32/169], [94mLoss[0m : 2.71905
[1mStep[0m  [48/169], [94mLoss[0m : 2.42424
[1mStep[0m  [64/169], [94mLoss[0m : 2.48864
[1mStep[0m  [80/169], [94mLoss[0m : 3.00804
[1mStep[0m  [96/169], [94mLoss[0m : 2.66544
[1mStep[0m  [112/169], [94mLoss[0m : 2.25168
[1mStep[0m  [128/169], [94mLoss[0m : 2.49422
[1mStep[0m  [144/169], [94mLoss[0m : 2.52871
[1mStep[0m  [160/169], [94mLoss[0m : 2.07879

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.313, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35903
[1mStep[0m  [16/169], [94mLoss[0m : 2.90744
[1mStep[0m  [32/169], [94mLoss[0m : 2.66418
[1mStep[0m  [48/169], [94mLoss[0m : 2.42074
[1mStep[0m  [64/169], [94mLoss[0m : 2.40505
[1mStep[0m  [80/169], [94mLoss[0m : 2.36046
[1mStep[0m  [96/169], [94mLoss[0m : 2.22734
[1mStep[0m  [112/169], [94mLoss[0m : 2.18719
[1mStep[0m  [128/169], [94mLoss[0m : 2.36111
[1mStep[0m  [144/169], [94mLoss[0m : 2.48061
[1mStep[0m  [160/169], [94mLoss[0m : 2.24180

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06513
[1mStep[0m  [16/169], [94mLoss[0m : 2.07466
[1mStep[0m  [32/169], [94mLoss[0m : 2.51545
[1mStep[0m  [48/169], [94mLoss[0m : 2.25700
[1mStep[0m  [64/169], [94mLoss[0m : 2.58638
[1mStep[0m  [80/169], [94mLoss[0m : 2.31452
[1mStep[0m  [96/169], [94mLoss[0m : 2.18540
[1mStep[0m  [112/169], [94mLoss[0m : 2.24801
[1mStep[0m  [128/169], [94mLoss[0m : 2.42769
[1mStep[0m  [144/169], [94mLoss[0m : 2.19309
[1mStep[0m  [160/169], [94mLoss[0m : 2.71186

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.00124
[1mStep[0m  [16/169], [94mLoss[0m : 2.23224
[1mStep[0m  [32/169], [94mLoss[0m : 2.07966
[1mStep[0m  [48/169], [94mLoss[0m : 1.92103
[1mStep[0m  [64/169], [94mLoss[0m : 1.89269
[1mStep[0m  [80/169], [94mLoss[0m : 1.98964
[1mStep[0m  [96/169], [94mLoss[0m : 2.52533
[1mStep[0m  [112/169], [94mLoss[0m : 2.10181
[1mStep[0m  [128/169], [94mLoss[0m : 2.57471
[1mStep[0m  [144/169], [94mLoss[0m : 1.79175
[1mStep[0m  [160/169], [94mLoss[0m : 2.89139

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.237, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48216
[1mStep[0m  [16/169], [94mLoss[0m : 1.98923
[1mStep[0m  [32/169], [94mLoss[0m : 2.09397
[1mStep[0m  [48/169], [94mLoss[0m : 2.21244
[1mStep[0m  [64/169], [94mLoss[0m : 2.00541
[1mStep[0m  [80/169], [94mLoss[0m : 2.22731
[1mStep[0m  [96/169], [94mLoss[0m : 2.27160
[1mStep[0m  [112/169], [94mLoss[0m : 2.31681
[1mStep[0m  [128/169], [94mLoss[0m : 2.01412
[1mStep[0m  [144/169], [94mLoss[0m : 1.99214
[1mStep[0m  [160/169], [94mLoss[0m : 2.10534

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88769
[1mStep[0m  [16/169], [94mLoss[0m : 2.07724
[1mStep[0m  [32/169], [94mLoss[0m : 2.09778
[1mStep[0m  [48/169], [94mLoss[0m : 2.20414
[1mStep[0m  [64/169], [94mLoss[0m : 2.15301
[1mStep[0m  [80/169], [94mLoss[0m : 1.81893
[1mStep[0m  [96/169], [94mLoss[0m : 1.77321
[1mStep[0m  [112/169], [94mLoss[0m : 2.19849
[1mStep[0m  [128/169], [94mLoss[0m : 2.22390
[1mStep[0m  [144/169], [94mLoss[0m : 2.61654
[1mStep[0m  [160/169], [94mLoss[0m : 2.59118

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28231
[1mStep[0m  [16/169], [94mLoss[0m : 1.59558
[1mStep[0m  [32/169], [94mLoss[0m : 1.85864
[1mStep[0m  [48/169], [94mLoss[0m : 1.82052
[1mStep[0m  [64/169], [94mLoss[0m : 1.69087
[1mStep[0m  [80/169], [94mLoss[0m : 1.88899
[1mStep[0m  [96/169], [94mLoss[0m : 2.28094
[1mStep[0m  [112/169], [94mLoss[0m : 2.01637
[1mStep[0m  [128/169], [94mLoss[0m : 2.01534
[1mStep[0m  [144/169], [94mLoss[0m : 2.01386
[1mStep[0m  [160/169], [94mLoss[0m : 2.23729

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74696
[1mStep[0m  [16/169], [94mLoss[0m : 2.06184
[1mStep[0m  [32/169], [94mLoss[0m : 2.06239
[1mStep[0m  [48/169], [94mLoss[0m : 1.74418
[1mStep[0m  [64/169], [94mLoss[0m : 1.99336
[1mStep[0m  [80/169], [94mLoss[0m : 1.84585
[1mStep[0m  [96/169], [94mLoss[0m : 1.92142
[1mStep[0m  [112/169], [94mLoss[0m : 1.88437
[1mStep[0m  [128/169], [94mLoss[0m : 1.93585
[1mStep[0m  [144/169], [94mLoss[0m : 1.66083
[1mStep[0m  [160/169], [94mLoss[0m : 1.91219

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87731
[1mStep[0m  [16/169], [94mLoss[0m : 1.73563
[1mStep[0m  [32/169], [94mLoss[0m : 1.74713
[1mStep[0m  [48/169], [94mLoss[0m : 1.58817
[1mStep[0m  [64/169], [94mLoss[0m : 2.16844
[1mStep[0m  [80/169], [94mLoss[0m : 1.91738
[1mStep[0m  [96/169], [94mLoss[0m : 2.09612
[1mStep[0m  [112/169], [94mLoss[0m : 1.81453
[1mStep[0m  [128/169], [94mLoss[0m : 2.17141
[1mStep[0m  [144/169], [94mLoss[0m : 2.07903
[1mStep[0m  [160/169], [94mLoss[0m : 2.28997

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87235
[1mStep[0m  [16/169], [94mLoss[0m : 2.28326
[1mStep[0m  [32/169], [94mLoss[0m : 1.57366
[1mStep[0m  [48/169], [94mLoss[0m : 1.77342
[1mStep[0m  [64/169], [94mLoss[0m : 1.56683
[1mStep[0m  [80/169], [94mLoss[0m : 1.75860
[1mStep[0m  [96/169], [94mLoss[0m : 2.04884
[1mStep[0m  [112/169], [94mLoss[0m : 1.85727
[1mStep[0m  [128/169], [94mLoss[0m : 1.91171
[1mStep[0m  [144/169], [94mLoss[0m : 1.82836
[1mStep[0m  [160/169], [94mLoss[0m : 2.02727

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76076
[1mStep[0m  [16/169], [94mLoss[0m : 2.20833
[1mStep[0m  [32/169], [94mLoss[0m : 2.03569
[1mStep[0m  [48/169], [94mLoss[0m : 1.92689
[1mStep[0m  [64/169], [94mLoss[0m : 1.72319
[1mStep[0m  [80/169], [94mLoss[0m : 1.43646
[1mStep[0m  [96/169], [94mLoss[0m : 1.75039
[1mStep[0m  [112/169], [94mLoss[0m : 1.68749
[1mStep[0m  [128/169], [94mLoss[0m : 1.85585
[1mStep[0m  [144/169], [94mLoss[0m : 1.86736
[1mStep[0m  [160/169], [94mLoss[0m : 1.90954

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52082
[1mStep[0m  [16/169], [94mLoss[0m : 1.44955
[1mStep[0m  [32/169], [94mLoss[0m : 1.87119
[1mStep[0m  [48/169], [94mLoss[0m : 2.09656
[1mStep[0m  [64/169], [94mLoss[0m : 1.82795
[1mStep[0m  [80/169], [94mLoss[0m : 1.42410
[1mStep[0m  [96/169], [94mLoss[0m : 1.96573
[1mStep[0m  [112/169], [94mLoss[0m : 1.57532
[1mStep[0m  [128/169], [94mLoss[0m : 1.76028
[1mStep[0m  [144/169], [94mLoss[0m : 1.83133
[1mStep[0m  [160/169], [94mLoss[0m : 1.77855

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77610
[1mStep[0m  [16/169], [94mLoss[0m : 1.61713
[1mStep[0m  [32/169], [94mLoss[0m : 1.89101
[1mStep[0m  [48/169], [94mLoss[0m : 1.87106
[1mStep[0m  [64/169], [94mLoss[0m : 1.72729
[1mStep[0m  [80/169], [94mLoss[0m : 2.04423
[1mStep[0m  [96/169], [94mLoss[0m : 1.97745
[1mStep[0m  [112/169], [94mLoss[0m : 1.89596
[1mStep[0m  [128/169], [94mLoss[0m : 1.76157
[1mStep[0m  [144/169], [94mLoss[0m : 1.73292
[1mStep[0m  [160/169], [94mLoss[0m : 1.51119

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.751, [92mTest[0m: 2.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84314
[1mStep[0m  [16/169], [94mLoss[0m : 1.39848
[1mStep[0m  [32/169], [94mLoss[0m : 2.22240
[1mStep[0m  [48/169], [94mLoss[0m : 1.62765
[1mStep[0m  [64/169], [94mLoss[0m : 1.55227
[1mStep[0m  [80/169], [94mLoss[0m : 1.58878
[1mStep[0m  [96/169], [94mLoss[0m : 1.59224
[1mStep[0m  [112/169], [94mLoss[0m : 1.65479
[1mStep[0m  [128/169], [94mLoss[0m : 1.69413
[1mStep[0m  [144/169], [94mLoss[0m : 1.46158
[1mStep[0m  [160/169], [94mLoss[0m : 1.73278

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.546, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60916
[1mStep[0m  [16/169], [94mLoss[0m : 1.53020
[1mStep[0m  [32/169], [94mLoss[0m : 1.88050
[1mStep[0m  [48/169], [94mLoss[0m : 1.85802
[1mStep[0m  [64/169], [94mLoss[0m : 1.51450
[1mStep[0m  [80/169], [94mLoss[0m : 1.65670
[1mStep[0m  [96/169], [94mLoss[0m : 1.34825
[1mStep[0m  [112/169], [94mLoss[0m : 1.27121
[1mStep[0m  [128/169], [94mLoss[0m : 2.63468
[1mStep[0m  [144/169], [94mLoss[0m : 1.40602
[1mStep[0m  [160/169], [94mLoss[0m : 2.05476

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.549, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59730
[1mStep[0m  [16/169], [94mLoss[0m : 1.61907
[1mStep[0m  [32/169], [94mLoss[0m : 1.53194
[1mStep[0m  [48/169], [94mLoss[0m : 1.66767
[1mStep[0m  [64/169], [94mLoss[0m : 1.78356
[1mStep[0m  [80/169], [94mLoss[0m : 1.51869
[1mStep[0m  [96/169], [94mLoss[0m : 1.31564
[1mStep[0m  [112/169], [94mLoss[0m : 1.65374
[1mStep[0m  [128/169], [94mLoss[0m : 1.95125
[1mStep[0m  [144/169], [94mLoss[0m : 1.60741
[1mStep[0m  [160/169], [94mLoss[0m : 1.66943

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.675, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62667
[1mStep[0m  [16/169], [94mLoss[0m : 1.80575
[1mStep[0m  [32/169], [94mLoss[0m : 1.52028
[1mStep[0m  [48/169], [94mLoss[0m : 1.43047
[1mStep[0m  [64/169], [94mLoss[0m : 1.66990
[1mStep[0m  [80/169], [94mLoss[0m : 1.57109
[1mStep[0m  [96/169], [94mLoss[0m : 1.71249
[1mStep[0m  [112/169], [94mLoss[0m : 1.77531
[1mStep[0m  [128/169], [94mLoss[0m : 1.81465
[1mStep[0m  [144/169], [94mLoss[0m : 2.16564
[1mStep[0m  [160/169], [94mLoss[0m : 1.64551

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.649, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51075
[1mStep[0m  [16/169], [94mLoss[0m : 1.71163
[1mStep[0m  [32/169], [94mLoss[0m : 1.75037
[1mStep[0m  [48/169], [94mLoss[0m : 1.29409
[1mStep[0m  [64/169], [94mLoss[0m : 1.55052
[1mStep[0m  [80/169], [94mLoss[0m : 1.56279
[1mStep[0m  [96/169], [94mLoss[0m : 1.52790
[1mStep[0m  [112/169], [94mLoss[0m : 1.89692
[1mStep[0m  [128/169], [94mLoss[0m : 1.96652
[1mStep[0m  [144/169], [94mLoss[0m : 1.67401
[1mStep[0m  [160/169], [94mLoss[0m : 1.77695

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.619, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49037
[1mStep[0m  [16/169], [94mLoss[0m : 1.78899
[1mStep[0m  [32/169], [94mLoss[0m : 1.09610
[1mStep[0m  [48/169], [94mLoss[0m : 1.63862
[1mStep[0m  [64/169], [94mLoss[0m : 1.48504
[1mStep[0m  [80/169], [94mLoss[0m : 1.40513
[1mStep[0m  [96/169], [94mLoss[0m : 1.52206
[1mStep[0m  [112/169], [94mLoss[0m : 1.33000
[1mStep[0m  [128/169], [94mLoss[0m : 1.80200
[1mStep[0m  [144/169], [94mLoss[0m : 1.60336
[1mStep[0m  [160/169], [94mLoss[0m : 1.50652

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.570, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63337
[1mStep[0m  [16/169], [94mLoss[0m : 1.40777
[1mStep[0m  [32/169], [94mLoss[0m : 2.01754
[1mStep[0m  [48/169], [94mLoss[0m : 1.51416
[1mStep[0m  [64/169], [94mLoss[0m : 1.55104
[1mStep[0m  [80/169], [94mLoss[0m : 1.52649
[1mStep[0m  [96/169], [94mLoss[0m : 1.28621
[1mStep[0m  [112/169], [94mLoss[0m : 1.48880
[1mStep[0m  [128/169], [94mLoss[0m : 1.49351
[1mStep[0m  [144/169], [94mLoss[0m : 2.01284
[1mStep[0m  [160/169], [94mLoss[0m : 1.63662

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.551, [92mTest[0m: 2.493, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61877
[1mStep[0m  [16/169], [94mLoss[0m : 1.68158
[1mStep[0m  [32/169], [94mLoss[0m : 1.36706
[1mStep[0m  [48/169], [94mLoss[0m : 1.30477
[1mStep[0m  [64/169], [94mLoss[0m : 1.53855
[1mStep[0m  [80/169], [94mLoss[0m : 1.73072
[1mStep[0m  [96/169], [94mLoss[0m : 1.58496
[1mStep[0m  [112/169], [94mLoss[0m : 1.32057
[1mStep[0m  [128/169], [94mLoss[0m : 1.47801
[1mStep[0m  [144/169], [94mLoss[0m : 1.67866
[1mStep[0m  [160/169], [94mLoss[0m : 1.46718

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.495, [92mTest[0m: 2.529, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38360
[1mStep[0m  [16/169], [94mLoss[0m : 1.28345
[1mStep[0m  [32/169], [94mLoss[0m : 1.54850
[1mStep[0m  [48/169], [94mLoss[0m : 1.41316
[1mStep[0m  [64/169], [94mLoss[0m : 1.47477
[1mStep[0m  [80/169], [94mLoss[0m : 1.48788
[1mStep[0m  [96/169], [94mLoss[0m : 1.73018
[1mStep[0m  [112/169], [94mLoss[0m : 1.56267
[1mStep[0m  [128/169], [94mLoss[0m : 1.47146
[1mStep[0m  [144/169], [94mLoss[0m : 1.48946
[1mStep[0m  [160/169], [94mLoss[0m : 1.24570

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32350
[1mStep[0m  [16/169], [94mLoss[0m : 1.25080
[1mStep[0m  [32/169], [94mLoss[0m : 1.58817
[1mStep[0m  [48/169], [94mLoss[0m : 1.84973
[1mStep[0m  [64/169], [94mLoss[0m : 1.14360
[1mStep[0m  [80/169], [94mLoss[0m : 1.39718
[1mStep[0m  [96/169], [94mLoss[0m : 1.35957
[1mStep[0m  [112/169], [94mLoss[0m : 1.31689
[1mStep[0m  [128/169], [94mLoss[0m : 1.78357
[1mStep[0m  [144/169], [94mLoss[0m : 1.35841
[1mStep[0m  [160/169], [94mLoss[0m : 1.30269

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49545
[1mStep[0m  [16/169], [94mLoss[0m : 1.43735
[1mStep[0m  [32/169], [94mLoss[0m : 1.41672
[1mStep[0m  [48/169], [94mLoss[0m : 1.21232
[1mStep[0m  [64/169], [94mLoss[0m : 1.37731
[1mStep[0m  [80/169], [94mLoss[0m : 1.66237
[1mStep[0m  [96/169], [94mLoss[0m : 1.39876
[1mStep[0m  [112/169], [94mLoss[0m : 1.50394
[1mStep[0m  [128/169], [94mLoss[0m : 1.15273
[1mStep[0m  [144/169], [94mLoss[0m : 1.98139
[1mStep[0m  [160/169], [94mLoss[0m : 1.50153

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.537, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50015
[1mStep[0m  [16/169], [94mLoss[0m : 1.18304
[1mStep[0m  [32/169], [94mLoss[0m : 1.30742
[1mStep[0m  [48/169], [94mLoss[0m : 1.77299
[1mStep[0m  [64/169], [94mLoss[0m : 1.23318
[1mStep[0m  [80/169], [94mLoss[0m : 1.22540
[1mStep[0m  [96/169], [94mLoss[0m : 1.30413
[1mStep[0m  [112/169], [94mLoss[0m : 1.42498
[1mStep[0m  [128/169], [94mLoss[0m : 1.44124
[1mStep[0m  [144/169], [94mLoss[0m : 1.27646
[1mStep[0m  [160/169], [94mLoss[0m : 1.41394

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.405, [92mTest[0m: 2.575, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 24 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.512
====================================

Phase 2 - Evaluation MAE:  2.5122190288134982
MAE score P1        2.313314
MAE score P2        2.512219
loss                1.405234
learning_rate       0.002575
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay           0.001
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.25571
[1mStep[0m  [8/84], [94mLoss[0m : 10.93396
[1mStep[0m  [16/84], [94mLoss[0m : 10.60600
[1mStep[0m  [24/84], [94mLoss[0m : 10.17681
[1mStep[0m  [32/84], [94mLoss[0m : 11.01335
[1mStep[0m  [40/84], [94mLoss[0m : 10.40325
[1mStep[0m  [48/84], [94mLoss[0m : 11.01908
[1mStep[0m  [56/84], [94mLoss[0m : 10.69907
[1mStep[0m  [64/84], [94mLoss[0m : 10.44755
[1mStep[0m  [72/84], [94mLoss[0m : 10.48109
[1mStep[0m  [80/84], [94mLoss[0m : 10.50212

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.695, [92mTest[0m: 10.762, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.57203
[1mStep[0m  [8/84], [94mLoss[0m : 9.83897
[1mStep[0m  [16/84], [94mLoss[0m : 10.37880
[1mStep[0m  [24/84], [94mLoss[0m : 10.80994
[1mStep[0m  [32/84], [94mLoss[0m : 11.11766
[1mStep[0m  [40/84], [94mLoss[0m : 10.22842
[1mStep[0m  [48/84], [94mLoss[0m : 10.23714
[1mStep[0m  [56/84], [94mLoss[0m : 10.71647
[1mStep[0m  [64/84], [94mLoss[0m : 10.18528
[1mStep[0m  [72/84], [94mLoss[0m : 9.92161
[1mStep[0m  [80/84], [94mLoss[0m : 9.79246

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.402, [92mTest[0m: 10.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.33183
[1mStep[0m  [8/84], [94mLoss[0m : 10.39556
[1mStep[0m  [16/84], [94mLoss[0m : 10.65376
[1mStep[0m  [24/84], [94mLoss[0m : 10.36317
[1mStep[0m  [32/84], [94mLoss[0m : 9.57227
[1mStep[0m  [40/84], [94mLoss[0m : 10.00703
[1mStep[0m  [48/84], [94mLoss[0m : 10.21546
[1mStep[0m  [56/84], [94mLoss[0m : 10.32308
[1mStep[0m  [64/84], [94mLoss[0m : 10.12752
[1mStep[0m  [72/84], [94mLoss[0m : 9.97586
[1mStep[0m  [80/84], [94mLoss[0m : 9.39997

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 10.093, [92mTest[0m: 10.061, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.92236
[1mStep[0m  [8/84], [94mLoss[0m : 10.35389
[1mStep[0m  [16/84], [94mLoss[0m : 9.74623
[1mStep[0m  [24/84], [94mLoss[0m : 9.62679
[1mStep[0m  [32/84], [94mLoss[0m : 9.57881
[1mStep[0m  [40/84], [94mLoss[0m : 10.42219
[1mStep[0m  [48/84], [94mLoss[0m : 9.82553
[1mStep[0m  [56/84], [94mLoss[0m : 9.74468
[1mStep[0m  [64/84], [94mLoss[0m : 9.61452
[1mStep[0m  [72/84], [94mLoss[0m : 9.67193
[1mStep[0m  [80/84], [94mLoss[0m : 9.53420

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.779, [92mTest[0m: 9.681, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.06652
[1mStep[0m  [8/84], [94mLoss[0m : 9.54681
[1mStep[0m  [16/84], [94mLoss[0m : 9.99659
[1mStep[0m  [24/84], [94mLoss[0m : 8.95351
[1mStep[0m  [32/84], [94mLoss[0m : 9.41177
[1mStep[0m  [40/84], [94mLoss[0m : 9.54848
[1mStep[0m  [48/84], [94mLoss[0m : 8.81890
[1mStep[0m  [56/84], [94mLoss[0m : 9.61901
[1mStep[0m  [64/84], [94mLoss[0m : 9.59482
[1mStep[0m  [72/84], [94mLoss[0m : 9.15793
[1mStep[0m  [80/84], [94mLoss[0m : 9.16577

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 9.423, [92mTest[0m: 9.206, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.01253
[1mStep[0m  [8/84], [94mLoss[0m : 8.77646
[1mStep[0m  [16/84], [94mLoss[0m : 8.88500
[1mStep[0m  [24/84], [94mLoss[0m : 9.01204
[1mStep[0m  [32/84], [94mLoss[0m : 8.82890
[1mStep[0m  [40/84], [94mLoss[0m : 8.76714
[1mStep[0m  [48/84], [94mLoss[0m : 8.70759
[1mStep[0m  [56/84], [94mLoss[0m : 8.62854
[1mStep[0m  [64/84], [94mLoss[0m : 8.94884
[1mStep[0m  [72/84], [94mLoss[0m : 9.12193
[1mStep[0m  [80/84], [94mLoss[0m : 9.02205

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 9.018, [92mTest[0m: 8.751, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.00136
[1mStep[0m  [8/84], [94mLoss[0m : 8.43674
[1mStep[0m  [16/84], [94mLoss[0m : 9.05954
[1mStep[0m  [24/84], [94mLoss[0m : 8.55618
[1mStep[0m  [32/84], [94mLoss[0m : 8.10325
[1mStep[0m  [40/84], [94mLoss[0m : 8.45650
[1mStep[0m  [48/84], [94mLoss[0m : 8.35037
[1mStep[0m  [56/84], [94mLoss[0m : 8.76543
[1mStep[0m  [64/84], [94mLoss[0m : 8.66844
[1mStep[0m  [72/84], [94mLoss[0m : 7.97245
[1mStep[0m  [80/84], [94mLoss[0m : 8.49555

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 8.550, [92mTest[0m: 8.111, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.03277
[1mStep[0m  [8/84], [94mLoss[0m : 7.98390
[1mStep[0m  [16/84], [94mLoss[0m : 8.23589
[1mStep[0m  [24/84], [94mLoss[0m : 8.52308
[1mStep[0m  [32/84], [94mLoss[0m : 8.04340
[1mStep[0m  [40/84], [94mLoss[0m : 7.90983
[1mStep[0m  [48/84], [94mLoss[0m : 7.63096
[1mStep[0m  [56/84], [94mLoss[0m : 7.57431
[1mStep[0m  [64/84], [94mLoss[0m : 8.48405
[1mStep[0m  [72/84], [94mLoss[0m : 8.08884
[1mStep[0m  [80/84], [94mLoss[0m : 7.89787

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 8.008, [92mTest[0m: 7.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.79057
[1mStep[0m  [8/84], [94mLoss[0m : 7.06862
[1mStep[0m  [16/84], [94mLoss[0m : 7.68527
[1mStep[0m  [24/84], [94mLoss[0m : 7.85495
[1mStep[0m  [32/84], [94mLoss[0m : 7.15259
[1mStep[0m  [40/84], [94mLoss[0m : 6.69865
[1mStep[0m  [48/84], [94mLoss[0m : 7.83794
[1mStep[0m  [56/84], [94mLoss[0m : 7.36431
[1mStep[0m  [64/84], [94mLoss[0m : 6.57419
[1mStep[0m  [72/84], [94mLoss[0m : 7.14245
[1mStep[0m  [80/84], [94mLoss[0m : 6.93951

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 7.439, [92mTest[0m: 7.002, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.13164
[1mStep[0m  [8/84], [94mLoss[0m : 6.57194
[1mStep[0m  [16/84], [94mLoss[0m : 6.63202
[1mStep[0m  [24/84], [94mLoss[0m : 6.98301
[1mStep[0m  [32/84], [94mLoss[0m : 7.04984
[1mStep[0m  [40/84], [94mLoss[0m : 7.46724
[1mStep[0m  [48/84], [94mLoss[0m : 7.08223
[1mStep[0m  [56/84], [94mLoss[0m : 7.03727
[1mStep[0m  [64/84], [94mLoss[0m : 7.00377
[1mStep[0m  [72/84], [94mLoss[0m : 6.75671
[1mStep[0m  [80/84], [94mLoss[0m : 6.76233

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.905, [92mTest[0m: 6.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.61994
[1mStep[0m  [8/84], [94mLoss[0m : 6.90878
[1mStep[0m  [16/84], [94mLoss[0m : 6.67353
[1mStep[0m  [24/84], [94mLoss[0m : 6.15364
[1mStep[0m  [32/84], [94mLoss[0m : 6.18533
[1mStep[0m  [40/84], [94mLoss[0m : 6.93032
[1mStep[0m  [48/84], [94mLoss[0m : 6.82536
[1mStep[0m  [56/84], [94mLoss[0m : 6.46897
[1mStep[0m  [64/84], [94mLoss[0m : 6.42442
[1mStep[0m  [72/84], [94mLoss[0m : 6.29602
[1mStep[0m  [80/84], [94mLoss[0m : 6.46047

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 6.398, [92mTest[0m: 5.873, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.96517
[1mStep[0m  [8/84], [94mLoss[0m : 5.96831
[1mStep[0m  [16/84], [94mLoss[0m : 5.37112
[1mStep[0m  [24/84], [94mLoss[0m : 5.84109
[1mStep[0m  [32/84], [94mLoss[0m : 5.72417
[1mStep[0m  [40/84], [94mLoss[0m : 6.45108
[1mStep[0m  [48/84], [94mLoss[0m : 5.77269
[1mStep[0m  [56/84], [94mLoss[0m : 5.90317
[1mStep[0m  [64/84], [94mLoss[0m : 5.71770
[1mStep[0m  [72/84], [94mLoss[0m : 5.66199
[1mStep[0m  [80/84], [94mLoss[0m : 5.83073

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.897, [92mTest[0m: 5.471, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.66164
[1mStep[0m  [8/84], [94mLoss[0m : 5.91315
[1mStep[0m  [16/84], [94mLoss[0m : 5.99495
[1mStep[0m  [24/84], [94mLoss[0m : 5.36656
[1mStep[0m  [32/84], [94mLoss[0m : 5.04295
[1mStep[0m  [40/84], [94mLoss[0m : 5.31354
[1mStep[0m  [48/84], [94mLoss[0m : 5.13801
[1mStep[0m  [56/84], [94mLoss[0m : 5.07124
[1mStep[0m  [64/84], [94mLoss[0m : 4.76161
[1mStep[0m  [72/84], [94mLoss[0m : 4.85079
[1mStep[0m  [80/84], [94mLoss[0m : 4.95225

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 5.352, [92mTest[0m: 4.675, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.05807
[1mStep[0m  [8/84], [94mLoss[0m : 5.73272
[1mStep[0m  [16/84], [94mLoss[0m : 4.65840
[1mStep[0m  [24/84], [94mLoss[0m : 4.72371
[1mStep[0m  [32/84], [94mLoss[0m : 4.86719
[1mStep[0m  [40/84], [94mLoss[0m : 4.92478
[1mStep[0m  [48/84], [94mLoss[0m : 4.89133
[1mStep[0m  [56/84], [94mLoss[0m : 4.92113
[1mStep[0m  [64/84], [94mLoss[0m : 4.60441
[1mStep[0m  [72/84], [94mLoss[0m : 4.44144
[1mStep[0m  [80/84], [94mLoss[0m : 4.72888

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.807, [92mTest[0m: 4.177, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.10075
[1mStep[0m  [8/84], [94mLoss[0m : 4.02969
[1mStep[0m  [16/84], [94mLoss[0m : 4.60046
[1mStep[0m  [24/84], [94mLoss[0m : 4.32977
[1mStep[0m  [32/84], [94mLoss[0m : 4.16224
[1mStep[0m  [40/84], [94mLoss[0m : 4.06164
[1mStep[0m  [48/84], [94mLoss[0m : 4.35093
[1mStep[0m  [56/84], [94mLoss[0m : 3.75166
[1mStep[0m  [64/84], [94mLoss[0m : 4.09678
[1mStep[0m  [72/84], [94mLoss[0m : 3.79796
[1mStep[0m  [80/84], [94mLoss[0m : 4.75215

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 4.255, [92mTest[0m: 3.618, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.18718
[1mStep[0m  [8/84], [94mLoss[0m : 3.43266
[1mStep[0m  [16/84], [94mLoss[0m : 3.68531
[1mStep[0m  [24/84], [94mLoss[0m : 3.90237
[1mStep[0m  [32/84], [94mLoss[0m : 3.60792
[1mStep[0m  [40/84], [94mLoss[0m : 4.04858
[1mStep[0m  [48/84], [94mLoss[0m : 4.11929
[1mStep[0m  [56/84], [94mLoss[0m : 3.31633
[1mStep[0m  [64/84], [94mLoss[0m : 3.72935
[1mStep[0m  [72/84], [94mLoss[0m : 3.43585
[1mStep[0m  [80/84], [94mLoss[0m : 3.60860

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.750, [92mTest[0m: 3.295, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.56858
[1mStep[0m  [8/84], [94mLoss[0m : 3.68141
[1mStep[0m  [16/84], [94mLoss[0m : 3.31054
[1mStep[0m  [24/84], [94mLoss[0m : 3.29432
[1mStep[0m  [32/84], [94mLoss[0m : 3.64215
[1mStep[0m  [40/84], [94mLoss[0m : 3.29667
[1mStep[0m  [48/84], [94mLoss[0m : 3.00806
[1mStep[0m  [56/84], [94mLoss[0m : 3.16937
[1mStep[0m  [64/84], [94mLoss[0m : 3.04223
[1mStep[0m  [72/84], [94mLoss[0m : 3.42369
[1mStep[0m  [80/84], [94mLoss[0m : 3.25133

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 3.359, [92mTest[0m: 2.946, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.36561
[1mStep[0m  [8/84], [94mLoss[0m : 3.06027
[1mStep[0m  [16/84], [94mLoss[0m : 3.24810
[1mStep[0m  [24/84], [94mLoss[0m : 3.47656
[1mStep[0m  [32/84], [94mLoss[0m : 3.04716
[1mStep[0m  [40/84], [94mLoss[0m : 3.01419
[1mStep[0m  [48/84], [94mLoss[0m : 3.16843
[1mStep[0m  [56/84], [94mLoss[0m : 2.73023
[1mStep[0m  [64/84], [94mLoss[0m : 2.62244
[1mStep[0m  [72/84], [94mLoss[0m : 2.61405
[1mStep[0m  [80/84], [94mLoss[0m : 3.22707

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 3.062, [92mTest[0m: 2.670, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66457
[1mStep[0m  [8/84], [94mLoss[0m : 2.76679
[1mStep[0m  [16/84], [94mLoss[0m : 2.88929
[1mStep[0m  [24/84], [94mLoss[0m : 3.62788
[1mStep[0m  [32/84], [94mLoss[0m : 2.69202
[1mStep[0m  [40/84], [94mLoss[0m : 2.65373
[1mStep[0m  [48/84], [94mLoss[0m : 3.14077
[1mStep[0m  [56/84], [94mLoss[0m : 3.00361
[1mStep[0m  [64/84], [94mLoss[0m : 2.90445
[1mStep[0m  [72/84], [94mLoss[0m : 3.39027
[1mStep[0m  [80/84], [94mLoss[0m : 2.91272

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.870, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89685
[1mStep[0m  [8/84], [94mLoss[0m : 2.63699
[1mStep[0m  [16/84], [94mLoss[0m : 2.70269
[1mStep[0m  [24/84], [94mLoss[0m : 2.87693
[1mStep[0m  [32/84], [94mLoss[0m : 2.75439
[1mStep[0m  [40/84], [94mLoss[0m : 2.80125
[1mStep[0m  [48/84], [94mLoss[0m : 2.83512
[1mStep[0m  [56/84], [94mLoss[0m : 2.49253
[1mStep[0m  [64/84], [94mLoss[0m : 2.59188
[1mStep[0m  [72/84], [94mLoss[0m : 3.06085
[1mStep[0m  [80/84], [94mLoss[0m : 2.75404

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.752, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52116
[1mStep[0m  [8/84], [94mLoss[0m : 2.95378
[1mStep[0m  [16/84], [94mLoss[0m : 3.07942
[1mStep[0m  [24/84], [94mLoss[0m : 2.60707
[1mStep[0m  [32/84], [94mLoss[0m : 2.92011
[1mStep[0m  [40/84], [94mLoss[0m : 2.94244
[1mStep[0m  [48/84], [94mLoss[0m : 2.92031
[1mStep[0m  [56/84], [94mLoss[0m : 2.80045
[1mStep[0m  [64/84], [94mLoss[0m : 2.76314
[1mStep[0m  [72/84], [94mLoss[0m : 2.95215
[1mStep[0m  [80/84], [94mLoss[0m : 2.94225

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.724, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50750
[1mStep[0m  [8/84], [94mLoss[0m : 2.73962
[1mStep[0m  [16/84], [94mLoss[0m : 2.82584
[1mStep[0m  [24/84], [94mLoss[0m : 2.89130
[1mStep[0m  [32/84], [94mLoss[0m : 2.79258
[1mStep[0m  [40/84], [94mLoss[0m : 2.69307
[1mStep[0m  [48/84], [94mLoss[0m : 2.51170
[1mStep[0m  [56/84], [94mLoss[0m : 2.81005
[1mStep[0m  [64/84], [94mLoss[0m : 2.69411
[1mStep[0m  [72/84], [94mLoss[0m : 2.93535
[1mStep[0m  [80/84], [94mLoss[0m : 2.83460

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69266
[1mStep[0m  [8/84], [94mLoss[0m : 2.71540
[1mStep[0m  [16/84], [94mLoss[0m : 2.90202
[1mStep[0m  [24/84], [94mLoss[0m : 2.73922
[1mStep[0m  [32/84], [94mLoss[0m : 2.72331
[1mStep[0m  [40/84], [94mLoss[0m : 2.45053
[1mStep[0m  [48/84], [94mLoss[0m : 3.00723
[1mStep[0m  [56/84], [94mLoss[0m : 2.59615
[1mStep[0m  [64/84], [94mLoss[0m : 2.59105
[1mStep[0m  [72/84], [94mLoss[0m : 2.40590
[1mStep[0m  [80/84], [94mLoss[0m : 2.79828

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75165
[1mStep[0m  [8/84], [94mLoss[0m : 2.46417
[1mStep[0m  [16/84], [94mLoss[0m : 2.96718
[1mStep[0m  [24/84], [94mLoss[0m : 2.53882
[1mStep[0m  [32/84], [94mLoss[0m : 2.47980
[1mStep[0m  [40/84], [94mLoss[0m : 2.76520
[1mStep[0m  [48/84], [94mLoss[0m : 2.63172
[1mStep[0m  [56/84], [94mLoss[0m : 2.67603
[1mStep[0m  [64/84], [94mLoss[0m : 2.62146
[1mStep[0m  [72/84], [94mLoss[0m : 3.01296
[1mStep[0m  [80/84], [94mLoss[0m : 2.65698

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.661, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36300
[1mStep[0m  [8/84], [94mLoss[0m : 2.78853
[1mStep[0m  [16/84], [94mLoss[0m : 2.62808
[1mStep[0m  [24/84], [94mLoss[0m : 2.66614
[1mStep[0m  [32/84], [94mLoss[0m : 2.33866
[1mStep[0m  [40/84], [94mLoss[0m : 2.91103
[1mStep[0m  [48/84], [94mLoss[0m : 2.96465
[1mStep[0m  [56/84], [94mLoss[0m : 2.42615
[1mStep[0m  [64/84], [94mLoss[0m : 2.72272
[1mStep[0m  [72/84], [94mLoss[0m : 2.97364
[1mStep[0m  [80/84], [94mLoss[0m : 2.72220

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.680, [92mTest[0m: 2.376, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42352
[1mStep[0m  [8/84], [94mLoss[0m : 2.49212
[1mStep[0m  [16/84], [94mLoss[0m : 2.63428
[1mStep[0m  [24/84], [94mLoss[0m : 2.52682
[1mStep[0m  [32/84], [94mLoss[0m : 2.83356
[1mStep[0m  [40/84], [94mLoss[0m : 2.57314
[1mStep[0m  [48/84], [94mLoss[0m : 3.08059
[1mStep[0m  [56/84], [94mLoss[0m : 2.55545
[1mStep[0m  [64/84], [94mLoss[0m : 2.57166
[1mStep[0m  [72/84], [94mLoss[0m : 3.11743
[1mStep[0m  [80/84], [94mLoss[0m : 2.81118

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81091
[1mStep[0m  [8/84], [94mLoss[0m : 2.54902
[1mStep[0m  [16/84], [94mLoss[0m : 2.74607
[1mStep[0m  [24/84], [94mLoss[0m : 2.60705
[1mStep[0m  [32/84], [94mLoss[0m : 2.55549
[1mStep[0m  [40/84], [94mLoss[0m : 2.52748
[1mStep[0m  [48/84], [94mLoss[0m : 2.55052
[1mStep[0m  [56/84], [94mLoss[0m : 2.44018
[1mStep[0m  [64/84], [94mLoss[0m : 2.53188
[1mStep[0m  [72/84], [94mLoss[0m : 2.57581
[1mStep[0m  [80/84], [94mLoss[0m : 2.75456

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67227
[1mStep[0m  [8/84], [94mLoss[0m : 2.64636
[1mStep[0m  [16/84], [94mLoss[0m : 2.82958
[1mStep[0m  [24/84], [94mLoss[0m : 2.63334
[1mStep[0m  [32/84], [94mLoss[0m : 2.49919
[1mStep[0m  [40/84], [94mLoss[0m : 2.36808
[1mStep[0m  [48/84], [94mLoss[0m : 2.57016
[1mStep[0m  [56/84], [94mLoss[0m : 2.69705
[1mStep[0m  [64/84], [94mLoss[0m : 2.54488
[1mStep[0m  [72/84], [94mLoss[0m : 2.28844
[1mStep[0m  [80/84], [94mLoss[0m : 2.35927

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.635, [92mTest[0m: 2.365, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50938
[1mStep[0m  [8/84], [94mLoss[0m : 2.45338
[1mStep[0m  [16/84], [94mLoss[0m : 2.71830
[1mStep[0m  [24/84], [94mLoss[0m : 2.59821
[1mStep[0m  [32/84], [94mLoss[0m : 2.32871
[1mStep[0m  [40/84], [94mLoss[0m : 2.58919
[1mStep[0m  [48/84], [94mLoss[0m : 2.70509
[1mStep[0m  [56/84], [94mLoss[0m : 2.28050
[1mStep[0m  [64/84], [94mLoss[0m : 2.51245
[1mStep[0m  [72/84], [94mLoss[0m : 2.63449
[1mStep[0m  [80/84], [94mLoss[0m : 2.55859

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.376, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26778
[1mStep[0m  [8/84], [94mLoss[0m : 2.83698
[1mStep[0m  [16/84], [94mLoss[0m : 2.64979
[1mStep[0m  [24/84], [94mLoss[0m : 2.58532
[1mStep[0m  [32/84], [94mLoss[0m : 2.61208
[1mStep[0m  [40/84], [94mLoss[0m : 2.91084
[1mStep[0m  [48/84], [94mLoss[0m : 2.56157
[1mStep[0m  [56/84], [94mLoss[0m : 2.50265
[1mStep[0m  [64/84], [94mLoss[0m : 2.49260
[1mStep[0m  [72/84], [94mLoss[0m : 2.50180
[1mStep[0m  [80/84], [94mLoss[0m : 2.52548

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.613, [92mTest[0m: 2.363, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.378
====================================

Phase 1 - Evaluation MAE:  2.3776828902108327
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.50867
[1mStep[0m  [8/84], [94mLoss[0m : 2.75642
[1mStep[0m  [16/84], [94mLoss[0m : 2.49642
[1mStep[0m  [24/84], [94mLoss[0m : 2.79664
[1mStep[0m  [32/84], [94mLoss[0m : 2.63707
[1mStep[0m  [40/84], [94mLoss[0m : 2.68692
[1mStep[0m  [48/84], [94mLoss[0m : 2.61853
[1mStep[0m  [56/84], [94mLoss[0m : 2.84699
[1mStep[0m  [64/84], [94mLoss[0m : 2.56699
[1mStep[0m  [72/84], [94mLoss[0m : 2.83726
[1mStep[0m  [80/84], [94mLoss[0m : 2.72497

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40474
[1mStep[0m  [8/84], [94mLoss[0m : 2.50272
[1mStep[0m  [16/84], [94mLoss[0m : 2.92067
[1mStep[0m  [24/84], [94mLoss[0m : 2.79491
[1mStep[0m  [32/84], [94mLoss[0m : 2.60919
[1mStep[0m  [40/84], [94mLoss[0m : 2.80773
[1mStep[0m  [48/84], [94mLoss[0m : 2.62166
[1mStep[0m  [56/84], [94mLoss[0m : 2.47858
[1mStep[0m  [64/84], [94mLoss[0m : 2.74802
[1mStep[0m  [72/84], [94mLoss[0m : 2.74724
[1mStep[0m  [80/84], [94mLoss[0m : 2.62028

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.657, [92mTest[0m: 2.489, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.94663
[1mStep[0m  [8/84], [94mLoss[0m : 2.53059
[1mStep[0m  [16/84], [94mLoss[0m : 2.37561
[1mStep[0m  [24/84], [94mLoss[0m : 2.87995
[1mStep[0m  [32/84], [94mLoss[0m : 2.46633
[1mStep[0m  [40/84], [94mLoss[0m : 2.75312
[1mStep[0m  [48/84], [94mLoss[0m : 2.54915
[1mStep[0m  [56/84], [94mLoss[0m : 2.22339
[1mStep[0m  [64/84], [94mLoss[0m : 2.70698
[1mStep[0m  [72/84], [94mLoss[0m : 2.66688
[1mStep[0m  [80/84], [94mLoss[0m : 2.49590

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.475, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74590
[1mStep[0m  [8/84], [94mLoss[0m : 2.91300
[1mStep[0m  [16/84], [94mLoss[0m : 2.90103
[1mStep[0m  [24/84], [94mLoss[0m : 2.44540
[1mStep[0m  [32/84], [94mLoss[0m : 2.63073
[1mStep[0m  [40/84], [94mLoss[0m : 2.80390
[1mStep[0m  [48/84], [94mLoss[0m : 2.42664
[1mStep[0m  [56/84], [94mLoss[0m : 2.52548
[1mStep[0m  [64/84], [94mLoss[0m : 2.57269
[1mStep[0m  [72/84], [94mLoss[0m : 2.69524
[1mStep[0m  [80/84], [94mLoss[0m : 2.47423

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60261
[1mStep[0m  [8/84], [94mLoss[0m : 2.68445
[1mStep[0m  [16/84], [94mLoss[0m : 2.62261
[1mStep[0m  [24/84], [94mLoss[0m : 2.49196
[1mStep[0m  [32/84], [94mLoss[0m : 2.34840
[1mStep[0m  [40/84], [94mLoss[0m : 2.58283
[1mStep[0m  [48/84], [94mLoss[0m : 2.73796
[1mStep[0m  [56/84], [94mLoss[0m : 2.52769
[1mStep[0m  [64/84], [94mLoss[0m : 2.63772
[1mStep[0m  [72/84], [94mLoss[0m : 2.37932
[1mStep[0m  [80/84], [94mLoss[0m : 2.43565

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74534
[1mStep[0m  [8/84], [94mLoss[0m : 2.64714
[1mStep[0m  [16/84], [94mLoss[0m : 2.43189
[1mStep[0m  [24/84], [94mLoss[0m : 2.81190
[1mStep[0m  [32/84], [94mLoss[0m : 2.40451
[1mStep[0m  [40/84], [94mLoss[0m : 2.59184
[1mStep[0m  [48/84], [94mLoss[0m : 2.44059
[1mStep[0m  [56/84], [94mLoss[0m : 2.09158
[1mStep[0m  [64/84], [94mLoss[0m : 2.44885
[1mStep[0m  [72/84], [94mLoss[0m : 2.61206
[1mStep[0m  [80/84], [94mLoss[0m : 2.61152

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.477, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71840
[1mStep[0m  [8/84], [94mLoss[0m : 2.61004
[1mStep[0m  [16/84], [94mLoss[0m : 2.65018
[1mStep[0m  [24/84], [94mLoss[0m : 2.88786
[1mStep[0m  [32/84], [94mLoss[0m : 2.57789
[1mStep[0m  [40/84], [94mLoss[0m : 2.39134
[1mStep[0m  [48/84], [94mLoss[0m : 2.33310
[1mStep[0m  [56/84], [94mLoss[0m : 2.47059
[1mStep[0m  [64/84], [94mLoss[0m : 2.42594
[1mStep[0m  [72/84], [94mLoss[0m : 2.23149
[1mStep[0m  [80/84], [94mLoss[0m : 2.40065

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24060
[1mStep[0m  [8/84], [94mLoss[0m : 2.28822
[1mStep[0m  [16/84], [94mLoss[0m : 2.45072
[1mStep[0m  [24/84], [94mLoss[0m : 2.39526
[1mStep[0m  [32/84], [94mLoss[0m : 2.34838
[1mStep[0m  [40/84], [94mLoss[0m : 2.17602
[1mStep[0m  [48/84], [94mLoss[0m : 2.56148
[1mStep[0m  [56/84], [94mLoss[0m : 2.28708
[1mStep[0m  [64/84], [94mLoss[0m : 2.78319
[1mStep[0m  [72/84], [94mLoss[0m : 2.41737
[1mStep[0m  [80/84], [94mLoss[0m : 2.29372

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35619
[1mStep[0m  [8/84], [94mLoss[0m : 2.50772
[1mStep[0m  [16/84], [94mLoss[0m : 2.43035
[1mStep[0m  [24/84], [94mLoss[0m : 2.30061
[1mStep[0m  [32/84], [94mLoss[0m : 2.43148
[1mStep[0m  [40/84], [94mLoss[0m : 2.27501
[1mStep[0m  [48/84], [94mLoss[0m : 2.47608
[1mStep[0m  [56/84], [94mLoss[0m : 2.39695
[1mStep[0m  [64/84], [94mLoss[0m : 2.76117
[1mStep[0m  [72/84], [94mLoss[0m : 2.74369
[1mStep[0m  [80/84], [94mLoss[0m : 2.83456

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53159
[1mStep[0m  [8/84], [94mLoss[0m : 2.44420
[1mStep[0m  [16/84], [94mLoss[0m : 2.37407
[1mStep[0m  [24/84], [94mLoss[0m : 2.48279
[1mStep[0m  [32/84], [94mLoss[0m : 2.44695
[1mStep[0m  [40/84], [94mLoss[0m : 2.75944
[1mStep[0m  [48/84], [94mLoss[0m : 2.59720
[1mStep[0m  [56/84], [94mLoss[0m : 2.43333
[1mStep[0m  [64/84], [94mLoss[0m : 2.55512
[1mStep[0m  [72/84], [94mLoss[0m : 2.89680
[1mStep[0m  [80/84], [94mLoss[0m : 2.45352

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.491, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56354
[1mStep[0m  [8/84], [94mLoss[0m : 2.38986
[1mStep[0m  [16/84], [94mLoss[0m : 2.37667
[1mStep[0m  [24/84], [94mLoss[0m : 2.36176
[1mStep[0m  [32/84], [94mLoss[0m : 2.23249
[1mStep[0m  [40/84], [94mLoss[0m : 2.40516
[1mStep[0m  [48/84], [94mLoss[0m : 2.66254
[1mStep[0m  [56/84], [94mLoss[0m : 2.56552
[1mStep[0m  [64/84], [94mLoss[0m : 2.54656
[1mStep[0m  [72/84], [94mLoss[0m : 2.70407
[1mStep[0m  [80/84], [94mLoss[0m : 2.34574

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.526, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.14556
[1mStep[0m  [8/84], [94mLoss[0m : 2.79972
[1mStep[0m  [16/84], [94mLoss[0m : 2.45140
[1mStep[0m  [24/84], [94mLoss[0m : 2.31430
[1mStep[0m  [32/84], [94mLoss[0m : 2.66041
[1mStep[0m  [40/84], [94mLoss[0m : 2.43321
[1mStep[0m  [48/84], [94mLoss[0m : 2.37171
[1mStep[0m  [56/84], [94mLoss[0m : 2.50844
[1mStep[0m  [64/84], [94mLoss[0m : 2.31009
[1mStep[0m  [72/84], [94mLoss[0m : 2.59202
[1mStep[0m  [80/84], [94mLoss[0m : 2.55271

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28389
[1mStep[0m  [8/84], [94mLoss[0m : 2.57660
[1mStep[0m  [16/84], [94mLoss[0m : 2.64589
[1mStep[0m  [24/84], [94mLoss[0m : 2.28727
[1mStep[0m  [32/84], [94mLoss[0m : 2.41279
[1mStep[0m  [40/84], [94mLoss[0m : 2.25743
[1mStep[0m  [48/84], [94mLoss[0m : 2.25794
[1mStep[0m  [56/84], [94mLoss[0m : 2.29615
[1mStep[0m  [64/84], [94mLoss[0m : 2.36889
[1mStep[0m  [72/84], [94mLoss[0m : 2.24338
[1mStep[0m  [80/84], [94mLoss[0m : 2.58322

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29425
[1mStep[0m  [8/84], [94mLoss[0m : 2.65846
[1mStep[0m  [16/84], [94mLoss[0m : 2.31837
[1mStep[0m  [24/84], [94mLoss[0m : 2.42232
[1mStep[0m  [32/84], [94mLoss[0m : 2.10087
[1mStep[0m  [40/84], [94mLoss[0m : 2.34548
[1mStep[0m  [48/84], [94mLoss[0m : 2.47837
[1mStep[0m  [56/84], [94mLoss[0m : 2.45902
[1mStep[0m  [64/84], [94mLoss[0m : 2.27734
[1mStep[0m  [72/84], [94mLoss[0m : 2.49196
[1mStep[0m  [80/84], [94mLoss[0m : 2.26880

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.551, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26162
[1mStep[0m  [8/84], [94mLoss[0m : 1.86539
[1mStep[0m  [16/84], [94mLoss[0m : 2.41965
[1mStep[0m  [24/84], [94mLoss[0m : 2.09542
[1mStep[0m  [32/84], [94mLoss[0m : 2.21082
[1mStep[0m  [40/84], [94mLoss[0m : 1.95876
[1mStep[0m  [48/84], [94mLoss[0m : 2.39107
[1mStep[0m  [56/84], [94mLoss[0m : 2.45378
[1mStep[0m  [64/84], [94mLoss[0m : 2.18859
[1mStep[0m  [72/84], [94mLoss[0m : 2.43407
[1mStep[0m  [80/84], [94mLoss[0m : 2.39266

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40664
[1mStep[0m  [8/84], [94mLoss[0m : 2.24297
[1mStep[0m  [16/84], [94mLoss[0m : 2.35981
[1mStep[0m  [24/84], [94mLoss[0m : 2.48701
[1mStep[0m  [32/84], [94mLoss[0m : 2.39559
[1mStep[0m  [40/84], [94mLoss[0m : 2.31778
[1mStep[0m  [48/84], [94mLoss[0m : 2.10404
[1mStep[0m  [56/84], [94mLoss[0m : 2.25891
[1mStep[0m  [64/84], [94mLoss[0m : 2.09747
[1mStep[0m  [72/84], [94mLoss[0m : 2.26765
[1mStep[0m  [80/84], [94mLoss[0m : 2.21749

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.309, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47853
[1mStep[0m  [8/84], [94mLoss[0m : 2.20244
[1mStep[0m  [16/84], [94mLoss[0m : 2.22495
[1mStep[0m  [24/84], [94mLoss[0m : 2.19989
[1mStep[0m  [32/84], [94mLoss[0m : 2.13452
[1mStep[0m  [40/84], [94mLoss[0m : 2.31197
[1mStep[0m  [48/84], [94mLoss[0m : 2.19961
[1mStep[0m  [56/84], [94mLoss[0m : 2.23534
[1mStep[0m  [64/84], [94mLoss[0m : 2.31755
[1mStep[0m  [72/84], [94mLoss[0m : 2.38487
[1mStep[0m  [80/84], [94mLoss[0m : 2.01993

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31234
[1mStep[0m  [8/84], [94mLoss[0m : 2.38210
[1mStep[0m  [16/84], [94mLoss[0m : 2.18606
[1mStep[0m  [24/84], [94mLoss[0m : 2.18658
[1mStep[0m  [32/84], [94mLoss[0m : 2.08246
[1mStep[0m  [40/84], [94mLoss[0m : 2.26722
[1mStep[0m  [48/84], [94mLoss[0m : 2.21244
[1mStep[0m  [56/84], [94mLoss[0m : 2.23178
[1mStep[0m  [64/84], [94mLoss[0m : 2.18326
[1mStep[0m  [72/84], [94mLoss[0m : 2.18799
[1mStep[0m  [80/84], [94mLoss[0m : 2.43196

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.15970
[1mStep[0m  [8/84], [94mLoss[0m : 2.20598
[1mStep[0m  [16/84], [94mLoss[0m : 2.17783
[1mStep[0m  [24/84], [94mLoss[0m : 2.17044
[1mStep[0m  [32/84], [94mLoss[0m : 2.42683
[1mStep[0m  [40/84], [94mLoss[0m : 2.27272
[1mStep[0m  [48/84], [94mLoss[0m : 2.05781
[1mStep[0m  [56/84], [94mLoss[0m : 2.24103
[1mStep[0m  [64/84], [94mLoss[0m : 2.07568
[1mStep[0m  [72/84], [94mLoss[0m : 2.12992
[1mStep[0m  [80/84], [94mLoss[0m : 2.14373

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.612, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32898
[1mStep[0m  [8/84], [94mLoss[0m : 2.19047
[1mStep[0m  [16/84], [94mLoss[0m : 2.42927
[1mStep[0m  [24/84], [94mLoss[0m : 2.19664
[1mStep[0m  [32/84], [94mLoss[0m : 2.25646
[1mStep[0m  [40/84], [94mLoss[0m : 2.35599
[1mStep[0m  [48/84], [94mLoss[0m : 2.12650
[1mStep[0m  [56/84], [94mLoss[0m : 2.32220
[1mStep[0m  [64/84], [94mLoss[0m : 2.05944
[1mStep[0m  [72/84], [94mLoss[0m : 2.16434
[1mStep[0m  [80/84], [94mLoss[0m : 1.99582

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19066
[1mStep[0m  [8/84], [94mLoss[0m : 1.98764
[1mStep[0m  [16/84], [94mLoss[0m : 2.30030
[1mStep[0m  [24/84], [94mLoss[0m : 2.23102
[1mStep[0m  [32/84], [94mLoss[0m : 2.05455
[1mStep[0m  [40/84], [94mLoss[0m : 2.03097
[1mStep[0m  [48/84], [94mLoss[0m : 2.14742
[1mStep[0m  [56/84], [94mLoss[0m : 2.09768
[1mStep[0m  [64/84], [94mLoss[0m : 2.23152
[1mStep[0m  [72/84], [94mLoss[0m : 2.15695
[1mStep[0m  [80/84], [94mLoss[0m : 2.08253

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08620
[1mStep[0m  [8/84], [94mLoss[0m : 2.20812
[1mStep[0m  [16/84], [94mLoss[0m : 2.30282
[1mStep[0m  [24/84], [94mLoss[0m : 1.98986
[1mStep[0m  [32/84], [94mLoss[0m : 2.10310
[1mStep[0m  [40/84], [94mLoss[0m : 1.85344
[1mStep[0m  [48/84], [94mLoss[0m : 2.06054
[1mStep[0m  [56/84], [94mLoss[0m : 2.01645
[1mStep[0m  [64/84], [94mLoss[0m : 2.29964
[1mStep[0m  [72/84], [94mLoss[0m : 2.35139
[1mStep[0m  [80/84], [94mLoss[0m : 2.19682

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.184, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.19663
[1mStep[0m  [8/84], [94mLoss[0m : 2.32068
[1mStep[0m  [16/84], [94mLoss[0m : 2.03952
[1mStep[0m  [24/84], [94mLoss[0m : 2.13716
[1mStep[0m  [32/84], [94mLoss[0m : 2.37885
[1mStep[0m  [40/84], [94mLoss[0m : 2.08187
[1mStep[0m  [48/84], [94mLoss[0m : 2.04780
[1mStep[0m  [56/84], [94mLoss[0m : 2.33201
[1mStep[0m  [64/84], [94mLoss[0m : 2.13376
[1mStep[0m  [72/84], [94mLoss[0m : 2.11343
[1mStep[0m  [80/84], [94mLoss[0m : 1.94253

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.184, [92mTest[0m: 2.514, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24853
[1mStep[0m  [8/84], [94mLoss[0m : 1.95384
[1mStep[0m  [16/84], [94mLoss[0m : 2.16550
[1mStep[0m  [24/84], [94mLoss[0m : 2.16772
[1mStep[0m  [32/84], [94mLoss[0m : 2.09603
[1mStep[0m  [40/84], [94mLoss[0m : 2.21279
[1mStep[0m  [48/84], [94mLoss[0m : 1.96728
[1mStep[0m  [56/84], [94mLoss[0m : 2.19877
[1mStep[0m  [64/84], [94mLoss[0m : 2.36815
[1mStep[0m  [72/84], [94mLoss[0m : 2.05713
[1mStep[0m  [80/84], [94mLoss[0m : 2.20262

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20357
[1mStep[0m  [8/84], [94mLoss[0m : 1.96909
[1mStep[0m  [16/84], [94mLoss[0m : 2.00597
[1mStep[0m  [24/84], [94mLoss[0m : 2.05745
[1mStep[0m  [32/84], [94mLoss[0m : 1.86049
[1mStep[0m  [40/84], [94mLoss[0m : 1.94222
[1mStep[0m  [48/84], [94mLoss[0m : 2.04848
[1mStep[0m  [56/84], [94mLoss[0m : 1.78423
[1mStep[0m  [64/84], [94mLoss[0m : 2.22549
[1mStep[0m  [72/84], [94mLoss[0m : 2.23773
[1mStep[0m  [80/84], [94mLoss[0m : 1.81682

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29653
[1mStep[0m  [8/84], [94mLoss[0m : 2.10379
[1mStep[0m  [16/84], [94mLoss[0m : 2.24985
[1mStep[0m  [24/84], [94mLoss[0m : 2.16206
[1mStep[0m  [32/84], [94mLoss[0m : 2.17786
[1mStep[0m  [40/84], [94mLoss[0m : 2.03920
[1mStep[0m  [48/84], [94mLoss[0m : 1.98176
[1mStep[0m  [56/84], [94mLoss[0m : 2.26045
[1mStep[0m  [64/84], [94mLoss[0m : 2.21240
[1mStep[0m  [72/84], [94mLoss[0m : 1.94097
[1mStep[0m  [80/84], [94mLoss[0m : 2.17334

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.091, [92mTest[0m: 2.536, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22333
[1mStep[0m  [8/84], [94mLoss[0m : 1.91944
[1mStep[0m  [16/84], [94mLoss[0m : 1.96168
[1mStep[0m  [24/84], [94mLoss[0m : 2.05541
[1mStep[0m  [32/84], [94mLoss[0m : 2.11340
[1mStep[0m  [40/84], [94mLoss[0m : 2.22407
[1mStep[0m  [48/84], [94mLoss[0m : 1.96394
[1mStep[0m  [56/84], [94mLoss[0m : 1.97365
[1mStep[0m  [64/84], [94mLoss[0m : 1.94536
[1mStep[0m  [72/84], [94mLoss[0m : 2.08775
[1mStep[0m  [80/84], [94mLoss[0m : 2.27596

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.505, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01085
[1mStep[0m  [8/84], [94mLoss[0m : 1.92282
[1mStep[0m  [16/84], [94mLoss[0m : 2.04056
[1mStep[0m  [24/84], [94mLoss[0m : 2.15938
[1mStep[0m  [32/84], [94mLoss[0m : 2.13755
[1mStep[0m  [40/84], [94mLoss[0m : 2.08795
[1mStep[0m  [48/84], [94mLoss[0m : 1.96836
[1mStep[0m  [56/84], [94mLoss[0m : 1.91391
[1mStep[0m  [64/84], [94mLoss[0m : 2.00335
[1mStep[0m  [72/84], [94mLoss[0m : 2.11889
[1mStep[0m  [80/84], [94mLoss[0m : 1.97495

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.527, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06365
[1mStep[0m  [8/84], [94mLoss[0m : 2.31743
[1mStep[0m  [16/84], [94mLoss[0m : 1.83302
[1mStep[0m  [24/84], [94mLoss[0m : 2.12866
[1mStep[0m  [32/84], [94mLoss[0m : 1.84395
[1mStep[0m  [40/84], [94mLoss[0m : 2.08574
[1mStep[0m  [48/84], [94mLoss[0m : 2.06867
[1mStep[0m  [56/84], [94mLoss[0m : 1.92688
[1mStep[0m  [64/84], [94mLoss[0m : 2.18258
[1mStep[0m  [72/84], [94mLoss[0m : 2.04957
[1mStep[0m  [80/84], [94mLoss[0m : 2.13737

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08045
[1mStep[0m  [8/84], [94mLoss[0m : 1.97768
[1mStep[0m  [16/84], [94mLoss[0m : 2.01984
[1mStep[0m  [24/84], [94mLoss[0m : 1.82780
[1mStep[0m  [32/84], [94mLoss[0m : 2.35610
[1mStep[0m  [40/84], [94mLoss[0m : 1.94049
[1mStep[0m  [48/84], [94mLoss[0m : 1.87935
[1mStep[0m  [56/84], [94mLoss[0m : 2.12616
[1mStep[0m  [64/84], [94mLoss[0m : 1.81700
[1mStep[0m  [72/84], [94mLoss[0m : 1.99542
[1mStep[0m  [80/84], [94mLoss[0m : 2.25324

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.011, [92mTest[0m: 2.583, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.506
====================================

Phase 2 - Evaluation MAE:  2.505790489060538
MAE score P1        2.377683
MAE score P2         2.50579
loss                2.010745
learning_rate       0.002575
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay            0.01
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 10.71405
[1mStep[0m  [8/84], [94mLoss[0m : 10.14276
[1mStep[0m  [16/84], [94mLoss[0m : 8.45189
[1mStep[0m  [24/84], [94mLoss[0m : 7.96459
[1mStep[0m  [32/84], [94mLoss[0m : 7.26668
[1mStep[0m  [40/84], [94mLoss[0m : 5.40301
[1mStep[0m  [48/84], [94mLoss[0m : 4.88484
[1mStep[0m  [56/84], [94mLoss[0m : 3.58074
[1mStep[0m  [64/84], [94mLoss[0m : 3.21682
[1mStep[0m  [72/84], [94mLoss[0m : 3.20901
[1mStep[0m  [80/84], [94mLoss[0m : 3.16088

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.044, [92mTest[0m: 10.899, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.17916
[1mStep[0m  [8/84], [94mLoss[0m : 3.07840
[1mStep[0m  [16/84], [94mLoss[0m : 2.64753
[1mStep[0m  [24/84], [94mLoss[0m : 2.38055
[1mStep[0m  [32/84], [94mLoss[0m : 2.54798
[1mStep[0m  [40/84], [94mLoss[0m : 2.48971
[1mStep[0m  [48/84], [94mLoss[0m : 2.35858
[1mStep[0m  [56/84], [94mLoss[0m : 2.49640
[1mStep[0m  [64/84], [94mLoss[0m : 2.56937
[1mStep[0m  [72/84], [94mLoss[0m : 2.38232
[1mStep[0m  [80/84], [94mLoss[0m : 2.42286

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.923, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56240
[1mStep[0m  [8/84], [94mLoss[0m : 2.45414
[1mStep[0m  [16/84], [94mLoss[0m : 2.56577
[1mStep[0m  [24/84], [94mLoss[0m : 2.52827
[1mStep[0m  [32/84], [94mLoss[0m : 2.85969
[1mStep[0m  [40/84], [94mLoss[0m : 2.56437
[1mStep[0m  [48/84], [94mLoss[0m : 2.52424
[1mStep[0m  [56/84], [94mLoss[0m : 2.42588
[1mStep[0m  [64/84], [94mLoss[0m : 2.47287
[1mStep[0m  [72/84], [94mLoss[0m : 2.27379
[1mStep[0m  [80/84], [94mLoss[0m : 2.45085

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71347
[1mStep[0m  [8/84], [94mLoss[0m : 2.62129
[1mStep[0m  [16/84], [94mLoss[0m : 2.59072
[1mStep[0m  [24/84], [94mLoss[0m : 2.52007
[1mStep[0m  [32/84], [94mLoss[0m : 2.75524
[1mStep[0m  [40/84], [94mLoss[0m : 2.53105
[1mStep[0m  [48/84], [94mLoss[0m : 2.39618
[1mStep[0m  [56/84], [94mLoss[0m : 2.68043
[1mStep[0m  [64/84], [94mLoss[0m : 2.44044
[1mStep[0m  [72/84], [94mLoss[0m : 2.18699
[1mStep[0m  [80/84], [94mLoss[0m : 2.38181

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60152
[1mStep[0m  [8/84], [94mLoss[0m : 2.50978
[1mStep[0m  [16/84], [94mLoss[0m : 2.78702
[1mStep[0m  [24/84], [94mLoss[0m : 2.63731
[1mStep[0m  [32/84], [94mLoss[0m : 2.73696
[1mStep[0m  [40/84], [94mLoss[0m : 2.29871
[1mStep[0m  [48/84], [94mLoss[0m : 2.37577
[1mStep[0m  [56/84], [94mLoss[0m : 2.45127
[1mStep[0m  [64/84], [94mLoss[0m : 2.57083
[1mStep[0m  [72/84], [94mLoss[0m : 2.42113
[1mStep[0m  [80/84], [94mLoss[0m : 2.73848

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35842
[1mStep[0m  [8/84], [94mLoss[0m : 2.51928
[1mStep[0m  [16/84], [94mLoss[0m : 2.27723
[1mStep[0m  [24/84], [94mLoss[0m : 2.39274
[1mStep[0m  [32/84], [94mLoss[0m : 2.35122
[1mStep[0m  [40/84], [94mLoss[0m : 2.44815
[1mStep[0m  [48/84], [94mLoss[0m : 2.46175
[1mStep[0m  [56/84], [94mLoss[0m : 2.61719
[1mStep[0m  [64/84], [94mLoss[0m : 2.13636
[1mStep[0m  [72/84], [94mLoss[0m : 2.54503
[1mStep[0m  [80/84], [94mLoss[0m : 2.34087

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46849
[1mStep[0m  [8/84], [94mLoss[0m : 2.46206
[1mStep[0m  [16/84], [94mLoss[0m : 2.37130
[1mStep[0m  [24/84], [94mLoss[0m : 2.96979
[1mStep[0m  [32/84], [94mLoss[0m : 2.43044
[1mStep[0m  [40/84], [94mLoss[0m : 2.55419
[1mStep[0m  [48/84], [94mLoss[0m : 2.63891
[1mStep[0m  [56/84], [94mLoss[0m : 2.61920
[1mStep[0m  [64/84], [94mLoss[0m : 2.56042
[1mStep[0m  [72/84], [94mLoss[0m : 2.35096
[1mStep[0m  [80/84], [94mLoss[0m : 2.39244

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.384, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40135
[1mStep[0m  [8/84], [94mLoss[0m : 2.54830
[1mStep[0m  [16/84], [94mLoss[0m : 2.36206
[1mStep[0m  [24/84], [94mLoss[0m : 2.81756
[1mStep[0m  [32/84], [94mLoss[0m : 2.62190
[1mStep[0m  [40/84], [94mLoss[0m : 2.64384
[1mStep[0m  [48/84], [94mLoss[0m : 2.59547
[1mStep[0m  [56/84], [94mLoss[0m : 2.23426
[1mStep[0m  [64/84], [94mLoss[0m : 2.39956
[1mStep[0m  [72/84], [94mLoss[0m : 2.37686
[1mStep[0m  [80/84], [94mLoss[0m : 2.54168

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.492, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55038
[1mStep[0m  [8/84], [94mLoss[0m : 2.69992
[1mStep[0m  [16/84], [94mLoss[0m : 2.52182
[1mStep[0m  [24/84], [94mLoss[0m : 2.66508
[1mStep[0m  [32/84], [94mLoss[0m : 2.49335
[1mStep[0m  [40/84], [94mLoss[0m : 2.43900
[1mStep[0m  [48/84], [94mLoss[0m : 2.43214
[1mStep[0m  [56/84], [94mLoss[0m : 2.11653
[1mStep[0m  [64/84], [94mLoss[0m : 2.48769
[1mStep[0m  [72/84], [94mLoss[0m : 2.79182
[1mStep[0m  [80/84], [94mLoss[0m : 2.43314

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61493
[1mStep[0m  [8/84], [94mLoss[0m : 2.53848
[1mStep[0m  [16/84], [94mLoss[0m : 2.31198
[1mStep[0m  [24/84], [94mLoss[0m : 2.63096
[1mStep[0m  [32/84], [94mLoss[0m : 3.00629
[1mStep[0m  [40/84], [94mLoss[0m : 2.28258
[1mStep[0m  [48/84], [94mLoss[0m : 2.69398
[1mStep[0m  [56/84], [94mLoss[0m : 2.51134
[1mStep[0m  [64/84], [94mLoss[0m : 2.75464
[1mStep[0m  [72/84], [94mLoss[0m : 2.71417
[1mStep[0m  [80/84], [94mLoss[0m : 2.63638

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69903
[1mStep[0m  [8/84], [94mLoss[0m : 2.86137
[1mStep[0m  [16/84], [94mLoss[0m : 2.44037
[1mStep[0m  [24/84], [94mLoss[0m : 2.27235
[1mStep[0m  [32/84], [94mLoss[0m : 2.12791
[1mStep[0m  [40/84], [94mLoss[0m : 2.32097
[1mStep[0m  [48/84], [94mLoss[0m : 2.65812
[1mStep[0m  [56/84], [94mLoss[0m : 2.67063
[1mStep[0m  [64/84], [94mLoss[0m : 2.57789
[1mStep[0m  [72/84], [94mLoss[0m : 2.21596
[1mStep[0m  [80/84], [94mLoss[0m : 2.38763

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61632
[1mStep[0m  [8/84], [94mLoss[0m : 2.59212
[1mStep[0m  [16/84], [94mLoss[0m : 2.59490
[1mStep[0m  [24/84], [94mLoss[0m : 2.41524
[1mStep[0m  [32/84], [94mLoss[0m : 2.14557
[1mStep[0m  [40/84], [94mLoss[0m : 2.31388
[1mStep[0m  [48/84], [94mLoss[0m : 2.63778
[1mStep[0m  [56/84], [94mLoss[0m : 2.39803
[1mStep[0m  [64/84], [94mLoss[0m : 2.52665
[1mStep[0m  [72/84], [94mLoss[0m : 2.57434
[1mStep[0m  [80/84], [94mLoss[0m : 2.19746

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71483
[1mStep[0m  [8/84], [94mLoss[0m : 2.55375
[1mStep[0m  [16/84], [94mLoss[0m : 2.56617
[1mStep[0m  [24/84], [94mLoss[0m : 2.10051
[1mStep[0m  [32/84], [94mLoss[0m : 2.63355
[1mStep[0m  [40/84], [94mLoss[0m : 2.32441
[1mStep[0m  [48/84], [94mLoss[0m : 2.16813
[1mStep[0m  [56/84], [94mLoss[0m : 2.22685
[1mStep[0m  [64/84], [94mLoss[0m : 2.37398
[1mStep[0m  [72/84], [94mLoss[0m : 2.64100
[1mStep[0m  [80/84], [94mLoss[0m : 2.63103

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36924
[1mStep[0m  [8/84], [94mLoss[0m : 2.45535
[1mStep[0m  [16/84], [94mLoss[0m : 2.45004
[1mStep[0m  [24/84], [94mLoss[0m : 2.45681
[1mStep[0m  [32/84], [94mLoss[0m : 2.22437
[1mStep[0m  [40/84], [94mLoss[0m : 2.42351
[1mStep[0m  [48/84], [94mLoss[0m : 2.23189
[1mStep[0m  [56/84], [94mLoss[0m : 2.15470
[1mStep[0m  [64/84], [94mLoss[0m : 2.34508
[1mStep[0m  [72/84], [94mLoss[0m : 2.36040
[1mStep[0m  [80/84], [94mLoss[0m : 2.56440

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60548
[1mStep[0m  [8/84], [94mLoss[0m : 2.48879
[1mStep[0m  [16/84], [94mLoss[0m : 2.42943
[1mStep[0m  [24/84], [94mLoss[0m : 2.50239
[1mStep[0m  [32/84], [94mLoss[0m : 2.40396
[1mStep[0m  [40/84], [94mLoss[0m : 2.20773
[1mStep[0m  [48/84], [94mLoss[0m : 2.34730
[1mStep[0m  [56/84], [94mLoss[0m : 2.73455
[1mStep[0m  [64/84], [94mLoss[0m : 2.38614
[1mStep[0m  [72/84], [94mLoss[0m : 2.36186
[1mStep[0m  [80/84], [94mLoss[0m : 2.45904

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.66822
[1mStep[0m  [8/84], [94mLoss[0m : 2.70431
[1mStep[0m  [16/84], [94mLoss[0m : 2.26931
[1mStep[0m  [24/84], [94mLoss[0m : 2.62650
[1mStep[0m  [32/84], [94mLoss[0m : 2.43175
[1mStep[0m  [40/84], [94mLoss[0m : 2.35316
[1mStep[0m  [48/84], [94mLoss[0m : 2.78722
[1mStep[0m  [56/84], [94mLoss[0m : 2.41326
[1mStep[0m  [64/84], [94mLoss[0m : 2.48545
[1mStep[0m  [72/84], [94mLoss[0m : 2.37669
[1mStep[0m  [80/84], [94mLoss[0m : 2.49905

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21725
[1mStep[0m  [8/84], [94mLoss[0m : 2.65162
[1mStep[0m  [16/84], [94mLoss[0m : 2.60775
[1mStep[0m  [24/84], [94mLoss[0m : 2.67983
[1mStep[0m  [32/84], [94mLoss[0m : 2.61523
[1mStep[0m  [40/84], [94mLoss[0m : 2.53566
[1mStep[0m  [48/84], [94mLoss[0m : 2.09259
[1mStep[0m  [56/84], [94mLoss[0m : 2.28821
[1mStep[0m  [64/84], [94mLoss[0m : 2.41176
[1mStep[0m  [72/84], [94mLoss[0m : 2.51504
[1mStep[0m  [80/84], [94mLoss[0m : 2.38376

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36754
[1mStep[0m  [8/84], [94mLoss[0m : 2.36381
[1mStep[0m  [16/84], [94mLoss[0m : 2.56214
[1mStep[0m  [24/84], [94mLoss[0m : 2.48716
[1mStep[0m  [32/84], [94mLoss[0m : 2.31005
[1mStep[0m  [40/84], [94mLoss[0m : 2.45352
[1mStep[0m  [48/84], [94mLoss[0m : 2.45385
[1mStep[0m  [56/84], [94mLoss[0m : 2.35008
[1mStep[0m  [64/84], [94mLoss[0m : 2.30209
[1mStep[0m  [72/84], [94mLoss[0m : 2.44899
[1mStep[0m  [80/84], [94mLoss[0m : 2.41579

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55839
[1mStep[0m  [8/84], [94mLoss[0m : 2.40364
[1mStep[0m  [16/84], [94mLoss[0m : 2.45127
[1mStep[0m  [24/84], [94mLoss[0m : 2.06516
[1mStep[0m  [32/84], [94mLoss[0m : 2.41597
[1mStep[0m  [40/84], [94mLoss[0m : 2.37546
[1mStep[0m  [48/84], [94mLoss[0m : 2.70642
[1mStep[0m  [56/84], [94mLoss[0m : 2.40361
[1mStep[0m  [64/84], [94mLoss[0m : 2.19486
[1mStep[0m  [72/84], [94mLoss[0m : 2.69407
[1mStep[0m  [80/84], [94mLoss[0m : 2.73106

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59631
[1mStep[0m  [8/84], [94mLoss[0m : 2.46652
[1mStep[0m  [16/84], [94mLoss[0m : 2.59060
[1mStep[0m  [24/84], [94mLoss[0m : 2.44317
[1mStep[0m  [32/84], [94mLoss[0m : 2.47091
[1mStep[0m  [40/84], [94mLoss[0m : 2.39869
[1mStep[0m  [48/84], [94mLoss[0m : 2.52027
[1mStep[0m  [56/84], [94mLoss[0m : 2.64793
[1mStep[0m  [64/84], [94mLoss[0m : 2.51032
[1mStep[0m  [72/84], [94mLoss[0m : 2.61420
[1mStep[0m  [80/84], [94mLoss[0m : 2.44823

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54742
[1mStep[0m  [8/84], [94mLoss[0m : 2.49256
[1mStep[0m  [16/84], [94mLoss[0m : 2.75079
[1mStep[0m  [24/84], [94mLoss[0m : 2.15702
[1mStep[0m  [32/84], [94mLoss[0m : 2.21605
[1mStep[0m  [40/84], [94mLoss[0m : 2.59198
[1mStep[0m  [48/84], [94mLoss[0m : 2.45452
[1mStep[0m  [56/84], [94mLoss[0m : 2.39575
[1mStep[0m  [64/84], [94mLoss[0m : 2.25321
[1mStep[0m  [72/84], [94mLoss[0m : 2.53108
[1mStep[0m  [80/84], [94mLoss[0m : 2.31328

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37490
[1mStep[0m  [8/84], [94mLoss[0m : 2.51584
[1mStep[0m  [16/84], [94mLoss[0m : 2.53499
[1mStep[0m  [24/84], [94mLoss[0m : 2.37514
[1mStep[0m  [32/84], [94mLoss[0m : 2.26429
[1mStep[0m  [40/84], [94mLoss[0m : 2.53613
[1mStep[0m  [48/84], [94mLoss[0m : 2.53866
[1mStep[0m  [56/84], [94mLoss[0m : 2.63136
[1mStep[0m  [64/84], [94mLoss[0m : 2.68124
[1mStep[0m  [72/84], [94mLoss[0m : 2.28197
[1mStep[0m  [80/84], [94mLoss[0m : 2.34257

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46238
[1mStep[0m  [8/84], [94mLoss[0m : 2.24448
[1mStep[0m  [16/84], [94mLoss[0m : 2.41651
[1mStep[0m  [24/84], [94mLoss[0m : 2.39231
[1mStep[0m  [32/84], [94mLoss[0m : 2.20254
[1mStep[0m  [40/84], [94mLoss[0m : 2.41047
[1mStep[0m  [48/84], [94mLoss[0m : 2.44726
[1mStep[0m  [56/84], [94mLoss[0m : 2.51929
[1mStep[0m  [64/84], [94mLoss[0m : 2.39566
[1mStep[0m  [72/84], [94mLoss[0m : 3.07074
[1mStep[0m  [80/84], [94mLoss[0m : 2.45353

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52517
[1mStep[0m  [8/84], [94mLoss[0m : 2.11961
[1mStep[0m  [16/84], [94mLoss[0m : 2.14846
[1mStep[0m  [24/84], [94mLoss[0m : 2.34464
[1mStep[0m  [32/84], [94mLoss[0m : 2.65534
[1mStep[0m  [40/84], [94mLoss[0m : 2.82742
[1mStep[0m  [48/84], [94mLoss[0m : 2.40481
[1mStep[0m  [56/84], [94mLoss[0m : 2.45146
[1mStep[0m  [64/84], [94mLoss[0m : 2.18735
[1mStep[0m  [72/84], [94mLoss[0m : 2.65356
[1mStep[0m  [80/84], [94mLoss[0m : 2.46374

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.44786
[1mStep[0m  [8/84], [94mLoss[0m : 2.52776
[1mStep[0m  [16/84], [94mLoss[0m : 2.38262
[1mStep[0m  [24/84], [94mLoss[0m : 2.23225
[1mStep[0m  [32/84], [94mLoss[0m : 2.49356
[1mStep[0m  [40/84], [94mLoss[0m : 2.13763
[1mStep[0m  [48/84], [94mLoss[0m : 2.38906
[1mStep[0m  [56/84], [94mLoss[0m : 2.53217
[1mStep[0m  [64/84], [94mLoss[0m : 2.37443
[1mStep[0m  [72/84], [94mLoss[0m : 2.76803
[1mStep[0m  [80/84], [94mLoss[0m : 2.37334

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58763
[1mStep[0m  [8/84], [94mLoss[0m : 2.34370
[1mStep[0m  [16/84], [94mLoss[0m : 2.43958
[1mStep[0m  [24/84], [94mLoss[0m : 2.46571
[1mStep[0m  [32/84], [94mLoss[0m : 2.73748
[1mStep[0m  [40/84], [94mLoss[0m : 2.55255
[1mStep[0m  [48/84], [94mLoss[0m : 2.28332
[1mStep[0m  [56/84], [94mLoss[0m : 2.50924
[1mStep[0m  [64/84], [94mLoss[0m : 2.18764
[1mStep[0m  [72/84], [94mLoss[0m : 2.16594
[1mStep[0m  [80/84], [94mLoss[0m : 2.43250

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50525
[1mStep[0m  [8/84], [94mLoss[0m : 2.53250
[1mStep[0m  [16/84], [94mLoss[0m : 2.62444
[1mStep[0m  [24/84], [94mLoss[0m : 2.45609
[1mStep[0m  [32/84], [94mLoss[0m : 2.09898
[1mStep[0m  [40/84], [94mLoss[0m : 2.59671
[1mStep[0m  [48/84], [94mLoss[0m : 1.97451
[1mStep[0m  [56/84], [94mLoss[0m : 2.56821
[1mStep[0m  [64/84], [94mLoss[0m : 2.20876
[1mStep[0m  [72/84], [94mLoss[0m : 2.45409
[1mStep[0m  [80/84], [94mLoss[0m : 2.78547

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60859
[1mStep[0m  [8/84], [94mLoss[0m : 2.46302
[1mStep[0m  [16/84], [94mLoss[0m : 2.36692
[1mStep[0m  [24/84], [94mLoss[0m : 2.40917
[1mStep[0m  [32/84], [94mLoss[0m : 2.42462
[1mStep[0m  [40/84], [94mLoss[0m : 2.31921
[1mStep[0m  [48/84], [94mLoss[0m : 2.44339
[1mStep[0m  [56/84], [94mLoss[0m : 2.26954
[1mStep[0m  [64/84], [94mLoss[0m : 2.34599
[1mStep[0m  [72/84], [94mLoss[0m : 2.65491
[1mStep[0m  [80/84], [94mLoss[0m : 2.44442

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55515
[1mStep[0m  [8/84], [94mLoss[0m : 2.71209
[1mStep[0m  [16/84], [94mLoss[0m : 2.52113
[1mStep[0m  [24/84], [94mLoss[0m : 2.40848
[1mStep[0m  [32/84], [94mLoss[0m : 2.10041
[1mStep[0m  [40/84], [94mLoss[0m : 2.59501
[1mStep[0m  [48/84], [94mLoss[0m : 2.70479
[1mStep[0m  [56/84], [94mLoss[0m : 2.39795
[1mStep[0m  [64/84], [94mLoss[0m : 2.30289
[1mStep[0m  [72/84], [94mLoss[0m : 2.21891
[1mStep[0m  [80/84], [94mLoss[0m : 2.58627

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28001
[1mStep[0m  [8/84], [94mLoss[0m : 2.42685
[1mStep[0m  [16/84], [94mLoss[0m : 2.43757
[1mStep[0m  [24/84], [94mLoss[0m : 2.68168
[1mStep[0m  [32/84], [94mLoss[0m : 2.84343
[1mStep[0m  [40/84], [94mLoss[0m : 2.34136
[1mStep[0m  [48/84], [94mLoss[0m : 2.41838
[1mStep[0m  [56/84], [94mLoss[0m : 2.35657
[1mStep[0m  [64/84], [94mLoss[0m : 2.33457
[1mStep[0m  [72/84], [94mLoss[0m : 2.41865
[1mStep[0m  [80/84], [94mLoss[0m : 2.54039

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.328
====================================

Phase 1 - Evaluation MAE:  2.3282423870904103
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/84], [94mLoss[0m : 2.34632
[1mStep[0m  [8/84], [94mLoss[0m : 2.44762
[1mStep[0m  [16/84], [94mLoss[0m : 2.37081
[1mStep[0m  [24/84], [94mLoss[0m : 2.29325
[1mStep[0m  [32/84], [94mLoss[0m : 2.68007
[1mStep[0m  [40/84], [94mLoss[0m : 2.22023
[1mStep[0m  [48/84], [94mLoss[0m : 2.68536
[1mStep[0m  [56/84], [94mLoss[0m : 2.48644
[1mStep[0m  [64/84], [94mLoss[0m : 2.51536
[1mStep[0m  [72/84], [94mLoss[0m : 2.50788
[1mStep[0m  [80/84], [94mLoss[0m : 2.34322

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80060
[1mStep[0m  [8/84], [94mLoss[0m : 2.58232
[1mStep[0m  [16/84], [94mLoss[0m : 2.30611
[1mStep[0m  [24/84], [94mLoss[0m : 2.52453
[1mStep[0m  [32/84], [94mLoss[0m : 2.74983
[1mStep[0m  [40/84], [94mLoss[0m : 2.19493
[1mStep[0m  [48/84], [94mLoss[0m : 2.32719
[1mStep[0m  [56/84], [94mLoss[0m : 2.21487
[1mStep[0m  [64/84], [94mLoss[0m : 2.51593
[1mStep[0m  [72/84], [94mLoss[0m : 2.29140
[1mStep[0m  [80/84], [94mLoss[0m : 2.42907

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.327, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39732
[1mStep[0m  [8/84], [94mLoss[0m : 2.29228
[1mStep[0m  [16/84], [94mLoss[0m : 2.28547
[1mStep[0m  [24/84], [94mLoss[0m : 2.41044
[1mStep[0m  [32/84], [94mLoss[0m : 2.23332
[1mStep[0m  [40/84], [94mLoss[0m : 2.68703
[1mStep[0m  [48/84], [94mLoss[0m : 2.44166
[1mStep[0m  [56/84], [94mLoss[0m : 2.22228
[1mStep[0m  [64/84], [94mLoss[0m : 2.64841
[1mStep[0m  [72/84], [94mLoss[0m : 2.30461
[1mStep[0m  [80/84], [94mLoss[0m : 2.44131

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48321
[1mStep[0m  [8/84], [94mLoss[0m : 2.11540
[1mStep[0m  [16/84], [94mLoss[0m : 2.23817
[1mStep[0m  [24/84], [94mLoss[0m : 2.32569
[1mStep[0m  [32/84], [94mLoss[0m : 2.31242
[1mStep[0m  [40/84], [94mLoss[0m : 2.42935
[1mStep[0m  [48/84], [94mLoss[0m : 2.34533
[1mStep[0m  [56/84], [94mLoss[0m : 2.77928
[1mStep[0m  [64/84], [94mLoss[0m : 2.04117
[1mStep[0m  [72/84], [94mLoss[0m : 2.37433
[1mStep[0m  [80/84], [94mLoss[0m : 2.16631

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54623
[1mStep[0m  [8/84], [94mLoss[0m : 2.30623
[1mStep[0m  [16/84], [94mLoss[0m : 2.27487
[1mStep[0m  [24/84], [94mLoss[0m : 2.72633
[1mStep[0m  [32/84], [94mLoss[0m : 2.40819
[1mStep[0m  [40/84], [94mLoss[0m : 2.32532
[1mStep[0m  [48/84], [94mLoss[0m : 2.42482
[1mStep[0m  [56/84], [94mLoss[0m : 2.21574
[1mStep[0m  [64/84], [94mLoss[0m : 2.15997
[1mStep[0m  [72/84], [94mLoss[0m : 2.51515
[1mStep[0m  [80/84], [94mLoss[0m : 2.56922

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55995
[1mStep[0m  [8/84], [94mLoss[0m : 2.44802
[1mStep[0m  [16/84], [94mLoss[0m : 2.54649
[1mStep[0m  [24/84], [94mLoss[0m : 2.34243
[1mStep[0m  [32/84], [94mLoss[0m : 2.19963
[1mStep[0m  [40/84], [94mLoss[0m : 2.04514
[1mStep[0m  [48/84], [94mLoss[0m : 1.98628
[1mStep[0m  [56/84], [94mLoss[0m : 2.32817
[1mStep[0m  [64/84], [94mLoss[0m : 2.53377
[1mStep[0m  [72/84], [94mLoss[0m : 2.29815
[1mStep[0m  [80/84], [94mLoss[0m : 2.54109

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.324, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07314
[1mStep[0m  [8/84], [94mLoss[0m : 2.14416
[1mStep[0m  [16/84], [94mLoss[0m : 2.09229
[1mStep[0m  [24/84], [94mLoss[0m : 2.65759
[1mStep[0m  [32/84], [94mLoss[0m : 1.91620
[1mStep[0m  [40/84], [94mLoss[0m : 2.43082
[1mStep[0m  [48/84], [94mLoss[0m : 2.37895
[1mStep[0m  [56/84], [94mLoss[0m : 2.28458
[1mStep[0m  [64/84], [94mLoss[0m : 2.10703
[1mStep[0m  [72/84], [94mLoss[0m : 2.55465
[1mStep[0m  [80/84], [94mLoss[0m : 2.49295

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.303, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32113
[1mStep[0m  [8/84], [94mLoss[0m : 2.10757
[1mStep[0m  [16/84], [94mLoss[0m : 2.54011
[1mStep[0m  [24/84], [94mLoss[0m : 2.28290
[1mStep[0m  [32/84], [94mLoss[0m : 2.17719
[1mStep[0m  [40/84], [94mLoss[0m : 2.22381
[1mStep[0m  [48/84], [94mLoss[0m : 2.33868
[1mStep[0m  [56/84], [94mLoss[0m : 2.17034
[1mStep[0m  [64/84], [94mLoss[0m : 2.91175
[1mStep[0m  [72/84], [94mLoss[0m : 2.17207
[1mStep[0m  [80/84], [94mLoss[0m : 2.29344

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.271, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.22758
[1mStep[0m  [8/84], [94mLoss[0m : 1.96060
[1mStep[0m  [16/84], [94mLoss[0m : 2.31232
[1mStep[0m  [24/84], [94mLoss[0m : 2.37467
[1mStep[0m  [32/84], [94mLoss[0m : 2.19523
[1mStep[0m  [40/84], [94mLoss[0m : 2.17814
[1mStep[0m  [48/84], [94mLoss[0m : 2.12382
[1mStep[0m  [56/84], [94mLoss[0m : 2.23988
[1mStep[0m  [64/84], [94mLoss[0m : 2.43222
[1mStep[0m  [72/84], [94mLoss[0m : 2.35909
[1mStep[0m  [80/84], [94mLoss[0m : 2.53299

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.258, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28385
[1mStep[0m  [8/84], [94mLoss[0m : 2.15456
[1mStep[0m  [16/84], [94mLoss[0m : 2.12584
[1mStep[0m  [24/84], [94mLoss[0m : 2.19373
[1mStep[0m  [32/84], [94mLoss[0m : 2.23881
[1mStep[0m  [40/84], [94mLoss[0m : 2.04792
[1mStep[0m  [48/84], [94mLoss[0m : 2.15805
[1mStep[0m  [56/84], [94mLoss[0m : 2.17688
[1mStep[0m  [64/84], [94mLoss[0m : 2.02667
[1mStep[0m  [72/84], [94mLoss[0m : 2.26968
[1mStep[0m  [80/84], [94mLoss[0m : 2.49767

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.218, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07724
[1mStep[0m  [8/84], [94mLoss[0m : 2.02601
[1mStep[0m  [16/84], [94mLoss[0m : 1.96262
[1mStep[0m  [24/84], [94mLoss[0m : 2.06958
[1mStep[0m  [32/84], [94mLoss[0m : 2.50347
[1mStep[0m  [40/84], [94mLoss[0m : 2.14465
[1mStep[0m  [48/84], [94mLoss[0m : 2.14612
[1mStep[0m  [56/84], [94mLoss[0m : 1.94878
[1mStep[0m  [64/84], [94mLoss[0m : 2.04343
[1mStep[0m  [72/84], [94mLoss[0m : 2.22132
[1mStep[0m  [80/84], [94mLoss[0m : 2.40264

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.161, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00396
[1mStep[0m  [8/84], [94mLoss[0m : 2.05956
[1mStep[0m  [16/84], [94mLoss[0m : 2.08682
[1mStep[0m  [24/84], [94mLoss[0m : 1.93255
[1mStep[0m  [32/84], [94mLoss[0m : 2.19275
[1mStep[0m  [40/84], [94mLoss[0m : 2.19078
[1mStep[0m  [48/84], [94mLoss[0m : 2.25188
[1mStep[0m  [56/84], [94mLoss[0m : 2.11787
[1mStep[0m  [64/84], [94mLoss[0m : 2.39882
[1mStep[0m  [72/84], [94mLoss[0m : 1.82606
[1mStep[0m  [80/84], [94mLoss[0m : 2.26757

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79562
[1mStep[0m  [8/84], [94mLoss[0m : 2.39232
[1mStep[0m  [16/84], [94mLoss[0m : 2.15219
[1mStep[0m  [24/84], [94mLoss[0m : 2.15180
[1mStep[0m  [32/84], [94mLoss[0m : 1.96153
[1mStep[0m  [40/84], [94mLoss[0m : 1.98139
[1mStep[0m  [48/84], [94mLoss[0m : 2.05352
[1mStep[0m  [56/84], [94mLoss[0m : 2.14140
[1mStep[0m  [64/84], [94mLoss[0m : 2.21862
[1mStep[0m  [72/84], [94mLoss[0m : 2.23805
[1mStep[0m  [80/84], [94mLoss[0m : 2.20481

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.098, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88891
[1mStep[0m  [8/84], [94mLoss[0m : 1.89393
[1mStep[0m  [16/84], [94mLoss[0m : 2.04993
[1mStep[0m  [24/84], [94mLoss[0m : 2.06996
[1mStep[0m  [32/84], [94mLoss[0m : 2.25732
[1mStep[0m  [40/84], [94mLoss[0m : 2.12864
[1mStep[0m  [48/84], [94mLoss[0m : 1.94888
[1mStep[0m  [56/84], [94mLoss[0m : 2.19639
[1mStep[0m  [64/84], [94mLoss[0m : 1.92206
[1mStep[0m  [72/84], [94mLoss[0m : 2.23797
[1mStep[0m  [80/84], [94mLoss[0m : 2.18017

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.076, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.91460
[1mStep[0m  [8/84], [94mLoss[0m : 1.86719
[1mStep[0m  [16/84], [94mLoss[0m : 2.20116
[1mStep[0m  [24/84], [94mLoss[0m : 2.18567
[1mStep[0m  [32/84], [94mLoss[0m : 1.78532
[1mStep[0m  [40/84], [94mLoss[0m : 2.29824
[1mStep[0m  [48/84], [94mLoss[0m : 2.07634
[1mStep[0m  [56/84], [94mLoss[0m : 2.16201
[1mStep[0m  [64/84], [94mLoss[0m : 1.76195
[1mStep[0m  [72/84], [94mLoss[0m : 1.95976
[1mStep[0m  [80/84], [94mLoss[0m : 1.97401

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73665
[1mStep[0m  [8/84], [94mLoss[0m : 2.00581
[1mStep[0m  [16/84], [94mLoss[0m : 1.98346
[1mStep[0m  [24/84], [94mLoss[0m : 1.72093
[1mStep[0m  [32/84], [94mLoss[0m : 2.25166
[1mStep[0m  [40/84], [94mLoss[0m : 1.70824
[1mStep[0m  [48/84], [94mLoss[0m : 1.86748
[1mStep[0m  [56/84], [94mLoss[0m : 1.75879
[1mStep[0m  [64/84], [94mLoss[0m : 2.03328
[1mStep[0m  [72/84], [94mLoss[0m : 2.06140
[1mStep[0m  [80/84], [94mLoss[0m : 1.82098

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.64154
[1mStep[0m  [8/84], [94mLoss[0m : 1.81974
[1mStep[0m  [16/84], [94mLoss[0m : 1.76404
[1mStep[0m  [24/84], [94mLoss[0m : 1.93513
[1mStep[0m  [32/84], [94mLoss[0m : 1.89376
[1mStep[0m  [40/84], [94mLoss[0m : 1.83123
[1mStep[0m  [48/84], [94mLoss[0m : 1.80387
[1mStep[0m  [56/84], [94mLoss[0m : 1.97780
[1mStep[0m  [64/84], [94mLoss[0m : 1.91967
[1mStep[0m  [72/84], [94mLoss[0m : 1.89556
[1mStep[0m  [80/84], [94mLoss[0m : 1.99857

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.959, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35370
[1mStep[0m  [8/84], [94mLoss[0m : 2.06688
[1mStep[0m  [16/84], [94mLoss[0m : 1.83969
[1mStep[0m  [24/84], [94mLoss[0m : 1.87370
[1mStep[0m  [32/84], [94mLoss[0m : 1.71354
[1mStep[0m  [40/84], [94mLoss[0m : 1.92711
[1mStep[0m  [48/84], [94mLoss[0m : 1.90586
[1mStep[0m  [56/84], [94mLoss[0m : 1.62218
[1mStep[0m  [64/84], [94mLoss[0m : 1.60430
[1mStep[0m  [72/84], [94mLoss[0m : 1.59029
[1mStep[0m  [80/84], [94mLoss[0m : 1.98947

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01576
[1mStep[0m  [8/84], [94mLoss[0m : 1.82386
[1mStep[0m  [16/84], [94mLoss[0m : 1.92590
[1mStep[0m  [24/84], [94mLoss[0m : 1.67773
[1mStep[0m  [32/84], [94mLoss[0m : 1.77473
[1mStep[0m  [40/84], [94mLoss[0m : 1.78870
[1mStep[0m  [48/84], [94mLoss[0m : 1.60810
[1mStep[0m  [56/84], [94mLoss[0m : 1.58553
[1mStep[0m  [64/84], [94mLoss[0m : 1.83413
[1mStep[0m  [72/84], [94mLoss[0m : 1.98172
[1mStep[0m  [80/84], [94mLoss[0m : 1.86651

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59799
[1mStep[0m  [8/84], [94mLoss[0m : 1.88216
[1mStep[0m  [16/84], [94mLoss[0m : 1.71248
[1mStep[0m  [24/84], [94mLoss[0m : 1.63652
[1mStep[0m  [32/84], [94mLoss[0m : 1.83965
[1mStep[0m  [40/84], [94mLoss[0m : 1.64690
[1mStep[0m  [48/84], [94mLoss[0m : 1.81797
[1mStep[0m  [56/84], [94mLoss[0m : 1.92263
[1mStep[0m  [64/84], [94mLoss[0m : 1.67101
[1mStep[0m  [72/84], [94mLoss[0m : 2.06887
[1mStep[0m  [80/84], [94mLoss[0m : 1.96846

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.829, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.74453
[1mStep[0m  [8/84], [94mLoss[0m : 1.67169
[1mStep[0m  [16/84], [94mLoss[0m : 1.96461
[1mStep[0m  [24/84], [94mLoss[0m : 1.85523
[1mStep[0m  [32/84], [94mLoss[0m : 1.75697
[1mStep[0m  [40/84], [94mLoss[0m : 1.88889
[1mStep[0m  [48/84], [94mLoss[0m : 1.88743
[1mStep[0m  [56/84], [94mLoss[0m : 1.69540
[1mStep[0m  [64/84], [94mLoss[0m : 1.68932
[1mStep[0m  [72/84], [94mLoss[0m : 1.80683
[1mStep[0m  [80/84], [94mLoss[0m : 1.71597

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.495, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92199
[1mStep[0m  [8/84], [94mLoss[0m : 1.57916
[1mStep[0m  [16/84], [94mLoss[0m : 1.54770
[1mStep[0m  [24/84], [94mLoss[0m : 1.54284
[1mStep[0m  [32/84], [94mLoss[0m : 1.82978
[1mStep[0m  [40/84], [94mLoss[0m : 1.81458
[1mStep[0m  [48/84], [94mLoss[0m : 1.71350
[1mStep[0m  [56/84], [94mLoss[0m : 1.70701
[1mStep[0m  [64/84], [94mLoss[0m : 1.64063
[1mStep[0m  [72/84], [94mLoss[0m : 1.68340
[1mStep[0m  [80/84], [94mLoss[0m : 1.61288

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.754, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.84646
[1mStep[0m  [8/84], [94mLoss[0m : 1.49265
[1mStep[0m  [16/84], [94mLoss[0m : 1.72713
[1mStep[0m  [24/84], [94mLoss[0m : 1.62356
[1mStep[0m  [32/84], [94mLoss[0m : 1.69785
[1mStep[0m  [40/84], [94mLoss[0m : 1.89879
[1mStep[0m  [48/84], [94mLoss[0m : 1.92026
[1mStep[0m  [56/84], [94mLoss[0m : 1.55783
[1mStep[0m  [64/84], [94mLoss[0m : 1.90721
[1mStep[0m  [72/84], [94mLoss[0m : 1.74794
[1mStep[0m  [80/84], [94mLoss[0m : 1.72790

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78311
[1mStep[0m  [8/84], [94mLoss[0m : 1.74629
[1mStep[0m  [16/84], [94mLoss[0m : 1.40286
[1mStep[0m  [24/84], [94mLoss[0m : 1.73543
[1mStep[0m  [32/84], [94mLoss[0m : 1.71631
[1mStep[0m  [40/84], [94mLoss[0m : 1.69065
[1mStep[0m  [48/84], [94mLoss[0m : 1.85579
[1mStep[0m  [56/84], [94mLoss[0m : 1.79231
[1mStep[0m  [64/84], [94mLoss[0m : 1.70197
[1mStep[0m  [72/84], [94mLoss[0m : 1.62849
[1mStep[0m  [80/84], [94mLoss[0m : 1.64541

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.705, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.66380
[1mStep[0m  [8/84], [94mLoss[0m : 1.65133
[1mStep[0m  [16/84], [94mLoss[0m : 1.67073
[1mStep[0m  [24/84], [94mLoss[0m : 1.53429
[1mStep[0m  [32/84], [94mLoss[0m : 1.65760
[1mStep[0m  [40/84], [94mLoss[0m : 1.73611
[1mStep[0m  [48/84], [94mLoss[0m : 1.76438
[1mStep[0m  [56/84], [94mLoss[0m : 1.71222
[1mStep[0m  [64/84], [94mLoss[0m : 1.68849
[1mStep[0m  [72/84], [94mLoss[0m : 1.71541
[1mStep[0m  [80/84], [94mLoss[0m : 1.86325

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83840
[1mStep[0m  [8/84], [94mLoss[0m : 1.60680
[1mStep[0m  [16/84], [94mLoss[0m : 1.71232
[1mStep[0m  [24/84], [94mLoss[0m : 1.72316
[1mStep[0m  [32/84], [94mLoss[0m : 1.71820
[1mStep[0m  [40/84], [94mLoss[0m : 1.62355
[1mStep[0m  [48/84], [94mLoss[0m : 1.60608
[1mStep[0m  [56/84], [94mLoss[0m : 1.61301
[1mStep[0m  [64/84], [94mLoss[0m : 1.64369
[1mStep[0m  [72/84], [94mLoss[0m : 1.61019
[1mStep[0m  [80/84], [94mLoss[0m : 1.66464

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.48265
[1mStep[0m  [8/84], [94mLoss[0m : 1.71808
[1mStep[0m  [16/84], [94mLoss[0m : 1.64667
[1mStep[0m  [24/84], [94mLoss[0m : 1.88371
[1mStep[0m  [32/84], [94mLoss[0m : 1.75999
[1mStep[0m  [40/84], [94mLoss[0m : 1.77783
[1mStep[0m  [48/84], [94mLoss[0m : 1.88547
[1mStep[0m  [56/84], [94mLoss[0m : 1.74756
[1mStep[0m  [64/84], [94mLoss[0m : 1.72882
[1mStep[0m  [72/84], [94mLoss[0m : 1.77458
[1mStep[0m  [80/84], [94mLoss[0m : 1.59497

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.658, [92mTest[0m: 2.461, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.51573
[1mStep[0m  [8/84], [94mLoss[0m : 1.38576
[1mStep[0m  [16/84], [94mLoss[0m : 1.61051
[1mStep[0m  [24/84], [94mLoss[0m : 1.68675
[1mStep[0m  [32/84], [94mLoss[0m : 1.62031
[1mStep[0m  [40/84], [94mLoss[0m : 1.53590
[1mStep[0m  [48/84], [94mLoss[0m : 1.37497
[1mStep[0m  [56/84], [94mLoss[0m : 1.60896
[1mStep[0m  [64/84], [94mLoss[0m : 1.65737
[1mStep[0m  [72/84], [94mLoss[0m : 1.64196
[1mStep[0m  [80/84], [94mLoss[0m : 1.43192

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.39083
[1mStep[0m  [8/84], [94mLoss[0m : 1.74986
[1mStep[0m  [16/84], [94mLoss[0m : 1.52240
[1mStep[0m  [24/84], [94mLoss[0m : 1.47794
[1mStep[0m  [32/84], [94mLoss[0m : 1.69206
[1mStep[0m  [40/84], [94mLoss[0m : 1.51512
[1mStep[0m  [48/84], [94mLoss[0m : 1.88453
[1mStep[0m  [56/84], [94mLoss[0m : 1.63342
[1mStep[0m  [64/84], [94mLoss[0m : 1.55709
[1mStep[0m  [72/84], [94mLoss[0m : 1.49771
[1mStep[0m  [80/84], [94mLoss[0m : 1.61201

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.46256
[1mStep[0m  [8/84], [94mLoss[0m : 1.45123
[1mStep[0m  [16/84], [94mLoss[0m : 1.55420
[1mStep[0m  [24/84], [94mLoss[0m : 1.59605
[1mStep[0m  [32/84], [94mLoss[0m : 1.43806
[1mStep[0m  [40/84], [94mLoss[0m : 1.29896
[1mStep[0m  [48/84], [94mLoss[0m : 1.66544
[1mStep[0m  [56/84], [94mLoss[0m : 1.49931
[1mStep[0m  [64/84], [94mLoss[0m : 1.47820
[1mStep[0m  [72/84], [94mLoss[0m : 1.65674
[1mStep[0m  [80/84], [94mLoss[0m : 1.81502

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.473, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.478
====================================

Phase 2 - Evaluation MAE:  2.478314552988325
MAE score P1        2.328242
MAE score P2        2.478315
loss                1.585965
learning_rate       0.002575
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.5
weight_decay            0.01
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.43237
[1mStep[0m  [16/169], [94mLoss[0m : 10.67883
[1mStep[0m  [32/169], [94mLoss[0m : 9.84923
[1mStep[0m  [48/169], [94mLoss[0m : 9.32463
[1mStep[0m  [64/169], [94mLoss[0m : 8.28539
[1mStep[0m  [80/169], [94mLoss[0m : 7.26560
[1mStep[0m  [96/169], [94mLoss[0m : 6.83653
[1mStep[0m  [112/169], [94mLoss[0m : 6.93118
[1mStep[0m  [128/169], [94mLoss[0m : 4.67462
[1mStep[0m  [144/169], [94mLoss[0m : 4.06404
[1mStep[0m  [160/169], [94mLoss[0m : 4.08628

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.427, [92mTest[0m: 10.919, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.32048
[1mStep[0m  [16/169], [94mLoss[0m : 3.71171
[1mStep[0m  [32/169], [94mLoss[0m : 2.92797
[1mStep[0m  [48/169], [94mLoss[0m : 2.79495
[1mStep[0m  [64/169], [94mLoss[0m : 2.98541
[1mStep[0m  [80/169], [94mLoss[0m : 3.16997
[1mStep[0m  [96/169], [94mLoss[0m : 2.55513
[1mStep[0m  [112/169], [94mLoss[0m : 3.19175
[1mStep[0m  [128/169], [94mLoss[0m : 3.12319
[1mStep[0m  [144/169], [94mLoss[0m : 2.58038
[1mStep[0m  [160/169], [94mLoss[0m : 2.82991

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.899, [92mTest[0m: 2.970, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63728
[1mStep[0m  [16/169], [94mLoss[0m : 3.03201
[1mStep[0m  [32/169], [94mLoss[0m : 2.72221
[1mStep[0m  [48/169], [94mLoss[0m : 2.89110
[1mStep[0m  [64/169], [94mLoss[0m : 2.16441
[1mStep[0m  [80/169], [94mLoss[0m : 2.45790
[1mStep[0m  [96/169], [94mLoss[0m : 2.51396
[1mStep[0m  [112/169], [94mLoss[0m : 2.81896
[1mStep[0m  [128/169], [94mLoss[0m : 3.19334
[1mStep[0m  [144/169], [94mLoss[0m : 3.35209
[1mStep[0m  [160/169], [94mLoss[0m : 2.86465

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83997
[1mStep[0m  [16/169], [94mLoss[0m : 2.26357
[1mStep[0m  [32/169], [94mLoss[0m : 2.30327
[1mStep[0m  [48/169], [94mLoss[0m : 2.78253
[1mStep[0m  [64/169], [94mLoss[0m : 2.76033
[1mStep[0m  [80/169], [94mLoss[0m : 2.91515
[1mStep[0m  [96/169], [94mLoss[0m : 2.34800
[1mStep[0m  [112/169], [94mLoss[0m : 2.94247
[1mStep[0m  [128/169], [94mLoss[0m : 2.61262
[1mStep[0m  [144/169], [94mLoss[0m : 2.47537
[1mStep[0m  [160/169], [94mLoss[0m : 2.57772

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90749
[1mStep[0m  [16/169], [94mLoss[0m : 2.14391
[1mStep[0m  [32/169], [94mLoss[0m : 2.29292
[1mStep[0m  [48/169], [94mLoss[0m : 2.41714
[1mStep[0m  [64/169], [94mLoss[0m : 2.43353
[1mStep[0m  [80/169], [94mLoss[0m : 3.11607
[1mStep[0m  [96/169], [94mLoss[0m : 2.27132
[1mStep[0m  [112/169], [94mLoss[0m : 2.29069
[1mStep[0m  [128/169], [94mLoss[0m : 2.32001
[1mStep[0m  [144/169], [94mLoss[0m : 2.96773
[1mStep[0m  [160/169], [94mLoss[0m : 2.59470

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.577, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82330
[1mStep[0m  [16/169], [94mLoss[0m : 2.45078
[1mStep[0m  [32/169], [94mLoss[0m : 2.24854
[1mStep[0m  [48/169], [94mLoss[0m : 2.41108
[1mStep[0m  [64/169], [94mLoss[0m : 2.59211
[1mStep[0m  [80/169], [94mLoss[0m : 2.30399
[1mStep[0m  [96/169], [94mLoss[0m : 2.34362
[1mStep[0m  [112/169], [94mLoss[0m : 2.33053
[1mStep[0m  [128/169], [94mLoss[0m : 2.25625
[1mStep[0m  [144/169], [94mLoss[0m : 2.50597
[1mStep[0m  [160/169], [94mLoss[0m : 2.37927

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48112
[1mStep[0m  [16/169], [94mLoss[0m : 3.01431
[1mStep[0m  [32/169], [94mLoss[0m : 1.95952
[1mStep[0m  [48/169], [94mLoss[0m : 2.46746
[1mStep[0m  [64/169], [94mLoss[0m : 2.53037
[1mStep[0m  [80/169], [94mLoss[0m : 2.01514
[1mStep[0m  [96/169], [94mLoss[0m : 2.30699
[1mStep[0m  [112/169], [94mLoss[0m : 2.46751
[1mStep[0m  [128/169], [94mLoss[0m : 2.61321
[1mStep[0m  [144/169], [94mLoss[0m : 2.39311
[1mStep[0m  [160/169], [94mLoss[0m : 2.54083

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65283
[1mStep[0m  [16/169], [94mLoss[0m : 2.02660
[1mStep[0m  [32/169], [94mLoss[0m : 2.56566
[1mStep[0m  [48/169], [94mLoss[0m : 2.38537
[1mStep[0m  [64/169], [94mLoss[0m : 2.16064
[1mStep[0m  [80/169], [94mLoss[0m : 2.62548
[1mStep[0m  [96/169], [94mLoss[0m : 2.32302
[1mStep[0m  [112/169], [94mLoss[0m : 2.77714
[1mStep[0m  [128/169], [94mLoss[0m : 2.56626
[1mStep[0m  [144/169], [94mLoss[0m : 2.68216
[1mStep[0m  [160/169], [94mLoss[0m : 2.72868

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55641
[1mStep[0m  [16/169], [94mLoss[0m : 2.82019
[1mStep[0m  [32/169], [94mLoss[0m : 2.83931
[1mStep[0m  [48/169], [94mLoss[0m : 2.50770
[1mStep[0m  [64/169], [94mLoss[0m : 2.54580
[1mStep[0m  [80/169], [94mLoss[0m : 2.58593
[1mStep[0m  [96/169], [94mLoss[0m : 2.67991
[1mStep[0m  [112/169], [94mLoss[0m : 2.09278
[1mStep[0m  [128/169], [94mLoss[0m : 2.22499
[1mStep[0m  [144/169], [94mLoss[0m : 2.76564
[1mStep[0m  [160/169], [94mLoss[0m : 2.60194

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39593
[1mStep[0m  [16/169], [94mLoss[0m : 2.71149
[1mStep[0m  [32/169], [94mLoss[0m : 2.90021
[1mStep[0m  [48/169], [94mLoss[0m : 2.25574
[1mStep[0m  [64/169], [94mLoss[0m : 2.58726
[1mStep[0m  [80/169], [94mLoss[0m : 2.11237
[1mStep[0m  [96/169], [94mLoss[0m : 2.32163
[1mStep[0m  [112/169], [94mLoss[0m : 2.41288
[1mStep[0m  [128/169], [94mLoss[0m : 2.96532
[1mStep[0m  [144/169], [94mLoss[0m : 2.65182
[1mStep[0m  [160/169], [94mLoss[0m : 2.48331

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23791
[1mStep[0m  [16/169], [94mLoss[0m : 2.52576
[1mStep[0m  [32/169], [94mLoss[0m : 2.41079
[1mStep[0m  [48/169], [94mLoss[0m : 2.38887
[1mStep[0m  [64/169], [94mLoss[0m : 2.36638
[1mStep[0m  [80/169], [94mLoss[0m : 2.33165
[1mStep[0m  [96/169], [94mLoss[0m : 2.52957
[1mStep[0m  [112/169], [94mLoss[0m : 2.71940
[1mStep[0m  [128/169], [94mLoss[0m : 2.81175
[1mStep[0m  [144/169], [94mLoss[0m : 2.57393
[1mStep[0m  [160/169], [94mLoss[0m : 2.60787

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.97015
[1mStep[0m  [16/169], [94mLoss[0m : 2.43404
[1mStep[0m  [32/169], [94mLoss[0m : 2.77104
[1mStep[0m  [48/169], [94mLoss[0m : 2.26674
[1mStep[0m  [64/169], [94mLoss[0m : 2.31587
[1mStep[0m  [80/169], [94mLoss[0m : 2.43245
[1mStep[0m  [96/169], [94mLoss[0m : 2.13682
[1mStep[0m  [112/169], [94mLoss[0m : 2.29435
[1mStep[0m  [128/169], [94mLoss[0m : 2.31255
[1mStep[0m  [144/169], [94mLoss[0m : 2.42067
[1mStep[0m  [160/169], [94mLoss[0m : 2.01959

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67724
[1mStep[0m  [16/169], [94mLoss[0m : 2.50083
[1mStep[0m  [32/169], [94mLoss[0m : 2.45676
[1mStep[0m  [48/169], [94mLoss[0m : 2.39046
[1mStep[0m  [64/169], [94mLoss[0m : 2.42026
[1mStep[0m  [80/169], [94mLoss[0m : 2.41441
[1mStep[0m  [96/169], [94mLoss[0m : 2.25758
[1mStep[0m  [112/169], [94mLoss[0m : 2.46088
[1mStep[0m  [128/169], [94mLoss[0m : 3.05209
[1mStep[0m  [144/169], [94mLoss[0m : 2.62904
[1mStep[0m  [160/169], [94mLoss[0m : 2.39846

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.07481
[1mStep[0m  [16/169], [94mLoss[0m : 2.14786
[1mStep[0m  [32/169], [94mLoss[0m : 2.35946
[1mStep[0m  [48/169], [94mLoss[0m : 2.66383
[1mStep[0m  [64/169], [94mLoss[0m : 2.79850
[1mStep[0m  [80/169], [94mLoss[0m : 2.76216
[1mStep[0m  [96/169], [94mLoss[0m : 2.45683
[1mStep[0m  [112/169], [94mLoss[0m : 2.60068
[1mStep[0m  [128/169], [94mLoss[0m : 2.36191
[1mStep[0m  [144/169], [94mLoss[0m : 2.44755
[1mStep[0m  [160/169], [94mLoss[0m : 2.50313

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84303
[1mStep[0m  [16/169], [94mLoss[0m : 2.61887
[1mStep[0m  [32/169], [94mLoss[0m : 2.32613
[1mStep[0m  [48/169], [94mLoss[0m : 2.53677
[1mStep[0m  [64/169], [94mLoss[0m : 2.64282
[1mStep[0m  [80/169], [94mLoss[0m : 2.51597
[1mStep[0m  [96/169], [94mLoss[0m : 2.47996
[1mStep[0m  [112/169], [94mLoss[0m : 2.48148
[1mStep[0m  [128/169], [94mLoss[0m : 2.67046
[1mStep[0m  [144/169], [94mLoss[0m : 2.80264
[1mStep[0m  [160/169], [94mLoss[0m : 2.49915

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.325, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07132
[1mStep[0m  [16/169], [94mLoss[0m : 2.32531
[1mStep[0m  [32/169], [94mLoss[0m : 2.06104
[1mStep[0m  [48/169], [94mLoss[0m : 2.46781
[1mStep[0m  [64/169], [94mLoss[0m : 2.81475
[1mStep[0m  [80/169], [94mLoss[0m : 2.88393
[1mStep[0m  [96/169], [94mLoss[0m : 2.57096
[1mStep[0m  [112/169], [94mLoss[0m : 2.69510
[1mStep[0m  [128/169], [94mLoss[0m : 2.15577
[1mStep[0m  [144/169], [94mLoss[0m : 2.31703
[1mStep[0m  [160/169], [94mLoss[0m : 2.41081

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41232
[1mStep[0m  [16/169], [94mLoss[0m : 2.59162
[1mStep[0m  [32/169], [94mLoss[0m : 2.39915
[1mStep[0m  [48/169], [94mLoss[0m : 1.85755
[1mStep[0m  [64/169], [94mLoss[0m : 2.44579
[1mStep[0m  [80/169], [94mLoss[0m : 2.33580
[1mStep[0m  [96/169], [94mLoss[0m : 2.31107
[1mStep[0m  [112/169], [94mLoss[0m : 2.30834
[1mStep[0m  [128/169], [94mLoss[0m : 2.74810
[1mStep[0m  [144/169], [94mLoss[0m : 2.29494
[1mStep[0m  [160/169], [94mLoss[0m : 2.21168

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52530
[1mStep[0m  [16/169], [94mLoss[0m : 2.87297
[1mStep[0m  [32/169], [94mLoss[0m : 2.25069
[1mStep[0m  [48/169], [94mLoss[0m : 2.48928
[1mStep[0m  [64/169], [94mLoss[0m : 2.81109
[1mStep[0m  [80/169], [94mLoss[0m : 2.20233
[1mStep[0m  [96/169], [94mLoss[0m : 2.36470
[1mStep[0m  [112/169], [94mLoss[0m : 2.38775
[1mStep[0m  [128/169], [94mLoss[0m : 2.04151
[1mStep[0m  [144/169], [94mLoss[0m : 2.47986
[1mStep[0m  [160/169], [94mLoss[0m : 2.62355

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61071
[1mStep[0m  [16/169], [94mLoss[0m : 2.56443
[1mStep[0m  [32/169], [94mLoss[0m : 2.92916
[1mStep[0m  [48/169], [94mLoss[0m : 2.54960
[1mStep[0m  [64/169], [94mLoss[0m : 2.46216
[1mStep[0m  [80/169], [94mLoss[0m : 2.19793
[1mStep[0m  [96/169], [94mLoss[0m : 2.84856
[1mStep[0m  [112/169], [94mLoss[0m : 2.25791
[1mStep[0m  [128/169], [94mLoss[0m : 2.25138
[1mStep[0m  [144/169], [94mLoss[0m : 2.49636
[1mStep[0m  [160/169], [94mLoss[0m : 2.59898

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55762
[1mStep[0m  [16/169], [94mLoss[0m : 2.59443
[1mStep[0m  [32/169], [94mLoss[0m : 2.32361
[1mStep[0m  [48/169], [94mLoss[0m : 2.66151
[1mStep[0m  [64/169], [94mLoss[0m : 2.98018
[1mStep[0m  [80/169], [94mLoss[0m : 2.35932
[1mStep[0m  [96/169], [94mLoss[0m : 2.56840
[1mStep[0m  [112/169], [94mLoss[0m : 1.95498
[1mStep[0m  [128/169], [94mLoss[0m : 2.26682
[1mStep[0m  [144/169], [94mLoss[0m : 2.41061
[1mStep[0m  [160/169], [94mLoss[0m : 2.70643

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32966
[1mStep[0m  [16/169], [94mLoss[0m : 2.79875
[1mStep[0m  [32/169], [94mLoss[0m : 2.30275
[1mStep[0m  [48/169], [94mLoss[0m : 2.71415
[1mStep[0m  [64/169], [94mLoss[0m : 2.44263
[1mStep[0m  [80/169], [94mLoss[0m : 2.39642
[1mStep[0m  [96/169], [94mLoss[0m : 2.58734
[1mStep[0m  [112/169], [94mLoss[0m : 2.41977
[1mStep[0m  [128/169], [94mLoss[0m : 2.75065
[1mStep[0m  [144/169], [94mLoss[0m : 2.04597
[1mStep[0m  [160/169], [94mLoss[0m : 2.86852

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36021
[1mStep[0m  [16/169], [94mLoss[0m : 2.30693
[1mStep[0m  [32/169], [94mLoss[0m : 2.24403
[1mStep[0m  [48/169], [94mLoss[0m : 2.29596
[1mStep[0m  [64/169], [94mLoss[0m : 2.30203
[1mStep[0m  [80/169], [94mLoss[0m : 2.58537
[1mStep[0m  [96/169], [94mLoss[0m : 2.28240
[1mStep[0m  [112/169], [94mLoss[0m : 2.82719
[1mStep[0m  [128/169], [94mLoss[0m : 2.43436
[1mStep[0m  [144/169], [94mLoss[0m : 2.48353
[1mStep[0m  [160/169], [94mLoss[0m : 2.14431

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91768
[1mStep[0m  [16/169], [94mLoss[0m : 2.41219
[1mStep[0m  [32/169], [94mLoss[0m : 2.09998
[1mStep[0m  [48/169], [94mLoss[0m : 2.52374
[1mStep[0m  [64/169], [94mLoss[0m : 2.32358
[1mStep[0m  [80/169], [94mLoss[0m : 2.25966
[1mStep[0m  [96/169], [94mLoss[0m : 2.05884
[1mStep[0m  [112/169], [94mLoss[0m : 1.98684
[1mStep[0m  [128/169], [94mLoss[0m : 2.35460
[1mStep[0m  [144/169], [94mLoss[0m : 2.46106
[1mStep[0m  [160/169], [94mLoss[0m : 2.40279

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60872
[1mStep[0m  [16/169], [94mLoss[0m : 2.75091
[1mStep[0m  [32/169], [94mLoss[0m : 2.37933
[1mStep[0m  [48/169], [94mLoss[0m : 2.32463
[1mStep[0m  [64/169], [94mLoss[0m : 2.39281
[1mStep[0m  [80/169], [94mLoss[0m : 2.20112
[1mStep[0m  [96/169], [94mLoss[0m : 2.35271
[1mStep[0m  [112/169], [94mLoss[0m : 2.23752
[1mStep[0m  [128/169], [94mLoss[0m : 2.22957
[1mStep[0m  [144/169], [94mLoss[0m : 2.57226
[1mStep[0m  [160/169], [94mLoss[0m : 2.04228

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60794
[1mStep[0m  [16/169], [94mLoss[0m : 2.40920
[1mStep[0m  [32/169], [94mLoss[0m : 2.41156
[1mStep[0m  [48/169], [94mLoss[0m : 2.73523
[1mStep[0m  [64/169], [94mLoss[0m : 2.07759
[1mStep[0m  [80/169], [94mLoss[0m : 2.44285
[1mStep[0m  [96/169], [94mLoss[0m : 2.11430
[1mStep[0m  [112/169], [94mLoss[0m : 2.23890
[1mStep[0m  [128/169], [94mLoss[0m : 2.60400
[1mStep[0m  [144/169], [94mLoss[0m : 2.62675
[1mStep[0m  [160/169], [94mLoss[0m : 2.44076

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45032
[1mStep[0m  [16/169], [94mLoss[0m : 2.91466
[1mStep[0m  [32/169], [94mLoss[0m : 2.30016
[1mStep[0m  [48/169], [94mLoss[0m : 2.38638
[1mStep[0m  [64/169], [94mLoss[0m : 2.23652
[1mStep[0m  [80/169], [94mLoss[0m : 2.43784
[1mStep[0m  [96/169], [94mLoss[0m : 2.26312
[1mStep[0m  [112/169], [94mLoss[0m : 2.01565
[1mStep[0m  [128/169], [94mLoss[0m : 2.73930
[1mStep[0m  [144/169], [94mLoss[0m : 2.58953
[1mStep[0m  [160/169], [94mLoss[0m : 2.70498

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54283
[1mStep[0m  [16/169], [94mLoss[0m : 2.46995
[1mStep[0m  [32/169], [94mLoss[0m : 2.40082
[1mStep[0m  [48/169], [94mLoss[0m : 2.33550
[1mStep[0m  [64/169], [94mLoss[0m : 2.42620
[1mStep[0m  [80/169], [94mLoss[0m : 2.66442
[1mStep[0m  [96/169], [94mLoss[0m : 2.65842
[1mStep[0m  [112/169], [94mLoss[0m : 2.40117
[1mStep[0m  [128/169], [94mLoss[0m : 2.17848
[1mStep[0m  [144/169], [94mLoss[0m : 2.55862
[1mStep[0m  [160/169], [94mLoss[0m : 2.23726

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32291
[1mStep[0m  [16/169], [94mLoss[0m : 2.14285
[1mStep[0m  [32/169], [94mLoss[0m : 2.50882
[1mStep[0m  [48/169], [94mLoss[0m : 2.77062
[1mStep[0m  [64/169], [94mLoss[0m : 2.45928
[1mStep[0m  [80/169], [94mLoss[0m : 2.67195
[1mStep[0m  [96/169], [94mLoss[0m : 2.77944
[1mStep[0m  [112/169], [94mLoss[0m : 2.50478
[1mStep[0m  [128/169], [94mLoss[0m : 2.50800
[1mStep[0m  [144/169], [94mLoss[0m : 2.51027
[1mStep[0m  [160/169], [94mLoss[0m : 2.53511

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19333
[1mStep[0m  [16/169], [94mLoss[0m : 2.26571
[1mStep[0m  [32/169], [94mLoss[0m : 2.52612
[1mStep[0m  [48/169], [94mLoss[0m : 2.57093
[1mStep[0m  [64/169], [94mLoss[0m : 2.16886
[1mStep[0m  [80/169], [94mLoss[0m : 2.70677
[1mStep[0m  [96/169], [94mLoss[0m : 2.30653
[1mStep[0m  [112/169], [94mLoss[0m : 2.42603
[1mStep[0m  [128/169], [94mLoss[0m : 2.72665
[1mStep[0m  [144/169], [94mLoss[0m : 2.28920
[1mStep[0m  [160/169], [94mLoss[0m : 2.40588

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84925
[1mStep[0m  [16/169], [94mLoss[0m : 2.50626
[1mStep[0m  [32/169], [94mLoss[0m : 2.36194
[1mStep[0m  [48/169], [94mLoss[0m : 1.86633
[1mStep[0m  [64/169], [94mLoss[0m : 2.46275
[1mStep[0m  [80/169], [94mLoss[0m : 2.13444
[1mStep[0m  [96/169], [94mLoss[0m : 1.97758
[1mStep[0m  [112/169], [94mLoss[0m : 2.49079
[1mStep[0m  [128/169], [94mLoss[0m : 2.43767
[1mStep[0m  [144/169], [94mLoss[0m : 2.35556
[1mStep[0m  [160/169], [94mLoss[0m : 2.78789

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.319, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.360
====================================

Phase 1 - Evaluation MAE:  2.360358789563179
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.63485
[1mStep[0m  [16/169], [94mLoss[0m : 2.71252
[1mStep[0m  [32/169], [94mLoss[0m : 2.85857
[1mStep[0m  [48/169], [94mLoss[0m : 2.72402
[1mStep[0m  [64/169], [94mLoss[0m : 2.31345
[1mStep[0m  [80/169], [94mLoss[0m : 2.33797
[1mStep[0m  [96/169], [94mLoss[0m : 2.66987
[1mStep[0m  [112/169], [94mLoss[0m : 2.42117
[1mStep[0m  [128/169], [94mLoss[0m : 2.31149
[1mStep[0m  [144/169], [94mLoss[0m : 1.94095
[1mStep[0m  [160/169], [94mLoss[0m : 2.72683

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14808
[1mStep[0m  [16/169], [94mLoss[0m : 2.69254
[1mStep[0m  [32/169], [94mLoss[0m : 2.28072
[1mStep[0m  [48/169], [94mLoss[0m : 2.61323
[1mStep[0m  [64/169], [94mLoss[0m : 2.37896
[1mStep[0m  [80/169], [94mLoss[0m : 2.20256
[1mStep[0m  [96/169], [94mLoss[0m : 2.65982
[1mStep[0m  [112/169], [94mLoss[0m : 2.22407
[1mStep[0m  [128/169], [94mLoss[0m : 2.52001
[1mStep[0m  [144/169], [94mLoss[0m : 2.60339
[1mStep[0m  [160/169], [94mLoss[0m : 2.07024

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18411
[1mStep[0m  [16/169], [94mLoss[0m : 2.30964
[1mStep[0m  [32/169], [94mLoss[0m : 2.37772
[1mStep[0m  [48/169], [94mLoss[0m : 2.41970
[1mStep[0m  [64/169], [94mLoss[0m : 2.31217
[1mStep[0m  [80/169], [94mLoss[0m : 1.93859
[1mStep[0m  [96/169], [94mLoss[0m : 2.08160
[1mStep[0m  [112/169], [94mLoss[0m : 2.50065
[1mStep[0m  [128/169], [94mLoss[0m : 2.36032
[1mStep[0m  [144/169], [94mLoss[0m : 2.61037
[1mStep[0m  [160/169], [94mLoss[0m : 2.31762

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.248, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97583
[1mStep[0m  [16/169], [94mLoss[0m : 2.52022
[1mStep[0m  [32/169], [94mLoss[0m : 2.66302
[1mStep[0m  [48/169], [94mLoss[0m : 2.12271
[1mStep[0m  [64/169], [94mLoss[0m : 2.13298
[1mStep[0m  [80/169], [94mLoss[0m : 2.30271
[1mStep[0m  [96/169], [94mLoss[0m : 2.06313
[1mStep[0m  [112/169], [94mLoss[0m : 2.29921
[1mStep[0m  [128/169], [94mLoss[0m : 2.43493
[1mStep[0m  [144/169], [94mLoss[0m : 2.29072
[1mStep[0m  [160/169], [94mLoss[0m : 2.43267

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.189, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97148
[1mStep[0m  [16/169], [94mLoss[0m : 2.20971
[1mStep[0m  [32/169], [94mLoss[0m : 1.58030
[1mStep[0m  [48/169], [94mLoss[0m : 2.01173
[1mStep[0m  [64/169], [94mLoss[0m : 2.14220
[1mStep[0m  [80/169], [94mLoss[0m : 1.76744
[1mStep[0m  [96/169], [94mLoss[0m : 1.77862
[1mStep[0m  [112/169], [94mLoss[0m : 1.90479
[1mStep[0m  [128/169], [94mLoss[0m : 2.41715
[1mStep[0m  [144/169], [94mLoss[0m : 2.05696
[1mStep[0m  [160/169], [94mLoss[0m : 2.18791

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52437
[1mStep[0m  [16/169], [94mLoss[0m : 2.14406
[1mStep[0m  [32/169], [94mLoss[0m : 1.89762
[1mStep[0m  [48/169], [94mLoss[0m : 2.14061
[1mStep[0m  [64/169], [94mLoss[0m : 2.35279
[1mStep[0m  [80/169], [94mLoss[0m : 1.90364
[1mStep[0m  [96/169], [94mLoss[0m : 2.01635
[1mStep[0m  [112/169], [94mLoss[0m : 2.18722
[1mStep[0m  [128/169], [94mLoss[0m : 1.99154
[1mStep[0m  [144/169], [94mLoss[0m : 2.66458
[1mStep[0m  [160/169], [94mLoss[0m : 2.32367

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97579
[1mStep[0m  [16/169], [94mLoss[0m : 1.89661
[1mStep[0m  [32/169], [94mLoss[0m : 1.53259
[1mStep[0m  [48/169], [94mLoss[0m : 1.64673
[1mStep[0m  [64/169], [94mLoss[0m : 2.38822
[1mStep[0m  [80/169], [94mLoss[0m : 2.20998
[1mStep[0m  [96/169], [94mLoss[0m : 1.49375
[1mStep[0m  [112/169], [94mLoss[0m : 2.18880
[1mStep[0m  [128/169], [94mLoss[0m : 1.87625
[1mStep[0m  [144/169], [94mLoss[0m : 1.98725
[1mStep[0m  [160/169], [94mLoss[0m : 2.02579

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25417
[1mStep[0m  [16/169], [94mLoss[0m : 2.18873
[1mStep[0m  [32/169], [94mLoss[0m : 1.61927
[1mStep[0m  [48/169], [94mLoss[0m : 2.25612
[1mStep[0m  [64/169], [94mLoss[0m : 1.63629
[1mStep[0m  [80/169], [94mLoss[0m : 2.24546
[1mStep[0m  [96/169], [94mLoss[0m : 2.10962
[1mStep[0m  [112/169], [94mLoss[0m : 2.01626
[1mStep[0m  [128/169], [94mLoss[0m : 2.11877
[1mStep[0m  [144/169], [94mLoss[0m : 1.88180
[1mStep[0m  [160/169], [94mLoss[0m : 1.84916

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.977, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85547
[1mStep[0m  [16/169], [94mLoss[0m : 2.15704
[1mStep[0m  [32/169], [94mLoss[0m : 1.76131
[1mStep[0m  [48/169], [94mLoss[0m : 1.83011
[1mStep[0m  [64/169], [94mLoss[0m : 1.91155
[1mStep[0m  [80/169], [94mLoss[0m : 2.23291
[1mStep[0m  [96/169], [94mLoss[0m : 2.03916
[1mStep[0m  [112/169], [94mLoss[0m : 1.64246
[1mStep[0m  [128/169], [94mLoss[0m : 1.69730
[1mStep[0m  [144/169], [94mLoss[0m : 1.78642
[1mStep[0m  [160/169], [94mLoss[0m : 2.25534

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.902, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66075
[1mStep[0m  [16/169], [94mLoss[0m : 1.68592
[1mStep[0m  [32/169], [94mLoss[0m : 1.95856
[1mStep[0m  [48/169], [94mLoss[0m : 1.35865
[1mStep[0m  [64/169], [94mLoss[0m : 1.82170
[1mStep[0m  [80/169], [94mLoss[0m : 1.51649
[1mStep[0m  [96/169], [94mLoss[0m : 1.80139
[1mStep[0m  [112/169], [94mLoss[0m : 2.01224
[1mStep[0m  [128/169], [94mLoss[0m : 1.76810
[1mStep[0m  [144/169], [94mLoss[0m : 2.20362
[1mStep[0m  [160/169], [94mLoss[0m : 2.13152

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.856, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65846
[1mStep[0m  [16/169], [94mLoss[0m : 1.42021
[1mStep[0m  [32/169], [94mLoss[0m : 2.02784
[1mStep[0m  [48/169], [94mLoss[0m : 1.68192
[1mStep[0m  [64/169], [94mLoss[0m : 1.58443
[1mStep[0m  [80/169], [94mLoss[0m : 2.17016
[1mStep[0m  [96/169], [94mLoss[0m : 1.93996
[1mStep[0m  [112/169], [94mLoss[0m : 1.92841
[1mStep[0m  [128/169], [94mLoss[0m : 1.84026
[1mStep[0m  [144/169], [94mLoss[0m : 1.42636
[1mStep[0m  [160/169], [94mLoss[0m : 1.56700

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.811, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79063
[1mStep[0m  [16/169], [94mLoss[0m : 1.73259
[1mStep[0m  [32/169], [94mLoss[0m : 1.88344
[1mStep[0m  [48/169], [94mLoss[0m : 1.71164
[1mStep[0m  [64/169], [94mLoss[0m : 1.96075
[1mStep[0m  [80/169], [94mLoss[0m : 2.25003
[1mStep[0m  [96/169], [94mLoss[0m : 1.82673
[1mStep[0m  [112/169], [94mLoss[0m : 2.17287
[1mStep[0m  [128/169], [94mLoss[0m : 2.13373
[1mStep[0m  [144/169], [94mLoss[0m : 1.80757
[1mStep[0m  [160/169], [94mLoss[0m : 1.40616

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45422
[1mStep[0m  [16/169], [94mLoss[0m : 1.46644
[1mStep[0m  [32/169], [94mLoss[0m : 1.84382
[1mStep[0m  [48/169], [94mLoss[0m : 1.69317
[1mStep[0m  [64/169], [94mLoss[0m : 2.03884
[1mStep[0m  [80/169], [94mLoss[0m : 1.49779
[1mStep[0m  [96/169], [94mLoss[0m : 2.02353
[1mStep[0m  [112/169], [94mLoss[0m : 1.82756
[1mStep[0m  [128/169], [94mLoss[0m : 1.44558
[1mStep[0m  [144/169], [94mLoss[0m : 1.53336
[1mStep[0m  [160/169], [94mLoss[0m : 1.51879

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.732, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88795
[1mStep[0m  [16/169], [94mLoss[0m : 1.58929
[1mStep[0m  [32/169], [94mLoss[0m : 1.39158
[1mStep[0m  [48/169], [94mLoss[0m : 1.46724
[1mStep[0m  [64/169], [94mLoss[0m : 1.66307
[1mStep[0m  [80/169], [94mLoss[0m : 1.73566
[1mStep[0m  [96/169], [94mLoss[0m : 1.89527
[1mStep[0m  [112/169], [94mLoss[0m : 1.52317
[1mStep[0m  [128/169], [94mLoss[0m : 1.77430
[1mStep[0m  [144/169], [94mLoss[0m : 1.86873
[1mStep[0m  [160/169], [94mLoss[0m : 1.86887

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.45538
[1mStep[0m  [16/169], [94mLoss[0m : 1.66224
[1mStep[0m  [32/169], [94mLoss[0m : 1.68444
[1mStep[0m  [48/169], [94mLoss[0m : 1.72405
[1mStep[0m  [64/169], [94mLoss[0m : 1.84102
[1mStep[0m  [80/169], [94mLoss[0m : 1.55149
[1mStep[0m  [96/169], [94mLoss[0m : 1.47185
[1mStep[0m  [112/169], [94mLoss[0m : 1.49176
[1mStep[0m  [128/169], [94mLoss[0m : 1.63946
[1mStep[0m  [144/169], [94mLoss[0m : 1.37759
[1mStep[0m  [160/169], [94mLoss[0m : 2.16906

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59741
[1mStep[0m  [16/169], [94mLoss[0m : 1.48124
[1mStep[0m  [32/169], [94mLoss[0m : 1.34849
[1mStep[0m  [48/169], [94mLoss[0m : 1.62642
[1mStep[0m  [64/169], [94mLoss[0m : 2.14051
[1mStep[0m  [80/169], [94mLoss[0m : 1.49309
[1mStep[0m  [96/169], [94mLoss[0m : 1.47412
[1mStep[0m  [112/169], [94mLoss[0m : 1.73040
[1mStep[0m  [128/169], [94mLoss[0m : 1.57992
[1mStep[0m  [144/169], [94mLoss[0m : 1.45904
[1mStep[0m  [160/169], [94mLoss[0m : 1.87865

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97941
[1mStep[0m  [16/169], [94mLoss[0m : 1.48380
[1mStep[0m  [32/169], [94mLoss[0m : 1.51002
[1mStep[0m  [48/169], [94mLoss[0m : 1.45979
[1mStep[0m  [64/169], [94mLoss[0m : 1.45274
[1mStep[0m  [80/169], [94mLoss[0m : 1.42843
[1mStep[0m  [96/169], [94mLoss[0m : 1.76871
[1mStep[0m  [112/169], [94mLoss[0m : 1.57081
[1mStep[0m  [128/169], [94mLoss[0m : 1.68587
[1mStep[0m  [144/169], [94mLoss[0m : 1.59458
[1mStep[0m  [160/169], [94mLoss[0m : 1.61504

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87129
[1mStep[0m  [16/169], [94mLoss[0m : 1.83595
[1mStep[0m  [32/169], [94mLoss[0m : 1.95497
[1mStep[0m  [48/169], [94mLoss[0m : 1.54661
[1mStep[0m  [64/169], [94mLoss[0m : 1.31327
[1mStep[0m  [80/169], [94mLoss[0m : 1.94231
[1mStep[0m  [96/169], [94mLoss[0m : 1.64537
[1mStep[0m  [112/169], [94mLoss[0m : 1.59284
[1mStep[0m  [128/169], [94mLoss[0m : 1.31634
[1mStep[0m  [144/169], [94mLoss[0m : 1.94375
[1mStep[0m  [160/169], [94mLoss[0m : 1.76813

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70690
[1mStep[0m  [16/169], [94mLoss[0m : 1.59127
[1mStep[0m  [32/169], [94mLoss[0m : 1.49957
[1mStep[0m  [48/169], [94mLoss[0m : 1.60998
[1mStep[0m  [64/169], [94mLoss[0m : 1.59596
[1mStep[0m  [80/169], [94mLoss[0m : 1.88061
[1mStep[0m  [96/169], [94mLoss[0m : 1.54312
[1mStep[0m  [112/169], [94mLoss[0m : 1.28178
[1mStep[0m  [128/169], [94mLoss[0m : 1.37976
[1mStep[0m  [144/169], [94mLoss[0m : 1.95871
[1mStep[0m  [160/169], [94mLoss[0m : 1.59622

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.44240
[1mStep[0m  [16/169], [94mLoss[0m : 1.21681
[1mStep[0m  [32/169], [94mLoss[0m : 1.56991
[1mStep[0m  [48/169], [94mLoss[0m : 1.55471
[1mStep[0m  [64/169], [94mLoss[0m : 1.46768
[1mStep[0m  [80/169], [94mLoss[0m : 1.60362
[1mStep[0m  [96/169], [94mLoss[0m : 1.35946
[1mStep[0m  [112/169], [94mLoss[0m : 1.46259
[1mStep[0m  [128/169], [94mLoss[0m : 1.76974
[1mStep[0m  [144/169], [94mLoss[0m : 1.41825
[1mStep[0m  [160/169], [94mLoss[0m : 1.32466

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.520, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41778
[1mStep[0m  [16/169], [94mLoss[0m : 1.66578
[1mStep[0m  [32/169], [94mLoss[0m : 1.37294
[1mStep[0m  [48/169], [94mLoss[0m : 1.28581
[1mStep[0m  [64/169], [94mLoss[0m : 1.54960
[1mStep[0m  [80/169], [94mLoss[0m : 1.40778
[1mStep[0m  [96/169], [94mLoss[0m : 1.60402
[1mStep[0m  [112/169], [94mLoss[0m : 1.38276
[1mStep[0m  [128/169], [94mLoss[0m : 1.42899
[1mStep[0m  [144/169], [94mLoss[0m : 1.49703
[1mStep[0m  [160/169], [94mLoss[0m : 1.63008

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.491, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.35964
[1mStep[0m  [16/169], [94mLoss[0m : 1.58702
[1mStep[0m  [32/169], [94mLoss[0m : 1.65415
[1mStep[0m  [48/169], [94mLoss[0m : 1.16731
[1mStep[0m  [64/169], [94mLoss[0m : 1.36617
[1mStep[0m  [80/169], [94mLoss[0m : 1.79611
[1mStep[0m  [96/169], [94mLoss[0m : 1.65629
[1mStep[0m  [112/169], [94mLoss[0m : 1.34626
[1mStep[0m  [128/169], [94mLoss[0m : 1.50693
[1mStep[0m  [144/169], [94mLoss[0m : 1.47602
[1mStep[0m  [160/169], [94mLoss[0m : 1.33946

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.465, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63207
[1mStep[0m  [16/169], [94mLoss[0m : 1.29992
[1mStep[0m  [32/169], [94mLoss[0m : 1.25462
[1mStep[0m  [48/169], [94mLoss[0m : 1.37145
[1mStep[0m  [64/169], [94mLoss[0m : 1.38574
[1mStep[0m  [80/169], [94mLoss[0m : 1.14531
[1mStep[0m  [96/169], [94mLoss[0m : 1.24172
[1mStep[0m  [112/169], [94mLoss[0m : 1.45028
[1mStep[0m  [128/169], [94mLoss[0m : 1.45756
[1mStep[0m  [144/169], [94mLoss[0m : 1.95133
[1mStep[0m  [160/169], [94mLoss[0m : 1.41375

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.446, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.25279
[1mStep[0m  [16/169], [94mLoss[0m : 1.38710
[1mStep[0m  [32/169], [94mLoss[0m : 1.60969
[1mStep[0m  [48/169], [94mLoss[0m : 1.60628
[1mStep[0m  [64/169], [94mLoss[0m : 1.54285
[1mStep[0m  [80/169], [94mLoss[0m : 1.27634
[1mStep[0m  [96/169], [94mLoss[0m : 1.21596
[1mStep[0m  [112/169], [94mLoss[0m : 1.66299
[1mStep[0m  [128/169], [94mLoss[0m : 1.27009
[1mStep[0m  [144/169], [94mLoss[0m : 1.29715
[1mStep[0m  [160/169], [94mLoss[0m : 1.26171

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.414, [92mTest[0m: 2.493, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.43273
[1mStep[0m  [16/169], [94mLoss[0m : 1.47167
[1mStep[0m  [32/169], [94mLoss[0m : 1.12063
[1mStep[0m  [48/169], [94mLoss[0m : 1.28035
[1mStep[0m  [64/169], [94mLoss[0m : 1.31080
[1mStep[0m  [80/169], [94mLoss[0m : 1.64704
[1mStep[0m  [96/169], [94mLoss[0m : 1.49127
[1mStep[0m  [112/169], [94mLoss[0m : 1.35874
[1mStep[0m  [128/169], [94mLoss[0m : 1.54957
[1mStep[0m  [144/169], [94mLoss[0m : 1.56427
[1mStep[0m  [160/169], [94mLoss[0m : 1.87586

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.415, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47146
[1mStep[0m  [16/169], [94mLoss[0m : 1.23702
[1mStep[0m  [32/169], [94mLoss[0m : 1.63267
[1mStep[0m  [48/169], [94mLoss[0m : 1.46103
[1mStep[0m  [64/169], [94mLoss[0m : 1.55163
[1mStep[0m  [80/169], [94mLoss[0m : 1.35503
[1mStep[0m  [96/169], [94mLoss[0m : 1.34615
[1mStep[0m  [112/169], [94mLoss[0m : 1.40068
[1mStep[0m  [128/169], [94mLoss[0m : 1.30224
[1mStep[0m  [144/169], [94mLoss[0m : 1.31280
[1mStep[0m  [160/169], [94mLoss[0m : 1.56102

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.387, [92mTest[0m: 2.493, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.475
====================================

Phase 2 - Evaluation MAE:  2.4754333794116974
MAE score P1      2.360359
MAE score P2      2.475433
loss              1.386894
learning_rate     0.002575
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.70956
[1mStep[0m  [8/84], [94mLoss[0m : 10.88522
[1mStep[0m  [16/84], [94mLoss[0m : 10.89203
[1mStep[0m  [24/84], [94mLoss[0m : 10.48383
[1mStep[0m  [32/84], [94mLoss[0m : 10.09564
[1mStep[0m  [40/84], [94mLoss[0m : 11.27271
[1mStep[0m  [48/84], [94mLoss[0m : 10.95270
[1mStep[0m  [56/84], [94mLoss[0m : 11.03191
[1mStep[0m  [64/84], [94mLoss[0m : 10.77660
[1mStep[0m  [72/84], [94mLoss[0m : 10.70632
[1mStep[0m  [80/84], [94mLoss[0m : 10.65937

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.717, [92mTest[0m: 10.937, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.05088
[1mStep[0m  [8/84], [94mLoss[0m : 10.41518
[1mStep[0m  [16/84], [94mLoss[0m : 11.15003
[1mStep[0m  [24/84], [94mLoss[0m : 10.49620
[1mStep[0m  [32/84], [94mLoss[0m : 10.07523
[1mStep[0m  [40/84], [94mLoss[0m : 10.84122
[1mStep[0m  [48/84], [94mLoss[0m : 10.34764
[1mStep[0m  [56/84], [94mLoss[0m : 10.67508
[1mStep[0m  [64/84], [94mLoss[0m : 9.64862
[1mStep[0m  [72/84], [94mLoss[0m : 9.66563
[1mStep[0m  [80/84], [94mLoss[0m : 10.16548

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.179, [92mTest[0m: 10.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.45273
[1mStep[0m  [8/84], [94mLoss[0m : 10.00238
[1mStep[0m  [16/84], [94mLoss[0m : 9.59241
[1mStep[0m  [24/84], [94mLoss[0m : 9.40751
[1mStep[0m  [32/84], [94mLoss[0m : 9.74065
[1mStep[0m  [40/84], [94mLoss[0m : 10.30893
[1mStep[0m  [48/84], [94mLoss[0m : 9.69471
[1mStep[0m  [56/84], [94mLoss[0m : 9.41611
[1mStep[0m  [64/84], [94mLoss[0m : 9.35916
[1mStep[0m  [72/84], [94mLoss[0m : 9.31663
[1mStep[0m  [80/84], [94mLoss[0m : 9.35239

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.554, [92mTest[0m: 9.704, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.05853
[1mStep[0m  [8/84], [94mLoss[0m : 9.24388
[1mStep[0m  [16/84], [94mLoss[0m : 8.52060
[1mStep[0m  [24/84], [94mLoss[0m : 9.00360
[1mStep[0m  [32/84], [94mLoss[0m : 8.78896
[1mStep[0m  [40/84], [94mLoss[0m : 8.64025
[1mStep[0m  [48/84], [94mLoss[0m : 8.77084
[1mStep[0m  [56/84], [94mLoss[0m : 8.32354
[1mStep[0m  [64/84], [94mLoss[0m : 8.71217
[1mStep[0m  [72/84], [94mLoss[0m : 8.86902
[1mStep[0m  [80/84], [94mLoss[0m : 8.35241

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.793, [92mTest[0m: 8.895, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.04307
[1mStep[0m  [8/84], [94mLoss[0m : 7.91289
[1mStep[0m  [16/84], [94mLoss[0m : 7.62468
[1mStep[0m  [24/84], [94mLoss[0m : 7.83207
[1mStep[0m  [32/84], [94mLoss[0m : 8.42036
[1mStep[0m  [40/84], [94mLoss[0m : 7.88001
[1mStep[0m  [48/84], [94mLoss[0m : 7.36835
[1mStep[0m  [56/84], [94mLoss[0m : 7.61594
[1mStep[0m  [64/84], [94mLoss[0m : 7.34262
[1mStep[0m  [72/84], [94mLoss[0m : 7.31695
[1mStep[0m  [80/84], [94mLoss[0m : 6.70039

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.729, [92mTest[0m: 7.879, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.68420
[1mStep[0m  [8/84], [94mLoss[0m : 6.92449
[1mStep[0m  [16/84], [94mLoss[0m : 6.70210
[1mStep[0m  [24/84], [94mLoss[0m : 7.22530
[1mStep[0m  [32/84], [94mLoss[0m : 6.55268
[1mStep[0m  [40/84], [94mLoss[0m : 6.14026
[1mStep[0m  [48/84], [94mLoss[0m : 6.54983
[1mStep[0m  [56/84], [94mLoss[0m : 6.20539
[1mStep[0m  [64/84], [94mLoss[0m : 6.32732
[1mStep[0m  [72/84], [94mLoss[0m : 5.83915
[1mStep[0m  [80/84], [94mLoss[0m : 6.11664

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.553, [92mTest[0m: 6.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.31451
[1mStep[0m  [8/84], [94mLoss[0m : 6.03077
[1mStep[0m  [16/84], [94mLoss[0m : 5.76130
[1mStep[0m  [24/84], [94mLoss[0m : 6.10716
[1mStep[0m  [32/84], [94mLoss[0m : 5.93422
[1mStep[0m  [40/84], [94mLoss[0m : 6.08825
[1mStep[0m  [48/84], [94mLoss[0m : 5.68823
[1mStep[0m  [56/84], [94mLoss[0m : 5.09453
[1mStep[0m  [64/84], [94mLoss[0m : 5.17426
[1mStep[0m  [72/84], [94mLoss[0m : 5.25190
[1mStep[0m  [80/84], [94mLoss[0m : 5.37904

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.531, [92mTest[0m: 5.209, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.63241
[1mStep[0m  [8/84], [94mLoss[0m : 4.64641
[1mStep[0m  [16/84], [94mLoss[0m : 5.11781
[1mStep[0m  [24/84], [94mLoss[0m : 4.84098
[1mStep[0m  [32/84], [94mLoss[0m : 4.81760
[1mStep[0m  [40/84], [94mLoss[0m : 4.50586
[1mStep[0m  [48/84], [94mLoss[0m : 5.17617
[1mStep[0m  [56/84], [94mLoss[0m : 4.15388
[1mStep[0m  [64/84], [94mLoss[0m : 3.91266
[1mStep[0m  [72/84], [94mLoss[0m : 4.62403
[1mStep[0m  [80/84], [94mLoss[0m : 4.19004

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.594, [92mTest[0m: 4.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.66349
[1mStep[0m  [8/84], [94mLoss[0m : 4.44287
[1mStep[0m  [16/84], [94mLoss[0m : 4.11604
[1mStep[0m  [24/84], [94mLoss[0m : 4.16418
[1mStep[0m  [32/84], [94mLoss[0m : 3.84189
[1mStep[0m  [40/84], [94mLoss[0m : 3.77817
[1mStep[0m  [48/84], [94mLoss[0m : 3.45803
[1mStep[0m  [56/84], [94mLoss[0m : 3.47101
[1mStep[0m  [64/84], [94mLoss[0m : 3.53169
[1mStep[0m  [72/84], [94mLoss[0m : 3.42147
[1mStep[0m  [80/84], [94mLoss[0m : 3.21284

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.815, [92mTest[0m: 3.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.50140
[1mStep[0m  [8/84], [94mLoss[0m : 3.40511
[1mStep[0m  [16/84], [94mLoss[0m : 2.93350
[1mStep[0m  [24/84], [94mLoss[0m : 2.69482
[1mStep[0m  [32/84], [94mLoss[0m : 3.59868
[1mStep[0m  [40/84], [94mLoss[0m : 3.19066
[1mStep[0m  [48/84], [94mLoss[0m : 3.05893
[1mStep[0m  [56/84], [94mLoss[0m : 3.05832
[1mStep[0m  [64/84], [94mLoss[0m : 2.96632
[1mStep[0m  [72/84], [94mLoss[0m : 3.11433
[1mStep[0m  [80/84], [94mLoss[0m : 3.27564

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 3.225, [92mTest[0m: 3.028, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.19905
[1mStep[0m  [8/84], [94mLoss[0m : 3.15063
[1mStep[0m  [16/84], [94mLoss[0m : 2.63318
[1mStep[0m  [24/84], [94mLoss[0m : 2.76448
[1mStep[0m  [32/84], [94mLoss[0m : 2.92110
[1mStep[0m  [40/84], [94mLoss[0m : 2.81885
[1mStep[0m  [48/84], [94mLoss[0m : 2.63167
[1mStep[0m  [56/84], [94mLoss[0m : 2.41669
[1mStep[0m  [64/84], [94mLoss[0m : 3.37792
[1mStep[0m  [72/84], [94mLoss[0m : 2.70578
[1mStep[0m  [80/84], [94mLoss[0m : 2.46277

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.828, [92mTest[0m: 2.638, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78919
[1mStep[0m  [8/84], [94mLoss[0m : 2.53250
[1mStep[0m  [16/84], [94mLoss[0m : 2.77011
[1mStep[0m  [24/84], [94mLoss[0m : 2.60902
[1mStep[0m  [32/84], [94mLoss[0m : 2.70813
[1mStep[0m  [40/84], [94mLoss[0m : 2.70445
[1mStep[0m  [48/84], [94mLoss[0m : 2.98984
[1mStep[0m  [56/84], [94mLoss[0m : 2.61612
[1mStep[0m  [64/84], [94mLoss[0m : 2.46664
[1mStep[0m  [72/84], [94mLoss[0m : 2.57918
[1mStep[0m  [80/84], [94mLoss[0m : 2.65940

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.469, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59011
[1mStep[0m  [8/84], [94mLoss[0m : 2.72670
[1mStep[0m  [16/84], [94mLoss[0m : 2.20362
[1mStep[0m  [24/84], [94mLoss[0m : 2.56565
[1mStep[0m  [32/84], [94mLoss[0m : 2.31643
[1mStep[0m  [40/84], [94mLoss[0m : 2.49140
[1mStep[0m  [48/84], [94mLoss[0m : 2.68458
[1mStep[0m  [56/84], [94mLoss[0m : 2.88464
[1mStep[0m  [64/84], [94mLoss[0m : 2.39021
[1mStep[0m  [72/84], [94mLoss[0m : 2.31915
[1mStep[0m  [80/84], [94mLoss[0m : 3.07768

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.629, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26712
[1mStep[0m  [8/84], [94mLoss[0m : 2.46238
[1mStep[0m  [16/84], [94mLoss[0m : 2.71897
[1mStep[0m  [24/84], [94mLoss[0m : 2.70540
[1mStep[0m  [32/84], [94mLoss[0m : 2.87245
[1mStep[0m  [40/84], [94mLoss[0m : 2.56462
[1mStep[0m  [48/84], [94mLoss[0m : 2.61228
[1mStep[0m  [56/84], [94mLoss[0m : 2.51393
[1mStep[0m  [64/84], [94mLoss[0m : 2.64700
[1mStep[0m  [72/84], [94mLoss[0m : 2.51167
[1mStep[0m  [80/84], [94mLoss[0m : 2.48720

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47771
[1mStep[0m  [8/84], [94mLoss[0m : 2.66446
[1mStep[0m  [16/84], [94mLoss[0m : 2.47709
[1mStep[0m  [24/84], [94mLoss[0m : 2.75041
[1mStep[0m  [32/84], [94mLoss[0m : 2.57436
[1mStep[0m  [40/84], [94mLoss[0m : 2.79250
[1mStep[0m  [48/84], [94mLoss[0m : 2.91044
[1mStep[0m  [56/84], [94mLoss[0m : 2.44100
[1mStep[0m  [64/84], [94mLoss[0m : 2.42835
[1mStep[0m  [72/84], [94mLoss[0m : 2.68612
[1mStep[0m  [80/84], [94mLoss[0m : 2.61437

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26549
[1mStep[0m  [8/84], [94mLoss[0m : 2.26892
[1mStep[0m  [16/84], [94mLoss[0m : 2.45060
[1mStep[0m  [24/84], [94mLoss[0m : 2.51930
[1mStep[0m  [32/84], [94mLoss[0m : 2.77535
[1mStep[0m  [40/84], [94mLoss[0m : 2.54648
[1mStep[0m  [48/84], [94mLoss[0m : 2.40407
[1mStep[0m  [56/84], [94mLoss[0m : 2.93297
[1mStep[0m  [64/84], [94mLoss[0m : 2.36366
[1mStep[0m  [72/84], [94mLoss[0m : 2.76358
[1mStep[0m  [80/84], [94mLoss[0m : 2.28629

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67476
[1mStep[0m  [8/84], [94mLoss[0m : 2.35919
[1mStep[0m  [16/84], [94mLoss[0m : 2.49126
[1mStep[0m  [24/84], [94mLoss[0m : 2.38364
[1mStep[0m  [32/84], [94mLoss[0m : 2.76429
[1mStep[0m  [40/84], [94mLoss[0m : 2.88209
[1mStep[0m  [48/84], [94mLoss[0m : 2.67011
[1mStep[0m  [56/84], [94mLoss[0m : 2.73269
[1mStep[0m  [64/84], [94mLoss[0m : 2.62432
[1mStep[0m  [72/84], [94mLoss[0m : 2.30835
[1mStep[0m  [80/84], [94mLoss[0m : 2.84824

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46769
[1mStep[0m  [8/84], [94mLoss[0m : 2.23594
[1mStep[0m  [16/84], [94mLoss[0m : 2.41612
[1mStep[0m  [24/84], [94mLoss[0m : 2.45426
[1mStep[0m  [32/84], [94mLoss[0m : 2.71062
[1mStep[0m  [40/84], [94mLoss[0m : 2.72681
[1mStep[0m  [48/84], [94mLoss[0m : 2.36401
[1mStep[0m  [56/84], [94mLoss[0m : 2.29951
[1mStep[0m  [64/84], [94mLoss[0m : 2.46494
[1mStep[0m  [72/84], [94mLoss[0m : 2.79393
[1mStep[0m  [80/84], [94mLoss[0m : 2.27663

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.57177
[1mStep[0m  [8/84], [94mLoss[0m : 2.50654
[1mStep[0m  [16/84], [94mLoss[0m : 2.33130
[1mStep[0m  [24/84], [94mLoss[0m : 2.50027
[1mStep[0m  [32/84], [94mLoss[0m : 2.63761
[1mStep[0m  [40/84], [94mLoss[0m : 2.38099
[1mStep[0m  [48/84], [94mLoss[0m : 2.40894
[1mStep[0m  [56/84], [94mLoss[0m : 2.34922
[1mStep[0m  [64/84], [94mLoss[0m : 2.67435
[1mStep[0m  [72/84], [94mLoss[0m : 2.96560
[1mStep[0m  [80/84], [94mLoss[0m : 2.44719

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.374, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64577
[1mStep[0m  [8/84], [94mLoss[0m : 2.64780
[1mStep[0m  [16/84], [94mLoss[0m : 2.86488
[1mStep[0m  [24/84], [94mLoss[0m : 2.26267
[1mStep[0m  [32/84], [94mLoss[0m : 2.26913
[1mStep[0m  [40/84], [94mLoss[0m : 2.52035
[1mStep[0m  [48/84], [94mLoss[0m : 2.63258
[1mStep[0m  [56/84], [94mLoss[0m : 2.54039
[1mStep[0m  [64/84], [94mLoss[0m : 2.75235
[1mStep[0m  [72/84], [94mLoss[0m : 2.31941
[1mStep[0m  [80/84], [94mLoss[0m : 2.43222

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49998
[1mStep[0m  [8/84], [94mLoss[0m : 2.39197
[1mStep[0m  [16/84], [94mLoss[0m : 2.26966
[1mStep[0m  [24/84], [94mLoss[0m : 2.62324
[1mStep[0m  [32/84], [94mLoss[0m : 2.42682
[1mStep[0m  [40/84], [94mLoss[0m : 2.42409
[1mStep[0m  [48/84], [94mLoss[0m : 2.84388
[1mStep[0m  [56/84], [94mLoss[0m : 2.44427
[1mStep[0m  [64/84], [94mLoss[0m : 2.66052
[1mStep[0m  [72/84], [94mLoss[0m : 2.90586
[1mStep[0m  [80/84], [94mLoss[0m : 2.62183

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.357, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.74110
[1mStep[0m  [8/84], [94mLoss[0m : 2.29486
[1mStep[0m  [16/84], [94mLoss[0m : 2.64744
[1mStep[0m  [24/84], [94mLoss[0m : 2.79605
[1mStep[0m  [32/84], [94mLoss[0m : 2.77039
[1mStep[0m  [40/84], [94mLoss[0m : 2.71633
[1mStep[0m  [48/84], [94mLoss[0m : 2.64692
[1mStep[0m  [56/84], [94mLoss[0m : 2.60610
[1mStep[0m  [64/84], [94mLoss[0m : 2.62320
[1mStep[0m  [72/84], [94mLoss[0m : 2.34801
[1mStep[0m  [80/84], [94mLoss[0m : 2.62498

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34984
[1mStep[0m  [8/84], [94mLoss[0m : 2.60286
[1mStep[0m  [16/84], [94mLoss[0m : 2.46437
[1mStep[0m  [24/84], [94mLoss[0m : 2.57418
[1mStep[0m  [32/84], [94mLoss[0m : 2.47013
[1mStep[0m  [40/84], [94mLoss[0m : 2.67624
[1mStep[0m  [48/84], [94mLoss[0m : 2.38492
[1mStep[0m  [56/84], [94mLoss[0m : 2.65031
[1mStep[0m  [64/84], [94mLoss[0m : 2.69994
[1mStep[0m  [72/84], [94mLoss[0m : 2.73547
[1mStep[0m  [80/84], [94mLoss[0m : 2.66581

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59190
[1mStep[0m  [8/84], [94mLoss[0m : 2.61124
[1mStep[0m  [16/84], [94mLoss[0m : 2.28779
[1mStep[0m  [24/84], [94mLoss[0m : 2.59873
[1mStep[0m  [32/84], [94mLoss[0m : 2.63201
[1mStep[0m  [40/84], [94mLoss[0m : 2.62279
[1mStep[0m  [48/84], [94mLoss[0m : 2.21572
[1mStep[0m  [56/84], [94mLoss[0m : 2.42563
[1mStep[0m  [64/84], [94mLoss[0m : 2.33425
[1mStep[0m  [72/84], [94mLoss[0m : 2.46864
[1mStep[0m  [80/84], [94mLoss[0m : 2.48296

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.516, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37491
[1mStep[0m  [8/84], [94mLoss[0m : 2.56996
[1mStep[0m  [16/84], [94mLoss[0m : 2.44642
[1mStep[0m  [24/84], [94mLoss[0m : 2.49396
[1mStep[0m  [32/84], [94mLoss[0m : 2.51600
[1mStep[0m  [40/84], [94mLoss[0m : 2.35827
[1mStep[0m  [48/84], [94mLoss[0m : 2.62203
[1mStep[0m  [56/84], [94mLoss[0m : 2.43047
[1mStep[0m  [64/84], [94mLoss[0m : 2.53157
[1mStep[0m  [72/84], [94mLoss[0m : 2.39177
[1mStep[0m  [80/84], [94mLoss[0m : 2.53218

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69473
[1mStep[0m  [8/84], [94mLoss[0m : 2.61283
[1mStep[0m  [16/84], [94mLoss[0m : 2.39840
[1mStep[0m  [24/84], [94mLoss[0m : 2.81471
[1mStep[0m  [32/84], [94mLoss[0m : 2.82746
[1mStep[0m  [40/84], [94mLoss[0m : 2.71497
[1mStep[0m  [48/84], [94mLoss[0m : 2.49391
[1mStep[0m  [56/84], [94mLoss[0m : 2.49628
[1mStep[0m  [64/84], [94mLoss[0m : 2.36299
[1mStep[0m  [72/84], [94mLoss[0m : 2.48762
[1mStep[0m  [80/84], [94mLoss[0m : 2.41891

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31779
[1mStep[0m  [8/84], [94mLoss[0m : 2.34315
[1mStep[0m  [16/84], [94mLoss[0m : 2.60855
[1mStep[0m  [24/84], [94mLoss[0m : 2.79404
[1mStep[0m  [32/84], [94mLoss[0m : 2.67901
[1mStep[0m  [40/84], [94mLoss[0m : 2.56420
[1mStep[0m  [48/84], [94mLoss[0m : 2.64291
[1mStep[0m  [56/84], [94mLoss[0m : 2.32740
[1mStep[0m  [64/84], [94mLoss[0m : 2.82469
[1mStep[0m  [72/84], [94mLoss[0m : 2.08829
[1mStep[0m  [80/84], [94mLoss[0m : 2.46926

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17663
[1mStep[0m  [8/84], [94mLoss[0m : 2.84302
[1mStep[0m  [16/84], [94mLoss[0m : 2.34158
[1mStep[0m  [24/84], [94mLoss[0m : 2.13715
[1mStep[0m  [32/84], [94mLoss[0m : 2.45082
[1mStep[0m  [40/84], [94mLoss[0m : 2.72334
[1mStep[0m  [48/84], [94mLoss[0m : 2.54145
[1mStep[0m  [56/84], [94mLoss[0m : 2.43715
[1mStep[0m  [64/84], [94mLoss[0m : 2.45567
[1mStep[0m  [72/84], [94mLoss[0m : 2.46735
[1mStep[0m  [80/84], [94mLoss[0m : 2.50053

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.339, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59692
[1mStep[0m  [8/84], [94mLoss[0m : 2.48844
[1mStep[0m  [16/84], [94mLoss[0m : 2.50621
[1mStep[0m  [24/84], [94mLoss[0m : 2.55867
[1mStep[0m  [32/84], [94mLoss[0m : 2.32447
[1mStep[0m  [40/84], [94mLoss[0m : 2.34285
[1mStep[0m  [48/84], [94mLoss[0m : 2.20456
[1mStep[0m  [56/84], [94mLoss[0m : 2.52702
[1mStep[0m  [64/84], [94mLoss[0m : 2.39989
[1mStep[0m  [72/84], [94mLoss[0m : 2.08309
[1mStep[0m  [80/84], [94mLoss[0m : 2.22243

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36922
[1mStep[0m  [8/84], [94mLoss[0m : 2.78666
[1mStep[0m  [16/84], [94mLoss[0m : 2.68898
[1mStep[0m  [24/84], [94mLoss[0m : 2.25995
[1mStep[0m  [32/84], [94mLoss[0m : 2.39778
[1mStep[0m  [40/84], [94mLoss[0m : 2.77652
[1mStep[0m  [48/84], [94mLoss[0m : 2.12063
[1mStep[0m  [56/84], [94mLoss[0m : 2.62774
[1mStep[0m  [64/84], [94mLoss[0m : 2.67800
[1mStep[0m  [72/84], [94mLoss[0m : 2.20445
[1mStep[0m  [80/84], [94mLoss[0m : 2.47792

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.342
====================================

Phase 1 - Evaluation MAE:  2.342173772198813
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.20493
[1mStep[0m  [8/84], [94mLoss[0m : 2.54520
[1mStep[0m  [16/84], [94mLoss[0m : 2.28767
[1mStep[0m  [24/84], [94mLoss[0m : 2.94884
[1mStep[0m  [32/84], [94mLoss[0m : 2.33040
[1mStep[0m  [40/84], [94mLoss[0m : 2.56264
[1mStep[0m  [48/84], [94mLoss[0m : 2.53482
[1mStep[0m  [56/84], [94mLoss[0m : 2.55740
[1mStep[0m  [64/84], [94mLoss[0m : 2.35047
[1mStep[0m  [72/84], [94mLoss[0m : 2.48942
[1mStep[0m  [80/84], [94mLoss[0m : 2.60394

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55305
[1mStep[0m  [8/84], [94mLoss[0m : 2.36201
[1mStep[0m  [16/84], [94mLoss[0m : 2.51230
[1mStep[0m  [24/84], [94mLoss[0m : 2.51909
[1mStep[0m  [32/84], [94mLoss[0m : 2.70617
[1mStep[0m  [40/84], [94mLoss[0m : 2.43925
[1mStep[0m  [48/84], [94mLoss[0m : 2.49504
[1mStep[0m  [56/84], [94mLoss[0m : 2.62124
[1mStep[0m  [64/84], [94mLoss[0m : 2.51502
[1mStep[0m  [72/84], [94mLoss[0m : 2.41793
[1mStep[0m  [80/84], [94mLoss[0m : 2.55485

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36647
[1mStep[0m  [8/84], [94mLoss[0m : 2.57582
[1mStep[0m  [16/84], [94mLoss[0m : 2.82176
[1mStep[0m  [24/84], [94mLoss[0m : 2.87347
[1mStep[0m  [32/84], [94mLoss[0m : 2.55354
[1mStep[0m  [40/84], [94mLoss[0m : 2.72629
[1mStep[0m  [48/84], [94mLoss[0m : 2.55435
[1mStep[0m  [56/84], [94mLoss[0m : 2.47735
[1mStep[0m  [64/84], [94mLoss[0m : 2.67935
[1mStep[0m  [72/84], [94mLoss[0m : 2.26110
[1mStep[0m  [80/84], [94mLoss[0m : 2.35951

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34349
[1mStep[0m  [8/84], [94mLoss[0m : 2.35674
[1mStep[0m  [16/84], [94mLoss[0m : 2.39278
[1mStep[0m  [24/84], [94mLoss[0m : 2.30462
[1mStep[0m  [32/84], [94mLoss[0m : 2.29236
[1mStep[0m  [40/84], [94mLoss[0m : 2.89017
[1mStep[0m  [48/84], [94mLoss[0m : 2.50646
[1mStep[0m  [56/84], [94mLoss[0m : 2.17135
[1mStep[0m  [64/84], [94mLoss[0m : 2.92968
[1mStep[0m  [72/84], [94mLoss[0m : 2.07094
[1mStep[0m  [80/84], [94mLoss[0m : 2.14923

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35623
[1mStep[0m  [8/84], [94mLoss[0m : 2.06761
[1mStep[0m  [16/84], [94mLoss[0m : 2.35788
[1mStep[0m  [24/84], [94mLoss[0m : 2.08541
[1mStep[0m  [32/84], [94mLoss[0m : 2.20670
[1mStep[0m  [40/84], [94mLoss[0m : 2.56552
[1mStep[0m  [48/84], [94mLoss[0m : 2.32981
[1mStep[0m  [56/84], [94mLoss[0m : 2.18369
[1mStep[0m  [64/84], [94mLoss[0m : 2.64498
[1mStep[0m  [72/84], [94mLoss[0m : 2.44234
[1mStep[0m  [80/84], [94mLoss[0m : 2.58351

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49020
[1mStep[0m  [8/84], [94mLoss[0m : 2.38263
[1mStep[0m  [16/84], [94mLoss[0m : 2.37256
[1mStep[0m  [24/84], [94mLoss[0m : 2.26256
[1mStep[0m  [32/84], [94mLoss[0m : 2.41031
[1mStep[0m  [40/84], [94mLoss[0m : 2.42273
[1mStep[0m  [48/84], [94mLoss[0m : 2.23444
[1mStep[0m  [56/84], [94mLoss[0m : 2.13294
[1mStep[0m  [64/84], [94mLoss[0m : 2.47600
[1mStep[0m  [72/84], [94mLoss[0m : 2.09098
[1mStep[0m  [80/84], [94mLoss[0m : 2.17261

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13989
[1mStep[0m  [8/84], [94mLoss[0m : 2.25265
[1mStep[0m  [16/84], [94mLoss[0m : 2.69249
[1mStep[0m  [24/84], [94mLoss[0m : 2.34780
[1mStep[0m  [32/84], [94mLoss[0m : 2.18127
[1mStep[0m  [40/84], [94mLoss[0m : 2.20861
[1mStep[0m  [48/84], [94mLoss[0m : 2.51147
[1mStep[0m  [56/84], [94mLoss[0m : 2.61999
[1mStep[0m  [64/84], [94mLoss[0m : 2.44713
[1mStep[0m  [72/84], [94mLoss[0m : 2.46707
[1mStep[0m  [80/84], [94mLoss[0m : 2.35653

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60784
[1mStep[0m  [8/84], [94mLoss[0m : 1.95806
[1mStep[0m  [16/84], [94mLoss[0m : 2.37133
[1mStep[0m  [24/84], [94mLoss[0m : 2.02154
[1mStep[0m  [32/84], [94mLoss[0m : 2.33135
[1mStep[0m  [40/84], [94mLoss[0m : 2.35414
[1mStep[0m  [48/84], [94mLoss[0m : 2.52192
[1mStep[0m  [56/84], [94mLoss[0m : 2.60224
[1mStep[0m  [64/84], [94mLoss[0m : 1.92876
[1mStep[0m  [72/84], [94mLoss[0m : 2.35448
[1mStep[0m  [80/84], [94mLoss[0m : 2.52328

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.494, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38583
[1mStep[0m  [8/84], [94mLoss[0m : 2.23289
[1mStep[0m  [16/84], [94mLoss[0m : 2.23727
[1mStep[0m  [24/84], [94mLoss[0m : 2.21027
[1mStep[0m  [32/84], [94mLoss[0m : 2.07152
[1mStep[0m  [40/84], [94mLoss[0m : 2.46185
[1mStep[0m  [48/84], [94mLoss[0m : 2.28912
[1mStep[0m  [56/84], [94mLoss[0m : 2.46977
[1mStep[0m  [64/84], [94mLoss[0m : 2.27565
[1mStep[0m  [72/84], [94mLoss[0m : 2.00751
[1mStep[0m  [80/84], [94mLoss[0m : 2.35416

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42900
[1mStep[0m  [8/84], [94mLoss[0m : 2.40759
[1mStep[0m  [16/84], [94mLoss[0m : 2.32119
[1mStep[0m  [24/84], [94mLoss[0m : 2.21007
[1mStep[0m  [32/84], [94mLoss[0m : 2.27036
[1mStep[0m  [40/84], [94mLoss[0m : 2.19870
[1mStep[0m  [48/84], [94mLoss[0m : 2.08710
[1mStep[0m  [56/84], [94mLoss[0m : 2.32384
[1mStep[0m  [64/84], [94mLoss[0m : 2.03246
[1mStep[0m  [72/84], [94mLoss[0m : 2.45757
[1mStep[0m  [80/84], [94mLoss[0m : 1.87207

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29797
[1mStep[0m  [8/84], [94mLoss[0m : 1.99546
[1mStep[0m  [16/84], [94mLoss[0m : 2.11592
[1mStep[0m  [24/84], [94mLoss[0m : 2.19251
[1mStep[0m  [32/84], [94mLoss[0m : 1.87541
[1mStep[0m  [40/84], [94mLoss[0m : 2.19552
[1mStep[0m  [48/84], [94mLoss[0m : 2.21503
[1mStep[0m  [56/84], [94mLoss[0m : 2.19136
[1mStep[0m  [64/84], [94mLoss[0m : 1.99668
[1mStep[0m  [72/84], [94mLoss[0m : 2.33150
[1mStep[0m  [80/84], [94mLoss[0m : 2.08979

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.209, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07536
[1mStep[0m  [8/84], [94mLoss[0m : 2.19976
[1mStep[0m  [16/84], [94mLoss[0m : 2.32343
[1mStep[0m  [24/84], [94mLoss[0m : 2.14666
[1mStep[0m  [32/84], [94mLoss[0m : 2.34128
[1mStep[0m  [40/84], [94mLoss[0m : 2.25023
[1mStep[0m  [48/84], [94mLoss[0m : 2.35117
[1mStep[0m  [56/84], [94mLoss[0m : 2.20238
[1mStep[0m  [64/84], [94mLoss[0m : 2.44875
[1mStep[0m  [72/84], [94mLoss[0m : 2.22296
[1mStep[0m  [80/84], [94mLoss[0m : 2.32450

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34941
[1mStep[0m  [8/84], [94mLoss[0m : 2.10702
[1mStep[0m  [16/84], [94mLoss[0m : 2.16427
[1mStep[0m  [24/84], [94mLoss[0m : 2.18682
[1mStep[0m  [32/84], [94mLoss[0m : 2.07153
[1mStep[0m  [40/84], [94mLoss[0m : 2.08295
[1mStep[0m  [48/84], [94mLoss[0m : 2.12694
[1mStep[0m  [56/84], [94mLoss[0m : 2.03086
[1mStep[0m  [64/84], [94mLoss[0m : 2.12029
[1mStep[0m  [72/84], [94mLoss[0m : 2.18110
[1mStep[0m  [80/84], [94mLoss[0m : 2.19670

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00687
[1mStep[0m  [8/84], [94mLoss[0m : 2.14562
[1mStep[0m  [16/84], [94mLoss[0m : 2.33978
[1mStep[0m  [24/84], [94mLoss[0m : 1.98968
[1mStep[0m  [32/84], [94mLoss[0m : 2.20255
[1mStep[0m  [40/84], [94mLoss[0m : 2.04183
[1mStep[0m  [48/84], [94mLoss[0m : 1.77931
[1mStep[0m  [56/84], [94mLoss[0m : 2.04350
[1mStep[0m  [64/84], [94mLoss[0m : 2.29586
[1mStep[0m  [72/84], [94mLoss[0m : 2.21789
[1mStep[0m  [80/84], [94mLoss[0m : 2.46732

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90797
[1mStep[0m  [8/84], [94mLoss[0m : 2.04436
[1mStep[0m  [16/84], [94mLoss[0m : 1.78818
[1mStep[0m  [24/84], [94mLoss[0m : 2.02717
[1mStep[0m  [32/84], [94mLoss[0m : 1.95760
[1mStep[0m  [40/84], [94mLoss[0m : 2.28170
[1mStep[0m  [48/84], [94mLoss[0m : 1.89556
[1mStep[0m  [56/84], [94mLoss[0m : 1.85278
[1mStep[0m  [64/84], [94mLoss[0m : 1.98041
[1mStep[0m  [72/84], [94mLoss[0m : 2.03725
[1mStep[0m  [80/84], [94mLoss[0m : 1.93580

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.555, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.99536
[1mStep[0m  [8/84], [94mLoss[0m : 1.83663
[1mStep[0m  [16/84], [94mLoss[0m : 1.99899
[1mStep[0m  [24/84], [94mLoss[0m : 1.73603
[1mStep[0m  [32/84], [94mLoss[0m : 1.95774
[1mStep[0m  [40/84], [94mLoss[0m : 1.94716
[1mStep[0m  [48/84], [94mLoss[0m : 1.78004
[1mStep[0m  [56/84], [94mLoss[0m : 2.02672
[1mStep[0m  [64/84], [94mLoss[0m : 2.06445
[1mStep[0m  [72/84], [94mLoss[0m : 2.01395
[1mStep[0m  [80/84], [94mLoss[0m : 2.04445

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.031, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85852
[1mStep[0m  [8/84], [94mLoss[0m : 1.69137
[1mStep[0m  [16/84], [94mLoss[0m : 1.77286
[1mStep[0m  [24/84], [94mLoss[0m : 2.17157
[1mStep[0m  [32/84], [94mLoss[0m : 1.93008
[1mStep[0m  [40/84], [94mLoss[0m : 2.19705
[1mStep[0m  [48/84], [94mLoss[0m : 2.14405
[1mStep[0m  [56/84], [94mLoss[0m : 2.06174
[1mStep[0m  [64/84], [94mLoss[0m : 1.96554
[1mStep[0m  [72/84], [94mLoss[0m : 2.06125
[1mStep[0m  [80/84], [94mLoss[0m : 1.94125

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86715
[1mStep[0m  [8/84], [94mLoss[0m : 1.81419
[1mStep[0m  [16/84], [94mLoss[0m : 1.90957
[1mStep[0m  [24/84], [94mLoss[0m : 1.97494
[1mStep[0m  [32/84], [94mLoss[0m : 2.23592
[1mStep[0m  [40/84], [94mLoss[0m : 2.26737
[1mStep[0m  [48/84], [94mLoss[0m : 1.79155
[1mStep[0m  [56/84], [94mLoss[0m : 2.12467
[1mStep[0m  [64/84], [94mLoss[0m : 1.91272
[1mStep[0m  [72/84], [94mLoss[0m : 1.97196
[1mStep[0m  [80/84], [94mLoss[0m : 1.86750

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.954, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86104
[1mStep[0m  [8/84], [94mLoss[0m : 2.09689
[1mStep[0m  [16/84], [94mLoss[0m : 2.00739
[1mStep[0m  [24/84], [94mLoss[0m : 1.85330
[1mStep[0m  [32/84], [94mLoss[0m : 1.97712
[1mStep[0m  [40/84], [94mLoss[0m : 1.74418
[1mStep[0m  [48/84], [94mLoss[0m : 1.78329
[1mStep[0m  [56/84], [94mLoss[0m : 1.92877
[1mStep[0m  [64/84], [94mLoss[0m : 1.95487
[1mStep[0m  [72/84], [94mLoss[0m : 1.79600
[1mStep[0m  [80/84], [94mLoss[0m : 1.78618

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79033
[1mStep[0m  [8/84], [94mLoss[0m : 2.00804
[1mStep[0m  [16/84], [94mLoss[0m : 1.74206
[1mStep[0m  [24/84], [94mLoss[0m : 2.09759
[1mStep[0m  [32/84], [94mLoss[0m : 1.97186
[1mStep[0m  [40/84], [94mLoss[0m : 1.79912
[1mStep[0m  [48/84], [94mLoss[0m : 1.88765
[1mStep[0m  [56/84], [94mLoss[0m : 2.06084
[1mStep[0m  [64/84], [94mLoss[0m : 1.83493
[1mStep[0m  [72/84], [94mLoss[0m : 1.97100
[1mStep[0m  [80/84], [94mLoss[0m : 2.18485

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88440
[1mStep[0m  [8/84], [94mLoss[0m : 1.85366
[1mStep[0m  [16/84], [94mLoss[0m : 1.94746
[1mStep[0m  [24/84], [94mLoss[0m : 2.02633
[1mStep[0m  [32/84], [94mLoss[0m : 1.83606
[1mStep[0m  [40/84], [94mLoss[0m : 1.91092
[1mStep[0m  [48/84], [94mLoss[0m : 1.81659
[1mStep[0m  [56/84], [94mLoss[0m : 1.85774
[1mStep[0m  [64/84], [94mLoss[0m : 1.71579
[1mStep[0m  [72/84], [94mLoss[0m : 1.82036
[1mStep[0m  [80/84], [94mLoss[0m : 1.77539

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.460, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.83534
[1mStep[0m  [8/84], [94mLoss[0m : 2.01308
[1mStep[0m  [16/84], [94mLoss[0m : 1.68357
[1mStep[0m  [24/84], [94mLoss[0m : 1.93023
[1mStep[0m  [32/84], [94mLoss[0m : 1.99808
[1mStep[0m  [40/84], [94mLoss[0m : 1.80452
[1mStep[0m  [48/84], [94mLoss[0m : 1.78176
[1mStep[0m  [56/84], [94mLoss[0m : 1.61421
[1mStep[0m  [64/84], [94mLoss[0m : 1.78766
[1mStep[0m  [72/84], [94mLoss[0m : 1.66823
[1mStep[0m  [80/84], [94mLoss[0m : 2.05405

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.847, [92mTest[0m: 2.462, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.76619
[1mStep[0m  [8/84], [94mLoss[0m : 1.81548
[1mStep[0m  [16/84], [94mLoss[0m : 1.70764
[1mStep[0m  [24/84], [94mLoss[0m : 1.84563
[1mStep[0m  [32/84], [94mLoss[0m : 1.68728
[1mStep[0m  [40/84], [94mLoss[0m : 1.81812
[1mStep[0m  [48/84], [94mLoss[0m : 2.07167
[1mStep[0m  [56/84], [94mLoss[0m : 1.77177
[1mStep[0m  [64/84], [94mLoss[0m : 1.80711
[1mStep[0m  [72/84], [94mLoss[0m : 1.86071
[1mStep[0m  [80/84], [94mLoss[0m : 1.83897

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.799, [92mTest[0m: 2.441, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92012
[1mStep[0m  [8/84], [94mLoss[0m : 1.72180
[1mStep[0m  [16/84], [94mLoss[0m : 1.70534
[1mStep[0m  [24/84], [94mLoss[0m : 1.56445
[1mStep[0m  [32/84], [94mLoss[0m : 1.54370
[1mStep[0m  [40/84], [94mLoss[0m : 1.79253
[1mStep[0m  [48/84], [94mLoss[0m : 1.85228
[1mStep[0m  [56/84], [94mLoss[0m : 1.65439
[1mStep[0m  [64/84], [94mLoss[0m : 1.83425
[1mStep[0m  [72/84], [94mLoss[0m : 1.75400
[1mStep[0m  [80/84], [94mLoss[0m : 1.76195

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.768, [92mTest[0m: 2.467, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.01736
[1mStep[0m  [8/84], [94mLoss[0m : 1.78687
[1mStep[0m  [16/84], [94mLoss[0m : 2.07325
[1mStep[0m  [24/84], [94mLoss[0m : 1.63377
[1mStep[0m  [32/84], [94mLoss[0m : 1.61717
[1mStep[0m  [40/84], [94mLoss[0m : 1.64479
[1mStep[0m  [48/84], [94mLoss[0m : 1.96948
[1mStep[0m  [56/84], [94mLoss[0m : 1.79243
[1mStep[0m  [64/84], [94mLoss[0m : 1.93883
[1mStep[0m  [72/84], [94mLoss[0m : 1.83587
[1mStep[0m  [80/84], [94mLoss[0m : 1.79166

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.753, [92mTest[0m: 2.486, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79690
[1mStep[0m  [8/84], [94mLoss[0m : 1.83289
[1mStep[0m  [16/84], [94mLoss[0m : 1.74936
[1mStep[0m  [24/84], [94mLoss[0m : 1.66537
[1mStep[0m  [32/84], [94mLoss[0m : 1.73358
[1mStep[0m  [40/84], [94mLoss[0m : 1.71497
[1mStep[0m  [48/84], [94mLoss[0m : 1.82337
[1mStep[0m  [56/84], [94mLoss[0m : 1.75021
[1mStep[0m  [64/84], [94mLoss[0m : 1.79283
[1mStep[0m  [72/84], [94mLoss[0m : 1.96634
[1mStep[0m  [80/84], [94mLoss[0m : 1.72774

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73350
[1mStep[0m  [8/84], [94mLoss[0m : 1.73973
[1mStep[0m  [16/84], [94mLoss[0m : 1.90620
[1mStep[0m  [24/84], [94mLoss[0m : 1.66805
[1mStep[0m  [32/84], [94mLoss[0m : 1.66264
[1mStep[0m  [40/84], [94mLoss[0m : 1.45427
[1mStep[0m  [48/84], [94mLoss[0m : 1.67100
[1mStep[0m  [56/84], [94mLoss[0m : 1.88902
[1mStep[0m  [64/84], [94mLoss[0m : 1.72832
[1mStep[0m  [72/84], [94mLoss[0m : 1.82411
[1mStep[0m  [80/84], [94mLoss[0m : 1.69476

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.63801
[1mStep[0m  [8/84], [94mLoss[0m : 1.49995
[1mStep[0m  [16/84], [94mLoss[0m : 1.85965
[1mStep[0m  [24/84], [94mLoss[0m : 1.58832
[1mStep[0m  [32/84], [94mLoss[0m : 1.70250
[1mStep[0m  [40/84], [94mLoss[0m : 1.50653
[1mStep[0m  [48/84], [94mLoss[0m : 1.72223
[1mStep[0m  [56/84], [94mLoss[0m : 1.71182
[1mStep[0m  [64/84], [94mLoss[0m : 1.92326
[1mStep[0m  [72/84], [94mLoss[0m : 1.93743
[1mStep[0m  [80/84], [94mLoss[0m : 1.54146

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.693, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.41091
[1mStep[0m  [8/84], [94mLoss[0m : 1.83788
[1mStep[0m  [16/84], [94mLoss[0m : 1.59871
[1mStep[0m  [24/84], [94mLoss[0m : 1.63174
[1mStep[0m  [32/84], [94mLoss[0m : 1.60938
[1mStep[0m  [40/84], [94mLoss[0m : 1.73890
[1mStep[0m  [48/84], [94mLoss[0m : 1.78787
[1mStep[0m  [56/84], [94mLoss[0m : 1.55312
[1mStep[0m  [64/84], [94mLoss[0m : 1.78061
[1mStep[0m  [72/84], [94mLoss[0m : 1.86972
[1mStep[0m  [80/84], [94mLoss[0m : 1.71582

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.502, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.57126
[1mStep[0m  [8/84], [94mLoss[0m : 1.69545
[1mStep[0m  [16/84], [94mLoss[0m : 1.56974
[1mStep[0m  [24/84], [94mLoss[0m : 1.63879
[1mStep[0m  [32/84], [94mLoss[0m : 1.46307
[1mStep[0m  [40/84], [94mLoss[0m : 1.95212
[1mStep[0m  [48/84], [94mLoss[0m : 1.56285
[1mStep[0m  [56/84], [94mLoss[0m : 1.61549
[1mStep[0m  [64/84], [94mLoss[0m : 1.84328
[1mStep[0m  [72/84], [94mLoss[0m : 1.71479
[1mStep[0m  [80/84], [94mLoss[0m : 1.75769

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.489
====================================

Phase 2 - Evaluation MAE:  2.4886548604284013
MAE score P1       2.342174
MAE score P2       2.488655
loss               1.646864
learning_rate      0.002575
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 11.01776
[1mStep[0m  [16/169], [94mLoss[0m : 10.47717
[1mStep[0m  [32/169], [94mLoss[0m : 10.70179
[1mStep[0m  [48/169], [94mLoss[0m : 11.62198
[1mStep[0m  [64/169], [94mLoss[0m : 11.06984
[1mStep[0m  [80/169], [94mLoss[0m : 10.70873
[1mStep[0m  [96/169], [94mLoss[0m : 10.33564
[1mStep[0m  [112/169], [94mLoss[0m : 10.14982
[1mStep[0m  [128/169], [94mLoss[0m : 10.59395
[1mStep[0m  [144/169], [94mLoss[0m : 9.88423
[1mStep[0m  [160/169], [94mLoss[0m : 10.40067

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.640, [92mTest[0m: 10.835, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 10.11166
[1mStep[0m  [16/169], [94mLoss[0m : 10.43633
[1mStep[0m  [32/169], [94mLoss[0m : 10.36722
[1mStep[0m  [48/169], [94mLoss[0m : 9.41686
[1mStep[0m  [64/169], [94mLoss[0m : 10.10676
[1mStep[0m  [80/169], [94mLoss[0m : 9.98894
[1mStep[0m  [96/169], [94mLoss[0m : 10.05143
[1mStep[0m  [112/169], [94mLoss[0m : 9.90114
[1mStep[0m  [128/169], [94mLoss[0m : 10.16652
[1mStep[0m  [144/169], [94mLoss[0m : 9.70548
[1mStep[0m  [160/169], [94mLoss[0m : 9.94232

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.021, [92mTest[0m: 10.190, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 9.81953
[1mStep[0m  [16/169], [94mLoss[0m : 10.43378
[1mStep[0m  [32/169], [94mLoss[0m : 9.26803
[1mStep[0m  [48/169], [94mLoss[0m : 8.43271
[1mStep[0m  [64/169], [94mLoss[0m : 9.00146
[1mStep[0m  [80/169], [94mLoss[0m : 8.52365
[1mStep[0m  [96/169], [94mLoss[0m : 9.47992
[1mStep[0m  [112/169], [94mLoss[0m : 9.29788
[1mStep[0m  [128/169], [94mLoss[0m : 9.63861
[1mStep[0m  [144/169], [94mLoss[0m : 9.97901
[1mStep[0m  [160/169], [94mLoss[0m : 9.44901

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.317, [92mTest[0m: 9.308, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.78656
[1mStep[0m  [16/169], [94mLoss[0m : 8.19871
[1mStep[0m  [32/169], [94mLoss[0m : 8.70453
[1mStep[0m  [48/169], [94mLoss[0m : 8.32534
[1mStep[0m  [64/169], [94mLoss[0m : 8.50844
[1mStep[0m  [80/169], [94mLoss[0m : 8.63351
[1mStep[0m  [96/169], [94mLoss[0m : 8.15109
[1mStep[0m  [112/169], [94mLoss[0m : 8.71205
[1mStep[0m  [128/169], [94mLoss[0m : 7.39346
[1mStep[0m  [144/169], [94mLoss[0m : 7.81997
[1mStep[0m  [160/169], [94mLoss[0m : 8.37850

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 8.371, [92mTest[0m: 8.278, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.88089
[1mStep[0m  [16/169], [94mLoss[0m : 8.12062
[1mStep[0m  [32/169], [94mLoss[0m : 7.17487
[1mStep[0m  [48/169], [94mLoss[0m : 7.45797
[1mStep[0m  [64/169], [94mLoss[0m : 8.26391
[1mStep[0m  [80/169], [94mLoss[0m : 6.70194
[1mStep[0m  [96/169], [94mLoss[0m : 7.31971
[1mStep[0m  [112/169], [94mLoss[0m : 7.00363
[1mStep[0m  [128/169], [94mLoss[0m : 5.98751
[1mStep[0m  [144/169], [94mLoss[0m : 7.09484
[1mStep[0m  [160/169], [94mLoss[0m : 6.77689

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 7.207, [92mTest[0m: 6.965, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 7.11037
[1mStep[0m  [16/169], [94mLoss[0m : 6.36667
[1mStep[0m  [32/169], [94mLoss[0m : 6.16366
[1mStep[0m  [48/169], [94mLoss[0m : 6.49406
[1mStep[0m  [64/169], [94mLoss[0m : 6.82471
[1mStep[0m  [80/169], [94mLoss[0m : 6.03589
[1mStep[0m  [96/169], [94mLoss[0m : 5.07961
[1mStep[0m  [112/169], [94mLoss[0m : 6.54504
[1mStep[0m  [128/169], [94mLoss[0m : 6.11307
[1mStep[0m  [144/169], [94mLoss[0m : 5.26342
[1mStep[0m  [160/169], [94mLoss[0m : 5.83486

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 6.114, [92mTest[0m: 5.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.05216
[1mStep[0m  [16/169], [94mLoss[0m : 5.41166
[1mStep[0m  [32/169], [94mLoss[0m : 4.92327
[1mStep[0m  [48/169], [94mLoss[0m : 5.22321
[1mStep[0m  [64/169], [94mLoss[0m : 5.09922
[1mStep[0m  [80/169], [94mLoss[0m : 5.42103
[1mStep[0m  [96/169], [94mLoss[0m : 5.18236
[1mStep[0m  [112/169], [94mLoss[0m : 5.11909
[1mStep[0m  [128/169], [94mLoss[0m : 5.31361
[1mStep[0m  [144/169], [94mLoss[0m : 4.82693
[1mStep[0m  [160/169], [94mLoss[0m : 4.52018

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.029, [92mTest[0m: 4.797, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.67390
[1mStep[0m  [16/169], [94mLoss[0m : 4.04817
[1mStep[0m  [32/169], [94mLoss[0m : 4.18486
[1mStep[0m  [48/169], [94mLoss[0m : 4.38372
[1mStep[0m  [64/169], [94mLoss[0m : 4.14959
[1mStep[0m  [80/169], [94mLoss[0m : 4.18219
[1mStep[0m  [96/169], [94mLoss[0m : 3.85776
[1mStep[0m  [112/169], [94mLoss[0m : 4.04359
[1mStep[0m  [128/169], [94mLoss[0m : 3.60045
[1mStep[0m  [144/169], [94mLoss[0m : 3.22465
[1mStep[0m  [160/169], [94mLoss[0m : 3.28199

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.966, [92mTest[0m: 3.702, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.48703
[1mStep[0m  [16/169], [94mLoss[0m : 3.96934
[1mStep[0m  [32/169], [94mLoss[0m : 4.05798
[1mStep[0m  [48/169], [94mLoss[0m : 2.79033
[1mStep[0m  [64/169], [94mLoss[0m : 3.17004
[1mStep[0m  [80/169], [94mLoss[0m : 3.64004
[1mStep[0m  [96/169], [94mLoss[0m : 3.15253
[1mStep[0m  [112/169], [94mLoss[0m : 3.01984
[1mStep[0m  [128/169], [94mLoss[0m : 3.08758
[1mStep[0m  [144/169], [94mLoss[0m : 2.23036
[1mStep[0m  [160/169], [94mLoss[0m : 2.65584

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.085, [92mTest[0m: 2.925, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87830
[1mStep[0m  [16/169], [94mLoss[0m : 3.36339
[1mStep[0m  [32/169], [94mLoss[0m : 2.44370
[1mStep[0m  [48/169], [94mLoss[0m : 2.20119
[1mStep[0m  [64/169], [94mLoss[0m : 2.69095
[1mStep[0m  [80/169], [94mLoss[0m : 2.78889
[1mStep[0m  [96/169], [94mLoss[0m : 2.52896
[1mStep[0m  [112/169], [94mLoss[0m : 2.80614
[1mStep[0m  [128/169], [94mLoss[0m : 2.64901
[1mStep[0m  [144/169], [94mLoss[0m : 2.44971
[1mStep[0m  [160/169], [94mLoss[0m : 2.74475

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.744, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38320
[1mStep[0m  [16/169], [94mLoss[0m : 3.01057
[1mStep[0m  [32/169], [94mLoss[0m : 2.80144
[1mStep[0m  [48/169], [94mLoss[0m : 2.79843
[1mStep[0m  [64/169], [94mLoss[0m : 2.92103
[1mStep[0m  [80/169], [94mLoss[0m : 2.55747
[1mStep[0m  [96/169], [94mLoss[0m : 2.96561
[1mStep[0m  [112/169], [94mLoss[0m : 2.62007
[1mStep[0m  [128/169], [94mLoss[0m : 2.40440
[1mStep[0m  [144/169], [94mLoss[0m : 2.36050
[1mStep[0m  [160/169], [94mLoss[0m : 2.56116

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62731
[1mStep[0m  [16/169], [94mLoss[0m : 2.74122
[1mStep[0m  [32/169], [94mLoss[0m : 2.81681
[1mStep[0m  [48/169], [94mLoss[0m : 2.75289
[1mStep[0m  [64/169], [94mLoss[0m : 2.14829
[1mStep[0m  [80/169], [94mLoss[0m : 2.82194
[1mStep[0m  [96/169], [94mLoss[0m : 2.66483
[1mStep[0m  [112/169], [94mLoss[0m : 2.59540
[1mStep[0m  [128/169], [94mLoss[0m : 3.36561
[1mStep[0m  [144/169], [94mLoss[0m : 2.70320
[1mStep[0m  [160/169], [94mLoss[0m : 2.37048

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83319
[1mStep[0m  [16/169], [94mLoss[0m : 2.50571
[1mStep[0m  [32/169], [94mLoss[0m : 2.66295
[1mStep[0m  [48/169], [94mLoss[0m : 2.80356
[1mStep[0m  [64/169], [94mLoss[0m : 2.60481
[1mStep[0m  [80/169], [94mLoss[0m : 2.71092
[1mStep[0m  [96/169], [94mLoss[0m : 2.63043
[1mStep[0m  [112/169], [94mLoss[0m : 2.12184
[1mStep[0m  [128/169], [94mLoss[0m : 2.41812
[1mStep[0m  [144/169], [94mLoss[0m : 2.64060
[1mStep[0m  [160/169], [94mLoss[0m : 2.85645

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31362
[1mStep[0m  [16/169], [94mLoss[0m : 3.00348
[1mStep[0m  [32/169], [94mLoss[0m : 3.06047
[1mStep[0m  [48/169], [94mLoss[0m : 2.69878
[1mStep[0m  [64/169], [94mLoss[0m : 2.44779
[1mStep[0m  [80/169], [94mLoss[0m : 2.87941
[1mStep[0m  [96/169], [94mLoss[0m : 2.61983
[1mStep[0m  [112/169], [94mLoss[0m : 2.15272
[1mStep[0m  [128/169], [94mLoss[0m : 2.15742
[1mStep[0m  [144/169], [94mLoss[0m : 2.41818
[1mStep[0m  [160/169], [94mLoss[0m : 2.37736

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47548
[1mStep[0m  [16/169], [94mLoss[0m : 2.21328
[1mStep[0m  [32/169], [94mLoss[0m : 2.67084
[1mStep[0m  [48/169], [94mLoss[0m : 2.41293
[1mStep[0m  [64/169], [94mLoss[0m : 2.24994
[1mStep[0m  [80/169], [94mLoss[0m : 2.76259
[1mStep[0m  [96/169], [94mLoss[0m : 2.71268
[1mStep[0m  [112/169], [94mLoss[0m : 2.79056
[1mStep[0m  [128/169], [94mLoss[0m : 2.23431
[1mStep[0m  [144/169], [94mLoss[0m : 3.09523
[1mStep[0m  [160/169], [94mLoss[0m : 2.39471

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99689
[1mStep[0m  [16/169], [94mLoss[0m : 2.65913
[1mStep[0m  [32/169], [94mLoss[0m : 2.58450
[1mStep[0m  [48/169], [94mLoss[0m : 2.56303
[1mStep[0m  [64/169], [94mLoss[0m : 2.44357
[1mStep[0m  [80/169], [94mLoss[0m : 2.25420
[1mStep[0m  [96/169], [94mLoss[0m : 2.61911
[1mStep[0m  [112/169], [94mLoss[0m : 2.60655
[1mStep[0m  [128/169], [94mLoss[0m : 2.93630
[1mStep[0m  [144/169], [94mLoss[0m : 2.35068
[1mStep[0m  [160/169], [94mLoss[0m : 2.49760

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51085
[1mStep[0m  [16/169], [94mLoss[0m : 2.93525
[1mStep[0m  [32/169], [94mLoss[0m : 2.76835
[1mStep[0m  [48/169], [94mLoss[0m : 2.20730
[1mStep[0m  [64/169], [94mLoss[0m : 2.56263
[1mStep[0m  [80/169], [94mLoss[0m : 2.62830
[1mStep[0m  [96/169], [94mLoss[0m : 2.27773
[1mStep[0m  [112/169], [94mLoss[0m : 2.19742
[1mStep[0m  [128/169], [94mLoss[0m : 2.70316
[1mStep[0m  [144/169], [94mLoss[0m : 2.79225
[1mStep[0m  [160/169], [94mLoss[0m : 2.59797

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60819
[1mStep[0m  [16/169], [94mLoss[0m : 2.60982
[1mStep[0m  [32/169], [94mLoss[0m : 2.73293
[1mStep[0m  [48/169], [94mLoss[0m : 2.61295
[1mStep[0m  [64/169], [94mLoss[0m : 2.46631
[1mStep[0m  [80/169], [94mLoss[0m : 3.12734
[1mStep[0m  [96/169], [94mLoss[0m : 2.38342
[1mStep[0m  [112/169], [94mLoss[0m : 2.69305
[1mStep[0m  [128/169], [94mLoss[0m : 3.11293
[1mStep[0m  [144/169], [94mLoss[0m : 2.59661
[1mStep[0m  [160/169], [94mLoss[0m : 2.25190

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74595
[1mStep[0m  [16/169], [94mLoss[0m : 2.87094
[1mStep[0m  [32/169], [94mLoss[0m : 2.53709
[1mStep[0m  [48/169], [94mLoss[0m : 2.31725
[1mStep[0m  [64/169], [94mLoss[0m : 2.32216
[1mStep[0m  [80/169], [94mLoss[0m : 2.39655
[1mStep[0m  [96/169], [94mLoss[0m : 2.60137
[1mStep[0m  [112/169], [94mLoss[0m : 2.64399
[1mStep[0m  [128/169], [94mLoss[0m : 2.58300
[1mStep[0m  [144/169], [94mLoss[0m : 2.31400
[1mStep[0m  [160/169], [94mLoss[0m : 2.18225

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57719
[1mStep[0m  [16/169], [94mLoss[0m : 2.48778
[1mStep[0m  [32/169], [94mLoss[0m : 2.32426
[1mStep[0m  [48/169], [94mLoss[0m : 2.62183
[1mStep[0m  [64/169], [94mLoss[0m : 2.68754
[1mStep[0m  [80/169], [94mLoss[0m : 2.91364
[1mStep[0m  [96/169], [94mLoss[0m : 2.14844
[1mStep[0m  [112/169], [94mLoss[0m : 2.37928
[1mStep[0m  [128/169], [94mLoss[0m : 2.21495
[1mStep[0m  [144/169], [94mLoss[0m : 2.33151
[1mStep[0m  [160/169], [94mLoss[0m : 2.83398

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40942
[1mStep[0m  [16/169], [94mLoss[0m : 2.59795
[1mStep[0m  [32/169], [94mLoss[0m : 2.79189
[1mStep[0m  [48/169], [94mLoss[0m : 2.58176
[1mStep[0m  [64/169], [94mLoss[0m : 1.93811
[1mStep[0m  [80/169], [94mLoss[0m : 2.67822
[1mStep[0m  [96/169], [94mLoss[0m : 2.25743
[1mStep[0m  [112/169], [94mLoss[0m : 2.79193
[1mStep[0m  [128/169], [94mLoss[0m : 2.18395
[1mStep[0m  [144/169], [94mLoss[0m : 2.22046
[1mStep[0m  [160/169], [94mLoss[0m : 2.81434

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55917
[1mStep[0m  [16/169], [94mLoss[0m : 2.74682
[1mStep[0m  [32/169], [94mLoss[0m : 2.47241
[1mStep[0m  [48/169], [94mLoss[0m : 2.97048
[1mStep[0m  [64/169], [94mLoss[0m : 2.52775
[1mStep[0m  [80/169], [94mLoss[0m : 2.70718
[1mStep[0m  [96/169], [94mLoss[0m : 2.47136
[1mStep[0m  [112/169], [94mLoss[0m : 1.92532
[1mStep[0m  [128/169], [94mLoss[0m : 2.38090
[1mStep[0m  [144/169], [94mLoss[0m : 2.61520
[1mStep[0m  [160/169], [94mLoss[0m : 2.16105

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77386
[1mStep[0m  [16/169], [94mLoss[0m : 2.27744
[1mStep[0m  [32/169], [94mLoss[0m : 3.11647
[1mStep[0m  [48/169], [94mLoss[0m : 2.84391
[1mStep[0m  [64/169], [94mLoss[0m : 2.98418
[1mStep[0m  [80/169], [94mLoss[0m : 2.18074
[1mStep[0m  [96/169], [94mLoss[0m : 2.54189
[1mStep[0m  [112/169], [94mLoss[0m : 2.39784
[1mStep[0m  [128/169], [94mLoss[0m : 2.48198
[1mStep[0m  [144/169], [94mLoss[0m : 2.83507
[1mStep[0m  [160/169], [94mLoss[0m : 2.83004

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.510, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50403
[1mStep[0m  [16/169], [94mLoss[0m : 2.66390
[1mStep[0m  [32/169], [94mLoss[0m : 2.52649
[1mStep[0m  [48/169], [94mLoss[0m : 2.47677
[1mStep[0m  [64/169], [94mLoss[0m : 2.29430
[1mStep[0m  [80/169], [94mLoss[0m : 2.42226
[1mStep[0m  [96/169], [94mLoss[0m : 2.35589
[1mStep[0m  [112/169], [94mLoss[0m : 2.19714
[1mStep[0m  [128/169], [94mLoss[0m : 2.19335
[1mStep[0m  [144/169], [94mLoss[0m : 2.47261
[1mStep[0m  [160/169], [94mLoss[0m : 3.00280

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31761
[1mStep[0m  [16/169], [94mLoss[0m : 2.10340
[1mStep[0m  [32/169], [94mLoss[0m : 2.38346
[1mStep[0m  [48/169], [94mLoss[0m : 2.49131
[1mStep[0m  [64/169], [94mLoss[0m : 2.11993
[1mStep[0m  [80/169], [94mLoss[0m : 2.92865
[1mStep[0m  [96/169], [94mLoss[0m : 2.54485
[1mStep[0m  [112/169], [94mLoss[0m : 2.26653
[1mStep[0m  [128/169], [94mLoss[0m : 2.65167
[1mStep[0m  [144/169], [94mLoss[0m : 2.51939
[1mStep[0m  [160/169], [94mLoss[0m : 3.11213

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81696
[1mStep[0m  [16/169], [94mLoss[0m : 2.76732
[1mStep[0m  [32/169], [94mLoss[0m : 2.79348
[1mStep[0m  [48/169], [94mLoss[0m : 2.19098
[1mStep[0m  [64/169], [94mLoss[0m : 2.12719
[1mStep[0m  [80/169], [94mLoss[0m : 2.65223
[1mStep[0m  [96/169], [94mLoss[0m : 3.05050
[1mStep[0m  [112/169], [94mLoss[0m : 2.49370
[1mStep[0m  [128/169], [94mLoss[0m : 2.69463
[1mStep[0m  [144/169], [94mLoss[0m : 2.30536
[1mStep[0m  [160/169], [94mLoss[0m : 2.07817

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29927
[1mStep[0m  [16/169], [94mLoss[0m : 1.98902
[1mStep[0m  [32/169], [94mLoss[0m : 2.23167
[1mStep[0m  [48/169], [94mLoss[0m : 2.41615
[1mStep[0m  [64/169], [94mLoss[0m : 2.22134
[1mStep[0m  [80/169], [94mLoss[0m : 2.84957
[1mStep[0m  [96/169], [94mLoss[0m : 2.95063
[1mStep[0m  [112/169], [94mLoss[0m : 2.32391
[1mStep[0m  [128/169], [94mLoss[0m : 2.47911
[1mStep[0m  [144/169], [94mLoss[0m : 2.66317
[1mStep[0m  [160/169], [94mLoss[0m : 2.38954

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41169
[1mStep[0m  [16/169], [94mLoss[0m : 2.37618
[1mStep[0m  [32/169], [94mLoss[0m : 2.33857
[1mStep[0m  [48/169], [94mLoss[0m : 2.31744
[1mStep[0m  [64/169], [94mLoss[0m : 2.60796
[1mStep[0m  [80/169], [94mLoss[0m : 2.06835
[1mStep[0m  [96/169], [94mLoss[0m : 2.51852
[1mStep[0m  [112/169], [94mLoss[0m : 2.37606
[1mStep[0m  [128/169], [94mLoss[0m : 2.83753
[1mStep[0m  [144/169], [94mLoss[0m : 2.57171
[1mStep[0m  [160/169], [94mLoss[0m : 2.45584

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48692
[1mStep[0m  [16/169], [94mLoss[0m : 2.42980
[1mStep[0m  [32/169], [94mLoss[0m : 2.51209
[1mStep[0m  [48/169], [94mLoss[0m : 2.49783
[1mStep[0m  [64/169], [94mLoss[0m : 3.07383
[1mStep[0m  [80/169], [94mLoss[0m : 2.42898
[1mStep[0m  [96/169], [94mLoss[0m : 2.54801
[1mStep[0m  [112/169], [94mLoss[0m : 2.25346
[1mStep[0m  [128/169], [94mLoss[0m : 2.57610
[1mStep[0m  [144/169], [94mLoss[0m : 2.26644
[1mStep[0m  [160/169], [94mLoss[0m : 2.75889

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05423
[1mStep[0m  [16/169], [94mLoss[0m : 2.07292
[1mStep[0m  [32/169], [94mLoss[0m : 2.91026
[1mStep[0m  [48/169], [94mLoss[0m : 2.70768
[1mStep[0m  [64/169], [94mLoss[0m : 2.64126
[1mStep[0m  [80/169], [94mLoss[0m : 2.54890
[1mStep[0m  [96/169], [94mLoss[0m : 2.23577
[1mStep[0m  [112/169], [94mLoss[0m : 2.47832
[1mStep[0m  [128/169], [94mLoss[0m : 2.38209
[1mStep[0m  [144/169], [94mLoss[0m : 2.02615
[1mStep[0m  [160/169], [94mLoss[0m : 2.20066

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.344
====================================

Phase 1 - Evaluation MAE:  2.343893208674022
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.41594
[1mStep[0m  [16/169], [94mLoss[0m : 2.89823
[1mStep[0m  [32/169], [94mLoss[0m : 2.08972
[1mStep[0m  [48/169], [94mLoss[0m : 2.22107
[1mStep[0m  [64/169], [94mLoss[0m : 2.29360
[1mStep[0m  [80/169], [94mLoss[0m : 2.56377
[1mStep[0m  [96/169], [94mLoss[0m : 2.64441
[1mStep[0m  [112/169], [94mLoss[0m : 2.45419
[1mStep[0m  [128/169], [94mLoss[0m : 2.69997
[1mStep[0m  [144/169], [94mLoss[0m : 2.54832
[1mStep[0m  [160/169], [94mLoss[0m : 2.76685

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25199
[1mStep[0m  [16/169], [94mLoss[0m : 2.42207
[1mStep[0m  [32/169], [94mLoss[0m : 2.74906
[1mStep[0m  [48/169], [94mLoss[0m : 2.69839
[1mStep[0m  [64/169], [94mLoss[0m : 2.81016
[1mStep[0m  [80/169], [94mLoss[0m : 2.33576
[1mStep[0m  [96/169], [94mLoss[0m : 2.20946
[1mStep[0m  [112/169], [94mLoss[0m : 2.64276
[1mStep[0m  [128/169], [94mLoss[0m : 2.16396
[1mStep[0m  [144/169], [94mLoss[0m : 2.25010
[1mStep[0m  [160/169], [94mLoss[0m : 2.59422

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19464
[1mStep[0m  [16/169], [94mLoss[0m : 2.63174
[1mStep[0m  [32/169], [94mLoss[0m : 2.16895
[1mStep[0m  [48/169], [94mLoss[0m : 2.50731
[1mStep[0m  [64/169], [94mLoss[0m : 2.40469
[1mStep[0m  [80/169], [94mLoss[0m : 2.47530
[1mStep[0m  [96/169], [94mLoss[0m : 2.30822
[1mStep[0m  [112/169], [94mLoss[0m : 2.57985
[1mStep[0m  [128/169], [94mLoss[0m : 2.34296
[1mStep[0m  [144/169], [94mLoss[0m : 2.51597
[1mStep[0m  [160/169], [94mLoss[0m : 2.37677

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34367
[1mStep[0m  [16/169], [94mLoss[0m : 2.54384
[1mStep[0m  [32/169], [94mLoss[0m : 3.30143
[1mStep[0m  [48/169], [94mLoss[0m : 2.19243
[1mStep[0m  [64/169], [94mLoss[0m : 1.97116
[1mStep[0m  [80/169], [94mLoss[0m : 2.73226
[1mStep[0m  [96/169], [94mLoss[0m : 2.86029
[1mStep[0m  [112/169], [94mLoss[0m : 2.64181
[1mStep[0m  [128/169], [94mLoss[0m : 2.42105
[1mStep[0m  [144/169], [94mLoss[0m : 2.58417
[1mStep[0m  [160/169], [94mLoss[0m : 2.35964

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40166
[1mStep[0m  [16/169], [94mLoss[0m : 2.35847
[1mStep[0m  [32/169], [94mLoss[0m : 2.76902
[1mStep[0m  [48/169], [94mLoss[0m : 2.37587
[1mStep[0m  [64/169], [94mLoss[0m : 1.93077
[1mStep[0m  [80/169], [94mLoss[0m : 2.49429
[1mStep[0m  [96/169], [94mLoss[0m : 2.31822
[1mStep[0m  [112/169], [94mLoss[0m : 2.75209
[1mStep[0m  [128/169], [94mLoss[0m : 2.37295
[1mStep[0m  [144/169], [94mLoss[0m : 2.46203
[1mStep[0m  [160/169], [94mLoss[0m : 2.27461

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33866
[1mStep[0m  [16/169], [94mLoss[0m : 2.12978
[1mStep[0m  [32/169], [94mLoss[0m : 2.30959
[1mStep[0m  [48/169], [94mLoss[0m : 2.97341
[1mStep[0m  [64/169], [94mLoss[0m : 2.00222
[1mStep[0m  [80/169], [94mLoss[0m : 2.14958
[1mStep[0m  [96/169], [94mLoss[0m : 2.17709
[1mStep[0m  [112/169], [94mLoss[0m : 2.53042
[1mStep[0m  [128/169], [94mLoss[0m : 2.16840
[1mStep[0m  [144/169], [94mLoss[0m : 2.48536
[1mStep[0m  [160/169], [94mLoss[0m : 2.22541

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17956
[1mStep[0m  [16/169], [94mLoss[0m : 1.93093
[1mStep[0m  [32/169], [94mLoss[0m : 2.15577
[1mStep[0m  [48/169], [94mLoss[0m : 2.27037
[1mStep[0m  [64/169], [94mLoss[0m : 2.43642
[1mStep[0m  [80/169], [94mLoss[0m : 2.51084
[1mStep[0m  [96/169], [94mLoss[0m : 2.36329
[1mStep[0m  [112/169], [94mLoss[0m : 1.98120
[1mStep[0m  [128/169], [94mLoss[0m : 2.32366
[1mStep[0m  [144/169], [94mLoss[0m : 2.30481
[1mStep[0m  [160/169], [94mLoss[0m : 2.44570

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35455
[1mStep[0m  [16/169], [94mLoss[0m : 2.07293
[1mStep[0m  [32/169], [94mLoss[0m : 2.03915
[1mStep[0m  [48/169], [94mLoss[0m : 2.20633
[1mStep[0m  [64/169], [94mLoss[0m : 2.08187
[1mStep[0m  [80/169], [94mLoss[0m : 2.45450
[1mStep[0m  [96/169], [94mLoss[0m : 2.18131
[1mStep[0m  [112/169], [94mLoss[0m : 2.17839
[1mStep[0m  [128/169], [94mLoss[0m : 2.13514
[1mStep[0m  [144/169], [94mLoss[0m : 2.21185
[1mStep[0m  [160/169], [94mLoss[0m : 2.69761

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10480
[1mStep[0m  [16/169], [94mLoss[0m : 1.90094
[1mStep[0m  [32/169], [94mLoss[0m : 2.33821
[1mStep[0m  [48/169], [94mLoss[0m : 2.22172
[1mStep[0m  [64/169], [94mLoss[0m : 2.01030
[1mStep[0m  [80/169], [94mLoss[0m : 2.37522
[1mStep[0m  [96/169], [94mLoss[0m : 2.17024
[1mStep[0m  [112/169], [94mLoss[0m : 1.89367
[1mStep[0m  [128/169], [94mLoss[0m : 2.64217
[1mStep[0m  [144/169], [94mLoss[0m : 2.75861
[1mStep[0m  [160/169], [94mLoss[0m : 1.92018

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.597, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15293
[1mStep[0m  [16/169], [94mLoss[0m : 2.26715
[1mStep[0m  [32/169], [94mLoss[0m : 2.53796
[1mStep[0m  [48/169], [94mLoss[0m : 1.84710
[1mStep[0m  [64/169], [94mLoss[0m : 2.06245
[1mStep[0m  [80/169], [94mLoss[0m : 2.61158
[1mStep[0m  [96/169], [94mLoss[0m : 2.38301
[1mStep[0m  [112/169], [94mLoss[0m : 2.37679
[1mStep[0m  [128/169], [94mLoss[0m : 1.80786
[1mStep[0m  [144/169], [94mLoss[0m : 2.16170
[1mStep[0m  [160/169], [94mLoss[0m : 2.57843

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38731
[1mStep[0m  [16/169], [94mLoss[0m : 2.05594
[1mStep[0m  [32/169], [94mLoss[0m : 2.04041
[1mStep[0m  [48/169], [94mLoss[0m : 2.20208
[1mStep[0m  [64/169], [94mLoss[0m : 2.25391
[1mStep[0m  [80/169], [94mLoss[0m : 2.15009
[1mStep[0m  [96/169], [94mLoss[0m : 2.30475
[1mStep[0m  [112/169], [94mLoss[0m : 2.22853
[1mStep[0m  [128/169], [94mLoss[0m : 1.97347
[1mStep[0m  [144/169], [94mLoss[0m : 1.78635
[1mStep[0m  [160/169], [94mLoss[0m : 2.20252

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.24913
[1mStep[0m  [16/169], [94mLoss[0m : 2.09137
[1mStep[0m  [32/169], [94mLoss[0m : 2.30514
[1mStep[0m  [48/169], [94mLoss[0m : 1.76276
[1mStep[0m  [64/169], [94mLoss[0m : 2.17791
[1mStep[0m  [80/169], [94mLoss[0m : 2.55123
[1mStep[0m  [96/169], [94mLoss[0m : 2.25309
[1mStep[0m  [112/169], [94mLoss[0m : 2.64992
[1mStep[0m  [128/169], [94mLoss[0m : 2.53252
[1mStep[0m  [144/169], [94mLoss[0m : 2.39930
[1mStep[0m  [160/169], [94mLoss[0m : 2.48040

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.143, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19776
[1mStep[0m  [16/169], [94mLoss[0m : 2.12542
[1mStep[0m  [32/169], [94mLoss[0m : 1.94283
[1mStep[0m  [48/169], [94mLoss[0m : 1.92700
[1mStep[0m  [64/169], [94mLoss[0m : 2.03595
[1mStep[0m  [80/169], [94mLoss[0m : 1.97338
[1mStep[0m  [96/169], [94mLoss[0m : 2.54416
[1mStep[0m  [112/169], [94mLoss[0m : 2.03977
[1mStep[0m  [128/169], [94mLoss[0m : 2.17723
[1mStep[0m  [144/169], [94mLoss[0m : 1.65032
[1mStep[0m  [160/169], [94mLoss[0m : 1.81066

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.112, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55661
[1mStep[0m  [16/169], [94mLoss[0m : 1.84515
[1mStep[0m  [32/169], [94mLoss[0m : 1.99311
[1mStep[0m  [48/169], [94mLoss[0m : 2.14411
[1mStep[0m  [64/169], [94mLoss[0m : 2.24613
[1mStep[0m  [80/169], [94mLoss[0m : 1.70674
[1mStep[0m  [96/169], [94mLoss[0m : 2.60680
[1mStep[0m  [112/169], [94mLoss[0m : 1.84817
[1mStep[0m  [128/169], [94mLoss[0m : 1.76460
[1mStep[0m  [144/169], [94mLoss[0m : 2.09326
[1mStep[0m  [160/169], [94mLoss[0m : 1.88642

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.078, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08889
[1mStep[0m  [16/169], [94mLoss[0m : 2.09988
[1mStep[0m  [32/169], [94mLoss[0m : 1.68199
[1mStep[0m  [48/169], [94mLoss[0m : 1.61277
[1mStep[0m  [64/169], [94mLoss[0m : 1.83876
[1mStep[0m  [80/169], [94mLoss[0m : 2.00438
[1mStep[0m  [96/169], [94mLoss[0m : 2.12973
[1mStep[0m  [112/169], [94mLoss[0m : 2.13268
[1mStep[0m  [128/169], [94mLoss[0m : 1.88853
[1mStep[0m  [144/169], [94mLoss[0m : 2.11935
[1mStep[0m  [160/169], [94mLoss[0m : 2.07537

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.060, [92mTest[0m: 2.506, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87898
[1mStep[0m  [16/169], [94mLoss[0m : 1.94347
[1mStep[0m  [32/169], [94mLoss[0m : 2.13205
[1mStep[0m  [48/169], [94mLoss[0m : 1.84050
[1mStep[0m  [64/169], [94mLoss[0m : 1.74899
[1mStep[0m  [80/169], [94mLoss[0m : 1.79028
[1mStep[0m  [96/169], [94mLoss[0m : 1.88063
[1mStep[0m  [112/169], [94mLoss[0m : 2.20727
[1mStep[0m  [128/169], [94mLoss[0m : 1.79795
[1mStep[0m  [144/169], [94mLoss[0m : 1.85749
[1mStep[0m  [160/169], [94mLoss[0m : 1.85223

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.004, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70195
[1mStep[0m  [16/169], [94mLoss[0m : 2.56253
[1mStep[0m  [32/169], [94mLoss[0m : 1.69628
[1mStep[0m  [48/169], [94mLoss[0m : 1.66922
[1mStep[0m  [64/169], [94mLoss[0m : 1.95950
[1mStep[0m  [80/169], [94mLoss[0m : 1.90056
[1mStep[0m  [96/169], [94mLoss[0m : 1.68707
[1mStep[0m  [112/169], [94mLoss[0m : 2.00345
[1mStep[0m  [128/169], [94mLoss[0m : 2.15884
[1mStep[0m  [144/169], [94mLoss[0m : 1.83919
[1mStep[0m  [160/169], [94mLoss[0m : 1.97711

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09141
[1mStep[0m  [16/169], [94mLoss[0m : 1.89520
[1mStep[0m  [32/169], [94mLoss[0m : 1.84444
[1mStep[0m  [48/169], [94mLoss[0m : 1.79161
[1mStep[0m  [64/169], [94mLoss[0m : 2.33881
[1mStep[0m  [80/169], [94mLoss[0m : 1.90334
[1mStep[0m  [96/169], [94mLoss[0m : 2.05651
[1mStep[0m  [112/169], [94mLoss[0m : 1.76217
[1mStep[0m  [128/169], [94mLoss[0m : 1.77134
[1mStep[0m  [144/169], [94mLoss[0m : 1.82858
[1mStep[0m  [160/169], [94mLoss[0m : 1.38578

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19785
[1mStep[0m  [16/169], [94mLoss[0m : 2.09887
[1mStep[0m  [32/169], [94mLoss[0m : 1.71544
[1mStep[0m  [48/169], [94mLoss[0m : 2.79699
[1mStep[0m  [64/169], [94mLoss[0m : 1.99068
[1mStep[0m  [80/169], [94mLoss[0m : 1.84057
[1mStep[0m  [96/169], [94mLoss[0m : 2.21706
[1mStep[0m  [112/169], [94mLoss[0m : 1.88496
[1mStep[0m  [128/169], [94mLoss[0m : 1.83486
[1mStep[0m  [144/169], [94mLoss[0m : 1.89920
[1mStep[0m  [160/169], [94mLoss[0m : 1.86091

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.903, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76583
[1mStep[0m  [16/169], [94mLoss[0m : 1.70215
[1mStep[0m  [32/169], [94mLoss[0m : 2.37838
[1mStep[0m  [48/169], [94mLoss[0m : 1.87872
[1mStep[0m  [64/169], [94mLoss[0m : 1.93440
[1mStep[0m  [80/169], [94mLoss[0m : 1.79983
[1mStep[0m  [96/169], [94mLoss[0m : 1.90795
[1mStep[0m  [112/169], [94mLoss[0m : 1.74425
[1mStep[0m  [128/169], [94mLoss[0m : 1.81853
[1mStep[0m  [144/169], [94mLoss[0m : 1.73441
[1mStep[0m  [160/169], [94mLoss[0m : 2.00400

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.878, [92mTest[0m: 2.429, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50522
[1mStep[0m  [16/169], [94mLoss[0m : 2.01312
[1mStep[0m  [32/169], [94mLoss[0m : 1.75722
[1mStep[0m  [48/169], [94mLoss[0m : 2.01679
[1mStep[0m  [64/169], [94mLoss[0m : 1.62100
[1mStep[0m  [80/169], [94mLoss[0m : 1.74042
[1mStep[0m  [96/169], [94mLoss[0m : 1.65258
[1mStep[0m  [112/169], [94mLoss[0m : 2.12069
[1mStep[0m  [128/169], [94mLoss[0m : 2.07753
[1mStep[0m  [144/169], [94mLoss[0m : 1.80648
[1mStep[0m  [160/169], [94mLoss[0m : 1.71752

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.832, [92mTest[0m: 2.429, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88256
[1mStep[0m  [16/169], [94mLoss[0m : 1.87244
[1mStep[0m  [32/169], [94mLoss[0m : 1.93526
[1mStep[0m  [48/169], [94mLoss[0m : 1.89459
[1mStep[0m  [64/169], [94mLoss[0m : 1.85161
[1mStep[0m  [80/169], [94mLoss[0m : 1.66164
[1mStep[0m  [96/169], [94mLoss[0m : 1.84094
[1mStep[0m  [112/169], [94mLoss[0m : 2.01728
[1mStep[0m  [128/169], [94mLoss[0m : 1.53383
[1mStep[0m  [144/169], [94mLoss[0m : 1.90393
[1mStep[0m  [160/169], [94mLoss[0m : 1.99793

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.819, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85305
[1mStep[0m  [16/169], [94mLoss[0m : 1.80296
[1mStep[0m  [32/169], [94mLoss[0m : 1.72075
[1mStep[0m  [48/169], [94mLoss[0m : 2.02119
[1mStep[0m  [64/169], [94mLoss[0m : 1.76202
[1mStep[0m  [80/169], [94mLoss[0m : 1.56843
[1mStep[0m  [96/169], [94mLoss[0m : 2.08768
[1mStep[0m  [112/169], [94mLoss[0m : 1.78196
[1mStep[0m  [128/169], [94mLoss[0m : 2.27355
[1mStep[0m  [144/169], [94mLoss[0m : 1.77818
[1mStep[0m  [160/169], [94mLoss[0m : 1.77938

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.788, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55569
[1mStep[0m  [16/169], [94mLoss[0m : 1.62827
[1mStep[0m  [32/169], [94mLoss[0m : 1.78642
[1mStep[0m  [48/169], [94mLoss[0m : 1.91920
[1mStep[0m  [64/169], [94mLoss[0m : 1.52159
[1mStep[0m  [80/169], [94mLoss[0m : 1.81907
[1mStep[0m  [96/169], [94mLoss[0m : 1.67759
[1mStep[0m  [112/169], [94mLoss[0m : 1.49955
[1mStep[0m  [128/169], [94mLoss[0m : 1.77774
[1mStep[0m  [144/169], [94mLoss[0m : 1.86086
[1mStep[0m  [160/169], [94mLoss[0m : 1.84827

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66839
[1mStep[0m  [16/169], [94mLoss[0m : 1.54597
[1mStep[0m  [32/169], [94mLoss[0m : 1.79736
[1mStep[0m  [48/169], [94mLoss[0m : 1.78596
[1mStep[0m  [64/169], [94mLoss[0m : 1.82966
[1mStep[0m  [80/169], [94mLoss[0m : 1.94910
[1mStep[0m  [96/169], [94mLoss[0m : 1.74542
[1mStep[0m  [112/169], [94mLoss[0m : 1.66313
[1mStep[0m  [128/169], [94mLoss[0m : 1.65227
[1mStep[0m  [144/169], [94mLoss[0m : 1.85712
[1mStep[0m  [160/169], [94mLoss[0m : 1.41266

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87283
[1mStep[0m  [16/169], [94mLoss[0m : 1.78279
[1mStep[0m  [32/169], [94mLoss[0m : 1.56850
[1mStep[0m  [48/169], [94mLoss[0m : 2.08461
[1mStep[0m  [64/169], [94mLoss[0m : 1.66908
[1mStep[0m  [80/169], [94mLoss[0m : 1.44166
[1mStep[0m  [96/169], [94mLoss[0m : 1.35708
[1mStep[0m  [112/169], [94mLoss[0m : 1.65763
[1mStep[0m  [128/169], [94mLoss[0m : 1.56664
[1mStep[0m  [144/169], [94mLoss[0m : 1.40640
[1mStep[0m  [160/169], [94mLoss[0m : 2.00141

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59925
[1mStep[0m  [16/169], [94mLoss[0m : 1.89289
[1mStep[0m  [32/169], [94mLoss[0m : 1.68749
[1mStep[0m  [48/169], [94mLoss[0m : 1.81887
[1mStep[0m  [64/169], [94mLoss[0m : 1.49644
[1mStep[0m  [80/169], [94mLoss[0m : 1.74919
[1mStep[0m  [96/169], [94mLoss[0m : 1.67956
[1mStep[0m  [112/169], [94mLoss[0m : 1.74659
[1mStep[0m  [128/169], [94mLoss[0m : 1.57751
[1mStep[0m  [144/169], [94mLoss[0m : 1.61113
[1mStep[0m  [160/169], [94mLoss[0m : 1.76416

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.704, [92mTest[0m: 2.527, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36177
[1mStep[0m  [16/169], [94mLoss[0m : 1.82331
[1mStep[0m  [32/169], [94mLoss[0m : 1.94840
[1mStep[0m  [48/169], [94mLoss[0m : 1.62861
[1mStep[0m  [64/169], [94mLoss[0m : 1.30886
[1mStep[0m  [80/169], [94mLoss[0m : 1.71742
[1mStep[0m  [96/169], [94mLoss[0m : 1.46714
[1mStep[0m  [112/169], [94mLoss[0m : 1.36809
[1mStep[0m  [128/169], [94mLoss[0m : 1.91510
[1mStep[0m  [144/169], [94mLoss[0m : 1.85802
[1mStep[0m  [160/169], [94mLoss[0m : 1.75650

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74793
[1mStep[0m  [16/169], [94mLoss[0m : 1.73857
[1mStep[0m  [32/169], [94mLoss[0m : 1.81726
[1mStep[0m  [48/169], [94mLoss[0m : 1.47103
[1mStep[0m  [64/169], [94mLoss[0m : 1.67661
[1mStep[0m  [80/169], [94mLoss[0m : 1.48604
[1mStep[0m  [96/169], [94mLoss[0m : 1.76045
[1mStep[0m  [112/169], [94mLoss[0m : 1.67543
[1mStep[0m  [128/169], [94mLoss[0m : 1.77346
[1mStep[0m  [144/169], [94mLoss[0m : 1.77028
[1mStep[0m  [160/169], [94mLoss[0m : 1.81075

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.659, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64495
[1mStep[0m  [16/169], [94mLoss[0m : 1.75845
[1mStep[0m  [32/169], [94mLoss[0m : 1.69814
[1mStep[0m  [48/169], [94mLoss[0m : 1.40147
[1mStep[0m  [64/169], [94mLoss[0m : 1.34005
[1mStep[0m  [80/169], [94mLoss[0m : 1.95022
[1mStep[0m  [96/169], [94mLoss[0m : 1.62521
[1mStep[0m  [112/169], [94mLoss[0m : 1.84467
[1mStep[0m  [128/169], [94mLoss[0m : 1.61530
[1mStep[0m  [144/169], [94mLoss[0m : 1.50413
[1mStep[0m  [160/169], [94mLoss[0m : 1.72643

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.580, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.458
====================================

Phase 2 - Evaluation MAE:  2.458153392587389
MAE score P1        2.343893
MAE score P2        2.458153
loss                 1.61623
learning_rate       0.002575
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay            0.01
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 11.07373
[1mStep[0m  [8/84], [94mLoss[0m : 10.35703
[1mStep[0m  [16/84], [94mLoss[0m : 9.96633
[1mStep[0m  [24/84], [94mLoss[0m : 9.39406
[1mStep[0m  [32/84], [94mLoss[0m : 8.85709
[1mStep[0m  [40/84], [94mLoss[0m : 7.60472
[1mStep[0m  [48/84], [94mLoss[0m : 7.05472
[1mStep[0m  [56/84], [94mLoss[0m : 7.31305
[1mStep[0m  [64/84], [94mLoss[0m : 5.97158
[1mStep[0m  [72/84], [94mLoss[0m : 5.32339
[1mStep[0m  [80/84], [94mLoss[0m : 5.38560

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.743, [92mTest[0m: 10.840, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.83884
[1mStep[0m  [8/84], [94mLoss[0m : 4.20516
[1mStep[0m  [16/84], [94mLoss[0m : 3.79978
[1mStep[0m  [24/84], [94mLoss[0m : 3.82367
[1mStep[0m  [32/84], [94mLoss[0m : 3.43545
[1mStep[0m  [40/84], [94mLoss[0m : 2.85167
[1mStep[0m  [48/84], [94mLoss[0m : 3.39236
[1mStep[0m  [56/84], [94mLoss[0m : 2.92568
[1mStep[0m  [64/84], [94mLoss[0m : 2.99361
[1mStep[0m  [72/84], [94mLoss[0m : 2.93642
[1mStep[0m  [80/84], [94mLoss[0m : 2.99537

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.431, [92mTest[0m: 4.726, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78870
[1mStep[0m  [8/84], [94mLoss[0m : 2.65549
[1mStep[0m  [16/84], [94mLoss[0m : 2.68306
[1mStep[0m  [24/84], [94mLoss[0m : 2.56162
[1mStep[0m  [32/84], [94mLoss[0m : 3.11634
[1mStep[0m  [40/84], [94mLoss[0m : 2.59988
[1mStep[0m  [48/84], [94mLoss[0m : 2.51809
[1mStep[0m  [56/84], [94mLoss[0m : 3.35257
[1mStep[0m  [64/84], [94mLoss[0m : 2.70245
[1mStep[0m  [72/84], [94mLoss[0m : 2.58739
[1mStep[0m  [80/84], [94mLoss[0m : 2.37987

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.660, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58064
[1mStep[0m  [8/84], [94mLoss[0m : 2.57225
[1mStep[0m  [16/84], [94mLoss[0m : 2.71678
[1mStep[0m  [24/84], [94mLoss[0m : 2.62307
[1mStep[0m  [32/84], [94mLoss[0m : 2.54646
[1mStep[0m  [40/84], [94mLoss[0m : 2.43734
[1mStep[0m  [48/84], [94mLoss[0m : 3.17046
[1mStep[0m  [56/84], [94mLoss[0m : 2.57238
[1mStep[0m  [64/84], [94mLoss[0m : 2.60668
[1mStep[0m  [72/84], [94mLoss[0m : 2.86096
[1mStep[0m  [80/84], [94mLoss[0m : 2.63853

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64061
[1mStep[0m  [8/84], [94mLoss[0m : 2.72433
[1mStep[0m  [16/84], [94mLoss[0m : 2.39796
[1mStep[0m  [24/84], [94mLoss[0m : 2.60703
[1mStep[0m  [32/84], [94mLoss[0m : 2.45037
[1mStep[0m  [40/84], [94mLoss[0m : 2.45423
[1mStep[0m  [48/84], [94mLoss[0m : 2.43166
[1mStep[0m  [56/84], [94mLoss[0m : 2.45905
[1mStep[0m  [64/84], [94mLoss[0m : 2.41646
[1mStep[0m  [72/84], [94mLoss[0m : 2.49795
[1mStep[0m  [80/84], [94mLoss[0m : 2.78459

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36280
[1mStep[0m  [8/84], [94mLoss[0m : 2.85914
[1mStep[0m  [16/84], [94mLoss[0m : 2.61412
[1mStep[0m  [24/84], [94mLoss[0m : 2.44868
[1mStep[0m  [32/84], [94mLoss[0m : 2.94395
[1mStep[0m  [40/84], [94mLoss[0m : 2.28824
[1mStep[0m  [48/84], [94mLoss[0m : 2.64646
[1mStep[0m  [56/84], [94mLoss[0m : 2.43040
[1mStep[0m  [64/84], [94mLoss[0m : 2.47239
[1mStep[0m  [72/84], [94mLoss[0m : 2.20652
[1mStep[0m  [80/84], [94mLoss[0m : 2.32077

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56961
[1mStep[0m  [8/84], [94mLoss[0m : 2.63915
[1mStep[0m  [16/84], [94mLoss[0m : 2.35768
[1mStep[0m  [24/84], [94mLoss[0m : 2.46893
[1mStep[0m  [32/84], [94mLoss[0m : 2.76192
[1mStep[0m  [40/84], [94mLoss[0m : 2.52476
[1mStep[0m  [48/84], [94mLoss[0m : 2.50708
[1mStep[0m  [56/84], [94mLoss[0m : 2.35966
[1mStep[0m  [64/84], [94mLoss[0m : 2.35703
[1mStep[0m  [72/84], [94mLoss[0m : 2.43154
[1mStep[0m  [80/84], [94mLoss[0m : 2.65936

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.49361
[1mStep[0m  [8/84], [94mLoss[0m : 2.39606
[1mStep[0m  [16/84], [94mLoss[0m : 2.51709
[1mStep[0m  [24/84], [94mLoss[0m : 2.53218
[1mStep[0m  [32/84], [94mLoss[0m : 2.34752
[1mStep[0m  [40/84], [94mLoss[0m : 2.42752
[1mStep[0m  [48/84], [94mLoss[0m : 2.41182
[1mStep[0m  [56/84], [94mLoss[0m : 2.54699
[1mStep[0m  [64/84], [94mLoss[0m : 2.39629
[1mStep[0m  [72/84], [94mLoss[0m : 2.62679
[1mStep[0m  [80/84], [94mLoss[0m : 2.17032

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43151
[1mStep[0m  [8/84], [94mLoss[0m : 2.54801
[1mStep[0m  [16/84], [94mLoss[0m : 2.26684
[1mStep[0m  [24/84], [94mLoss[0m : 2.62995
[1mStep[0m  [32/84], [94mLoss[0m : 2.52540
[1mStep[0m  [40/84], [94mLoss[0m : 2.68702
[1mStep[0m  [48/84], [94mLoss[0m : 2.17363
[1mStep[0m  [56/84], [94mLoss[0m : 2.59881
[1mStep[0m  [64/84], [94mLoss[0m : 2.53260
[1mStep[0m  [72/84], [94mLoss[0m : 2.36606
[1mStep[0m  [80/84], [94mLoss[0m : 2.66832

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65140
[1mStep[0m  [8/84], [94mLoss[0m : 2.44184
[1mStep[0m  [16/84], [94mLoss[0m : 2.24629
[1mStep[0m  [24/84], [94mLoss[0m : 2.44371
[1mStep[0m  [32/84], [94mLoss[0m : 2.55001
[1mStep[0m  [40/84], [94mLoss[0m : 2.40058
[1mStep[0m  [48/84], [94mLoss[0m : 2.54212
[1mStep[0m  [56/84], [94mLoss[0m : 2.56038
[1mStep[0m  [64/84], [94mLoss[0m : 2.57785
[1mStep[0m  [72/84], [94mLoss[0m : 2.63679
[1mStep[0m  [80/84], [94mLoss[0m : 2.49978

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27031
[1mStep[0m  [8/84], [94mLoss[0m : 2.49785
[1mStep[0m  [16/84], [94mLoss[0m : 2.67338
[1mStep[0m  [24/84], [94mLoss[0m : 2.26156
[1mStep[0m  [32/84], [94mLoss[0m : 2.45504
[1mStep[0m  [40/84], [94mLoss[0m : 2.81401
[1mStep[0m  [48/84], [94mLoss[0m : 2.37271
[1mStep[0m  [56/84], [94mLoss[0m : 2.37160
[1mStep[0m  [64/84], [94mLoss[0m : 2.44761
[1mStep[0m  [72/84], [94mLoss[0m : 2.54451
[1mStep[0m  [80/84], [94mLoss[0m : 2.63028

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.504, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.64995
[1mStep[0m  [8/84], [94mLoss[0m : 2.48466
[1mStep[0m  [16/84], [94mLoss[0m : 2.37651
[1mStep[0m  [24/84], [94mLoss[0m : 2.00705
[1mStep[0m  [32/84], [94mLoss[0m : 2.59979
[1mStep[0m  [40/84], [94mLoss[0m : 2.12474
[1mStep[0m  [48/84], [94mLoss[0m : 2.46574
[1mStep[0m  [56/84], [94mLoss[0m : 2.65198
[1mStep[0m  [64/84], [94mLoss[0m : 2.69316
[1mStep[0m  [72/84], [94mLoss[0m : 2.28404
[1mStep[0m  [80/84], [94mLoss[0m : 2.41923

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60559
[1mStep[0m  [8/84], [94mLoss[0m : 2.65809
[1mStep[0m  [16/84], [94mLoss[0m : 2.63857
[1mStep[0m  [24/84], [94mLoss[0m : 2.56515
[1mStep[0m  [32/84], [94mLoss[0m : 2.44445
[1mStep[0m  [40/84], [94mLoss[0m : 2.36843
[1mStep[0m  [48/84], [94mLoss[0m : 2.77745
[1mStep[0m  [56/84], [94mLoss[0m : 2.61661
[1mStep[0m  [64/84], [94mLoss[0m : 2.00771
[1mStep[0m  [72/84], [94mLoss[0m : 2.44734
[1mStep[0m  [80/84], [94mLoss[0m : 2.31764

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31254
[1mStep[0m  [8/84], [94mLoss[0m : 2.54276
[1mStep[0m  [16/84], [94mLoss[0m : 2.59339
[1mStep[0m  [24/84], [94mLoss[0m : 2.29195
[1mStep[0m  [32/84], [94mLoss[0m : 2.46328
[1mStep[0m  [40/84], [94mLoss[0m : 2.39887
[1mStep[0m  [48/84], [94mLoss[0m : 2.44230
[1mStep[0m  [56/84], [94mLoss[0m : 2.40901
[1mStep[0m  [64/84], [94mLoss[0m : 2.33871
[1mStep[0m  [72/84], [94mLoss[0m : 2.44049
[1mStep[0m  [80/84], [94mLoss[0m : 2.85371

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40440
[1mStep[0m  [8/84], [94mLoss[0m : 2.44905
[1mStep[0m  [16/84], [94mLoss[0m : 2.54675
[1mStep[0m  [24/84], [94mLoss[0m : 2.44608
[1mStep[0m  [32/84], [94mLoss[0m : 2.31765
[1mStep[0m  [40/84], [94mLoss[0m : 2.27332
[1mStep[0m  [48/84], [94mLoss[0m : 2.64888
[1mStep[0m  [56/84], [94mLoss[0m : 2.79137
[1mStep[0m  [64/84], [94mLoss[0m : 2.55923
[1mStep[0m  [72/84], [94mLoss[0m : 2.66549
[1mStep[0m  [80/84], [94mLoss[0m : 2.57190

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72379
[1mStep[0m  [8/84], [94mLoss[0m : 2.49243
[1mStep[0m  [16/84], [94mLoss[0m : 2.63435
[1mStep[0m  [24/84], [94mLoss[0m : 2.47317
[1mStep[0m  [32/84], [94mLoss[0m : 2.59405
[1mStep[0m  [40/84], [94mLoss[0m : 2.55090
[1mStep[0m  [48/84], [94mLoss[0m : 2.12185
[1mStep[0m  [56/84], [94mLoss[0m : 2.50849
[1mStep[0m  [64/84], [94mLoss[0m : 2.57511
[1mStep[0m  [72/84], [94mLoss[0m : 2.34446
[1mStep[0m  [80/84], [94mLoss[0m : 2.60569

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.346, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.71926
[1mStep[0m  [8/84], [94mLoss[0m : 2.43414
[1mStep[0m  [16/84], [94mLoss[0m : 2.55228
[1mStep[0m  [24/84], [94mLoss[0m : 2.38822
[1mStep[0m  [32/84], [94mLoss[0m : 2.34470
[1mStep[0m  [40/84], [94mLoss[0m : 2.42927
[1mStep[0m  [48/84], [94mLoss[0m : 2.43882
[1mStep[0m  [56/84], [94mLoss[0m : 2.60621
[1mStep[0m  [64/84], [94mLoss[0m : 2.54031
[1mStep[0m  [72/84], [94mLoss[0m : 2.86880
[1mStep[0m  [80/84], [94mLoss[0m : 2.50470

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.70271
[1mStep[0m  [8/84], [94mLoss[0m : 2.46887
[1mStep[0m  [16/84], [94mLoss[0m : 2.29998
[1mStep[0m  [24/84], [94mLoss[0m : 2.51690
[1mStep[0m  [32/84], [94mLoss[0m : 2.28117
[1mStep[0m  [40/84], [94mLoss[0m : 2.59464
[1mStep[0m  [48/84], [94mLoss[0m : 2.59673
[1mStep[0m  [56/84], [94mLoss[0m : 2.62829
[1mStep[0m  [64/84], [94mLoss[0m : 2.74684
[1mStep[0m  [72/84], [94mLoss[0m : 2.77677
[1mStep[0m  [80/84], [94mLoss[0m : 2.39226

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47084
[1mStep[0m  [8/84], [94mLoss[0m : 2.22929
[1mStep[0m  [16/84], [94mLoss[0m : 2.68749
[1mStep[0m  [24/84], [94mLoss[0m : 2.52533
[1mStep[0m  [32/84], [94mLoss[0m : 2.64938
[1mStep[0m  [40/84], [94mLoss[0m : 2.30787
[1mStep[0m  [48/84], [94mLoss[0m : 2.54011
[1mStep[0m  [56/84], [94mLoss[0m : 2.48235
[1mStep[0m  [64/84], [94mLoss[0m : 2.28338
[1mStep[0m  [72/84], [94mLoss[0m : 2.50291
[1mStep[0m  [80/84], [94mLoss[0m : 2.65838

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40296
[1mStep[0m  [8/84], [94mLoss[0m : 2.37568
[1mStep[0m  [16/84], [94mLoss[0m : 2.29079
[1mStep[0m  [24/84], [94mLoss[0m : 2.60576
[1mStep[0m  [32/84], [94mLoss[0m : 2.24402
[1mStep[0m  [40/84], [94mLoss[0m : 2.44769
[1mStep[0m  [48/84], [94mLoss[0m : 2.31081
[1mStep[0m  [56/84], [94mLoss[0m : 2.55296
[1mStep[0m  [64/84], [94mLoss[0m : 2.32562
[1mStep[0m  [72/84], [94mLoss[0m : 2.36646
[1mStep[0m  [80/84], [94mLoss[0m : 2.39502

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79617
[1mStep[0m  [8/84], [94mLoss[0m : 2.53686
[1mStep[0m  [16/84], [94mLoss[0m : 2.30507
[1mStep[0m  [24/84], [94mLoss[0m : 2.89554
[1mStep[0m  [32/84], [94mLoss[0m : 2.67951
[1mStep[0m  [40/84], [94mLoss[0m : 2.37482
[1mStep[0m  [48/84], [94mLoss[0m : 2.44910
[1mStep[0m  [56/84], [94mLoss[0m : 2.18978
[1mStep[0m  [64/84], [94mLoss[0m : 2.62512
[1mStep[0m  [72/84], [94mLoss[0m : 2.99963
[1mStep[0m  [80/84], [94mLoss[0m : 2.37263

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59612
[1mStep[0m  [8/84], [94mLoss[0m : 2.65260
[1mStep[0m  [16/84], [94mLoss[0m : 2.24431
[1mStep[0m  [24/84], [94mLoss[0m : 2.16066
[1mStep[0m  [32/84], [94mLoss[0m : 2.37794
[1mStep[0m  [40/84], [94mLoss[0m : 2.66467
[1mStep[0m  [48/84], [94mLoss[0m : 2.59424
[1mStep[0m  [56/84], [94mLoss[0m : 2.66974
[1mStep[0m  [64/84], [94mLoss[0m : 2.42551
[1mStep[0m  [72/84], [94mLoss[0m : 2.34098
[1mStep[0m  [80/84], [94mLoss[0m : 2.35888

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.50205
[1mStep[0m  [8/84], [94mLoss[0m : 2.17738
[1mStep[0m  [16/84], [94mLoss[0m : 2.03648
[1mStep[0m  [24/84], [94mLoss[0m : 3.04283
[1mStep[0m  [32/84], [94mLoss[0m : 2.47206
[1mStep[0m  [40/84], [94mLoss[0m : 2.53686
[1mStep[0m  [48/84], [94mLoss[0m : 2.49634
[1mStep[0m  [56/84], [94mLoss[0m : 2.68010
[1mStep[0m  [64/84], [94mLoss[0m : 2.29240
[1mStep[0m  [72/84], [94mLoss[0m : 2.18225
[1mStep[0m  [80/84], [94mLoss[0m : 2.48439

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20807
[1mStep[0m  [8/84], [94mLoss[0m : 2.50175
[1mStep[0m  [16/84], [94mLoss[0m : 2.61852
[1mStep[0m  [24/84], [94mLoss[0m : 2.43322
[1mStep[0m  [32/84], [94mLoss[0m : 2.38173
[1mStep[0m  [40/84], [94mLoss[0m : 2.55964
[1mStep[0m  [48/84], [94mLoss[0m : 2.51897
[1mStep[0m  [56/84], [94mLoss[0m : 2.61411
[1mStep[0m  [64/84], [94mLoss[0m : 2.21067
[1mStep[0m  [72/84], [94mLoss[0m : 2.60473
[1mStep[0m  [80/84], [94mLoss[0m : 2.34767

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54655
[1mStep[0m  [8/84], [94mLoss[0m : 2.20745
[1mStep[0m  [16/84], [94mLoss[0m : 2.46552
[1mStep[0m  [24/84], [94mLoss[0m : 2.46830
[1mStep[0m  [32/84], [94mLoss[0m : 2.63307
[1mStep[0m  [40/84], [94mLoss[0m : 2.51298
[1mStep[0m  [48/84], [94mLoss[0m : 2.49207
[1mStep[0m  [56/84], [94mLoss[0m : 2.62295
[1mStep[0m  [64/84], [94mLoss[0m : 2.75482
[1mStep[0m  [72/84], [94mLoss[0m : 2.10296
[1mStep[0m  [80/84], [94mLoss[0m : 2.76097

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31895
[1mStep[0m  [8/84], [94mLoss[0m : 2.66660
[1mStep[0m  [16/84], [94mLoss[0m : 2.78512
[1mStep[0m  [24/84], [94mLoss[0m : 2.70100
[1mStep[0m  [32/84], [94mLoss[0m : 2.43342
[1mStep[0m  [40/84], [94mLoss[0m : 2.19301
[1mStep[0m  [48/84], [94mLoss[0m : 2.29298
[1mStep[0m  [56/84], [94mLoss[0m : 2.43785
[1mStep[0m  [64/84], [94mLoss[0m : 2.38848
[1mStep[0m  [72/84], [94mLoss[0m : 2.51411
[1mStep[0m  [80/84], [94mLoss[0m : 2.25342

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45613
[1mStep[0m  [8/84], [94mLoss[0m : 2.56171
[1mStep[0m  [16/84], [94mLoss[0m : 2.53594
[1mStep[0m  [24/84], [94mLoss[0m : 2.49108
[1mStep[0m  [32/84], [94mLoss[0m : 2.38740
[1mStep[0m  [40/84], [94mLoss[0m : 2.33205
[1mStep[0m  [48/84], [94mLoss[0m : 2.73095
[1mStep[0m  [56/84], [94mLoss[0m : 2.67333
[1mStep[0m  [64/84], [94mLoss[0m : 2.58103
[1mStep[0m  [72/84], [94mLoss[0m : 2.46042
[1mStep[0m  [80/84], [94mLoss[0m : 2.34907

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25451
[1mStep[0m  [8/84], [94mLoss[0m : 2.42420
[1mStep[0m  [16/84], [94mLoss[0m : 2.36344
[1mStep[0m  [24/84], [94mLoss[0m : 2.30106
[1mStep[0m  [32/84], [94mLoss[0m : 2.68002
[1mStep[0m  [40/84], [94mLoss[0m : 2.31970
[1mStep[0m  [48/84], [94mLoss[0m : 2.73205
[1mStep[0m  [56/84], [94mLoss[0m : 2.43759
[1mStep[0m  [64/84], [94mLoss[0m : 2.79084
[1mStep[0m  [72/84], [94mLoss[0m : 2.41334
[1mStep[0m  [80/84], [94mLoss[0m : 2.54643

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31417
[1mStep[0m  [8/84], [94mLoss[0m : 2.44742
[1mStep[0m  [16/84], [94mLoss[0m : 2.46369
[1mStep[0m  [24/84], [94mLoss[0m : 2.26383
[1mStep[0m  [32/84], [94mLoss[0m : 2.38772
[1mStep[0m  [40/84], [94mLoss[0m : 2.39483
[1mStep[0m  [48/84], [94mLoss[0m : 2.62337
[1mStep[0m  [56/84], [94mLoss[0m : 2.38382
[1mStep[0m  [64/84], [94mLoss[0m : 2.22653
[1mStep[0m  [72/84], [94mLoss[0m : 2.36298
[1mStep[0m  [80/84], [94mLoss[0m : 2.19819

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29558
[1mStep[0m  [8/84], [94mLoss[0m : 2.37659
[1mStep[0m  [16/84], [94mLoss[0m : 2.24200
[1mStep[0m  [24/84], [94mLoss[0m : 2.31730
[1mStep[0m  [32/84], [94mLoss[0m : 2.45738
[1mStep[0m  [40/84], [94mLoss[0m : 2.50286
[1mStep[0m  [48/84], [94mLoss[0m : 2.55547
[1mStep[0m  [56/84], [94mLoss[0m : 2.17520
[1mStep[0m  [64/84], [94mLoss[0m : 2.23805
[1mStep[0m  [72/84], [94mLoss[0m : 2.94383
[1mStep[0m  [80/84], [94mLoss[0m : 2.37416

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.321
====================================

Phase 1 - Evaluation MAE:  2.320952926363264
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.43107
[1mStep[0m  [8/84], [94mLoss[0m : 2.38432
[1mStep[0m  [16/84], [94mLoss[0m : 2.33457
[1mStep[0m  [24/84], [94mLoss[0m : 2.39934
[1mStep[0m  [32/84], [94mLoss[0m : 2.39825
[1mStep[0m  [40/84], [94mLoss[0m : 2.66486
[1mStep[0m  [48/84], [94mLoss[0m : 2.35877
[1mStep[0m  [56/84], [94mLoss[0m : 2.47525
[1mStep[0m  [64/84], [94mLoss[0m : 2.64496
[1mStep[0m  [72/84], [94mLoss[0m : 2.46353
[1mStep[0m  [80/84], [94mLoss[0m : 2.57578

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.329, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18826
[1mStep[0m  [8/84], [94mLoss[0m : 2.13720
[1mStep[0m  [16/84], [94mLoss[0m : 2.36152
[1mStep[0m  [24/84], [94mLoss[0m : 2.68400
[1mStep[0m  [32/84], [94mLoss[0m : 2.41903
[1mStep[0m  [40/84], [94mLoss[0m : 2.37837
[1mStep[0m  [48/84], [94mLoss[0m : 2.38728
[1mStep[0m  [56/84], [94mLoss[0m : 2.62497
[1mStep[0m  [64/84], [94mLoss[0m : 2.41659
[1mStep[0m  [72/84], [94mLoss[0m : 2.53610
[1mStep[0m  [80/84], [94mLoss[0m : 2.23512

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.36807
[1mStep[0m  [8/84], [94mLoss[0m : 2.33173
[1mStep[0m  [16/84], [94mLoss[0m : 2.35026
[1mStep[0m  [24/84], [94mLoss[0m : 2.33707
[1mStep[0m  [32/84], [94mLoss[0m : 2.68492
[1mStep[0m  [40/84], [94mLoss[0m : 2.38420
[1mStep[0m  [48/84], [94mLoss[0m : 2.66120
[1mStep[0m  [56/84], [94mLoss[0m : 2.10189
[1mStep[0m  [64/84], [94mLoss[0m : 2.38012
[1mStep[0m  [72/84], [94mLoss[0m : 2.48105
[1mStep[0m  [80/84], [94mLoss[0m : 2.29180

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46689
[1mStep[0m  [8/84], [94mLoss[0m : 2.63427
[1mStep[0m  [16/84], [94mLoss[0m : 2.15355
[1mStep[0m  [24/84], [94mLoss[0m : 2.62346
[1mStep[0m  [32/84], [94mLoss[0m : 2.38720
[1mStep[0m  [40/84], [94mLoss[0m : 2.43219
[1mStep[0m  [48/84], [94mLoss[0m : 2.27566
[1mStep[0m  [56/84], [94mLoss[0m : 2.45997
[1mStep[0m  [64/84], [94mLoss[0m : 2.47392
[1mStep[0m  [72/84], [94mLoss[0m : 2.55749
[1mStep[0m  [80/84], [94mLoss[0m : 2.44085

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41947
[1mStep[0m  [8/84], [94mLoss[0m : 2.51705
[1mStep[0m  [16/84], [94mLoss[0m : 2.52601
[1mStep[0m  [24/84], [94mLoss[0m : 2.56308
[1mStep[0m  [32/84], [94mLoss[0m : 2.46207
[1mStep[0m  [40/84], [94mLoss[0m : 2.58987
[1mStep[0m  [48/84], [94mLoss[0m : 2.55168
[1mStep[0m  [56/84], [94mLoss[0m : 2.42372
[1mStep[0m  [64/84], [94mLoss[0m : 2.22758
[1mStep[0m  [72/84], [94mLoss[0m : 2.37730
[1mStep[0m  [80/84], [94mLoss[0m : 2.38587

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.532, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29438
[1mStep[0m  [8/84], [94mLoss[0m : 2.23641
[1mStep[0m  [16/84], [94mLoss[0m : 2.11737
[1mStep[0m  [24/84], [94mLoss[0m : 2.45402
[1mStep[0m  [32/84], [94mLoss[0m : 2.67088
[1mStep[0m  [40/84], [94mLoss[0m : 2.40973
[1mStep[0m  [48/84], [94mLoss[0m : 2.49886
[1mStep[0m  [56/84], [94mLoss[0m : 2.29619
[1mStep[0m  [64/84], [94mLoss[0m : 2.55626
[1mStep[0m  [72/84], [94mLoss[0m : 2.30507
[1mStep[0m  [80/84], [94mLoss[0m : 2.57540

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.552, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59721
[1mStep[0m  [8/84], [94mLoss[0m : 2.29904
[1mStep[0m  [16/84], [94mLoss[0m : 2.03509
[1mStep[0m  [24/84], [94mLoss[0m : 2.42124
[1mStep[0m  [32/84], [94mLoss[0m : 2.33240
[1mStep[0m  [40/84], [94mLoss[0m : 2.46570
[1mStep[0m  [48/84], [94mLoss[0m : 2.46387
[1mStep[0m  [56/84], [94mLoss[0m : 2.41804
[1mStep[0m  [64/84], [94mLoss[0m : 2.62649
[1mStep[0m  [72/84], [94mLoss[0m : 2.52356
[1mStep[0m  [80/84], [94mLoss[0m : 2.33308

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53402
[1mStep[0m  [8/84], [94mLoss[0m : 2.22515
[1mStep[0m  [16/84], [94mLoss[0m : 2.52301
[1mStep[0m  [24/84], [94mLoss[0m : 2.38484
[1mStep[0m  [32/84], [94mLoss[0m : 2.46824
[1mStep[0m  [40/84], [94mLoss[0m : 2.55508
[1mStep[0m  [48/84], [94mLoss[0m : 2.54805
[1mStep[0m  [56/84], [94mLoss[0m : 2.15052
[1mStep[0m  [64/84], [94mLoss[0m : 2.17148
[1mStep[0m  [72/84], [94mLoss[0m : 2.37366
[1mStep[0m  [80/84], [94mLoss[0m : 2.47812

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33444
[1mStep[0m  [8/84], [94mLoss[0m : 2.22843
[1mStep[0m  [16/84], [94mLoss[0m : 2.55613
[1mStep[0m  [24/84], [94mLoss[0m : 2.42419
[1mStep[0m  [32/84], [94mLoss[0m : 2.35611
[1mStep[0m  [40/84], [94mLoss[0m : 2.59817
[1mStep[0m  [48/84], [94mLoss[0m : 2.46355
[1mStep[0m  [56/84], [94mLoss[0m : 2.36988
[1mStep[0m  [64/84], [94mLoss[0m : 2.34992
[1mStep[0m  [72/84], [94mLoss[0m : 2.47919
[1mStep[0m  [80/84], [94mLoss[0m : 2.10141

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.533, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40861
[1mStep[0m  [8/84], [94mLoss[0m : 2.04587
[1mStep[0m  [16/84], [94mLoss[0m : 2.22824
[1mStep[0m  [24/84], [94mLoss[0m : 2.54288
[1mStep[0m  [32/84], [94mLoss[0m : 2.41942
[1mStep[0m  [40/84], [94mLoss[0m : 2.25737
[1mStep[0m  [48/84], [94mLoss[0m : 2.05844
[1mStep[0m  [56/84], [94mLoss[0m : 2.31258
[1mStep[0m  [64/84], [94mLoss[0m : 2.18656
[1mStep[0m  [72/84], [94mLoss[0m : 2.50924
[1mStep[0m  [80/84], [94mLoss[0m : 2.18389

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.51987
[1mStep[0m  [8/84], [94mLoss[0m : 2.37998
[1mStep[0m  [16/84], [94mLoss[0m : 2.27237
[1mStep[0m  [24/84], [94mLoss[0m : 2.42930
[1mStep[0m  [32/84], [94mLoss[0m : 2.36797
[1mStep[0m  [40/84], [94mLoss[0m : 2.54749
[1mStep[0m  [48/84], [94mLoss[0m : 2.06304
[1mStep[0m  [56/84], [94mLoss[0m : 2.30981
[1mStep[0m  [64/84], [94mLoss[0m : 2.10639
[1mStep[0m  [72/84], [94mLoss[0m : 2.10469
[1mStep[0m  [80/84], [94mLoss[0m : 2.40530

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.302, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05642
[1mStep[0m  [8/84], [94mLoss[0m : 2.17359
[1mStep[0m  [16/84], [94mLoss[0m : 2.32782
[1mStep[0m  [24/84], [94mLoss[0m : 2.38683
[1mStep[0m  [32/84], [94mLoss[0m : 2.29045
[1mStep[0m  [40/84], [94mLoss[0m : 2.39933
[1mStep[0m  [48/84], [94mLoss[0m : 2.56139
[1mStep[0m  [56/84], [94mLoss[0m : 2.18940
[1mStep[0m  [64/84], [94mLoss[0m : 2.44628
[1mStep[0m  [72/84], [94mLoss[0m : 2.33943
[1mStep[0m  [80/84], [94mLoss[0m : 2.42782

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.277, [92mTest[0m: 2.514, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48234
[1mStep[0m  [8/84], [94mLoss[0m : 2.03569
[1mStep[0m  [16/84], [94mLoss[0m : 2.40523
[1mStep[0m  [24/84], [94mLoss[0m : 2.30185
[1mStep[0m  [32/84], [94mLoss[0m : 2.26305
[1mStep[0m  [40/84], [94mLoss[0m : 2.14544
[1mStep[0m  [48/84], [94mLoss[0m : 2.02069
[1mStep[0m  [56/84], [94mLoss[0m : 2.28811
[1mStep[0m  [64/84], [94mLoss[0m : 2.21758
[1mStep[0m  [72/84], [94mLoss[0m : 2.22895
[1mStep[0m  [80/84], [94mLoss[0m : 2.24900

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27632
[1mStep[0m  [8/84], [94mLoss[0m : 2.39908
[1mStep[0m  [16/84], [94mLoss[0m : 2.19783
[1mStep[0m  [24/84], [94mLoss[0m : 2.15613
[1mStep[0m  [32/84], [94mLoss[0m : 2.34502
[1mStep[0m  [40/84], [94mLoss[0m : 2.25344
[1mStep[0m  [48/84], [94mLoss[0m : 1.90323
[1mStep[0m  [56/84], [94mLoss[0m : 2.12238
[1mStep[0m  [64/84], [94mLoss[0m : 2.32689
[1mStep[0m  [72/84], [94mLoss[0m : 2.44513
[1mStep[0m  [80/84], [94mLoss[0m : 2.10115

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.232, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18732
[1mStep[0m  [8/84], [94mLoss[0m : 2.12664
[1mStep[0m  [16/84], [94mLoss[0m : 2.34195
[1mStep[0m  [24/84], [94mLoss[0m : 1.93996
[1mStep[0m  [32/84], [94mLoss[0m : 2.14848
[1mStep[0m  [40/84], [94mLoss[0m : 2.43528
[1mStep[0m  [48/84], [94mLoss[0m : 2.33820
[1mStep[0m  [56/84], [94mLoss[0m : 2.08031
[1mStep[0m  [64/84], [94mLoss[0m : 2.19843
[1mStep[0m  [72/84], [94mLoss[0m : 2.10317
[1mStep[0m  [80/84], [94mLoss[0m : 2.51515

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.222, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07813
[1mStep[0m  [8/84], [94mLoss[0m : 2.35343
[1mStep[0m  [16/84], [94mLoss[0m : 2.23629
[1mStep[0m  [24/84], [94mLoss[0m : 2.22777
[1mStep[0m  [32/84], [94mLoss[0m : 1.99161
[1mStep[0m  [40/84], [94mLoss[0m : 2.25543
[1mStep[0m  [48/84], [94mLoss[0m : 2.03340
[1mStep[0m  [56/84], [94mLoss[0m : 2.33139
[1mStep[0m  [64/84], [94mLoss[0m : 2.04716
[1mStep[0m  [72/84], [94mLoss[0m : 2.39407
[1mStep[0m  [80/84], [94mLoss[0m : 2.45361

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.07816
[1mStep[0m  [8/84], [94mLoss[0m : 2.35671
[1mStep[0m  [16/84], [94mLoss[0m : 2.16136
[1mStep[0m  [24/84], [94mLoss[0m : 2.32292
[1mStep[0m  [32/84], [94mLoss[0m : 2.15538
[1mStep[0m  [40/84], [94mLoss[0m : 2.27443
[1mStep[0m  [48/84], [94mLoss[0m : 1.98735
[1mStep[0m  [56/84], [94mLoss[0m : 2.38147
[1mStep[0m  [64/84], [94mLoss[0m : 1.94316
[1mStep[0m  [72/84], [94mLoss[0m : 2.11388
[1mStep[0m  [80/84], [94mLoss[0m : 2.30367

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.183, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.79679
[1mStep[0m  [8/84], [94mLoss[0m : 2.18155
[1mStep[0m  [16/84], [94mLoss[0m : 2.07590
[1mStep[0m  [24/84], [94mLoss[0m : 2.16527
[1mStep[0m  [32/84], [94mLoss[0m : 2.08780
[1mStep[0m  [40/84], [94mLoss[0m : 1.96186
[1mStep[0m  [48/84], [94mLoss[0m : 2.07870
[1mStep[0m  [56/84], [94mLoss[0m : 2.07657
[1mStep[0m  [64/84], [94mLoss[0m : 2.30583
[1mStep[0m  [72/84], [94mLoss[0m : 2.13173
[1mStep[0m  [80/84], [94mLoss[0m : 1.90450

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.162, [92mTest[0m: 2.460, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23669
[1mStep[0m  [8/84], [94mLoss[0m : 1.95354
[1mStep[0m  [16/84], [94mLoss[0m : 2.12951
[1mStep[0m  [24/84], [94mLoss[0m : 2.35394
[1mStep[0m  [32/84], [94mLoss[0m : 2.23284
[1mStep[0m  [40/84], [94mLoss[0m : 1.98974
[1mStep[0m  [48/84], [94mLoss[0m : 2.15749
[1mStep[0m  [56/84], [94mLoss[0m : 2.18268
[1mStep[0m  [64/84], [94mLoss[0m : 1.91231
[1mStep[0m  [72/84], [94mLoss[0m : 2.15593
[1mStep[0m  [80/84], [94mLoss[0m : 2.12871

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.135, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11874
[1mStep[0m  [8/84], [94mLoss[0m : 1.81398
[1mStep[0m  [16/84], [94mLoss[0m : 1.99253
[1mStep[0m  [24/84], [94mLoss[0m : 2.09627
[1mStep[0m  [32/84], [94mLoss[0m : 2.32515
[1mStep[0m  [40/84], [94mLoss[0m : 1.72512
[1mStep[0m  [48/84], [94mLoss[0m : 2.14842
[1mStep[0m  [56/84], [94mLoss[0m : 2.34625
[1mStep[0m  [64/84], [94mLoss[0m : 1.90561
[1mStep[0m  [72/84], [94mLoss[0m : 2.03541
[1mStep[0m  [80/84], [94mLoss[0m : 2.23033

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80633
[1mStep[0m  [8/84], [94mLoss[0m : 2.00931
[1mStep[0m  [16/84], [94mLoss[0m : 2.10032
[1mStep[0m  [24/84], [94mLoss[0m : 1.95877
[1mStep[0m  [32/84], [94mLoss[0m : 2.02818
[1mStep[0m  [40/84], [94mLoss[0m : 2.04840
[1mStep[0m  [48/84], [94mLoss[0m : 1.93092
[1mStep[0m  [56/84], [94mLoss[0m : 2.33085
[1mStep[0m  [64/84], [94mLoss[0m : 2.30577
[1mStep[0m  [72/84], [94mLoss[0m : 1.96925
[1mStep[0m  [80/84], [94mLoss[0m : 2.24621

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12997
[1mStep[0m  [8/84], [94mLoss[0m : 2.00968
[1mStep[0m  [16/84], [94mLoss[0m : 1.74374
[1mStep[0m  [24/84], [94mLoss[0m : 1.87940
[1mStep[0m  [32/84], [94mLoss[0m : 1.94628
[1mStep[0m  [40/84], [94mLoss[0m : 1.79235
[1mStep[0m  [48/84], [94mLoss[0m : 2.17103
[1mStep[0m  [56/84], [94mLoss[0m : 2.26820
[1mStep[0m  [64/84], [94mLoss[0m : 2.03458
[1mStep[0m  [72/84], [94mLoss[0m : 2.38284
[1mStep[0m  [80/84], [94mLoss[0m : 2.15064

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87608
[1mStep[0m  [8/84], [94mLoss[0m : 2.23600
[1mStep[0m  [16/84], [94mLoss[0m : 2.36832
[1mStep[0m  [24/84], [94mLoss[0m : 1.87316
[1mStep[0m  [32/84], [94mLoss[0m : 2.08749
[1mStep[0m  [40/84], [94mLoss[0m : 2.09766
[1mStep[0m  [48/84], [94mLoss[0m : 2.08094
[1mStep[0m  [56/84], [94mLoss[0m : 2.13863
[1mStep[0m  [64/84], [94mLoss[0m : 2.09359
[1mStep[0m  [72/84], [94mLoss[0m : 1.91410
[1mStep[0m  [80/84], [94mLoss[0m : 2.08754

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34181
[1mStep[0m  [8/84], [94mLoss[0m : 1.84494
[1mStep[0m  [16/84], [94mLoss[0m : 2.21071
[1mStep[0m  [24/84], [94mLoss[0m : 2.04067
[1mStep[0m  [32/84], [94mLoss[0m : 1.83210
[1mStep[0m  [40/84], [94mLoss[0m : 1.90935
[1mStep[0m  [48/84], [94mLoss[0m : 2.02166
[1mStep[0m  [56/84], [94mLoss[0m : 2.38448
[1mStep[0m  [64/84], [94mLoss[0m : 1.85249
[1mStep[0m  [72/84], [94mLoss[0m : 1.81298
[1mStep[0m  [80/84], [94mLoss[0m : 1.86453

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.96278
[1mStep[0m  [8/84], [94mLoss[0m : 1.84895
[1mStep[0m  [16/84], [94mLoss[0m : 1.85851
[1mStep[0m  [24/84], [94mLoss[0m : 1.90152
[1mStep[0m  [32/84], [94mLoss[0m : 1.73698
[1mStep[0m  [40/84], [94mLoss[0m : 2.09238
[1mStep[0m  [48/84], [94mLoss[0m : 2.18807
[1mStep[0m  [56/84], [94mLoss[0m : 1.77539
[1mStep[0m  [64/84], [94mLoss[0m : 2.02858
[1mStep[0m  [72/84], [94mLoss[0m : 2.09695
[1mStep[0m  [80/84], [94mLoss[0m : 1.90614

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.987, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.85632
[1mStep[0m  [8/84], [94mLoss[0m : 2.03794
[1mStep[0m  [16/84], [94mLoss[0m : 1.80851
[1mStep[0m  [24/84], [94mLoss[0m : 1.99457
[1mStep[0m  [32/84], [94mLoss[0m : 1.87215
[1mStep[0m  [40/84], [94mLoss[0m : 2.07960
[1mStep[0m  [48/84], [94mLoss[0m : 2.11576
[1mStep[0m  [56/84], [94mLoss[0m : 1.99014
[1mStep[0m  [64/84], [94mLoss[0m : 2.02687
[1mStep[0m  [72/84], [94mLoss[0m : 2.29944
[1mStep[0m  [80/84], [94mLoss[0m : 1.70953

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.968, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93465
[1mStep[0m  [8/84], [94mLoss[0m : 1.75048
[1mStep[0m  [16/84], [94mLoss[0m : 1.96495
[1mStep[0m  [24/84], [94mLoss[0m : 1.90243
[1mStep[0m  [32/84], [94mLoss[0m : 1.84336
[1mStep[0m  [40/84], [94mLoss[0m : 2.10206
[1mStep[0m  [48/84], [94mLoss[0m : 2.17205
[1mStep[0m  [56/84], [94mLoss[0m : 1.89062
[1mStep[0m  [64/84], [94mLoss[0m : 1.96173
[1mStep[0m  [72/84], [94mLoss[0m : 1.74230
[1mStep[0m  [80/84], [94mLoss[0m : 1.96005

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.947, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88241
[1mStep[0m  [8/84], [94mLoss[0m : 1.77204
[1mStep[0m  [16/84], [94mLoss[0m : 1.82614
[1mStep[0m  [24/84], [94mLoss[0m : 2.04368
[1mStep[0m  [32/84], [94mLoss[0m : 1.94344
[1mStep[0m  [40/84], [94mLoss[0m : 2.09686
[1mStep[0m  [48/84], [94mLoss[0m : 1.97348
[1mStep[0m  [56/84], [94mLoss[0m : 1.92538
[1mStep[0m  [64/84], [94mLoss[0m : 2.12584
[1mStep[0m  [72/84], [94mLoss[0m : 1.83455
[1mStep[0m  [80/84], [94mLoss[0m : 1.91932

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92209
[1mStep[0m  [8/84], [94mLoss[0m : 1.85061
[1mStep[0m  [16/84], [94mLoss[0m : 1.90537
[1mStep[0m  [24/84], [94mLoss[0m : 2.18133
[1mStep[0m  [32/84], [94mLoss[0m : 1.84735
[1mStep[0m  [40/84], [94mLoss[0m : 1.85962
[1mStep[0m  [48/84], [94mLoss[0m : 2.03315
[1mStep[0m  [56/84], [94mLoss[0m : 1.73061
[1mStep[0m  [64/84], [94mLoss[0m : 1.84092
[1mStep[0m  [72/84], [94mLoss[0m : 2.11315
[1mStep[0m  [80/84], [94mLoss[0m : 1.95423

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.02125
[1mStep[0m  [8/84], [94mLoss[0m : 1.89202
[1mStep[0m  [16/84], [94mLoss[0m : 2.01164
[1mStep[0m  [24/84], [94mLoss[0m : 2.19039
[1mStep[0m  [32/84], [94mLoss[0m : 1.57295
[1mStep[0m  [40/84], [94mLoss[0m : 1.96198
[1mStep[0m  [48/84], [94mLoss[0m : 1.94711
[1mStep[0m  [56/84], [94mLoss[0m : 1.75491
[1mStep[0m  [64/84], [94mLoss[0m : 1.90609
[1mStep[0m  [72/84], [94mLoss[0m : 1.77030
[1mStep[0m  [80/84], [94mLoss[0m : 1.94246

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.888, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.449
====================================

Phase 2 - Evaluation MAE:  2.448896356991359
MAE score P1      2.320953
MAE score P2      2.448896
loss              1.887852
learning_rate     0.002575
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay         0.001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 11.08403
[1mStep[0m  [8/84], [94mLoss[0m : 10.70160
[1mStep[0m  [16/84], [94mLoss[0m : 11.20073
[1mStep[0m  [24/84], [94mLoss[0m : 10.30555
[1mStep[0m  [32/84], [94mLoss[0m : 10.72551
[1mStep[0m  [40/84], [94mLoss[0m : 10.57445
[1mStep[0m  [48/84], [94mLoss[0m : 10.82894
[1mStep[0m  [56/84], [94mLoss[0m : 10.85875
[1mStep[0m  [64/84], [94mLoss[0m : 11.24773
[1mStep[0m  [72/84], [94mLoss[0m : 10.16508
[1mStep[0m  [80/84], [94mLoss[0m : 10.10315

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.690, [92mTest[0m: 10.965, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.22801
[1mStep[0m  [8/84], [94mLoss[0m : 10.34179
[1mStep[0m  [16/84], [94mLoss[0m : 10.28053
[1mStep[0m  [24/84], [94mLoss[0m : 10.67963
[1mStep[0m  [32/84], [94mLoss[0m : 10.23871
[1mStep[0m  [40/84], [94mLoss[0m : 10.30055
[1mStep[0m  [48/84], [94mLoss[0m : 10.12690
[1mStep[0m  [56/84], [94mLoss[0m : 10.10423
[1mStep[0m  [64/84], [94mLoss[0m : 10.50264
[1mStep[0m  [72/84], [94mLoss[0m : 10.42327
[1mStep[0m  [80/84], [94mLoss[0m : 9.63049

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.291, [92mTest[0m: 10.324, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.70647
[1mStep[0m  [8/84], [94mLoss[0m : 9.51517
[1mStep[0m  [16/84], [94mLoss[0m : 9.97620
[1mStep[0m  [24/84], [94mLoss[0m : 9.56004
[1mStep[0m  [32/84], [94mLoss[0m : 10.12223
[1mStep[0m  [40/84], [94mLoss[0m : 10.00718
[1mStep[0m  [48/84], [94mLoss[0m : 9.97230
[1mStep[0m  [56/84], [94mLoss[0m : 9.52698
[1mStep[0m  [64/84], [94mLoss[0m : 10.17646
[1mStep[0m  [72/84], [94mLoss[0m : 9.89407
[1mStep[0m  [80/84], [94mLoss[0m : 9.24900

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.826, [92mTest[0m: 9.734, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 9.52528
[1mStep[0m  [8/84], [94mLoss[0m : 9.12100
[1mStep[0m  [16/84], [94mLoss[0m : 8.98326
[1mStep[0m  [24/84], [94mLoss[0m : 9.31796
[1mStep[0m  [32/84], [94mLoss[0m : 9.25919
[1mStep[0m  [40/84], [94mLoss[0m : 8.92363
[1mStep[0m  [48/84], [94mLoss[0m : 8.94810
[1mStep[0m  [56/84], [94mLoss[0m : 8.85738
[1mStep[0m  [64/84], [94mLoss[0m : 9.45740
[1mStep[0m  [72/84], [94mLoss[0m : 9.32437
[1mStep[0m  [80/84], [94mLoss[0m : 8.70028

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.303, [92mTest[0m: 9.042, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.79318
[1mStep[0m  [8/84], [94mLoss[0m : 8.61078
[1mStep[0m  [16/84], [94mLoss[0m : 8.50604
[1mStep[0m  [24/84], [94mLoss[0m : 8.72569
[1mStep[0m  [32/84], [94mLoss[0m : 8.46979
[1mStep[0m  [40/84], [94mLoss[0m : 9.18408
[1mStep[0m  [48/84], [94mLoss[0m : 8.77474
[1mStep[0m  [56/84], [94mLoss[0m : 8.56310
[1mStep[0m  [64/84], [94mLoss[0m : 8.46142
[1mStep[0m  [72/84], [94mLoss[0m : 9.34210
[1mStep[0m  [80/84], [94mLoss[0m : 8.92321

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.730, [92mTest[0m: 8.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.58833
[1mStep[0m  [8/84], [94mLoss[0m : 8.38663
[1mStep[0m  [16/84], [94mLoss[0m : 8.32720
[1mStep[0m  [24/84], [94mLoss[0m : 7.87024
[1mStep[0m  [32/84], [94mLoss[0m : 8.27523
[1mStep[0m  [40/84], [94mLoss[0m : 8.21506
[1mStep[0m  [48/84], [94mLoss[0m : 8.50581
[1mStep[0m  [56/84], [94mLoss[0m : 8.12028
[1mStep[0m  [64/84], [94mLoss[0m : 7.74233
[1mStep[0m  [72/84], [94mLoss[0m : 8.17495
[1mStep[0m  [80/84], [94mLoss[0m : 8.01528

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.172, [92mTest[0m: 7.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.93159
[1mStep[0m  [8/84], [94mLoss[0m : 8.06087
[1mStep[0m  [16/84], [94mLoss[0m : 7.89523
[1mStep[0m  [24/84], [94mLoss[0m : 7.60647
[1mStep[0m  [32/84], [94mLoss[0m : 7.64662
[1mStep[0m  [40/84], [94mLoss[0m : 7.03670
[1mStep[0m  [48/84], [94mLoss[0m : 7.29919
[1mStep[0m  [56/84], [94mLoss[0m : 7.39489
[1mStep[0m  [64/84], [94mLoss[0m : 7.49178
[1mStep[0m  [72/84], [94mLoss[0m : 7.44194
[1mStep[0m  [80/84], [94mLoss[0m : 7.42448

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.630, [92mTest[0m: 6.970, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.53735
[1mStep[0m  [8/84], [94mLoss[0m : 7.00482
[1mStep[0m  [16/84], [94mLoss[0m : 6.91762
[1mStep[0m  [24/84], [94mLoss[0m : 7.61664
[1mStep[0m  [32/84], [94mLoss[0m : 7.29891
[1mStep[0m  [40/84], [94mLoss[0m : 6.71767
[1mStep[0m  [48/84], [94mLoss[0m : 7.38522
[1mStep[0m  [56/84], [94mLoss[0m : 6.74565
[1mStep[0m  [64/84], [94mLoss[0m : 6.80410
[1mStep[0m  [72/84], [94mLoss[0m : 6.95092
[1mStep[0m  [80/84], [94mLoss[0m : 7.19893

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 7.160, [92mTest[0m: 6.275, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.96238
[1mStep[0m  [8/84], [94mLoss[0m : 6.75328
[1mStep[0m  [16/84], [94mLoss[0m : 7.12080
[1mStep[0m  [24/84], [94mLoss[0m : 6.74648
[1mStep[0m  [32/84], [94mLoss[0m : 7.04628
[1mStep[0m  [40/84], [94mLoss[0m : 7.12936
[1mStep[0m  [48/84], [94mLoss[0m : 6.97104
[1mStep[0m  [56/84], [94mLoss[0m : 6.17401
[1mStep[0m  [64/84], [94mLoss[0m : 6.37784
[1mStep[0m  [72/84], [94mLoss[0m : 6.64543
[1mStep[0m  [80/84], [94mLoss[0m : 6.51949

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.707, [92mTest[0m: 6.114, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 7.18162
[1mStep[0m  [8/84], [94mLoss[0m : 6.61365
[1mStep[0m  [16/84], [94mLoss[0m : 6.04009
[1mStep[0m  [24/84], [94mLoss[0m : 6.23873
[1mStep[0m  [32/84], [94mLoss[0m : 6.53159
[1mStep[0m  [40/84], [94mLoss[0m : 5.90615
[1mStep[0m  [48/84], [94mLoss[0m : 6.48285
[1mStep[0m  [56/84], [94mLoss[0m : 5.86242
[1mStep[0m  [64/84], [94mLoss[0m : 5.83324
[1mStep[0m  [72/84], [94mLoss[0m : 6.11860
[1mStep[0m  [80/84], [94mLoss[0m : 6.31894

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 6.258, [92mTest[0m: 5.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.47041
[1mStep[0m  [8/84], [94mLoss[0m : 5.62184
[1mStep[0m  [16/84], [94mLoss[0m : 6.42907
[1mStep[0m  [24/84], [94mLoss[0m : 5.48326
[1mStep[0m  [32/84], [94mLoss[0m : 6.21145
[1mStep[0m  [40/84], [94mLoss[0m : 5.98668
[1mStep[0m  [48/84], [94mLoss[0m : 5.60319
[1mStep[0m  [56/84], [94mLoss[0m : 5.91777
[1mStep[0m  [64/84], [94mLoss[0m : 5.52534
[1mStep[0m  [72/84], [94mLoss[0m : 4.85502
[1mStep[0m  [80/84], [94mLoss[0m : 5.03508

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 5.771, [92mTest[0m: 4.939, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.34611
[1mStep[0m  [8/84], [94mLoss[0m : 5.96938
[1mStep[0m  [16/84], [94mLoss[0m : 5.86236
[1mStep[0m  [24/84], [94mLoss[0m : 4.79507
[1mStep[0m  [32/84], [94mLoss[0m : 5.54260
[1mStep[0m  [40/84], [94mLoss[0m : 5.41742
[1mStep[0m  [48/84], [94mLoss[0m : 5.42663
[1mStep[0m  [56/84], [94mLoss[0m : 5.35965
[1mStep[0m  [64/84], [94mLoss[0m : 5.61741
[1mStep[0m  [72/84], [94mLoss[0m : 5.16439
[1mStep[0m  [80/84], [94mLoss[0m : 4.68557

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 5.273, [92mTest[0m: 4.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.11196
[1mStep[0m  [8/84], [94mLoss[0m : 5.02896
[1mStep[0m  [16/84], [94mLoss[0m : 5.33745
[1mStep[0m  [24/84], [94mLoss[0m : 4.51476
[1mStep[0m  [32/84], [94mLoss[0m : 4.88963
[1mStep[0m  [40/84], [94mLoss[0m : 4.80650
[1mStep[0m  [48/84], [94mLoss[0m : 4.41261
[1mStep[0m  [56/84], [94mLoss[0m : 4.59795
[1mStep[0m  [64/84], [94mLoss[0m : 4.75264
[1mStep[0m  [72/84], [94mLoss[0m : 4.24414
[1mStep[0m  [80/84], [94mLoss[0m : 4.67256

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 4.723, [92mTest[0m: 4.101, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.43203
[1mStep[0m  [8/84], [94mLoss[0m : 3.70376
[1mStep[0m  [16/84], [94mLoss[0m : 4.34575
[1mStep[0m  [24/84], [94mLoss[0m : 4.56053
[1mStep[0m  [32/84], [94mLoss[0m : 4.28227
[1mStep[0m  [40/84], [94mLoss[0m : 4.17862
[1mStep[0m  [48/84], [94mLoss[0m : 3.99085
[1mStep[0m  [56/84], [94mLoss[0m : 4.29103
[1mStep[0m  [64/84], [94mLoss[0m : 3.75131
[1mStep[0m  [72/84], [94mLoss[0m : 3.55177
[1mStep[0m  [80/84], [94mLoss[0m : 3.90013

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 4.153, [92mTest[0m: 3.492, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.72157
[1mStep[0m  [8/84], [94mLoss[0m : 3.95428
[1mStep[0m  [16/84], [94mLoss[0m : 3.45808
[1mStep[0m  [24/84], [94mLoss[0m : 3.69079
[1mStep[0m  [32/84], [94mLoss[0m : 3.44620
[1mStep[0m  [40/84], [94mLoss[0m : 3.55712
[1mStep[0m  [48/84], [94mLoss[0m : 3.52860
[1mStep[0m  [56/84], [94mLoss[0m : 3.41211
[1mStep[0m  [64/84], [94mLoss[0m : 3.39131
[1mStep[0m  [72/84], [94mLoss[0m : 3.94146
[1mStep[0m  [80/84], [94mLoss[0m : 3.33018

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 3.613, [92mTest[0m: 3.051, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.19818
[1mStep[0m  [8/84], [94mLoss[0m : 3.27881
[1mStep[0m  [16/84], [94mLoss[0m : 2.99115
[1mStep[0m  [24/84], [94mLoss[0m : 3.19309
[1mStep[0m  [32/84], [94mLoss[0m : 3.43824
[1mStep[0m  [40/84], [94mLoss[0m : 3.26586
[1mStep[0m  [48/84], [94mLoss[0m : 2.78676
[1mStep[0m  [56/84], [94mLoss[0m : 3.08575
[1mStep[0m  [64/84], [94mLoss[0m : 3.04983
[1mStep[0m  [72/84], [94mLoss[0m : 3.39752
[1mStep[0m  [80/84], [94mLoss[0m : 3.31400

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 3.202, [92mTest[0m: 2.647, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.97174
[1mStep[0m  [8/84], [94mLoss[0m : 3.02198
[1mStep[0m  [16/84], [94mLoss[0m : 2.82255
[1mStep[0m  [24/84], [94mLoss[0m : 3.06653
[1mStep[0m  [32/84], [94mLoss[0m : 3.33698
[1mStep[0m  [40/84], [94mLoss[0m : 2.73955
[1mStep[0m  [48/84], [94mLoss[0m : 2.85417
[1mStep[0m  [56/84], [94mLoss[0m : 3.23278
[1mStep[0m  [64/84], [94mLoss[0m : 2.83155
[1mStep[0m  [72/84], [94mLoss[0m : 2.49153
[1mStep[0m  [80/84], [94mLoss[0m : 2.83540

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.960, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.99195
[1mStep[0m  [8/84], [94mLoss[0m : 2.51617
[1mStep[0m  [16/84], [94mLoss[0m : 2.85145
[1mStep[0m  [24/84], [94mLoss[0m : 2.77603
[1mStep[0m  [32/84], [94mLoss[0m : 2.88415
[1mStep[0m  [40/84], [94mLoss[0m : 2.58759
[1mStep[0m  [48/84], [94mLoss[0m : 2.72056
[1mStep[0m  [56/84], [94mLoss[0m : 2.36604
[1mStep[0m  [64/84], [94mLoss[0m : 2.94226
[1mStep[0m  [72/84], [94mLoss[0m : 2.57514
[1mStep[0m  [80/84], [94mLoss[0m : 2.94478

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.840, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.98468
[1mStep[0m  [8/84], [94mLoss[0m : 2.91802
[1mStep[0m  [16/84], [94mLoss[0m : 2.87633
[1mStep[0m  [24/84], [94mLoss[0m : 2.78928
[1mStep[0m  [32/84], [94mLoss[0m : 2.87385
[1mStep[0m  [40/84], [94mLoss[0m : 2.75063
[1mStep[0m  [48/84], [94mLoss[0m : 2.54020
[1mStep[0m  [56/84], [94mLoss[0m : 2.71859
[1mStep[0m  [64/84], [94mLoss[0m : 3.00847
[1mStep[0m  [72/84], [94mLoss[0m : 2.64442
[1mStep[0m  [80/84], [94mLoss[0m : 3.21810

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.798, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.83945
[1mStep[0m  [8/84], [94mLoss[0m : 2.72729
[1mStep[0m  [16/84], [94mLoss[0m : 2.31221
[1mStep[0m  [24/84], [94mLoss[0m : 3.13383
[1mStep[0m  [32/84], [94mLoss[0m : 2.68622
[1mStep[0m  [40/84], [94mLoss[0m : 2.39869
[1mStep[0m  [48/84], [94mLoss[0m : 3.05938
[1mStep[0m  [56/84], [94mLoss[0m : 3.00269
[1mStep[0m  [64/84], [94mLoss[0m : 2.63274
[1mStep[0m  [72/84], [94mLoss[0m : 2.69078
[1mStep[0m  [80/84], [94mLoss[0m : 2.85347

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.767, [92mTest[0m: 2.369, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79184
[1mStep[0m  [8/84], [94mLoss[0m : 2.94671
[1mStep[0m  [16/84], [94mLoss[0m : 2.77557
[1mStep[0m  [24/84], [94mLoss[0m : 2.50455
[1mStep[0m  [32/84], [94mLoss[0m : 2.71500
[1mStep[0m  [40/84], [94mLoss[0m : 3.05161
[1mStep[0m  [48/84], [94mLoss[0m : 2.65790
[1mStep[0m  [56/84], [94mLoss[0m : 2.65255
[1mStep[0m  [64/84], [94mLoss[0m : 2.43134
[1mStep[0m  [72/84], [94mLoss[0m : 2.72969
[1mStep[0m  [80/84], [94mLoss[0m : 2.40508

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.728, [92mTest[0m: 2.399, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75567
[1mStep[0m  [8/84], [94mLoss[0m : 2.44663
[1mStep[0m  [16/84], [94mLoss[0m : 2.69753
[1mStep[0m  [24/84], [94mLoss[0m : 2.68578
[1mStep[0m  [32/84], [94mLoss[0m : 2.68769
[1mStep[0m  [40/84], [94mLoss[0m : 2.74802
[1mStep[0m  [48/84], [94mLoss[0m : 2.87054
[1mStep[0m  [56/84], [94mLoss[0m : 2.46287
[1mStep[0m  [64/84], [94mLoss[0m : 2.60478
[1mStep[0m  [72/84], [94mLoss[0m : 2.60131
[1mStep[0m  [80/84], [94mLoss[0m : 2.42380

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.740, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.81845
[1mStep[0m  [8/84], [94mLoss[0m : 2.83809
[1mStep[0m  [16/84], [94mLoss[0m : 2.90999
[1mStep[0m  [24/84], [94mLoss[0m : 2.56095
[1mStep[0m  [32/84], [94mLoss[0m : 2.58368
[1mStep[0m  [40/84], [94mLoss[0m : 2.68117
[1mStep[0m  [48/84], [94mLoss[0m : 2.94920
[1mStep[0m  [56/84], [94mLoss[0m : 2.55679
[1mStep[0m  [64/84], [94mLoss[0m : 2.84401
[1mStep[0m  [72/84], [94mLoss[0m : 2.60304
[1mStep[0m  [80/84], [94mLoss[0m : 2.32603

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.403, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.89052
[1mStep[0m  [8/84], [94mLoss[0m : 2.91182
[1mStep[0m  [16/84], [94mLoss[0m : 2.89848
[1mStep[0m  [24/84], [94mLoss[0m : 2.24660
[1mStep[0m  [32/84], [94mLoss[0m : 2.73705
[1mStep[0m  [40/84], [94mLoss[0m : 3.10644
[1mStep[0m  [48/84], [94mLoss[0m : 3.00023
[1mStep[0m  [56/84], [94mLoss[0m : 3.01341
[1mStep[0m  [64/84], [94mLoss[0m : 2.93538
[1mStep[0m  [72/84], [94mLoss[0m : 2.85131
[1mStep[0m  [80/84], [94mLoss[0m : 3.03274

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.735, [92mTest[0m: 2.381, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93263
[1mStep[0m  [8/84], [94mLoss[0m : 2.71309
[1mStep[0m  [16/84], [94mLoss[0m : 2.80041
[1mStep[0m  [24/84], [94mLoss[0m : 2.69320
[1mStep[0m  [32/84], [94mLoss[0m : 2.49801
[1mStep[0m  [40/84], [94mLoss[0m : 2.91091
[1mStep[0m  [48/84], [94mLoss[0m : 2.73914
[1mStep[0m  [56/84], [94mLoss[0m : 2.67129
[1mStep[0m  [64/84], [94mLoss[0m : 2.71169
[1mStep[0m  [72/84], [94mLoss[0m : 2.56471
[1mStep[0m  [80/84], [94mLoss[0m : 2.81978

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.686, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.05176
[1mStep[0m  [8/84], [94mLoss[0m : 2.53204
[1mStep[0m  [16/84], [94mLoss[0m : 2.65657
[1mStep[0m  [24/84], [94mLoss[0m : 2.38671
[1mStep[0m  [32/84], [94mLoss[0m : 2.70017
[1mStep[0m  [40/84], [94mLoss[0m : 2.96906
[1mStep[0m  [48/84], [94mLoss[0m : 2.95487
[1mStep[0m  [56/84], [94mLoss[0m : 2.92155
[1mStep[0m  [64/84], [94mLoss[0m : 2.54601
[1mStep[0m  [72/84], [94mLoss[0m : 2.46846
[1mStep[0m  [80/84], [94mLoss[0m : 3.12037

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.691, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69109
[1mStep[0m  [8/84], [94mLoss[0m : 2.64321
[1mStep[0m  [16/84], [94mLoss[0m : 2.79085
[1mStep[0m  [24/84], [94mLoss[0m : 3.00444
[1mStep[0m  [32/84], [94mLoss[0m : 2.77608
[1mStep[0m  [40/84], [94mLoss[0m : 2.76311
[1mStep[0m  [48/84], [94mLoss[0m : 2.53359
[1mStep[0m  [56/84], [94mLoss[0m : 2.62272
[1mStep[0m  [64/84], [94mLoss[0m : 2.74888
[1mStep[0m  [72/84], [94mLoss[0m : 2.59658
[1mStep[0m  [80/84], [94mLoss[0m : 2.69799

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.372, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.76920
[1mStep[0m  [8/84], [94mLoss[0m : 3.14147
[1mStep[0m  [16/84], [94mLoss[0m : 2.23210
[1mStep[0m  [24/84], [94mLoss[0m : 2.76463
[1mStep[0m  [32/84], [94mLoss[0m : 2.42699
[1mStep[0m  [40/84], [94mLoss[0m : 2.57651
[1mStep[0m  [48/84], [94mLoss[0m : 3.02571
[1mStep[0m  [56/84], [94mLoss[0m : 2.28768
[1mStep[0m  [64/84], [94mLoss[0m : 2.79722
[1mStep[0m  [72/84], [94mLoss[0m : 2.81808
[1mStep[0m  [80/84], [94mLoss[0m : 2.70108

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.699, [92mTest[0m: 2.366, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.87070
[1mStep[0m  [8/84], [94mLoss[0m : 2.41891
[1mStep[0m  [16/84], [94mLoss[0m : 2.59893
[1mStep[0m  [24/84], [94mLoss[0m : 2.59294
[1mStep[0m  [32/84], [94mLoss[0m : 2.63479
[1mStep[0m  [40/84], [94mLoss[0m : 2.92251
[1mStep[0m  [48/84], [94mLoss[0m : 2.85628
[1mStep[0m  [56/84], [94mLoss[0m : 2.73935
[1mStep[0m  [64/84], [94mLoss[0m : 2.73601
[1mStep[0m  [72/84], [94mLoss[0m : 2.72400
[1mStep[0m  [80/84], [94mLoss[0m : 2.58445

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.650, [92mTest[0m: 2.363, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30996
[1mStep[0m  [8/84], [94mLoss[0m : 2.85157
[1mStep[0m  [16/84], [94mLoss[0m : 2.56844
[1mStep[0m  [24/84], [94mLoss[0m : 2.46390
[1mStep[0m  [32/84], [94mLoss[0m : 2.69469
[1mStep[0m  [40/84], [94mLoss[0m : 2.68760
[1mStep[0m  [48/84], [94mLoss[0m : 2.36774
[1mStep[0m  [56/84], [94mLoss[0m : 2.51376
[1mStep[0m  [64/84], [94mLoss[0m : 2.55094
[1mStep[0m  [72/84], [94mLoss[0m : 2.56392
[1mStep[0m  [80/84], [94mLoss[0m : 2.76270

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.364, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.367
====================================

Phase 1 - Evaluation MAE:  2.366902393954141
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.94707
[1mStep[0m  [8/84], [94mLoss[0m : 2.86541
[1mStep[0m  [16/84], [94mLoss[0m : 2.53759
[1mStep[0m  [24/84], [94mLoss[0m : 2.49464
[1mStep[0m  [32/84], [94mLoss[0m : 2.73796
[1mStep[0m  [40/84], [94mLoss[0m : 3.07521
[1mStep[0m  [48/84], [94mLoss[0m : 2.64004
[1mStep[0m  [56/84], [94mLoss[0m : 2.49091
[1mStep[0m  [64/84], [94mLoss[0m : 2.69100
[1mStep[0m  [72/84], [94mLoss[0m : 2.70149
[1mStep[0m  [80/84], [94mLoss[0m : 2.52736

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.754, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84210
[1mStep[0m  [8/84], [94mLoss[0m : 2.91099
[1mStep[0m  [16/84], [94mLoss[0m : 2.60268
[1mStep[0m  [24/84], [94mLoss[0m : 3.19549
[1mStep[0m  [32/84], [94mLoss[0m : 3.05313
[1mStep[0m  [40/84], [94mLoss[0m : 2.90472
[1mStep[0m  [48/84], [94mLoss[0m : 2.58095
[1mStep[0m  [56/84], [94mLoss[0m : 2.72744
[1mStep[0m  [64/84], [94mLoss[0m : 2.78378
[1mStep[0m  [72/84], [94mLoss[0m : 2.80725
[1mStep[0m  [80/84], [94mLoss[0m : 2.93212

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53176
[1mStep[0m  [8/84], [94mLoss[0m : 3.01446
[1mStep[0m  [16/84], [94mLoss[0m : 2.90736
[1mStep[0m  [24/84], [94mLoss[0m : 2.56418
[1mStep[0m  [32/84], [94mLoss[0m : 2.86341
[1mStep[0m  [40/84], [94mLoss[0m : 2.50198
[1mStep[0m  [48/84], [94mLoss[0m : 2.61950
[1mStep[0m  [56/84], [94mLoss[0m : 2.64896
[1mStep[0m  [64/84], [94mLoss[0m : 2.60705
[1mStep[0m  [72/84], [94mLoss[0m : 2.42685
[1mStep[0m  [80/84], [94mLoss[0m : 2.87753

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.537, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56959
[1mStep[0m  [8/84], [94mLoss[0m : 2.79780
[1mStep[0m  [16/84], [94mLoss[0m : 2.53443
[1mStep[0m  [24/84], [94mLoss[0m : 2.71294
[1mStep[0m  [32/84], [94mLoss[0m : 2.74715
[1mStep[0m  [40/84], [94mLoss[0m : 2.86312
[1mStep[0m  [48/84], [94mLoss[0m : 2.88891
[1mStep[0m  [56/84], [94mLoss[0m : 2.68833
[1mStep[0m  [64/84], [94mLoss[0m : 2.71385
[1mStep[0m  [72/84], [94mLoss[0m : 2.78840
[1mStep[0m  [80/84], [94mLoss[0m : 2.58294

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.636, [92mTest[0m: 2.556, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54572
[1mStep[0m  [8/84], [94mLoss[0m : 2.61978
[1mStep[0m  [16/84], [94mLoss[0m : 2.46388
[1mStep[0m  [24/84], [94mLoss[0m : 2.41816
[1mStep[0m  [32/84], [94mLoss[0m : 2.72446
[1mStep[0m  [40/84], [94mLoss[0m : 2.44439
[1mStep[0m  [48/84], [94mLoss[0m : 2.54425
[1mStep[0m  [56/84], [94mLoss[0m : 2.51840
[1mStep[0m  [64/84], [94mLoss[0m : 3.10212
[1mStep[0m  [72/84], [94mLoss[0m : 2.42397
[1mStep[0m  [80/84], [94mLoss[0m : 2.53485

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.00417
[1mStep[0m  [8/84], [94mLoss[0m : 2.49768
[1mStep[0m  [16/84], [94mLoss[0m : 2.58616
[1mStep[0m  [24/84], [94mLoss[0m : 2.84310
[1mStep[0m  [32/84], [94mLoss[0m : 2.76470
[1mStep[0m  [40/84], [94mLoss[0m : 2.79721
[1mStep[0m  [48/84], [94mLoss[0m : 2.45592
[1mStep[0m  [56/84], [94mLoss[0m : 2.68861
[1mStep[0m  [64/84], [94mLoss[0m : 2.82640
[1mStep[0m  [72/84], [94mLoss[0m : 2.58997
[1mStep[0m  [80/84], [94mLoss[0m : 2.49283

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.767, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93930
[1mStep[0m  [8/84], [94mLoss[0m : 2.69660
[1mStep[0m  [16/84], [94mLoss[0m : 2.28393
[1mStep[0m  [24/84], [94mLoss[0m : 2.79675
[1mStep[0m  [32/84], [94mLoss[0m : 2.70692
[1mStep[0m  [40/84], [94mLoss[0m : 2.31292
[1mStep[0m  [48/84], [94mLoss[0m : 2.71742
[1mStep[0m  [56/84], [94mLoss[0m : 2.47728
[1mStep[0m  [64/84], [94mLoss[0m : 2.32070
[1mStep[0m  [72/84], [94mLoss[0m : 2.85771
[1mStep[0m  [80/84], [94mLoss[0m : 2.32005

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.649, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.78146
[1mStep[0m  [8/84], [94mLoss[0m : 2.66635
[1mStep[0m  [16/84], [94mLoss[0m : 2.46349
[1mStep[0m  [24/84], [94mLoss[0m : 2.57656
[1mStep[0m  [32/84], [94mLoss[0m : 2.28834
[1mStep[0m  [40/84], [94mLoss[0m : 2.85588
[1mStep[0m  [48/84], [94mLoss[0m : 2.67964
[1mStep[0m  [56/84], [94mLoss[0m : 2.60114
[1mStep[0m  [64/84], [94mLoss[0m : 2.39392
[1mStep[0m  [72/84], [94mLoss[0m : 2.66650
[1mStep[0m  [80/84], [94mLoss[0m : 2.37358

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.605, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24550
[1mStep[0m  [8/84], [94mLoss[0m : 2.37059
[1mStep[0m  [16/84], [94mLoss[0m : 2.51245
[1mStep[0m  [24/84], [94mLoss[0m : 2.60551
[1mStep[0m  [32/84], [94mLoss[0m : 2.17992
[1mStep[0m  [40/84], [94mLoss[0m : 2.59653
[1mStep[0m  [48/84], [94mLoss[0m : 2.44057
[1mStep[0m  [56/84], [94mLoss[0m : 2.27761
[1mStep[0m  [64/84], [94mLoss[0m : 2.36472
[1mStep[0m  [72/84], [94mLoss[0m : 2.54381
[1mStep[0m  [80/84], [94mLoss[0m : 2.46609

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.67754
[1mStep[0m  [8/84], [94mLoss[0m : 2.69112
[1mStep[0m  [16/84], [94mLoss[0m : 2.50336
[1mStep[0m  [24/84], [94mLoss[0m : 2.50557
[1mStep[0m  [32/84], [94mLoss[0m : 2.76352
[1mStep[0m  [40/84], [94mLoss[0m : 2.41412
[1mStep[0m  [48/84], [94mLoss[0m : 2.88368
[1mStep[0m  [56/84], [94mLoss[0m : 2.35781
[1mStep[0m  [64/84], [94mLoss[0m : 2.34507
[1mStep[0m  [72/84], [94mLoss[0m : 2.69183
[1mStep[0m  [80/84], [94mLoss[0m : 2.44620

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.683, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.48325
[1mStep[0m  [8/84], [94mLoss[0m : 2.02935
[1mStep[0m  [16/84], [94mLoss[0m : 2.50790
[1mStep[0m  [24/84], [94mLoss[0m : 2.76799
[1mStep[0m  [32/84], [94mLoss[0m : 2.77164
[1mStep[0m  [40/84], [94mLoss[0m : 2.50745
[1mStep[0m  [48/84], [94mLoss[0m : 2.30371
[1mStep[0m  [56/84], [94mLoss[0m : 2.54258
[1mStep[0m  [64/84], [94mLoss[0m : 2.32761
[1mStep[0m  [72/84], [94mLoss[0m : 2.61700
[1mStep[0m  [80/84], [94mLoss[0m : 2.89054

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.700, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23186
[1mStep[0m  [8/84], [94mLoss[0m : 2.64654
[1mStep[0m  [16/84], [94mLoss[0m : 2.45487
[1mStep[0m  [24/84], [94mLoss[0m : 2.30120
[1mStep[0m  [32/84], [94mLoss[0m : 2.20054
[1mStep[0m  [40/84], [94mLoss[0m : 2.55880
[1mStep[0m  [48/84], [94mLoss[0m : 2.30804
[1mStep[0m  [56/84], [94mLoss[0m : 2.40949
[1mStep[0m  [64/84], [94mLoss[0m : 2.57936
[1mStep[0m  [72/84], [94mLoss[0m : 2.89759
[1mStep[0m  [80/84], [94mLoss[0m : 2.33941

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.767, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38023
[1mStep[0m  [8/84], [94mLoss[0m : 2.27598
[1mStep[0m  [16/84], [94mLoss[0m : 2.22660
[1mStep[0m  [24/84], [94mLoss[0m : 2.49248
[1mStep[0m  [32/84], [94mLoss[0m : 2.29701
[1mStep[0m  [40/84], [94mLoss[0m : 2.37467
[1mStep[0m  [48/84], [94mLoss[0m : 2.33379
[1mStep[0m  [56/84], [94mLoss[0m : 2.61442
[1mStep[0m  [64/84], [94mLoss[0m : 2.41794
[1mStep[0m  [72/84], [94mLoss[0m : 2.46310
[1mStep[0m  [80/84], [94mLoss[0m : 2.55835

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.855, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.53061
[1mStep[0m  [8/84], [94mLoss[0m : 2.02960
[1mStep[0m  [16/84], [94mLoss[0m : 2.38675
[1mStep[0m  [24/84], [94mLoss[0m : 2.16936
[1mStep[0m  [32/84], [94mLoss[0m : 2.54789
[1mStep[0m  [40/84], [94mLoss[0m : 1.95234
[1mStep[0m  [48/84], [94mLoss[0m : 2.71626
[1mStep[0m  [56/84], [94mLoss[0m : 2.63995
[1mStep[0m  [64/84], [94mLoss[0m : 2.32254
[1mStep[0m  [72/84], [94mLoss[0m : 2.51448
[1mStep[0m  [80/84], [94mLoss[0m : 2.48211

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.842, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33464
[1mStep[0m  [8/84], [94mLoss[0m : 2.23628
[1mStep[0m  [16/84], [94mLoss[0m : 2.30297
[1mStep[0m  [24/84], [94mLoss[0m : 2.68766
[1mStep[0m  [32/84], [94mLoss[0m : 2.50044
[1mStep[0m  [40/84], [94mLoss[0m : 2.48941
[1mStep[0m  [48/84], [94mLoss[0m : 2.55802
[1mStep[0m  [56/84], [94mLoss[0m : 2.55053
[1mStep[0m  [64/84], [94mLoss[0m : 2.13532
[1mStep[0m  [72/84], [94mLoss[0m : 2.32044
[1mStep[0m  [80/84], [94mLoss[0m : 2.39264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.366, [92mTest[0m: 2.691, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27404
[1mStep[0m  [8/84], [94mLoss[0m : 2.22820
[1mStep[0m  [16/84], [94mLoss[0m : 2.28626
[1mStep[0m  [24/84], [94mLoss[0m : 2.24135
[1mStep[0m  [32/84], [94mLoss[0m : 2.25682
[1mStep[0m  [40/84], [94mLoss[0m : 2.48353
[1mStep[0m  [48/84], [94mLoss[0m : 2.38295
[1mStep[0m  [56/84], [94mLoss[0m : 2.36099
[1mStep[0m  [64/84], [94mLoss[0m : 2.49016
[1mStep[0m  [72/84], [94mLoss[0m : 2.80848
[1mStep[0m  [80/84], [94mLoss[0m : 2.34760

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.721, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24387
[1mStep[0m  [8/84], [94mLoss[0m : 2.24489
[1mStep[0m  [16/84], [94mLoss[0m : 2.41331
[1mStep[0m  [24/84], [94mLoss[0m : 2.30913
[1mStep[0m  [32/84], [94mLoss[0m : 2.09996
[1mStep[0m  [40/84], [94mLoss[0m : 2.39140
[1mStep[0m  [48/84], [94mLoss[0m : 2.25412
[1mStep[0m  [56/84], [94mLoss[0m : 2.15245
[1mStep[0m  [64/84], [94mLoss[0m : 2.20931
[1mStep[0m  [72/84], [94mLoss[0m : 2.37149
[1mStep[0m  [80/84], [94mLoss[0m : 2.28410

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.796, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32643
[1mStep[0m  [8/84], [94mLoss[0m : 2.14552
[1mStep[0m  [16/84], [94mLoss[0m : 2.46504
[1mStep[0m  [24/84], [94mLoss[0m : 2.28598
[1mStep[0m  [32/84], [94mLoss[0m : 2.37678
[1mStep[0m  [40/84], [94mLoss[0m : 2.38442
[1mStep[0m  [48/84], [94mLoss[0m : 2.23334
[1mStep[0m  [56/84], [94mLoss[0m : 1.97922
[1mStep[0m  [64/84], [94mLoss[0m : 2.27094
[1mStep[0m  [72/84], [94mLoss[0m : 2.17463
[1mStep[0m  [80/84], [94mLoss[0m : 2.46662

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.819, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27493
[1mStep[0m  [8/84], [94mLoss[0m : 2.09241
[1mStep[0m  [16/84], [94mLoss[0m : 2.27590
[1mStep[0m  [24/84], [94mLoss[0m : 2.63129
[1mStep[0m  [32/84], [94mLoss[0m : 2.20302
[1mStep[0m  [40/84], [94mLoss[0m : 1.92446
[1mStep[0m  [48/84], [94mLoss[0m : 2.21545
[1mStep[0m  [56/84], [94mLoss[0m : 2.02329
[1mStep[0m  [64/84], [94mLoss[0m : 2.06679
[1mStep[0m  [72/84], [94mLoss[0m : 2.38115
[1mStep[0m  [80/84], [94mLoss[0m : 2.24152

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.280, [92mTest[0m: 2.933, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38100
[1mStep[0m  [8/84], [94mLoss[0m : 2.29246
[1mStep[0m  [16/84], [94mLoss[0m : 2.13386
[1mStep[0m  [24/84], [94mLoss[0m : 2.23578
[1mStep[0m  [32/84], [94mLoss[0m : 2.08616
[1mStep[0m  [40/84], [94mLoss[0m : 2.00111
[1mStep[0m  [48/84], [94mLoss[0m : 2.32209
[1mStep[0m  [56/84], [94mLoss[0m : 2.17387
[1mStep[0m  [64/84], [94mLoss[0m : 2.05186
[1mStep[0m  [72/84], [94mLoss[0m : 2.38717
[1mStep[0m  [80/84], [94mLoss[0m : 2.19949

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.235, [92mTest[0m: 3.066, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17810
[1mStep[0m  [8/84], [94mLoss[0m : 2.30982
[1mStep[0m  [16/84], [94mLoss[0m : 2.23691
[1mStep[0m  [24/84], [94mLoss[0m : 2.09729
[1mStep[0m  [32/84], [94mLoss[0m : 2.30287
[1mStep[0m  [40/84], [94mLoss[0m : 2.34869
[1mStep[0m  [48/84], [94mLoss[0m : 2.15486
[1mStep[0m  [56/84], [94mLoss[0m : 2.30518
[1mStep[0m  [64/84], [94mLoss[0m : 2.31969
[1mStep[0m  [72/84], [94mLoss[0m : 1.94989
[1mStep[0m  [80/84], [94mLoss[0m : 2.22775

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.851, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.08532
[1mStep[0m  [8/84], [94mLoss[0m : 2.05395
[1mStep[0m  [16/84], [94mLoss[0m : 2.18293
[1mStep[0m  [24/84], [94mLoss[0m : 1.97211
[1mStep[0m  [32/84], [94mLoss[0m : 2.45430
[1mStep[0m  [40/84], [94mLoss[0m : 2.35060
[1mStep[0m  [48/84], [94mLoss[0m : 2.00146
[1mStep[0m  [56/84], [94mLoss[0m : 2.15745
[1mStep[0m  [64/84], [94mLoss[0m : 2.31435
[1mStep[0m  [72/84], [94mLoss[0m : 1.94864
[1mStep[0m  [80/84], [94mLoss[0m : 2.34391

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.194, [92mTest[0m: 2.959, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23056
[1mStep[0m  [8/84], [94mLoss[0m : 2.34697
[1mStep[0m  [16/84], [94mLoss[0m : 2.30996
[1mStep[0m  [24/84], [94mLoss[0m : 2.57178
[1mStep[0m  [32/84], [94mLoss[0m : 2.14876
[1mStep[0m  [40/84], [94mLoss[0m : 2.24994
[1mStep[0m  [48/84], [94mLoss[0m : 2.18452
[1mStep[0m  [56/84], [94mLoss[0m : 2.16428
[1mStep[0m  [64/84], [94mLoss[0m : 2.43775
[1mStep[0m  [72/84], [94mLoss[0m : 2.10816
[1mStep[0m  [80/84], [94mLoss[0m : 2.06190

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.792, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.11568
[1mStep[0m  [8/84], [94mLoss[0m : 1.98570
[1mStep[0m  [16/84], [94mLoss[0m : 2.15201
[1mStep[0m  [24/84], [94mLoss[0m : 1.91599
[1mStep[0m  [32/84], [94mLoss[0m : 2.09326
[1mStep[0m  [40/84], [94mLoss[0m : 2.14404
[1mStep[0m  [48/84], [94mLoss[0m : 1.85590
[1mStep[0m  [56/84], [94mLoss[0m : 2.11343
[1mStep[0m  [64/84], [94mLoss[0m : 2.17297
[1mStep[0m  [72/84], [94mLoss[0m : 2.16968
[1mStep[0m  [80/84], [94mLoss[0m : 2.27347

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.118, [92mTest[0m: 2.920, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06503
[1mStep[0m  [8/84], [94mLoss[0m : 2.09856
[1mStep[0m  [16/84], [94mLoss[0m : 2.15662
[1mStep[0m  [24/84], [94mLoss[0m : 1.87170
[1mStep[0m  [32/84], [94mLoss[0m : 2.01196
[1mStep[0m  [40/84], [94mLoss[0m : 2.14370
[1mStep[0m  [48/84], [94mLoss[0m : 2.04382
[1mStep[0m  [56/84], [94mLoss[0m : 2.11133
[1mStep[0m  [64/84], [94mLoss[0m : 2.52263
[1mStep[0m  [72/84], [94mLoss[0m : 2.39520
[1mStep[0m  [80/84], [94mLoss[0m : 2.36731

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.876, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.04499
[1mStep[0m  [8/84], [94mLoss[0m : 2.14401
[1mStep[0m  [16/84], [94mLoss[0m : 1.96889
[1mStep[0m  [24/84], [94mLoss[0m : 2.05204
[1mStep[0m  [32/84], [94mLoss[0m : 1.76372
[1mStep[0m  [40/84], [94mLoss[0m : 1.88084
[1mStep[0m  [48/84], [94mLoss[0m : 2.14334
[1mStep[0m  [56/84], [94mLoss[0m : 1.96338
[1mStep[0m  [64/84], [94mLoss[0m : 1.94302
[1mStep[0m  [72/84], [94mLoss[0m : 2.40593
[1mStep[0m  [80/84], [94mLoss[0m : 2.13127

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.680, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93405
[1mStep[0m  [8/84], [94mLoss[0m : 1.92873
[1mStep[0m  [16/84], [94mLoss[0m : 2.22333
[1mStep[0m  [24/84], [94mLoss[0m : 2.10767
[1mStep[0m  [32/84], [94mLoss[0m : 1.95862
[1mStep[0m  [40/84], [94mLoss[0m : 1.99986
[1mStep[0m  [48/84], [94mLoss[0m : 1.98596
[1mStep[0m  [56/84], [94mLoss[0m : 2.21010
[1mStep[0m  [64/84], [94mLoss[0m : 1.86406
[1mStep[0m  [72/84], [94mLoss[0m : 2.04580
[1mStep[0m  [80/84], [94mLoss[0m : 1.94211

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.077, [92mTest[0m: 2.871, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80098
[1mStep[0m  [8/84], [94mLoss[0m : 1.97009
[1mStep[0m  [16/84], [94mLoss[0m : 1.97848
[1mStep[0m  [24/84], [94mLoss[0m : 2.06326
[1mStep[0m  [32/84], [94mLoss[0m : 2.11998
[1mStep[0m  [40/84], [94mLoss[0m : 1.88334
[1mStep[0m  [48/84], [94mLoss[0m : 2.19561
[1mStep[0m  [56/84], [94mLoss[0m : 1.91227
[1mStep[0m  [64/84], [94mLoss[0m : 1.98096
[1mStep[0m  [72/84], [94mLoss[0m : 1.74450
[1mStep[0m  [80/84], [94mLoss[0m : 2.11400

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.040, [92mTest[0m: 2.743, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.00480
[1mStep[0m  [8/84], [94mLoss[0m : 1.97719
[1mStep[0m  [16/84], [94mLoss[0m : 2.06878
[1mStep[0m  [24/84], [94mLoss[0m : 1.76288
[1mStep[0m  [32/84], [94mLoss[0m : 1.86512
[1mStep[0m  [40/84], [94mLoss[0m : 1.90754
[1mStep[0m  [48/84], [94mLoss[0m : 2.01841
[1mStep[0m  [56/84], [94mLoss[0m : 1.97587
[1mStep[0m  [64/84], [94mLoss[0m : 1.83654
[1mStep[0m  [72/84], [94mLoss[0m : 2.32245
[1mStep[0m  [80/84], [94mLoss[0m : 1.96413

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.798, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12910
[1mStep[0m  [8/84], [94mLoss[0m : 2.14678
[1mStep[0m  [16/84], [94mLoss[0m : 2.11047
[1mStep[0m  [24/84], [94mLoss[0m : 2.14856
[1mStep[0m  [32/84], [94mLoss[0m : 1.80720
[1mStep[0m  [40/84], [94mLoss[0m : 1.88955
[1mStep[0m  [48/84], [94mLoss[0m : 1.91834
[1mStep[0m  [56/84], [94mLoss[0m : 2.02949
[1mStep[0m  [64/84], [94mLoss[0m : 2.27457
[1mStep[0m  [72/84], [94mLoss[0m : 2.04710
[1mStep[0m  [80/84], [94mLoss[0m : 2.06493

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.689, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.741
====================================

Phase 2 - Evaluation MAE:  2.741240680217743
MAE score P1      2.366902
MAE score P2      2.741241
loss              2.006951
learning_rate     0.002575
batch_size             128
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 10, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 12.30068
[1mStep[0m  [16/169], [94mLoss[0m : 10.48994
[1mStep[0m  [32/169], [94mLoss[0m : 10.58065
[1mStep[0m  [48/169], [94mLoss[0m : 9.23457
[1mStep[0m  [64/169], [94mLoss[0m : 7.66801
[1mStep[0m  [80/169], [94mLoss[0m : 7.85158
[1mStep[0m  [96/169], [94mLoss[0m : 7.72301
[1mStep[0m  [112/169], [94mLoss[0m : 5.31573
[1mStep[0m  [128/169], [94mLoss[0m : 5.25553
[1mStep[0m  [144/169], [94mLoss[0m : 5.05989
[1mStep[0m  [160/169], [94mLoss[0m : 3.57486

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.173, [92mTest[0m: 10.913, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.93385
[1mStep[0m  [16/169], [94mLoss[0m : 3.03985
[1mStep[0m  [32/169], [94mLoss[0m : 3.33966
[1mStep[0m  [48/169], [94mLoss[0m : 2.63288
[1mStep[0m  [64/169], [94mLoss[0m : 3.62676
[1mStep[0m  [80/169], [94mLoss[0m : 3.05149
[1mStep[0m  [96/169], [94mLoss[0m : 2.90999
[1mStep[0m  [112/169], [94mLoss[0m : 3.01610
[1mStep[0m  [128/169], [94mLoss[0m : 2.65208
[1mStep[0m  [144/169], [94mLoss[0m : 3.14275
[1mStep[0m  [160/169], [94mLoss[0m : 2.61547

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.089, [92mTest[0m: 4.913, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96873
[1mStep[0m  [16/169], [94mLoss[0m : 2.85497
[1mStep[0m  [32/169], [94mLoss[0m : 2.74105
[1mStep[0m  [48/169], [94mLoss[0m : 2.21063
[1mStep[0m  [64/169], [94mLoss[0m : 2.80207
[1mStep[0m  [80/169], [94mLoss[0m : 2.50826
[1mStep[0m  [96/169], [94mLoss[0m : 3.03700
[1mStep[0m  [112/169], [94mLoss[0m : 3.61114
[1mStep[0m  [128/169], [94mLoss[0m : 3.80645
[1mStep[0m  [144/169], [94mLoss[0m : 2.64163
[1mStep[0m  [160/169], [94mLoss[0m : 3.31441

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.815, [92mTest[0m: 2.746, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.15046
[1mStep[0m  [16/169], [94mLoss[0m : 3.19139
[1mStep[0m  [32/169], [94mLoss[0m : 2.79257
[1mStep[0m  [48/169], [94mLoss[0m : 2.84117
[1mStep[0m  [64/169], [94mLoss[0m : 2.78507
[1mStep[0m  [80/169], [94mLoss[0m : 2.65546
[1mStep[0m  [96/169], [94mLoss[0m : 2.49487
[1mStep[0m  [112/169], [94mLoss[0m : 2.64207
[1mStep[0m  [128/169], [94mLoss[0m : 3.35833
[1mStep[0m  [144/169], [94mLoss[0m : 2.95004
[1mStep[0m  [160/169], [94mLoss[0m : 2.59384

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.785, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29104
[1mStep[0m  [16/169], [94mLoss[0m : 2.52994
[1mStep[0m  [32/169], [94mLoss[0m : 2.55001
[1mStep[0m  [48/169], [94mLoss[0m : 2.83060
[1mStep[0m  [64/169], [94mLoss[0m : 2.38653
[1mStep[0m  [80/169], [94mLoss[0m : 2.73192
[1mStep[0m  [96/169], [94mLoss[0m : 2.71873
[1mStep[0m  [112/169], [94mLoss[0m : 2.52815
[1mStep[0m  [128/169], [94mLoss[0m : 2.67962
[1mStep[0m  [144/169], [94mLoss[0m : 2.67226
[1mStep[0m  [160/169], [94mLoss[0m : 2.57808

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.750, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72913
[1mStep[0m  [16/169], [94mLoss[0m : 2.91376
[1mStep[0m  [32/169], [94mLoss[0m : 2.78132
[1mStep[0m  [48/169], [94mLoss[0m : 2.67108
[1mStep[0m  [64/169], [94mLoss[0m : 2.95932
[1mStep[0m  [80/169], [94mLoss[0m : 2.73158
[1mStep[0m  [96/169], [94mLoss[0m : 2.91647
[1mStep[0m  [112/169], [94mLoss[0m : 2.80925
[1mStep[0m  [128/169], [94mLoss[0m : 2.73556
[1mStep[0m  [144/169], [94mLoss[0m : 3.00794
[1mStep[0m  [160/169], [94mLoss[0m : 2.58586

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37335
[1mStep[0m  [16/169], [94mLoss[0m : 2.79677
[1mStep[0m  [32/169], [94mLoss[0m : 2.54350
[1mStep[0m  [48/169], [94mLoss[0m : 2.76311
[1mStep[0m  [64/169], [94mLoss[0m : 2.10685
[1mStep[0m  [80/169], [94mLoss[0m : 3.01935
[1mStep[0m  [96/169], [94mLoss[0m : 2.58831
[1mStep[0m  [112/169], [94mLoss[0m : 2.66047
[1mStep[0m  [128/169], [94mLoss[0m : 2.83381
[1mStep[0m  [144/169], [94mLoss[0m : 2.49863
[1mStep[0m  [160/169], [94mLoss[0m : 2.68221

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.685, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83642
[1mStep[0m  [16/169], [94mLoss[0m : 2.54746
[1mStep[0m  [32/169], [94mLoss[0m : 2.79611
[1mStep[0m  [48/169], [94mLoss[0m : 2.53716
[1mStep[0m  [64/169], [94mLoss[0m : 2.55151
[1mStep[0m  [80/169], [94mLoss[0m : 2.71431
[1mStep[0m  [96/169], [94mLoss[0m : 2.81430
[1mStep[0m  [112/169], [94mLoss[0m : 3.05271
[1mStep[0m  [128/169], [94mLoss[0m : 2.24344
[1mStep[0m  [144/169], [94mLoss[0m : 2.72167
[1mStep[0m  [160/169], [94mLoss[0m : 2.48984

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.666, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.33372
[1mStep[0m  [16/169], [94mLoss[0m : 2.41749
[1mStep[0m  [32/169], [94mLoss[0m : 2.52681
[1mStep[0m  [48/169], [94mLoss[0m : 2.47683
[1mStep[0m  [64/169], [94mLoss[0m : 2.57169
[1mStep[0m  [80/169], [94mLoss[0m : 2.64640
[1mStep[0m  [96/169], [94mLoss[0m : 2.62507
[1mStep[0m  [112/169], [94mLoss[0m : 2.76647
[1mStep[0m  [128/169], [94mLoss[0m : 2.92709
[1mStep[0m  [144/169], [94mLoss[0m : 2.74314
[1mStep[0m  [160/169], [94mLoss[0m : 2.68744

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54011
[1mStep[0m  [16/169], [94mLoss[0m : 2.34864
[1mStep[0m  [32/169], [94mLoss[0m : 2.27944
[1mStep[0m  [48/169], [94mLoss[0m : 2.78192
[1mStep[0m  [64/169], [94mLoss[0m : 2.64231
[1mStep[0m  [80/169], [94mLoss[0m : 2.32598
[1mStep[0m  [96/169], [94mLoss[0m : 2.61571
[1mStep[0m  [112/169], [94mLoss[0m : 2.45676
[1mStep[0m  [128/169], [94mLoss[0m : 3.26940
[1mStep[0m  [144/169], [94mLoss[0m : 2.52669
[1mStep[0m  [160/169], [94mLoss[0m : 2.58272

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.381, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.07154
[1mStep[0m  [16/169], [94mLoss[0m : 2.57396
[1mStep[0m  [32/169], [94mLoss[0m : 2.83050
[1mStep[0m  [48/169], [94mLoss[0m : 3.20438
[1mStep[0m  [64/169], [94mLoss[0m : 2.84115
[1mStep[0m  [80/169], [94mLoss[0m : 2.84240
[1mStep[0m  [96/169], [94mLoss[0m : 3.09414
[1mStep[0m  [112/169], [94mLoss[0m : 2.62836
[1mStep[0m  [128/169], [94mLoss[0m : 2.71725
[1mStep[0m  [144/169], [94mLoss[0m : 3.31663
[1mStep[0m  [160/169], [94mLoss[0m : 2.67461

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68082
[1mStep[0m  [16/169], [94mLoss[0m : 2.89452
[1mStep[0m  [32/169], [94mLoss[0m : 2.73589
[1mStep[0m  [48/169], [94mLoss[0m : 2.30928
[1mStep[0m  [64/169], [94mLoss[0m : 2.47433
[1mStep[0m  [80/169], [94mLoss[0m : 2.76986
[1mStep[0m  [96/169], [94mLoss[0m : 2.29628
[1mStep[0m  [112/169], [94mLoss[0m : 2.52633
[1mStep[0m  [128/169], [94mLoss[0m : 2.41149
[1mStep[0m  [144/169], [94mLoss[0m : 2.49546
[1mStep[0m  [160/169], [94mLoss[0m : 2.52668

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23975
[1mStep[0m  [16/169], [94mLoss[0m : 2.17721
[1mStep[0m  [32/169], [94mLoss[0m : 2.56218
[1mStep[0m  [48/169], [94mLoss[0m : 2.39407
[1mStep[0m  [64/169], [94mLoss[0m : 2.60828
[1mStep[0m  [80/169], [94mLoss[0m : 2.46118
[1mStep[0m  [96/169], [94mLoss[0m : 2.06394
[1mStep[0m  [112/169], [94mLoss[0m : 2.62911
[1mStep[0m  [128/169], [94mLoss[0m : 2.84392
[1mStep[0m  [144/169], [94mLoss[0m : 2.77123
[1mStep[0m  [160/169], [94mLoss[0m : 2.62089

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.351, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54370
[1mStep[0m  [16/169], [94mLoss[0m : 2.22129
[1mStep[0m  [32/169], [94mLoss[0m : 2.48756
[1mStep[0m  [48/169], [94mLoss[0m : 2.19244
[1mStep[0m  [64/169], [94mLoss[0m : 2.61792
[1mStep[0m  [80/169], [94mLoss[0m : 2.56082
[1mStep[0m  [96/169], [94mLoss[0m : 2.31546
[1mStep[0m  [112/169], [94mLoss[0m : 2.49787
[1mStep[0m  [128/169], [94mLoss[0m : 2.70037
[1mStep[0m  [144/169], [94mLoss[0m : 2.80221
[1mStep[0m  [160/169], [94mLoss[0m : 2.65047

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30865
[1mStep[0m  [16/169], [94mLoss[0m : 3.17435
[1mStep[0m  [32/169], [94mLoss[0m : 2.66270
[1mStep[0m  [48/169], [94mLoss[0m : 3.23490
[1mStep[0m  [64/169], [94mLoss[0m : 2.78840
[1mStep[0m  [80/169], [94mLoss[0m : 2.48071
[1mStep[0m  [96/169], [94mLoss[0m : 2.66540
[1mStep[0m  [112/169], [94mLoss[0m : 2.13802
[1mStep[0m  [128/169], [94mLoss[0m : 2.73208
[1mStep[0m  [144/169], [94mLoss[0m : 2.12202
[1mStep[0m  [160/169], [94mLoss[0m : 2.51696

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17358
[1mStep[0m  [16/169], [94mLoss[0m : 2.61471
[1mStep[0m  [32/169], [94mLoss[0m : 1.92162
[1mStep[0m  [48/169], [94mLoss[0m : 2.48677
[1mStep[0m  [64/169], [94mLoss[0m : 2.85224
[1mStep[0m  [80/169], [94mLoss[0m : 2.47215
[1mStep[0m  [96/169], [94mLoss[0m : 1.89244
[1mStep[0m  [112/169], [94mLoss[0m : 2.47683
[1mStep[0m  [128/169], [94mLoss[0m : 2.31639
[1mStep[0m  [144/169], [94mLoss[0m : 2.51300
[1mStep[0m  [160/169], [94mLoss[0m : 2.12374

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77219
[1mStep[0m  [16/169], [94mLoss[0m : 2.39732
[1mStep[0m  [32/169], [94mLoss[0m : 2.39257
[1mStep[0m  [48/169], [94mLoss[0m : 2.97408
[1mStep[0m  [64/169], [94mLoss[0m : 2.69892
[1mStep[0m  [80/169], [94mLoss[0m : 2.23260
[1mStep[0m  [96/169], [94mLoss[0m : 2.33745
[1mStep[0m  [112/169], [94mLoss[0m : 2.83929
[1mStep[0m  [128/169], [94mLoss[0m : 2.48655
[1mStep[0m  [144/169], [94mLoss[0m : 2.41002
[1mStep[0m  [160/169], [94mLoss[0m : 2.70928

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34224
[1mStep[0m  [16/169], [94mLoss[0m : 2.86584
[1mStep[0m  [32/169], [94mLoss[0m : 2.50537
[1mStep[0m  [48/169], [94mLoss[0m : 2.47612
[1mStep[0m  [64/169], [94mLoss[0m : 2.79298
[1mStep[0m  [80/169], [94mLoss[0m : 2.55118
[1mStep[0m  [96/169], [94mLoss[0m : 2.65820
[1mStep[0m  [112/169], [94mLoss[0m : 2.79153
[1mStep[0m  [128/169], [94mLoss[0m : 2.55934
[1mStep[0m  [144/169], [94mLoss[0m : 2.07112
[1mStep[0m  [160/169], [94mLoss[0m : 2.48405

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47028
[1mStep[0m  [16/169], [94mLoss[0m : 2.72780
[1mStep[0m  [32/169], [94mLoss[0m : 2.31048
[1mStep[0m  [48/169], [94mLoss[0m : 2.58250
[1mStep[0m  [64/169], [94mLoss[0m : 1.76461
[1mStep[0m  [80/169], [94mLoss[0m : 2.41351
[1mStep[0m  [96/169], [94mLoss[0m : 1.93297
[1mStep[0m  [112/169], [94mLoss[0m : 2.62628
[1mStep[0m  [128/169], [94mLoss[0m : 2.44330
[1mStep[0m  [144/169], [94mLoss[0m : 2.44309
[1mStep[0m  [160/169], [94mLoss[0m : 2.80689

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.365, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.74316
[1mStep[0m  [16/169], [94mLoss[0m : 2.79362
[1mStep[0m  [32/169], [94mLoss[0m : 2.34748
[1mStep[0m  [48/169], [94mLoss[0m : 2.51770
[1mStep[0m  [64/169], [94mLoss[0m : 2.54945
[1mStep[0m  [80/169], [94mLoss[0m : 2.52057
[1mStep[0m  [96/169], [94mLoss[0m : 2.51046
[1mStep[0m  [112/169], [94mLoss[0m : 2.47109
[1mStep[0m  [128/169], [94mLoss[0m : 2.38246
[1mStep[0m  [144/169], [94mLoss[0m : 2.77618
[1mStep[0m  [160/169], [94mLoss[0m : 2.45860

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.355, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51591
[1mStep[0m  [16/169], [94mLoss[0m : 3.17938
[1mStep[0m  [32/169], [94mLoss[0m : 2.55032
[1mStep[0m  [48/169], [94mLoss[0m : 2.46004
[1mStep[0m  [64/169], [94mLoss[0m : 2.27489
[1mStep[0m  [80/169], [94mLoss[0m : 2.55752
[1mStep[0m  [96/169], [94mLoss[0m : 2.33509
[1mStep[0m  [112/169], [94mLoss[0m : 2.62900
[1mStep[0m  [128/169], [94mLoss[0m : 2.17085
[1mStep[0m  [144/169], [94mLoss[0m : 2.31155
[1mStep[0m  [160/169], [94mLoss[0m : 2.49232

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.381, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38940
[1mStep[0m  [16/169], [94mLoss[0m : 2.12152
[1mStep[0m  [32/169], [94mLoss[0m : 2.51474
[1mStep[0m  [48/169], [94mLoss[0m : 2.59485
[1mStep[0m  [64/169], [94mLoss[0m : 2.43234
[1mStep[0m  [80/169], [94mLoss[0m : 2.46209
[1mStep[0m  [96/169], [94mLoss[0m : 2.90232
[1mStep[0m  [112/169], [94mLoss[0m : 2.26235
[1mStep[0m  [128/169], [94mLoss[0m : 2.65683
[1mStep[0m  [144/169], [94mLoss[0m : 2.35933
[1mStep[0m  [160/169], [94mLoss[0m : 2.13431

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31257
[1mStep[0m  [16/169], [94mLoss[0m : 2.65776
[1mStep[0m  [32/169], [94mLoss[0m : 2.35970
[1mStep[0m  [48/169], [94mLoss[0m : 2.18618
[1mStep[0m  [64/169], [94mLoss[0m : 2.31799
[1mStep[0m  [80/169], [94mLoss[0m : 2.89377
[1mStep[0m  [96/169], [94mLoss[0m : 2.40742
[1mStep[0m  [112/169], [94mLoss[0m : 2.78944
[1mStep[0m  [128/169], [94mLoss[0m : 2.74734
[1mStep[0m  [144/169], [94mLoss[0m : 2.51705
[1mStep[0m  [160/169], [94mLoss[0m : 2.49631

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36854
[1mStep[0m  [16/169], [94mLoss[0m : 2.46158
[1mStep[0m  [32/169], [94mLoss[0m : 2.39707
[1mStep[0m  [48/169], [94mLoss[0m : 2.71720
[1mStep[0m  [64/169], [94mLoss[0m : 2.77616
[1mStep[0m  [80/169], [94mLoss[0m : 2.67533
[1mStep[0m  [96/169], [94mLoss[0m : 2.75375
[1mStep[0m  [112/169], [94mLoss[0m : 2.84759
[1mStep[0m  [128/169], [94mLoss[0m : 2.24747
[1mStep[0m  [144/169], [94mLoss[0m : 1.96521
[1mStep[0m  [160/169], [94mLoss[0m : 2.88768

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77076
[1mStep[0m  [16/169], [94mLoss[0m : 2.72059
[1mStep[0m  [32/169], [94mLoss[0m : 2.47954
[1mStep[0m  [48/169], [94mLoss[0m : 2.28715
[1mStep[0m  [64/169], [94mLoss[0m : 2.38390
[1mStep[0m  [80/169], [94mLoss[0m : 2.25722
[1mStep[0m  [96/169], [94mLoss[0m : 2.67031
[1mStep[0m  [112/169], [94mLoss[0m : 2.42372
[1mStep[0m  [128/169], [94mLoss[0m : 1.83921
[1mStep[0m  [144/169], [94mLoss[0m : 2.55992
[1mStep[0m  [160/169], [94mLoss[0m : 2.54560

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93645
[1mStep[0m  [16/169], [94mLoss[0m : 2.41266
[1mStep[0m  [32/169], [94mLoss[0m : 2.44017
[1mStep[0m  [48/169], [94mLoss[0m : 2.49921
[1mStep[0m  [64/169], [94mLoss[0m : 2.20411
[1mStep[0m  [80/169], [94mLoss[0m : 2.46311
[1mStep[0m  [96/169], [94mLoss[0m : 2.82207
[1mStep[0m  [112/169], [94mLoss[0m : 2.31766
[1mStep[0m  [128/169], [94mLoss[0m : 2.79029
[1mStep[0m  [144/169], [94mLoss[0m : 2.61545
[1mStep[0m  [160/169], [94mLoss[0m : 2.35292

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31723
[1mStep[0m  [16/169], [94mLoss[0m : 2.20544
[1mStep[0m  [32/169], [94mLoss[0m : 2.59787
[1mStep[0m  [48/169], [94mLoss[0m : 2.39206
[1mStep[0m  [64/169], [94mLoss[0m : 2.31197
[1mStep[0m  [80/169], [94mLoss[0m : 2.56108
[1mStep[0m  [96/169], [94mLoss[0m : 2.19034
[1mStep[0m  [112/169], [94mLoss[0m : 3.01916
[1mStep[0m  [128/169], [94mLoss[0m : 2.88779
[1mStep[0m  [144/169], [94mLoss[0m : 2.69500
[1mStep[0m  [160/169], [94mLoss[0m : 2.45500

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27087
[1mStep[0m  [16/169], [94mLoss[0m : 2.40798
[1mStep[0m  [32/169], [94mLoss[0m : 2.41610
[1mStep[0m  [48/169], [94mLoss[0m : 2.21433
[1mStep[0m  [64/169], [94mLoss[0m : 2.80678
[1mStep[0m  [80/169], [94mLoss[0m : 2.38585
[1mStep[0m  [96/169], [94mLoss[0m : 2.38155
[1mStep[0m  [112/169], [94mLoss[0m : 2.87595
[1mStep[0m  [128/169], [94mLoss[0m : 2.20819
[1mStep[0m  [144/169], [94mLoss[0m : 2.13474
[1mStep[0m  [160/169], [94mLoss[0m : 2.95518

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54527
[1mStep[0m  [16/169], [94mLoss[0m : 2.36480
[1mStep[0m  [32/169], [94mLoss[0m : 2.87465
[1mStep[0m  [48/169], [94mLoss[0m : 2.76012
[1mStep[0m  [64/169], [94mLoss[0m : 2.29655
[1mStep[0m  [80/169], [94mLoss[0m : 3.03184
[1mStep[0m  [96/169], [94mLoss[0m : 2.11316
[1mStep[0m  [112/169], [94mLoss[0m : 2.38178
[1mStep[0m  [128/169], [94mLoss[0m : 2.55730
[1mStep[0m  [144/169], [94mLoss[0m : 2.57983
[1mStep[0m  [160/169], [94mLoss[0m : 2.12211

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.336, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25316
[1mStep[0m  [16/169], [94mLoss[0m : 2.32878
[1mStep[0m  [32/169], [94mLoss[0m : 2.90734
[1mStep[0m  [48/169], [94mLoss[0m : 2.57874
[1mStep[0m  [64/169], [94mLoss[0m : 1.83834
[1mStep[0m  [80/169], [94mLoss[0m : 2.42027
[1mStep[0m  [96/169], [94mLoss[0m : 2.65228
[1mStep[0m  [112/169], [94mLoss[0m : 2.33064
[1mStep[0m  [128/169], [94mLoss[0m : 2.81775
[1mStep[0m  [144/169], [94mLoss[0m : 2.09819
[1mStep[0m  [160/169], [94mLoss[0m : 1.99076

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.317, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.332793946777071
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 1.88786
[1mStep[0m  [16/169], [94mLoss[0m : 2.97791
[1mStep[0m  [32/169], [94mLoss[0m : 2.43100
[1mStep[0m  [48/169], [94mLoss[0m : 2.44542
[1mStep[0m  [64/169], [94mLoss[0m : 2.57805
[1mStep[0m  [80/169], [94mLoss[0m : 2.28980
[1mStep[0m  [96/169], [94mLoss[0m : 2.64588
[1mStep[0m  [112/169], [94mLoss[0m : 3.26025
[1mStep[0m  [128/169], [94mLoss[0m : 2.37107
[1mStep[0m  [144/169], [94mLoss[0m : 2.63577
[1mStep[0m  [160/169], [94mLoss[0m : 2.27468

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.321, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81333
[1mStep[0m  [16/169], [94mLoss[0m : 2.50947
[1mStep[0m  [32/169], [94mLoss[0m : 2.55132
[1mStep[0m  [48/169], [94mLoss[0m : 2.49987
[1mStep[0m  [64/169], [94mLoss[0m : 2.41999
[1mStep[0m  [80/169], [94mLoss[0m : 2.89108
[1mStep[0m  [96/169], [94mLoss[0m : 3.09590
[1mStep[0m  [112/169], [94mLoss[0m : 2.42065
[1mStep[0m  [128/169], [94mLoss[0m : 2.91125
[1mStep[0m  [144/169], [94mLoss[0m : 2.29367
[1mStep[0m  [160/169], [94mLoss[0m : 2.24848

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.555, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43636
[1mStep[0m  [16/169], [94mLoss[0m : 2.93597
[1mStep[0m  [32/169], [94mLoss[0m : 2.35659
[1mStep[0m  [48/169], [94mLoss[0m : 2.37057
[1mStep[0m  [64/169], [94mLoss[0m : 2.59827
[1mStep[0m  [80/169], [94mLoss[0m : 2.75084
[1mStep[0m  [96/169], [94mLoss[0m : 2.45734
[1mStep[0m  [112/169], [94mLoss[0m : 2.41233
[1mStep[0m  [128/169], [94mLoss[0m : 2.75091
[1mStep[0m  [144/169], [94mLoss[0m : 2.55681
[1mStep[0m  [160/169], [94mLoss[0m : 2.44003

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.549, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55873
[1mStep[0m  [16/169], [94mLoss[0m : 2.57741
[1mStep[0m  [32/169], [94mLoss[0m : 2.55302
[1mStep[0m  [48/169], [94mLoss[0m : 2.42020
[1mStep[0m  [64/169], [94mLoss[0m : 2.27591
[1mStep[0m  [80/169], [94mLoss[0m : 1.90847
[1mStep[0m  [96/169], [94mLoss[0m : 2.52251
[1mStep[0m  [112/169], [94mLoss[0m : 2.51513
[1mStep[0m  [128/169], [94mLoss[0m : 2.43259
[1mStep[0m  [144/169], [94mLoss[0m : 2.27759
[1mStep[0m  [160/169], [94mLoss[0m : 2.57686

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.583, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15007
[1mStep[0m  [16/169], [94mLoss[0m : 2.39999
[1mStep[0m  [32/169], [94mLoss[0m : 2.51025
[1mStep[0m  [48/169], [94mLoss[0m : 2.14460
[1mStep[0m  [64/169], [94mLoss[0m : 2.61323
[1mStep[0m  [80/169], [94mLoss[0m : 2.44630
[1mStep[0m  [96/169], [94mLoss[0m : 2.28957
[1mStep[0m  [112/169], [94mLoss[0m : 2.33091
[1mStep[0m  [128/169], [94mLoss[0m : 2.68332
[1mStep[0m  [144/169], [94mLoss[0m : 2.35106
[1mStep[0m  [160/169], [94mLoss[0m : 2.72641

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57807
[1mStep[0m  [16/169], [94mLoss[0m : 2.01166
[1mStep[0m  [32/169], [94mLoss[0m : 2.58379
[1mStep[0m  [48/169], [94mLoss[0m : 2.39297
[1mStep[0m  [64/169], [94mLoss[0m : 2.49832
[1mStep[0m  [80/169], [94mLoss[0m : 2.45281
[1mStep[0m  [96/169], [94mLoss[0m : 2.17135
[1mStep[0m  [112/169], [94mLoss[0m : 2.77100
[1mStep[0m  [128/169], [94mLoss[0m : 2.03114
[1mStep[0m  [144/169], [94mLoss[0m : 3.10061
[1mStep[0m  [160/169], [94mLoss[0m : 2.21530

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96797
[1mStep[0m  [16/169], [94mLoss[0m : 2.31325
[1mStep[0m  [32/169], [94mLoss[0m : 2.21869
[1mStep[0m  [48/169], [94mLoss[0m : 2.44553
[1mStep[0m  [64/169], [94mLoss[0m : 2.39751
[1mStep[0m  [80/169], [94mLoss[0m : 2.50796
[1mStep[0m  [96/169], [94mLoss[0m : 2.56737
[1mStep[0m  [112/169], [94mLoss[0m : 2.41390
[1mStep[0m  [128/169], [94mLoss[0m : 2.85903
[1mStep[0m  [144/169], [94mLoss[0m : 2.14022
[1mStep[0m  [160/169], [94mLoss[0m : 2.14816

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30855
[1mStep[0m  [16/169], [94mLoss[0m : 2.02908
[1mStep[0m  [32/169], [94mLoss[0m : 2.20705
[1mStep[0m  [48/169], [94mLoss[0m : 2.36869
[1mStep[0m  [64/169], [94mLoss[0m : 2.58546
[1mStep[0m  [80/169], [94mLoss[0m : 1.66863
[1mStep[0m  [96/169], [94mLoss[0m : 2.72931
[1mStep[0m  [112/169], [94mLoss[0m : 1.94367
[1mStep[0m  [128/169], [94mLoss[0m : 1.70689
[1mStep[0m  [144/169], [94mLoss[0m : 2.09572
[1mStep[0m  [160/169], [94mLoss[0m : 1.83844

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.278, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11688
[1mStep[0m  [16/169], [94mLoss[0m : 2.17249
[1mStep[0m  [32/169], [94mLoss[0m : 2.08155
[1mStep[0m  [48/169], [94mLoss[0m : 2.57849
[1mStep[0m  [64/169], [94mLoss[0m : 2.08864
[1mStep[0m  [80/169], [94mLoss[0m : 2.04485
[1mStep[0m  [96/169], [94mLoss[0m : 2.18011
[1mStep[0m  [112/169], [94mLoss[0m : 2.20135
[1mStep[0m  [128/169], [94mLoss[0m : 2.63048
[1mStep[0m  [144/169], [94mLoss[0m : 1.86000
[1mStep[0m  [160/169], [94mLoss[0m : 2.13593

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20104
[1mStep[0m  [16/169], [94mLoss[0m : 2.29270
[1mStep[0m  [32/169], [94mLoss[0m : 2.06300
[1mStep[0m  [48/169], [94mLoss[0m : 2.31712
[1mStep[0m  [64/169], [94mLoss[0m : 2.46044
[1mStep[0m  [80/169], [94mLoss[0m : 2.70385
[1mStep[0m  [96/169], [94mLoss[0m : 2.26762
[1mStep[0m  [112/169], [94mLoss[0m : 2.36140
[1mStep[0m  [128/169], [94mLoss[0m : 2.12256
[1mStep[0m  [144/169], [94mLoss[0m : 2.27992
[1mStep[0m  [160/169], [94mLoss[0m : 2.42766

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.224, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05289
[1mStep[0m  [16/169], [94mLoss[0m : 2.36865
[1mStep[0m  [32/169], [94mLoss[0m : 2.57539
[1mStep[0m  [48/169], [94mLoss[0m : 1.94760
[1mStep[0m  [64/169], [94mLoss[0m : 2.25182
[1mStep[0m  [80/169], [94mLoss[0m : 2.23298
[1mStep[0m  [96/169], [94mLoss[0m : 2.12613
[1mStep[0m  [112/169], [94mLoss[0m : 2.40633
[1mStep[0m  [128/169], [94mLoss[0m : 2.19009
[1mStep[0m  [144/169], [94mLoss[0m : 2.23905
[1mStep[0m  [160/169], [94mLoss[0m : 2.13728

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86594
[1mStep[0m  [16/169], [94mLoss[0m : 2.18446
[1mStep[0m  [32/169], [94mLoss[0m : 1.77023
[1mStep[0m  [48/169], [94mLoss[0m : 2.08311
[1mStep[0m  [64/169], [94mLoss[0m : 2.36886
[1mStep[0m  [80/169], [94mLoss[0m : 1.89309
[1mStep[0m  [96/169], [94mLoss[0m : 1.84806
[1mStep[0m  [112/169], [94mLoss[0m : 2.08169
[1mStep[0m  [128/169], [94mLoss[0m : 2.50209
[1mStep[0m  [144/169], [94mLoss[0m : 2.03004
[1mStep[0m  [160/169], [94mLoss[0m : 2.29122

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.132, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01463
[1mStep[0m  [16/169], [94mLoss[0m : 2.17282
[1mStep[0m  [32/169], [94mLoss[0m : 2.04906
[1mStep[0m  [48/169], [94mLoss[0m : 2.11794
[1mStep[0m  [64/169], [94mLoss[0m : 1.91840
[1mStep[0m  [80/169], [94mLoss[0m : 2.05376
[1mStep[0m  [96/169], [94mLoss[0m : 2.08739
[1mStep[0m  [112/169], [94mLoss[0m : 2.61134
[1mStep[0m  [128/169], [94mLoss[0m : 1.74527
[1mStep[0m  [144/169], [94mLoss[0m : 2.00359
[1mStep[0m  [160/169], [94mLoss[0m : 2.20039

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71596
[1mStep[0m  [16/169], [94mLoss[0m : 2.39292
[1mStep[0m  [32/169], [94mLoss[0m : 2.04708
[1mStep[0m  [48/169], [94mLoss[0m : 1.69750
[1mStep[0m  [64/169], [94mLoss[0m : 1.56451
[1mStep[0m  [80/169], [94mLoss[0m : 1.96529
[1mStep[0m  [96/169], [94mLoss[0m : 1.98595
[1mStep[0m  [112/169], [94mLoss[0m : 2.19113
[1mStep[0m  [128/169], [94mLoss[0m : 2.19720
[1mStep[0m  [144/169], [94mLoss[0m : 2.25685
[1mStep[0m  [160/169], [94mLoss[0m : 1.94022

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49209
[1mStep[0m  [16/169], [94mLoss[0m : 2.16788
[1mStep[0m  [32/169], [94mLoss[0m : 2.18050
[1mStep[0m  [48/169], [94mLoss[0m : 1.99865
[1mStep[0m  [64/169], [94mLoss[0m : 2.19571
[1mStep[0m  [80/169], [94mLoss[0m : 2.05764
[1mStep[0m  [96/169], [94mLoss[0m : 2.05021
[1mStep[0m  [112/169], [94mLoss[0m : 1.75994
[1mStep[0m  [128/169], [94mLoss[0m : 2.24748
[1mStep[0m  [144/169], [94mLoss[0m : 2.49822
[1mStep[0m  [160/169], [94mLoss[0m : 2.07180

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.007, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68557
[1mStep[0m  [16/169], [94mLoss[0m : 1.91184
[1mStep[0m  [32/169], [94mLoss[0m : 1.83599
[1mStep[0m  [48/169], [94mLoss[0m : 1.87966
[1mStep[0m  [64/169], [94mLoss[0m : 1.85909
[1mStep[0m  [80/169], [94mLoss[0m : 1.78034
[1mStep[0m  [96/169], [94mLoss[0m : 1.49411
[1mStep[0m  [112/169], [94mLoss[0m : 1.90920
[1mStep[0m  [128/169], [94mLoss[0m : 1.93134
[1mStep[0m  [144/169], [94mLoss[0m : 1.97261
[1mStep[0m  [160/169], [94mLoss[0m : 1.97800

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32419
[1mStep[0m  [16/169], [94mLoss[0m : 1.91310
[1mStep[0m  [32/169], [94mLoss[0m : 1.94976
[1mStep[0m  [48/169], [94mLoss[0m : 1.76102
[1mStep[0m  [64/169], [94mLoss[0m : 2.16611
[1mStep[0m  [80/169], [94mLoss[0m : 1.99227
[1mStep[0m  [96/169], [94mLoss[0m : 1.60896
[1mStep[0m  [112/169], [94mLoss[0m : 1.96740
[1mStep[0m  [128/169], [94mLoss[0m : 2.10795
[1mStep[0m  [144/169], [94mLoss[0m : 2.32779
[1mStep[0m  [160/169], [94mLoss[0m : 1.68601

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.960, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72732
[1mStep[0m  [16/169], [94mLoss[0m : 1.95143
[1mStep[0m  [32/169], [94mLoss[0m : 1.57637
[1mStep[0m  [48/169], [94mLoss[0m : 1.69680
[1mStep[0m  [64/169], [94mLoss[0m : 1.93662
[1mStep[0m  [80/169], [94mLoss[0m : 1.85793
[1mStep[0m  [96/169], [94mLoss[0m : 1.55646
[1mStep[0m  [112/169], [94mLoss[0m : 2.29459
[1mStep[0m  [128/169], [94mLoss[0m : 2.07435
[1mStep[0m  [144/169], [94mLoss[0m : 1.97106
[1mStep[0m  [160/169], [94mLoss[0m : 2.23358

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.934, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02458
[1mStep[0m  [16/169], [94mLoss[0m : 1.77337
[1mStep[0m  [32/169], [94mLoss[0m : 1.70709
[1mStep[0m  [48/169], [94mLoss[0m : 2.05589
[1mStep[0m  [64/169], [94mLoss[0m : 1.84116
[1mStep[0m  [80/169], [94mLoss[0m : 1.95131
[1mStep[0m  [96/169], [94mLoss[0m : 1.96260
[1mStep[0m  [112/169], [94mLoss[0m : 1.68633
[1mStep[0m  [128/169], [94mLoss[0m : 1.84790
[1mStep[0m  [144/169], [94mLoss[0m : 2.18466
[1mStep[0m  [160/169], [94mLoss[0m : 1.67552

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.923, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05936
[1mStep[0m  [16/169], [94mLoss[0m : 1.83051
[1mStep[0m  [32/169], [94mLoss[0m : 1.90936
[1mStep[0m  [48/169], [94mLoss[0m : 1.64286
[1mStep[0m  [64/169], [94mLoss[0m : 1.94183
[1mStep[0m  [80/169], [94mLoss[0m : 1.81363
[1mStep[0m  [96/169], [94mLoss[0m : 1.85962
[1mStep[0m  [112/169], [94mLoss[0m : 1.75940
[1mStep[0m  [128/169], [94mLoss[0m : 2.00928
[1mStep[0m  [144/169], [94mLoss[0m : 1.75087
[1mStep[0m  [160/169], [94mLoss[0m : 2.49415

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.895, [92mTest[0m: 2.546, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87441
[1mStep[0m  [16/169], [94mLoss[0m : 2.12345
[1mStep[0m  [32/169], [94mLoss[0m : 2.44642
[1mStep[0m  [48/169], [94mLoss[0m : 1.95708
[1mStep[0m  [64/169], [94mLoss[0m : 1.47275
[1mStep[0m  [80/169], [94mLoss[0m : 1.72453
[1mStep[0m  [96/169], [94mLoss[0m : 1.98445
[1mStep[0m  [112/169], [94mLoss[0m : 1.88467
[1mStep[0m  [128/169], [94mLoss[0m : 2.01686
[1mStep[0m  [144/169], [94mLoss[0m : 1.54554
[1mStep[0m  [160/169], [94mLoss[0m : 1.67346

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.834, [92mTest[0m: 2.530, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66212
[1mStep[0m  [16/169], [94mLoss[0m : 1.87137
[1mStep[0m  [32/169], [94mLoss[0m : 1.71052
[1mStep[0m  [48/169], [94mLoss[0m : 1.67179
[1mStep[0m  [64/169], [94mLoss[0m : 1.76274
[1mStep[0m  [80/169], [94mLoss[0m : 2.15035
[1mStep[0m  [96/169], [94mLoss[0m : 1.70333
[1mStep[0m  [112/169], [94mLoss[0m : 1.87625
[1mStep[0m  [128/169], [94mLoss[0m : 1.79943
[1mStep[0m  [144/169], [94mLoss[0m : 1.50438
[1mStep[0m  [160/169], [94mLoss[0m : 1.97872

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.817, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81302
[1mStep[0m  [16/169], [94mLoss[0m : 1.55947
[1mStep[0m  [32/169], [94mLoss[0m : 1.62852
[1mStep[0m  [48/169], [94mLoss[0m : 1.91392
[1mStep[0m  [64/169], [94mLoss[0m : 1.61864
[1mStep[0m  [80/169], [94mLoss[0m : 1.58811
[1mStep[0m  [96/169], [94mLoss[0m : 1.85252
[1mStep[0m  [112/169], [94mLoss[0m : 2.01098
[1mStep[0m  [128/169], [94mLoss[0m : 1.86056
[1mStep[0m  [144/169], [94mLoss[0m : 1.76317
[1mStep[0m  [160/169], [94mLoss[0m : 1.67793

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.771, [92mTest[0m: 2.562, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64495
[1mStep[0m  [16/169], [94mLoss[0m : 2.05841
[1mStep[0m  [32/169], [94mLoss[0m : 2.05445
[1mStep[0m  [48/169], [94mLoss[0m : 1.83240
[1mStep[0m  [64/169], [94mLoss[0m : 1.45562
[1mStep[0m  [80/169], [94mLoss[0m : 1.39801
[1mStep[0m  [96/169], [94mLoss[0m : 1.80786
[1mStep[0m  [112/169], [94mLoss[0m : 1.77208
[1mStep[0m  [128/169], [94mLoss[0m : 1.36277
[1mStep[0m  [144/169], [94mLoss[0m : 1.75492
[1mStep[0m  [160/169], [94mLoss[0m : 1.48125

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.755, [92mTest[0m: 2.527, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.80105
[1mStep[0m  [16/169], [94mLoss[0m : 2.32808
[1mStep[0m  [32/169], [94mLoss[0m : 1.53333
[1mStep[0m  [48/169], [94mLoss[0m : 2.08827
[1mStep[0m  [64/169], [94mLoss[0m : 1.56706
[1mStep[0m  [80/169], [94mLoss[0m : 1.80002
[1mStep[0m  [96/169], [94mLoss[0m : 1.33817
[1mStep[0m  [112/169], [94mLoss[0m : 1.35898
[1mStep[0m  [128/169], [94mLoss[0m : 1.94728
[1mStep[0m  [144/169], [94mLoss[0m : 1.72355
[1mStep[0m  [160/169], [94mLoss[0m : 1.58446

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.758, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47416
[1mStep[0m  [16/169], [94mLoss[0m : 1.96748
[1mStep[0m  [32/169], [94mLoss[0m : 1.73383
[1mStep[0m  [48/169], [94mLoss[0m : 1.39154
[1mStep[0m  [64/169], [94mLoss[0m : 1.90949
[1mStep[0m  [80/169], [94mLoss[0m : 1.51731
[1mStep[0m  [96/169], [94mLoss[0m : 1.58493
[1mStep[0m  [112/169], [94mLoss[0m : 1.78651
[1mStep[0m  [128/169], [94mLoss[0m : 1.74548
[1mStep[0m  [144/169], [94mLoss[0m : 1.69425
[1mStep[0m  [160/169], [94mLoss[0m : 1.92608

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83223
[1mStep[0m  [16/169], [94mLoss[0m : 1.55532
[1mStep[0m  [32/169], [94mLoss[0m : 1.76335
[1mStep[0m  [48/169], [94mLoss[0m : 1.71873
[1mStep[0m  [64/169], [94mLoss[0m : 1.47879
[1mStep[0m  [80/169], [94mLoss[0m : 1.45581
[1mStep[0m  [96/169], [94mLoss[0m : 1.55525
[1mStep[0m  [112/169], [94mLoss[0m : 1.61294
[1mStep[0m  [128/169], [94mLoss[0m : 1.59553
[1mStep[0m  [144/169], [94mLoss[0m : 2.03717
[1mStep[0m  [160/169], [94mLoss[0m : 1.79485

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.715, [92mTest[0m: 2.563, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84597
[1mStep[0m  [16/169], [94mLoss[0m : 1.73567
[1mStep[0m  [32/169], [94mLoss[0m : 1.58744
[1mStep[0m  [48/169], [94mLoss[0m : 1.72108
[1mStep[0m  [64/169], [94mLoss[0m : 1.75985
[1mStep[0m  [80/169], [94mLoss[0m : 1.52435
[1mStep[0m  [96/169], [94mLoss[0m : 1.88571
[1mStep[0m  [112/169], [94mLoss[0m : 1.86690
[1mStep[0m  [128/169], [94mLoss[0m : 1.92300
[1mStep[0m  [144/169], [94mLoss[0m : 1.84219
[1mStep[0m  [160/169], [94mLoss[0m : 1.45492

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79959
[1mStep[0m  [16/169], [94mLoss[0m : 1.51317
[1mStep[0m  [32/169], [94mLoss[0m : 1.99950
[1mStep[0m  [48/169], [94mLoss[0m : 1.45573
[1mStep[0m  [64/169], [94mLoss[0m : 2.00923
[1mStep[0m  [80/169], [94mLoss[0m : 1.77941
[1mStep[0m  [96/169], [94mLoss[0m : 1.61380
[1mStep[0m  [112/169], [94mLoss[0m : 1.69960
[1mStep[0m  [128/169], [94mLoss[0m : 1.67356
[1mStep[0m  [144/169], [94mLoss[0m : 1.55650
[1mStep[0m  [160/169], [94mLoss[0m : 1.57671

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.685, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71105
[1mStep[0m  [16/169], [94mLoss[0m : 1.56277
[1mStep[0m  [32/169], [94mLoss[0m : 1.65585
[1mStep[0m  [48/169], [94mLoss[0m : 1.75643
[1mStep[0m  [64/169], [94mLoss[0m : 1.76420
[1mStep[0m  [80/169], [94mLoss[0m : 1.76914
[1mStep[0m  [96/169], [94mLoss[0m : 1.83223
[1mStep[0m  [112/169], [94mLoss[0m : 1.73382
[1mStep[0m  [128/169], [94mLoss[0m : 1.33332
[1mStep[0m  [144/169], [94mLoss[0m : 1.58368
[1mStep[0m  [160/169], [94mLoss[0m : 1.45029

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.542, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.527
====================================

Phase 2 - Evaluation MAE:  2.5270822857107436
MAE score P1       2.332794
MAE score P2       2.527082
loss                1.64737
learning_rate      0.002575
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 10.39664
[1mStep[0m  [8/84], [94mLoss[0m : 8.87476
[1mStep[0m  [16/84], [94mLoss[0m : 6.76684
[1mStep[0m  [24/84], [94mLoss[0m : 5.14084
[1mStep[0m  [32/84], [94mLoss[0m : 4.12724
[1mStep[0m  [40/84], [94mLoss[0m : 3.09418
[1mStep[0m  [48/84], [94mLoss[0m : 2.68510
[1mStep[0m  [56/84], [94mLoss[0m : 3.06112
[1mStep[0m  [64/84], [94mLoss[0m : 2.83265
[1mStep[0m  [72/84], [94mLoss[0m : 2.64626
[1mStep[0m  [80/84], [94mLoss[0m : 3.09658

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.553, [92mTest[0m: 10.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77778
[1mStep[0m  [8/84], [94mLoss[0m : 2.86176
[1mStep[0m  [16/84], [94mLoss[0m : 2.64125
[1mStep[0m  [24/84], [94mLoss[0m : 2.68348
[1mStep[0m  [32/84], [94mLoss[0m : 3.04362
[1mStep[0m  [40/84], [94mLoss[0m : 2.44378
[1mStep[0m  [48/84], [94mLoss[0m : 2.62968
[1mStep[0m  [56/84], [94mLoss[0m : 2.66428
[1mStep[0m  [64/84], [94mLoss[0m : 2.80867
[1mStep[0m  [72/84], [94mLoss[0m : 2.76235
[1mStep[0m  [80/84], [94mLoss[0m : 2.47834

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.639, [92mTest[0m: 2.681, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.46219
[1mStep[0m  [8/84], [94mLoss[0m : 2.14968
[1mStep[0m  [16/84], [94mLoss[0m : 2.54368
[1mStep[0m  [24/84], [94mLoss[0m : 2.75735
[1mStep[0m  [32/84], [94mLoss[0m : 2.33352
[1mStep[0m  [40/84], [94mLoss[0m : 2.51585
[1mStep[0m  [48/84], [94mLoss[0m : 2.58319
[1mStep[0m  [56/84], [94mLoss[0m : 2.44996
[1mStep[0m  [64/84], [94mLoss[0m : 2.57484
[1mStep[0m  [72/84], [94mLoss[0m : 2.79039
[1mStep[0m  [80/84], [94mLoss[0m : 2.43470

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61996
[1mStep[0m  [8/84], [94mLoss[0m : 2.37633
[1mStep[0m  [16/84], [94mLoss[0m : 2.29572
[1mStep[0m  [24/84], [94mLoss[0m : 2.44628
[1mStep[0m  [32/84], [94mLoss[0m : 2.37863
[1mStep[0m  [40/84], [94mLoss[0m : 2.52292
[1mStep[0m  [48/84], [94mLoss[0m : 2.59801
[1mStep[0m  [56/84], [94mLoss[0m : 2.69851
[1mStep[0m  [64/84], [94mLoss[0m : 2.58244
[1mStep[0m  [72/84], [94mLoss[0m : 2.70259
[1mStep[0m  [80/84], [94mLoss[0m : 2.59256

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.59441
[1mStep[0m  [8/84], [94mLoss[0m : 2.43486
[1mStep[0m  [16/84], [94mLoss[0m : 2.49514
[1mStep[0m  [24/84], [94mLoss[0m : 2.37474
[1mStep[0m  [32/84], [94mLoss[0m : 2.60779
[1mStep[0m  [40/84], [94mLoss[0m : 2.54992
[1mStep[0m  [48/84], [94mLoss[0m : 2.75068
[1mStep[0m  [56/84], [94mLoss[0m : 2.57799
[1mStep[0m  [64/84], [94mLoss[0m : 2.35401
[1mStep[0m  [72/84], [94mLoss[0m : 2.78738
[1mStep[0m  [80/84], [94mLoss[0m : 2.63663

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35147
[1mStep[0m  [8/84], [94mLoss[0m : 2.45239
[1mStep[0m  [16/84], [94mLoss[0m : 2.43049
[1mStep[0m  [24/84], [94mLoss[0m : 2.53534
[1mStep[0m  [32/84], [94mLoss[0m : 2.62744
[1mStep[0m  [40/84], [94mLoss[0m : 2.41003
[1mStep[0m  [48/84], [94mLoss[0m : 2.27362
[1mStep[0m  [56/84], [94mLoss[0m : 2.10676
[1mStep[0m  [64/84], [94mLoss[0m : 2.72520
[1mStep[0m  [72/84], [94mLoss[0m : 2.67395
[1mStep[0m  [80/84], [94mLoss[0m : 2.32324

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.93912
[1mStep[0m  [8/84], [94mLoss[0m : 2.19814
[1mStep[0m  [16/84], [94mLoss[0m : 2.60482
[1mStep[0m  [24/84], [94mLoss[0m : 2.33844
[1mStep[0m  [32/84], [94mLoss[0m : 2.60405
[1mStep[0m  [40/84], [94mLoss[0m : 2.20373
[1mStep[0m  [48/84], [94mLoss[0m : 2.68086
[1mStep[0m  [56/84], [94mLoss[0m : 2.18113
[1mStep[0m  [64/84], [94mLoss[0m : 2.49099
[1mStep[0m  [72/84], [94mLoss[0m : 2.38809
[1mStep[0m  [80/84], [94mLoss[0m : 2.50948

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.58153
[1mStep[0m  [8/84], [94mLoss[0m : 2.63600
[1mStep[0m  [16/84], [94mLoss[0m : 2.49711
[1mStep[0m  [24/84], [94mLoss[0m : 2.53794
[1mStep[0m  [32/84], [94mLoss[0m : 2.39155
[1mStep[0m  [40/84], [94mLoss[0m : 2.59488
[1mStep[0m  [48/84], [94mLoss[0m : 2.54618
[1mStep[0m  [56/84], [94mLoss[0m : 2.52795
[1mStep[0m  [64/84], [94mLoss[0m : 2.64163
[1mStep[0m  [72/84], [94mLoss[0m : 2.42065
[1mStep[0m  [80/84], [94mLoss[0m : 2.47371

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.63961
[1mStep[0m  [8/84], [94mLoss[0m : 2.66389
[1mStep[0m  [16/84], [94mLoss[0m : 2.51767
[1mStep[0m  [24/84], [94mLoss[0m : 2.25426
[1mStep[0m  [32/84], [94mLoss[0m : 2.68262
[1mStep[0m  [40/84], [94mLoss[0m : 2.71945
[1mStep[0m  [48/84], [94mLoss[0m : 2.79300
[1mStep[0m  [56/84], [94mLoss[0m : 2.48043
[1mStep[0m  [64/84], [94mLoss[0m : 2.82821
[1mStep[0m  [72/84], [94mLoss[0m : 2.24978
[1mStep[0m  [80/84], [94mLoss[0m : 2.41645

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.397, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.56325
[1mStep[0m  [8/84], [94mLoss[0m : 2.74762
[1mStep[0m  [16/84], [94mLoss[0m : 2.53405
[1mStep[0m  [24/84], [94mLoss[0m : 2.49060
[1mStep[0m  [32/84], [94mLoss[0m : 2.42450
[1mStep[0m  [40/84], [94mLoss[0m : 2.44713
[1mStep[0m  [48/84], [94mLoss[0m : 2.34747
[1mStep[0m  [56/84], [94mLoss[0m : 2.23074
[1mStep[0m  [64/84], [94mLoss[0m : 2.59325
[1mStep[0m  [72/84], [94mLoss[0m : 2.58397
[1mStep[0m  [80/84], [94mLoss[0m : 2.27354

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.474, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.29138
[1mStep[0m  [8/84], [94mLoss[0m : 2.70425
[1mStep[0m  [16/84], [94mLoss[0m : 2.62385
[1mStep[0m  [24/84], [94mLoss[0m : 2.05239
[1mStep[0m  [32/84], [94mLoss[0m : 2.34741
[1mStep[0m  [40/84], [94mLoss[0m : 2.68034
[1mStep[0m  [48/84], [94mLoss[0m : 2.70514
[1mStep[0m  [56/84], [94mLoss[0m : 2.38604
[1mStep[0m  [64/84], [94mLoss[0m : 2.44187
[1mStep[0m  [72/84], [94mLoss[0m : 2.70350
[1mStep[0m  [80/84], [94mLoss[0m : 2.64180

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18172
[1mStep[0m  [8/84], [94mLoss[0m : 2.59176
[1mStep[0m  [16/84], [94mLoss[0m : 2.54591
[1mStep[0m  [24/84], [94mLoss[0m : 2.58671
[1mStep[0m  [32/84], [94mLoss[0m : 2.46509
[1mStep[0m  [40/84], [94mLoss[0m : 2.32041
[1mStep[0m  [48/84], [94mLoss[0m : 2.44963
[1mStep[0m  [56/84], [94mLoss[0m : 2.25261
[1mStep[0m  [64/84], [94mLoss[0m : 2.07851
[1mStep[0m  [72/84], [94mLoss[0m : 2.69004
[1mStep[0m  [80/84], [94mLoss[0m : 2.70047

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.384, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.80197
[1mStep[0m  [8/84], [94mLoss[0m : 2.30869
[1mStep[0m  [16/84], [94mLoss[0m : 2.51995
[1mStep[0m  [24/84], [94mLoss[0m : 2.54201
[1mStep[0m  [32/84], [94mLoss[0m : 2.24281
[1mStep[0m  [40/84], [94mLoss[0m : 2.45033
[1mStep[0m  [48/84], [94mLoss[0m : 2.36223
[1mStep[0m  [56/84], [94mLoss[0m : 2.45936
[1mStep[0m  [64/84], [94mLoss[0m : 2.13765
[1mStep[0m  [72/84], [94mLoss[0m : 2.45466
[1mStep[0m  [80/84], [94mLoss[0m : 2.69051

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.72837
[1mStep[0m  [8/84], [94mLoss[0m : 2.44005
[1mStep[0m  [16/84], [94mLoss[0m : 2.55983
[1mStep[0m  [24/84], [94mLoss[0m : 2.48054
[1mStep[0m  [32/84], [94mLoss[0m : 2.50640
[1mStep[0m  [40/84], [94mLoss[0m : 2.57098
[1mStep[0m  [48/84], [94mLoss[0m : 2.19359
[1mStep[0m  [56/84], [94mLoss[0m : 2.36871
[1mStep[0m  [64/84], [94mLoss[0m : 2.26224
[1mStep[0m  [72/84], [94mLoss[0m : 2.75926
[1mStep[0m  [80/84], [94mLoss[0m : 2.37862

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26485
[1mStep[0m  [8/84], [94mLoss[0m : 2.53312
[1mStep[0m  [16/84], [94mLoss[0m : 2.22603
[1mStep[0m  [24/84], [94mLoss[0m : 2.49779
[1mStep[0m  [32/84], [94mLoss[0m : 2.48600
[1mStep[0m  [40/84], [94mLoss[0m : 2.64532
[1mStep[0m  [48/84], [94mLoss[0m : 2.34821
[1mStep[0m  [56/84], [94mLoss[0m : 2.71069
[1mStep[0m  [64/84], [94mLoss[0m : 2.40662
[1mStep[0m  [72/84], [94mLoss[0m : 2.28851
[1mStep[0m  [80/84], [94mLoss[0m : 2.56888

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75021
[1mStep[0m  [8/84], [94mLoss[0m : 2.53037
[1mStep[0m  [16/84], [94mLoss[0m : 2.41409
[1mStep[0m  [24/84], [94mLoss[0m : 2.52657
[1mStep[0m  [32/84], [94mLoss[0m : 2.41186
[1mStep[0m  [40/84], [94mLoss[0m : 2.22270
[1mStep[0m  [48/84], [94mLoss[0m : 2.32395
[1mStep[0m  [56/84], [94mLoss[0m : 2.47740
[1mStep[0m  [64/84], [94mLoss[0m : 2.49125
[1mStep[0m  [72/84], [94mLoss[0m : 2.27088
[1mStep[0m  [80/84], [94mLoss[0m : 2.42688

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.33266
[1mStep[0m  [8/84], [94mLoss[0m : 2.40958
[1mStep[0m  [16/84], [94mLoss[0m : 2.66067
[1mStep[0m  [24/84], [94mLoss[0m : 2.56584
[1mStep[0m  [32/84], [94mLoss[0m : 2.52085
[1mStep[0m  [40/84], [94mLoss[0m : 2.56145
[1mStep[0m  [48/84], [94mLoss[0m : 2.33792
[1mStep[0m  [56/84], [94mLoss[0m : 2.63088
[1mStep[0m  [64/84], [94mLoss[0m : 2.31477
[1mStep[0m  [72/84], [94mLoss[0m : 2.45680
[1mStep[0m  [80/84], [94mLoss[0m : 2.43184

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47214
[1mStep[0m  [8/84], [94mLoss[0m : 2.48277
[1mStep[0m  [16/84], [94mLoss[0m : 2.44346
[1mStep[0m  [24/84], [94mLoss[0m : 2.49504
[1mStep[0m  [32/84], [94mLoss[0m : 2.42118
[1mStep[0m  [40/84], [94mLoss[0m : 2.40599
[1mStep[0m  [48/84], [94mLoss[0m : 2.34020
[1mStep[0m  [56/84], [94mLoss[0m : 2.42969
[1mStep[0m  [64/84], [94mLoss[0m : 2.52174
[1mStep[0m  [72/84], [94mLoss[0m : 2.48179
[1mStep[0m  [80/84], [94mLoss[0m : 2.57081

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.75748
[1mStep[0m  [8/84], [94mLoss[0m : 2.58129
[1mStep[0m  [16/84], [94mLoss[0m : 2.25490
[1mStep[0m  [24/84], [94mLoss[0m : 2.55485
[1mStep[0m  [32/84], [94mLoss[0m : 2.50223
[1mStep[0m  [40/84], [94mLoss[0m : 2.37120
[1mStep[0m  [48/84], [94mLoss[0m : 2.55514
[1mStep[0m  [56/84], [94mLoss[0m : 2.56511
[1mStep[0m  [64/84], [94mLoss[0m : 2.45801
[1mStep[0m  [72/84], [94mLoss[0m : 2.26073
[1mStep[0m  [80/84], [94mLoss[0m : 2.44439

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.24872
[1mStep[0m  [8/84], [94mLoss[0m : 2.36393
[1mStep[0m  [16/84], [94mLoss[0m : 2.32783
[1mStep[0m  [24/84], [94mLoss[0m : 2.41068
[1mStep[0m  [32/84], [94mLoss[0m : 2.17826
[1mStep[0m  [40/84], [94mLoss[0m : 2.20308
[1mStep[0m  [48/84], [94mLoss[0m : 2.35557
[1mStep[0m  [56/84], [94mLoss[0m : 2.72153
[1mStep[0m  [64/84], [94mLoss[0m : 2.41684
[1mStep[0m  [72/84], [94mLoss[0m : 2.38278
[1mStep[0m  [80/84], [94mLoss[0m : 2.52069

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.362, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37682
[1mStep[0m  [8/84], [94mLoss[0m : 2.66527
[1mStep[0m  [16/84], [94mLoss[0m : 2.47844
[1mStep[0m  [24/84], [94mLoss[0m : 2.37160
[1mStep[0m  [32/84], [94mLoss[0m : 2.28403
[1mStep[0m  [40/84], [94mLoss[0m : 2.74097
[1mStep[0m  [48/84], [94mLoss[0m : 2.65606
[1mStep[0m  [56/84], [94mLoss[0m : 2.48627
[1mStep[0m  [64/84], [94mLoss[0m : 1.94605
[1mStep[0m  [72/84], [94mLoss[0m : 2.48938
[1mStep[0m  [80/84], [94mLoss[0m : 2.43599

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.356, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42572
[1mStep[0m  [8/84], [94mLoss[0m : 2.38074
[1mStep[0m  [16/84], [94mLoss[0m : 2.40568
[1mStep[0m  [24/84], [94mLoss[0m : 2.35151
[1mStep[0m  [32/84], [94mLoss[0m : 2.66456
[1mStep[0m  [40/84], [94mLoss[0m : 2.47293
[1mStep[0m  [48/84], [94mLoss[0m : 2.48283
[1mStep[0m  [56/84], [94mLoss[0m : 2.45197
[1mStep[0m  [64/84], [94mLoss[0m : 2.22402
[1mStep[0m  [72/84], [94mLoss[0m : 2.30887
[1mStep[0m  [80/84], [94mLoss[0m : 2.25451

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25801
[1mStep[0m  [8/84], [94mLoss[0m : 2.58151
[1mStep[0m  [16/84], [94mLoss[0m : 2.59021
[1mStep[0m  [24/84], [94mLoss[0m : 2.23853
[1mStep[0m  [32/84], [94mLoss[0m : 2.31064
[1mStep[0m  [40/84], [94mLoss[0m : 2.23563
[1mStep[0m  [48/84], [94mLoss[0m : 2.27044
[1mStep[0m  [56/84], [94mLoss[0m : 2.51882
[1mStep[0m  [64/84], [94mLoss[0m : 2.70230
[1mStep[0m  [72/84], [94mLoss[0m : 2.50650
[1mStep[0m  [80/84], [94mLoss[0m : 2.55467

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.55421
[1mStep[0m  [8/84], [94mLoss[0m : 2.62774
[1mStep[0m  [16/84], [94mLoss[0m : 2.54543
[1mStep[0m  [24/84], [94mLoss[0m : 2.46511
[1mStep[0m  [32/84], [94mLoss[0m : 2.64460
[1mStep[0m  [40/84], [94mLoss[0m : 2.48447
[1mStep[0m  [48/84], [94mLoss[0m : 2.31841
[1mStep[0m  [56/84], [94mLoss[0m : 2.51719
[1mStep[0m  [64/84], [94mLoss[0m : 2.67592
[1mStep[0m  [72/84], [94mLoss[0m : 2.21178
[1mStep[0m  [80/84], [94mLoss[0m : 2.05237

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35053
[1mStep[0m  [8/84], [94mLoss[0m : 2.25225
[1mStep[0m  [16/84], [94mLoss[0m : 2.17051
[1mStep[0m  [24/84], [94mLoss[0m : 2.48876
[1mStep[0m  [32/84], [94mLoss[0m : 2.35991
[1mStep[0m  [40/84], [94mLoss[0m : 2.93503
[1mStep[0m  [48/84], [94mLoss[0m : 2.51398
[1mStep[0m  [56/84], [94mLoss[0m : 2.34246
[1mStep[0m  [64/84], [94mLoss[0m : 2.65237
[1mStep[0m  [72/84], [94mLoss[0m : 2.33131
[1mStep[0m  [80/84], [94mLoss[0m : 2.47316

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.352, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65891
[1mStep[0m  [8/84], [94mLoss[0m : 2.31672
[1mStep[0m  [16/84], [94mLoss[0m : 2.48443
[1mStep[0m  [24/84], [94mLoss[0m : 2.37513
[1mStep[0m  [32/84], [94mLoss[0m : 2.34181
[1mStep[0m  [40/84], [94mLoss[0m : 2.22392
[1mStep[0m  [48/84], [94mLoss[0m : 2.45355
[1mStep[0m  [56/84], [94mLoss[0m : 2.17499
[1mStep[0m  [64/84], [94mLoss[0m : 2.28103
[1mStep[0m  [72/84], [94mLoss[0m : 2.52357
[1mStep[0m  [80/84], [94mLoss[0m : 2.24490

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21726
[1mStep[0m  [8/84], [94mLoss[0m : 2.50485
[1mStep[0m  [16/84], [94mLoss[0m : 2.44737
[1mStep[0m  [24/84], [94mLoss[0m : 2.52666
[1mStep[0m  [32/84], [94mLoss[0m : 2.80751
[1mStep[0m  [40/84], [94mLoss[0m : 2.45978
[1mStep[0m  [48/84], [94mLoss[0m : 2.28240
[1mStep[0m  [56/84], [94mLoss[0m : 2.52024
[1mStep[0m  [64/84], [94mLoss[0m : 2.57335
[1mStep[0m  [72/84], [94mLoss[0m : 2.44923
[1mStep[0m  [80/84], [94mLoss[0m : 2.19471

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77064
[1mStep[0m  [8/84], [94mLoss[0m : 2.55066
[1mStep[0m  [16/84], [94mLoss[0m : 2.31988
[1mStep[0m  [24/84], [94mLoss[0m : 2.64268
[1mStep[0m  [32/84], [94mLoss[0m : 2.40389
[1mStep[0m  [40/84], [94mLoss[0m : 2.53902
[1mStep[0m  [48/84], [94mLoss[0m : 2.52335
[1mStep[0m  [56/84], [94mLoss[0m : 2.60922
[1mStep[0m  [64/84], [94mLoss[0m : 2.38959
[1mStep[0m  [72/84], [94mLoss[0m : 2.41270
[1mStep[0m  [80/84], [94mLoss[0m : 2.42754

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62785
[1mStep[0m  [8/84], [94mLoss[0m : 2.47148
[1mStep[0m  [16/84], [94mLoss[0m : 2.53291
[1mStep[0m  [24/84], [94mLoss[0m : 2.28878
[1mStep[0m  [32/84], [94mLoss[0m : 2.44228
[1mStep[0m  [40/84], [94mLoss[0m : 2.17883
[1mStep[0m  [48/84], [94mLoss[0m : 2.43571
[1mStep[0m  [56/84], [94mLoss[0m : 2.32909
[1mStep[0m  [64/84], [94mLoss[0m : 2.45111
[1mStep[0m  [72/84], [94mLoss[0m : 2.48295
[1mStep[0m  [80/84], [94mLoss[0m : 2.67173

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.37110
[1mStep[0m  [8/84], [94mLoss[0m : 2.61154
[1mStep[0m  [16/84], [94mLoss[0m : 2.33228
[1mStep[0m  [24/84], [94mLoss[0m : 2.17866
[1mStep[0m  [32/84], [94mLoss[0m : 2.22281
[1mStep[0m  [40/84], [94mLoss[0m : 2.56862
[1mStep[0m  [48/84], [94mLoss[0m : 2.34843
[1mStep[0m  [56/84], [94mLoss[0m : 2.61346
[1mStep[0m  [64/84], [94mLoss[0m : 2.52399
[1mStep[0m  [72/84], [94mLoss[0m : 2.73544
[1mStep[0m  [80/84], [94mLoss[0m : 2.38079

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.340, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.347
====================================

Phase 1 - Evaluation MAE:  2.3472543401377544
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/84], [94mLoss[0m : 2.55031
[1mStep[0m  [8/84], [94mLoss[0m : 2.38588
[1mStep[0m  [16/84], [94mLoss[0m : 2.34672
[1mStep[0m  [24/84], [94mLoss[0m : 2.55673
[1mStep[0m  [32/84], [94mLoss[0m : 2.47385
[1mStep[0m  [40/84], [94mLoss[0m : 2.39028
[1mStep[0m  [48/84], [94mLoss[0m : 2.44729
[1mStep[0m  [56/84], [94mLoss[0m : 2.49513
[1mStep[0m  [64/84], [94mLoss[0m : 2.41917
[1mStep[0m  [72/84], [94mLoss[0m : 2.25232
[1mStep[0m  [80/84], [94mLoss[0m : 2.54024

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52185
[1mStep[0m  [8/84], [94mLoss[0m : 2.52105
[1mStep[0m  [16/84], [94mLoss[0m : 2.66838
[1mStep[0m  [24/84], [94mLoss[0m : 2.38913
[1mStep[0m  [32/84], [94mLoss[0m : 2.17572
[1mStep[0m  [40/84], [94mLoss[0m : 2.22750
[1mStep[0m  [48/84], [94mLoss[0m : 2.39901
[1mStep[0m  [56/84], [94mLoss[0m : 2.48553
[1mStep[0m  [64/84], [94mLoss[0m : 2.45533
[1mStep[0m  [72/84], [94mLoss[0m : 2.55265
[1mStep[0m  [80/84], [94mLoss[0m : 2.53656

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45368
[1mStep[0m  [8/84], [94mLoss[0m : 2.12827
[1mStep[0m  [16/84], [94mLoss[0m : 2.42884
[1mStep[0m  [24/84], [94mLoss[0m : 2.38577
[1mStep[0m  [32/84], [94mLoss[0m : 2.24470
[1mStep[0m  [40/84], [94mLoss[0m : 2.32494
[1mStep[0m  [48/84], [94mLoss[0m : 2.55018
[1mStep[0m  [56/84], [94mLoss[0m : 2.33091
[1mStep[0m  [64/84], [94mLoss[0m : 2.64970
[1mStep[0m  [72/84], [94mLoss[0m : 2.72082
[1mStep[0m  [80/84], [94mLoss[0m : 2.38273

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42197
[1mStep[0m  [8/84], [94mLoss[0m : 2.32791
[1mStep[0m  [16/84], [94mLoss[0m : 2.48414
[1mStep[0m  [24/84], [94mLoss[0m : 2.34746
[1mStep[0m  [32/84], [94mLoss[0m : 2.48078
[1mStep[0m  [40/84], [94mLoss[0m : 2.57794
[1mStep[0m  [48/84], [94mLoss[0m : 2.26838
[1mStep[0m  [56/84], [94mLoss[0m : 2.71896
[1mStep[0m  [64/84], [94mLoss[0m : 2.53824
[1mStep[0m  [72/84], [94mLoss[0m : 2.17657
[1mStep[0m  [80/84], [94mLoss[0m : 2.18152

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23689
[1mStep[0m  [8/84], [94mLoss[0m : 2.38296
[1mStep[0m  [16/84], [94mLoss[0m : 2.24404
[1mStep[0m  [24/84], [94mLoss[0m : 2.25875
[1mStep[0m  [32/84], [94mLoss[0m : 2.41873
[1mStep[0m  [40/84], [94mLoss[0m : 2.36342
[1mStep[0m  [48/84], [94mLoss[0m : 2.28658
[1mStep[0m  [56/84], [94mLoss[0m : 2.17426
[1mStep[0m  [64/84], [94mLoss[0m : 2.49719
[1mStep[0m  [72/84], [94mLoss[0m : 2.53642
[1mStep[0m  [80/84], [94mLoss[0m : 2.62648

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.530, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.31313
[1mStep[0m  [8/84], [94mLoss[0m : 2.25445
[1mStep[0m  [16/84], [94mLoss[0m : 2.07298
[1mStep[0m  [24/84], [94mLoss[0m : 2.11750
[1mStep[0m  [32/84], [94mLoss[0m : 2.32871
[1mStep[0m  [40/84], [94mLoss[0m : 2.31250
[1mStep[0m  [48/84], [94mLoss[0m : 2.57749
[1mStep[0m  [56/84], [94mLoss[0m : 2.27931
[1mStep[0m  [64/84], [94mLoss[0m : 2.48952
[1mStep[0m  [72/84], [94mLoss[0m : 2.33857
[1mStep[0m  [80/84], [94mLoss[0m : 2.41247

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.566, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27037
[1mStep[0m  [8/84], [94mLoss[0m : 2.02747
[1mStep[0m  [16/84], [94mLoss[0m : 2.24022
[1mStep[0m  [24/84], [94mLoss[0m : 2.50029
[1mStep[0m  [32/84], [94mLoss[0m : 2.43618
[1mStep[0m  [40/84], [94mLoss[0m : 2.61348
[1mStep[0m  [48/84], [94mLoss[0m : 2.36040
[1mStep[0m  [56/84], [94mLoss[0m : 2.29814
[1mStep[0m  [64/84], [94mLoss[0m : 2.22356
[1mStep[0m  [72/84], [94mLoss[0m : 2.30228
[1mStep[0m  [80/84], [94mLoss[0m : 2.62108

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.32411
[1mStep[0m  [8/84], [94mLoss[0m : 2.47224
[1mStep[0m  [16/84], [94mLoss[0m : 2.65770
[1mStep[0m  [24/84], [94mLoss[0m : 2.38167
[1mStep[0m  [32/84], [94mLoss[0m : 2.43762
[1mStep[0m  [40/84], [94mLoss[0m : 2.77786
[1mStep[0m  [48/84], [94mLoss[0m : 2.12259
[1mStep[0m  [56/84], [94mLoss[0m : 2.41245
[1mStep[0m  [64/84], [94mLoss[0m : 2.67200
[1mStep[0m  [72/84], [94mLoss[0m : 2.33404
[1mStep[0m  [80/84], [94mLoss[0m : 2.47101

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.572, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.60429
[1mStep[0m  [8/84], [94mLoss[0m : 2.34487
[1mStep[0m  [16/84], [94mLoss[0m : 2.50009
[1mStep[0m  [24/84], [94mLoss[0m : 2.23872
[1mStep[0m  [32/84], [94mLoss[0m : 2.21922
[1mStep[0m  [40/84], [94mLoss[0m : 2.30481
[1mStep[0m  [48/84], [94mLoss[0m : 2.25077
[1mStep[0m  [56/84], [94mLoss[0m : 2.32262
[1mStep[0m  [64/84], [94mLoss[0m : 2.27385
[1mStep[0m  [72/84], [94mLoss[0m : 2.37922
[1mStep[0m  [80/84], [94mLoss[0m : 2.23963

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.544, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35070
[1mStep[0m  [8/84], [94mLoss[0m : 2.39455
[1mStep[0m  [16/84], [94mLoss[0m : 2.29699
[1mStep[0m  [24/84], [94mLoss[0m : 2.16015
[1mStep[0m  [32/84], [94mLoss[0m : 2.34406
[1mStep[0m  [40/84], [94mLoss[0m : 2.48837
[1mStep[0m  [48/84], [94mLoss[0m : 1.97994
[1mStep[0m  [56/84], [94mLoss[0m : 2.17296
[1mStep[0m  [64/84], [94mLoss[0m : 2.36541
[1mStep[0m  [72/84], [94mLoss[0m : 2.22259
[1mStep[0m  [80/84], [94mLoss[0m : 2.41981

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.557, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.21651
[1mStep[0m  [8/84], [94mLoss[0m : 2.34233
[1mStep[0m  [16/84], [94mLoss[0m : 2.39322
[1mStep[0m  [24/84], [94mLoss[0m : 2.06618
[1mStep[0m  [32/84], [94mLoss[0m : 2.45967
[1mStep[0m  [40/84], [94mLoss[0m : 2.16974
[1mStep[0m  [48/84], [94mLoss[0m : 2.28566
[1mStep[0m  [56/84], [94mLoss[0m : 2.14102
[1mStep[0m  [64/84], [94mLoss[0m : 2.59566
[1mStep[0m  [72/84], [94mLoss[0m : 2.58236
[1mStep[0m  [80/84], [94mLoss[0m : 1.98154

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.292, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.40577
[1mStep[0m  [8/84], [94mLoss[0m : 2.29894
[1mStep[0m  [16/84], [94mLoss[0m : 1.91264
[1mStep[0m  [24/84], [94mLoss[0m : 2.13342
[1mStep[0m  [32/84], [94mLoss[0m : 2.63259
[1mStep[0m  [40/84], [94mLoss[0m : 2.13657
[1mStep[0m  [48/84], [94mLoss[0m : 2.25502
[1mStep[0m  [56/84], [94mLoss[0m : 2.37515
[1mStep[0m  [64/84], [94mLoss[0m : 2.47465
[1mStep[0m  [72/84], [94mLoss[0m : 2.22006
[1mStep[0m  [80/84], [94mLoss[0m : 2.38262

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.592, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12200
[1mStep[0m  [8/84], [94mLoss[0m : 2.28430
[1mStep[0m  [16/84], [94mLoss[0m : 2.38943
[1mStep[0m  [24/84], [94mLoss[0m : 2.70198
[1mStep[0m  [32/84], [94mLoss[0m : 2.32259
[1mStep[0m  [40/84], [94mLoss[0m : 2.46697
[1mStep[0m  [48/84], [94mLoss[0m : 2.09406
[1mStep[0m  [56/84], [94mLoss[0m : 2.21323
[1mStep[0m  [64/84], [94mLoss[0m : 2.44023
[1mStep[0m  [72/84], [94mLoss[0m : 2.03881
[1mStep[0m  [80/84], [94mLoss[0m : 2.13236

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.609, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52755
[1mStep[0m  [8/84], [94mLoss[0m : 2.37096
[1mStep[0m  [16/84], [94mLoss[0m : 2.23956
[1mStep[0m  [24/84], [94mLoss[0m : 2.25661
[1mStep[0m  [32/84], [94mLoss[0m : 2.21095
[1mStep[0m  [40/84], [94mLoss[0m : 2.07958
[1mStep[0m  [48/84], [94mLoss[0m : 2.47618
[1mStep[0m  [56/84], [94mLoss[0m : 2.29037
[1mStep[0m  [64/84], [94mLoss[0m : 2.26239
[1mStep[0m  [72/84], [94mLoss[0m : 2.04867
[1mStep[0m  [80/84], [94mLoss[0m : 2.40255

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.578, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.86800
[1mStep[0m  [8/84], [94mLoss[0m : 2.30977
[1mStep[0m  [16/84], [94mLoss[0m : 2.19034
[1mStep[0m  [24/84], [94mLoss[0m : 2.29492
[1mStep[0m  [32/84], [94mLoss[0m : 2.28120
[1mStep[0m  [40/84], [94mLoss[0m : 2.05570
[1mStep[0m  [48/84], [94mLoss[0m : 2.33901
[1mStep[0m  [56/84], [94mLoss[0m : 2.38408
[1mStep[0m  [64/84], [94mLoss[0m : 2.34793
[1mStep[0m  [72/84], [94mLoss[0m : 2.00820
[1mStep[0m  [80/84], [94mLoss[0m : 2.42716

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.05561
[1mStep[0m  [8/84], [94mLoss[0m : 2.27295
[1mStep[0m  [16/84], [94mLoss[0m : 2.15638
[1mStep[0m  [24/84], [94mLoss[0m : 2.28351
[1mStep[0m  [32/84], [94mLoss[0m : 2.38774
[1mStep[0m  [40/84], [94mLoss[0m : 2.16662
[1mStep[0m  [48/84], [94mLoss[0m : 2.17171
[1mStep[0m  [56/84], [94mLoss[0m : 2.45175
[1mStep[0m  [64/84], [94mLoss[0m : 2.55024
[1mStep[0m  [72/84], [94mLoss[0m : 2.34670
[1mStep[0m  [80/84], [94mLoss[0m : 2.18302

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.529, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17146
[1mStep[0m  [8/84], [94mLoss[0m : 2.46079
[1mStep[0m  [16/84], [94mLoss[0m : 2.21930
[1mStep[0m  [24/84], [94mLoss[0m : 2.37854
[1mStep[0m  [32/84], [94mLoss[0m : 2.22121
[1mStep[0m  [40/84], [94mLoss[0m : 2.29712
[1mStep[0m  [48/84], [94mLoss[0m : 2.07876
[1mStep[0m  [56/84], [94mLoss[0m : 2.17262
[1mStep[0m  [64/84], [94mLoss[0m : 2.20704
[1mStep[0m  [72/84], [94mLoss[0m : 2.66438
[1mStep[0m  [80/84], [94mLoss[0m : 2.25365

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.184, [92mTest[0m: 2.570, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.12445
[1mStep[0m  [8/84], [94mLoss[0m : 2.17797
[1mStep[0m  [16/84], [94mLoss[0m : 2.15801
[1mStep[0m  [24/84], [94mLoss[0m : 2.07653
[1mStep[0m  [32/84], [94mLoss[0m : 2.00490
[1mStep[0m  [40/84], [94mLoss[0m : 2.08313
[1mStep[0m  [48/84], [94mLoss[0m : 2.30464
[1mStep[0m  [56/84], [94mLoss[0m : 2.34138
[1mStep[0m  [64/84], [94mLoss[0m : 2.17222
[1mStep[0m  [72/84], [94mLoss[0m : 1.89513
[1mStep[0m  [80/84], [94mLoss[0m : 2.11549

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.169, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.23666
[1mStep[0m  [8/84], [94mLoss[0m : 2.27854
[1mStep[0m  [16/84], [94mLoss[0m : 2.01276
[1mStep[0m  [24/84], [94mLoss[0m : 2.42249
[1mStep[0m  [32/84], [94mLoss[0m : 2.20853
[1mStep[0m  [40/84], [94mLoss[0m : 1.90788
[1mStep[0m  [48/84], [94mLoss[0m : 2.07648
[1mStep[0m  [56/84], [94mLoss[0m : 2.26445
[1mStep[0m  [64/84], [94mLoss[0m : 2.30881
[1mStep[0m  [72/84], [94mLoss[0m : 2.03776
[1mStep[0m  [80/84], [94mLoss[0m : 2.11422

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93957
[1mStep[0m  [8/84], [94mLoss[0m : 2.12844
[1mStep[0m  [16/84], [94mLoss[0m : 1.93399
[1mStep[0m  [24/84], [94mLoss[0m : 2.26193
[1mStep[0m  [32/84], [94mLoss[0m : 2.14876
[1mStep[0m  [40/84], [94mLoss[0m : 1.97714
[1mStep[0m  [48/84], [94mLoss[0m : 2.22229
[1mStep[0m  [56/84], [94mLoss[0m : 2.04035
[1mStep[0m  [64/84], [94mLoss[0m : 2.15575
[1mStep[0m  [72/84], [94mLoss[0m : 1.90302
[1mStep[0m  [80/84], [94mLoss[0m : 1.98052

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.130, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.18250
[1mStep[0m  [8/84], [94mLoss[0m : 2.19705
[1mStep[0m  [16/84], [94mLoss[0m : 2.24245
[1mStep[0m  [24/84], [94mLoss[0m : 2.20947
[1mStep[0m  [32/84], [94mLoss[0m : 2.05889
[1mStep[0m  [40/84], [94mLoss[0m : 2.31464
[1mStep[0m  [48/84], [94mLoss[0m : 2.14907
[1mStep[0m  [56/84], [94mLoss[0m : 2.19796
[1mStep[0m  [64/84], [94mLoss[0m : 1.79402
[1mStep[0m  [72/84], [94mLoss[0m : 2.18991
[1mStep[0m  [80/84], [94mLoss[0m : 2.15314

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.107, [92mTest[0m: 2.597, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68858
[1mStep[0m  [8/84], [94mLoss[0m : 2.36665
[1mStep[0m  [16/84], [94mLoss[0m : 2.21202
[1mStep[0m  [24/84], [94mLoss[0m : 1.71270
[1mStep[0m  [32/84], [94mLoss[0m : 1.97057
[1mStep[0m  [40/84], [94mLoss[0m : 1.85512
[1mStep[0m  [48/84], [94mLoss[0m : 1.80853
[1mStep[0m  [56/84], [94mLoss[0m : 2.31592
[1mStep[0m  [64/84], [94mLoss[0m : 2.35508
[1mStep[0m  [72/84], [94mLoss[0m : 2.10053
[1mStep[0m  [80/84], [94mLoss[0m : 1.96589

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.092, [92mTest[0m: 2.549, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82463
[1mStep[0m  [8/84], [94mLoss[0m : 2.02510
[1mStep[0m  [16/84], [94mLoss[0m : 2.28630
[1mStep[0m  [24/84], [94mLoss[0m : 2.18750
[1mStep[0m  [32/84], [94mLoss[0m : 2.08082
[1mStep[0m  [40/84], [94mLoss[0m : 2.01527
[1mStep[0m  [48/84], [94mLoss[0m : 2.10308
[1mStep[0m  [56/84], [94mLoss[0m : 2.07717
[1mStep[0m  [64/84], [94mLoss[0m : 2.22011
[1mStep[0m  [72/84], [94mLoss[0m : 1.94742
[1mStep[0m  [80/84], [94mLoss[0m : 2.10836

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.578, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.25431
[1mStep[0m  [8/84], [94mLoss[0m : 1.82863
[1mStep[0m  [16/84], [94mLoss[0m : 2.34578
[1mStep[0m  [24/84], [94mLoss[0m : 2.33721
[1mStep[0m  [32/84], [94mLoss[0m : 2.13826
[1mStep[0m  [40/84], [94mLoss[0m : 1.91361
[1mStep[0m  [48/84], [94mLoss[0m : 1.96700
[1mStep[0m  [56/84], [94mLoss[0m : 2.37208
[1mStep[0m  [64/84], [94mLoss[0m : 1.95866
[1mStep[0m  [72/84], [94mLoss[0m : 1.96426
[1mStep[0m  [80/84], [94mLoss[0m : 2.23436

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.041, [92mTest[0m: 2.536, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73904
[1mStep[0m  [8/84], [94mLoss[0m : 1.79671
[1mStep[0m  [16/84], [94mLoss[0m : 1.81309
[1mStep[0m  [24/84], [94mLoss[0m : 2.09902
[1mStep[0m  [32/84], [94mLoss[0m : 1.84448
[1mStep[0m  [40/84], [94mLoss[0m : 1.90284
[1mStep[0m  [48/84], [94mLoss[0m : 2.05843
[1mStep[0m  [56/84], [94mLoss[0m : 2.14262
[1mStep[0m  [64/84], [94mLoss[0m : 2.39348
[1mStep[0m  [72/84], [94mLoss[0m : 2.23730
[1mStep[0m  [80/84], [94mLoss[0m : 1.87774

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.006, [92mTest[0m: 2.515, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81340
[1mStep[0m  [8/84], [94mLoss[0m : 1.89777
[1mStep[0m  [16/84], [94mLoss[0m : 1.85488
[1mStep[0m  [24/84], [94mLoss[0m : 2.00927
[1mStep[0m  [32/84], [94mLoss[0m : 2.03963
[1mStep[0m  [40/84], [94mLoss[0m : 1.97124
[1mStep[0m  [48/84], [94mLoss[0m : 1.64460
[1mStep[0m  [56/84], [94mLoss[0m : 2.28179
[1mStep[0m  [64/84], [94mLoss[0m : 2.21441
[1mStep[0m  [72/84], [94mLoss[0m : 1.73403
[1mStep[0m  [80/84], [94mLoss[0m : 1.76570

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.985, [92mTest[0m: 2.560, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17811
[1mStep[0m  [8/84], [94mLoss[0m : 1.78588
[1mStep[0m  [16/84], [94mLoss[0m : 1.93775
[1mStep[0m  [24/84], [94mLoss[0m : 1.95210
[1mStep[0m  [32/84], [94mLoss[0m : 1.88260
[1mStep[0m  [40/84], [94mLoss[0m : 1.92601
[1mStep[0m  [48/84], [94mLoss[0m : 1.93406
[1mStep[0m  [56/84], [94mLoss[0m : 1.77649
[1mStep[0m  [64/84], [94mLoss[0m : 2.07632
[1mStep[0m  [72/84], [94mLoss[0m : 2.35867
[1mStep[0m  [80/84], [94mLoss[0m : 2.15863

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.979, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09819
[1mStep[0m  [8/84], [94mLoss[0m : 2.06269
[1mStep[0m  [16/84], [94mLoss[0m : 1.83659
[1mStep[0m  [24/84], [94mLoss[0m : 1.75541
[1mStep[0m  [32/84], [94mLoss[0m : 1.94807
[1mStep[0m  [40/84], [94mLoss[0m : 1.85459
[1mStep[0m  [48/84], [94mLoss[0m : 1.81442
[1mStep[0m  [56/84], [94mLoss[0m : 1.66058
[1mStep[0m  [64/84], [94mLoss[0m : 1.88084
[1mStep[0m  [72/84], [94mLoss[0m : 2.15299
[1mStep[0m  [80/84], [94mLoss[0m : 2.12250

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.544, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78446
[1mStep[0m  [8/84], [94mLoss[0m : 1.96685
[1mStep[0m  [16/84], [94mLoss[0m : 1.90716
[1mStep[0m  [24/84], [94mLoss[0m : 1.99425
[1mStep[0m  [32/84], [94mLoss[0m : 1.84330
[1mStep[0m  [40/84], [94mLoss[0m : 1.64920
[1mStep[0m  [48/84], [94mLoss[0m : 2.06369
[1mStep[0m  [56/84], [94mLoss[0m : 1.83739
[1mStep[0m  [64/84], [94mLoss[0m : 1.79599
[1mStep[0m  [72/84], [94mLoss[0m : 1.97374
[1mStep[0m  [80/84], [94mLoss[0m : 1.94950

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.68762
[1mStep[0m  [8/84], [94mLoss[0m : 1.98925
[1mStep[0m  [16/84], [94mLoss[0m : 2.03155
[1mStep[0m  [24/84], [94mLoss[0m : 1.94613
[1mStep[0m  [32/84], [94mLoss[0m : 1.65866
[1mStep[0m  [40/84], [94mLoss[0m : 2.05692
[1mStep[0m  [48/84], [94mLoss[0m : 2.01341
[1mStep[0m  [56/84], [94mLoss[0m : 1.89736
[1mStep[0m  [64/84], [94mLoss[0m : 1.99572
[1mStep[0m  [72/84], [94mLoss[0m : 1.74710
[1mStep[0m  [80/84], [94mLoss[0m : 1.89171

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.893, [92mTest[0m: 2.563, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.496
====================================

Phase 2 - Evaluation MAE:  2.4960307478904724
MAE score P1      2.347254
MAE score P2      2.496031
loss              1.892998
learning_rate     0.002575
batch_size             128
hidden_sizes         [300]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.1
weight_decay        0.0001
Name: 12, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.0001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 10.83718
[1mStep[0m  [8/84], [94mLoss[0m : 10.52624
[1mStep[0m  [16/84], [94mLoss[0m : 10.75919
[1mStep[0m  [24/84], [94mLoss[0m : 10.02768
[1mStep[0m  [32/84], [94mLoss[0m : 10.96829
[1mStep[0m  [40/84], [94mLoss[0m : 10.94630
[1mStep[0m  [48/84], [94mLoss[0m : 10.43371
[1mStep[0m  [56/84], [94mLoss[0m : 10.56225
[1mStep[0m  [64/84], [94mLoss[0m : 10.65604
[1mStep[0m  [72/84], [94mLoss[0m : 10.40601
[1mStep[0m  [80/84], [94mLoss[0m : 10.53786

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.584, [92mTest[0m: 11.140, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 10.28892
[1mStep[0m  [8/84], [94mLoss[0m : 10.74233
[1mStep[0m  [16/84], [94mLoss[0m : 10.39662
[1mStep[0m  [24/84], [94mLoss[0m : 9.29699
[1mStep[0m  [32/84], [94mLoss[0m : 9.08002
[1mStep[0m  [40/84], [94mLoss[0m : 10.17713
[1mStep[0m  [48/84], [94mLoss[0m : 10.08165
[1mStep[0m  [56/84], [94mLoss[0m : 9.34142
[1mStep[0m  [64/84], [94mLoss[0m : 9.88124
[1mStep[0m  [72/84], [94mLoss[0m : 9.74692
[1mStep[0m  [80/84], [94mLoss[0m : 9.78095

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.869, [92mTest[0m: 10.126, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.77507
[1mStep[0m  [8/84], [94mLoss[0m : 9.66192
[1mStep[0m  [16/84], [94mLoss[0m : 9.07201
[1mStep[0m  [24/84], [94mLoss[0m : 9.73001
[1mStep[0m  [32/84], [94mLoss[0m : 9.37896
[1mStep[0m  [40/84], [94mLoss[0m : 8.61452
[1mStep[0m  [48/84], [94mLoss[0m : 8.61928
[1mStep[0m  [56/84], [94mLoss[0m : 8.97774
[1mStep[0m  [64/84], [94mLoss[0m : 8.47824
[1mStep[0m  [72/84], [94mLoss[0m : 8.26345
[1mStep[0m  [80/84], [94mLoss[0m : 8.15322

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.977, [92mTest[0m: 9.115, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 8.62807
[1mStep[0m  [8/84], [94mLoss[0m : 7.99536
[1mStep[0m  [16/84], [94mLoss[0m : 8.66928
[1mStep[0m  [24/84], [94mLoss[0m : 8.28707
[1mStep[0m  [32/84], [94mLoss[0m : 7.63945
[1mStep[0m  [40/84], [94mLoss[0m : 7.66405
[1mStep[0m  [48/84], [94mLoss[0m : 8.02001
[1mStep[0m  [56/84], [94mLoss[0m : 7.41441
[1mStep[0m  [64/84], [94mLoss[0m : 6.87870
[1mStep[0m  [72/84], [94mLoss[0m : 7.58932
[1mStep[0m  [80/84], [94mLoss[0m : 7.51358

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.856, [92mTest[0m: 7.699, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.76790
[1mStep[0m  [8/84], [94mLoss[0m : 7.60400
[1mStep[0m  [16/84], [94mLoss[0m : 6.98426
[1mStep[0m  [24/84], [94mLoss[0m : 7.12263
[1mStep[0m  [32/84], [94mLoss[0m : 6.87102
[1mStep[0m  [40/84], [94mLoss[0m : 6.91094
[1mStep[0m  [48/84], [94mLoss[0m : 7.43603
[1mStep[0m  [56/84], [94mLoss[0m : 6.38268
[1mStep[0m  [64/84], [94mLoss[0m : 6.22310
[1mStep[0m  [72/84], [94mLoss[0m : 6.43254
[1mStep[0m  [80/84], [94mLoss[0m : 6.61093

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.812, [92mTest[0m: 6.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 6.03884
[1mStep[0m  [8/84], [94mLoss[0m : 6.45244
[1mStep[0m  [16/84], [94mLoss[0m : 6.37987
[1mStep[0m  [24/84], [94mLoss[0m : 5.79104
[1mStep[0m  [32/84], [94mLoss[0m : 6.10233
[1mStep[0m  [40/84], [94mLoss[0m : 5.92952
[1mStep[0m  [48/84], [94mLoss[0m : 6.35368
[1mStep[0m  [56/84], [94mLoss[0m : 5.23074
[1mStep[0m  [64/84], [94mLoss[0m : 6.28856
[1mStep[0m  [72/84], [94mLoss[0m : 6.14693
[1mStep[0m  [80/84], [94mLoss[0m : 5.87128

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.997, [92mTest[0m: 5.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 5.60427
[1mStep[0m  [8/84], [94mLoss[0m : 5.69078
[1mStep[0m  [16/84], [94mLoss[0m : 5.40506
[1mStep[0m  [24/84], [94mLoss[0m : 5.57889
[1mStep[0m  [32/84], [94mLoss[0m : 5.16761
[1mStep[0m  [40/84], [94mLoss[0m : 5.85223
[1mStep[0m  [48/84], [94mLoss[0m : 5.27926
[1mStep[0m  [56/84], [94mLoss[0m : 5.28775
[1mStep[0m  [64/84], [94mLoss[0m : 5.36482
[1mStep[0m  [72/84], [94mLoss[0m : 4.73051
[1mStep[0m  [80/84], [94mLoss[0m : 5.06718

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 5.203, [92mTest[0m: 4.793, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 4.79275
[1mStep[0m  [8/84], [94mLoss[0m : 4.59446
[1mStep[0m  [16/84], [94mLoss[0m : 4.27477
[1mStep[0m  [24/84], [94mLoss[0m : 4.30005
[1mStep[0m  [32/84], [94mLoss[0m : 4.49122
[1mStep[0m  [40/84], [94mLoss[0m : 4.29121
[1mStep[0m  [48/84], [94mLoss[0m : 4.56688
[1mStep[0m  [56/84], [94mLoss[0m : 4.09844
[1mStep[0m  [64/84], [94mLoss[0m : 4.30213
[1mStep[0m  [72/84], [94mLoss[0m : 3.95289
[1mStep[0m  [80/84], [94mLoss[0m : 3.83908

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 4.346, [92mTest[0m: 3.955, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.98526
[1mStep[0m  [8/84], [94mLoss[0m : 3.65292
[1mStep[0m  [16/84], [94mLoss[0m : 3.99215
[1mStep[0m  [24/84], [94mLoss[0m : 3.43569
[1mStep[0m  [32/84], [94mLoss[0m : 3.52512
[1mStep[0m  [40/84], [94mLoss[0m : 3.33840
[1mStep[0m  [48/84], [94mLoss[0m : 3.18536
[1mStep[0m  [56/84], [94mLoss[0m : 3.29656
[1mStep[0m  [64/84], [94mLoss[0m : 3.22795
[1mStep[0m  [72/84], [94mLoss[0m : 2.88457
[1mStep[0m  [80/84], [94mLoss[0m : 3.03464

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 3.517, [92mTest[0m: 3.180, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96894
[1mStep[0m  [8/84], [94mLoss[0m : 3.43888
[1mStep[0m  [16/84], [94mLoss[0m : 2.92037
[1mStep[0m  [24/84], [94mLoss[0m : 3.17875
[1mStep[0m  [32/84], [94mLoss[0m : 2.59759
[1mStep[0m  [40/84], [94mLoss[0m : 3.18247
[1mStep[0m  [48/84], [94mLoss[0m : 2.79958
[1mStep[0m  [56/84], [94mLoss[0m : 2.88292
[1mStep[0m  [64/84], [94mLoss[0m : 2.66577
[1mStep[0m  [72/84], [94mLoss[0m : 3.11784
[1mStep[0m  [80/84], [94mLoss[0m : 3.02197

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.961, [92mTest[0m: 2.601, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 3.15092
[1mStep[0m  [8/84], [94mLoss[0m : 2.63722
[1mStep[0m  [16/84], [94mLoss[0m : 2.66082
[1mStep[0m  [24/84], [94mLoss[0m : 3.00566
[1mStep[0m  [32/84], [94mLoss[0m : 2.57781
[1mStep[0m  [40/84], [94mLoss[0m : 2.68505
[1mStep[0m  [48/84], [94mLoss[0m : 3.21427
[1mStep[0m  [56/84], [94mLoss[0m : 2.64161
[1mStep[0m  [64/84], [94mLoss[0m : 2.49504
[1mStep[0m  [72/84], [94mLoss[0m : 2.78025
[1mStep[0m  [80/84], [94mLoss[0m : 2.64151

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.762, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.77936
[1mStep[0m  [8/84], [94mLoss[0m : 2.67563
[1mStep[0m  [16/84], [94mLoss[0m : 2.72290
[1mStep[0m  [24/84], [94mLoss[0m : 2.73825
[1mStep[0m  [32/84], [94mLoss[0m : 2.67431
[1mStep[0m  [40/84], [94mLoss[0m : 2.84608
[1mStep[0m  [48/84], [94mLoss[0m : 2.52839
[1mStep[0m  [56/84], [94mLoss[0m : 3.07811
[1mStep[0m  [64/84], [94mLoss[0m : 2.51113
[1mStep[0m  [72/84], [94mLoss[0m : 2.63079
[1mStep[0m  [80/84], [94mLoss[0m : 2.93614

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42930
[1mStep[0m  [8/84], [94mLoss[0m : 2.47715
[1mStep[0m  [16/84], [94mLoss[0m : 2.74005
[1mStep[0m  [24/84], [94mLoss[0m : 2.64288
[1mStep[0m  [32/84], [94mLoss[0m : 2.92426
[1mStep[0m  [40/84], [94mLoss[0m : 2.50581
[1mStep[0m  [48/84], [94mLoss[0m : 2.83451
[1mStep[0m  [56/84], [94mLoss[0m : 2.58774
[1mStep[0m  [64/84], [94mLoss[0m : 2.68641
[1mStep[0m  [72/84], [94mLoss[0m : 2.46337
[1mStep[0m  [80/84], [94mLoss[0m : 2.74696

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.96014
[1mStep[0m  [8/84], [94mLoss[0m : 2.39738
[1mStep[0m  [16/84], [94mLoss[0m : 2.41818
[1mStep[0m  [24/84], [94mLoss[0m : 2.35630
[1mStep[0m  [32/84], [94mLoss[0m : 2.57779
[1mStep[0m  [40/84], [94mLoss[0m : 2.50553
[1mStep[0m  [48/84], [94mLoss[0m : 2.55433
[1mStep[0m  [56/84], [94mLoss[0m : 2.62159
[1mStep[0m  [64/84], [94mLoss[0m : 2.79379
[1mStep[0m  [72/84], [94mLoss[0m : 2.71459
[1mStep[0m  [80/84], [94mLoss[0m : 2.61845

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.79438
[1mStep[0m  [8/84], [94mLoss[0m : 2.29323
[1mStep[0m  [16/84], [94mLoss[0m : 2.68337
[1mStep[0m  [24/84], [94mLoss[0m : 2.35590
[1mStep[0m  [32/84], [94mLoss[0m : 2.28157
[1mStep[0m  [40/84], [94mLoss[0m : 2.46464
[1mStep[0m  [48/84], [94mLoss[0m : 2.79807
[1mStep[0m  [56/84], [94mLoss[0m : 2.81091
[1mStep[0m  [64/84], [94mLoss[0m : 2.56289
[1mStep[0m  [72/84], [94mLoss[0m : 2.36991
[1mStep[0m  [80/84], [94mLoss[0m : 2.58297

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.622, [92mTest[0m: 2.368, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41352
[1mStep[0m  [8/84], [94mLoss[0m : 2.64292
[1mStep[0m  [16/84], [94mLoss[0m : 2.62514
[1mStep[0m  [24/84], [94mLoss[0m : 2.54690
[1mStep[0m  [32/84], [94mLoss[0m : 2.96446
[1mStep[0m  [40/84], [94mLoss[0m : 2.61181
[1mStep[0m  [48/84], [94mLoss[0m : 2.50690
[1mStep[0m  [56/84], [94mLoss[0m : 2.60029
[1mStep[0m  [64/84], [94mLoss[0m : 3.06454
[1mStep[0m  [72/84], [94mLoss[0m : 2.73562
[1mStep[0m  [80/84], [94mLoss[0m : 2.72286

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.616, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52358
[1mStep[0m  [8/84], [94mLoss[0m : 2.42808
[1mStep[0m  [16/84], [94mLoss[0m : 2.51151
[1mStep[0m  [24/84], [94mLoss[0m : 2.55164
[1mStep[0m  [32/84], [94mLoss[0m : 2.51019
[1mStep[0m  [40/84], [94mLoss[0m : 2.42771
[1mStep[0m  [48/84], [94mLoss[0m : 2.53307
[1mStep[0m  [56/84], [94mLoss[0m : 2.48080
[1mStep[0m  [64/84], [94mLoss[0m : 2.55144
[1mStep[0m  [72/84], [94mLoss[0m : 2.66440
[1mStep[0m  [80/84], [94mLoss[0m : 2.43984

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.609, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.73907
[1mStep[0m  [8/84], [94mLoss[0m : 2.79040
[1mStep[0m  [16/84], [94mLoss[0m : 2.34170
[1mStep[0m  [24/84], [94mLoss[0m : 2.32908
[1mStep[0m  [32/84], [94mLoss[0m : 2.48443
[1mStep[0m  [40/84], [94mLoss[0m : 2.58752
[1mStep[0m  [48/84], [94mLoss[0m : 2.70139
[1mStep[0m  [56/84], [94mLoss[0m : 2.86079
[1mStep[0m  [64/84], [94mLoss[0m : 2.61958
[1mStep[0m  [72/84], [94mLoss[0m : 2.61892
[1mStep[0m  [80/84], [94mLoss[0m : 2.87921

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.47048
[1mStep[0m  [8/84], [94mLoss[0m : 2.49059
[1mStep[0m  [16/84], [94mLoss[0m : 2.58875
[1mStep[0m  [24/84], [94mLoss[0m : 2.48589
[1mStep[0m  [32/84], [94mLoss[0m : 2.58620
[1mStep[0m  [40/84], [94mLoss[0m : 2.64823
[1mStep[0m  [48/84], [94mLoss[0m : 2.16129
[1mStep[0m  [56/84], [94mLoss[0m : 2.67113
[1mStep[0m  [64/84], [94mLoss[0m : 2.39890
[1mStep[0m  [72/84], [94mLoss[0m : 2.76735
[1mStep[0m  [80/84], [94mLoss[0m : 2.55241

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.610, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.68870
[1mStep[0m  [8/84], [94mLoss[0m : 2.72337
[1mStep[0m  [16/84], [94mLoss[0m : 2.53999
[1mStep[0m  [24/84], [94mLoss[0m : 3.06271
[1mStep[0m  [32/84], [94mLoss[0m : 2.84364
[1mStep[0m  [40/84], [94mLoss[0m : 2.38248
[1mStep[0m  [48/84], [94mLoss[0m : 2.44324
[1mStep[0m  [56/84], [94mLoss[0m : 2.44978
[1mStep[0m  [64/84], [94mLoss[0m : 2.79046
[1mStep[0m  [72/84], [94mLoss[0m : 2.48194
[1mStep[0m  [80/84], [94mLoss[0m : 2.72063

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.602, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.62617
[1mStep[0m  [8/84], [94mLoss[0m : 2.50213
[1mStep[0m  [16/84], [94mLoss[0m : 2.27647
[1mStep[0m  [24/84], [94mLoss[0m : 2.67459
[1mStep[0m  [32/84], [94mLoss[0m : 2.77535
[1mStep[0m  [40/84], [94mLoss[0m : 2.60277
[1mStep[0m  [48/84], [94mLoss[0m : 2.46036
[1mStep[0m  [56/84], [94mLoss[0m : 2.55535
[1mStep[0m  [64/84], [94mLoss[0m : 2.50828
[1mStep[0m  [72/84], [94mLoss[0m : 2.42240
[1mStep[0m  [80/84], [94mLoss[0m : 2.85795

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.42831
[1mStep[0m  [8/84], [94mLoss[0m : 2.84242
[1mStep[0m  [16/84], [94mLoss[0m : 2.74798
[1mStep[0m  [24/84], [94mLoss[0m : 2.20659
[1mStep[0m  [32/84], [94mLoss[0m : 2.49467
[1mStep[0m  [40/84], [94mLoss[0m : 2.41559
[1mStep[0m  [48/84], [94mLoss[0m : 2.28336
[1mStep[0m  [56/84], [94mLoss[0m : 2.79890
[1mStep[0m  [64/84], [94mLoss[0m : 2.40215
[1mStep[0m  [72/84], [94mLoss[0m : 2.33402
[1mStep[0m  [80/84], [94mLoss[0m : 2.72231

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.361, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.52901
[1mStep[0m  [8/84], [94mLoss[0m : 2.49518
[1mStep[0m  [16/84], [94mLoss[0m : 2.60718
[1mStep[0m  [24/84], [94mLoss[0m : 2.63850
[1mStep[0m  [32/84], [94mLoss[0m : 3.03983
[1mStep[0m  [40/84], [94mLoss[0m : 2.85901
[1mStep[0m  [48/84], [94mLoss[0m : 2.65096
[1mStep[0m  [56/84], [94mLoss[0m : 2.43018
[1mStep[0m  [64/84], [94mLoss[0m : 2.51606
[1mStep[0m  [72/84], [94mLoss[0m : 2.75200
[1mStep[0m  [80/84], [94mLoss[0m : 2.37768

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.584, [92mTest[0m: 2.365, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43100
[1mStep[0m  [8/84], [94mLoss[0m : 2.37794
[1mStep[0m  [16/84], [94mLoss[0m : 2.64061
[1mStep[0m  [24/84], [94mLoss[0m : 2.93153
[1mStep[0m  [32/84], [94mLoss[0m : 2.60124
[1mStep[0m  [40/84], [94mLoss[0m : 2.46516
[1mStep[0m  [48/84], [94mLoss[0m : 2.44164
[1mStep[0m  [56/84], [94mLoss[0m : 2.56902
[1mStep[0m  [64/84], [94mLoss[0m : 2.67810
[1mStep[0m  [72/84], [94mLoss[0m : 2.65294
[1mStep[0m  [80/84], [94mLoss[0m : 2.35467

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.360, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.39609
[1mStep[0m  [8/84], [94mLoss[0m : 2.60391
[1mStep[0m  [16/84], [94mLoss[0m : 2.63948
[1mStep[0m  [24/84], [94mLoss[0m : 2.42846
[1mStep[0m  [32/84], [94mLoss[0m : 2.66601
[1mStep[0m  [40/84], [94mLoss[0m : 2.60452
[1mStep[0m  [48/84], [94mLoss[0m : 2.48126
[1mStep[0m  [56/84], [94mLoss[0m : 3.04666
[1mStep[0m  [64/84], [94mLoss[0m : 2.43354
[1mStep[0m  [72/84], [94mLoss[0m : 2.58720
[1mStep[0m  [80/84], [94mLoss[0m : 2.46331

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.362, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.45154
[1mStep[0m  [8/84], [94mLoss[0m : 2.57933
[1mStep[0m  [16/84], [94mLoss[0m : 2.78936
[1mStep[0m  [24/84], [94mLoss[0m : 2.58811
[1mStep[0m  [32/84], [94mLoss[0m : 2.49095
[1mStep[0m  [40/84], [94mLoss[0m : 2.85704
[1mStep[0m  [48/84], [94mLoss[0m : 2.42383
[1mStep[0m  [56/84], [94mLoss[0m : 2.58987
[1mStep[0m  [64/84], [94mLoss[0m : 2.48942
[1mStep[0m  [72/84], [94mLoss[0m : 2.44447
[1mStep[0m  [80/84], [94mLoss[0m : 2.68442

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.41555
[1mStep[0m  [8/84], [94mLoss[0m : 2.87920
[1mStep[0m  [16/84], [94mLoss[0m : 2.67145
[1mStep[0m  [24/84], [94mLoss[0m : 2.66779
[1mStep[0m  [32/84], [94mLoss[0m : 2.70226
[1mStep[0m  [40/84], [94mLoss[0m : 2.57720
[1mStep[0m  [48/84], [94mLoss[0m : 2.42680
[1mStep[0m  [56/84], [94mLoss[0m : 2.56288
[1mStep[0m  [64/84], [94mLoss[0m : 2.83643
[1mStep[0m  [72/84], [94mLoss[0m : 2.51183
[1mStep[0m  [80/84], [94mLoss[0m : 2.39671

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.345, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.69901
[1mStep[0m  [8/84], [94mLoss[0m : 2.50034
[1mStep[0m  [16/84], [94mLoss[0m : 2.54577
[1mStep[0m  [24/84], [94mLoss[0m : 2.43260
[1mStep[0m  [32/84], [94mLoss[0m : 2.46217
[1mStep[0m  [40/84], [94mLoss[0m : 2.60518
[1mStep[0m  [48/84], [94mLoss[0m : 2.52230
[1mStep[0m  [56/84], [94mLoss[0m : 2.92825
[1mStep[0m  [64/84], [94mLoss[0m : 2.46026
[1mStep[0m  [72/84], [94mLoss[0m : 2.58423
[1mStep[0m  [80/84], [94mLoss[0m : 2.67367

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.30058
[1mStep[0m  [8/84], [94mLoss[0m : 2.42635
[1mStep[0m  [16/84], [94mLoss[0m : 2.83703
[1mStep[0m  [24/84], [94mLoss[0m : 2.74005
[1mStep[0m  [32/84], [94mLoss[0m : 2.45935
[1mStep[0m  [40/84], [94mLoss[0m : 2.57810
[1mStep[0m  [48/84], [94mLoss[0m : 2.48303
[1mStep[0m  [56/84], [94mLoss[0m : 2.73879
[1mStep[0m  [64/84], [94mLoss[0m : 2.18823
[1mStep[0m  [72/84], [94mLoss[0m : 2.50857
[1mStep[0m  [80/84], [94mLoss[0m : 2.82994

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.43709
[1mStep[0m  [8/84], [94mLoss[0m : 2.71601
[1mStep[0m  [16/84], [94mLoss[0m : 2.43258
[1mStep[0m  [24/84], [94mLoss[0m : 2.70939
[1mStep[0m  [32/84], [94mLoss[0m : 2.47845
[1mStep[0m  [40/84], [94mLoss[0m : 2.09862
[1mStep[0m  [48/84], [94mLoss[0m : 2.38768
[1mStep[0m  [56/84], [94mLoss[0m : 2.77589
[1mStep[0m  [64/84], [94mLoss[0m : 2.57913
[1mStep[0m  [72/84], [94mLoss[0m : 2.64400
[1mStep[0m  [80/84], [94mLoss[0m : 2.51690

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.355, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.344
====================================

Phase 1 - Evaluation MAE:  2.344495428459985
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/84], [94mLoss[0m : 2.59038
[1mStep[0m  [8/84], [94mLoss[0m : 2.73582
[1mStep[0m  [16/84], [94mLoss[0m : 2.46913
[1mStep[0m  [24/84], [94mLoss[0m : 2.90806
[1mStep[0m  [32/84], [94mLoss[0m : 2.60291
[1mStep[0m  [40/84], [94mLoss[0m : 2.60577
[1mStep[0m  [48/84], [94mLoss[0m : 2.44999
[1mStep[0m  [56/84], [94mLoss[0m : 2.57375
[1mStep[0m  [64/84], [94mLoss[0m : 2.69008
[1mStep[0m  [72/84], [94mLoss[0m : 2.72846
[1mStep[0m  [80/84], [94mLoss[0m : 2.52759

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.334, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.84374
[1mStep[0m  [8/84], [94mLoss[0m : 2.67589
[1mStep[0m  [16/84], [94mLoss[0m : 2.44183
[1mStep[0m  [24/84], [94mLoss[0m : 2.15919
[1mStep[0m  [32/84], [94mLoss[0m : 2.52257
[1mStep[0m  [40/84], [94mLoss[0m : 2.57717
[1mStep[0m  [48/84], [94mLoss[0m : 2.59450
[1mStep[0m  [56/84], [94mLoss[0m : 2.63560
[1mStep[0m  [64/84], [94mLoss[0m : 2.55105
[1mStep[0m  [72/84], [94mLoss[0m : 2.53229
[1mStep[0m  [80/84], [94mLoss[0m : 2.31182

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.475, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.61306
[1mStep[0m  [8/84], [94mLoss[0m : 2.46145
[1mStep[0m  [16/84], [94mLoss[0m : 2.70188
[1mStep[0m  [24/84], [94mLoss[0m : 2.32648
[1mStep[0m  [32/84], [94mLoss[0m : 2.40127
[1mStep[0m  [40/84], [94mLoss[0m : 2.39313
[1mStep[0m  [48/84], [94mLoss[0m : 2.21671
[1mStep[0m  [56/84], [94mLoss[0m : 2.40212
[1mStep[0m  [64/84], [94mLoss[0m : 2.45358
[1mStep[0m  [72/84], [94mLoss[0m : 2.62916
[1mStep[0m  [80/84], [94mLoss[0m : 2.38195

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.460, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.65338
[1mStep[0m  [8/84], [94mLoss[0m : 2.25209
[1mStep[0m  [16/84], [94mLoss[0m : 2.53274
[1mStep[0m  [24/84], [94mLoss[0m : 2.56945
[1mStep[0m  [32/84], [94mLoss[0m : 2.54463
[1mStep[0m  [40/84], [94mLoss[0m : 2.46457
[1mStep[0m  [48/84], [94mLoss[0m : 2.19878
[1mStep[0m  [56/84], [94mLoss[0m : 2.23100
[1mStep[0m  [64/84], [94mLoss[0m : 2.32326
[1mStep[0m  [72/84], [94mLoss[0m : 2.50424
[1mStep[0m  [80/84], [94mLoss[0m : 2.41560

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.501, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.20571
[1mStep[0m  [8/84], [94mLoss[0m : 2.17994
[1mStep[0m  [16/84], [94mLoss[0m : 2.27084
[1mStep[0m  [24/84], [94mLoss[0m : 2.40472
[1mStep[0m  [32/84], [94mLoss[0m : 2.20330
[1mStep[0m  [40/84], [94mLoss[0m : 2.21119
[1mStep[0m  [48/84], [94mLoss[0m : 2.63937
[1mStep[0m  [56/84], [94mLoss[0m : 2.61846
[1mStep[0m  [64/84], [94mLoss[0m : 2.91959
[1mStep[0m  [72/84], [94mLoss[0m : 2.45370
[1mStep[0m  [80/84], [94mLoss[0m : 2.62718

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.461, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.54848
[1mStep[0m  [8/84], [94mLoss[0m : 2.07478
[1mStep[0m  [16/84], [94mLoss[0m : 2.31176
[1mStep[0m  [24/84], [94mLoss[0m : 2.72469
[1mStep[0m  [32/84], [94mLoss[0m : 2.41738
[1mStep[0m  [40/84], [94mLoss[0m : 2.30115
[1mStep[0m  [48/84], [94mLoss[0m : 2.49742
[1mStep[0m  [56/84], [94mLoss[0m : 2.47784
[1mStep[0m  [64/84], [94mLoss[0m : 2.27205
[1mStep[0m  [72/84], [94mLoss[0m : 2.13924
[1mStep[0m  [80/84], [94mLoss[0m : 2.44319

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.542, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.38672
[1mStep[0m  [8/84], [94mLoss[0m : 2.34908
[1mStep[0m  [16/84], [94mLoss[0m : 2.20603
[1mStep[0m  [24/84], [94mLoss[0m : 2.64071
[1mStep[0m  [32/84], [94mLoss[0m : 2.30982
[1mStep[0m  [40/84], [94mLoss[0m : 2.50538
[1mStep[0m  [48/84], [94mLoss[0m : 2.56736
[1mStep[0m  [56/84], [94mLoss[0m : 2.25601
[1mStep[0m  [64/84], [94mLoss[0m : 2.41568
[1mStep[0m  [72/84], [94mLoss[0m : 2.29590
[1mStep[0m  [80/84], [94mLoss[0m : 2.85563

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.512, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.35201
[1mStep[0m  [8/84], [94mLoss[0m : 2.54744
[1mStep[0m  [16/84], [94mLoss[0m : 2.30717
[1mStep[0m  [24/84], [94mLoss[0m : 2.54157
[1mStep[0m  [32/84], [94mLoss[0m : 2.10350
[1mStep[0m  [40/84], [94mLoss[0m : 2.43888
[1mStep[0m  [48/84], [94mLoss[0m : 1.86421
[1mStep[0m  [56/84], [94mLoss[0m : 2.42175
[1mStep[0m  [64/84], [94mLoss[0m : 1.99436
[1mStep[0m  [72/84], [94mLoss[0m : 2.46507
[1mStep[0m  [80/84], [94mLoss[0m : 2.83065

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.28465
[1mStep[0m  [8/84], [94mLoss[0m : 2.29375
[1mStep[0m  [16/84], [94mLoss[0m : 2.34681
[1mStep[0m  [24/84], [94mLoss[0m : 2.37081
[1mStep[0m  [32/84], [94mLoss[0m : 2.29949
[1mStep[0m  [40/84], [94mLoss[0m : 2.18194
[1mStep[0m  [48/84], [94mLoss[0m : 2.07430
[1mStep[0m  [56/84], [94mLoss[0m : 2.58002
[1mStep[0m  [64/84], [94mLoss[0m : 2.06796
[1mStep[0m  [72/84], [94mLoss[0m : 2.41571
[1mStep[0m  [80/84], [94mLoss[0m : 2.47936

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.289, [92mTest[0m: 2.526, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.27368
[1mStep[0m  [8/84], [94mLoss[0m : 2.32398
[1mStep[0m  [16/84], [94mLoss[0m : 2.18082
[1mStep[0m  [24/84], [94mLoss[0m : 2.35546
[1mStep[0m  [32/84], [94mLoss[0m : 1.95994
[1mStep[0m  [40/84], [94mLoss[0m : 2.10272
[1mStep[0m  [48/84], [94mLoss[0m : 2.24151
[1mStep[0m  [56/84], [94mLoss[0m : 2.23391
[1mStep[0m  [64/84], [94mLoss[0m : 2.10701
[1mStep[0m  [72/84], [94mLoss[0m : 2.27708
[1mStep[0m  [80/84], [94mLoss[0m : 2.23539

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.266, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.17380
[1mStep[0m  [8/84], [94mLoss[0m : 2.36511
[1mStep[0m  [16/84], [94mLoss[0m : 2.36180
[1mStep[0m  [24/84], [94mLoss[0m : 2.43092
[1mStep[0m  [32/84], [94mLoss[0m : 2.31285
[1mStep[0m  [40/84], [94mLoss[0m : 2.52746
[1mStep[0m  [48/84], [94mLoss[0m : 1.94207
[1mStep[0m  [56/84], [94mLoss[0m : 2.18212
[1mStep[0m  [64/84], [94mLoss[0m : 2.21760
[1mStep[0m  [72/84], [94mLoss[0m : 2.21207
[1mStep[0m  [80/84], [94mLoss[0m : 2.09256

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.225, [92mTest[0m: 2.534, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.26797
[1mStep[0m  [8/84], [94mLoss[0m : 2.35494
[1mStep[0m  [16/84], [94mLoss[0m : 2.16450
[1mStep[0m  [24/84], [94mLoss[0m : 2.20094
[1mStep[0m  [32/84], [94mLoss[0m : 2.09645
[1mStep[0m  [40/84], [94mLoss[0m : 2.08019
[1mStep[0m  [48/84], [94mLoss[0m : 1.93377
[1mStep[0m  [56/84], [94mLoss[0m : 2.32044
[1mStep[0m  [64/84], [94mLoss[0m : 2.26655
[1mStep[0m  [72/84], [94mLoss[0m : 2.65356
[1mStep[0m  [80/84], [94mLoss[0m : 1.98120

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.09748
[1mStep[0m  [8/84], [94mLoss[0m : 1.91325
[1mStep[0m  [16/84], [94mLoss[0m : 1.94440
[1mStep[0m  [24/84], [94mLoss[0m : 2.12684
[1mStep[0m  [32/84], [94mLoss[0m : 1.82601
[1mStep[0m  [40/84], [94mLoss[0m : 2.39159
[1mStep[0m  [48/84], [94mLoss[0m : 2.22293
[1mStep[0m  [56/84], [94mLoss[0m : 1.99736
[1mStep[0m  [64/84], [94mLoss[0m : 2.21159
[1mStep[0m  [72/84], [94mLoss[0m : 1.99052
[1mStep[0m  [80/84], [94mLoss[0m : 2.14360

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.153, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.06067
[1mStep[0m  [8/84], [94mLoss[0m : 1.99154
[1mStep[0m  [16/84], [94mLoss[0m : 2.10675
[1mStep[0m  [24/84], [94mLoss[0m : 1.93109
[1mStep[0m  [32/84], [94mLoss[0m : 2.02894
[1mStep[0m  [40/84], [94mLoss[0m : 2.08911
[1mStep[0m  [48/84], [94mLoss[0m : 2.15077
[1mStep[0m  [56/84], [94mLoss[0m : 2.07509
[1mStep[0m  [64/84], [94mLoss[0m : 2.16159
[1mStep[0m  [72/84], [94mLoss[0m : 2.12121
[1mStep[0m  [80/84], [94mLoss[0m : 1.98664

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.34019
[1mStep[0m  [8/84], [94mLoss[0m : 2.28522
[1mStep[0m  [16/84], [94mLoss[0m : 2.24021
[1mStep[0m  [24/84], [94mLoss[0m : 2.05370
[1mStep[0m  [32/84], [94mLoss[0m : 2.09619
[1mStep[0m  [40/84], [94mLoss[0m : 1.96947
[1mStep[0m  [48/84], [94mLoss[0m : 2.08432
[1mStep[0m  [56/84], [94mLoss[0m : 2.27479
[1mStep[0m  [64/84], [94mLoss[0m : 1.97184
[1mStep[0m  [72/84], [94mLoss[0m : 2.16738
[1mStep[0m  [80/84], [94mLoss[0m : 2.52962

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.82738
[1mStep[0m  [8/84], [94mLoss[0m : 2.08256
[1mStep[0m  [16/84], [94mLoss[0m : 1.94840
[1mStep[0m  [24/84], [94mLoss[0m : 2.05504
[1mStep[0m  [32/84], [94mLoss[0m : 2.09737
[1mStep[0m  [40/84], [94mLoss[0m : 2.00520
[1mStep[0m  [48/84], [94mLoss[0m : 1.59191
[1mStep[0m  [56/84], [94mLoss[0m : 2.20118
[1mStep[0m  [64/84], [94mLoss[0m : 2.04482
[1mStep[0m  [72/84], [94mLoss[0m : 1.86035
[1mStep[0m  [80/84], [94mLoss[0m : 2.11261

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.541, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.87121
[1mStep[0m  [8/84], [94mLoss[0m : 1.88927
[1mStep[0m  [16/84], [94mLoss[0m : 1.92228
[1mStep[0m  [24/84], [94mLoss[0m : 2.01260
[1mStep[0m  [32/84], [94mLoss[0m : 2.12457
[1mStep[0m  [40/84], [94mLoss[0m : 2.18573
[1mStep[0m  [48/84], [94mLoss[0m : 1.81041
[1mStep[0m  [56/84], [94mLoss[0m : 1.91035
[1mStep[0m  [64/84], [94mLoss[0m : 2.10625
[1mStep[0m  [72/84], [94mLoss[0m : 2.32543
[1mStep[0m  [80/84], [94mLoss[0m : 1.91287

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.90134
[1mStep[0m  [8/84], [94mLoss[0m : 1.92863
[1mStep[0m  [16/84], [94mLoss[0m : 1.88788
[1mStep[0m  [24/84], [94mLoss[0m : 1.96699
[1mStep[0m  [32/84], [94mLoss[0m : 1.82329
[1mStep[0m  [40/84], [94mLoss[0m : 2.17423
[1mStep[0m  [48/84], [94mLoss[0m : 1.97642
[1mStep[0m  [56/84], [94mLoss[0m : 1.84816
[1mStep[0m  [64/84], [94mLoss[0m : 2.21317
[1mStep[0m  [72/84], [94mLoss[0m : 2.19191
[1mStep[0m  [80/84], [94mLoss[0m : 1.90914

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.543, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 2.13666
[1mStep[0m  [8/84], [94mLoss[0m : 2.12873
[1mStep[0m  [16/84], [94mLoss[0m : 2.36805
[1mStep[0m  [24/84], [94mLoss[0m : 2.16006
[1mStep[0m  [32/84], [94mLoss[0m : 2.09105
[1mStep[0m  [40/84], [94mLoss[0m : 1.99022
[1mStep[0m  [48/84], [94mLoss[0m : 2.12491
[1mStep[0m  [56/84], [94mLoss[0m : 2.19445
[1mStep[0m  [64/84], [94mLoss[0m : 1.90284
[1mStep[0m  [72/84], [94mLoss[0m : 1.85572
[1mStep[0m  [80/84], [94mLoss[0m : 1.83432

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92686
[1mStep[0m  [8/84], [94mLoss[0m : 1.70500
[1mStep[0m  [16/84], [94mLoss[0m : 1.94311
[1mStep[0m  [24/84], [94mLoss[0m : 1.89429
[1mStep[0m  [32/84], [94mLoss[0m : 1.91943
[1mStep[0m  [40/84], [94mLoss[0m : 2.03222
[1mStep[0m  [48/84], [94mLoss[0m : 1.98566
[1mStep[0m  [56/84], [94mLoss[0m : 1.95661
[1mStep[0m  [64/84], [94mLoss[0m : 1.78593
[1mStep[0m  [72/84], [94mLoss[0m : 1.79318
[1mStep[0m  [80/84], [94mLoss[0m : 2.20818

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.486, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.88082
[1mStep[0m  [8/84], [94mLoss[0m : 1.97447
[1mStep[0m  [16/84], [94mLoss[0m : 1.78995
[1mStep[0m  [24/84], [94mLoss[0m : 2.00649
[1mStep[0m  [32/84], [94mLoss[0m : 1.76944
[1mStep[0m  [40/84], [94mLoss[0m : 1.52530
[1mStep[0m  [48/84], [94mLoss[0m : 1.82595
[1mStep[0m  [56/84], [94mLoss[0m : 1.59573
[1mStep[0m  [64/84], [94mLoss[0m : 1.94752
[1mStep[0m  [72/84], [94mLoss[0m : 1.80536
[1mStep[0m  [80/84], [94mLoss[0m : 1.93123

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.889, [92mTest[0m: 2.584, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.92879
[1mStep[0m  [8/84], [94mLoss[0m : 1.86694
[1mStep[0m  [16/84], [94mLoss[0m : 1.78081
[1mStep[0m  [24/84], [94mLoss[0m : 1.77996
[1mStep[0m  [32/84], [94mLoss[0m : 2.00482
[1mStep[0m  [40/84], [94mLoss[0m : 1.96886
[1mStep[0m  [48/84], [94mLoss[0m : 2.00995
[1mStep[0m  [56/84], [94mLoss[0m : 1.75938
[1mStep[0m  [64/84], [94mLoss[0m : 1.60514
[1mStep[0m  [72/84], [94mLoss[0m : 1.75271
[1mStep[0m  [80/84], [94mLoss[0m : 1.88058

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.574, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.78006
[1mStep[0m  [8/84], [94mLoss[0m : 1.97224
[1mStep[0m  [16/84], [94mLoss[0m : 1.94031
[1mStep[0m  [24/84], [94mLoss[0m : 1.77989
[1mStep[0m  [32/84], [94mLoss[0m : 1.98870
[1mStep[0m  [40/84], [94mLoss[0m : 1.87478
[1mStep[0m  [48/84], [94mLoss[0m : 1.54840
[1mStep[0m  [56/84], [94mLoss[0m : 1.86373
[1mStep[0m  [64/84], [94mLoss[0m : 1.86756
[1mStep[0m  [72/84], [94mLoss[0m : 1.68429
[1mStep[0m  [80/84], [94mLoss[0m : 1.64801

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.537, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.80668
[1mStep[0m  [8/84], [94mLoss[0m : 1.57226
[1mStep[0m  [16/84], [94mLoss[0m : 1.75800
[1mStep[0m  [24/84], [94mLoss[0m : 1.77003
[1mStep[0m  [32/84], [94mLoss[0m : 1.87845
[1mStep[0m  [40/84], [94mLoss[0m : 1.75616
[1mStep[0m  [48/84], [94mLoss[0m : 1.78388
[1mStep[0m  [56/84], [94mLoss[0m : 1.68101
[1mStep[0m  [64/84], [94mLoss[0m : 1.75679
[1mStep[0m  [72/84], [94mLoss[0m : 1.81333
[1mStep[0m  [80/84], [94mLoss[0m : 1.71081

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.542, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.81864
[1mStep[0m  [8/84], [94mLoss[0m : 1.73109
[1mStep[0m  [16/84], [94mLoss[0m : 1.88854
[1mStep[0m  [24/84], [94mLoss[0m : 2.03846
[1mStep[0m  [32/84], [94mLoss[0m : 1.83030
[1mStep[0m  [40/84], [94mLoss[0m : 1.70451
[1mStep[0m  [48/84], [94mLoss[0m : 1.65268
[1mStep[0m  [56/84], [94mLoss[0m : 1.54280
[1mStep[0m  [64/84], [94mLoss[0m : 1.57845
[1mStep[0m  [72/84], [94mLoss[0m : 1.67171
[1mStep[0m  [80/84], [94mLoss[0m : 1.81562

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.73026
[1mStep[0m  [8/84], [94mLoss[0m : 1.71953
[1mStep[0m  [16/84], [94mLoss[0m : 1.71335
[1mStep[0m  [24/84], [94mLoss[0m : 1.72906
[1mStep[0m  [32/84], [94mLoss[0m : 1.95029
[1mStep[0m  [40/84], [94mLoss[0m : 2.09802
[1mStep[0m  [48/84], [94mLoss[0m : 1.68312
[1mStep[0m  [56/84], [94mLoss[0m : 1.81405
[1mStep[0m  [64/84], [94mLoss[0m : 1.73279
[1mStep[0m  [72/84], [94mLoss[0m : 1.81023
[1mStep[0m  [80/84], [94mLoss[0m : 1.83982

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.502, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.69563
[1mStep[0m  [8/84], [94mLoss[0m : 1.79365
[1mStep[0m  [16/84], [94mLoss[0m : 1.82044
[1mStep[0m  [24/84], [94mLoss[0m : 1.65051
[1mStep[0m  [32/84], [94mLoss[0m : 1.74641
[1mStep[0m  [40/84], [94mLoss[0m : 1.63740
[1mStep[0m  [48/84], [94mLoss[0m : 1.66926
[1mStep[0m  [56/84], [94mLoss[0m : 1.97971
[1mStep[0m  [64/84], [94mLoss[0m : 1.80703
[1mStep[0m  [72/84], [94mLoss[0m : 1.72143
[1mStep[0m  [80/84], [94mLoss[0m : 1.79910

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.739, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.93026
[1mStep[0m  [8/84], [94mLoss[0m : 1.54355
[1mStep[0m  [16/84], [94mLoss[0m : 1.74283
[1mStep[0m  [24/84], [94mLoss[0m : 1.61535
[1mStep[0m  [32/84], [94mLoss[0m : 1.80407
[1mStep[0m  [40/84], [94mLoss[0m : 1.76621
[1mStep[0m  [48/84], [94mLoss[0m : 1.85380
[1mStep[0m  [56/84], [94mLoss[0m : 1.64966
[1mStep[0m  [64/84], [94mLoss[0m : 1.79713
[1mStep[0m  [72/84], [94mLoss[0m : 1.68908
[1mStep[0m  [80/84], [94mLoss[0m : 1.72214

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.59557
[1mStep[0m  [8/84], [94mLoss[0m : 1.80330
[1mStep[0m  [16/84], [94mLoss[0m : 1.72076
[1mStep[0m  [24/84], [94mLoss[0m : 1.58432
[1mStep[0m  [32/84], [94mLoss[0m : 1.43717
[1mStep[0m  [40/84], [94mLoss[0m : 1.60606
[1mStep[0m  [48/84], [94mLoss[0m : 1.86409
[1mStep[0m  [56/84], [94mLoss[0m : 1.95660
[1mStep[0m  [64/84], [94mLoss[0m : 1.61617
[1mStep[0m  [72/84], [94mLoss[0m : 1.97961
[1mStep[0m  [80/84], [94mLoss[0m : 1.65810

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.713, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/84], [94mLoss[0m : 1.61088
[1mStep[0m  [8/84], [94mLoss[0m : 1.46825
[1mStep[0m  [16/84], [94mLoss[0m : 1.52236
[1mStep[0m  [24/84], [94mLoss[0m : 1.58956
[1mStep[0m  [32/84], [94mLoss[0m : 1.84276
[1mStep[0m  [40/84], [94mLoss[0m : 1.51180
[1mStep[0m  [48/84], [94mLoss[0m : 1.93725
[1mStep[0m  [56/84], [94mLoss[0m : 1.80401
[1mStep[0m  [64/84], [94mLoss[0m : 1.83226
[1mStep[0m  [72/84], [94mLoss[0m : 1.73625
[1mStep[0m  [80/84], [94mLoss[0m : 1.85933

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.679, [92mTest[0m: 2.531, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.51587621654783
MAE score P1      2.344495
MAE score P2      2.515876
loss              1.679002
learning_rate     0.002575
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.66719
[1mStep[0m  [16/169], [94mLoss[0m : 9.26485
[1mStep[0m  [32/169], [94mLoss[0m : 8.53341
[1mStep[0m  [48/169], [94mLoss[0m : 7.80904
[1mStep[0m  [64/169], [94mLoss[0m : 6.66772
[1mStep[0m  [80/169], [94mLoss[0m : 4.42971
[1mStep[0m  [96/169], [94mLoss[0m : 4.10411
[1mStep[0m  [112/169], [94mLoss[0m : 4.14097
[1mStep[0m  [128/169], [94mLoss[0m : 2.79968
[1mStep[0m  [144/169], [94mLoss[0m : 3.47307
[1mStep[0m  [160/169], [94mLoss[0m : 2.39254

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.799, [92mTest[0m: 10.780, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81280
[1mStep[0m  [16/169], [94mLoss[0m : 2.56669
[1mStep[0m  [32/169], [94mLoss[0m : 2.68124
[1mStep[0m  [48/169], [94mLoss[0m : 2.71317
[1mStep[0m  [64/169], [94mLoss[0m : 2.58226
[1mStep[0m  [80/169], [94mLoss[0m : 3.58772
[1mStep[0m  [96/169], [94mLoss[0m : 2.49326
[1mStep[0m  [112/169], [94mLoss[0m : 3.06406
[1mStep[0m  [128/169], [94mLoss[0m : 2.44481
[1mStep[0m  [144/169], [94mLoss[0m : 2.54251
[1mStep[0m  [160/169], [94mLoss[0m : 2.59788

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.702, [92mTest[0m: 2.728, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44590
[1mStep[0m  [16/169], [94mLoss[0m : 2.98815
[1mStep[0m  [32/169], [94mLoss[0m : 2.44104
[1mStep[0m  [48/169], [94mLoss[0m : 2.76409
[1mStep[0m  [64/169], [94mLoss[0m : 2.35663
[1mStep[0m  [80/169], [94mLoss[0m : 2.63301
[1mStep[0m  [96/169], [94mLoss[0m : 2.64783
[1mStep[0m  [112/169], [94mLoss[0m : 3.19278
[1mStep[0m  [128/169], [94mLoss[0m : 2.36249
[1mStep[0m  [144/169], [94mLoss[0m : 2.71652
[1mStep[0m  [160/169], [94mLoss[0m : 2.91284

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.626, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54856
[1mStep[0m  [16/169], [94mLoss[0m : 2.67719
[1mStep[0m  [32/169], [94mLoss[0m : 2.85358
[1mStep[0m  [48/169], [94mLoss[0m : 2.92960
[1mStep[0m  [64/169], [94mLoss[0m : 2.53252
[1mStep[0m  [80/169], [94mLoss[0m : 2.32894
[1mStep[0m  [96/169], [94mLoss[0m : 2.89251
[1mStep[0m  [112/169], [94mLoss[0m : 2.59906
[1mStep[0m  [128/169], [94mLoss[0m : 2.66304
[1mStep[0m  [144/169], [94mLoss[0m : 2.89017
[1mStep[0m  [160/169], [94mLoss[0m : 2.88562

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67825
[1mStep[0m  [16/169], [94mLoss[0m : 2.65102
[1mStep[0m  [32/169], [94mLoss[0m : 2.82077
[1mStep[0m  [48/169], [94mLoss[0m : 2.39009
[1mStep[0m  [64/169], [94mLoss[0m : 2.23339
[1mStep[0m  [80/169], [94mLoss[0m : 2.06299
[1mStep[0m  [96/169], [94mLoss[0m : 2.89044
[1mStep[0m  [112/169], [94mLoss[0m : 2.55316
[1mStep[0m  [128/169], [94mLoss[0m : 2.57785
[1mStep[0m  [144/169], [94mLoss[0m : 2.53719
[1mStep[0m  [160/169], [94mLoss[0m : 2.72727

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.04983
[1mStep[0m  [16/169], [94mLoss[0m : 2.58355
[1mStep[0m  [32/169], [94mLoss[0m : 2.48039
[1mStep[0m  [48/169], [94mLoss[0m : 2.93207
[1mStep[0m  [64/169], [94mLoss[0m : 2.35952
[1mStep[0m  [80/169], [94mLoss[0m : 2.33159
[1mStep[0m  [96/169], [94mLoss[0m : 2.91886
[1mStep[0m  [112/169], [94mLoss[0m : 2.79242
[1mStep[0m  [128/169], [94mLoss[0m : 2.55002
[1mStep[0m  [144/169], [94mLoss[0m : 2.30377
[1mStep[0m  [160/169], [94mLoss[0m : 2.29475

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78481
[1mStep[0m  [16/169], [94mLoss[0m : 2.55844
[1mStep[0m  [32/169], [94mLoss[0m : 2.56933
[1mStep[0m  [48/169], [94mLoss[0m : 2.42444
[1mStep[0m  [64/169], [94mLoss[0m : 2.30402
[1mStep[0m  [80/169], [94mLoss[0m : 2.78184
[1mStep[0m  [96/169], [94mLoss[0m : 1.97615
[1mStep[0m  [112/169], [94mLoss[0m : 2.47611
[1mStep[0m  [128/169], [94mLoss[0m : 3.04766
[1mStep[0m  [144/169], [94mLoss[0m : 2.39691
[1mStep[0m  [160/169], [94mLoss[0m : 2.88368

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36098
[1mStep[0m  [16/169], [94mLoss[0m : 2.59709
[1mStep[0m  [32/169], [94mLoss[0m : 3.13726
[1mStep[0m  [48/169], [94mLoss[0m : 2.17400
[1mStep[0m  [64/169], [94mLoss[0m : 2.43791
[1mStep[0m  [80/169], [94mLoss[0m : 2.42525
[1mStep[0m  [96/169], [94mLoss[0m : 2.76346
[1mStep[0m  [112/169], [94mLoss[0m : 2.47251
[1mStep[0m  [128/169], [94mLoss[0m : 2.31191
[1mStep[0m  [144/169], [94mLoss[0m : 3.16812
[1mStep[0m  [160/169], [94mLoss[0m : 2.52849

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62003
[1mStep[0m  [16/169], [94mLoss[0m : 2.17023
[1mStep[0m  [32/169], [94mLoss[0m : 2.50687
[1mStep[0m  [48/169], [94mLoss[0m : 2.05129
[1mStep[0m  [64/169], [94mLoss[0m : 2.55204
[1mStep[0m  [80/169], [94mLoss[0m : 2.62511
[1mStep[0m  [96/169], [94mLoss[0m : 2.38312
[1mStep[0m  [112/169], [94mLoss[0m : 2.52687
[1mStep[0m  [128/169], [94mLoss[0m : 2.30087
[1mStep[0m  [144/169], [94mLoss[0m : 2.47695
[1mStep[0m  [160/169], [94mLoss[0m : 2.49767

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.89073
[1mStep[0m  [16/169], [94mLoss[0m : 2.72425
[1mStep[0m  [32/169], [94mLoss[0m : 2.31952
[1mStep[0m  [48/169], [94mLoss[0m : 2.54764
[1mStep[0m  [64/169], [94mLoss[0m : 2.62011
[1mStep[0m  [80/169], [94mLoss[0m : 2.32330
[1mStep[0m  [96/169], [94mLoss[0m : 2.36411
[1mStep[0m  [112/169], [94mLoss[0m : 2.34318
[1mStep[0m  [128/169], [94mLoss[0m : 2.44640
[1mStep[0m  [144/169], [94mLoss[0m : 2.35805
[1mStep[0m  [160/169], [94mLoss[0m : 2.52680

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.344, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68211
[1mStep[0m  [16/169], [94mLoss[0m : 2.46621
[1mStep[0m  [32/169], [94mLoss[0m : 2.50583
[1mStep[0m  [48/169], [94mLoss[0m : 2.27532
[1mStep[0m  [64/169], [94mLoss[0m : 2.92828
[1mStep[0m  [80/169], [94mLoss[0m : 2.54565
[1mStep[0m  [96/169], [94mLoss[0m : 2.24928
[1mStep[0m  [112/169], [94mLoss[0m : 2.47205
[1mStep[0m  [128/169], [94mLoss[0m : 3.00379
[1mStep[0m  [144/169], [94mLoss[0m : 2.96395
[1mStep[0m  [160/169], [94mLoss[0m : 2.04109

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65669
[1mStep[0m  [16/169], [94mLoss[0m : 1.88638
[1mStep[0m  [32/169], [94mLoss[0m : 2.29986
[1mStep[0m  [48/169], [94mLoss[0m : 2.77178
[1mStep[0m  [64/169], [94mLoss[0m : 2.39469
[1mStep[0m  [80/169], [94mLoss[0m : 2.69966
[1mStep[0m  [96/169], [94mLoss[0m : 3.09057
[1mStep[0m  [112/169], [94mLoss[0m : 2.24682
[1mStep[0m  [128/169], [94mLoss[0m : 2.74405
[1mStep[0m  [144/169], [94mLoss[0m : 2.19385
[1mStep[0m  [160/169], [94mLoss[0m : 2.52298

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55333
[1mStep[0m  [16/169], [94mLoss[0m : 2.42128
[1mStep[0m  [32/169], [94mLoss[0m : 2.63601
[1mStep[0m  [48/169], [94mLoss[0m : 2.27089
[1mStep[0m  [64/169], [94mLoss[0m : 2.64911
[1mStep[0m  [80/169], [94mLoss[0m : 2.39202
[1mStep[0m  [96/169], [94mLoss[0m : 2.80256
[1mStep[0m  [112/169], [94mLoss[0m : 2.55715
[1mStep[0m  [128/169], [94mLoss[0m : 2.42657
[1mStep[0m  [144/169], [94mLoss[0m : 2.36361
[1mStep[0m  [160/169], [94mLoss[0m : 2.42397

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56743
[1mStep[0m  [16/169], [94mLoss[0m : 2.74250
[1mStep[0m  [32/169], [94mLoss[0m : 2.84556
[1mStep[0m  [48/169], [94mLoss[0m : 2.14816
[1mStep[0m  [64/169], [94mLoss[0m : 2.41139
[1mStep[0m  [80/169], [94mLoss[0m : 2.43563
[1mStep[0m  [96/169], [94mLoss[0m : 2.94611
[1mStep[0m  [112/169], [94mLoss[0m : 2.73112
[1mStep[0m  [128/169], [94mLoss[0m : 2.59746
[1mStep[0m  [144/169], [94mLoss[0m : 2.40469
[1mStep[0m  [160/169], [94mLoss[0m : 2.61140

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36102
[1mStep[0m  [16/169], [94mLoss[0m : 2.10303
[1mStep[0m  [32/169], [94mLoss[0m : 2.27858
[1mStep[0m  [48/169], [94mLoss[0m : 2.34693
[1mStep[0m  [64/169], [94mLoss[0m : 2.59434
[1mStep[0m  [80/169], [94mLoss[0m : 2.62869
[1mStep[0m  [96/169], [94mLoss[0m : 2.48983
[1mStep[0m  [112/169], [94mLoss[0m : 2.30249
[1mStep[0m  [128/169], [94mLoss[0m : 2.62877
[1mStep[0m  [144/169], [94mLoss[0m : 2.53603
[1mStep[0m  [160/169], [94mLoss[0m : 2.63942

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.561, [92mTest[0m: 2.341, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30592
[1mStep[0m  [16/169], [94mLoss[0m : 2.26501
[1mStep[0m  [32/169], [94mLoss[0m : 2.68464
[1mStep[0m  [48/169], [94mLoss[0m : 2.75547
[1mStep[0m  [64/169], [94mLoss[0m : 2.29041
[1mStep[0m  [80/169], [94mLoss[0m : 2.63403
[1mStep[0m  [96/169], [94mLoss[0m : 2.57079
[1mStep[0m  [112/169], [94mLoss[0m : 2.29337
[1mStep[0m  [128/169], [94mLoss[0m : 2.64934
[1mStep[0m  [144/169], [94mLoss[0m : 2.45118
[1mStep[0m  [160/169], [94mLoss[0m : 3.06944

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46885
[1mStep[0m  [16/169], [94mLoss[0m : 2.44212
[1mStep[0m  [32/169], [94mLoss[0m : 2.77775
[1mStep[0m  [48/169], [94mLoss[0m : 2.65977
[1mStep[0m  [64/169], [94mLoss[0m : 2.58448
[1mStep[0m  [80/169], [94mLoss[0m : 2.27537
[1mStep[0m  [96/169], [94mLoss[0m : 2.77198
[1mStep[0m  [112/169], [94mLoss[0m : 2.87342
[1mStep[0m  [128/169], [94mLoss[0m : 2.71515
[1mStep[0m  [144/169], [94mLoss[0m : 2.92261
[1mStep[0m  [160/169], [94mLoss[0m : 2.45832

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58211
[1mStep[0m  [16/169], [94mLoss[0m : 2.45565
[1mStep[0m  [32/169], [94mLoss[0m : 2.79184
[1mStep[0m  [48/169], [94mLoss[0m : 2.62451
[1mStep[0m  [64/169], [94mLoss[0m : 2.44058
[1mStep[0m  [80/169], [94mLoss[0m : 2.71230
[1mStep[0m  [96/169], [94mLoss[0m : 2.60598
[1mStep[0m  [112/169], [94mLoss[0m : 2.89322
[1mStep[0m  [128/169], [94mLoss[0m : 2.89918
[1mStep[0m  [144/169], [94mLoss[0m : 2.62727
[1mStep[0m  [160/169], [94mLoss[0m : 2.47208

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28614
[1mStep[0m  [16/169], [94mLoss[0m : 2.76591
[1mStep[0m  [32/169], [94mLoss[0m : 2.52722
[1mStep[0m  [48/169], [94mLoss[0m : 2.89713
[1mStep[0m  [64/169], [94mLoss[0m : 2.60526
[1mStep[0m  [80/169], [94mLoss[0m : 2.43540
[1mStep[0m  [96/169], [94mLoss[0m : 2.87209
[1mStep[0m  [112/169], [94mLoss[0m : 2.73109
[1mStep[0m  [128/169], [94mLoss[0m : 2.70301
[1mStep[0m  [144/169], [94mLoss[0m : 3.07730
[1mStep[0m  [160/169], [94mLoss[0m : 2.19010

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09037
[1mStep[0m  [16/169], [94mLoss[0m : 3.14287
[1mStep[0m  [32/169], [94mLoss[0m : 2.16493
[1mStep[0m  [48/169], [94mLoss[0m : 2.23646
[1mStep[0m  [64/169], [94mLoss[0m : 2.90905
[1mStep[0m  [80/169], [94mLoss[0m : 2.53780
[1mStep[0m  [96/169], [94mLoss[0m : 2.51952
[1mStep[0m  [112/169], [94mLoss[0m : 2.42762
[1mStep[0m  [128/169], [94mLoss[0m : 2.94293
[1mStep[0m  [144/169], [94mLoss[0m : 2.56003
[1mStep[0m  [160/169], [94mLoss[0m : 2.83537

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45398
[1mStep[0m  [16/169], [94mLoss[0m : 2.61917
[1mStep[0m  [32/169], [94mLoss[0m : 2.55905
[1mStep[0m  [48/169], [94mLoss[0m : 2.36614
[1mStep[0m  [64/169], [94mLoss[0m : 2.52248
[1mStep[0m  [80/169], [94mLoss[0m : 2.61448
[1mStep[0m  [96/169], [94mLoss[0m : 2.83841
[1mStep[0m  [112/169], [94mLoss[0m : 2.44131
[1mStep[0m  [128/169], [94mLoss[0m : 2.42903
[1mStep[0m  [144/169], [94mLoss[0m : 2.08112
[1mStep[0m  [160/169], [94mLoss[0m : 2.29486

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25088
[1mStep[0m  [16/169], [94mLoss[0m : 2.60651
[1mStep[0m  [32/169], [94mLoss[0m : 2.79802
[1mStep[0m  [48/169], [94mLoss[0m : 2.65273
[1mStep[0m  [64/169], [94mLoss[0m : 2.32692
[1mStep[0m  [80/169], [94mLoss[0m : 2.23805
[1mStep[0m  [96/169], [94mLoss[0m : 2.50480
[1mStep[0m  [112/169], [94mLoss[0m : 2.88431
[1mStep[0m  [128/169], [94mLoss[0m : 2.80090
[1mStep[0m  [144/169], [94mLoss[0m : 3.18898
[1mStep[0m  [160/169], [94mLoss[0m : 2.96916

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68233
[1mStep[0m  [16/169], [94mLoss[0m : 2.68788
[1mStep[0m  [32/169], [94mLoss[0m : 2.21666
[1mStep[0m  [48/169], [94mLoss[0m : 2.54456
[1mStep[0m  [64/169], [94mLoss[0m : 2.41957
[1mStep[0m  [80/169], [94mLoss[0m : 2.81994
[1mStep[0m  [96/169], [94mLoss[0m : 2.77327
[1mStep[0m  [112/169], [94mLoss[0m : 2.83780
[1mStep[0m  [128/169], [94mLoss[0m : 2.69149
[1mStep[0m  [144/169], [94mLoss[0m : 2.28020
[1mStep[0m  [160/169], [94mLoss[0m : 2.63225

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82481
[1mStep[0m  [16/169], [94mLoss[0m : 2.44833
[1mStep[0m  [32/169], [94mLoss[0m : 2.95159
[1mStep[0m  [48/169], [94mLoss[0m : 3.00353
[1mStep[0m  [64/169], [94mLoss[0m : 2.61780
[1mStep[0m  [80/169], [94mLoss[0m : 2.38358
[1mStep[0m  [96/169], [94mLoss[0m : 2.05528
[1mStep[0m  [112/169], [94mLoss[0m : 2.43251
[1mStep[0m  [128/169], [94mLoss[0m : 2.79386
[1mStep[0m  [144/169], [94mLoss[0m : 2.17973
[1mStep[0m  [160/169], [94mLoss[0m : 2.15852

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77052
[1mStep[0m  [16/169], [94mLoss[0m : 2.60548
[1mStep[0m  [32/169], [94mLoss[0m : 2.73640
[1mStep[0m  [48/169], [94mLoss[0m : 2.64771
[1mStep[0m  [64/169], [94mLoss[0m : 2.13342
[1mStep[0m  [80/169], [94mLoss[0m : 2.50838
[1mStep[0m  [96/169], [94mLoss[0m : 2.69784
[1mStep[0m  [112/169], [94mLoss[0m : 2.23846
[1mStep[0m  [128/169], [94mLoss[0m : 2.90566
[1mStep[0m  [144/169], [94mLoss[0m : 2.56405
[1mStep[0m  [160/169], [94mLoss[0m : 2.69752

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73393
[1mStep[0m  [16/169], [94mLoss[0m : 2.69566
[1mStep[0m  [32/169], [94mLoss[0m : 2.57414
[1mStep[0m  [48/169], [94mLoss[0m : 1.92441
[1mStep[0m  [64/169], [94mLoss[0m : 2.59386
[1mStep[0m  [80/169], [94mLoss[0m : 2.15217
[1mStep[0m  [96/169], [94mLoss[0m : 2.56224
[1mStep[0m  [112/169], [94mLoss[0m : 2.41742
[1mStep[0m  [128/169], [94mLoss[0m : 2.78364
[1mStep[0m  [144/169], [94mLoss[0m : 2.46658
[1mStep[0m  [160/169], [94mLoss[0m : 2.55999

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47734
[1mStep[0m  [16/169], [94mLoss[0m : 2.59149
[1mStep[0m  [32/169], [94mLoss[0m : 2.62248
[1mStep[0m  [48/169], [94mLoss[0m : 2.83476
[1mStep[0m  [64/169], [94mLoss[0m : 2.70603
[1mStep[0m  [80/169], [94mLoss[0m : 2.28291
[1mStep[0m  [96/169], [94mLoss[0m : 2.35235
[1mStep[0m  [112/169], [94mLoss[0m : 2.68243
[1mStep[0m  [128/169], [94mLoss[0m : 2.74996
[1mStep[0m  [144/169], [94mLoss[0m : 2.18462
[1mStep[0m  [160/169], [94mLoss[0m : 1.93555

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66724
[1mStep[0m  [16/169], [94mLoss[0m : 2.35077
[1mStep[0m  [32/169], [94mLoss[0m : 2.56118
[1mStep[0m  [48/169], [94mLoss[0m : 2.48255
[1mStep[0m  [64/169], [94mLoss[0m : 2.97764
[1mStep[0m  [80/169], [94mLoss[0m : 2.30139
[1mStep[0m  [96/169], [94mLoss[0m : 2.18628
[1mStep[0m  [112/169], [94mLoss[0m : 2.78963
[1mStep[0m  [128/169], [94mLoss[0m : 2.16365
[1mStep[0m  [144/169], [94mLoss[0m : 2.18295
[1mStep[0m  [160/169], [94mLoss[0m : 2.86947

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44593
[1mStep[0m  [16/169], [94mLoss[0m : 2.43198
[1mStep[0m  [32/169], [94mLoss[0m : 2.45687
[1mStep[0m  [48/169], [94mLoss[0m : 2.37188
[1mStep[0m  [64/169], [94mLoss[0m : 2.23227
[1mStep[0m  [80/169], [94mLoss[0m : 2.49588
[1mStep[0m  [96/169], [94mLoss[0m : 2.39605
[1mStep[0m  [112/169], [94mLoss[0m : 2.88531
[1mStep[0m  [128/169], [94mLoss[0m : 2.87758
[1mStep[0m  [144/169], [94mLoss[0m : 2.89837
[1mStep[0m  [160/169], [94mLoss[0m : 2.73049

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84163
[1mStep[0m  [16/169], [94mLoss[0m : 2.26648
[1mStep[0m  [32/169], [94mLoss[0m : 2.66312
[1mStep[0m  [48/169], [94mLoss[0m : 2.69476
[1mStep[0m  [64/169], [94mLoss[0m : 2.72925
[1mStep[0m  [80/169], [94mLoss[0m : 2.55910
[1mStep[0m  [96/169], [94mLoss[0m : 2.50521
[1mStep[0m  [112/169], [94mLoss[0m : 2.47320
[1mStep[0m  [128/169], [94mLoss[0m : 1.90653
[1mStep[0m  [144/169], [94mLoss[0m : 2.23705
[1mStep[0m  [160/169], [94mLoss[0m : 2.61785

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.324, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3309446764843806
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.76572
[1mStep[0m  [16/169], [94mLoss[0m : 2.10373
[1mStep[0m  [32/169], [94mLoss[0m : 2.22764
[1mStep[0m  [48/169], [94mLoss[0m : 2.63058
[1mStep[0m  [64/169], [94mLoss[0m : 3.05037
[1mStep[0m  [80/169], [94mLoss[0m : 2.28135
[1mStep[0m  [96/169], [94mLoss[0m : 2.78987
[1mStep[0m  [112/169], [94mLoss[0m : 2.37001
[1mStep[0m  [128/169], [94mLoss[0m : 2.81757
[1mStep[0m  [144/169], [94mLoss[0m : 2.14106
[1mStep[0m  [160/169], [94mLoss[0m : 2.09248

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82217
[1mStep[0m  [16/169], [94mLoss[0m : 2.28874
[1mStep[0m  [32/169], [94mLoss[0m : 2.53353
[1mStep[0m  [48/169], [94mLoss[0m : 2.18136
[1mStep[0m  [64/169], [94mLoss[0m : 2.77448
[1mStep[0m  [80/169], [94mLoss[0m : 2.90922
[1mStep[0m  [96/169], [94mLoss[0m : 2.30305
[1mStep[0m  [112/169], [94mLoss[0m : 2.41200
[1mStep[0m  [128/169], [94mLoss[0m : 2.23068
[1mStep[0m  [144/169], [94mLoss[0m : 2.14824
[1mStep[0m  [160/169], [94mLoss[0m : 2.29767

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96717
[1mStep[0m  [16/169], [94mLoss[0m : 2.50721
[1mStep[0m  [32/169], [94mLoss[0m : 2.62776
[1mStep[0m  [48/169], [94mLoss[0m : 2.42043
[1mStep[0m  [64/169], [94mLoss[0m : 2.28834
[1mStep[0m  [80/169], [94mLoss[0m : 2.32660
[1mStep[0m  [96/169], [94mLoss[0m : 2.62041
[1mStep[0m  [112/169], [94mLoss[0m : 2.33663
[1mStep[0m  [128/169], [94mLoss[0m : 2.73866
[1mStep[0m  [144/169], [94mLoss[0m : 2.49014
[1mStep[0m  [160/169], [94mLoss[0m : 2.53642

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.519, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48339
[1mStep[0m  [16/169], [94mLoss[0m : 2.44265
[1mStep[0m  [32/169], [94mLoss[0m : 2.90081
[1mStep[0m  [48/169], [94mLoss[0m : 2.59548
[1mStep[0m  [64/169], [94mLoss[0m : 2.62370
[1mStep[0m  [80/169], [94mLoss[0m : 2.48425
[1mStep[0m  [96/169], [94mLoss[0m : 2.61898
[1mStep[0m  [112/169], [94mLoss[0m : 2.41432
[1mStep[0m  [128/169], [94mLoss[0m : 2.78450
[1mStep[0m  [144/169], [94mLoss[0m : 2.43860
[1mStep[0m  [160/169], [94mLoss[0m : 2.51160

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.626, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45663
[1mStep[0m  [16/169], [94mLoss[0m : 2.39559
[1mStep[0m  [32/169], [94mLoss[0m : 2.78009
[1mStep[0m  [48/169], [94mLoss[0m : 2.37480
[1mStep[0m  [64/169], [94mLoss[0m : 2.16899
[1mStep[0m  [80/169], [94mLoss[0m : 2.30620
[1mStep[0m  [96/169], [94mLoss[0m : 2.89995
[1mStep[0m  [112/169], [94mLoss[0m : 2.58600
[1mStep[0m  [128/169], [94mLoss[0m : 2.14427
[1mStep[0m  [144/169], [94mLoss[0m : 2.49229
[1mStep[0m  [160/169], [94mLoss[0m : 1.89461

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27431
[1mStep[0m  [16/169], [94mLoss[0m : 2.39394
[1mStep[0m  [32/169], [94mLoss[0m : 2.29726
[1mStep[0m  [48/169], [94mLoss[0m : 2.28325
[1mStep[0m  [64/169], [94mLoss[0m : 2.13931
[1mStep[0m  [80/169], [94mLoss[0m : 2.09606
[1mStep[0m  [96/169], [94mLoss[0m : 2.71541
[1mStep[0m  [112/169], [94mLoss[0m : 2.53082
[1mStep[0m  [128/169], [94mLoss[0m : 2.24797
[1mStep[0m  [144/169], [94mLoss[0m : 2.02864
[1mStep[0m  [160/169], [94mLoss[0m : 2.27845

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.558, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67993
[1mStep[0m  [16/169], [94mLoss[0m : 2.41206
[1mStep[0m  [32/169], [94mLoss[0m : 2.17823
[1mStep[0m  [48/169], [94mLoss[0m : 2.23653
[1mStep[0m  [64/169], [94mLoss[0m : 2.85502
[1mStep[0m  [80/169], [94mLoss[0m : 2.28931
[1mStep[0m  [96/169], [94mLoss[0m : 2.24414
[1mStep[0m  [112/169], [94mLoss[0m : 2.17043
[1mStep[0m  [128/169], [94mLoss[0m : 2.39342
[1mStep[0m  [144/169], [94mLoss[0m : 2.29746
[1mStep[0m  [160/169], [94mLoss[0m : 2.99726

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22744
[1mStep[0m  [16/169], [94mLoss[0m : 2.28185
[1mStep[0m  [32/169], [94mLoss[0m : 2.60816
[1mStep[0m  [48/169], [94mLoss[0m : 2.19127
[1mStep[0m  [64/169], [94mLoss[0m : 2.35544
[1mStep[0m  [80/169], [94mLoss[0m : 2.24510
[1mStep[0m  [96/169], [94mLoss[0m : 2.18513
[1mStep[0m  [112/169], [94mLoss[0m : 2.39286
[1mStep[0m  [128/169], [94mLoss[0m : 2.15334
[1mStep[0m  [144/169], [94mLoss[0m : 1.90355
[1mStep[0m  [160/169], [94mLoss[0m : 2.35838

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93696
[1mStep[0m  [16/169], [94mLoss[0m : 2.13024
[1mStep[0m  [32/169], [94mLoss[0m : 2.17033
[1mStep[0m  [48/169], [94mLoss[0m : 2.37089
[1mStep[0m  [64/169], [94mLoss[0m : 2.20186
[1mStep[0m  [80/169], [94mLoss[0m : 2.05072
[1mStep[0m  [96/169], [94mLoss[0m : 2.27173
[1mStep[0m  [112/169], [94mLoss[0m : 2.82104
[1mStep[0m  [128/169], [94mLoss[0m : 2.13144
[1mStep[0m  [144/169], [94mLoss[0m : 2.08827
[1mStep[0m  [160/169], [94mLoss[0m : 1.92217

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11307
[1mStep[0m  [16/169], [94mLoss[0m : 2.36685
[1mStep[0m  [32/169], [94mLoss[0m : 2.26862
[1mStep[0m  [48/169], [94mLoss[0m : 2.44914
[1mStep[0m  [64/169], [94mLoss[0m : 1.77954
[1mStep[0m  [80/169], [94mLoss[0m : 2.36493
[1mStep[0m  [96/169], [94mLoss[0m : 2.29271
[1mStep[0m  [112/169], [94mLoss[0m : 2.03814
[1mStep[0m  [128/169], [94mLoss[0m : 1.96924
[1mStep[0m  [144/169], [94mLoss[0m : 2.04830
[1mStep[0m  [160/169], [94mLoss[0m : 2.37299

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.213, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92932
[1mStep[0m  [16/169], [94mLoss[0m : 2.03934
[1mStep[0m  [32/169], [94mLoss[0m : 2.33004
[1mStep[0m  [48/169], [94mLoss[0m : 2.34766
[1mStep[0m  [64/169], [94mLoss[0m : 2.54077
[1mStep[0m  [80/169], [94mLoss[0m : 2.08794
[1mStep[0m  [96/169], [94mLoss[0m : 1.96568
[1mStep[0m  [112/169], [94mLoss[0m : 2.02792
[1mStep[0m  [128/169], [94mLoss[0m : 1.88889
[1mStep[0m  [144/169], [94mLoss[0m : 2.26081
[1mStep[0m  [160/169], [94mLoss[0m : 2.26608

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.80126
[1mStep[0m  [16/169], [94mLoss[0m : 2.56339
[1mStep[0m  [32/169], [94mLoss[0m : 1.86049
[1mStep[0m  [48/169], [94mLoss[0m : 2.85793
[1mStep[0m  [64/169], [94mLoss[0m : 2.42887
[1mStep[0m  [80/169], [94mLoss[0m : 2.29250
[1mStep[0m  [96/169], [94mLoss[0m : 2.65141
[1mStep[0m  [112/169], [94mLoss[0m : 2.24426
[1mStep[0m  [128/169], [94mLoss[0m : 2.30408
[1mStep[0m  [144/169], [94mLoss[0m : 2.18372
[1mStep[0m  [160/169], [94mLoss[0m : 2.34667

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.140, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05735
[1mStep[0m  [16/169], [94mLoss[0m : 2.01955
[1mStep[0m  [32/169], [94mLoss[0m : 1.74761
[1mStep[0m  [48/169], [94mLoss[0m : 2.08212
[1mStep[0m  [64/169], [94mLoss[0m : 1.79464
[1mStep[0m  [80/169], [94mLoss[0m : 1.96846
[1mStep[0m  [96/169], [94mLoss[0m : 2.06293
[1mStep[0m  [112/169], [94mLoss[0m : 2.55428
[1mStep[0m  [128/169], [94mLoss[0m : 2.33652
[1mStep[0m  [144/169], [94mLoss[0m : 2.22906
[1mStep[0m  [160/169], [94mLoss[0m : 2.43145

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.089, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92592
[1mStep[0m  [16/169], [94mLoss[0m : 1.91669
[1mStep[0m  [32/169], [94mLoss[0m : 1.79960
[1mStep[0m  [48/169], [94mLoss[0m : 2.06308
[1mStep[0m  [64/169], [94mLoss[0m : 2.16297
[1mStep[0m  [80/169], [94mLoss[0m : 2.27189
[1mStep[0m  [96/169], [94mLoss[0m : 1.78664
[1mStep[0m  [112/169], [94mLoss[0m : 2.09563
[1mStep[0m  [128/169], [94mLoss[0m : 2.18033
[1mStep[0m  [144/169], [94mLoss[0m : 2.03726
[1mStep[0m  [160/169], [94mLoss[0m : 2.03791

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98439
[1mStep[0m  [16/169], [94mLoss[0m : 1.75356
[1mStep[0m  [32/169], [94mLoss[0m : 1.67870
[1mStep[0m  [48/169], [94mLoss[0m : 1.92489
[1mStep[0m  [64/169], [94mLoss[0m : 1.58946
[1mStep[0m  [80/169], [94mLoss[0m : 2.06247
[1mStep[0m  [96/169], [94mLoss[0m : 2.26698
[1mStep[0m  [112/169], [94mLoss[0m : 2.28013
[1mStep[0m  [128/169], [94mLoss[0m : 2.26337
[1mStep[0m  [144/169], [94mLoss[0m : 1.88013
[1mStep[0m  [160/169], [94mLoss[0m : 1.70786

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72223
[1mStep[0m  [16/169], [94mLoss[0m : 1.95044
[1mStep[0m  [32/169], [94mLoss[0m : 2.37912
[1mStep[0m  [48/169], [94mLoss[0m : 2.08903
[1mStep[0m  [64/169], [94mLoss[0m : 1.85919
[1mStep[0m  [80/169], [94mLoss[0m : 2.06403
[1mStep[0m  [96/169], [94mLoss[0m : 1.70362
[1mStep[0m  [112/169], [94mLoss[0m : 1.78731
[1mStep[0m  [128/169], [94mLoss[0m : 2.41826
[1mStep[0m  [144/169], [94mLoss[0m : 2.16780
[1mStep[0m  [160/169], [94mLoss[0m : 1.88250

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.993, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97446
[1mStep[0m  [16/169], [94mLoss[0m : 1.75632
[1mStep[0m  [32/169], [94mLoss[0m : 2.09092
[1mStep[0m  [48/169], [94mLoss[0m : 1.92847
[1mStep[0m  [64/169], [94mLoss[0m : 1.68144
[1mStep[0m  [80/169], [94mLoss[0m : 2.25426
[1mStep[0m  [96/169], [94mLoss[0m : 1.88887
[1mStep[0m  [112/169], [94mLoss[0m : 1.71169
[1mStep[0m  [128/169], [94mLoss[0m : 2.02515
[1mStep[0m  [144/169], [94mLoss[0m : 2.03637
[1mStep[0m  [160/169], [94mLoss[0m : 2.14946

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.963, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27691
[1mStep[0m  [16/169], [94mLoss[0m : 2.17924
[1mStep[0m  [32/169], [94mLoss[0m : 2.05927
[1mStep[0m  [48/169], [94mLoss[0m : 1.71866
[1mStep[0m  [64/169], [94mLoss[0m : 1.54344
[1mStep[0m  [80/169], [94mLoss[0m : 1.67064
[1mStep[0m  [96/169], [94mLoss[0m : 2.08600
[1mStep[0m  [112/169], [94mLoss[0m : 1.37880
[1mStep[0m  [128/169], [94mLoss[0m : 1.89992
[1mStep[0m  [144/169], [94mLoss[0m : 2.14268
[1mStep[0m  [160/169], [94mLoss[0m : 1.80981

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00559
[1mStep[0m  [16/169], [94mLoss[0m : 1.86851
[1mStep[0m  [32/169], [94mLoss[0m : 2.01086
[1mStep[0m  [48/169], [94mLoss[0m : 1.47148
[1mStep[0m  [64/169], [94mLoss[0m : 1.97326
[1mStep[0m  [80/169], [94mLoss[0m : 1.80932
[1mStep[0m  [96/169], [94mLoss[0m : 1.83414
[1mStep[0m  [112/169], [94mLoss[0m : 1.99120
[1mStep[0m  [128/169], [94mLoss[0m : 1.88706
[1mStep[0m  [144/169], [94mLoss[0m : 1.85728
[1mStep[0m  [160/169], [94mLoss[0m : 1.95583

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.493, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72746
[1mStep[0m  [16/169], [94mLoss[0m : 1.90030
[1mStep[0m  [32/169], [94mLoss[0m : 1.70282
[1mStep[0m  [48/169], [94mLoss[0m : 2.00049
[1mStep[0m  [64/169], [94mLoss[0m : 1.90816
[1mStep[0m  [80/169], [94mLoss[0m : 1.65583
[1mStep[0m  [96/169], [94mLoss[0m : 1.96959
[1mStep[0m  [112/169], [94mLoss[0m : 1.89479
[1mStep[0m  [128/169], [94mLoss[0m : 1.86915
[1mStep[0m  [144/169], [94mLoss[0m : 1.84184
[1mStep[0m  [160/169], [94mLoss[0m : 2.46444

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89874
[1mStep[0m  [16/169], [94mLoss[0m : 1.74432
[1mStep[0m  [32/169], [94mLoss[0m : 1.95909
[1mStep[0m  [48/169], [94mLoss[0m : 1.64920
[1mStep[0m  [64/169], [94mLoss[0m : 2.06757
[1mStep[0m  [80/169], [94mLoss[0m : 1.84230
[1mStep[0m  [96/169], [94mLoss[0m : 1.79119
[1mStep[0m  [112/169], [94mLoss[0m : 1.35946
[1mStep[0m  [128/169], [94mLoss[0m : 1.69108
[1mStep[0m  [144/169], [94mLoss[0m : 1.72211
[1mStep[0m  [160/169], [94mLoss[0m : 1.69804

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.841, [92mTest[0m: 2.458, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95432
[1mStep[0m  [16/169], [94mLoss[0m : 2.01386
[1mStep[0m  [32/169], [94mLoss[0m : 1.46707
[1mStep[0m  [48/169], [94mLoss[0m : 1.42354
[1mStep[0m  [64/169], [94mLoss[0m : 1.66776
[1mStep[0m  [80/169], [94mLoss[0m : 1.78866
[1mStep[0m  [96/169], [94mLoss[0m : 2.03570
[1mStep[0m  [112/169], [94mLoss[0m : 2.20940
[1mStep[0m  [128/169], [94mLoss[0m : 2.11010
[1mStep[0m  [144/169], [94mLoss[0m : 1.97559
[1mStep[0m  [160/169], [94mLoss[0m : 1.62972

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.826, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81671
[1mStep[0m  [16/169], [94mLoss[0m : 1.77106
[1mStep[0m  [32/169], [94mLoss[0m : 1.89014
[1mStep[0m  [48/169], [94mLoss[0m : 1.79074
[1mStep[0m  [64/169], [94mLoss[0m : 1.83805
[1mStep[0m  [80/169], [94mLoss[0m : 1.69090
[1mStep[0m  [96/169], [94mLoss[0m : 2.02035
[1mStep[0m  [112/169], [94mLoss[0m : 1.62979
[1mStep[0m  [128/169], [94mLoss[0m : 1.97193
[1mStep[0m  [144/169], [94mLoss[0m : 2.00034
[1mStep[0m  [160/169], [94mLoss[0m : 1.66642

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.794, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90594
[1mStep[0m  [16/169], [94mLoss[0m : 1.90014
[1mStep[0m  [32/169], [94mLoss[0m : 1.73465
[1mStep[0m  [48/169], [94mLoss[0m : 1.82453
[1mStep[0m  [64/169], [94mLoss[0m : 1.60662
[1mStep[0m  [80/169], [94mLoss[0m : 1.74151
[1mStep[0m  [96/169], [94mLoss[0m : 1.90564
[1mStep[0m  [112/169], [94mLoss[0m : 1.80770
[1mStep[0m  [128/169], [94mLoss[0m : 1.65535
[1mStep[0m  [144/169], [94mLoss[0m : 1.71020
[1mStep[0m  [160/169], [94mLoss[0m : 1.89796

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.444, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51253
[1mStep[0m  [16/169], [94mLoss[0m : 1.69190
[1mStep[0m  [32/169], [94mLoss[0m : 1.70854
[1mStep[0m  [48/169], [94mLoss[0m : 1.86707
[1mStep[0m  [64/169], [94mLoss[0m : 1.54971
[1mStep[0m  [80/169], [94mLoss[0m : 1.75243
[1mStep[0m  [96/169], [94mLoss[0m : 1.58652
[1mStep[0m  [112/169], [94mLoss[0m : 2.24799
[1mStep[0m  [128/169], [94mLoss[0m : 1.71387
[1mStep[0m  [144/169], [94mLoss[0m : 2.07699
[1mStep[0m  [160/169], [94mLoss[0m : 1.68538

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.780, [92mTest[0m: 2.468, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53815
[1mStep[0m  [16/169], [94mLoss[0m : 1.49432
[1mStep[0m  [32/169], [94mLoss[0m : 1.78027
[1mStep[0m  [48/169], [94mLoss[0m : 1.78017
[1mStep[0m  [64/169], [94mLoss[0m : 1.64250
[1mStep[0m  [80/169], [94mLoss[0m : 2.24124
[1mStep[0m  [96/169], [94mLoss[0m : 1.67312
[1mStep[0m  [112/169], [94mLoss[0m : 1.63926
[1mStep[0m  [128/169], [94mLoss[0m : 1.52800
[1mStep[0m  [144/169], [94mLoss[0m : 1.85394
[1mStep[0m  [160/169], [94mLoss[0m : 1.64173

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.733, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99555
[1mStep[0m  [16/169], [94mLoss[0m : 1.78102
[1mStep[0m  [32/169], [94mLoss[0m : 2.03376
[1mStep[0m  [48/169], [94mLoss[0m : 1.67922
[1mStep[0m  [64/169], [94mLoss[0m : 1.83137
[1mStep[0m  [80/169], [94mLoss[0m : 1.95994
[1mStep[0m  [96/169], [94mLoss[0m : 1.38994
[1mStep[0m  [112/169], [94mLoss[0m : 1.60698
[1mStep[0m  [128/169], [94mLoss[0m : 1.84059
[1mStep[0m  [144/169], [94mLoss[0m : 1.37695
[1mStep[0m  [160/169], [94mLoss[0m : 1.26490

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77351
[1mStep[0m  [16/169], [94mLoss[0m : 1.63116
[1mStep[0m  [32/169], [94mLoss[0m : 1.95355
[1mStep[0m  [48/169], [94mLoss[0m : 1.78365
[1mStep[0m  [64/169], [94mLoss[0m : 1.89271
[1mStep[0m  [80/169], [94mLoss[0m : 2.21578
[1mStep[0m  [96/169], [94mLoss[0m : 1.75236
[1mStep[0m  [112/169], [94mLoss[0m : 1.73607
[1mStep[0m  [128/169], [94mLoss[0m : 1.76574
[1mStep[0m  [144/169], [94mLoss[0m : 1.88554
[1mStep[0m  [160/169], [94mLoss[0m : 1.74463

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.476, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74207
[1mStep[0m  [16/169], [94mLoss[0m : 1.47420
[1mStep[0m  [32/169], [94mLoss[0m : 1.91107
[1mStep[0m  [48/169], [94mLoss[0m : 1.56111
[1mStep[0m  [64/169], [94mLoss[0m : 1.52564
[1mStep[0m  [80/169], [94mLoss[0m : 1.71642
[1mStep[0m  [96/169], [94mLoss[0m : 2.01598
[1mStep[0m  [112/169], [94mLoss[0m : 1.68585
[1mStep[0m  [128/169], [94mLoss[0m : 1.42832
[1mStep[0m  [144/169], [94mLoss[0m : 1.68306
[1mStep[0m  [160/169], [94mLoss[0m : 2.30728

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85793
[1mStep[0m  [16/169], [94mLoss[0m : 1.84173
[1mStep[0m  [32/169], [94mLoss[0m : 1.61753
[1mStep[0m  [48/169], [94mLoss[0m : 1.70131
[1mStep[0m  [64/169], [94mLoss[0m : 1.76239
[1mStep[0m  [80/169], [94mLoss[0m : 1.59471
[1mStep[0m  [96/169], [94mLoss[0m : 1.56175
[1mStep[0m  [112/169], [94mLoss[0m : 1.63591
[1mStep[0m  [128/169], [94mLoss[0m : 1.76282
[1mStep[0m  [144/169], [94mLoss[0m : 1.64827
[1mStep[0m  [160/169], [94mLoss[0m : 1.43196

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.449
====================================

Phase 2 - Evaluation MAE:  2.4492886534758975
MAE score P1       2.330945
MAE score P2       2.449289
loss               1.692463
learning_rate      0.002575
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Already used Hyperparameters:  {'hidden_sizes': [300], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.08445
[1mStep[0m  [16/169], [94mLoss[0m : 10.55309
[1mStep[0m  [32/169], [94mLoss[0m : 9.79546
[1mStep[0m  [48/169], [94mLoss[0m : 9.72631
[1mStep[0m  [64/169], [94mLoss[0m : 8.23429
[1mStep[0m  [80/169], [94mLoss[0m : 7.88292
[1mStep[0m  [96/169], [94mLoss[0m : 7.41892
[1mStep[0m  [112/169], [94mLoss[0m : 7.03339
[1mStep[0m  [128/169], [94mLoss[0m : 5.90013
[1mStep[0m  [144/169], [94mLoss[0m : 5.33958
[1mStep[0m  [160/169], [94mLoss[0m : 4.03051

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 8.008, [92mTest[0m: 10.942, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 4.37536
[1mStep[0m  [16/169], [94mLoss[0m : 3.83818
[1mStep[0m  [32/169], [94mLoss[0m : 3.41884
[1mStep[0m  [48/169], [94mLoss[0m : 2.97734
[1mStep[0m  [64/169], [94mLoss[0m : 2.71737
[1mStep[0m  [80/169], [94mLoss[0m : 3.39053
[1mStep[0m  [96/169], [94mLoss[0m : 2.33442
[1mStep[0m  [112/169], [94mLoss[0m : 3.58206
[1mStep[0m  [128/169], [94mLoss[0m : 2.75811
[1mStep[0m  [144/169], [94mLoss[0m : 3.14921
[1mStep[0m  [160/169], [94mLoss[0m : 2.56623

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.998, [92mTest[0m: 3.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53723
[1mStep[0m  [16/169], [94mLoss[0m : 3.04853
[1mStep[0m  [32/169], [94mLoss[0m : 2.82283
[1mStep[0m  [48/169], [94mLoss[0m : 3.22380
[1mStep[0m  [64/169], [94mLoss[0m : 2.57037
[1mStep[0m  [80/169], [94mLoss[0m : 2.45839
[1mStep[0m  [96/169], [94mLoss[0m : 2.74087
[1mStep[0m  [112/169], [94mLoss[0m : 2.51290
[1mStep[0m  [128/169], [94mLoss[0m : 2.71686
[1mStep[0m  [144/169], [94mLoss[0m : 2.62088
[1mStep[0m  [160/169], [94mLoss[0m : 2.87738

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.709, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61701
[1mStep[0m  [16/169], [94mLoss[0m : 2.63527
[1mStep[0m  [32/169], [94mLoss[0m : 3.14101
[1mStep[0m  [48/169], [94mLoss[0m : 2.71298
[1mStep[0m  [64/169], [94mLoss[0m : 2.53614
[1mStep[0m  [80/169], [94mLoss[0m : 2.72973
[1mStep[0m  [96/169], [94mLoss[0m : 2.70725
[1mStep[0m  [112/169], [94mLoss[0m : 3.04150
[1mStep[0m  [128/169], [94mLoss[0m : 2.17003
[1mStep[0m  [144/169], [94mLoss[0m : 2.55998
[1mStep[0m  [160/169], [94mLoss[0m : 2.59036

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22240
[1mStep[0m  [16/169], [94mLoss[0m : 2.63618
[1mStep[0m  [32/169], [94mLoss[0m : 2.48106
[1mStep[0m  [48/169], [94mLoss[0m : 3.18042
[1mStep[0m  [64/169], [94mLoss[0m : 2.55837
[1mStep[0m  [80/169], [94mLoss[0m : 3.05704
[1mStep[0m  [96/169], [94mLoss[0m : 2.22978
[1mStep[0m  [112/169], [94mLoss[0m : 2.69066
[1mStep[0m  [128/169], [94mLoss[0m : 2.78421
[1mStep[0m  [144/169], [94mLoss[0m : 2.50956
[1mStep[0m  [160/169], [94mLoss[0m : 2.57874

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.342, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49716
[1mStep[0m  [16/169], [94mLoss[0m : 2.73187
[1mStep[0m  [32/169], [94mLoss[0m : 2.14212
[1mStep[0m  [48/169], [94mLoss[0m : 2.37397
[1mStep[0m  [64/169], [94mLoss[0m : 2.29598
[1mStep[0m  [80/169], [94mLoss[0m : 2.64930
[1mStep[0m  [96/169], [94mLoss[0m : 2.76731
[1mStep[0m  [112/169], [94mLoss[0m : 2.88174
[1mStep[0m  [128/169], [94mLoss[0m : 2.56857
[1mStep[0m  [144/169], [94mLoss[0m : 2.61389
[1mStep[0m  [160/169], [94mLoss[0m : 2.64315

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.22853
[1mStep[0m  [16/169], [94mLoss[0m : 2.26531
[1mStep[0m  [32/169], [94mLoss[0m : 3.14037
[1mStep[0m  [48/169], [94mLoss[0m : 2.96039
[1mStep[0m  [64/169], [94mLoss[0m : 2.74922
[1mStep[0m  [80/169], [94mLoss[0m : 2.54544
[1mStep[0m  [96/169], [94mLoss[0m : 2.53949
[1mStep[0m  [112/169], [94mLoss[0m : 2.88662
[1mStep[0m  [128/169], [94mLoss[0m : 2.40149
[1mStep[0m  [144/169], [94mLoss[0m : 2.11497
[1mStep[0m  [160/169], [94mLoss[0m : 2.85034

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46080
[1mStep[0m  [16/169], [94mLoss[0m : 2.84636
[1mStep[0m  [32/169], [94mLoss[0m : 2.57298
[1mStep[0m  [48/169], [94mLoss[0m : 2.60103
[1mStep[0m  [64/169], [94mLoss[0m : 2.46590
[1mStep[0m  [80/169], [94mLoss[0m : 2.48209
[1mStep[0m  [96/169], [94mLoss[0m : 2.95701
[1mStep[0m  [112/169], [94mLoss[0m : 2.43839
[1mStep[0m  [128/169], [94mLoss[0m : 2.38729
[1mStep[0m  [144/169], [94mLoss[0m : 2.74524
[1mStep[0m  [160/169], [94mLoss[0m : 2.67537

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.327, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19368
[1mStep[0m  [16/169], [94mLoss[0m : 2.81593
[1mStep[0m  [32/169], [94mLoss[0m : 2.70579
[1mStep[0m  [48/169], [94mLoss[0m : 2.17279
[1mStep[0m  [64/169], [94mLoss[0m : 2.74069
[1mStep[0m  [80/169], [94mLoss[0m : 2.19189
[1mStep[0m  [96/169], [94mLoss[0m : 2.55451
[1mStep[0m  [112/169], [94mLoss[0m : 2.67582
[1mStep[0m  [128/169], [94mLoss[0m : 2.38699
[1mStep[0m  [144/169], [94mLoss[0m : 2.73526
[1mStep[0m  [160/169], [94mLoss[0m : 2.88002

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61595
[1mStep[0m  [16/169], [94mLoss[0m : 2.35692
[1mStep[0m  [32/169], [94mLoss[0m : 2.67130
[1mStep[0m  [48/169], [94mLoss[0m : 2.15936
[1mStep[0m  [64/169], [94mLoss[0m : 2.42723
[1mStep[0m  [80/169], [94mLoss[0m : 2.49794
[1mStep[0m  [96/169], [94mLoss[0m : 2.40442
[1mStep[0m  [112/169], [94mLoss[0m : 2.06615
[1mStep[0m  [128/169], [94mLoss[0m : 2.42900
[1mStep[0m  [144/169], [94mLoss[0m : 2.04706
[1mStep[0m  [160/169], [94mLoss[0m : 2.70461

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44499
[1mStep[0m  [16/169], [94mLoss[0m : 2.61863
[1mStep[0m  [32/169], [94mLoss[0m : 2.47172
[1mStep[0m  [48/169], [94mLoss[0m : 2.27936
[1mStep[0m  [64/169], [94mLoss[0m : 2.69848
[1mStep[0m  [80/169], [94mLoss[0m : 2.15398
[1mStep[0m  [96/169], [94mLoss[0m : 2.65331
[1mStep[0m  [112/169], [94mLoss[0m : 2.55683
[1mStep[0m  [128/169], [94mLoss[0m : 2.37032
[1mStep[0m  [144/169], [94mLoss[0m : 2.38867
[1mStep[0m  [160/169], [94mLoss[0m : 2.13091

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03545
[1mStep[0m  [16/169], [94mLoss[0m : 2.65471
[1mStep[0m  [32/169], [94mLoss[0m : 2.36010
[1mStep[0m  [48/169], [94mLoss[0m : 2.40076
[1mStep[0m  [64/169], [94mLoss[0m : 2.27752
[1mStep[0m  [80/169], [94mLoss[0m : 2.59183
[1mStep[0m  [96/169], [94mLoss[0m : 2.11395
[1mStep[0m  [112/169], [94mLoss[0m : 2.59717
[1mStep[0m  [128/169], [94mLoss[0m : 2.66392
[1mStep[0m  [144/169], [94mLoss[0m : 2.05043
[1mStep[0m  [160/169], [94mLoss[0m : 2.42675

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55385
[1mStep[0m  [16/169], [94mLoss[0m : 2.15868
[1mStep[0m  [32/169], [94mLoss[0m : 2.42837
[1mStep[0m  [48/169], [94mLoss[0m : 2.40653
[1mStep[0m  [64/169], [94mLoss[0m : 2.15991
[1mStep[0m  [80/169], [94mLoss[0m : 2.28558
[1mStep[0m  [96/169], [94mLoss[0m : 2.50444
[1mStep[0m  [112/169], [94mLoss[0m : 2.24616
[1mStep[0m  [128/169], [94mLoss[0m : 2.45582
[1mStep[0m  [144/169], [94mLoss[0m : 2.56826
[1mStep[0m  [160/169], [94mLoss[0m : 2.83893

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60813
[1mStep[0m  [16/169], [94mLoss[0m : 2.56261
[1mStep[0m  [32/169], [94mLoss[0m : 2.66710
[1mStep[0m  [48/169], [94mLoss[0m : 2.46307
[1mStep[0m  [64/169], [94mLoss[0m : 2.62463
[1mStep[0m  [80/169], [94mLoss[0m : 2.34690
[1mStep[0m  [96/169], [94mLoss[0m : 2.28597
[1mStep[0m  [112/169], [94mLoss[0m : 2.56956
[1mStep[0m  [128/169], [94mLoss[0m : 2.49810
[1mStep[0m  [144/169], [94mLoss[0m : 2.74245
[1mStep[0m  [160/169], [94mLoss[0m : 2.43397

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09292
[1mStep[0m  [16/169], [94mLoss[0m : 2.35820
[1mStep[0m  [32/169], [94mLoss[0m : 2.52721
[1mStep[0m  [48/169], [94mLoss[0m : 2.60475
[1mStep[0m  [64/169], [94mLoss[0m : 2.60370
[1mStep[0m  [80/169], [94mLoss[0m : 2.30380
[1mStep[0m  [96/169], [94mLoss[0m : 2.24670
[1mStep[0m  [112/169], [94mLoss[0m : 2.40644
[1mStep[0m  [128/169], [94mLoss[0m : 2.58210
[1mStep[0m  [144/169], [94mLoss[0m : 2.65234
[1mStep[0m  [160/169], [94mLoss[0m : 2.90289

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57191
[1mStep[0m  [16/169], [94mLoss[0m : 2.93536
[1mStep[0m  [32/169], [94mLoss[0m : 2.14199
[1mStep[0m  [48/169], [94mLoss[0m : 2.47752
[1mStep[0m  [64/169], [94mLoss[0m : 2.59500
[1mStep[0m  [80/169], [94mLoss[0m : 2.05549
[1mStep[0m  [96/169], [94mLoss[0m : 2.39336
[1mStep[0m  [112/169], [94mLoss[0m : 2.67801
[1mStep[0m  [128/169], [94mLoss[0m : 2.75879
[1mStep[0m  [144/169], [94mLoss[0m : 2.12691
[1mStep[0m  [160/169], [94mLoss[0m : 2.74588

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.336, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11142
[1mStep[0m  [16/169], [94mLoss[0m : 2.80437
[1mStep[0m  [32/169], [94mLoss[0m : 2.71882
[1mStep[0m  [48/169], [94mLoss[0m : 2.50691
[1mStep[0m  [64/169], [94mLoss[0m : 2.50770
[1mStep[0m  [80/169], [94mLoss[0m : 2.41959
[1mStep[0m  [96/169], [94mLoss[0m : 2.28466
[1mStep[0m  [112/169], [94mLoss[0m : 2.36002
[1mStep[0m  [128/169], [94mLoss[0m : 2.57057
[1mStep[0m  [144/169], [94mLoss[0m : 2.17450
[1mStep[0m  [160/169], [94mLoss[0m : 2.48863

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.332, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36029
[1mStep[0m  [16/169], [94mLoss[0m : 2.25655
[1mStep[0m  [32/169], [94mLoss[0m : 2.45545
[1mStep[0m  [48/169], [94mLoss[0m : 2.27987
[1mStep[0m  [64/169], [94mLoss[0m : 2.76015
[1mStep[0m  [80/169], [94mLoss[0m : 2.04191
[1mStep[0m  [96/169], [94mLoss[0m : 2.12228
[1mStep[0m  [112/169], [94mLoss[0m : 2.24028
[1mStep[0m  [128/169], [94mLoss[0m : 2.67942
[1mStep[0m  [144/169], [94mLoss[0m : 2.27527
[1mStep[0m  [160/169], [94mLoss[0m : 2.25710

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.328, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30456
[1mStep[0m  [16/169], [94mLoss[0m : 2.63343
[1mStep[0m  [32/169], [94mLoss[0m : 2.23810
[1mStep[0m  [48/169], [94mLoss[0m : 2.97205
[1mStep[0m  [64/169], [94mLoss[0m : 2.45065
[1mStep[0m  [80/169], [94mLoss[0m : 2.24287
[1mStep[0m  [96/169], [94mLoss[0m : 2.36870
[1mStep[0m  [112/169], [94mLoss[0m : 2.04055
[1mStep[0m  [128/169], [94mLoss[0m : 2.67235
[1mStep[0m  [144/169], [94mLoss[0m : 2.22668
[1mStep[0m  [160/169], [94mLoss[0m : 2.68548

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.348, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20064
[1mStep[0m  [16/169], [94mLoss[0m : 2.79729
[1mStep[0m  [32/169], [94mLoss[0m : 2.66908
[1mStep[0m  [48/169], [94mLoss[0m : 2.31999
[1mStep[0m  [64/169], [94mLoss[0m : 2.03461
[1mStep[0m  [80/169], [94mLoss[0m : 2.66720
[1mStep[0m  [96/169], [94mLoss[0m : 2.90074
[1mStep[0m  [112/169], [94mLoss[0m : 2.52085
[1mStep[0m  [128/169], [94mLoss[0m : 2.20208
[1mStep[0m  [144/169], [94mLoss[0m : 2.17832
[1mStep[0m  [160/169], [94mLoss[0m : 2.60836

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.90850
[1mStep[0m  [16/169], [94mLoss[0m : 2.32401
[1mStep[0m  [32/169], [94mLoss[0m : 2.55052
[1mStep[0m  [48/169], [94mLoss[0m : 2.61498
[1mStep[0m  [64/169], [94mLoss[0m : 2.14226
[1mStep[0m  [80/169], [94mLoss[0m : 2.22949
[1mStep[0m  [96/169], [94mLoss[0m : 2.25335
[1mStep[0m  [112/169], [94mLoss[0m : 2.12704
[1mStep[0m  [128/169], [94mLoss[0m : 2.61281
[1mStep[0m  [144/169], [94mLoss[0m : 2.65798
[1mStep[0m  [160/169], [94mLoss[0m : 2.85595

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.315, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36745
[1mStep[0m  [16/169], [94mLoss[0m : 2.48943
[1mStep[0m  [32/169], [94mLoss[0m : 2.30331
[1mStep[0m  [48/169], [94mLoss[0m : 2.51793
[1mStep[0m  [64/169], [94mLoss[0m : 2.69467
[1mStep[0m  [80/169], [94mLoss[0m : 2.43445
[1mStep[0m  [96/169], [94mLoss[0m : 2.60396
[1mStep[0m  [112/169], [94mLoss[0m : 2.01105
[1mStep[0m  [128/169], [94mLoss[0m : 2.50618
[1mStep[0m  [144/169], [94mLoss[0m : 2.10387
[1mStep[0m  [160/169], [94mLoss[0m : 2.36838

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34853
[1mStep[0m  [16/169], [94mLoss[0m : 1.89737
[1mStep[0m  [32/169], [94mLoss[0m : 2.11778
[1mStep[0m  [48/169], [94mLoss[0m : 2.41696
[1mStep[0m  [64/169], [94mLoss[0m : 2.81230
[1mStep[0m  [80/169], [94mLoss[0m : 2.78102
[1mStep[0m  [96/169], [94mLoss[0m : 2.40102
[1mStep[0m  [112/169], [94mLoss[0m : 2.34817
[1mStep[0m  [128/169], [94mLoss[0m : 2.63404
[1mStep[0m  [144/169], [94mLoss[0m : 2.66386
[1mStep[0m  [160/169], [94mLoss[0m : 2.75375

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.349, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08920
[1mStep[0m  [16/169], [94mLoss[0m : 2.45561
[1mStep[0m  [32/169], [94mLoss[0m : 2.42671
[1mStep[0m  [48/169], [94mLoss[0m : 2.57333
[1mStep[0m  [64/169], [94mLoss[0m : 2.23038
[1mStep[0m  [80/169], [94mLoss[0m : 2.24953
[1mStep[0m  [96/169], [94mLoss[0m : 2.49825
[1mStep[0m  [112/169], [94mLoss[0m : 3.04741
[1mStep[0m  [128/169], [94mLoss[0m : 2.12820
[1mStep[0m  [144/169], [94mLoss[0m : 2.47526
[1mStep[0m  [160/169], [94mLoss[0m : 2.13109

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40820
[1mStep[0m  [16/169], [94mLoss[0m : 2.10691
[1mStep[0m  [32/169], [94mLoss[0m : 2.36939
[1mStep[0m  [48/169], [94mLoss[0m : 2.37037
[1mStep[0m  [64/169], [94mLoss[0m : 2.37528
[1mStep[0m  [80/169], [94mLoss[0m : 2.56727
[1mStep[0m  [96/169], [94mLoss[0m : 2.24337
[1mStep[0m  [112/169], [94mLoss[0m : 2.28054
[1mStep[0m  [128/169], [94mLoss[0m : 2.41155
[1mStep[0m  [144/169], [94mLoss[0m : 2.09360
[1mStep[0m  [160/169], [94mLoss[0m : 2.60887

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38574
[1mStep[0m  [16/169], [94mLoss[0m : 2.08043
[1mStep[0m  [32/169], [94mLoss[0m : 2.37480
[1mStep[0m  [48/169], [94mLoss[0m : 2.19597
[1mStep[0m  [64/169], [94mLoss[0m : 2.41363
[1mStep[0m  [80/169], [94mLoss[0m : 2.40244
[1mStep[0m  [96/169], [94mLoss[0m : 2.84397
[1mStep[0m  [112/169], [94mLoss[0m : 2.41362
[1mStep[0m  [128/169], [94mLoss[0m : 2.50414
[1mStep[0m  [144/169], [94mLoss[0m : 2.92479
[1mStep[0m  [160/169], [94mLoss[0m : 2.32738

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.319, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21726
[1mStep[0m  [16/169], [94mLoss[0m : 2.70729
[1mStep[0m  [32/169], [94mLoss[0m : 2.34543
[1mStep[0m  [48/169], [94mLoss[0m : 2.49130
[1mStep[0m  [64/169], [94mLoss[0m : 2.44597
[1mStep[0m  [80/169], [94mLoss[0m : 2.43720
[1mStep[0m  [96/169], [94mLoss[0m : 2.78898
[1mStep[0m  [112/169], [94mLoss[0m : 2.46913
[1mStep[0m  [128/169], [94mLoss[0m : 2.69687
[1mStep[0m  [144/169], [94mLoss[0m : 2.08636
[1mStep[0m  [160/169], [94mLoss[0m : 2.29798

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53567
[1mStep[0m  [16/169], [94mLoss[0m : 2.11846
[1mStep[0m  [32/169], [94mLoss[0m : 2.59076
[1mStep[0m  [48/169], [94mLoss[0m : 2.12957
[1mStep[0m  [64/169], [94mLoss[0m : 2.23981
[1mStep[0m  [80/169], [94mLoss[0m : 2.39098
[1mStep[0m  [96/169], [94mLoss[0m : 2.26547
[1mStep[0m  [112/169], [94mLoss[0m : 2.28887
[1mStep[0m  [128/169], [94mLoss[0m : 2.20140
[1mStep[0m  [144/169], [94mLoss[0m : 2.17739
[1mStep[0m  [160/169], [94mLoss[0m : 2.20079

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84691
[1mStep[0m  [16/169], [94mLoss[0m : 2.72704
[1mStep[0m  [32/169], [94mLoss[0m : 2.61836
[1mStep[0m  [48/169], [94mLoss[0m : 2.52221
[1mStep[0m  [64/169], [94mLoss[0m : 2.17561
[1mStep[0m  [80/169], [94mLoss[0m : 2.36457
[1mStep[0m  [96/169], [94mLoss[0m : 2.33799
[1mStep[0m  [112/169], [94mLoss[0m : 2.41373
[1mStep[0m  [128/169], [94mLoss[0m : 2.19580
[1mStep[0m  [144/169], [94mLoss[0m : 2.80841
[1mStep[0m  [160/169], [94mLoss[0m : 2.20083

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.32217
[1mStep[0m  [16/169], [94mLoss[0m : 2.24067
[1mStep[0m  [32/169], [94mLoss[0m : 2.40614
[1mStep[0m  [48/169], [94mLoss[0m : 2.72975
[1mStep[0m  [64/169], [94mLoss[0m : 2.33361
[1mStep[0m  [80/169], [94mLoss[0m : 1.99679
[1mStep[0m  [96/169], [94mLoss[0m : 2.37210
[1mStep[0m  [112/169], [94mLoss[0m : 2.45005
[1mStep[0m  [128/169], [94mLoss[0m : 2.94743
[1mStep[0m  [144/169], [94mLoss[0m : 2.31564
[1mStep[0m  [160/169], [94mLoss[0m : 2.12046

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.326, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.333
====================================

Phase 1 - Evaluation MAE:  2.3329920172691345
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.26715
[1mStep[0m  [16/169], [94mLoss[0m : 3.13063
[1mStep[0m  [32/169], [94mLoss[0m : 2.76749
[1mStep[0m  [48/169], [94mLoss[0m : 2.33082
[1mStep[0m  [64/169], [94mLoss[0m : 2.44546
[1mStep[0m  [80/169], [94mLoss[0m : 2.10664
[1mStep[0m  [96/169], [94mLoss[0m : 2.35595
[1mStep[0m  [112/169], [94mLoss[0m : 2.46734
[1mStep[0m  [128/169], [94mLoss[0m : 2.55867
[1mStep[0m  [144/169], [94mLoss[0m : 2.13421
[1mStep[0m  [160/169], [94mLoss[0m : 2.62226

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.330, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30997
[1mStep[0m  [16/169], [94mLoss[0m : 2.48439
[1mStep[0m  [32/169], [94mLoss[0m : 2.00010
[1mStep[0m  [48/169], [94mLoss[0m : 2.53501
[1mStep[0m  [64/169], [94mLoss[0m : 2.04844
[1mStep[0m  [80/169], [94mLoss[0m : 2.31518
[1mStep[0m  [96/169], [94mLoss[0m : 2.04319
[1mStep[0m  [112/169], [94mLoss[0m : 1.83176
[1mStep[0m  [128/169], [94mLoss[0m : 2.12154
[1mStep[0m  [144/169], [94mLoss[0m : 2.65607
[1mStep[0m  [160/169], [94mLoss[0m : 1.98495

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10692
[1mStep[0m  [16/169], [94mLoss[0m : 2.06319
[1mStep[0m  [32/169], [94mLoss[0m : 2.13053
[1mStep[0m  [48/169], [94mLoss[0m : 2.43001
[1mStep[0m  [64/169], [94mLoss[0m : 2.50052
[1mStep[0m  [80/169], [94mLoss[0m : 2.35306
[1mStep[0m  [96/169], [94mLoss[0m : 1.96135
[1mStep[0m  [112/169], [94mLoss[0m : 2.16303
[1mStep[0m  [128/169], [94mLoss[0m : 2.06342
[1mStep[0m  [144/169], [94mLoss[0m : 2.24695
[1mStep[0m  [160/169], [94mLoss[0m : 2.45516

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90136
[1mStep[0m  [16/169], [94mLoss[0m : 2.01827
[1mStep[0m  [32/169], [94mLoss[0m : 2.11948
[1mStep[0m  [48/169], [94mLoss[0m : 2.24049
[1mStep[0m  [64/169], [94mLoss[0m : 2.45232
[1mStep[0m  [80/169], [94mLoss[0m : 2.25409
[1mStep[0m  [96/169], [94mLoss[0m : 2.30747
[1mStep[0m  [112/169], [94mLoss[0m : 2.61610
[1mStep[0m  [128/169], [94mLoss[0m : 2.28653
[1mStep[0m  [144/169], [94mLoss[0m : 2.30013
[1mStep[0m  [160/169], [94mLoss[0m : 2.71452

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.205, [92mTest[0m: 2.338, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99141
[1mStep[0m  [16/169], [94mLoss[0m : 2.01774
[1mStep[0m  [32/169], [94mLoss[0m : 2.13521
[1mStep[0m  [48/169], [94mLoss[0m : 2.31244
[1mStep[0m  [64/169], [94mLoss[0m : 2.41214
[1mStep[0m  [80/169], [94mLoss[0m : 2.36682
[1mStep[0m  [96/169], [94mLoss[0m : 2.17385
[1mStep[0m  [112/169], [94mLoss[0m : 2.47463
[1mStep[0m  [128/169], [94mLoss[0m : 1.66226
[1mStep[0m  [144/169], [94mLoss[0m : 2.18063
[1mStep[0m  [160/169], [94mLoss[0m : 2.32545

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.166, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91595
[1mStep[0m  [16/169], [94mLoss[0m : 2.20110
[1mStep[0m  [32/169], [94mLoss[0m : 2.26942
[1mStep[0m  [48/169], [94mLoss[0m : 1.95575
[1mStep[0m  [64/169], [94mLoss[0m : 1.97576
[1mStep[0m  [80/169], [94mLoss[0m : 2.52073
[1mStep[0m  [96/169], [94mLoss[0m : 2.20023
[1mStep[0m  [112/169], [94mLoss[0m : 1.71731
[1mStep[0m  [128/169], [94mLoss[0m : 2.09336
[1mStep[0m  [144/169], [94mLoss[0m : 2.16526
[1mStep[0m  [160/169], [94mLoss[0m : 1.97115

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.093, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04082
[1mStep[0m  [16/169], [94mLoss[0m : 2.02934
[1mStep[0m  [32/169], [94mLoss[0m : 1.89369
[1mStep[0m  [48/169], [94mLoss[0m : 1.96227
[1mStep[0m  [64/169], [94mLoss[0m : 2.49835
[1mStep[0m  [80/169], [94mLoss[0m : 2.02331
[1mStep[0m  [96/169], [94mLoss[0m : 2.24780
[1mStep[0m  [112/169], [94mLoss[0m : 2.52541
[1mStep[0m  [128/169], [94mLoss[0m : 1.88271
[1mStep[0m  [144/169], [94mLoss[0m : 2.03096
[1mStep[0m  [160/169], [94mLoss[0m : 1.74816

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83927
[1mStep[0m  [16/169], [94mLoss[0m : 1.63877
[1mStep[0m  [32/169], [94mLoss[0m : 2.37528
[1mStep[0m  [48/169], [94mLoss[0m : 1.87956
[1mStep[0m  [64/169], [94mLoss[0m : 2.48768
[1mStep[0m  [80/169], [94mLoss[0m : 1.82324
[1mStep[0m  [96/169], [94mLoss[0m : 1.91390
[1mStep[0m  [112/169], [94mLoss[0m : 2.35866
[1mStep[0m  [128/169], [94mLoss[0m : 1.96430
[1mStep[0m  [144/169], [94mLoss[0m : 2.13568
[1mStep[0m  [160/169], [94mLoss[0m : 2.33470

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65748
[1mStep[0m  [16/169], [94mLoss[0m : 2.18752
[1mStep[0m  [32/169], [94mLoss[0m : 2.35740
[1mStep[0m  [48/169], [94mLoss[0m : 2.14719
[1mStep[0m  [64/169], [94mLoss[0m : 1.89470
[1mStep[0m  [80/169], [94mLoss[0m : 1.92853
[1mStep[0m  [96/169], [94mLoss[0m : 1.82382
[1mStep[0m  [112/169], [94mLoss[0m : 1.99358
[1mStep[0m  [128/169], [94mLoss[0m : 1.75473
[1mStep[0m  [144/169], [94mLoss[0m : 2.06185
[1mStep[0m  [160/169], [94mLoss[0m : 2.16254

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79099
[1mStep[0m  [16/169], [94mLoss[0m : 1.69486
[1mStep[0m  [32/169], [94mLoss[0m : 1.92208
[1mStep[0m  [48/169], [94mLoss[0m : 1.47362
[1mStep[0m  [64/169], [94mLoss[0m : 1.87128
[1mStep[0m  [80/169], [94mLoss[0m : 1.43013
[1mStep[0m  [96/169], [94mLoss[0m : 2.16994
[1mStep[0m  [112/169], [94mLoss[0m : 2.30360
[1mStep[0m  [128/169], [94mLoss[0m : 2.00813
[1mStep[0m  [144/169], [94mLoss[0m : 1.97717
[1mStep[0m  [160/169], [94mLoss[0m : 1.84799

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.887, [92mTest[0m: 2.411, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11542
[1mStep[0m  [16/169], [94mLoss[0m : 1.83596
[1mStep[0m  [32/169], [94mLoss[0m : 1.76750
[1mStep[0m  [48/169], [94mLoss[0m : 1.84766
[1mStep[0m  [64/169], [94mLoss[0m : 1.77050
[1mStep[0m  [80/169], [94mLoss[0m : 2.03323
[1mStep[0m  [96/169], [94mLoss[0m : 1.71903
[1mStep[0m  [112/169], [94mLoss[0m : 1.87048
[1mStep[0m  [128/169], [94mLoss[0m : 1.99081
[1mStep[0m  [144/169], [94mLoss[0m : 1.91486
[1mStep[0m  [160/169], [94mLoss[0m : 1.96370

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.835, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63353
[1mStep[0m  [16/169], [94mLoss[0m : 1.60322
[1mStep[0m  [32/169], [94mLoss[0m : 1.98911
[1mStep[0m  [48/169], [94mLoss[0m : 1.50165
[1mStep[0m  [64/169], [94mLoss[0m : 1.69496
[1mStep[0m  [80/169], [94mLoss[0m : 2.14652
[1mStep[0m  [96/169], [94mLoss[0m : 1.40157
[1mStep[0m  [112/169], [94mLoss[0m : 1.75063
[1mStep[0m  [128/169], [94mLoss[0m : 1.81506
[1mStep[0m  [144/169], [94mLoss[0m : 1.88783
[1mStep[0m  [160/169], [94mLoss[0m : 1.80221

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.808, [92mTest[0m: 2.475, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71046
[1mStep[0m  [16/169], [94mLoss[0m : 1.37571
[1mStep[0m  [32/169], [94mLoss[0m : 1.61422
[1mStep[0m  [48/169], [94mLoss[0m : 1.63711
[1mStep[0m  [64/169], [94mLoss[0m : 1.85080
[1mStep[0m  [80/169], [94mLoss[0m : 2.19686
[1mStep[0m  [96/169], [94mLoss[0m : 1.54796
[1mStep[0m  [112/169], [94mLoss[0m : 1.61338
[1mStep[0m  [128/169], [94mLoss[0m : 1.87816
[1mStep[0m  [144/169], [94mLoss[0m : 2.00626
[1mStep[0m  [160/169], [94mLoss[0m : 2.01886

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.489, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67917
[1mStep[0m  [16/169], [94mLoss[0m : 1.88256
[1mStep[0m  [32/169], [94mLoss[0m : 1.77497
[1mStep[0m  [48/169], [94mLoss[0m : 1.94907
[1mStep[0m  [64/169], [94mLoss[0m : 1.64896
[1mStep[0m  [80/169], [94mLoss[0m : 1.62224
[1mStep[0m  [96/169], [94mLoss[0m : 1.74821
[1mStep[0m  [112/169], [94mLoss[0m : 1.41483
[1mStep[0m  [128/169], [94mLoss[0m : 2.12731
[1mStep[0m  [144/169], [94mLoss[0m : 1.87547
[1mStep[0m  [160/169], [94mLoss[0m : 1.76982

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61667
[1mStep[0m  [16/169], [94mLoss[0m : 1.47300
[1mStep[0m  [32/169], [94mLoss[0m : 1.48728
[1mStep[0m  [48/169], [94mLoss[0m : 1.96634
[1mStep[0m  [64/169], [94mLoss[0m : 1.83514
[1mStep[0m  [80/169], [94mLoss[0m : 2.00833
[1mStep[0m  [96/169], [94mLoss[0m : 1.60943
[1mStep[0m  [112/169], [94mLoss[0m : 1.56269
[1mStep[0m  [128/169], [94mLoss[0m : 1.96011
[1mStep[0m  [144/169], [94mLoss[0m : 1.89842
[1mStep[0m  [160/169], [94mLoss[0m : 1.53979

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.510, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75889
[1mStep[0m  [16/169], [94mLoss[0m : 1.72705
[1mStep[0m  [32/169], [94mLoss[0m : 1.58123
[1mStep[0m  [48/169], [94mLoss[0m : 1.47475
[1mStep[0m  [64/169], [94mLoss[0m : 1.64643
[1mStep[0m  [80/169], [94mLoss[0m : 1.51323
[1mStep[0m  [96/169], [94mLoss[0m : 1.89979
[1mStep[0m  [112/169], [94mLoss[0m : 1.46779
[1mStep[0m  [128/169], [94mLoss[0m : 1.87176
[1mStep[0m  [144/169], [94mLoss[0m : 1.78937
[1mStep[0m  [160/169], [94mLoss[0m : 1.59461

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.674, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49819
[1mStep[0m  [16/169], [94mLoss[0m : 1.65245
[1mStep[0m  [32/169], [94mLoss[0m : 1.57732
[1mStep[0m  [48/169], [94mLoss[0m : 1.91222
[1mStep[0m  [64/169], [94mLoss[0m : 1.42894
[1mStep[0m  [80/169], [94mLoss[0m : 1.62032
[1mStep[0m  [96/169], [94mLoss[0m : 1.58850
[1mStep[0m  [112/169], [94mLoss[0m : 1.39521
[1mStep[0m  [128/169], [94mLoss[0m : 1.52112
[1mStep[0m  [144/169], [94mLoss[0m : 1.94725
[1mStep[0m  [160/169], [94mLoss[0m : 1.73174

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.646, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46187
[1mStep[0m  [16/169], [94mLoss[0m : 1.56287
[1mStep[0m  [32/169], [94mLoss[0m : 1.73473
[1mStep[0m  [48/169], [94mLoss[0m : 1.33471
[1mStep[0m  [64/169], [94mLoss[0m : 1.54337
[1mStep[0m  [80/169], [94mLoss[0m : 1.53879
[1mStep[0m  [96/169], [94mLoss[0m : 1.38184
[1mStep[0m  [112/169], [94mLoss[0m : 1.38566
[1mStep[0m  [128/169], [94mLoss[0m : 1.92529
[1mStep[0m  [144/169], [94mLoss[0m : 1.76628
[1mStep[0m  [160/169], [94mLoss[0m : 1.60770

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75417
[1mStep[0m  [16/169], [94mLoss[0m : 1.71777
[1mStep[0m  [32/169], [94mLoss[0m : 1.42821
[1mStep[0m  [48/169], [94mLoss[0m : 1.80302
[1mStep[0m  [64/169], [94mLoss[0m : 1.58012
[1mStep[0m  [80/169], [94mLoss[0m : 1.64529
[1mStep[0m  [96/169], [94mLoss[0m : 1.78926
[1mStep[0m  [112/169], [94mLoss[0m : 1.34913
[1mStep[0m  [128/169], [94mLoss[0m : 1.45540
[1mStep[0m  [144/169], [94mLoss[0m : 1.87495
[1mStep[0m  [160/169], [94mLoss[0m : 1.45924

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.527, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66157
[1mStep[0m  [16/169], [94mLoss[0m : 1.63675
[1mStep[0m  [32/169], [94mLoss[0m : 1.51828
[1mStep[0m  [48/169], [94mLoss[0m : 1.76158
[1mStep[0m  [64/169], [94mLoss[0m : 1.67272
[1mStep[0m  [80/169], [94mLoss[0m : 1.61499
[1mStep[0m  [96/169], [94mLoss[0m : 1.54545
[1mStep[0m  [112/169], [94mLoss[0m : 1.58919
[1mStep[0m  [128/169], [94mLoss[0m : 1.42254
[1mStep[0m  [144/169], [94mLoss[0m : 1.57127
[1mStep[0m  [160/169], [94mLoss[0m : 1.65929

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.576, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81954
[1mStep[0m  [16/169], [94mLoss[0m : 1.71746
[1mStep[0m  [32/169], [94mLoss[0m : 1.61930
[1mStep[0m  [48/169], [94mLoss[0m : 1.88572
[1mStep[0m  [64/169], [94mLoss[0m : 1.89295
[1mStep[0m  [80/169], [94mLoss[0m : 1.54771
[1mStep[0m  [96/169], [94mLoss[0m : 1.31510
[1mStep[0m  [112/169], [94mLoss[0m : 1.87148
[1mStep[0m  [128/169], [94mLoss[0m : 1.61684
[1mStep[0m  [144/169], [94mLoss[0m : 1.72451
[1mStep[0m  [160/169], [94mLoss[0m : 1.52480

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63086
[1mStep[0m  [16/169], [94mLoss[0m : 1.17527
[1mStep[0m  [32/169], [94mLoss[0m : 1.34210
[1mStep[0m  [48/169], [94mLoss[0m : 1.60495
[1mStep[0m  [64/169], [94mLoss[0m : 1.34876
[1mStep[0m  [80/169], [94mLoss[0m : 1.52198
[1mStep[0m  [96/169], [94mLoss[0m : 1.46600
[1mStep[0m  [112/169], [94mLoss[0m : 1.39704
[1mStep[0m  [128/169], [94mLoss[0m : 1.35782
[1mStep[0m  [144/169], [94mLoss[0m : 1.95800
[1mStep[0m  [160/169], [94mLoss[0m : 1.44117

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40836
[1mStep[0m  [16/169], [94mLoss[0m : 1.20265
[1mStep[0m  [32/169], [94mLoss[0m : 1.43736
[1mStep[0m  [48/169], [94mLoss[0m : 1.56793
[1mStep[0m  [64/169], [94mLoss[0m : 1.57978
[1mStep[0m  [80/169], [94mLoss[0m : 1.36739
[1mStep[0m  [96/169], [94mLoss[0m : 1.69926
[1mStep[0m  [112/169], [94mLoss[0m : 1.64715
[1mStep[0m  [128/169], [94mLoss[0m : 1.20596
[1mStep[0m  [144/169], [94mLoss[0m : 1.47122
[1mStep[0m  [160/169], [94mLoss[0m : 1.43691

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56522
[1mStep[0m  [16/169], [94mLoss[0m : 1.29992
[1mStep[0m  [32/169], [94mLoss[0m : 1.29745
[1mStep[0m  [48/169], [94mLoss[0m : 1.65588
[1mStep[0m  [64/169], [94mLoss[0m : 1.27312
[1mStep[0m  [80/169], [94mLoss[0m : 1.80917
[1mStep[0m  [96/169], [94mLoss[0m : 1.38001
[1mStep[0m  [112/169], [94mLoss[0m : 1.20901
[1mStep[0m  [128/169], [94mLoss[0m : 1.55999
[1mStep[0m  [144/169], [94mLoss[0m : 1.25631
[1mStep[0m  [160/169], [94mLoss[0m : 1.31520

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.500, [92mTest[0m: 2.522, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59616
[1mStep[0m  [16/169], [94mLoss[0m : 1.56271
[1mStep[0m  [32/169], [94mLoss[0m : 1.37069
[1mStep[0m  [48/169], [94mLoss[0m : 1.45420
[1mStep[0m  [64/169], [94mLoss[0m : 1.45004
[1mStep[0m  [80/169], [94mLoss[0m : 1.39358
[1mStep[0m  [96/169], [94mLoss[0m : 1.39663
[1mStep[0m  [112/169], [94mLoss[0m : 1.59903
[1mStep[0m  [128/169], [94mLoss[0m : 1.30909
[1mStep[0m  [144/169], [94mLoss[0m : 1.33759
[1mStep[0m  [160/169], [94mLoss[0m : 1.76603

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.475, [92mTest[0m: 2.563, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32081
[1mStep[0m  [16/169], [94mLoss[0m : 1.84405
[1mStep[0m  [32/169], [94mLoss[0m : 1.32825
[1mStep[0m  [48/169], [94mLoss[0m : 1.36078
[1mStep[0m  [64/169], [94mLoss[0m : 1.48448
[1mStep[0m  [80/169], [94mLoss[0m : 1.35024
[1mStep[0m  [96/169], [94mLoss[0m : 1.44525
[1mStep[0m  [112/169], [94mLoss[0m : 1.47458
[1mStep[0m  [128/169], [94mLoss[0m : 1.20297
[1mStep[0m  [144/169], [94mLoss[0m : 1.23003
[1mStep[0m  [160/169], [94mLoss[0m : 1.57556

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.446, [92mTest[0m: 2.547, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54663
[1mStep[0m  [16/169], [94mLoss[0m : 1.15290
[1mStep[0m  [32/169], [94mLoss[0m : 1.22487
[1mStep[0m  [48/169], [94mLoss[0m : 1.47566
[1mStep[0m  [64/169], [94mLoss[0m : 1.84540
[1mStep[0m  [80/169], [94mLoss[0m : 1.32610
[1mStep[0m  [96/169], [94mLoss[0m : 1.50052
