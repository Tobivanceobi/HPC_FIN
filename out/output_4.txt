no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  4
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 11.92349
[1mStep[0m  [21/213], [94mLoss[0m : 4.45522
[1mStep[0m  [42/213], [94mLoss[0m : 2.72489
[1mStep[0m  [63/213], [94mLoss[0m : 2.50853
[1mStep[0m  [84/213], [94mLoss[0m : 2.70088
[1mStep[0m  [105/213], [94mLoss[0m : 2.67231
[1mStep[0m  [126/213], [94mLoss[0m : 2.30185
[1mStep[0m  [147/213], [94mLoss[0m : 2.51238
[1mStep[0m  [168/213], [94mLoss[0m : 2.57151
[1mStep[0m  [189/213], [94mLoss[0m : 2.76397
[1mStep[0m  [210/213], [94mLoss[0m : 2.37379

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.259, [92mTest[0m: 11.310, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.27946
[1mStep[0m  [21/213], [94mLoss[0m : 2.62217
[1mStep[0m  [42/213], [94mLoss[0m : 2.85587
[1mStep[0m  [63/213], [94mLoss[0m : 2.03528
[1mStep[0m  [84/213], [94mLoss[0m : 2.48300
[1mStep[0m  [105/213], [94mLoss[0m : 2.47555
[1mStep[0m  [126/213], [94mLoss[0m : 2.76762
[1mStep[0m  [147/213], [94mLoss[0m : 2.45627
[1mStep[0m  [168/213], [94mLoss[0m : 2.68675
[1mStep[0m  [189/213], [94mLoss[0m : 2.43952
[1mStep[0m  [210/213], [94mLoss[0m : 2.30866

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43161
[1mStep[0m  [21/213], [94mLoss[0m : 2.71737
[1mStep[0m  [42/213], [94mLoss[0m : 2.56637
[1mStep[0m  [63/213], [94mLoss[0m : 2.60969
[1mStep[0m  [84/213], [94mLoss[0m : 2.71112
[1mStep[0m  [105/213], [94mLoss[0m : 2.80911
[1mStep[0m  [126/213], [94mLoss[0m : 2.74435
[1mStep[0m  [147/213], [94mLoss[0m : 2.67376
[1mStep[0m  [168/213], [94mLoss[0m : 2.50241
[1mStep[0m  [189/213], [94mLoss[0m : 2.29380
[1mStep[0m  [210/213], [94mLoss[0m : 2.13330

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48081
[1mStep[0m  [21/213], [94mLoss[0m : 2.28143
[1mStep[0m  [42/213], [94mLoss[0m : 2.37101
[1mStep[0m  [63/213], [94mLoss[0m : 1.97761
[1mStep[0m  [84/213], [94mLoss[0m : 2.44708
[1mStep[0m  [105/213], [94mLoss[0m : 2.32050
[1mStep[0m  [126/213], [94mLoss[0m : 2.37412
[1mStep[0m  [147/213], [94mLoss[0m : 2.61190
[1mStep[0m  [168/213], [94mLoss[0m : 2.61915
[1mStep[0m  [189/213], [94mLoss[0m : 2.34721
[1mStep[0m  [210/213], [94mLoss[0m : 2.45189

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.443, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56315
[1mStep[0m  [21/213], [94mLoss[0m : 2.34247
[1mStep[0m  [42/213], [94mLoss[0m : 2.49068
[1mStep[0m  [63/213], [94mLoss[0m : 2.42884
[1mStep[0m  [84/213], [94mLoss[0m : 2.38364
[1mStep[0m  [105/213], [94mLoss[0m : 2.27335
[1mStep[0m  [126/213], [94mLoss[0m : 2.73827
[1mStep[0m  [147/213], [94mLoss[0m : 2.62447
[1mStep[0m  [168/213], [94mLoss[0m : 2.45652
[1mStep[0m  [189/213], [94mLoss[0m : 2.41102
[1mStep[0m  [210/213], [94mLoss[0m : 2.96680

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.27625
[1mStep[0m  [21/213], [94mLoss[0m : 2.38055
[1mStep[0m  [42/213], [94mLoss[0m : 2.51246
[1mStep[0m  [63/213], [94mLoss[0m : 2.47251
[1mStep[0m  [84/213], [94mLoss[0m : 2.74434
[1mStep[0m  [105/213], [94mLoss[0m : 2.74147
[1mStep[0m  [126/213], [94mLoss[0m : 2.39453
[1mStep[0m  [147/213], [94mLoss[0m : 2.59503
[1mStep[0m  [168/213], [94mLoss[0m : 2.64748
[1mStep[0m  [189/213], [94mLoss[0m : 2.70347
[1mStep[0m  [210/213], [94mLoss[0m : 2.32232

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.442, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44031
[1mStep[0m  [21/213], [94mLoss[0m : 2.48397
[1mStep[0m  [42/213], [94mLoss[0m : 2.70750
[1mStep[0m  [63/213], [94mLoss[0m : 2.72873
[1mStep[0m  [84/213], [94mLoss[0m : 2.02347
[1mStep[0m  [105/213], [94mLoss[0m : 2.38096
[1mStep[0m  [126/213], [94mLoss[0m : 2.60652
[1mStep[0m  [147/213], [94mLoss[0m : 2.46747
[1mStep[0m  [168/213], [94mLoss[0m : 2.31631
[1mStep[0m  [189/213], [94mLoss[0m : 3.09212
[1mStep[0m  [210/213], [94mLoss[0m : 2.07701

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.429, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38794
[1mStep[0m  [21/213], [94mLoss[0m : 2.62714
[1mStep[0m  [42/213], [94mLoss[0m : 2.42645
[1mStep[0m  [63/213], [94mLoss[0m : 2.75685
[1mStep[0m  [84/213], [94mLoss[0m : 2.48439
[1mStep[0m  [105/213], [94mLoss[0m : 2.53385
[1mStep[0m  [126/213], [94mLoss[0m : 1.95075
[1mStep[0m  [147/213], [94mLoss[0m : 2.12536
[1mStep[0m  [168/213], [94mLoss[0m : 2.03287
[1mStep[0m  [189/213], [94mLoss[0m : 1.65497
[1mStep[0m  [210/213], [94mLoss[0m : 2.47429

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39865
[1mStep[0m  [21/213], [94mLoss[0m : 2.56670
[1mStep[0m  [42/213], [94mLoss[0m : 2.71693
[1mStep[0m  [63/213], [94mLoss[0m : 1.99556
[1mStep[0m  [84/213], [94mLoss[0m : 2.50269
[1mStep[0m  [105/213], [94mLoss[0m : 2.54531
[1mStep[0m  [126/213], [94mLoss[0m : 2.16084
[1mStep[0m  [147/213], [94mLoss[0m : 2.37324
[1mStep[0m  [168/213], [94mLoss[0m : 2.71202
[1mStep[0m  [189/213], [94mLoss[0m : 2.60020
[1mStep[0m  [210/213], [94mLoss[0m : 2.28852

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35282
[1mStep[0m  [21/213], [94mLoss[0m : 2.17749
[1mStep[0m  [42/213], [94mLoss[0m : 2.11010
[1mStep[0m  [63/213], [94mLoss[0m : 2.42733
[1mStep[0m  [84/213], [94mLoss[0m : 2.19946
[1mStep[0m  [105/213], [94mLoss[0m : 2.41184
[1mStep[0m  [126/213], [94mLoss[0m : 2.69309
[1mStep[0m  [147/213], [94mLoss[0m : 2.54709
[1mStep[0m  [168/213], [94mLoss[0m : 2.19965
[1mStep[0m  [189/213], [94mLoss[0m : 2.35779
[1mStep[0m  [210/213], [94mLoss[0m : 2.54489

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45502
[1mStep[0m  [21/213], [94mLoss[0m : 2.46205
[1mStep[0m  [42/213], [94mLoss[0m : 2.43383
[1mStep[0m  [63/213], [94mLoss[0m : 2.43630
[1mStep[0m  [84/213], [94mLoss[0m : 2.61544
[1mStep[0m  [105/213], [94mLoss[0m : 2.20114
[1mStep[0m  [126/213], [94mLoss[0m : 2.57686
[1mStep[0m  [147/213], [94mLoss[0m : 2.28250
[1mStep[0m  [168/213], [94mLoss[0m : 2.96796
[1mStep[0m  [189/213], [94mLoss[0m : 3.08070
[1mStep[0m  [210/213], [94mLoss[0m : 2.92799

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.85423
[1mStep[0m  [21/213], [94mLoss[0m : 2.06544
[1mStep[0m  [42/213], [94mLoss[0m : 2.06312
[1mStep[0m  [63/213], [94mLoss[0m : 2.51508
[1mStep[0m  [84/213], [94mLoss[0m : 2.49518
[1mStep[0m  [105/213], [94mLoss[0m : 2.55868
[1mStep[0m  [126/213], [94mLoss[0m : 2.41334
[1mStep[0m  [147/213], [94mLoss[0m : 2.40089
[1mStep[0m  [168/213], [94mLoss[0m : 2.70113
[1mStep[0m  [189/213], [94mLoss[0m : 2.50179
[1mStep[0m  [210/213], [94mLoss[0m : 2.30432

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36616
[1mStep[0m  [21/213], [94mLoss[0m : 2.59279
[1mStep[0m  [42/213], [94mLoss[0m : 2.14623
[1mStep[0m  [63/213], [94mLoss[0m : 2.58942
[1mStep[0m  [84/213], [94mLoss[0m : 2.24646
[1mStep[0m  [105/213], [94mLoss[0m : 2.35856
[1mStep[0m  [126/213], [94mLoss[0m : 2.31913
[1mStep[0m  [147/213], [94mLoss[0m : 2.65675
[1mStep[0m  [168/213], [94mLoss[0m : 2.26383
[1mStep[0m  [189/213], [94mLoss[0m : 2.77480
[1mStep[0m  [210/213], [94mLoss[0m : 2.25596

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47059
[1mStep[0m  [21/213], [94mLoss[0m : 2.46687
[1mStep[0m  [42/213], [94mLoss[0m : 3.01915
[1mStep[0m  [63/213], [94mLoss[0m : 2.91191
[1mStep[0m  [84/213], [94mLoss[0m : 2.74823
[1mStep[0m  [105/213], [94mLoss[0m : 2.55940
[1mStep[0m  [126/213], [94mLoss[0m : 2.57590
[1mStep[0m  [147/213], [94mLoss[0m : 1.99304
[1mStep[0m  [168/213], [94mLoss[0m : 2.78992
[1mStep[0m  [189/213], [94mLoss[0m : 2.19963
[1mStep[0m  [210/213], [94mLoss[0m : 2.25882

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63004
[1mStep[0m  [21/213], [94mLoss[0m : 2.79332
[1mStep[0m  [42/213], [94mLoss[0m : 2.17386
[1mStep[0m  [63/213], [94mLoss[0m : 1.97115
[1mStep[0m  [84/213], [94mLoss[0m : 2.10599
[1mStep[0m  [105/213], [94mLoss[0m : 2.36974
[1mStep[0m  [126/213], [94mLoss[0m : 2.53655
[1mStep[0m  [147/213], [94mLoss[0m : 2.43644
[1mStep[0m  [168/213], [94mLoss[0m : 2.48845
[1mStep[0m  [189/213], [94mLoss[0m : 2.06615
[1mStep[0m  [210/213], [94mLoss[0m : 2.48262

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19141
[1mStep[0m  [21/213], [94mLoss[0m : 2.79503
[1mStep[0m  [42/213], [94mLoss[0m : 2.24271
[1mStep[0m  [63/213], [94mLoss[0m : 2.17246
[1mStep[0m  [84/213], [94mLoss[0m : 2.20878
[1mStep[0m  [105/213], [94mLoss[0m : 2.16221
[1mStep[0m  [126/213], [94mLoss[0m : 2.01180
[1mStep[0m  [147/213], [94mLoss[0m : 2.33578
[1mStep[0m  [168/213], [94mLoss[0m : 2.51128
[1mStep[0m  [189/213], [94mLoss[0m : 1.93319
[1mStep[0m  [210/213], [94mLoss[0m : 2.75375

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36423
[1mStep[0m  [21/213], [94mLoss[0m : 2.28245
[1mStep[0m  [42/213], [94mLoss[0m : 2.78926
[1mStep[0m  [63/213], [94mLoss[0m : 2.18526
[1mStep[0m  [84/213], [94mLoss[0m : 2.74119
[1mStep[0m  [105/213], [94mLoss[0m : 2.48905
[1mStep[0m  [126/213], [94mLoss[0m : 2.77979
[1mStep[0m  [147/213], [94mLoss[0m : 1.91185
[1mStep[0m  [168/213], [94mLoss[0m : 2.09440
[1mStep[0m  [189/213], [94mLoss[0m : 2.31192
[1mStep[0m  [210/213], [94mLoss[0m : 2.62748

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18456
[1mStep[0m  [21/213], [94mLoss[0m : 2.26458
[1mStep[0m  [42/213], [94mLoss[0m : 2.12874
[1mStep[0m  [63/213], [94mLoss[0m : 2.00038
[1mStep[0m  [84/213], [94mLoss[0m : 2.54727
[1mStep[0m  [105/213], [94mLoss[0m : 2.46110
[1mStep[0m  [126/213], [94mLoss[0m : 2.48904
[1mStep[0m  [147/213], [94mLoss[0m : 2.14030
[1mStep[0m  [168/213], [94mLoss[0m : 2.44634
[1mStep[0m  [189/213], [94mLoss[0m : 1.94132
[1mStep[0m  [210/213], [94mLoss[0m : 2.35544

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65973
[1mStep[0m  [21/213], [94mLoss[0m : 2.98924
[1mStep[0m  [42/213], [94mLoss[0m : 2.34701
[1mStep[0m  [63/213], [94mLoss[0m : 2.45801
[1mStep[0m  [84/213], [94mLoss[0m : 2.07019
[1mStep[0m  [105/213], [94mLoss[0m : 2.50287
[1mStep[0m  [126/213], [94mLoss[0m : 2.17658
[1mStep[0m  [147/213], [94mLoss[0m : 2.50943
[1mStep[0m  [168/213], [94mLoss[0m : 2.56551
[1mStep[0m  [189/213], [94mLoss[0m : 2.53657
[1mStep[0m  [210/213], [94mLoss[0m : 2.35282

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17000
[1mStep[0m  [21/213], [94mLoss[0m : 2.62875
[1mStep[0m  [42/213], [94mLoss[0m : 2.28615
[1mStep[0m  [63/213], [94mLoss[0m : 2.54346
[1mStep[0m  [84/213], [94mLoss[0m : 2.73707
[1mStep[0m  [105/213], [94mLoss[0m : 2.56094
[1mStep[0m  [126/213], [94mLoss[0m : 2.76308
[1mStep[0m  [147/213], [94mLoss[0m : 2.07916
[1mStep[0m  [168/213], [94mLoss[0m : 2.75050
[1mStep[0m  [189/213], [94mLoss[0m : 2.39155
[1mStep[0m  [210/213], [94mLoss[0m : 2.34774

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51300
[1mStep[0m  [21/213], [94mLoss[0m : 2.50783
[1mStep[0m  [42/213], [94mLoss[0m : 2.32476
[1mStep[0m  [63/213], [94mLoss[0m : 2.36284
[1mStep[0m  [84/213], [94mLoss[0m : 2.34831
[1mStep[0m  [105/213], [94mLoss[0m : 2.31421
[1mStep[0m  [126/213], [94mLoss[0m : 2.92150
[1mStep[0m  [147/213], [94mLoss[0m : 2.68426
[1mStep[0m  [168/213], [94mLoss[0m : 2.29679
[1mStep[0m  [189/213], [94mLoss[0m : 2.17512
[1mStep[0m  [210/213], [94mLoss[0m : 2.82678

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.99061
[1mStep[0m  [21/213], [94mLoss[0m : 2.52845
[1mStep[0m  [42/213], [94mLoss[0m : 2.66737
[1mStep[0m  [63/213], [94mLoss[0m : 2.41701
[1mStep[0m  [84/213], [94mLoss[0m : 2.16420
[1mStep[0m  [105/213], [94mLoss[0m : 2.23628
[1mStep[0m  [126/213], [94mLoss[0m : 2.77881
[1mStep[0m  [147/213], [94mLoss[0m : 2.55441
[1mStep[0m  [168/213], [94mLoss[0m : 2.28757
[1mStep[0m  [189/213], [94mLoss[0m : 2.42153
[1mStep[0m  [210/213], [94mLoss[0m : 2.13588

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.410, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40926
[1mStep[0m  [21/213], [94mLoss[0m : 2.31182
[1mStep[0m  [42/213], [94mLoss[0m : 2.28794
[1mStep[0m  [63/213], [94mLoss[0m : 2.85223
[1mStep[0m  [84/213], [94mLoss[0m : 2.57035
[1mStep[0m  [105/213], [94mLoss[0m : 2.99358
[1mStep[0m  [126/213], [94mLoss[0m : 3.16777
[1mStep[0m  [147/213], [94mLoss[0m : 3.16400
[1mStep[0m  [168/213], [94mLoss[0m : 2.31559
[1mStep[0m  [189/213], [94mLoss[0m : 2.37455
[1mStep[0m  [210/213], [94mLoss[0m : 2.22174

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.94534
[1mStep[0m  [21/213], [94mLoss[0m : 2.31055
[1mStep[0m  [42/213], [94mLoss[0m : 2.26172
[1mStep[0m  [63/213], [94mLoss[0m : 2.73977
[1mStep[0m  [84/213], [94mLoss[0m : 2.01612
[1mStep[0m  [105/213], [94mLoss[0m : 2.88017
[1mStep[0m  [126/213], [94mLoss[0m : 2.01460
[1mStep[0m  [147/213], [94mLoss[0m : 2.93491
[1mStep[0m  [168/213], [94mLoss[0m : 2.73458
[1mStep[0m  [189/213], [94mLoss[0m : 2.51617
[1mStep[0m  [210/213], [94mLoss[0m : 2.41341

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44489
[1mStep[0m  [21/213], [94mLoss[0m : 2.60504
[1mStep[0m  [42/213], [94mLoss[0m : 2.45827
[1mStep[0m  [63/213], [94mLoss[0m : 2.10638
[1mStep[0m  [84/213], [94mLoss[0m : 2.13403
[1mStep[0m  [105/213], [94mLoss[0m : 2.24620
[1mStep[0m  [126/213], [94mLoss[0m : 2.68294
[1mStep[0m  [147/213], [94mLoss[0m : 2.21087
[1mStep[0m  [168/213], [94mLoss[0m : 2.37153
[1mStep[0m  [189/213], [94mLoss[0m : 2.71627
[1mStep[0m  [210/213], [94mLoss[0m : 2.60174

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.400, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50650
[1mStep[0m  [21/213], [94mLoss[0m : 2.77859
[1mStep[0m  [42/213], [94mLoss[0m : 2.72429
[1mStep[0m  [63/213], [94mLoss[0m : 2.42847
[1mStep[0m  [84/213], [94mLoss[0m : 2.72503
[1mStep[0m  [105/213], [94mLoss[0m : 2.77028
[1mStep[0m  [126/213], [94mLoss[0m : 2.56203
[1mStep[0m  [147/213], [94mLoss[0m : 2.75994
[1mStep[0m  [168/213], [94mLoss[0m : 2.59703
[1mStep[0m  [189/213], [94mLoss[0m : 2.49803
[1mStep[0m  [210/213], [94mLoss[0m : 2.36191

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23061
[1mStep[0m  [21/213], [94mLoss[0m : 2.39995
[1mStep[0m  [42/213], [94mLoss[0m : 2.49166
[1mStep[0m  [63/213], [94mLoss[0m : 2.48174
[1mStep[0m  [84/213], [94mLoss[0m : 2.30601
[1mStep[0m  [105/213], [94mLoss[0m : 2.22022
[1mStep[0m  [126/213], [94mLoss[0m : 2.65685
[1mStep[0m  [147/213], [94mLoss[0m : 2.37908
[1mStep[0m  [168/213], [94mLoss[0m : 2.64823
[1mStep[0m  [189/213], [94mLoss[0m : 2.82939
[1mStep[0m  [210/213], [94mLoss[0m : 2.25320

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.88029
[1mStep[0m  [21/213], [94mLoss[0m : 2.44627
[1mStep[0m  [42/213], [94mLoss[0m : 2.29641
[1mStep[0m  [63/213], [94mLoss[0m : 2.65064
[1mStep[0m  [84/213], [94mLoss[0m : 2.61212
[1mStep[0m  [105/213], [94mLoss[0m : 2.53427
[1mStep[0m  [126/213], [94mLoss[0m : 2.75634
[1mStep[0m  [147/213], [94mLoss[0m : 2.10513
[1mStep[0m  [168/213], [94mLoss[0m : 2.58215
[1mStep[0m  [189/213], [94mLoss[0m : 2.28369
[1mStep[0m  [210/213], [94mLoss[0m : 2.91787

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30314
[1mStep[0m  [21/213], [94mLoss[0m : 2.69290
[1mStep[0m  [42/213], [94mLoss[0m : 2.43862
[1mStep[0m  [63/213], [94mLoss[0m : 2.22901
[1mStep[0m  [84/213], [94mLoss[0m : 2.25336
[1mStep[0m  [105/213], [94mLoss[0m : 2.82625
[1mStep[0m  [126/213], [94mLoss[0m : 2.71440
[1mStep[0m  [147/213], [94mLoss[0m : 2.21666
[1mStep[0m  [168/213], [94mLoss[0m : 2.26752
[1mStep[0m  [189/213], [94mLoss[0m : 2.24788
[1mStep[0m  [210/213], [94mLoss[0m : 2.22297

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51519
[1mStep[0m  [21/213], [94mLoss[0m : 2.16550
[1mStep[0m  [42/213], [94mLoss[0m : 2.23703
[1mStep[0m  [63/213], [94mLoss[0m : 2.24861
[1mStep[0m  [84/213], [94mLoss[0m : 2.28508
[1mStep[0m  [105/213], [94mLoss[0m : 2.20964
[1mStep[0m  [126/213], [94mLoss[0m : 2.44500
[1mStep[0m  [147/213], [94mLoss[0m : 2.04016
[1mStep[0m  [168/213], [94mLoss[0m : 2.38366
[1mStep[0m  [189/213], [94mLoss[0m : 2.46929
[1mStep[0m  [210/213], [94mLoss[0m : 2.10054

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.424
====================================

Phase 1 - Evaluation MAE:  2.4239624243862226
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 2.66556
[1mStep[0m  [21/213], [94mLoss[0m : 2.17790
[1mStep[0m  [42/213], [94mLoss[0m : 2.50835
[1mStep[0m  [63/213], [94mLoss[0m : 1.88381
[1mStep[0m  [84/213], [94mLoss[0m : 2.36051
[1mStep[0m  [105/213], [94mLoss[0m : 2.15449
[1mStep[0m  [126/213], [94mLoss[0m : 2.81716
[1mStep[0m  [147/213], [94mLoss[0m : 2.98983
[1mStep[0m  [168/213], [94mLoss[0m : 2.26579
[1mStep[0m  [189/213], [94mLoss[0m : 2.34964
[1mStep[0m  [210/213], [94mLoss[0m : 2.28478

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53858
[1mStep[0m  [21/213], [94mLoss[0m : 2.28950
[1mStep[0m  [42/213], [94mLoss[0m : 2.39610
[1mStep[0m  [63/213], [94mLoss[0m : 2.73631
[1mStep[0m  [84/213], [94mLoss[0m : 2.48378
[1mStep[0m  [105/213], [94mLoss[0m : 2.54739
[1mStep[0m  [126/213], [94mLoss[0m : 2.17366
[1mStep[0m  [147/213], [94mLoss[0m : 2.41109
[1mStep[0m  [168/213], [94mLoss[0m : 2.21802
[1mStep[0m  [189/213], [94mLoss[0m : 2.34837
[1mStep[0m  [210/213], [94mLoss[0m : 2.24320

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.643, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61034
[1mStep[0m  [21/213], [94mLoss[0m : 2.84500
[1mStep[0m  [42/213], [94mLoss[0m : 2.32457
[1mStep[0m  [63/213], [94mLoss[0m : 2.18881
[1mStep[0m  [84/213], [94mLoss[0m : 2.70757
[1mStep[0m  [105/213], [94mLoss[0m : 2.23370
[1mStep[0m  [126/213], [94mLoss[0m : 2.72614
[1mStep[0m  [147/213], [94mLoss[0m : 2.83651
[1mStep[0m  [168/213], [94mLoss[0m : 2.34237
[1mStep[0m  [189/213], [94mLoss[0m : 2.44176
[1mStep[0m  [210/213], [94mLoss[0m : 2.10486

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.373, [92mTest[0m: 2.649, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24418
[1mStep[0m  [21/213], [94mLoss[0m : 2.60724
[1mStep[0m  [42/213], [94mLoss[0m : 2.35448
[1mStep[0m  [63/213], [94mLoss[0m : 2.26493
[1mStep[0m  [84/213], [94mLoss[0m : 2.47877
[1mStep[0m  [105/213], [94mLoss[0m : 2.22088
[1mStep[0m  [126/213], [94mLoss[0m : 2.03931
[1mStep[0m  [147/213], [94mLoss[0m : 2.35808
[1mStep[0m  [168/213], [94mLoss[0m : 2.48656
[1mStep[0m  [189/213], [94mLoss[0m : 2.17859
[1mStep[0m  [210/213], [94mLoss[0m : 2.48330

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22991
[1mStep[0m  [21/213], [94mLoss[0m : 2.49218
[1mStep[0m  [42/213], [94mLoss[0m : 2.72248
[1mStep[0m  [63/213], [94mLoss[0m : 2.07594
[1mStep[0m  [84/213], [94mLoss[0m : 2.03940
[1mStep[0m  [105/213], [94mLoss[0m : 2.06653
[1mStep[0m  [126/213], [94mLoss[0m : 2.20645
[1mStep[0m  [147/213], [94mLoss[0m : 2.25085
[1mStep[0m  [168/213], [94mLoss[0m : 2.17482
[1mStep[0m  [189/213], [94mLoss[0m : 2.31890
[1mStep[0m  [210/213], [94mLoss[0m : 2.17406

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.565, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.03486
[1mStep[0m  [21/213], [94mLoss[0m : 2.09774
[1mStep[0m  [42/213], [94mLoss[0m : 2.38525
[1mStep[0m  [63/213], [94mLoss[0m : 2.35425
[1mStep[0m  [84/213], [94mLoss[0m : 2.42669
[1mStep[0m  [105/213], [94mLoss[0m : 1.94095
[1mStep[0m  [126/213], [94mLoss[0m : 2.15148
[1mStep[0m  [147/213], [94mLoss[0m : 2.08803
[1mStep[0m  [168/213], [94mLoss[0m : 2.70832
[1mStep[0m  [189/213], [94mLoss[0m : 2.10197
[1mStep[0m  [210/213], [94mLoss[0m : 2.84509

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.553, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87583
[1mStep[0m  [21/213], [94mLoss[0m : 2.51603
[1mStep[0m  [42/213], [94mLoss[0m : 2.36023
[1mStep[0m  [63/213], [94mLoss[0m : 1.74134
[1mStep[0m  [84/213], [94mLoss[0m : 2.29885
[1mStep[0m  [105/213], [94mLoss[0m : 2.16925
[1mStep[0m  [126/213], [94mLoss[0m : 2.19114
[1mStep[0m  [147/213], [94mLoss[0m : 2.63463
[1mStep[0m  [168/213], [94mLoss[0m : 2.11839
[1mStep[0m  [189/213], [94mLoss[0m : 2.25032
[1mStep[0m  [210/213], [94mLoss[0m : 1.75897

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.552, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02533
[1mStep[0m  [21/213], [94mLoss[0m : 1.83065
[1mStep[0m  [42/213], [94mLoss[0m : 1.92304
[1mStep[0m  [63/213], [94mLoss[0m : 2.03298
[1mStep[0m  [84/213], [94mLoss[0m : 2.07476
[1mStep[0m  [105/213], [94mLoss[0m : 2.05492
[1mStep[0m  [126/213], [94mLoss[0m : 1.65749
[1mStep[0m  [147/213], [94mLoss[0m : 1.94662
[1mStep[0m  [168/213], [94mLoss[0m : 2.18175
[1mStep[0m  [189/213], [94mLoss[0m : 2.27907
[1mStep[0m  [210/213], [94mLoss[0m : 2.40994

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01876
[1mStep[0m  [21/213], [94mLoss[0m : 2.10378
[1mStep[0m  [42/213], [94mLoss[0m : 2.27644
[1mStep[0m  [63/213], [94mLoss[0m : 2.14862
[1mStep[0m  [84/213], [94mLoss[0m : 1.75897
[1mStep[0m  [105/213], [94mLoss[0m : 1.81059
[1mStep[0m  [126/213], [94mLoss[0m : 1.88405
[1mStep[0m  [147/213], [94mLoss[0m : 1.91262
[1mStep[0m  [168/213], [94mLoss[0m : 1.70562
[1mStep[0m  [189/213], [94mLoss[0m : 2.22104
[1mStep[0m  [210/213], [94mLoss[0m : 1.83666

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.098, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14685
[1mStep[0m  [21/213], [94mLoss[0m : 1.90865
[1mStep[0m  [42/213], [94mLoss[0m : 2.04457
[1mStep[0m  [63/213], [94mLoss[0m : 2.09779
[1mStep[0m  [84/213], [94mLoss[0m : 2.16516
[1mStep[0m  [105/213], [94mLoss[0m : 1.83176
[1mStep[0m  [126/213], [94mLoss[0m : 2.06444
[1mStep[0m  [147/213], [94mLoss[0m : 1.76207
[1mStep[0m  [168/213], [94mLoss[0m : 2.14648
[1mStep[0m  [189/213], [94mLoss[0m : 2.28486
[1mStep[0m  [210/213], [94mLoss[0m : 1.96517

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17891
[1mStep[0m  [21/213], [94mLoss[0m : 1.91549
[1mStep[0m  [42/213], [94mLoss[0m : 1.86920
[1mStep[0m  [63/213], [94mLoss[0m : 2.04019
[1mStep[0m  [84/213], [94mLoss[0m : 1.73945
[1mStep[0m  [105/213], [94mLoss[0m : 2.07482
[1mStep[0m  [126/213], [94mLoss[0m : 1.70039
[1mStep[0m  [147/213], [94mLoss[0m : 1.77153
[1mStep[0m  [168/213], [94mLoss[0m : 2.11546
[1mStep[0m  [189/213], [94mLoss[0m : 2.47279
[1mStep[0m  [210/213], [94mLoss[0m : 2.09586

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19841
[1mStep[0m  [21/213], [94mLoss[0m : 2.23433
[1mStep[0m  [42/213], [94mLoss[0m : 1.75152
[1mStep[0m  [63/213], [94mLoss[0m : 1.68383
[1mStep[0m  [84/213], [94mLoss[0m : 1.77512
[1mStep[0m  [105/213], [94mLoss[0m : 1.97238
[1mStep[0m  [126/213], [94mLoss[0m : 1.80626
[1mStep[0m  [147/213], [94mLoss[0m : 1.79372
[1mStep[0m  [168/213], [94mLoss[0m : 2.21048
[1mStep[0m  [189/213], [94mLoss[0m : 1.91803
[1mStep[0m  [210/213], [94mLoss[0m : 2.10263

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89457
[1mStep[0m  [21/213], [94mLoss[0m : 1.56354
[1mStep[0m  [42/213], [94mLoss[0m : 1.93391
[1mStep[0m  [63/213], [94mLoss[0m : 1.69099
[1mStep[0m  [84/213], [94mLoss[0m : 2.01844
[1mStep[0m  [105/213], [94mLoss[0m : 2.01502
[1mStep[0m  [126/213], [94mLoss[0m : 2.17801
[1mStep[0m  [147/213], [94mLoss[0m : 2.35687
[1mStep[0m  [168/213], [94mLoss[0m : 2.00605
[1mStep[0m  [189/213], [94mLoss[0m : 2.11147
[1mStep[0m  [210/213], [94mLoss[0m : 1.89603

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39321
[1mStep[0m  [21/213], [94mLoss[0m : 1.94980
[1mStep[0m  [42/213], [94mLoss[0m : 1.61873
[1mStep[0m  [63/213], [94mLoss[0m : 1.77317
[1mStep[0m  [84/213], [94mLoss[0m : 1.97104
[1mStep[0m  [105/213], [94mLoss[0m : 1.93885
[1mStep[0m  [126/213], [94mLoss[0m : 1.88688
[1mStep[0m  [147/213], [94mLoss[0m : 1.84737
[1mStep[0m  [168/213], [94mLoss[0m : 1.97741
[1mStep[0m  [189/213], [94mLoss[0m : 2.01385
[1mStep[0m  [210/213], [94mLoss[0m : 2.19057

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.890, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.60311
[1mStep[0m  [21/213], [94mLoss[0m : 2.06161
[1mStep[0m  [42/213], [94mLoss[0m : 2.01804
[1mStep[0m  [63/213], [94mLoss[0m : 1.68871
[1mStep[0m  [84/213], [94mLoss[0m : 1.93778
[1mStep[0m  [105/213], [94mLoss[0m : 1.58802
[1mStep[0m  [126/213], [94mLoss[0m : 1.61878
[1mStep[0m  [147/213], [94mLoss[0m : 2.03762
[1mStep[0m  [168/213], [94mLoss[0m : 1.98888
[1mStep[0m  [189/213], [94mLoss[0m : 1.72463
[1mStep[0m  [210/213], [94mLoss[0m : 2.24289

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.846, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02856
[1mStep[0m  [21/213], [94mLoss[0m : 1.66384
[1mStep[0m  [42/213], [94mLoss[0m : 1.64486
[1mStep[0m  [63/213], [94mLoss[0m : 1.79607
[1mStep[0m  [84/213], [94mLoss[0m : 1.67895
[1mStep[0m  [105/213], [94mLoss[0m : 1.98505
[1mStep[0m  [126/213], [94mLoss[0m : 1.84141
[1mStep[0m  [147/213], [94mLoss[0m : 1.91189
[1mStep[0m  [168/213], [94mLoss[0m : 1.85616
[1mStep[0m  [189/213], [94mLoss[0m : 2.10924
[1mStep[0m  [210/213], [94mLoss[0m : 1.76379

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.825, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67113
[1mStep[0m  [21/213], [94mLoss[0m : 1.79208
[1mStep[0m  [42/213], [94mLoss[0m : 1.95214
[1mStep[0m  [63/213], [94mLoss[0m : 1.89004
[1mStep[0m  [84/213], [94mLoss[0m : 2.29003
[1mStep[0m  [105/213], [94mLoss[0m : 1.98824
[1mStep[0m  [126/213], [94mLoss[0m : 1.73754
[1mStep[0m  [147/213], [94mLoss[0m : 1.99897
[1mStep[0m  [168/213], [94mLoss[0m : 2.04868
[1mStep[0m  [189/213], [94mLoss[0m : 1.63300
[1mStep[0m  [210/213], [94mLoss[0m : 1.74807

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.15936
[1mStep[0m  [21/213], [94mLoss[0m : 1.67678
[1mStep[0m  [42/213], [94mLoss[0m : 1.94548
[1mStep[0m  [63/213], [94mLoss[0m : 1.70544
[1mStep[0m  [84/213], [94mLoss[0m : 1.87174
[1mStep[0m  [105/213], [94mLoss[0m : 2.05067
[1mStep[0m  [126/213], [94mLoss[0m : 1.76514
[1mStep[0m  [147/213], [94mLoss[0m : 1.68593
[1mStep[0m  [168/213], [94mLoss[0m : 1.74685
[1mStep[0m  [189/213], [94mLoss[0m : 1.66853
[1mStep[0m  [210/213], [94mLoss[0m : 1.60780

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.767, [92mTest[0m: 2.575, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80018
[1mStep[0m  [21/213], [94mLoss[0m : 1.77967
[1mStep[0m  [42/213], [94mLoss[0m : 1.81400
[1mStep[0m  [63/213], [94mLoss[0m : 1.96602
[1mStep[0m  [84/213], [94mLoss[0m : 1.59951
[1mStep[0m  [105/213], [94mLoss[0m : 1.37705
[1mStep[0m  [126/213], [94mLoss[0m : 1.85763
[1mStep[0m  [147/213], [94mLoss[0m : 1.57868
[1mStep[0m  [168/213], [94mLoss[0m : 1.68506
[1mStep[0m  [189/213], [94mLoss[0m : 1.75187
[1mStep[0m  [210/213], [94mLoss[0m : 1.55323

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.735, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.49424
[1mStep[0m  [21/213], [94mLoss[0m : 1.67968
[1mStep[0m  [42/213], [94mLoss[0m : 1.78404
[1mStep[0m  [63/213], [94mLoss[0m : 1.87237
[1mStep[0m  [84/213], [94mLoss[0m : 1.40032
[1mStep[0m  [105/213], [94mLoss[0m : 1.58801
[1mStep[0m  [126/213], [94mLoss[0m : 1.53057
[1mStep[0m  [147/213], [94mLoss[0m : 2.29110
[1mStep[0m  [168/213], [94mLoss[0m : 1.49096
[1mStep[0m  [189/213], [94mLoss[0m : 1.77458
[1mStep[0m  [210/213], [94mLoss[0m : 2.05222

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.50812
[1mStep[0m  [21/213], [94mLoss[0m : 1.57588
[1mStep[0m  [42/213], [94mLoss[0m : 1.67717
[1mStep[0m  [63/213], [94mLoss[0m : 1.43739
[1mStep[0m  [84/213], [94mLoss[0m : 1.77998
[1mStep[0m  [105/213], [94mLoss[0m : 1.67112
[1mStep[0m  [126/213], [94mLoss[0m : 1.43068
[1mStep[0m  [147/213], [94mLoss[0m : 1.84659
[1mStep[0m  [168/213], [94mLoss[0m : 2.06496
[1mStep[0m  [189/213], [94mLoss[0m : 1.45486
[1mStep[0m  [210/213], [94mLoss[0m : 1.71912

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.434, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.59799
[1mStep[0m  [21/213], [94mLoss[0m : 1.62970
[1mStep[0m  [42/213], [94mLoss[0m : 1.58210
[1mStep[0m  [63/213], [94mLoss[0m : 1.49301
[1mStep[0m  [84/213], [94mLoss[0m : 1.39980
[1mStep[0m  [105/213], [94mLoss[0m : 1.53470
[1mStep[0m  [126/213], [94mLoss[0m : 1.83044
[1mStep[0m  [147/213], [94mLoss[0m : 1.77299
[1mStep[0m  [168/213], [94mLoss[0m : 1.42493
[1mStep[0m  [189/213], [94mLoss[0m : 1.82894
[1mStep[0m  [210/213], [94mLoss[0m : 1.41216

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.49095
[1mStep[0m  [21/213], [94mLoss[0m : 1.75032
[1mStep[0m  [42/213], [94mLoss[0m : 1.67277
[1mStep[0m  [63/213], [94mLoss[0m : 1.39305
[1mStep[0m  [84/213], [94mLoss[0m : 1.41469
[1mStep[0m  [105/213], [94mLoss[0m : 1.58333
[1mStep[0m  [126/213], [94mLoss[0m : 1.47371
[1mStep[0m  [147/213], [94mLoss[0m : 1.52782
[1mStep[0m  [168/213], [94mLoss[0m : 1.93404
[1mStep[0m  [189/213], [94mLoss[0m : 1.63819
[1mStep[0m  [210/213], [94mLoss[0m : 1.17956

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.608, [92mTest[0m: 2.431, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47854
[1mStep[0m  [21/213], [94mLoss[0m : 1.48931
[1mStep[0m  [42/213], [94mLoss[0m : 1.88197
[1mStep[0m  [63/213], [94mLoss[0m : 1.75313
[1mStep[0m  [84/213], [94mLoss[0m : 1.85310
[1mStep[0m  [105/213], [94mLoss[0m : 1.47906
[1mStep[0m  [126/213], [94mLoss[0m : 1.53319
[1mStep[0m  [147/213], [94mLoss[0m : 1.68735
[1mStep[0m  [168/213], [94mLoss[0m : 1.54855
[1mStep[0m  [189/213], [94mLoss[0m : 1.52730
[1mStep[0m  [210/213], [94mLoss[0m : 1.56129

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67959
[1mStep[0m  [21/213], [94mLoss[0m : 1.44243
[1mStep[0m  [42/213], [94mLoss[0m : 1.54736
[1mStep[0m  [63/213], [94mLoss[0m : 1.74710
[1mStep[0m  [84/213], [94mLoss[0m : 1.55434
[1mStep[0m  [105/213], [94mLoss[0m : 1.21796
[1mStep[0m  [126/213], [94mLoss[0m : 1.48598
[1mStep[0m  [147/213], [94mLoss[0m : 1.43332
[1mStep[0m  [168/213], [94mLoss[0m : 1.67511
[1mStep[0m  [189/213], [94mLoss[0m : 1.72960
[1mStep[0m  [210/213], [94mLoss[0m : 1.57255

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.563, [92mTest[0m: 2.582, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.13349
[1mStep[0m  [21/213], [94mLoss[0m : 1.20183
[1mStep[0m  [42/213], [94mLoss[0m : 1.57774
[1mStep[0m  [63/213], [94mLoss[0m : 1.47365
[1mStep[0m  [84/213], [94mLoss[0m : 1.59299
[1mStep[0m  [105/213], [94mLoss[0m : 1.71752
[1mStep[0m  [126/213], [94mLoss[0m : 2.15994
[1mStep[0m  [147/213], [94mLoss[0m : 1.53987
[1mStep[0m  [168/213], [94mLoss[0m : 1.50744
[1mStep[0m  [189/213], [94mLoss[0m : 1.59273
[1mStep[0m  [210/213], [94mLoss[0m : 1.77246

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.558, [92mTest[0m: 2.494, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.34484
[1mStep[0m  [21/213], [94mLoss[0m : 1.49950
[1mStep[0m  [42/213], [94mLoss[0m : 1.10605
[1mStep[0m  [63/213], [94mLoss[0m : 1.68150
[1mStep[0m  [84/213], [94mLoss[0m : 1.36414
[1mStep[0m  [105/213], [94mLoss[0m : 1.61126
[1mStep[0m  [126/213], [94mLoss[0m : 1.70526
[1mStep[0m  [147/213], [94mLoss[0m : 1.63981
[1mStep[0m  [168/213], [94mLoss[0m : 1.56026
[1mStep[0m  [189/213], [94mLoss[0m : 1.21011
[1mStep[0m  [210/213], [94mLoss[0m : 1.26418

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.516, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45262
[1mStep[0m  [21/213], [94mLoss[0m : 1.54730
[1mStep[0m  [42/213], [94mLoss[0m : 1.66576
[1mStep[0m  [63/213], [94mLoss[0m : 1.72649
[1mStep[0m  [84/213], [94mLoss[0m : 1.43547
[1mStep[0m  [105/213], [94mLoss[0m : 1.21820
[1mStep[0m  [126/213], [94mLoss[0m : 1.64023
[1mStep[0m  [147/213], [94mLoss[0m : 1.38218
[1mStep[0m  [168/213], [94mLoss[0m : 1.68756
[1mStep[0m  [189/213], [94mLoss[0m : 1.27879
[1mStep[0m  [210/213], [94mLoss[0m : 1.77577

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.509, [92mTest[0m: 2.549, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.28845
[1mStep[0m  [21/213], [94mLoss[0m : 1.41148
[1mStep[0m  [42/213], [94mLoss[0m : 1.71925
[1mStep[0m  [63/213], [94mLoss[0m : 1.90605
[1mStep[0m  [84/213], [94mLoss[0m : 1.48875
[1mStep[0m  [105/213], [94mLoss[0m : 1.37657
[1mStep[0m  [126/213], [94mLoss[0m : 1.23705
[1mStep[0m  [147/213], [94mLoss[0m : 1.53613
[1mStep[0m  [168/213], [94mLoss[0m : 1.71578
[1mStep[0m  [189/213], [94mLoss[0m : 1.41719
[1mStep[0m  [210/213], [94mLoss[0m : 1.28747

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64972
[1mStep[0m  [21/213], [94mLoss[0m : 1.72169
[1mStep[0m  [42/213], [94mLoss[0m : 1.45400
[1mStep[0m  [63/213], [94mLoss[0m : 1.51362
[1mStep[0m  [84/213], [94mLoss[0m : 1.31454
[1mStep[0m  [105/213], [94mLoss[0m : 1.30546
[1mStep[0m  [126/213], [94mLoss[0m : 1.38595
[1mStep[0m  [147/213], [94mLoss[0m : 1.45270
[1mStep[0m  [168/213], [94mLoss[0m : 1.25773
[1mStep[0m  [189/213], [94mLoss[0m : 1.28272
[1mStep[0m  [210/213], [94mLoss[0m : 1.60698

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.469, [92mTest[0m: 2.564, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.535
====================================

Phase 2 - Evaluation MAE:  2.5348766412375108
MAE score P1      2.423962
MAE score P2      2.534877
loss              1.468709
learning_rate     0.002575
batch_size              64
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay        0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 10.36827
[1mStep[0m  [21/213], [94mLoss[0m : 6.77182
[1mStep[0m  [42/213], [94mLoss[0m : 3.40591
[1mStep[0m  [63/213], [94mLoss[0m : 2.78466
[1mStep[0m  [84/213], [94mLoss[0m : 2.71990
[1mStep[0m  [105/213], [94mLoss[0m : 3.32766
[1mStep[0m  [126/213], [94mLoss[0m : 3.57223
[1mStep[0m  [147/213], [94mLoss[0m : 2.93952
[1mStep[0m  [168/213], [94mLoss[0m : 3.03533
[1mStep[0m  [189/213], [94mLoss[0m : 2.81391
[1mStep[0m  [210/213], [94mLoss[0m : 3.09792

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.898, [92mTest[0m: 10.805, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46696
[1mStep[0m  [21/213], [94mLoss[0m : 2.80613
[1mStep[0m  [42/213], [94mLoss[0m : 3.40077
[1mStep[0m  [63/213], [94mLoss[0m : 2.70913
[1mStep[0m  [84/213], [94mLoss[0m : 2.43933
[1mStep[0m  [105/213], [94mLoss[0m : 3.26078
[1mStep[0m  [126/213], [94mLoss[0m : 2.64162
[1mStep[0m  [147/213], [94mLoss[0m : 2.69942
[1mStep[0m  [168/213], [94mLoss[0m : 2.67740
[1mStep[0m  [189/213], [94mLoss[0m : 2.76486
[1mStep[0m  [210/213], [94mLoss[0m : 2.83086

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.867, [92mTest[0m: 2.650, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73665
[1mStep[0m  [21/213], [94mLoss[0m : 3.02139
[1mStep[0m  [42/213], [94mLoss[0m : 2.55008
[1mStep[0m  [63/213], [94mLoss[0m : 2.48986
[1mStep[0m  [84/213], [94mLoss[0m : 2.96769
[1mStep[0m  [105/213], [94mLoss[0m : 2.95228
[1mStep[0m  [126/213], [94mLoss[0m : 3.03109
[1mStep[0m  [147/213], [94mLoss[0m : 3.12056
[1mStep[0m  [168/213], [94mLoss[0m : 2.41318
[1mStep[0m  [189/213], [94mLoss[0m : 2.54769
[1mStep[0m  [210/213], [94mLoss[0m : 2.15837

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.757, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.11601
[1mStep[0m  [21/213], [94mLoss[0m : 2.73999
[1mStep[0m  [42/213], [94mLoss[0m : 2.52064
[1mStep[0m  [63/213], [94mLoss[0m : 2.63738
[1mStep[0m  [84/213], [94mLoss[0m : 2.93170
[1mStep[0m  [105/213], [94mLoss[0m : 3.59671
[1mStep[0m  [126/213], [94mLoss[0m : 2.46777
[1mStep[0m  [147/213], [94mLoss[0m : 2.44837
[1mStep[0m  [168/213], [94mLoss[0m : 2.53243
[1mStep[0m  [189/213], [94mLoss[0m : 2.41432
[1mStep[0m  [210/213], [94mLoss[0m : 2.40398

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58372
[1mStep[0m  [21/213], [94mLoss[0m : 2.75649
[1mStep[0m  [42/213], [94mLoss[0m : 2.65997
[1mStep[0m  [63/213], [94mLoss[0m : 3.24416
[1mStep[0m  [84/213], [94mLoss[0m : 2.35122
[1mStep[0m  [105/213], [94mLoss[0m : 2.39164
[1mStep[0m  [126/213], [94mLoss[0m : 2.93204
[1mStep[0m  [147/213], [94mLoss[0m : 2.95288
[1mStep[0m  [168/213], [94mLoss[0m : 2.28669
[1mStep[0m  [189/213], [94mLoss[0m : 2.59249
[1mStep[0m  [210/213], [94mLoss[0m : 2.58703

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.667, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.71144
[1mStep[0m  [21/213], [94mLoss[0m : 2.16433
[1mStep[0m  [42/213], [94mLoss[0m : 2.22738
[1mStep[0m  [63/213], [94mLoss[0m : 2.66656
[1mStep[0m  [84/213], [94mLoss[0m : 2.80668
[1mStep[0m  [105/213], [94mLoss[0m : 2.50300
[1mStep[0m  [126/213], [94mLoss[0m : 2.23459
[1mStep[0m  [147/213], [94mLoss[0m : 2.52047
[1mStep[0m  [168/213], [94mLoss[0m : 3.06953
[1mStep[0m  [189/213], [94mLoss[0m : 2.79816
[1mStep[0m  [210/213], [94mLoss[0m : 2.66272

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.611, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68733
[1mStep[0m  [21/213], [94mLoss[0m : 2.28241
[1mStep[0m  [42/213], [94mLoss[0m : 2.84133
[1mStep[0m  [63/213], [94mLoss[0m : 2.36144
[1mStep[0m  [84/213], [94mLoss[0m : 2.74167
[1mStep[0m  [105/213], [94mLoss[0m : 2.22966
[1mStep[0m  [126/213], [94mLoss[0m : 2.92972
[1mStep[0m  [147/213], [94mLoss[0m : 2.57711
[1mStep[0m  [168/213], [94mLoss[0m : 2.36619
[1mStep[0m  [189/213], [94mLoss[0m : 2.49700
[1mStep[0m  [210/213], [94mLoss[0m : 2.88695

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.607, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62111
[1mStep[0m  [21/213], [94mLoss[0m : 2.57062
[1mStep[0m  [42/213], [94mLoss[0m : 3.06383
[1mStep[0m  [63/213], [94mLoss[0m : 2.53692
[1mStep[0m  [84/213], [94mLoss[0m : 2.26492
[1mStep[0m  [105/213], [94mLoss[0m : 2.30535
[1mStep[0m  [126/213], [94mLoss[0m : 2.43364
[1mStep[0m  [147/213], [94mLoss[0m : 2.23355
[1mStep[0m  [168/213], [94mLoss[0m : 2.31608
[1mStep[0m  [189/213], [94mLoss[0m : 2.37054
[1mStep[0m  [210/213], [94mLoss[0m : 2.92457

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.567, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53431
[1mStep[0m  [21/213], [94mLoss[0m : 2.46870
[1mStep[0m  [42/213], [94mLoss[0m : 2.39451
[1mStep[0m  [63/213], [94mLoss[0m : 2.56638
[1mStep[0m  [84/213], [94mLoss[0m : 2.93080
[1mStep[0m  [105/213], [94mLoss[0m : 2.81537
[1mStep[0m  [126/213], [94mLoss[0m : 1.94360
[1mStep[0m  [147/213], [94mLoss[0m : 2.43785
[1mStep[0m  [168/213], [94mLoss[0m : 2.19559
[1mStep[0m  [189/213], [94mLoss[0m : 2.14660
[1mStep[0m  [210/213], [94mLoss[0m : 2.63428

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.95041
[1mStep[0m  [21/213], [94mLoss[0m : 2.60335
[1mStep[0m  [42/213], [94mLoss[0m : 2.73409
[1mStep[0m  [63/213], [94mLoss[0m : 2.63357
[1mStep[0m  [84/213], [94mLoss[0m : 2.03699
[1mStep[0m  [105/213], [94mLoss[0m : 3.03290
[1mStep[0m  [126/213], [94mLoss[0m : 2.29350
[1mStep[0m  [147/213], [94mLoss[0m : 2.44196
[1mStep[0m  [168/213], [94mLoss[0m : 2.63779
[1mStep[0m  [189/213], [94mLoss[0m : 2.42104
[1mStep[0m  [210/213], [94mLoss[0m : 2.83204

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68109
[1mStep[0m  [21/213], [94mLoss[0m : 3.30189
[1mStep[0m  [42/213], [94mLoss[0m : 2.27593
[1mStep[0m  [63/213], [94mLoss[0m : 2.02363
[1mStep[0m  [84/213], [94mLoss[0m : 2.70031
[1mStep[0m  [105/213], [94mLoss[0m : 2.37661
[1mStep[0m  [126/213], [94mLoss[0m : 2.44802
[1mStep[0m  [147/213], [94mLoss[0m : 2.08266
[1mStep[0m  [168/213], [94mLoss[0m : 2.69549
[1mStep[0m  [189/213], [94mLoss[0m : 2.12301
[1mStep[0m  [210/213], [94mLoss[0m : 2.45181

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30775
[1mStep[0m  [21/213], [94mLoss[0m : 2.63893
[1mStep[0m  [42/213], [94mLoss[0m : 2.53680
[1mStep[0m  [63/213], [94mLoss[0m : 2.26840
[1mStep[0m  [84/213], [94mLoss[0m : 1.87415
[1mStep[0m  [105/213], [94mLoss[0m : 2.37132
[1mStep[0m  [126/213], [94mLoss[0m : 2.12935
[1mStep[0m  [147/213], [94mLoss[0m : 2.48171
[1mStep[0m  [168/213], [94mLoss[0m : 2.31700
[1mStep[0m  [189/213], [94mLoss[0m : 3.00681
[1mStep[0m  [210/213], [94mLoss[0m : 2.54386

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45918
[1mStep[0m  [21/213], [94mLoss[0m : 2.08287
[1mStep[0m  [42/213], [94mLoss[0m : 2.36718
[1mStep[0m  [63/213], [94mLoss[0m : 2.30051
[1mStep[0m  [84/213], [94mLoss[0m : 2.44053
[1mStep[0m  [105/213], [94mLoss[0m : 2.56771
[1mStep[0m  [126/213], [94mLoss[0m : 2.39354
[1mStep[0m  [147/213], [94mLoss[0m : 2.26330
[1mStep[0m  [168/213], [94mLoss[0m : 2.48145
[1mStep[0m  [189/213], [94mLoss[0m : 2.36941
[1mStep[0m  [210/213], [94mLoss[0m : 2.22737

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.359, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42756
[1mStep[0m  [21/213], [94mLoss[0m : 2.66148
[1mStep[0m  [42/213], [94mLoss[0m : 2.58139
[1mStep[0m  [63/213], [94mLoss[0m : 2.18657
[1mStep[0m  [84/213], [94mLoss[0m : 2.62222
[1mStep[0m  [105/213], [94mLoss[0m : 2.38169
[1mStep[0m  [126/213], [94mLoss[0m : 2.83591
[1mStep[0m  [147/213], [94mLoss[0m : 2.58298
[1mStep[0m  [168/213], [94mLoss[0m : 2.34650
[1mStep[0m  [189/213], [94mLoss[0m : 2.42010
[1mStep[0m  [210/213], [94mLoss[0m : 2.49655

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43165
[1mStep[0m  [21/213], [94mLoss[0m : 2.40451
[1mStep[0m  [42/213], [94mLoss[0m : 2.38448
[1mStep[0m  [63/213], [94mLoss[0m : 2.58117
[1mStep[0m  [84/213], [94mLoss[0m : 2.46857
[1mStep[0m  [105/213], [94mLoss[0m : 2.79170
[1mStep[0m  [126/213], [94mLoss[0m : 2.43779
[1mStep[0m  [147/213], [94mLoss[0m : 2.38785
[1mStep[0m  [168/213], [94mLoss[0m : 2.48041
[1mStep[0m  [189/213], [94mLoss[0m : 2.56360
[1mStep[0m  [210/213], [94mLoss[0m : 2.56703

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.352, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.41050
[1mStep[0m  [21/213], [94mLoss[0m : 2.61015
[1mStep[0m  [42/213], [94mLoss[0m : 2.30657
[1mStep[0m  [63/213], [94mLoss[0m : 1.93552
[1mStep[0m  [84/213], [94mLoss[0m : 2.47679
[1mStep[0m  [105/213], [94mLoss[0m : 2.21780
[1mStep[0m  [126/213], [94mLoss[0m : 2.29162
[1mStep[0m  [147/213], [94mLoss[0m : 2.94048
[1mStep[0m  [168/213], [94mLoss[0m : 2.59719
[1mStep[0m  [189/213], [94mLoss[0m : 2.44506
[1mStep[0m  [210/213], [94mLoss[0m : 2.11163

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43496
[1mStep[0m  [21/213], [94mLoss[0m : 2.44915
[1mStep[0m  [42/213], [94mLoss[0m : 2.43596
[1mStep[0m  [63/213], [94mLoss[0m : 2.51251
[1mStep[0m  [84/213], [94mLoss[0m : 2.67583
[1mStep[0m  [105/213], [94mLoss[0m : 2.79576
[1mStep[0m  [126/213], [94mLoss[0m : 2.16688
[1mStep[0m  [147/213], [94mLoss[0m : 2.46589
[1mStep[0m  [168/213], [94mLoss[0m : 2.05051
[1mStep[0m  [189/213], [94mLoss[0m : 2.75229
[1mStep[0m  [210/213], [94mLoss[0m : 2.53573

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19069
[1mStep[0m  [21/213], [94mLoss[0m : 2.22365
[1mStep[0m  [42/213], [94mLoss[0m : 2.38619
[1mStep[0m  [63/213], [94mLoss[0m : 2.13536
[1mStep[0m  [84/213], [94mLoss[0m : 2.40703
[1mStep[0m  [105/213], [94mLoss[0m : 2.57199
[1mStep[0m  [126/213], [94mLoss[0m : 2.26403
[1mStep[0m  [147/213], [94mLoss[0m : 2.05677
[1mStep[0m  [168/213], [94mLoss[0m : 2.35369
[1mStep[0m  [189/213], [94mLoss[0m : 2.72959
[1mStep[0m  [210/213], [94mLoss[0m : 2.16711

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.53798
[1mStep[0m  [21/213], [94mLoss[0m : 2.47791
[1mStep[0m  [42/213], [94mLoss[0m : 2.11434
[1mStep[0m  [63/213], [94mLoss[0m : 2.52981
[1mStep[0m  [84/213], [94mLoss[0m : 2.61921
[1mStep[0m  [105/213], [94mLoss[0m : 2.33918
[1mStep[0m  [126/213], [94mLoss[0m : 2.74375
[1mStep[0m  [147/213], [94mLoss[0m : 2.76400
[1mStep[0m  [168/213], [94mLoss[0m : 2.29280
[1mStep[0m  [189/213], [94mLoss[0m : 2.48889
[1mStep[0m  [210/213], [94mLoss[0m : 2.49444

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.349, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.82969
[1mStep[0m  [21/213], [94mLoss[0m : 2.23820
[1mStep[0m  [42/213], [94mLoss[0m : 2.76421
[1mStep[0m  [63/213], [94mLoss[0m : 2.44990
[1mStep[0m  [84/213], [94mLoss[0m : 2.25533
[1mStep[0m  [105/213], [94mLoss[0m : 2.62515
[1mStep[0m  [126/213], [94mLoss[0m : 2.45439
[1mStep[0m  [147/213], [94mLoss[0m : 2.57070
[1mStep[0m  [168/213], [94mLoss[0m : 2.45704
[1mStep[0m  [189/213], [94mLoss[0m : 2.67529
[1mStep[0m  [210/213], [94mLoss[0m : 2.05203

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.351, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25172
[1mStep[0m  [21/213], [94mLoss[0m : 2.67225
[1mStep[0m  [42/213], [94mLoss[0m : 2.57556
[1mStep[0m  [63/213], [94mLoss[0m : 2.66137
[1mStep[0m  [84/213], [94mLoss[0m : 2.79697
[1mStep[0m  [105/213], [94mLoss[0m : 2.38451
[1mStep[0m  [126/213], [94mLoss[0m : 2.14296
[1mStep[0m  [147/213], [94mLoss[0m : 2.19663
[1mStep[0m  [168/213], [94mLoss[0m : 2.42067
[1mStep[0m  [189/213], [94mLoss[0m : 3.00432
[1mStep[0m  [210/213], [94mLoss[0m : 2.37982

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.346, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.03504
[1mStep[0m  [21/213], [94mLoss[0m : 2.35802
[1mStep[0m  [42/213], [94mLoss[0m : 2.43237
[1mStep[0m  [63/213], [94mLoss[0m : 2.38793
[1mStep[0m  [84/213], [94mLoss[0m : 2.40139
[1mStep[0m  [105/213], [94mLoss[0m : 1.84994
[1mStep[0m  [126/213], [94mLoss[0m : 2.21217
[1mStep[0m  [147/213], [94mLoss[0m : 2.38720
[1mStep[0m  [168/213], [94mLoss[0m : 2.31762
[1mStep[0m  [189/213], [94mLoss[0m : 1.88943
[1mStep[0m  [210/213], [94mLoss[0m : 2.17361

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31908
[1mStep[0m  [21/213], [94mLoss[0m : 2.34303
[1mStep[0m  [42/213], [94mLoss[0m : 2.49206
[1mStep[0m  [63/213], [94mLoss[0m : 2.44505
[1mStep[0m  [84/213], [94mLoss[0m : 2.63679
[1mStep[0m  [105/213], [94mLoss[0m : 2.11134
[1mStep[0m  [126/213], [94mLoss[0m : 2.43274
[1mStep[0m  [147/213], [94mLoss[0m : 2.44069
[1mStep[0m  [168/213], [94mLoss[0m : 2.64464
[1mStep[0m  [189/213], [94mLoss[0m : 1.90061
[1mStep[0m  [210/213], [94mLoss[0m : 2.39096

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.338, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48031
[1mStep[0m  [21/213], [94mLoss[0m : 2.42455
[1mStep[0m  [42/213], [94mLoss[0m : 2.25918
[1mStep[0m  [63/213], [94mLoss[0m : 2.74457
[1mStep[0m  [84/213], [94mLoss[0m : 2.39184
[1mStep[0m  [105/213], [94mLoss[0m : 2.57539
[1mStep[0m  [126/213], [94mLoss[0m : 2.19817
[1mStep[0m  [147/213], [94mLoss[0m : 2.37460
[1mStep[0m  [168/213], [94mLoss[0m : 1.89072
[1mStep[0m  [189/213], [94mLoss[0m : 2.52160
[1mStep[0m  [210/213], [94mLoss[0m : 2.81274

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43146
[1mStep[0m  [21/213], [94mLoss[0m : 2.11120
[1mStep[0m  [42/213], [94mLoss[0m : 2.01924
[1mStep[0m  [63/213], [94mLoss[0m : 2.73848
[1mStep[0m  [84/213], [94mLoss[0m : 2.74996
[1mStep[0m  [105/213], [94mLoss[0m : 2.24003
[1mStep[0m  [126/213], [94mLoss[0m : 2.19729
[1mStep[0m  [147/213], [94mLoss[0m : 2.31812
[1mStep[0m  [168/213], [94mLoss[0m : 2.49657
[1mStep[0m  [189/213], [94mLoss[0m : 2.28139
[1mStep[0m  [210/213], [94mLoss[0m : 2.22680

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.341, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35322
[1mStep[0m  [21/213], [94mLoss[0m : 1.79078
[1mStep[0m  [42/213], [94mLoss[0m : 2.29019
[1mStep[0m  [63/213], [94mLoss[0m : 2.29195
[1mStep[0m  [84/213], [94mLoss[0m : 2.43048
[1mStep[0m  [105/213], [94mLoss[0m : 1.85089
[1mStep[0m  [126/213], [94mLoss[0m : 2.65490
[1mStep[0m  [147/213], [94mLoss[0m : 2.36867
[1mStep[0m  [168/213], [94mLoss[0m : 2.54847
[1mStep[0m  [189/213], [94mLoss[0m : 2.14566
[1mStep[0m  [210/213], [94mLoss[0m : 2.20180

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28418
[1mStep[0m  [21/213], [94mLoss[0m : 2.28472
[1mStep[0m  [42/213], [94mLoss[0m : 2.00420
[1mStep[0m  [63/213], [94mLoss[0m : 2.05788
[1mStep[0m  [84/213], [94mLoss[0m : 2.21335
[1mStep[0m  [105/213], [94mLoss[0m : 2.33837
[1mStep[0m  [126/213], [94mLoss[0m : 2.65657
[1mStep[0m  [147/213], [94mLoss[0m : 2.10468
[1mStep[0m  [168/213], [94mLoss[0m : 2.35349
[1mStep[0m  [189/213], [94mLoss[0m : 2.65810
[1mStep[0m  [210/213], [94mLoss[0m : 2.55480

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.329, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45286
[1mStep[0m  [21/213], [94mLoss[0m : 2.74379
[1mStep[0m  [42/213], [94mLoss[0m : 2.65024
[1mStep[0m  [63/213], [94mLoss[0m : 2.31157
[1mStep[0m  [84/213], [94mLoss[0m : 2.01522
[1mStep[0m  [105/213], [94mLoss[0m : 2.38887
[1mStep[0m  [126/213], [94mLoss[0m : 2.58894
[1mStep[0m  [147/213], [94mLoss[0m : 2.33438
[1mStep[0m  [168/213], [94mLoss[0m : 2.42435
[1mStep[0m  [189/213], [94mLoss[0m : 1.77811
[1mStep[0m  [210/213], [94mLoss[0m : 2.66259

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55463
[1mStep[0m  [21/213], [94mLoss[0m : 2.28786
[1mStep[0m  [42/213], [94mLoss[0m : 2.11669
[1mStep[0m  [63/213], [94mLoss[0m : 2.33529
[1mStep[0m  [84/213], [94mLoss[0m : 3.11885
[1mStep[0m  [105/213], [94mLoss[0m : 2.29141
[1mStep[0m  [126/213], [94mLoss[0m : 2.01571
[1mStep[0m  [147/213], [94mLoss[0m : 2.20653
[1mStep[0m  [168/213], [94mLoss[0m : 2.05338
[1mStep[0m  [189/213], [94mLoss[0m : 2.00311
[1mStep[0m  [210/213], [94mLoss[0m : 2.35922

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.67159
[1mStep[0m  [21/213], [94mLoss[0m : 2.31173
[1mStep[0m  [42/213], [94mLoss[0m : 2.45288
[1mStep[0m  [63/213], [94mLoss[0m : 2.45253
[1mStep[0m  [84/213], [94mLoss[0m : 2.32672
[1mStep[0m  [105/213], [94mLoss[0m : 2.67824
[1mStep[0m  [126/213], [94mLoss[0m : 1.93914
[1mStep[0m  [147/213], [94mLoss[0m : 2.16503
[1mStep[0m  [168/213], [94mLoss[0m : 2.37017
[1mStep[0m  [189/213], [94mLoss[0m : 2.57059
[1mStep[0m  [210/213], [94mLoss[0m : 2.32796

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.388, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3305634957439496
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 2.32004
[1mStep[0m  [21/213], [94mLoss[0m : 2.44429
[1mStep[0m  [42/213], [94mLoss[0m : 2.31588
[1mStep[0m  [63/213], [94mLoss[0m : 2.44355
[1mStep[0m  [84/213], [94mLoss[0m : 2.40274
[1mStep[0m  [105/213], [94mLoss[0m : 2.17087
[1mStep[0m  [126/213], [94mLoss[0m : 2.67297
[1mStep[0m  [147/213], [94mLoss[0m : 2.03331
[1mStep[0m  [168/213], [94mLoss[0m : 2.21063
[1mStep[0m  [189/213], [94mLoss[0m : 2.61219
[1mStep[0m  [210/213], [94mLoss[0m : 3.18162

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35775
[1mStep[0m  [21/213], [94mLoss[0m : 2.52492
[1mStep[0m  [42/213], [94mLoss[0m : 2.37964
[1mStep[0m  [63/213], [94mLoss[0m : 2.16809
[1mStep[0m  [84/213], [94mLoss[0m : 2.26638
[1mStep[0m  [105/213], [94mLoss[0m : 2.30000
[1mStep[0m  [126/213], [94mLoss[0m : 2.10950
[1mStep[0m  [147/213], [94mLoss[0m : 2.40687
[1mStep[0m  [168/213], [94mLoss[0m : 2.53761
[1mStep[0m  [189/213], [94mLoss[0m : 2.04353
[1mStep[0m  [210/213], [94mLoss[0m : 2.04098

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36515
[1mStep[0m  [21/213], [94mLoss[0m : 2.45947
[1mStep[0m  [42/213], [94mLoss[0m : 2.78254
[1mStep[0m  [63/213], [94mLoss[0m : 2.89083
[1mStep[0m  [84/213], [94mLoss[0m : 2.66397
[1mStep[0m  [105/213], [94mLoss[0m : 2.35415
[1mStep[0m  [126/213], [94mLoss[0m : 2.45766
[1mStep[0m  [147/213], [94mLoss[0m : 2.10981
[1mStep[0m  [168/213], [94mLoss[0m : 2.37877
[1mStep[0m  [189/213], [94mLoss[0m : 2.38161
[1mStep[0m  [210/213], [94mLoss[0m : 2.64354

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37156
[1mStep[0m  [21/213], [94mLoss[0m : 2.08128
[1mStep[0m  [42/213], [94mLoss[0m : 2.26296
[1mStep[0m  [63/213], [94mLoss[0m : 2.10766
[1mStep[0m  [84/213], [94mLoss[0m : 2.28537
[1mStep[0m  [105/213], [94mLoss[0m : 2.23957
[1mStep[0m  [126/213], [94mLoss[0m : 2.08727
[1mStep[0m  [147/213], [94mLoss[0m : 2.18100
[1mStep[0m  [168/213], [94mLoss[0m : 2.12960
[1mStep[0m  [189/213], [94mLoss[0m : 2.47452
[1mStep[0m  [210/213], [94mLoss[0m : 2.18869

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.85145
[1mStep[0m  [21/213], [94mLoss[0m : 2.36119
[1mStep[0m  [42/213], [94mLoss[0m : 2.25790
[1mStep[0m  [63/213], [94mLoss[0m : 2.14671
[1mStep[0m  [84/213], [94mLoss[0m : 2.09684
[1mStep[0m  [105/213], [94mLoss[0m : 2.25570
[1mStep[0m  [126/213], [94mLoss[0m : 2.37171
[1mStep[0m  [147/213], [94mLoss[0m : 2.07847
[1mStep[0m  [168/213], [94mLoss[0m : 2.30479
[1mStep[0m  [189/213], [94mLoss[0m : 1.76859
[1mStep[0m  [210/213], [94mLoss[0m : 2.22086

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.02955
[1mStep[0m  [21/213], [94mLoss[0m : 2.05625
[1mStep[0m  [42/213], [94mLoss[0m : 2.31742
[1mStep[0m  [63/213], [94mLoss[0m : 1.96701
[1mStep[0m  [84/213], [94mLoss[0m : 2.09856
[1mStep[0m  [105/213], [94mLoss[0m : 2.43567
[1mStep[0m  [126/213], [94mLoss[0m : 2.22909
[1mStep[0m  [147/213], [94mLoss[0m : 2.02623
[1mStep[0m  [168/213], [94mLoss[0m : 2.30476
[1mStep[0m  [189/213], [94mLoss[0m : 1.93254
[1mStep[0m  [210/213], [94mLoss[0m : 1.88881

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.125, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.11531
[1mStep[0m  [21/213], [94mLoss[0m : 1.97014
[1mStep[0m  [42/213], [94mLoss[0m : 2.10278
[1mStep[0m  [63/213], [94mLoss[0m : 2.18976
[1mStep[0m  [84/213], [94mLoss[0m : 1.89799
[1mStep[0m  [105/213], [94mLoss[0m : 1.80598
[1mStep[0m  [126/213], [94mLoss[0m : 2.07187
[1mStep[0m  [147/213], [94mLoss[0m : 2.18810
[1mStep[0m  [168/213], [94mLoss[0m : 2.10039
[1mStep[0m  [189/213], [94mLoss[0m : 2.14899
[1mStep[0m  [210/213], [94mLoss[0m : 2.18693

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12996
[1mStep[0m  [21/213], [94mLoss[0m : 1.67596
[1mStep[0m  [42/213], [94mLoss[0m : 2.28307
[1mStep[0m  [63/213], [94mLoss[0m : 1.67589
[1mStep[0m  [84/213], [94mLoss[0m : 1.99802
[1mStep[0m  [105/213], [94mLoss[0m : 1.77769
[1mStep[0m  [126/213], [94mLoss[0m : 1.92772
[1mStep[0m  [147/213], [94mLoss[0m : 2.12712
[1mStep[0m  [168/213], [94mLoss[0m : 2.11626
[1mStep[0m  [189/213], [94mLoss[0m : 2.33290
[1mStep[0m  [210/213], [94mLoss[0m : 2.42364

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.002, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.00991
[1mStep[0m  [21/213], [94mLoss[0m : 1.84298
[1mStep[0m  [42/213], [94mLoss[0m : 2.18039
[1mStep[0m  [63/213], [94mLoss[0m : 2.28664
[1mStep[0m  [84/213], [94mLoss[0m : 2.08868
[1mStep[0m  [105/213], [94mLoss[0m : 1.99851
[1mStep[0m  [126/213], [94mLoss[0m : 1.79493
[1mStep[0m  [147/213], [94mLoss[0m : 1.79428
[1mStep[0m  [168/213], [94mLoss[0m : 1.82494
[1mStep[0m  [189/213], [94mLoss[0m : 2.32579
[1mStep[0m  [210/213], [94mLoss[0m : 2.63723

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.984, [92mTest[0m: 2.517, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.07137
[1mStep[0m  [21/213], [94mLoss[0m : 2.32344
[1mStep[0m  [42/213], [94mLoss[0m : 1.73019
[1mStep[0m  [63/213], [94mLoss[0m : 1.76422
[1mStep[0m  [84/213], [94mLoss[0m : 1.83530
[1mStep[0m  [105/213], [94mLoss[0m : 2.23042
[1mStep[0m  [126/213], [94mLoss[0m : 1.69095
[1mStep[0m  [147/213], [94mLoss[0m : 1.88622
[1mStep[0m  [168/213], [94mLoss[0m : 2.01687
[1mStep[0m  [189/213], [94mLoss[0m : 2.17851
[1mStep[0m  [210/213], [94mLoss[0m : 2.21058

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.936, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12920
[1mStep[0m  [21/213], [94mLoss[0m : 2.29726
[1mStep[0m  [42/213], [94mLoss[0m : 1.73492
[1mStep[0m  [63/213], [94mLoss[0m : 2.29373
[1mStep[0m  [84/213], [94mLoss[0m : 2.07962
[1mStep[0m  [105/213], [94mLoss[0m : 1.86203
[1mStep[0m  [126/213], [94mLoss[0m : 1.90468
[1mStep[0m  [147/213], [94mLoss[0m : 1.69288
[1mStep[0m  [168/213], [94mLoss[0m : 1.77419
[1mStep[0m  [189/213], [94mLoss[0m : 1.60653
[1mStep[0m  [210/213], [94mLoss[0m : 1.78383

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.905, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.76761
[1mStep[0m  [21/213], [94mLoss[0m : 1.64285
[1mStep[0m  [42/213], [94mLoss[0m : 1.63390
[1mStep[0m  [63/213], [94mLoss[0m : 1.90900
[1mStep[0m  [84/213], [94mLoss[0m : 1.72265
[1mStep[0m  [105/213], [94mLoss[0m : 1.89132
[1mStep[0m  [126/213], [94mLoss[0m : 1.75778
[1mStep[0m  [147/213], [94mLoss[0m : 2.03204
[1mStep[0m  [168/213], [94mLoss[0m : 1.80407
[1mStep[0m  [189/213], [94mLoss[0m : 1.73492
[1mStep[0m  [210/213], [94mLoss[0m : 2.31643

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.45482
[1mStep[0m  [21/213], [94mLoss[0m : 1.25873
[1mStep[0m  [42/213], [94mLoss[0m : 2.00703
[1mStep[0m  [63/213], [94mLoss[0m : 1.60602
[1mStep[0m  [84/213], [94mLoss[0m : 1.94433
[1mStep[0m  [105/213], [94mLoss[0m : 2.03518
[1mStep[0m  [126/213], [94mLoss[0m : 1.79896
[1mStep[0m  [147/213], [94mLoss[0m : 1.88149
[1mStep[0m  [168/213], [94mLoss[0m : 1.54378
[1mStep[0m  [189/213], [94mLoss[0m : 1.90057
[1mStep[0m  [210/213], [94mLoss[0m : 1.75295

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.75699
[1mStep[0m  [21/213], [94mLoss[0m : 1.58403
[1mStep[0m  [42/213], [94mLoss[0m : 1.60502
[1mStep[0m  [63/213], [94mLoss[0m : 2.20271
[1mStep[0m  [84/213], [94mLoss[0m : 1.56558
[1mStep[0m  [105/213], [94mLoss[0m : 2.18645
[1mStep[0m  [126/213], [94mLoss[0m : 1.96912
[1mStep[0m  [147/213], [94mLoss[0m : 1.66676
[1mStep[0m  [168/213], [94mLoss[0m : 2.04022
[1mStep[0m  [189/213], [94mLoss[0m : 1.65609
[1mStep[0m  [210/213], [94mLoss[0m : 1.94527

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.96539
[1mStep[0m  [21/213], [94mLoss[0m : 1.78017
[1mStep[0m  [42/213], [94mLoss[0m : 1.93196
[1mStep[0m  [63/213], [94mLoss[0m : 1.94047
[1mStep[0m  [84/213], [94mLoss[0m : 1.69976
[1mStep[0m  [105/213], [94mLoss[0m : 1.68705
[1mStep[0m  [126/213], [94mLoss[0m : 2.00634
[1mStep[0m  [147/213], [94mLoss[0m : 1.70253
[1mStep[0m  [168/213], [94mLoss[0m : 1.92032
[1mStep[0m  [189/213], [94mLoss[0m : 1.84089
[1mStep[0m  [210/213], [94mLoss[0m : 1.34056

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.786, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56234
[1mStep[0m  [21/213], [94mLoss[0m : 1.68621
[1mStep[0m  [42/213], [94mLoss[0m : 1.61487
[1mStep[0m  [63/213], [94mLoss[0m : 1.60174
[1mStep[0m  [84/213], [94mLoss[0m : 1.62078
[1mStep[0m  [105/213], [94mLoss[0m : 1.70784
[1mStep[0m  [126/213], [94mLoss[0m : 1.72801
[1mStep[0m  [147/213], [94mLoss[0m : 1.83553
[1mStep[0m  [168/213], [94mLoss[0m : 2.04578
[1mStep[0m  [189/213], [94mLoss[0m : 1.76028
[1mStep[0m  [210/213], [94mLoss[0m : 1.72273

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.745, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.90975
[1mStep[0m  [21/213], [94mLoss[0m : 1.65545
[1mStep[0m  [42/213], [94mLoss[0m : 1.69655
[1mStep[0m  [63/213], [94mLoss[0m : 2.06283
[1mStep[0m  [84/213], [94mLoss[0m : 1.87133
[1mStep[0m  [105/213], [94mLoss[0m : 1.87523
[1mStep[0m  [126/213], [94mLoss[0m : 1.79101
[1mStep[0m  [147/213], [94mLoss[0m : 1.64129
[1mStep[0m  [168/213], [94mLoss[0m : 1.85022
[1mStep[0m  [189/213], [94mLoss[0m : 1.55970
[1mStep[0m  [210/213], [94mLoss[0m : 1.78938

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.725, [92mTest[0m: 2.525, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.43414
[1mStep[0m  [21/213], [94mLoss[0m : 1.58551
[1mStep[0m  [42/213], [94mLoss[0m : 1.66664
[1mStep[0m  [63/213], [94mLoss[0m : 1.50849
[1mStep[0m  [84/213], [94mLoss[0m : 1.66967
[1mStep[0m  [105/213], [94mLoss[0m : 1.89180
[1mStep[0m  [126/213], [94mLoss[0m : 1.40813
[1mStep[0m  [147/213], [94mLoss[0m : 1.95017
[1mStep[0m  [168/213], [94mLoss[0m : 2.00500
[1mStep[0m  [189/213], [94mLoss[0m : 1.90993
[1mStep[0m  [210/213], [94mLoss[0m : 1.95549

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.702, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.36854
[1mStep[0m  [21/213], [94mLoss[0m : 1.52696
[1mStep[0m  [42/213], [94mLoss[0m : 1.71627
[1mStep[0m  [63/213], [94mLoss[0m : 1.74720
[1mStep[0m  [84/213], [94mLoss[0m : 1.84780
[1mStep[0m  [105/213], [94mLoss[0m : 1.61257
[1mStep[0m  [126/213], [94mLoss[0m : 1.69776
[1mStep[0m  [147/213], [94mLoss[0m : 1.79826
[1mStep[0m  [168/213], [94mLoss[0m : 1.64841
[1mStep[0m  [189/213], [94mLoss[0m : 1.26590
[1mStep[0m  [210/213], [94mLoss[0m : 1.81930

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.668, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64343
[1mStep[0m  [21/213], [94mLoss[0m : 1.43087
[1mStep[0m  [42/213], [94mLoss[0m : 1.53358
[1mStep[0m  [63/213], [94mLoss[0m : 1.33699
[1mStep[0m  [84/213], [94mLoss[0m : 1.32641
[1mStep[0m  [105/213], [94mLoss[0m : 1.72908
[1mStep[0m  [126/213], [94mLoss[0m : 1.66735
[1mStep[0m  [147/213], [94mLoss[0m : 1.49310
[1mStep[0m  [168/213], [94mLoss[0m : 1.58413
[1mStep[0m  [189/213], [94mLoss[0m : 1.63096
[1mStep[0m  [210/213], [94mLoss[0m : 1.84745

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.638, [92mTest[0m: 2.475, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.76173
[1mStep[0m  [21/213], [94mLoss[0m : 1.53210
[1mStep[0m  [42/213], [94mLoss[0m : 1.31814
[1mStep[0m  [63/213], [94mLoss[0m : 1.87786
[1mStep[0m  [84/213], [94mLoss[0m : 1.88252
[1mStep[0m  [105/213], [94mLoss[0m : 1.61221
[1mStep[0m  [126/213], [94mLoss[0m : 1.55800
[1mStep[0m  [147/213], [94mLoss[0m : 1.69181
[1mStep[0m  [168/213], [94mLoss[0m : 1.43801
[1mStep[0m  [189/213], [94mLoss[0m : 1.45514
[1mStep[0m  [210/213], [94mLoss[0m : 1.70289

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.618, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.51911
[1mStep[0m  [21/213], [94mLoss[0m : 1.27586
[1mStep[0m  [42/213], [94mLoss[0m : 1.31668
[1mStep[0m  [63/213], [94mLoss[0m : 1.58492
[1mStep[0m  [84/213], [94mLoss[0m : 1.41124
[1mStep[0m  [105/213], [94mLoss[0m : 1.42023
[1mStep[0m  [126/213], [94mLoss[0m : 1.70639
[1mStep[0m  [147/213], [94mLoss[0m : 1.57980
[1mStep[0m  [168/213], [94mLoss[0m : 1.40011
[1mStep[0m  [189/213], [94mLoss[0m : 1.80768
[1mStep[0m  [210/213], [94mLoss[0m : 1.97038

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.574, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.59960
[1mStep[0m  [21/213], [94mLoss[0m : 1.60048
[1mStep[0m  [42/213], [94mLoss[0m : 1.29095
[1mStep[0m  [63/213], [94mLoss[0m : 1.35245
[1mStep[0m  [84/213], [94mLoss[0m : 1.42732
[1mStep[0m  [105/213], [94mLoss[0m : 1.80343
[1mStep[0m  [126/213], [94mLoss[0m : 1.79168
[1mStep[0m  [147/213], [94mLoss[0m : 1.75110
[1mStep[0m  [168/213], [94mLoss[0m : 1.56805
[1mStep[0m  [189/213], [94mLoss[0m : 1.61755
[1mStep[0m  [210/213], [94mLoss[0m : 1.60531

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.549, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45677
[1mStep[0m  [21/213], [94mLoss[0m : 1.49335
[1mStep[0m  [42/213], [94mLoss[0m : 1.62120
[1mStep[0m  [63/213], [94mLoss[0m : 1.77417
[1mStep[0m  [84/213], [94mLoss[0m : 1.75563
[1mStep[0m  [105/213], [94mLoss[0m : 1.30683
[1mStep[0m  [126/213], [94mLoss[0m : 1.50760
[1mStep[0m  [147/213], [94mLoss[0m : 1.38625
[1mStep[0m  [168/213], [94mLoss[0m : 1.42198
[1mStep[0m  [189/213], [94mLoss[0m : 1.40695
[1mStep[0m  [210/213], [94mLoss[0m : 1.30210

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.536, [92mTest[0m: 2.483, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64517
[1mStep[0m  [21/213], [94mLoss[0m : 1.66128
[1mStep[0m  [42/213], [94mLoss[0m : 1.30798
[1mStep[0m  [63/213], [94mLoss[0m : 1.39083
[1mStep[0m  [84/213], [94mLoss[0m : 1.76119
[1mStep[0m  [105/213], [94mLoss[0m : 1.61596
[1mStep[0m  [126/213], [94mLoss[0m : 1.53215
[1mStep[0m  [147/213], [94mLoss[0m : 1.43549
[1mStep[0m  [168/213], [94mLoss[0m : 1.50948
[1mStep[0m  [189/213], [94mLoss[0m : 1.14137
[1mStep[0m  [210/213], [94mLoss[0m : 1.50907

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.530, [92mTest[0m: 2.519, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73274
[1mStep[0m  [21/213], [94mLoss[0m : 1.49868
[1mStep[0m  [42/213], [94mLoss[0m : 1.78917
[1mStep[0m  [63/213], [94mLoss[0m : 1.19955
[1mStep[0m  [84/213], [94mLoss[0m : 1.73564
[1mStep[0m  [105/213], [94mLoss[0m : 1.38551
[1mStep[0m  [126/213], [94mLoss[0m : 1.73016
[1mStep[0m  [147/213], [94mLoss[0m : 2.14550
[1mStep[0m  [168/213], [94mLoss[0m : 1.54140
[1mStep[0m  [189/213], [94mLoss[0m : 1.52181
[1mStep[0m  [210/213], [94mLoss[0m : 1.45688

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.519, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.35377
[1mStep[0m  [21/213], [94mLoss[0m : 1.67075
[1mStep[0m  [42/213], [94mLoss[0m : 1.27585
[1mStep[0m  [63/213], [94mLoss[0m : 1.41916
[1mStep[0m  [84/213], [94mLoss[0m : 1.11775
[1mStep[0m  [105/213], [94mLoss[0m : 1.50055
[1mStep[0m  [126/213], [94mLoss[0m : 1.34334
[1mStep[0m  [147/213], [94mLoss[0m : 1.47637
[1mStep[0m  [168/213], [94mLoss[0m : 1.37106
[1mStep[0m  [189/213], [94mLoss[0m : 1.84623
[1mStep[0m  [210/213], [94mLoss[0m : 1.58794

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.502, [92mTest[0m: 2.498, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.31374
[1mStep[0m  [21/213], [94mLoss[0m : 1.22845
[1mStep[0m  [42/213], [94mLoss[0m : 1.57709
[1mStep[0m  [63/213], [94mLoss[0m : 1.47661
[1mStep[0m  [84/213], [94mLoss[0m : 1.35169
[1mStep[0m  [105/213], [94mLoss[0m : 1.65172
[1mStep[0m  [126/213], [94mLoss[0m : 1.34088
[1mStep[0m  [147/213], [94mLoss[0m : 1.49688
[1mStep[0m  [168/213], [94mLoss[0m : 1.57421
[1mStep[0m  [189/213], [94mLoss[0m : 1.51526
[1mStep[0m  [210/213], [94mLoss[0m : 1.40006

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.37415
[1mStep[0m  [21/213], [94mLoss[0m : 1.44062
[1mStep[0m  [42/213], [94mLoss[0m : 2.06334
[1mStep[0m  [63/213], [94mLoss[0m : 1.54584
[1mStep[0m  [84/213], [94mLoss[0m : 1.63942
[1mStep[0m  [105/213], [94mLoss[0m : 1.65112
[1mStep[0m  [126/213], [94mLoss[0m : 1.39051
[1mStep[0m  [147/213], [94mLoss[0m : 1.29425
[1mStep[0m  [168/213], [94mLoss[0m : 1.16021
[1mStep[0m  [189/213], [94mLoss[0m : 1.42127
[1mStep[0m  [210/213], [94mLoss[0m : 1.30946

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.464, [92mTest[0m: 2.460, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45310
[1mStep[0m  [21/213], [94mLoss[0m : 1.23071
[1mStep[0m  [42/213], [94mLoss[0m : 1.45810
[1mStep[0m  [63/213], [94mLoss[0m : 1.44867
[1mStep[0m  [84/213], [94mLoss[0m : 1.18843
[1mStep[0m  [105/213], [94mLoss[0m : 1.37876
[1mStep[0m  [126/213], [94mLoss[0m : 1.27359
[1mStep[0m  [147/213], [94mLoss[0m : 1.27384
[1mStep[0m  [168/213], [94mLoss[0m : 1.66272
[1mStep[0m  [189/213], [94mLoss[0m : 1.67551
[1mStep[0m  [210/213], [94mLoss[0m : 1.40435

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.506, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.517
====================================

Phase 2 - Evaluation MAE:  2.5172172897266893
MAE score P1       2.330563
MAE score P2       2.517217
loss               1.449656
learning_rate      0.002575
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay         0.0001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 10.39769
[1mStep[0m  [10/106], [94mLoss[0m : 9.72789
[1mStep[0m  [20/106], [94mLoss[0m : 8.85101
[1mStep[0m  [30/106], [94mLoss[0m : 8.30284
[1mStep[0m  [40/106], [94mLoss[0m : 7.91671
[1mStep[0m  [50/106], [94mLoss[0m : 6.80232
[1mStep[0m  [60/106], [94mLoss[0m : 5.29333
[1mStep[0m  [70/106], [94mLoss[0m : 5.49834
[1mStep[0m  [80/106], [94mLoss[0m : 4.68347
[1mStep[0m  [90/106], [94mLoss[0m : 3.94426
[1mStep[0m  [100/106], [94mLoss[0m : 3.29918

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.758, [92mTest[0m: 10.896, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.29419
[1mStep[0m  [10/106], [94mLoss[0m : 2.93412
[1mStep[0m  [20/106], [94mLoss[0m : 3.02922
[1mStep[0m  [30/106], [94mLoss[0m : 3.28467
[1mStep[0m  [40/106], [94mLoss[0m : 2.84249
[1mStep[0m  [50/106], [94mLoss[0m : 3.01483
[1mStep[0m  [60/106], [94mLoss[0m : 2.68170
[1mStep[0m  [70/106], [94mLoss[0m : 3.20912
[1mStep[0m  [80/106], [94mLoss[0m : 2.91130
[1mStep[0m  [90/106], [94mLoss[0m : 2.59215
[1mStep[0m  [100/106], [94mLoss[0m : 2.80535

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.848, [92mTest[0m: 3.950, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77960
[1mStep[0m  [10/106], [94mLoss[0m : 2.55522
[1mStep[0m  [20/106], [94mLoss[0m : 2.78638
[1mStep[0m  [30/106], [94mLoss[0m : 2.63308
[1mStep[0m  [40/106], [94mLoss[0m : 2.53486
[1mStep[0m  [50/106], [94mLoss[0m : 2.78243
[1mStep[0m  [60/106], [94mLoss[0m : 2.64006
[1mStep[0m  [70/106], [94mLoss[0m : 2.63217
[1mStep[0m  [80/106], [94mLoss[0m : 2.50853
[1mStep[0m  [90/106], [94mLoss[0m : 2.61131
[1mStep[0m  [100/106], [94mLoss[0m : 2.66172

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.654, [92mTest[0m: 2.741, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73203
[1mStep[0m  [10/106], [94mLoss[0m : 2.60790
[1mStep[0m  [20/106], [94mLoss[0m : 2.67616
[1mStep[0m  [30/106], [94mLoss[0m : 2.32277
[1mStep[0m  [40/106], [94mLoss[0m : 2.68889
[1mStep[0m  [50/106], [94mLoss[0m : 2.37151
[1mStep[0m  [60/106], [94mLoss[0m : 2.18052
[1mStep[0m  [70/106], [94mLoss[0m : 2.45000
[1mStep[0m  [80/106], [94mLoss[0m : 2.23222
[1mStep[0m  [90/106], [94mLoss[0m : 2.61389
[1mStep[0m  [100/106], [94mLoss[0m : 3.07431

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.614, [92mTest[0m: 2.632, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59509
[1mStep[0m  [10/106], [94mLoss[0m : 2.72011
[1mStep[0m  [20/106], [94mLoss[0m : 2.37047
[1mStep[0m  [30/106], [94mLoss[0m : 2.65509
[1mStep[0m  [40/106], [94mLoss[0m : 2.41233
[1mStep[0m  [50/106], [94mLoss[0m : 2.63783
[1mStep[0m  [60/106], [94mLoss[0m : 2.56326
[1mStep[0m  [70/106], [94mLoss[0m : 2.35984
[1mStep[0m  [80/106], [94mLoss[0m : 2.59456
[1mStep[0m  [90/106], [94mLoss[0m : 2.37244
[1mStep[0m  [100/106], [94mLoss[0m : 2.64389

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.597, [92mTest[0m: 2.593, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33151
[1mStep[0m  [10/106], [94mLoss[0m : 2.62305
[1mStep[0m  [20/106], [94mLoss[0m : 2.64394
[1mStep[0m  [30/106], [94mLoss[0m : 2.31099
[1mStep[0m  [40/106], [94mLoss[0m : 2.48779
[1mStep[0m  [50/106], [94mLoss[0m : 2.93776
[1mStep[0m  [60/106], [94mLoss[0m : 2.91659
[1mStep[0m  [70/106], [94mLoss[0m : 2.56128
[1mStep[0m  [80/106], [94mLoss[0m : 2.73380
[1mStep[0m  [90/106], [94mLoss[0m : 2.64211
[1mStep[0m  [100/106], [94mLoss[0m : 2.78752

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.549, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38514
[1mStep[0m  [10/106], [94mLoss[0m : 2.67801
[1mStep[0m  [20/106], [94mLoss[0m : 2.60393
[1mStep[0m  [30/106], [94mLoss[0m : 2.67345
[1mStep[0m  [40/106], [94mLoss[0m : 2.34565
[1mStep[0m  [50/106], [94mLoss[0m : 2.47108
[1mStep[0m  [60/106], [94mLoss[0m : 2.53724
[1mStep[0m  [70/106], [94mLoss[0m : 2.38913
[1mStep[0m  [80/106], [94mLoss[0m : 2.86769
[1mStep[0m  [90/106], [94mLoss[0m : 2.39796
[1mStep[0m  [100/106], [94mLoss[0m : 2.40644

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.518, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50202
[1mStep[0m  [10/106], [94mLoss[0m : 2.45020
[1mStep[0m  [20/106], [94mLoss[0m : 2.76898
[1mStep[0m  [30/106], [94mLoss[0m : 2.72876
[1mStep[0m  [40/106], [94mLoss[0m : 2.52147
[1mStep[0m  [50/106], [94mLoss[0m : 2.51876
[1mStep[0m  [60/106], [94mLoss[0m : 2.45603
[1mStep[0m  [70/106], [94mLoss[0m : 2.62564
[1mStep[0m  [80/106], [94mLoss[0m : 2.76246
[1mStep[0m  [90/106], [94mLoss[0m : 2.74237
[1mStep[0m  [100/106], [94mLoss[0m : 2.74495

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.537, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59308
[1mStep[0m  [10/106], [94mLoss[0m : 2.44153
[1mStep[0m  [20/106], [94mLoss[0m : 2.17037
[1mStep[0m  [30/106], [94mLoss[0m : 2.23835
[1mStep[0m  [40/106], [94mLoss[0m : 2.43562
[1mStep[0m  [50/106], [94mLoss[0m : 2.60128
[1mStep[0m  [60/106], [94mLoss[0m : 2.79617
[1mStep[0m  [70/106], [94mLoss[0m : 2.44389
[1mStep[0m  [80/106], [94mLoss[0m : 2.83950
[1mStep[0m  [90/106], [94mLoss[0m : 2.64990
[1mStep[0m  [100/106], [94mLoss[0m : 2.43799

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.507, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18024
[1mStep[0m  [10/106], [94mLoss[0m : 2.47493
[1mStep[0m  [20/106], [94mLoss[0m : 2.75675
[1mStep[0m  [30/106], [94mLoss[0m : 2.74360
[1mStep[0m  [40/106], [94mLoss[0m : 2.32779
[1mStep[0m  [50/106], [94mLoss[0m : 2.36575
[1mStep[0m  [60/106], [94mLoss[0m : 2.27926
[1mStep[0m  [70/106], [94mLoss[0m : 2.64420
[1mStep[0m  [80/106], [94mLoss[0m : 2.36614
[1mStep[0m  [90/106], [94mLoss[0m : 2.41515
[1mStep[0m  [100/106], [94mLoss[0m : 2.72132

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33331
[1mStep[0m  [10/106], [94mLoss[0m : 2.87985
[1mStep[0m  [20/106], [94mLoss[0m : 2.87085
[1mStep[0m  [30/106], [94mLoss[0m : 2.12335
[1mStep[0m  [40/106], [94mLoss[0m : 2.60387
[1mStep[0m  [50/106], [94mLoss[0m : 2.63380
[1mStep[0m  [60/106], [94mLoss[0m : 2.29047
[1mStep[0m  [70/106], [94mLoss[0m : 2.68266
[1mStep[0m  [80/106], [94mLoss[0m : 2.51920
[1mStep[0m  [90/106], [94mLoss[0m : 2.40125
[1mStep[0m  [100/106], [94mLoss[0m : 2.49274

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38871
[1mStep[0m  [10/106], [94mLoss[0m : 2.25706
[1mStep[0m  [20/106], [94mLoss[0m : 2.57950
[1mStep[0m  [30/106], [94mLoss[0m : 2.73603
[1mStep[0m  [40/106], [94mLoss[0m : 2.39465
[1mStep[0m  [50/106], [94mLoss[0m : 2.60887
[1mStep[0m  [60/106], [94mLoss[0m : 2.57843
[1mStep[0m  [70/106], [94mLoss[0m : 2.23263
[1mStep[0m  [80/106], [94mLoss[0m : 2.48286
[1mStep[0m  [90/106], [94mLoss[0m : 2.57552
[1mStep[0m  [100/106], [94mLoss[0m : 2.52956

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.526, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59174
[1mStep[0m  [10/106], [94mLoss[0m : 2.51713
[1mStep[0m  [20/106], [94mLoss[0m : 2.55646
[1mStep[0m  [30/106], [94mLoss[0m : 2.63840
[1mStep[0m  [40/106], [94mLoss[0m : 2.42992
[1mStep[0m  [50/106], [94mLoss[0m : 2.67323
[1mStep[0m  [60/106], [94mLoss[0m : 2.36341
[1mStep[0m  [70/106], [94mLoss[0m : 2.62811
[1mStep[0m  [80/106], [94mLoss[0m : 2.49194
[1mStep[0m  [90/106], [94mLoss[0m : 2.39977
[1mStep[0m  [100/106], [94mLoss[0m : 2.27748

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31156
[1mStep[0m  [10/106], [94mLoss[0m : 2.57482
[1mStep[0m  [20/106], [94mLoss[0m : 2.45984
[1mStep[0m  [30/106], [94mLoss[0m : 2.37977
[1mStep[0m  [40/106], [94mLoss[0m : 2.56972
[1mStep[0m  [50/106], [94mLoss[0m : 2.50334
[1mStep[0m  [60/106], [94mLoss[0m : 2.27160
[1mStep[0m  [70/106], [94mLoss[0m : 2.45311
[1mStep[0m  [80/106], [94mLoss[0m : 2.55759
[1mStep[0m  [90/106], [94mLoss[0m : 2.38885
[1mStep[0m  [100/106], [94mLoss[0m : 2.92117

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.450, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36567
[1mStep[0m  [10/106], [94mLoss[0m : 2.21859
[1mStep[0m  [20/106], [94mLoss[0m : 2.53300
[1mStep[0m  [30/106], [94mLoss[0m : 2.65906
[1mStep[0m  [40/106], [94mLoss[0m : 2.13736
[1mStep[0m  [50/106], [94mLoss[0m : 2.38859
[1mStep[0m  [60/106], [94mLoss[0m : 2.37010
[1mStep[0m  [70/106], [94mLoss[0m : 2.67629
[1mStep[0m  [80/106], [94mLoss[0m : 2.53604
[1mStep[0m  [90/106], [94mLoss[0m : 2.62219
[1mStep[0m  [100/106], [94mLoss[0m : 2.69871

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.470, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39392
[1mStep[0m  [10/106], [94mLoss[0m : 2.76053
[1mStep[0m  [20/106], [94mLoss[0m : 2.56516
[1mStep[0m  [30/106], [94mLoss[0m : 2.41326
[1mStep[0m  [40/106], [94mLoss[0m : 2.75388
[1mStep[0m  [50/106], [94mLoss[0m : 2.14883
[1mStep[0m  [60/106], [94mLoss[0m : 2.75460
[1mStep[0m  [70/106], [94mLoss[0m : 2.36795
[1mStep[0m  [80/106], [94mLoss[0m : 2.63477
[1mStep[0m  [90/106], [94mLoss[0m : 2.50031
[1mStep[0m  [100/106], [94mLoss[0m : 2.91261

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61527
[1mStep[0m  [10/106], [94mLoss[0m : 2.29989
[1mStep[0m  [20/106], [94mLoss[0m : 2.12177
[1mStep[0m  [30/106], [94mLoss[0m : 2.48085
[1mStep[0m  [40/106], [94mLoss[0m : 2.80837
[1mStep[0m  [50/106], [94mLoss[0m : 2.41606
[1mStep[0m  [60/106], [94mLoss[0m : 2.65660
[1mStep[0m  [70/106], [94mLoss[0m : 2.30290
[1mStep[0m  [80/106], [94mLoss[0m : 3.13089
[1mStep[0m  [90/106], [94mLoss[0m : 2.59832
[1mStep[0m  [100/106], [94mLoss[0m : 2.37471

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43416
[1mStep[0m  [10/106], [94mLoss[0m : 2.55925
[1mStep[0m  [20/106], [94mLoss[0m : 2.49521
[1mStep[0m  [30/106], [94mLoss[0m : 2.49601
[1mStep[0m  [40/106], [94mLoss[0m : 2.24221
[1mStep[0m  [50/106], [94mLoss[0m : 2.17863
[1mStep[0m  [60/106], [94mLoss[0m : 2.79490
[1mStep[0m  [70/106], [94mLoss[0m : 2.73046
[1mStep[0m  [80/106], [94mLoss[0m : 2.46374
[1mStep[0m  [90/106], [94mLoss[0m : 2.43856
[1mStep[0m  [100/106], [94mLoss[0m : 2.59367

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58068
[1mStep[0m  [10/106], [94mLoss[0m : 2.39261
[1mStep[0m  [20/106], [94mLoss[0m : 2.34683
[1mStep[0m  [30/106], [94mLoss[0m : 2.45142
[1mStep[0m  [40/106], [94mLoss[0m : 2.41793
[1mStep[0m  [50/106], [94mLoss[0m : 2.91863
[1mStep[0m  [60/106], [94mLoss[0m : 2.67947
[1mStep[0m  [70/106], [94mLoss[0m : 2.60594
[1mStep[0m  [80/106], [94mLoss[0m : 2.52382
[1mStep[0m  [90/106], [94mLoss[0m : 2.36710
[1mStep[0m  [100/106], [94mLoss[0m : 2.48796

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43631
[1mStep[0m  [10/106], [94mLoss[0m : 2.57047
[1mStep[0m  [20/106], [94mLoss[0m : 2.38052
[1mStep[0m  [30/106], [94mLoss[0m : 2.20479
[1mStep[0m  [40/106], [94mLoss[0m : 2.69031
[1mStep[0m  [50/106], [94mLoss[0m : 2.27654
[1mStep[0m  [60/106], [94mLoss[0m : 2.53793
[1mStep[0m  [70/106], [94mLoss[0m : 2.56437
[1mStep[0m  [80/106], [94mLoss[0m : 2.29769
[1mStep[0m  [90/106], [94mLoss[0m : 2.43971
[1mStep[0m  [100/106], [94mLoss[0m : 2.57303

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.425, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67310
[1mStep[0m  [10/106], [94mLoss[0m : 2.73403
[1mStep[0m  [20/106], [94mLoss[0m : 2.91081
[1mStep[0m  [30/106], [94mLoss[0m : 2.33673
[1mStep[0m  [40/106], [94mLoss[0m : 2.21546
[1mStep[0m  [50/106], [94mLoss[0m : 2.43533
[1mStep[0m  [60/106], [94mLoss[0m : 2.92802
[1mStep[0m  [70/106], [94mLoss[0m : 2.30297
[1mStep[0m  [80/106], [94mLoss[0m : 2.41396
[1mStep[0m  [90/106], [94mLoss[0m : 2.56456
[1mStep[0m  [100/106], [94mLoss[0m : 2.46769

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54865
[1mStep[0m  [10/106], [94mLoss[0m : 2.33142
[1mStep[0m  [20/106], [94mLoss[0m : 2.45396
[1mStep[0m  [30/106], [94mLoss[0m : 2.14340
[1mStep[0m  [40/106], [94mLoss[0m : 2.42361
[1mStep[0m  [50/106], [94mLoss[0m : 2.60268
[1mStep[0m  [60/106], [94mLoss[0m : 2.40104
[1mStep[0m  [70/106], [94mLoss[0m : 2.33291
[1mStep[0m  [80/106], [94mLoss[0m : 2.47252
[1mStep[0m  [90/106], [94mLoss[0m : 2.30779
[1mStep[0m  [100/106], [94mLoss[0m : 2.55578

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40007
[1mStep[0m  [10/106], [94mLoss[0m : 2.44749
[1mStep[0m  [20/106], [94mLoss[0m : 2.32718
[1mStep[0m  [30/106], [94mLoss[0m : 2.60258
[1mStep[0m  [40/106], [94mLoss[0m : 2.64359
[1mStep[0m  [50/106], [94mLoss[0m : 2.73595
[1mStep[0m  [60/106], [94mLoss[0m : 2.24313
[1mStep[0m  [70/106], [94mLoss[0m : 2.44792
[1mStep[0m  [80/106], [94mLoss[0m : 2.25719
[1mStep[0m  [90/106], [94mLoss[0m : 2.32220
[1mStep[0m  [100/106], [94mLoss[0m : 2.58380

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41118
[1mStep[0m  [10/106], [94mLoss[0m : 2.28735
[1mStep[0m  [20/106], [94mLoss[0m : 2.57906
[1mStep[0m  [30/106], [94mLoss[0m : 2.61909
[1mStep[0m  [40/106], [94mLoss[0m : 2.49776
[1mStep[0m  [50/106], [94mLoss[0m : 2.33092
[1mStep[0m  [60/106], [94mLoss[0m : 2.32349
[1mStep[0m  [70/106], [94mLoss[0m : 2.35036
[1mStep[0m  [80/106], [94mLoss[0m : 2.29019
[1mStep[0m  [90/106], [94mLoss[0m : 2.45434
[1mStep[0m  [100/106], [94mLoss[0m : 2.51604

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34012
[1mStep[0m  [10/106], [94mLoss[0m : 2.54841
[1mStep[0m  [20/106], [94mLoss[0m : 2.44122
[1mStep[0m  [30/106], [94mLoss[0m : 2.12518
[1mStep[0m  [40/106], [94mLoss[0m : 2.66929
[1mStep[0m  [50/106], [94mLoss[0m : 2.65479
[1mStep[0m  [60/106], [94mLoss[0m : 2.68021
[1mStep[0m  [70/106], [94mLoss[0m : 2.30769
[1mStep[0m  [80/106], [94mLoss[0m : 2.51785
[1mStep[0m  [90/106], [94mLoss[0m : 2.70104
[1mStep[0m  [100/106], [94mLoss[0m : 2.57017

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.398, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28655
[1mStep[0m  [10/106], [94mLoss[0m : 2.56799
[1mStep[0m  [20/106], [94mLoss[0m : 2.46136
[1mStep[0m  [30/106], [94mLoss[0m : 2.61588
[1mStep[0m  [40/106], [94mLoss[0m : 2.80035
[1mStep[0m  [50/106], [94mLoss[0m : 2.40016
[1mStep[0m  [60/106], [94mLoss[0m : 2.40016
[1mStep[0m  [70/106], [94mLoss[0m : 2.59081
[1mStep[0m  [80/106], [94mLoss[0m : 2.83185
[1mStep[0m  [90/106], [94mLoss[0m : 2.18850
[1mStep[0m  [100/106], [94mLoss[0m : 2.37664

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.413, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02835
[1mStep[0m  [10/106], [94mLoss[0m : 2.54129
[1mStep[0m  [20/106], [94mLoss[0m : 2.09742
[1mStep[0m  [30/106], [94mLoss[0m : 2.36282
[1mStep[0m  [40/106], [94mLoss[0m : 2.41058
[1mStep[0m  [50/106], [94mLoss[0m : 2.42911
[1mStep[0m  [60/106], [94mLoss[0m : 2.82010
[1mStep[0m  [70/106], [94mLoss[0m : 2.57782
[1mStep[0m  [80/106], [94mLoss[0m : 2.37441
[1mStep[0m  [90/106], [94mLoss[0m : 2.77616
[1mStep[0m  [100/106], [94mLoss[0m : 2.26384

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.386, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33620
[1mStep[0m  [10/106], [94mLoss[0m : 2.08698
[1mStep[0m  [20/106], [94mLoss[0m : 2.68122
[1mStep[0m  [30/106], [94mLoss[0m : 2.62724
[1mStep[0m  [40/106], [94mLoss[0m : 2.69825
[1mStep[0m  [50/106], [94mLoss[0m : 2.09053
[1mStep[0m  [60/106], [94mLoss[0m : 2.66047
[1mStep[0m  [70/106], [94mLoss[0m : 2.50666
[1mStep[0m  [80/106], [94mLoss[0m : 2.61610
[1mStep[0m  [90/106], [94mLoss[0m : 2.68008
[1mStep[0m  [100/106], [94mLoss[0m : 2.38732

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.410, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54903
[1mStep[0m  [10/106], [94mLoss[0m : 2.20515
[1mStep[0m  [20/106], [94mLoss[0m : 2.33373
[1mStep[0m  [30/106], [94mLoss[0m : 2.42477
[1mStep[0m  [40/106], [94mLoss[0m : 2.24739
[1mStep[0m  [50/106], [94mLoss[0m : 2.27823
[1mStep[0m  [60/106], [94mLoss[0m : 2.44462
[1mStep[0m  [70/106], [94mLoss[0m : 2.05976
[1mStep[0m  [80/106], [94mLoss[0m : 2.36238
[1mStep[0m  [90/106], [94mLoss[0m : 2.38605
[1mStep[0m  [100/106], [94mLoss[0m : 2.29729

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57590
[1mStep[0m  [10/106], [94mLoss[0m : 2.33063
[1mStep[0m  [20/106], [94mLoss[0m : 2.35836
[1mStep[0m  [30/106], [94mLoss[0m : 2.50378
[1mStep[0m  [40/106], [94mLoss[0m : 2.67942
[1mStep[0m  [50/106], [94mLoss[0m : 2.25938
[1mStep[0m  [60/106], [94mLoss[0m : 2.22553
[1mStep[0m  [70/106], [94mLoss[0m : 2.44927
[1mStep[0m  [80/106], [94mLoss[0m : 2.56642
[1mStep[0m  [90/106], [94mLoss[0m : 2.34941
[1mStep[0m  [100/106], [94mLoss[0m : 2.29949

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.387, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.385
====================================

Phase 1 - Evaluation MAE:  2.3852682338570648
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.33814
[1mStep[0m  [10/106], [94mLoss[0m : 2.70409
[1mStep[0m  [20/106], [94mLoss[0m : 2.59894
[1mStep[0m  [30/106], [94mLoss[0m : 2.65825
[1mStep[0m  [40/106], [94mLoss[0m : 2.32855
[1mStep[0m  [50/106], [94mLoss[0m : 2.39292
[1mStep[0m  [60/106], [94mLoss[0m : 2.77641
[1mStep[0m  [70/106], [94mLoss[0m : 2.52854
[1mStep[0m  [80/106], [94mLoss[0m : 2.54585
[1mStep[0m  [90/106], [94mLoss[0m : 2.64621
[1mStep[0m  [100/106], [94mLoss[0m : 2.23830

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73830
[1mStep[0m  [10/106], [94mLoss[0m : 2.50603
[1mStep[0m  [20/106], [94mLoss[0m : 2.66914
[1mStep[0m  [30/106], [94mLoss[0m : 2.43556
[1mStep[0m  [40/106], [94mLoss[0m : 2.64525
[1mStep[0m  [50/106], [94mLoss[0m : 2.38781
[1mStep[0m  [60/106], [94mLoss[0m : 2.35521
[1mStep[0m  [70/106], [94mLoss[0m : 2.43558
[1mStep[0m  [80/106], [94mLoss[0m : 2.63487
[1mStep[0m  [90/106], [94mLoss[0m : 2.55544
[1mStep[0m  [100/106], [94mLoss[0m : 2.30859

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.702, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43334
[1mStep[0m  [10/106], [94mLoss[0m : 2.27972
[1mStep[0m  [20/106], [94mLoss[0m : 2.42041
[1mStep[0m  [30/106], [94mLoss[0m : 2.33456
[1mStep[0m  [40/106], [94mLoss[0m : 2.45810
[1mStep[0m  [50/106], [94mLoss[0m : 2.20976
[1mStep[0m  [60/106], [94mLoss[0m : 2.25490
[1mStep[0m  [70/106], [94mLoss[0m : 2.51248
[1mStep[0m  [80/106], [94mLoss[0m : 2.28445
[1mStep[0m  [90/106], [94mLoss[0m : 2.48546
[1mStep[0m  [100/106], [94mLoss[0m : 2.68104

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.721, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31052
[1mStep[0m  [10/106], [94mLoss[0m : 2.47789
[1mStep[0m  [20/106], [94mLoss[0m : 2.43315
[1mStep[0m  [30/106], [94mLoss[0m : 2.60009
[1mStep[0m  [40/106], [94mLoss[0m : 2.23005
[1mStep[0m  [50/106], [94mLoss[0m : 2.64066
[1mStep[0m  [60/106], [94mLoss[0m : 2.05024
[1mStep[0m  [70/106], [94mLoss[0m : 1.96362
[1mStep[0m  [80/106], [94mLoss[0m : 2.17498
[1mStep[0m  [90/106], [94mLoss[0m : 2.44553
[1mStep[0m  [100/106], [94mLoss[0m : 2.26645

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.604, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42843
[1mStep[0m  [10/106], [94mLoss[0m : 2.11808
[1mStep[0m  [20/106], [94mLoss[0m : 2.92619
[1mStep[0m  [30/106], [94mLoss[0m : 2.12585
[1mStep[0m  [40/106], [94mLoss[0m : 2.60785
[1mStep[0m  [50/106], [94mLoss[0m : 2.05407
[1mStep[0m  [60/106], [94mLoss[0m : 2.46267
[1mStep[0m  [70/106], [94mLoss[0m : 2.34788
[1mStep[0m  [80/106], [94mLoss[0m : 2.37653
[1mStep[0m  [90/106], [94mLoss[0m : 2.19710
[1mStep[0m  [100/106], [94mLoss[0m : 2.67773

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.520, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41467
[1mStep[0m  [10/106], [94mLoss[0m : 2.28336
[1mStep[0m  [20/106], [94mLoss[0m : 2.09753
[1mStep[0m  [30/106], [94mLoss[0m : 2.41578
[1mStep[0m  [40/106], [94mLoss[0m : 2.50033
[1mStep[0m  [50/106], [94mLoss[0m : 2.27011
[1mStep[0m  [60/106], [94mLoss[0m : 2.51058
[1mStep[0m  [70/106], [94mLoss[0m : 2.31142
[1mStep[0m  [80/106], [94mLoss[0m : 2.29738
[1mStep[0m  [90/106], [94mLoss[0m : 2.46482
[1mStep[0m  [100/106], [94mLoss[0m : 2.13141

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.305, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22293
[1mStep[0m  [10/106], [94mLoss[0m : 2.16396
[1mStep[0m  [20/106], [94mLoss[0m : 2.18243
[1mStep[0m  [30/106], [94mLoss[0m : 2.46435
[1mStep[0m  [40/106], [94mLoss[0m : 2.30675
[1mStep[0m  [50/106], [94mLoss[0m : 2.03508
[1mStep[0m  [60/106], [94mLoss[0m : 2.43420
[1mStep[0m  [70/106], [94mLoss[0m : 2.53879
[1mStep[0m  [80/106], [94mLoss[0m : 2.35556
[1mStep[0m  [90/106], [94mLoss[0m : 2.36109
[1mStep[0m  [100/106], [94mLoss[0m : 2.32830

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30850
[1mStep[0m  [10/106], [94mLoss[0m : 2.11175
[1mStep[0m  [20/106], [94mLoss[0m : 2.22229
[1mStep[0m  [30/106], [94mLoss[0m : 1.93245
[1mStep[0m  [40/106], [94mLoss[0m : 2.42070
[1mStep[0m  [50/106], [94mLoss[0m : 2.46578
[1mStep[0m  [60/106], [94mLoss[0m : 2.34712
[1mStep[0m  [70/106], [94mLoss[0m : 2.32389
[1mStep[0m  [80/106], [94mLoss[0m : 2.32614
[1mStep[0m  [90/106], [94mLoss[0m : 2.44246
[1mStep[0m  [100/106], [94mLoss[0m : 2.36276

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08574
[1mStep[0m  [10/106], [94mLoss[0m : 2.05127
[1mStep[0m  [20/106], [94mLoss[0m : 2.29262
[1mStep[0m  [30/106], [94mLoss[0m : 1.96146
[1mStep[0m  [40/106], [94mLoss[0m : 1.84577
[1mStep[0m  [50/106], [94mLoss[0m : 2.37372
[1mStep[0m  [60/106], [94mLoss[0m : 2.64219
[1mStep[0m  [70/106], [94mLoss[0m : 2.15905
[1mStep[0m  [80/106], [94mLoss[0m : 2.30959
[1mStep[0m  [90/106], [94mLoss[0m : 2.23958
[1mStep[0m  [100/106], [94mLoss[0m : 2.49777

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.223, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13285
[1mStep[0m  [10/106], [94mLoss[0m : 1.79057
[1mStep[0m  [20/106], [94mLoss[0m : 2.10092
[1mStep[0m  [30/106], [94mLoss[0m : 2.14504
[1mStep[0m  [40/106], [94mLoss[0m : 2.27075
[1mStep[0m  [50/106], [94mLoss[0m : 2.16883
[1mStep[0m  [60/106], [94mLoss[0m : 2.15778
[1mStep[0m  [70/106], [94mLoss[0m : 2.11283
[1mStep[0m  [80/106], [94mLoss[0m : 2.08483
[1mStep[0m  [90/106], [94mLoss[0m : 2.30447
[1mStep[0m  [100/106], [94mLoss[0m : 2.24817

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.170, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21417
[1mStep[0m  [10/106], [94mLoss[0m : 2.13273
[1mStep[0m  [20/106], [94mLoss[0m : 2.15253
[1mStep[0m  [30/106], [94mLoss[0m : 2.47197
[1mStep[0m  [40/106], [94mLoss[0m : 2.03001
[1mStep[0m  [50/106], [94mLoss[0m : 2.11265
[1mStep[0m  [60/106], [94mLoss[0m : 1.92755
[1mStep[0m  [70/106], [94mLoss[0m : 2.41696
[1mStep[0m  [80/106], [94mLoss[0m : 2.04464
[1mStep[0m  [90/106], [94mLoss[0m : 1.95267
[1mStep[0m  [100/106], [94mLoss[0m : 2.42245

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.142, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02884
[1mStep[0m  [10/106], [94mLoss[0m : 2.39718
[1mStep[0m  [20/106], [94mLoss[0m : 1.87220
[1mStep[0m  [30/106], [94mLoss[0m : 1.93331
[1mStep[0m  [40/106], [94mLoss[0m : 1.91598
[1mStep[0m  [50/106], [94mLoss[0m : 2.14312
[1mStep[0m  [60/106], [94mLoss[0m : 2.02510
[1mStep[0m  [70/106], [94mLoss[0m : 2.26165
[1mStep[0m  [80/106], [94mLoss[0m : 2.19356
[1mStep[0m  [90/106], [94mLoss[0m : 1.86814
[1mStep[0m  [100/106], [94mLoss[0m : 1.83116

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87128
[1mStep[0m  [10/106], [94mLoss[0m : 2.12183
[1mStep[0m  [20/106], [94mLoss[0m : 2.23879
[1mStep[0m  [30/106], [94mLoss[0m : 2.04362
[1mStep[0m  [40/106], [94mLoss[0m : 2.09642
[1mStep[0m  [50/106], [94mLoss[0m : 2.17218
[1mStep[0m  [60/106], [94mLoss[0m : 1.95197
[1mStep[0m  [70/106], [94mLoss[0m : 1.89650
[1mStep[0m  [80/106], [94mLoss[0m : 1.76554
[1mStep[0m  [90/106], [94mLoss[0m : 2.09359
[1mStep[0m  [100/106], [94mLoss[0m : 1.96204

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03246
[1mStep[0m  [10/106], [94mLoss[0m : 2.19556
[1mStep[0m  [20/106], [94mLoss[0m : 1.87529
[1mStep[0m  [30/106], [94mLoss[0m : 1.97874
[1mStep[0m  [40/106], [94mLoss[0m : 2.30959
[1mStep[0m  [50/106], [94mLoss[0m : 1.82174
[1mStep[0m  [60/106], [94mLoss[0m : 1.99669
[1mStep[0m  [70/106], [94mLoss[0m : 2.04501
[1mStep[0m  [80/106], [94mLoss[0m : 2.17087
[1mStep[0m  [90/106], [94mLoss[0m : 2.11198
[1mStep[0m  [100/106], [94mLoss[0m : 1.97098

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86496
[1mStep[0m  [10/106], [94mLoss[0m : 1.81801
[1mStep[0m  [20/106], [94mLoss[0m : 2.06417
[1mStep[0m  [30/106], [94mLoss[0m : 1.94457
[1mStep[0m  [40/106], [94mLoss[0m : 1.81876
[1mStep[0m  [50/106], [94mLoss[0m : 1.87378
[1mStep[0m  [60/106], [94mLoss[0m : 1.85352
[1mStep[0m  [70/106], [94mLoss[0m : 2.15301
[1mStep[0m  [80/106], [94mLoss[0m : 1.81295
[1mStep[0m  [90/106], [94mLoss[0m : 2.06579
[1mStep[0m  [100/106], [94mLoss[0m : 1.80318

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.69729
[1mStep[0m  [10/106], [94mLoss[0m : 1.98303
[1mStep[0m  [20/106], [94mLoss[0m : 1.99729
[1mStep[0m  [30/106], [94mLoss[0m : 1.89702
[1mStep[0m  [40/106], [94mLoss[0m : 2.00253
[1mStep[0m  [50/106], [94mLoss[0m : 2.04304
[1mStep[0m  [60/106], [94mLoss[0m : 1.84465
[1mStep[0m  [70/106], [94mLoss[0m : 1.92199
[1mStep[0m  [80/106], [94mLoss[0m : 2.06383
[1mStep[0m  [90/106], [94mLoss[0m : 1.93245
[1mStep[0m  [100/106], [94mLoss[0m : 2.09028

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.952, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90201
[1mStep[0m  [10/106], [94mLoss[0m : 1.82883
[1mStep[0m  [20/106], [94mLoss[0m : 2.09628
[1mStep[0m  [30/106], [94mLoss[0m : 2.06815
[1mStep[0m  [40/106], [94mLoss[0m : 2.05087
[1mStep[0m  [50/106], [94mLoss[0m : 1.91764
[1mStep[0m  [60/106], [94mLoss[0m : 1.83594
[1mStep[0m  [70/106], [94mLoss[0m : 1.97910
[1mStep[0m  [80/106], [94mLoss[0m : 2.23802
[1mStep[0m  [90/106], [94mLoss[0m : 1.79659
[1mStep[0m  [100/106], [94mLoss[0m : 2.01962

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.926, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00902
[1mStep[0m  [10/106], [94mLoss[0m : 1.78731
[1mStep[0m  [20/106], [94mLoss[0m : 1.80941
[1mStep[0m  [30/106], [94mLoss[0m : 1.83253
[1mStep[0m  [40/106], [94mLoss[0m : 1.60464
[1mStep[0m  [50/106], [94mLoss[0m : 2.06793
[1mStep[0m  [60/106], [94mLoss[0m : 1.99352
[1mStep[0m  [70/106], [94mLoss[0m : 1.87125
[1mStep[0m  [80/106], [94mLoss[0m : 2.03797
[1mStep[0m  [90/106], [94mLoss[0m : 1.81822
[1mStep[0m  [100/106], [94mLoss[0m : 1.99194

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77316
[1mStep[0m  [10/106], [94mLoss[0m : 1.79151
[1mStep[0m  [20/106], [94mLoss[0m : 1.98084
[1mStep[0m  [30/106], [94mLoss[0m : 2.07180
[1mStep[0m  [40/106], [94mLoss[0m : 1.58090
[1mStep[0m  [50/106], [94mLoss[0m : 2.04551
[1mStep[0m  [60/106], [94mLoss[0m : 1.39156
[1mStep[0m  [70/106], [94mLoss[0m : 2.09335
[1mStep[0m  [80/106], [94mLoss[0m : 1.92086
[1mStep[0m  [90/106], [94mLoss[0m : 1.78552
[1mStep[0m  [100/106], [94mLoss[0m : 1.61410

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.855, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.99736
[1mStep[0m  [10/106], [94mLoss[0m : 1.72284
[1mStep[0m  [20/106], [94mLoss[0m : 1.84939
[1mStep[0m  [30/106], [94mLoss[0m : 1.76069
[1mStep[0m  [40/106], [94mLoss[0m : 2.04764
[1mStep[0m  [50/106], [94mLoss[0m : 1.63318
[1mStep[0m  [60/106], [94mLoss[0m : 1.69696
[1mStep[0m  [70/106], [94mLoss[0m : 2.08078
[1mStep[0m  [80/106], [94mLoss[0m : 1.71548
[1mStep[0m  [90/106], [94mLoss[0m : 1.85728
[1mStep[0m  [100/106], [94mLoss[0m : 1.71626

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.845, [92mTest[0m: 2.455, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.81730
[1mStep[0m  [10/106], [94mLoss[0m : 1.83044
[1mStep[0m  [20/106], [94mLoss[0m : 1.94166
[1mStep[0m  [30/106], [94mLoss[0m : 1.77357
[1mStep[0m  [40/106], [94mLoss[0m : 1.82158
[1mStep[0m  [50/106], [94mLoss[0m : 2.16907
[1mStep[0m  [60/106], [94mLoss[0m : 1.84022
[1mStep[0m  [70/106], [94mLoss[0m : 1.81691
[1mStep[0m  [80/106], [94mLoss[0m : 1.80779
[1mStep[0m  [90/106], [94mLoss[0m : 1.99830
[1mStep[0m  [100/106], [94mLoss[0m : 1.79750

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.807, [92mTest[0m: 2.456, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56721
[1mStep[0m  [10/106], [94mLoss[0m : 1.74831
[1mStep[0m  [20/106], [94mLoss[0m : 1.82880
[1mStep[0m  [30/106], [94mLoss[0m : 1.78848
[1mStep[0m  [40/106], [94mLoss[0m : 1.72420
[1mStep[0m  [50/106], [94mLoss[0m : 1.81363
[1mStep[0m  [60/106], [94mLoss[0m : 1.78402
[1mStep[0m  [70/106], [94mLoss[0m : 1.72740
[1mStep[0m  [80/106], [94mLoss[0m : 1.53700
[1mStep[0m  [90/106], [94mLoss[0m : 1.79789
[1mStep[0m  [100/106], [94mLoss[0m : 1.76414

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79122
[1mStep[0m  [10/106], [94mLoss[0m : 1.78417
[1mStep[0m  [20/106], [94mLoss[0m : 1.38943
[1mStep[0m  [30/106], [94mLoss[0m : 2.09046
[1mStep[0m  [40/106], [94mLoss[0m : 1.89057
[1mStep[0m  [50/106], [94mLoss[0m : 1.65188
[1mStep[0m  [60/106], [94mLoss[0m : 1.83927
[1mStep[0m  [70/106], [94mLoss[0m : 2.09586
[1mStep[0m  [80/106], [94mLoss[0m : 2.00545
[1mStep[0m  [90/106], [94mLoss[0m : 1.71526
[1mStep[0m  [100/106], [94mLoss[0m : 1.95142

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.782, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.45536
[1mStep[0m  [10/106], [94mLoss[0m : 1.68147
[1mStep[0m  [20/106], [94mLoss[0m : 1.80625
[1mStep[0m  [30/106], [94mLoss[0m : 1.62477
[1mStep[0m  [40/106], [94mLoss[0m : 1.67367
[1mStep[0m  [50/106], [94mLoss[0m : 1.71967
[1mStep[0m  [60/106], [94mLoss[0m : 1.84935
[1mStep[0m  [70/106], [94mLoss[0m : 1.78246
[1mStep[0m  [80/106], [94mLoss[0m : 1.75565
[1mStep[0m  [90/106], [94mLoss[0m : 1.70451
[1mStep[0m  [100/106], [94mLoss[0m : 1.65405

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62431
[1mStep[0m  [10/106], [94mLoss[0m : 1.68952
[1mStep[0m  [20/106], [94mLoss[0m : 1.68923
[1mStep[0m  [30/106], [94mLoss[0m : 1.81081
[1mStep[0m  [40/106], [94mLoss[0m : 1.77735
[1mStep[0m  [50/106], [94mLoss[0m : 1.89017
[1mStep[0m  [60/106], [94mLoss[0m : 1.90804
[1mStep[0m  [70/106], [94mLoss[0m : 1.79707
[1mStep[0m  [80/106], [94mLoss[0m : 1.93235
[1mStep[0m  [90/106], [94mLoss[0m : 1.62790
[1mStep[0m  [100/106], [94mLoss[0m : 1.77423

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.460, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60293
[1mStep[0m  [10/106], [94mLoss[0m : 1.59803
[1mStep[0m  [20/106], [94mLoss[0m : 1.75395
[1mStep[0m  [30/106], [94mLoss[0m : 1.59577
[1mStep[0m  [40/106], [94mLoss[0m : 1.63035
[1mStep[0m  [50/106], [94mLoss[0m : 1.55977
[1mStep[0m  [60/106], [94mLoss[0m : 1.62257
[1mStep[0m  [70/106], [94mLoss[0m : 1.76289
[1mStep[0m  [80/106], [94mLoss[0m : 1.46936
[1mStep[0m  [90/106], [94mLoss[0m : 1.73343
[1mStep[0m  [100/106], [94mLoss[0m : 1.70856

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72893
[1mStep[0m  [10/106], [94mLoss[0m : 1.65821
[1mStep[0m  [20/106], [94mLoss[0m : 1.75808
[1mStep[0m  [30/106], [94mLoss[0m : 1.87504
[1mStep[0m  [40/106], [94mLoss[0m : 1.84909
[1mStep[0m  [50/106], [94mLoss[0m : 1.87590
[1mStep[0m  [60/106], [94mLoss[0m : 1.52618
[1mStep[0m  [70/106], [94mLoss[0m : 1.57444
[1mStep[0m  [80/106], [94mLoss[0m : 1.75106
[1mStep[0m  [90/106], [94mLoss[0m : 1.67423
[1mStep[0m  [100/106], [94mLoss[0m : 1.65725

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.696, [92mTest[0m: 2.458, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.67455
[1mStep[0m  [10/106], [94mLoss[0m : 1.83949
[1mStep[0m  [20/106], [94mLoss[0m : 1.66102
[1mStep[0m  [30/106], [94mLoss[0m : 1.65456
[1mStep[0m  [40/106], [94mLoss[0m : 1.73767
[1mStep[0m  [50/106], [94mLoss[0m : 1.79255
[1mStep[0m  [60/106], [94mLoss[0m : 1.56746
[1mStep[0m  [70/106], [94mLoss[0m : 1.69484
[1mStep[0m  [80/106], [94mLoss[0m : 1.56451
[1mStep[0m  [90/106], [94mLoss[0m : 1.60429
[1mStep[0m  [100/106], [94mLoss[0m : 1.45415

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.662, [92mTest[0m: 2.461, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75717
[1mStep[0m  [10/106], [94mLoss[0m : 1.56678
[1mStep[0m  [20/106], [94mLoss[0m : 1.52698
[1mStep[0m  [30/106], [94mLoss[0m : 1.71907
[1mStep[0m  [40/106], [94mLoss[0m : 1.41445
[1mStep[0m  [50/106], [94mLoss[0m : 1.63267
[1mStep[0m  [60/106], [94mLoss[0m : 1.57402
[1mStep[0m  [70/106], [94mLoss[0m : 1.61230
[1mStep[0m  [80/106], [94mLoss[0m : 1.45269
[1mStep[0m  [90/106], [94mLoss[0m : 1.62550
[1mStep[0m  [100/106], [94mLoss[0m : 1.61878

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.485, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66952
[1mStep[0m  [10/106], [94mLoss[0m : 1.62111
[1mStep[0m  [20/106], [94mLoss[0m : 1.42567
[1mStep[0m  [30/106], [94mLoss[0m : 1.68869
[1mStep[0m  [40/106], [94mLoss[0m : 1.38856
[1mStep[0m  [50/106], [94mLoss[0m : 1.33905
[1mStep[0m  [60/106], [94mLoss[0m : 1.58539
[1mStep[0m  [70/106], [94mLoss[0m : 1.74975
[1mStep[0m  [80/106], [94mLoss[0m : 1.61114
[1mStep[0m  [90/106], [94mLoss[0m : 1.74683
[1mStep[0m  [100/106], [94mLoss[0m : 1.64357

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.459
====================================

Phase 2 - Evaluation MAE:  2.4589846898924628
MAE score P1      2.385268
MAE score P2      2.458985
loss              1.633244
learning_rate     0.002575
batch_size             128
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 10.79033
[1mStep[0m  [21/213], [94mLoss[0m : 10.81156
[1mStep[0m  [42/213], [94mLoss[0m : 11.00057
[1mStep[0m  [63/213], [94mLoss[0m : 10.67284
[1mStep[0m  [84/213], [94mLoss[0m : 9.33575
[1mStep[0m  [105/213], [94mLoss[0m : 9.82179
[1mStep[0m  [126/213], [94mLoss[0m : 10.47082
[1mStep[0m  [147/213], [94mLoss[0m : 8.55445
[1mStep[0m  [168/213], [94mLoss[0m : 10.01824
[1mStep[0m  [189/213], [94mLoss[0m : 9.42278
[1mStep[0m  [210/213], [94mLoss[0m : 8.89085

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.921, [92mTest[0m: 10.900, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.43018
[1mStep[0m  [21/213], [94mLoss[0m : 8.15699
[1mStep[0m  [42/213], [94mLoss[0m : 8.59298
[1mStep[0m  [63/213], [94mLoss[0m : 7.77622
[1mStep[0m  [84/213], [94mLoss[0m : 7.47146
[1mStep[0m  [105/213], [94mLoss[0m : 7.43943
[1mStep[0m  [126/213], [94mLoss[0m : 6.98377
[1mStep[0m  [147/213], [94mLoss[0m : 7.36812
[1mStep[0m  [168/213], [94mLoss[0m : 6.36284
[1mStep[0m  [189/213], [94mLoss[0m : 6.89765
[1mStep[0m  [210/213], [94mLoss[0m : 6.84965

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.637, [92mTest[0m: 8.165, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.50530
[1mStep[0m  [21/213], [94mLoss[0m : 6.36104
[1mStep[0m  [42/213], [94mLoss[0m : 6.75094
[1mStep[0m  [63/213], [94mLoss[0m : 6.63406
[1mStep[0m  [84/213], [94mLoss[0m : 6.16496
[1mStep[0m  [105/213], [94mLoss[0m : 5.30286
[1mStep[0m  [126/213], [94mLoss[0m : 5.78631
[1mStep[0m  [147/213], [94mLoss[0m : 4.45767
[1mStep[0m  [168/213], [94mLoss[0m : 4.89322
[1mStep[0m  [189/213], [94mLoss[0m : 4.02692
[1mStep[0m  [210/213], [94mLoss[0m : 4.31853

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 5.449, [92mTest[0m: 5.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.47545
[1mStep[0m  [21/213], [94mLoss[0m : 3.69562
[1mStep[0m  [42/213], [94mLoss[0m : 4.10865
[1mStep[0m  [63/213], [94mLoss[0m : 3.36616
[1mStep[0m  [84/213], [94mLoss[0m : 3.22933
[1mStep[0m  [105/213], [94mLoss[0m : 3.17906
[1mStep[0m  [126/213], [94mLoss[0m : 2.79774
[1mStep[0m  [147/213], [94mLoss[0m : 3.32854
[1mStep[0m  [168/213], [94mLoss[0m : 2.73150
[1mStep[0m  [189/213], [94mLoss[0m : 2.99698
[1mStep[0m  [210/213], [94mLoss[0m : 3.12047

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.356, [92mTest[0m: 3.310, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63921
[1mStep[0m  [21/213], [94mLoss[0m : 2.90089
[1mStep[0m  [42/213], [94mLoss[0m : 2.88841
[1mStep[0m  [63/213], [94mLoss[0m : 2.70269
[1mStep[0m  [84/213], [94mLoss[0m : 2.99963
[1mStep[0m  [105/213], [94mLoss[0m : 3.14614
[1mStep[0m  [126/213], [94mLoss[0m : 2.86506
[1mStep[0m  [147/213], [94mLoss[0m : 2.62358
[1mStep[0m  [168/213], [94mLoss[0m : 2.79088
[1mStep[0m  [189/213], [94mLoss[0m : 2.74302
[1mStep[0m  [210/213], [94mLoss[0m : 2.85356

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.829, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51216
[1mStep[0m  [21/213], [94mLoss[0m : 2.56079
[1mStep[0m  [42/213], [94mLoss[0m : 3.20040
[1mStep[0m  [63/213], [94mLoss[0m : 2.81621
[1mStep[0m  [84/213], [94mLoss[0m : 2.42892
[1mStep[0m  [105/213], [94mLoss[0m : 3.10516
[1mStep[0m  [126/213], [94mLoss[0m : 2.60754
[1mStep[0m  [147/213], [94mLoss[0m : 2.50363
[1mStep[0m  [168/213], [94mLoss[0m : 2.71165
[1mStep[0m  [189/213], [94mLoss[0m : 3.14318
[1mStep[0m  [210/213], [94mLoss[0m : 3.22145

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.750, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.67516
[1mStep[0m  [21/213], [94mLoss[0m : 2.38504
[1mStep[0m  [42/213], [94mLoss[0m : 2.33809
[1mStep[0m  [63/213], [94mLoss[0m : 3.00291
[1mStep[0m  [84/213], [94mLoss[0m : 3.02085
[1mStep[0m  [105/213], [94mLoss[0m : 2.70037
[1mStep[0m  [126/213], [94mLoss[0m : 2.63586
[1mStep[0m  [147/213], [94mLoss[0m : 2.53026
[1mStep[0m  [168/213], [94mLoss[0m : 2.65999
[1mStep[0m  [189/213], [94mLoss[0m : 2.81474
[1mStep[0m  [210/213], [94mLoss[0m : 2.92367

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.737, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73329
[1mStep[0m  [21/213], [94mLoss[0m : 2.92004
[1mStep[0m  [42/213], [94mLoss[0m : 2.62072
[1mStep[0m  [63/213], [94mLoss[0m : 2.65322
[1mStep[0m  [84/213], [94mLoss[0m : 2.79011
[1mStep[0m  [105/213], [94mLoss[0m : 2.82134
[1mStep[0m  [126/213], [94mLoss[0m : 2.70949
[1mStep[0m  [147/213], [94mLoss[0m : 2.56425
[1mStep[0m  [168/213], [94mLoss[0m : 2.43503
[1mStep[0m  [189/213], [94mLoss[0m : 2.63120
[1mStep[0m  [210/213], [94mLoss[0m : 3.07103

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.679, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61503
[1mStep[0m  [21/213], [94mLoss[0m : 2.75978
[1mStep[0m  [42/213], [94mLoss[0m : 2.66305
[1mStep[0m  [63/213], [94mLoss[0m : 2.60806
[1mStep[0m  [84/213], [94mLoss[0m : 2.83585
[1mStep[0m  [105/213], [94mLoss[0m : 2.55812
[1mStep[0m  [126/213], [94mLoss[0m : 2.72470
[1mStep[0m  [147/213], [94mLoss[0m : 3.04427
[1mStep[0m  [168/213], [94mLoss[0m : 2.46647
[1mStep[0m  [189/213], [94mLoss[0m : 3.14662
[1mStep[0m  [210/213], [94mLoss[0m : 2.60831

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.701, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.79341
[1mStep[0m  [21/213], [94mLoss[0m : 2.43148
[1mStep[0m  [42/213], [94mLoss[0m : 2.83386
[1mStep[0m  [63/213], [94mLoss[0m : 2.38096
[1mStep[0m  [84/213], [94mLoss[0m : 2.66629
[1mStep[0m  [105/213], [94mLoss[0m : 2.05509
[1mStep[0m  [126/213], [94mLoss[0m : 2.44787
[1mStep[0m  [147/213], [94mLoss[0m : 2.59595
[1mStep[0m  [168/213], [94mLoss[0m : 2.72605
[1mStep[0m  [189/213], [94mLoss[0m : 2.67047
[1mStep[0m  [210/213], [94mLoss[0m : 2.47948

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.651, [92mTest[0m: 2.430, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47327
[1mStep[0m  [21/213], [94mLoss[0m : 2.60784
[1mStep[0m  [42/213], [94mLoss[0m : 2.59851
[1mStep[0m  [63/213], [94mLoss[0m : 2.35347
[1mStep[0m  [84/213], [94mLoss[0m : 2.94259
[1mStep[0m  [105/213], [94mLoss[0m : 2.31477
[1mStep[0m  [126/213], [94mLoss[0m : 2.52346
[1mStep[0m  [147/213], [94mLoss[0m : 3.25310
[1mStep[0m  [168/213], [94mLoss[0m : 2.39850
[1mStep[0m  [189/213], [94mLoss[0m : 2.74634
[1mStep[0m  [210/213], [94mLoss[0m : 2.69758

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.05899
[1mStep[0m  [21/213], [94mLoss[0m : 2.27323
[1mStep[0m  [42/213], [94mLoss[0m : 2.45830
[1mStep[0m  [63/213], [94mLoss[0m : 2.32939
[1mStep[0m  [84/213], [94mLoss[0m : 2.21415
[1mStep[0m  [105/213], [94mLoss[0m : 2.68878
[1mStep[0m  [126/213], [94mLoss[0m : 2.45959
[1mStep[0m  [147/213], [94mLoss[0m : 2.50473
[1mStep[0m  [168/213], [94mLoss[0m : 2.43458
[1mStep[0m  [189/213], [94mLoss[0m : 2.81445
[1mStep[0m  [210/213], [94mLoss[0m : 2.38933

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73081
[1mStep[0m  [21/213], [94mLoss[0m : 2.93249
[1mStep[0m  [42/213], [94mLoss[0m : 2.62397
[1mStep[0m  [63/213], [94mLoss[0m : 3.14698
[1mStep[0m  [84/213], [94mLoss[0m : 2.69324
[1mStep[0m  [105/213], [94mLoss[0m : 2.46543
[1mStep[0m  [126/213], [94mLoss[0m : 2.89386
[1mStep[0m  [147/213], [94mLoss[0m : 2.93072
[1mStep[0m  [168/213], [94mLoss[0m : 2.61879
[1mStep[0m  [189/213], [94mLoss[0m : 2.85841
[1mStep[0m  [210/213], [94mLoss[0m : 2.78159

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.398, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.15494
[1mStep[0m  [21/213], [94mLoss[0m : 2.51399
[1mStep[0m  [42/213], [94mLoss[0m : 2.40662
[1mStep[0m  [63/213], [94mLoss[0m : 2.89037
[1mStep[0m  [84/213], [94mLoss[0m : 2.44868
[1mStep[0m  [105/213], [94mLoss[0m : 2.29317
[1mStep[0m  [126/213], [94mLoss[0m : 2.58408
[1mStep[0m  [147/213], [94mLoss[0m : 2.22002
[1mStep[0m  [168/213], [94mLoss[0m : 3.20932
[1mStep[0m  [189/213], [94mLoss[0m : 2.34423
[1mStep[0m  [210/213], [94mLoss[0m : 2.88362

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.03461
[1mStep[0m  [21/213], [94mLoss[0m : 2.56766
[1mStep[0m  [42/213], [94mLoss[0m : 2.36640
[1mStep[0m  [63/213], [94mLoss[0m : 2.32792
[1mStep[0m  [84/213], [94mLoss[0m : 2.73802
[1mStep[0m  [105/213], [94mLoss[0m : 2.68806
[1mStep[0m  [126/213], [94mLoss[0m : 2.94062
[1mStep[0m  [147/213], [94mLoss[0m : 2.23182
[1mStep[0m  [168/213], [94mLoss[0m : 2.45310
[1mStep[0m  [189/213], [94mLoss[0m : 2.79604
[1mStep[0m  [210/213], [94mLoss[0m : 2.70905

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28309
[1mStep[0m  [21/213], [94mLoss[0m : 2.57761
[1mStep[0m  [42/213], [94mLoss[0m : 2.39088
[1mStep[0m  [63/213], [94mLoss[0m : 2.70609
[1mStep[0m  [84/213], [94mLoss[0m : 2.64351
[1mStep[0m  [105/213], [94mLoss[0m : 2.55096
[1mStep[0m  [126/213], [94mLoss[0m : 2.31515
[1mStep[0m  [147/213], [94mLoss[0m : 2.21958
[1mStep[0m  [168/213], [94mLoss[0m : 2.43098
[1mStep[0m  [189/213], [94mLoss[0m : 2.50915
[1mStep[0m  [210/213], [94mLoss[0m : 2.61120

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.68735
[1mStep[0m  [21/213], [94mLoss[0m : 2.20542
[1mStep[0m  [42/213], [94mLoss[0m : 3.04488
[1mStep[0m  [63/213], [94mLoss[0m : 2.59380
[1mStep[0m  [84/213], [94mLoss[0m : 3.28953
[1mStep[0m  [105/213], [94mLoss[0m : 2.49261
[1mStep[0m  [126/213], [94mLoss[0m : 2.62153
[1mStep[0m  [147/213], [94mLoss[0m : 3.24390
[1mStep[0m  [168/213], [94mLoss[0m : 2.69702
[1mStep[0m  [189/213], [94mLoss[0m : 2.49829
[1mStep[0m  [210/213], [94mLoss[0m : 3.28883

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.579, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65063
[1mStep[0m  [21/213], [94mLoss[0m : 2.55960
[1mStep[0m  [42/213], [94mLoss[0m : 2.82299
[1mStep[0m  [63/213], [94mLoss[0m : 2.56766
[1mStep[0m  [84/213], [94mLoss[0m : 2.52892
[1mStep[0m  [105/213], [94mLoss[0m : 2.76464
[1mStep[0m  [126/213], [94mLoss[0m : 2.82592
[1mStep[0m  [147/213], [94mLoss[0m : 2.35898
[1mStep[0m  [168/213], [94mLoss[0m : 2.68730
[1mStep[0m  [189/213], [94mLoss[0m : 2.46315
[1mStep[0m  [210/213], [94mLoss[0m : 2.68232

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40841
[1mStep[0m  [21/213], [94mLoss[0m : 2.09882
[1mStep[0m  [42/213], [94mLoss[0m : 3.05138
[1mStep[0m  [63/213], [94mLoss[0m : 2.37630
[1mStep[0m  [84/213], [94mLoss[0m : 2.80852
[1mStep[0m  [105/213], [94mLoss[0m : 2.29328
[1mStep[0m  [126/213], [94mLoss[0m : 3.04153
[1mStep[0m  [147/213], [94mLoss[0m : 2.47026
[1mStep[0m  [168/213], [94mLoss[0m : 2.62898
[1mStep[0m  [189/213], [94mLoss[0m : 2.31439
[1mStep[0m  [210/213], [94mLoss[0m : 2.66279

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.88355
[1mStep[0m  [21/213], [94mLoss[0m : 2.64674
[1mStep[0m  [42/213], [94mLoss[0m : 2.54382
[1mStep[0m  [63/213], [94mLoss[0m : 2.81472
[1mStep[0m  [84/213], [94mLoss[0m : 2.79744
[1mStep[0m  [105/213], [94mLoss[0m : 2.46232
[1mStep[0m  [126/213], [94mLoss[0m : 2.56040
[1mStep[0m  [147/213], [94mLoss[0m : 3.06463
[1mStep[0m  [168/213], [94mLoss[0m : 2.38800
[1mStep[0m  [189/213], [94mLoss[0m : 2.29430
[1mStep[0m  [210/213], [94mLoss[0m : 2.94084

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55385
[1mStep[0m  [21/213], [94mLoss[0m : 2.74569
[1mStep[0m  [42/213], [94mLoss[0m : 2.53085
[1mStep[0m  [63/213], [94mLoss[0m : 2.69892
[1mStep[0m  [84/213], [94mLoss[0m : 2.63896
[1mStep[0m  [105/213], [94mLoss[0m : 2.02894
[1mStep[0m  [126/213], [94mLoss[0m : 2.44501
[1mStep[0m  [147/213], [94mLoss[0m : 2.22546
[1mStep[0m  [168/213], [94mLoss[0m : 2.09677
[1mStep[0m  [189/213], [94mLoss[0m : 2.58078
[1mStep[0m  [210/213], [94mLoss[0m : 2.88227

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62427
[1mStep[0m  [21/213], [94mLoss[0m : 2.60978
[1mStep[0m  [42/213], [94mLoss[0m : 2.12685
[1mStep[0m  [63/213], [94mLoss[0m : 2.39436
[1mStep[0m  [84/213], [94mLoss[0m : 2.24748
[1mStep[0m  [105/213], [94mLoss[0m : 2.66971
[1mStep[0m  [126/213], [94mLoss[0m : 2.81070
[1mStep[0m  [147/213], [94mLoss[0m : 2.75894
[1mStep[0m  [168/213], [94mLoss[0m : 2.02675
[1mStep[0m  [189/213], [94mLoss[0m : 2.59221
[1mStep[0m  [210/213], [94mLoss[0m : 2.49449

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.393, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.07363
[1mStep[0m  [21/213], [94mLoss[0m : 2.37822
[1mStep[0m  [42/213], [94mLoss[0m : 2.35610
[1mStep[0m  [63/213], [94mLoss[0m : 2.10982
[1mStep[0m  [84/213], [94mLoss[0m : 2.61729
[1mStep[0m  [105/213], [94mLoss[0m : 2.44740
[1mStep[0m  [126/213], [94mLoss[0m : 2.38921
[1mStep[0m  [147/213], [94mLoss[0m : 2.62460
[1mStep[0m  [168/213], [94mLoss[0m : 2.62091
[1mStep[0m  [189/213], [94mLoss[0m : 2.41732
[1mStep[0m  [210/213], [94mLoss[0m : 2.41637

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48578
[1mStep[0m  [21/213], [94mLoss[0m : 2.71670
[1mStep[0m  [42/213], [94mLoss[0m : 2.55289
[1mStep[0m  [63/213], [94mLoss[0m : 2.22947
[1mStep[0m  [84/213], [94mLoss[0m : 2.15776
[1mStep[0m  [105/213], [94mLoss[0m : 2.79738
[1mStep[0m  [126/213], [94mLoss[0m : 2.45649
[1mStep[0m  [147/213], [94mLoss[0m : 2.88255
[1mStep[0m  [168/213], [94mLoss[0m : 2.15504
[1mStep[0m  [189/213], [94mLoss[0m : 2.06336
[1mStep[0m  [210/213], [94mLoss[0m : 2.64132

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58609
[1mStep[0m  [21/213], [94mLoss[0m : 2.60571
[1mStep[0m  [42/213], [94mLoss[0m : 2.74202
[1mStep[0m  [63/213], [94mLoss[0m : 2.46295
[1mStep[0m  [84/213], [94mLoss[0m : 2.30493
[1mStep[0m  [105/213], [94mLoss[0m : 2.41181
[1mStep[0m  [126/213], [94mLoss[0m : 2.58186
[1mStep[0m  [147/213], [94mLoss[0m : 2.37726
[1mStep[0m  [168/213], [94mLoss[0m : 2.38205
[1mStep[0m  [189/213], [94mLoss[0m : 2.84935
[1mStep[0m  [210/213], [94mLoss[0m : 2.83694

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29724
[1mStep[0m  [21/213], [94mLoss[0m : 2.45620
[1mStep[0m  [42/213], [94mLoss[0m : 2.13231
[1mStep[0m  [63/213], [94mLoss[0m : 2.63555
[1mStep[0m  [84/213], [94mLoss[0m : 2.03776
[1mStep[0m  [105/213], [94mLoss[0m : 2.50531
[1mStep[0m  [126/213], [94mLoss[0m : 2.67939
[1mStep[0m  [147/213], [94mLoss[0m : 2.10341
[1mStep[0m  [168/213], [94mLoss[0m : 2.59478
[1mStep[0m  [189/213], [94mLoss[0m : 2.48647
[1mStep[0m  [210/213], [94mLoss[0m : 2.39744

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51102
[1mStep[0m  [21/213], [94mLoss[0m : 2.09230
[1mStep[0m  [42/213], [94mLoss[0m : 2.50600
[1mStep[0m  [63/213], [94mLoss[0m : 2.61562
[1mStep[0m  [84/213], [94mLoss[0m : 2.46662
[1mStep[0m  [105/213], [94mLoss[0m : 2.68277
[1mStep[0m  [126/213], [94mLoss[0m : 2.52056
[1mStep[0m  [147/213], [94mLoss[0m : 2.70295
[1mStep[0m  [168/213], [94mLoss[0m : 2.47146
[1mStep[0m  [189/213], [94mLoss[0m : 2.07910
[1mStep[0m  [210/213], [94mLoss[0m : 2.47042

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39709
[1mStep[0m  [21/213], [94mLoss[0m : 2.18828
[1mStep[0m  [42/213], [94mLoss[0m : 2.13931
[1mStep[0m  [63/213], [94mLoss[0m : 2.51504
[1mStep[0m  [84/213], [94mLoss[0m : 2.47753
[1mStep[0m  [105/213], [94mLoss[0m : 2.03783
[1mStep[0m  [126/213], [94mLoss[0m : 2.17194
[1mStep[0m  [147/213], [94mLoss[0m : 2.32553
[1mStep[0m  [168/213], [94mLoss[0m : 2.11730
[1mStep[0m  [189/213], [94mLoss[0m : 2.59542
[1mStep[0m  [210/213], [94mLoss[0m : 1.96037

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.83843
[1mStep[0m  [21/213], [94mLoss[0m : 2.47516
[1mStep[0m  [42/213], [94mLoss[0m : 2.83570
[1mStep[0m  [63/213], [94mLoss[0m : 2.81361
[1mStep[0m  [84/213], [94mLoss[0m : 2.90107
[1mStep[0m  [105/213], [94mLoss[0m : 2.25410
[1mStep[0m  [126/213], [94mLoss[0m : 2.30394
[1mStep[0m  [147/213], [94mLoss[0m : 2.51407
[1mStep[0m  [168/213], [94mLoss[0m : 2.40604
[1mStep[0m  [189/213], [94mLoss[0m : 2.63092
[1mStep[0m  [210/213], [94mLoss[0m : 2.83175

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44437
[1mStep[0m  [21/213], [94mLoss[0m : 2.16640
[1mStep[0m  [42/213], [94mLoss[0m : 2.90147
[1mStep[0m  [63/213], [94mLoss[0m : 2.55121
[1mStep[0m  [84/213], [94mLoss[0m : 2.66205
[1mStep[0m  [105/213], [94mLoss[0m : 2.36304
[1mStep[0m  [126/213], [94mLoss[0m : 2.25343
[1mStep[0m  [147/213], [94mLoss[0m : 2.03660
[1mStep[0m  [168/213], [94mLoss[0m : 2.01378
[1mStep[0m  [189/213], [94mLoss[0m : 2.19118
[1mStep[0m  [210/213], [94mLoss[0m : 2.38772

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.389
====================================

Phase 1 - Evaluation MAE:  2.3888049710471675
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/213], [94mLoss[0m : 2.24004
[1mStep[0m  [21/213], [94mLoss[0m : 2.77970
[1mStep[0m  [42/213], [94mLoss[0m : 2.96689
[1mStep[0m  [63/213], [94mLoss[0m : 2.66354
[1mStep[0m  [84/213], [94mLoss[0m : 2.42907
[1mStep[0m  [105/213], [94mLoss[0m : 2.58566
[1mStep[0m  [126/213], [94mLoss[0m : 3.01246
[1mStep[0m  [147/213], [94mLoss[0m : 2.31961
[1mStep[0m  [168/213], [94mLoss[0m : 2.90431
[1mStep[0m  [189/213], [94mLoss[0m : 2.36741
[1mStep[0m  [210/213], [94mLoss[0m : 2.49575

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54840
[1mStep[0m  [21/213], [94mLoss[0m : 2.54901
[1mStep[0m  [42/213], [94mLoss[0m : 2.94144
[1mStep[0m  [63/213], [94mLoss[0m : 2.87599
[1mStep[0m  [84/213], [94mLoss[0m : 2.22234
[1mStep[0m  [105/213], [94mLoss[0m : 2.11566
[1mStep[0m  [126/213], [94mLoss[0m : 2.30651
[1mStep[0m  [147/213], [94mLoss[0m : 2.75546
[1mStep[0m  [168/213], [94mLoss[0m : 2.09984
[1mStep[0m  [189/213], [94mLoss[0m : 2.54601
[1mStep[0m  [210/213], [94mLoss[0m : 2.67644

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.70178
[1mStep[0m  [21/213], [94mLoss[0m : 2.40359
[1mStep[0m  [42/213], [94mLoss[0m : 2.56656
[1mStep[0m  [63/213], [94mLoss[0m : 2.60089
[1mStep[0m  [84/213], [94mLoss[0m : 2.12072
[1mStep[0m  [105/213], [94mLoss[0m : 2.56721
[1mStep[0m  [126/213], [94mLoss[0m : 2.35005
[1mStep[0m  [147/213], [94mLoss[0m : 3.10260
[1mStep[0m  [168/213], [94mLoss[0m : 2.34416
[1mStep[0m  [189/213], [94mLoss[0m : 2.02119
[1mStep[0m  [210/213], [94mLoss[0m : 2.02447

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.89392
[1mStep[0m  [21/213], [94mLoss[0m : 2.58605
[1mStep[0m  [42/213], [94mLoss[0m : 2.91147
[1mStep[0m  [63/213], [94mLoss[0m : 2.15527
[1mStep[0m  [84/213], [94mLoss[0m : 2.27383
[1mStep[0m  [105/213], [94mLoss[0m : 2.12264
[1mStep[0m  [126/213], [94mLoss[0m : 2.33258
[1mStep[0m  [147/213], [94mLoss[0m : 2.23561
[1mStep[0m  [168/213], [94mLoss[0m : 2.02681
[1mStep[0m  [189/213], [94mLoss[0m : 2.00641
[1mStep[0m  [210/213], [94mLoss[0m : 2.28162

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.489, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24068
[1mStep[0m  [21/213], [94mLoss[0m : 2.16624
[1mStep[0m  [42/213], [94mLoss[0m : 2.15534
[1mStep[0m  [63/213], [94mLoss[0m : 1.85226
[1mStep[0m  [84/213], [94mLoss[0m : 2.72774
[1mStep[0m  [105/213], [94mLoss[0m : 2.19401
[1mStep[0m  [126/213], [94mLoss[0m : 2.15470
[1mStep[0m  [147/213], [94mLoss[0m : 2.30467
[1mStep[0m  [168/213], [94mLoss[0m : 2.10741
[1mStep[0m  [189/213], [94mLoss[0m : 2.49480
[1mStep[0m  [210/213], [94mLoss[0m : 2.24090

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14179
[1mStep[0m  [21/213], [94mLoss[0m : 2.36931
[1mStep[0m  [42/213], [94mLoss[0m : 2.34394
[1mStep[0m  [63/213], [94mLoss[0m : 2.27230
[1mStep[0m  [84/213], [94mLoss[0m : 2.42896
[1mStep[0m  [105/213], [94mLoss[0m : 2.14630
[1mStep[0m  [126/213], [94mLoss[0m : 2.29506
[1mStep[0m  [147/213], [94mLoss[0m : 2.61498
[1mStep[0m  [168/213], [94mLoss[0m : 2.77715
[1mStep[0m  [189/213], [94mLoss[0m : 2.43764
[1mStep[0m  [210/213], [94mLoss[0m : 2.80689

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97479
[1mStep[0m  [21/213], [94mLoss[0m : 2.64473
[1mStep[0m  [42/213], [94mLoss[0m : 1.95149
[1mStep[0m  [63/213], [94mLoss[0m : 2.18280
[1mStep[0m  [84/213], [94mLoss[0m : 1.87088
[1mStep[0m  [105/213], [94mLoss[0m : 1.92622
[1mStep[0m  [126/213], [94mLoss[0m : 2.08795
[1mStep[0m  [147/213], [94mLoss[0m : 2.07764
[1mStep[0m  [168/213], [94mLoss[0m : 2.50979
[1mStep[0m  [189/213], [94mLoss[0m : 2.37325
[1mStep[0m  [210/213], [94mLoss[0m : 2.08151

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43655
[1mStep[0m  [21/213], [94mLoss[0m : 2.24415
[1mStep[0m  [42/213], [94mLoss[0m : 2.18463
[1mStep[0m  [63/213], [94mLoss[0m : 1.91228
[1mStep[0m  [84/213], [94mLoss[0m : 1.87420
[1mStep[0m  [105/213], [94mLoss[0m : 2.21237
[1mStep[0m  [126/213], [94mLoss[0m : 2.16561
[1mStep[0m  [147/213], [94mLoss[0m : 1.81261
[1mStep[0m  [168/213], [94mLoss[0m : 2.29952
[1mStep[0m  [189/213], [94mLoss[0m : 2.56498
[1mStep[0m  [210/213], [94mLoss[0m : 1.49465

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.177, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92650
[1mStep[0m  [21/213], [94mLoss[0m : 2.08173
[1mStep[0m  [42/213], [94mLoss[0m : 1.93354
[1mStep[0m  [63/213], [94mLoss[0m : 2.14956
[1mStep[0m  [84/213], [94mLoss[0m : 2.59084
[1mStep[0m  [105/213], [94mLoss[0m : 2.38499
[1mStep[0m  [126/213], [94mLoss[0m : 2.07914
[1mStep[0m  [147/213], [94mLoss[0m : 2.39404
[1mStep[0m  [168/213], [94mLoss[0m : 2.39264
[1mStep[0m  [189/213], [94mLoss[0m : 2.04395
[1mStep[0m  [210/213], [94mLoss[0m : 1.91611

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.138, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.65514
[1mStep[0m  [21/213], [94mLoss[0m : 2.00675
[1mStep[0m  [42/213], [94mLoss[0m : 2.27525
[1mStep[0m  [63/213], [94mLoss[0m : 2.27071
[1mStep[0m  [84/213], [94mLoss[0m : 2.01564
[1mStep[0m  [105/213], [94mLoss[0m : 2.00680
[1mStep[0m  [126/213], [94mLoss[0m : 1.72862
[1mStep[0m  [147/213], [94mLoss[0m : 1.82843
[1mStep[0m  [168/213], [94mLoss[0m : 1.67880
[1mStep[0m  [189/213], [94mLoss[0m : 2.05310
[1mStep[0m  [210/213], [94mLoss[0m : 1.93844

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33117
[1mStep[0m  [21/213], [94mLoss[0m : 2.33020
[1mStep[0m  [42/213], [94mLoss[0m : 1.78020
[1mStep[0m  [63/213], [94mLoss[0m : 1.90652
[1mStep[0m  [84/213], [94mLoss[0m : 1.71689
[1mStep[0m  [105/213], [94mLoss[0m : 2.26195
[1mStep[0m  [126/213], [94mLoss[0m : 2.11358
[1mStep[0m  [147/213], [94mLoss[0m : 2.22942
[1mStep[0m  [168/213], [94mLoss[0m : 1.89614
[1mStep[0m  [189/213], [94mLoss[0m : 2.00345
[1mStep[0m  [210/213], [94mLoss[0m : 1.74734

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77900
[1mStep[0m  [21/213], [94mLoss[0m : 2.08029
[1mStep[0m  [42/213], [94mLoss[0m : 2.15355
[1mStep[0m  [63/213], [94mLoss[0m : 2.01509
[1mStep[0m  [84/213], [94mLoss[0m : 1.98914
[1mStep[0m  [105/213], [94mLoss[0m : 1.84019
[1mStep[0m  [126/213], [94mLoss[0m : 2.46609
[1mStep[0m  [147/213], [94mLoss[0m : 1.80252
[1mStep[0m  [168/213], [94mLoss[0m : 2.02905
[1mStep[0m  [189/213], [94mLoss[0m : 2.36147
[1mStep[0m  [210/213], [94mLoss[0m : 2.21123

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66839
[1mStep[0m  [21/213], [94mLoss[0m : 1.78712
[1mStep[0m  [42/213], [94mLoss[0m : 1.81633
[1mStep[0m  [63/213], [94mLoss[0m : 2.00531
[1mStep[0m  [84/213], [94mLoss[0m : 1.80963
[1mStep[0m  [105/213], [94mLoss[0m : 1.74751
[1mStep[0m  [126/213], [94mLoss[0m : 2.47574
[1mStep[0m  [147/213], [94mLoss[0m : 2.04900
[1mStep[0m  [168/213], [94mLoss[0m : 2.05099
[1mStep[0m  [189/213], [94mLoss[0m : 1.62765
[1mStep[0m  [210/213], [94mLoss[0m : 2.07545

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.41202
[1mStep[0m  [21/213], [94mLoss[0m : 2.20500
[1mStep[0m  [42/213], [94mLoss[0m : 2.01036
[1mStep[0m  [63/213], [94mLoss[0m : 1.83740
[1mStep[0m  [84/213], [94mLoss[0m : 1.92800
[1mStep[0m  [105/213], [94mLoss[0m : 1.80059
[1mStep[0m  [126/213], [94mLoss[0m : 2.03785
[1mStep[0m  [147/213], [94mLoss[0m : 2.09797
[1mStep[0m  [168/213], [94mLoss[0m : 1.83008
[1mStep[0m  [189/213], [94mLoss[0m : 1.92683
[1mStep[0m  [210/213], [94mLoss[0m : 1.70322

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.86295
[1mStep[0m  [21/213], [94mLoss[0m : 1.71931
[1mStep[0m  [42/213], [94mLoss[0m : 1.91624
[1mStep[0m  [63/213], [94mLoss[0m : 1.88467
[1mStep[0m  [84/213], [94mLoss[0m : 1.94350
[1mStep[0m  [105/213], [94mLoss[0m : 2.32025
[1mStep[0m  [126/213], [94mLoss[0m : 1.97871
[1mStep[0m  [147/213], [94mLoss[0m : 2.02349
[1mStep[0m  [168/213], [94mLoss[0m : 2.26828
[1mStep[0m  [189/213], [94mLoss[0m : 1.83577
[1mStep[0m  [210/213], [94mLoss[0m : 1.78158

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.920, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67495
[1mStep[0m  [21/213], [94mLoss[0m : 1.79403
[1mStep[0m  [42/213], [94mLoss[0m : 1.76755
[1mStep[0m  [63/213], [94mLoss[0m : 1.79718
[1mStep[0m  [84/213], [94mLoss[0m : 1.78564
[1mStep[0m  [105/213], [94mLoss[0m : 1.87782
[1mStep[0m  [126/213], [94mLoss[0m : 1.85913
[1mStep[0m  [147/213], [94mLoss[0m : 1.85055
[1mStep[0m  [168/213], [94mLoss[0m : 2.18946
[1mStep[0m  [189/213], [94mLoss[0m : 1.98761
[1mStep[0m  [210/213], [94mLoss[0m : 1.91222

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66585
[1mStep[0m  [21/213], [94mLoss[0m : 1.84492
[1mStep[0m  [42/213], [94mLoss[0m : 2.04175
[1mStep[0m  [63/213], [94mLoss[0m : 1.77254
[1mStep[0m  [84/213], [94mLoss[0m : 2.30483
[1mStep[0m  [105/213], [94mLoss[0m : 2.06670
[1mStep[0m  [126/213], [94mLoss[0m : 1.61613
[1mStep[0m  [147/213], [94mLoss[0m : 1.79050
[1mStep[0m  [168/213], [94mLoss[0m : 1.70851
[1mStep[0m  [189/213], [94mLoss[0m : 2.02312
[1mStep[0m  [210/213], [94mLoss[0m : 2.51384

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.39671
[1mStep[0m  [21/213], [94mLoss[0m : 1.86838
[1mStep[0m  [42/213], [94mLoss[0m : 1.65870
[1mStep[0m  [63/213], [94mLoss[0m : 1.67624
[1mStep[0m  [84/213], [94mLoss[0m : 2.34909
[1mStep[0m  [105/213], [94mLoss[0m : 2.00781
[1mStep[0m  [126/213], [94mLoss[0m : 1.71396
[1mStep[0m  [147/213], [94mLoss[0m : 1.70204
[1mStep[0m  [168/213], [94mLoss[0m : 1.88354
[1mStep[0m  [189/213], [94mLoss[0m : 1.86265
[1mStep[0m  [210/213], [94mLoss[0m : 1.53483

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.812, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.70405
[1mStep[0m  [21/213], [94mLoss[0m : 2.12410
[1mStep[0m  [42/213], [94mLoss[0m : 1.71369
[1mStep[0m  [63/213], [94mLoss[0m : 1.70586
[1mStep[0m  [84/213], [94mLoss[0m : 1.71521
[1mStep[0m  [105/213], [94mLoss[0m : 1.98809
[1mStep[0m  [126/213], [94mLoss[0m : 2.29526
[1mStep[0m  [147/213], [94mLoss[0m : 2.25316
[1mStep[0m  [168/213], [94mLoss[0m : 2.39949
[1mStep[0m  [189/213], [94mLoss[0m : 1.58273
[1mStep[0m  [210/213], [94mLoss[0m : 1.92327

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56608
[1mStep[0m  [21/213], [94mLoss[0m : 1.49529
[1mStep[0m  [42/213], [94mLoss[0m : 1.62192
[1mStep[0m  [63/213], [94mLoss[0m : 1.78274
[1mStep[0m  [84/213], [94mLoss[0m : 1.79521
[1mStep[0m  [105/213], [94mLoss[0m : 1.93635
[1mStep[0m  [126/213], [94mLoss[0m : 1.95519
[1mStep[0m  [147/213], [94mLoss[0m : 1.81256
[1mStep[0m  [168/213], [94mLoss[0m : 1.67814
[1mStep[0m  [189/213], [94mLoss[0m : 1.51830
[1mStep[0m  [210/213], [94mLoss[0m : 1.63914

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.763, [92mTest[0m: 2.515, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.75147
[1mStep[0m  [21/213], [94mLoss[0m : 1.60502
[1mStep[0m  [42/213], [94mLoss[0m : 1.75844
[1mStep[0m  [63/213], [94mLoss[0m : 1.78545
[1mStep[0m  [84/213], [94mLoss[0m : 1.98794
[1mStep[0m  [105/213], [94mLoss[0m : 1.82351
[1mStep[0m  [126/213], [94mLoss[0m : 1.82785
[1mStep[0m  [147/213], [94mLoss[0m : 1.65180
[1mStep[0m  [168/213], [94mLoss[0m : 1.95397
[1mStep[0m  [189/213], [94mLoss[0m : 1.58032
[1mStep[0m  [210/213], [94mLoss[0m : 2.14227

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.728, [92mTest[0m: 2.422, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.57577
[1mStep[0m  [21/213], [94mLoss[0m : 1.48502
[1mStep[0m  [42/213], [94mLoss[0m : 1.86264
[1mStep[0m  [63/213], [94mLoss[0m : 2.02783
[1mStep[0m  [84/213], [94mLoss[0m : 1.44022
[1mStep[0m  [105/213], [94mLoss[0m : 1.93873
[1mStep[0m  [126/213], [94mLoss[0m : 1.67268
[1mStep[0m  [147/213], [94mLoss[0m : 1.85766
[1mStep[0m  [168/213], [94mLoss[0m : 1.73393
[1mStep[0m  [189/213], [94mLoss[0m : 1.56897
[1mStep[0m  [210/213], [94mLoss[0m : 1.54535

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.60443
[1mStep[0m  [21/213], [94mLoss[0m : 1.77045
[1mStep[0m  [42/213], [94mLoss[0m : 1.72002
[1mStep[0m  [63/213], [94mLoss[0m : 1.62126
[1mStep[0m  [84/213], [94mLoss[0m : 1.61023
[1mStep[0m  [105/213], [94mLoss[0m : 1.90952
[1mStep[0m  [126/213], [94mLoss[0m : 1.43157
[1mStep[0m  [147/213], [94mLoss[0m : 1.55765
[1mStep[0m  [168/213], [94mLoss[0m : 1.78612
[1mStep[0m  [189/213], [94mLoss[0m : 1.65971
[1mStep[0m  [210/213], [94mLoss[0m : 2.05629

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.672, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.53138
[1mStep[0m  [21/213], [94mLoss[0m : 2.00051
[1mStep[0m  [42/213], [94mLoss[0m : 1.73470
[1mStep[0m  [63/213], [94mLoss[0m : 1.39785
[1mStep[0m  [84/213], [94mLoss[0m : 2.01796
[1mStep[0m  [105/213], [94mLoss[0m : 1.53194
[1mStep[0m  [126/213], [94mLoss[0m : 1.96401
[1mStep[0m  [147/213], [94mLoss[0m : 1.60400
[1mStep[0m  [168/213], [94mLoss[0m : 1.50398
[1mStep[0m  [189/213], [94mLoss[0m : 1.53127
[1mStep[0m  [210/213], [94mLoss[0m : 1.69314

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.663, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89164
[1mStep[0m  [21/213], [94mLoss[0m : 1.80089
[1mStep[0m  [42/213], [94mLoss[0m : 1.92765
[1mStep[0m  [63/213], [94mLoss[0m : 1.56291
[1mStep[0m  [84/213], [94mLoss[0m : 1.85977
[1mStep[0m  [105/213], [94mLoss[0m : 1.58078
[1mStep[0m  [126/213], [94mLoss[0m : 1.65796
[1mStep[0m  [147/213], [94mLoss[0m : 1.65362
[1mStep[0m  [168/213], [94mLoss[0m : 1.80644
[1mStep[0m  [189/213], [94mLoss[0m : 1.73809
[1mStep[0m  [210/213], [94mLoss[0m : 1.60891

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.651, [92mTest[0m: 2.438, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47154
[1mStep[0m  [21/213], [94mLoss[0m : 1.64831
[1mStep[0m  [42/213], [94mLoss[0m : 1.56859
[1mStep[0m  [63/213], [94mLoss[0m : 1.53595
[1mStep[0m  [84/213], [94mLoss[0m : 1.38312
[1mStep[0m  [105/213], [94mLoss[0m : 1.41388
[1mStep[0m  [126/213], [94mLoss[0m : 1.67150
[1mStep[0m  [147/213], [94mLoss[0m : 1.61265
[1mStep[0m  [168/213], [94mLoss[0m : 1.97713
[1mStep[0m  [189/213], [94mLoss[0m : 1.39829
[1mStep[0m  [210/213], [94mLoss[0m : 1.67691

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.613, [92mTest[0m: 2.418, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92621
[1mStep[0m  [21/213], [94mLoss[0m : 1.73862
[1mStep[0m  [42/213], [94mLoss[0m : 1.40231
[1mStep[0m  [63/213], [94mLoss[0m : 1.17948
[1mStep[0m  [84/213], [94mLoss[0m : 1.47428
[1mStep[0m  [105/213], [94mLoss[0m : 1.67037
[1mStep[0m  [126/213], [94mLoss[0m : 1.44022
[1mStep[0m  [147/213], [94mLoss[0m : 1.59100
[1mStep[0m  [168/213], [94mLoss[0m : 1.96191
[1mStep[0m  [189/213], [94mLoss[0m : 1.50809
[1mStep[0m  [210/213], [94mLoss[0m : 1.55905

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.610, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.83219
[1mStep[0m  [21/213], [94mLoss[0m : 1.40114
[1mStep[0m  [42/213], [94mLoss[0m : 1.41632
[1mStep[0m  [63/213], [94mLoss[0m : 1.56530
[1mStep[0m  [84/213], [94mLoss[0m : 1.79646
[1mStep[0m  [105/213], [94mLoss[0m : 1.75444
[1mStep[0m  [126/213], [94mLoss[0m : 1.46323
[1mStep[0m  [147/213], [94mLoss[0m : 1.82586
[1mStep[0m  [168/213], [94mLoss[0m : 1.72244
[1mStep[0m  [189/213], [94mLoss[0m : 1.95280
[1mStep[0m  [210/213], [94mLoss[0m : 1.50568

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.28437
[1mStep[0m  [21/213], [94mLoss[0m : 1.83942
[1mStep[0m  [42/213], [94mLoss[0m : 1.16566
[1mStep[0m  [63/213], [94mLoss[0m : 1.49948
[1mStep[0m  [84/213], [94mLoss[0m : 1.56856
[1mStep[0m  [105/213], [94mLoss[0m : 1.29956
[1mStep[0m  [126/213], [94mLoss[0m : 1.47111
[1mStep[0m  [147/213], [94mLoss[0m : 1.53899
[1mStep[0m  [168/213], [94mLoss[0m : 1.66101
[1mStep[0m  [189/213], [94mLoss[0m : 1.61555
[1mStep[0m  [210/213], [94mLoss[0m : 1.62183

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.582, [92mTest[0m: 2.469, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.52055
[1mStep[0m  [21/213], [94mLoss[0m : 1.27602
[1mStep[0m  [42/213], [94mLoss[0m : 1.57975
[1mStep[0m  [63/213], [94mLoss[0m : 1.64848
[1mStep[0m  [84/213], [94mLoss[0m : 1.33100
[1mStep[0m  [105/213], [94mLoss[0m : 1.54696
[1mStep[0m  [126/213], [94mLoss[0m : 1.85295
[1mStep[0m  [147/213], [94mLoss[0m : 1.78272
[1mStep[0m  [168/213], [94mLoss[0m : 1.57234
[1mStep[0m  [189/213], [94mLoss[0m : 1.62079
[1mStep[0m  [210/213], [94mLoss[0m : 1.65245

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.470, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.465
====================================

Phase 2 - Evaluation MAE:  2.4652425568058804
MAE score P1      2.388805
MAE score P2      2.465243
loss              1.564422
learning_rate     0.002575
batch_size              64
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.5
weight_decay          0.01
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.42781
[1mStep[0m  [10/106], [94mLoss[0m : 10.66072
[1mStep[0m  [20/106], [94mLoss[0m : 10.60468
[1mStep[0m  [30/106], [94mLoss[0m : 11.22686
[1mStep[0m  [40/106], [94mLoss[0m : 10.17227
[1mStep[0m  [50/106], [94mLoss[0m : 10.32310
[1mStep[0m  [60/106], [94mLoss[0m : 10.58876
[1mStep[0m  [70/106], [94mLoss[0m : 10.15512
[1mStep[0m  [80/106], [94mLoss[0m : 10.05505
[1mStep[0m  [90/106], [94mLoss[0m : 10.33140
[1mStep[0m  [100/106], [94mLoss[0m : 10.38609

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.526, [92mTest[0m: 10.870, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.23733
[1mStep[0m  [10/106], [94mLoss[0m : 10.02783
[1mStep[0m  [20/106], [94mLoss[0m : 9.89772
[1mStep[0m  [30/106], [94mLoss[0m : 10.48852
[1mStep[0m  [40/106], [94mLoss[0m : 9.88868
[1mStep[0m  [50/106], [94mLoss[0m : 9.61976
[1mStep[0m  [60/106], [94mLoss[0m : 9.62319
[1mStep[0m  [70/106], [94mLoss[0m : 9.80687
[1mStep[0m  [80/106], [94mLoss[0m : 9.59185
[1mStep[0m  [90/106], [94mLoss[0m : 9.97031
[1mStep[0m  [100/106], [94mLoss[0m : 9.51994

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.798, [92mTest[0m: 9.948, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.47296
[1mStep[0m  [10/106], [94mLoss[0m : 9.80960
[1mStep[0m  [20/106], [94mLoss[0m : 9.01630
[1mStep[0m  [30/106], [94mLoss[0m : 9.03492
[1mStep[0m  [40/106], [94mLoss[0m : 8.90715
[1mStep[0m  [50/106], [94mLoss[0m : 8.70912
[1mStep[0m  [60/106], [94mLoss[0m : 9.16470
[1mStep[0m  [70/106], [94mLoss[0m : 8.79836
[1mStep[0m  [80/106], [94mLoss[0m : 8.40995
[1mStep[0m  [90/106], [94mLoss[0m : 8.49099
[1mStep[0m  [100/106], [94mLoss[0m : 8.11591

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.903, [92mTest[0m: 8.978, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.21001
[1mStep[0m  [10/106], [94mLoss[0m : 7.51732
[1mStep[0m  [20/106], [94mLoss[0m : 8.15061
[1mStep[0m  [30/106], [94mLoss[0m : 8.15667
[1mStep[0m  [40/106], [94mLoss[0m : 8.02769
[1mStep[0m  [50/106], [94mLoss[0m : 8.11210
[1mStep[0m  [60/106], [94mLoss[0m : 7.27775
[1mStep[0m  [70/106], [94mLoss[0m : 7.96253
[1mStep[0m  [80/106], [94mLoss[0m : 7.75039
[1mStep[0m  [90/106], [94mLoss[0m : 7.29661
[1mStep[0m  [100/106], [94mLoss[0m : 7.22820

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.729, [92mTest[0m: 7.668, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.25362
[1mStep[0m  [10/106], [94mLoss[0m : 7.03093
[1mStep[0m  [20/106], [94mLoss[0m : 6.34172
[1mStep[0m  [30/106], [94mLoss[0m : 6.30801
[1mStep[0m  [40/106], [94mLoss[0m : 6.55432
[1mStep[0m  [50/106], [94mLoss[0m : 6.26582
[1mStep[0m  [60/106], [94mLoss[0m : 6.59425
[1mStep[0m  [70/106], [94mLoss[0m : 6.05819
[1mStep[0m  [80/106], [94mLoss[0m : 6.43566
[1mStep[0m  [90/106], [94mLoss[0m : 5.89312
[1mStep[0m  [100/106], [94mLoss[0m : 6.22609

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.466, [92mTest[0m: 6.256, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.66741
[1mStep[0m  [10/106], [94mLoss[0m : 6.25581
[1mStep[0m  [20/106], [94mLoss[0m : 5.62150
[1mStep[0m  [30/106], [94mLoss[0m : 4.86069
[1mStep[0m  [40/106], [94mLoss[0m : 5.78244
[1mStep[0m  [50/106], [94mLoss[0m : 5.48490
[1mStep[0m  [60/106], [94mLoss[0m : 5.03542
[1mStep[0m  [70/106], [94mLoss[0m : 4.77256
[1mStep[0m  [80/106], [94mLoss[0m : 4.98954
[1mStep[0m  [90/106], [94mLoss[0m : 4.42784
[1mStep[0m  [100/106], [94mLoss[0m : 4.00650

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 5.157, [92mTest[0m: 4.838, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.85066
[1mStep[0m  [10/106], [94mLoss[0m : 4.68474
[1mStep[0m  [20/106], [94mLoss[0m : 3.99325
[1mStep[0m  [30/106], [94mLoss[0m : 4.20273
[1mStep[0m  [40/106], [94mLoss[0m : 3.96754
[1mStep[0m  [50/106], [94mLoss[0m : 3.85591
[1mStep[0m  [60/106], [94mLoss[0m : 3.44285
[1mStep[0m  [70/106], [94mLoss[0m : 3.42104
[1mStep[0m  [80/106], [94mLoss[0m : 3.50956
[1mStep[0m  [90/106], [94mLoss[0m : 3.51468
[1mStep[0m  [100/106], [94mLoss[0m : 2.99479

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.862, [92mTest[0m: 3.780, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.12842
[1mStep[0m  [10/106], [94mLoss[0m : 3.27549
[1mStep[0m  [20/106], [94mLoss[0m : 3.10961
[1mStep[0m  [30/106], [94mLoss[0m : 3.15485
[1mStep[0m  [40/106], [94mLoss[0m : 3.00615
[1mStep[0m  [50/106], [94mLoss[0m : 3.07847
[1mStep[0m  [60/106], [94mLoss[0m : 2.71797
[1mStep[0m  [70/106], [94mLoss[0m : 2.99095
[1mStep[0m  [80/106], [94mLoss[0m : 2.55629
[1mStep[0m  [90/106], [94mLoss[0m : 2.77210
[1mStep[0m  [100/106], [94mLoss[0m : 2.83083

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 3.002, [92mTest[0m: 2.813, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65156
[1mStep[0m  [10/106], [94mLoss[0m : 2.83895
[1mStep[0m  [20/106], [94mLoss[0m : 2.80643
[1mStep[0m  [30/106], [94mLoss[0m : 2.62820
[1mStep[0m  [40/106], [94mLoss[0m : 3.14835
[1mStep[0m  [50/106], [94mLoss[0m : 2.77520
[1mStep[0m  [60/106], [94mLoss[0m : 3.14885
[1mStep[0m  [70/106], [94mLoss[0m : 3.04711
[1mStep[0m  [80/106], [94mLoss[0m : 2.74473
[1mStep[0m  [90/106], [94mLoss[0m : 2.81765
[1mStep[0m  [100/106], [94mLoss[0m : 2.88179

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.774, [92mTest[0m: 2.479, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.85930
[1mStep[0m  [10/106], [94mLoss[0m : 2.52867
[1mStep[0m  [20/106], [94mLoss[0m : 2.66861
[1mStep[0m  [30/106], [94mLoss[0m : 2.61282
[1mStep[0m  [40/106], [94mLoss[0m : 2.28785
[1mStep[0m  [50/106], [94mLoss[0m : 2.69300
[1mStep[0m  [60/106], [94mLoss[0m : 2.47925
[1mStep[0m  [70/106], [94mLoss[0m : 2.78839
[1mStep[0m  [80/106], [94mLoss[0m : 2.65371
[1mStep[0m  [90/106], [94mLoss[0m : 2.45407
[1mStep[0m  [100/106], [94mLoss[0m : 2.96882

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.718, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45864
[1mStep[0m  [10/106], [94mLoss[0m : 2.57993
[1mStep[0m  [20/106], [94mLoss[0m : 2.69577
[1mStep[0m  [30/106], [94mLoss[0m : 2.73327
[1mStep[0m  [40/106], [94mLoss[0m : 2.63798
[1mStep[0m  [50/106], [94mLoss[0m : 2.47847
[1mStep[0m  [60/106], [94mLoss[0m : 2.54101
[1mStep[0m  [70/106], [94mLoss[0m : 3.00666
[1mStep[0m  [80/106], [94mLoss[0m : 2.91016
[1mStep[0m  [90/106], [94mLoss[0m : 2.73272
[1mStep[0m  [100/106], [94mLoss[0m : 2.94631

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.684, [92mTest[0m: 2.466, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68703
[1mStep[0m  [10/106], [94mLoss[0m : 2.56475
[1mStep[0m  [20/106], [94mLoss[0m : 2.81630
[1mStep[0m  [30/106], [94mLoss[0m : 2.78093
[1mStep[0m  [40/106], [94mLoss[0m : 2.88415
[1mStep[0m  [50/106], [94mLoss[0m : 2.73852
[1mStep[0m  [60/106], [94mLoss[0m : 2.42842
[1mStep[0m  [70/106], [94mLoss[0m : 2.69497
[1mStep[0m  [80/106], [94mLoss[0m : 2.85265
[1mStep[0m  [90/106], [94mLoss[0m : 2.77476
[1mStep[0m  [100/106], [94mLoss[0m : 2.67983

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.484, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49613
[1mStep[0m  [10/106], [94mLoss[0m : 2.89518
[1mStep[0m  [20/106], [94mLoss[0m : 2.62204
[1mStep[0m  [30/106], [94mLoss[0m : 2.53240
[1mStep[0m  [40/106], [94mLoss[0m : 2.58422
[1mStep[0m  [50/106], [94mLoss[0m : 2.74942
[1mStep[0m  [60/106], [94mLoss[0m : 2.89554
[1mStep[0m  [70/106], [94mLoss[0m : 2.80524
[1mStep[0m  [80/106], [94mLoss[0m : 2.74864
[1mStep[0m  [90/106], [94mLoss[0m : 2.64054
[1mStep[0m  [100/106], [94mLoss[0m : 2.80611

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.671, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84613
[1mStep[0m  [10/106], [94mLoss[0m : 2.70212
[1mStep[0m  [20/106], [94mLoss[0m : 2.75543
[1mStep[0m  [30/106], [94mLoss[0m : 2.68996
[1mStep[0m  [40/106], [94mLoss[0m : 2.92339
[1mStep[0m  [50/106], [94mLoss[0m : 2.52565
[1mStep[0m  [60/106], [94mLoss[0m : 2.48435
[1mStep[0m  [70/106], [94mLoss[0m : 2.69446
[1mStep[0m  [80/106], [94mLoss[0m : 2.62810
[1mStep[0m  [90/106], [94mLoss[0m : 2.58921
[1mStep[0m  [100/106], [94mLoss[0m : 2.66474

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.646, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32314
[1mStep[0m  [10/106], [94mLoss[0m : 2.90454
[1mStep[0m  [20/106], [94mLoss[0m : 2.83064
[1mStep[0m  [30/106], [94mLoss[0m : 2.67464
[1mStep[0m  [40/106], [94mLoss[0m : 2.72707
[1mStep[0m  [50/106], [94mLoss[0m : 2.53195
[1mStep[0m  [60/106], [94mLoss[0m : 2.61231
[1mStep[0m  [70/106], [94mLoss[0m : 2.62989
[1mStep[0m  [80/106], [94mLoss[0m : 2.31160
[1mStep[0m  [90/106], [94mLoss[0m : 2.69370
[1mStep[0m  [100/106], [94mLoss[0m : 2.33652

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.84619
[1mStep[0m  [10/106], [94mLoss[0m : 2.51321
[1mStep[0m  [20/106], [94mLoss[0m : 2.66234
[1mStep[0m  [30/106], [94mLoss[0m : 2.58721
[1mStep[0m  [40/106], [94mLoss[0m : 3.24680
[1mStep[0m  [50/106], [94mLoss[0m : 2.83066
[1mStep[0m  [60/106], [94mLoss[0m : 2.47033
[1mStep[0m  [70/106], [94mLoss[0m : 2.55627
[1mStep[0m  [80/106], [94mLoss[0m : 2.54460
[1mStep[0m  [90/106], [94mLoss[0m : 2.35321
[1mStep[0m  [100/106], [94mLoss[0m : 2.56897

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67375
[1mStep[0m  [10/106], [94mLoss[0m : 2.30959
[1mStep[0m  [20/106], [94mLoss[0m : 2.56246
[1mStep[0m  [30/106], [94mLoss[0m : 2.84015
[1mStep[0m  [40/106], [94mLoss[0m : 2.48134
[1mStep[0m  [50/106], [94mLoss[0m : 2.88377
[1mStep[0m  [60/106], [94mLoss[0m : 2.64766
[1mStep[0m  [70/106], [94mLoss[0m : 3.24195
[1mStep[0m  [80/106], [94mLoss[0m : 2.29762
[1mStep[0m  [90/106], [94mLoss[0m : 2.45248
[1mStep[0m  [100/106], [94mLoss[0m : 2.94027

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.632, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58489
[1mStep[0m  [10/106], [94mLoss[0m : 2.48144
[1mStep[0m  [20/106], [94mLoss[0m : 2.50939
[1mStep[0m  [30/106], [94mLoss[0m : 2.66301
[1mStep[0m  [40/106], [94mLoss[0m : 2.51725
[1mStep[0m  [50/106], [94mLoss[0m : 2.53001
[1mStep[0m  [60/106], [94mLoss[0m : 2.52759
[1mStep[0m  [70/106], [94mLoss[0m : 2.46044
[1mStep[0m  [80/106], [94mLoss[0m : 2.49486
[1mStep[0m  [90/106], [94mLoss[0m : 2.62374
[1mStep[0m  [100/106], [94mLoss[0m : 2.57768

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70206
[1mStep[0m  [10/106], [94mLoss[0m : 2.67258
[1mStep[0m  [20/106], [94mLoss[0m : 2.42344
[1mStep[0m  [30/106], [94mLoss[0m : 2.68989
[1mStep[0m  [40/106], [94mLoss[0m : 2.53120
[1mStep[0m  [50/106], [94mLoss[0m : 2.80270
[1mStep[0m  [60/106], [94mLoss[0m : 2.40551
[1mStep[0m  [70/106], [94mLoss[0m : 2.47000
[1mStep[0m  [80/106], [94mLoss[0m : 2.31550
[1mStep[0m  [90/106], [94mLoss[0m : 2.80100
[1mStep[0m  [100/106], [94mLoss[0m : 2.66800

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.608, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70476
[1mStep[0m  [10/106], [94mLoss[0m : 2.53449
[1mStep[0m  [20/106], [94mLoss[0m : 2.29528
[1mStep[0m  [30/106], [94mLoss[0m : 2.67646
[1mStep[0m  [40/106], [94mLoss[0m : 2.26865
[1mStep[0m  [50/106], [94mLoss[0m : 2.40816
[1mStep[0m  [60/106], [94mLoss[0m : 2.56757
[1mStep[0m  [70/106], [94mLoss[0m : 2.37204
[1mStep[0m  [80/106], [94mLoss[0m : 2.65393
[1mStep[0m  [90/106], [94mLoss[0m : 2.97237
[1mStep[0m  [100/106], [94mLoss[0m : 2.49367

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58774
[1mStep[0m  [10/106], [94mLoss[0m : 2.36552
[1mStep[0m  [20/106], [94mLoss[0m : 2.80442
[1mStep[0m  [30/106], [94mLoss[0m : 2.47028
[1mStep[0m  [40/106], [94mLoss[0m : 2.59281
[1mStep[0m  [50/106], [94mLoss[0m : 2.82225
[1mStep[0m  [60/106], [94mLoss[0m : 2.38481
[1mStep[0m  [70/106], [94mLoss[0m : 2.57423
[1mStep[0m  [80/106], [94mLoss[0m : 2.54593
[1mStep[0m  [90/106], [94mLoss[0m : 2.36437
[1mStep[0m  [100/106], [94mLoss[0m : 2.65117

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68516
[1mStep[0m  [10/106], [94mLoss[0m : 2.51769
[1mStep[0m  [20/106], [94mLoss[0m : 2.89764
[1mStep[0m  [30/106], [94mLoss[0m : 2.41445
[1mStep[0m  [40/106], [94mLoss[0m : 2.48137
[1mStep[0m  [50/106], [94mLoss[0m : 2.66342
[1mStep[0m  [60/106], [94mLoss[0m : 2.55742
[1mStep[0m  [70/106], [94mLoss[0m : 3.10183
[1mStep[0m  [80/106], [94mLoss[0m : 2.68273
[1mStep[0m  [90/106], [94mLoss[0m : 2.56832
[1mStep[0m  [100/106], [94mLoss[0m : 2.73417

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35719
[1mStep[0m  [10/106], [94mLoss[0m : 2.62847
[1mStep[0m  [20/106], [94mLoss[0m : 2.31351
[1mStep[0m  [30/106], [94mLoss[0m : 2.42210
[1mStep[0m  [40/106], [94mLoss[0m : 2.65058
[1mStep[0m  [50/106], [94mLoss[0m : 2.65825
[1mStep[0m  [60/106], [94mLoss[0m : 2.57533
[1mStep[0m  [70/106], [94mLoss[0m : 2.23597
[1mStep[0m  [80/106], [94mLoss[0m : 2.74410
[1mStep[0m  [90/106], [94mLoss[0m : 2.55282
[1mStep[0m  [100/106], [94mLoss[0m : 2.48235

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.599, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35463
[1mStep[0m  [10/106], [94mLoss[0m : 2.84097
[1mStep[0m  [20/106], [94mLoss[0m : 2.60682
[1mStep[0m  [30/106], [94mLoss[0m : 2.64539
[1mStep[0m  [40/106], [94mLoss[0m : 2.58380
[1mStep[0m  [50/106], [94mLoss[0m : 2.50126
[1mStep[0m  [60/106], [94mLoss[0m : 2.64910
[1mStep[0m  [70/106], [94mLoss[0m : 2.28057
[1mStep[0m  [80/106], [94mLoss[0m : 2.77210
[1mStep[0m  [90/106], [94mLoss[0m : 2.66361
[1mStep[0m  [100/106], [94mLoss[0m : 2.75437

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.02579
[1mStep[0m  [10/106], [94mLoss[0m : 2.73497
[1mStep[0m  [20/106], [94mLoss[0m : 2.55000
[1mStep[0m  [30/106], [94mLoss[0m : 2.70980
[1mStep[0m  [40/106], [94mLoss[0m : 2.61994
[1mStep[0m  [50/106], [94mLoss[0m : 2.37552
[1mStep[0m  [60/106], [94mLoss[0m : 2.29481
[1mStep[0m  [70/106], [94mLoss[0m : 2.65358
[1mStep[0m  [80/106], [94mLoss[0m : 2.37460
[1mStep[0m  [90/106], [94mLoss[0m : 2.42565
[1mStep[0m  [100/106], [94mLoss[0m : 2.47091

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34502
[1mStep[0m  [10/106], [94mLoss[0m : 2.67863
[1mStep[0m  [20/106], [94mLoss[0m : 2.72319
[1mStep[0m  [30/106], [94mLoss[0m : 2.85054
[1mStep[0m  [40/106], [94mLoss[0m : 2.65704
[1mStep[0m  [50/106], [94mLoss[0m : 2.30436
[1mStep[0m  [60/106], [94mLoss[0m : 2.67906
[1mStep[0m  [70/106], [94mLoss[0m : 2.88657
[1mStep[0m  [80/106], [94mLoss[0m : 2.77244
[1mStep[0m  [90/106], [94mLoss[0m : 2.54533
[1mStep[0m  [100/106], [94mLoss[0m : 2.51315

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.418, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48626
[1mStep[0m  [10/106], [94mLoss[0m : 2.75642
[1mStep[0m  [20/106], [94mLoss[0m : 2.55013
[1mStep[0m  [30/106], [94mLoss[0m : 2.56781
[1mStep[0m  [40/106], [94mLoss[0m : 2.19078
[1mStep[0m  [50/106], [94mLoss[0m : 2.37845
[1mStep[0m  [60/106], [94mLoss[0m : 2.71136
[1mStep[0m  [70/106], [94mLoss[0m : 2.57225
[1mStep[0m  [80/106], [94mLoss[0m : 2.41904
[1mStep[0m  [90/106], [94mLoss[0m : 2.55205
[1mStep[0m  [100/106], [94mLoss[0m : 2.24988

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33857
[1mStep[0m  [10/106], [94mLoss[0m : 2.57489
[1mStep[0m  [20/106], [94mLoss[0m : 2.56819
[1mStep[0m  [30/106], [94mLoss[0m : 2.56190
[1mStep[0m  [40/106], [94mLoss[0m : 2.41122
[1mStep[0m  [50/106], [94mLoss[0m : 2.35019
[1mStep[0m  [60/106], [94mLoss[0m : 2.64201
[1mStep[0m  [70/106], [94mLoss[0m : 2.86804
[1mStep[0m  [80/106], [94mLoss[0m : 2.63861
[1mStep[0m  [90/106], [94mLoss[0m : 2.39672
[1mStep[0m  [100/106], [94mLoss[0m : 2.59187

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.410, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24521
[1mStep[0m  [10/106], [94mLoss[0m : 2.34183
[1mStep[0m  [20/106], [94mLoss[0m : 2.59575
[1mStep[0m  [30/106], [94mLoss[0m : 2.34577
[1mStep[0m  [40/106], [94mLoss[0m : 2.75155
[1mStep[0m  [50/106], [94mLoss[0m : 2.73504
[1mStep[0m  [60/106], [94mLoss[0m : 2.65966
[1mStep[0m  [70/106], [94mLoss[0m : 2.44909
[1mStep[0m  [80/106], [94mLoss[0m : 2.60994
[1mStep[0m  [90/106], [94mLoss[0m : 2.58192
[1mStep[0m  [100/106], [94mLoss[0m : 2.52077

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.412, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94118
[1mStep[0m  [10/106], [94mLoss[0m : 2.43862
[1mStep[0m  [20/106], [94mLoss[0m : 2.35323
[1mStep[0m  [30/106], [94mLoss[0m : 2.62510
[1mStep[0m  [40/106], [94mLoss[0m : 2.56055
[1mStep[0m  [50/106], [94mLoss[0m : 2.42477
[1mStep[0m  [60/106], [94mLoss[0m : 2.50509
[1mStep[0m  [70/106], [94mLoss[0m : 2.50339
[1mStep[0m  [80/106], [94mLoss[0m : 2.69985
[1mStep[0m  [90/106], [94mLoss[0m : 2.82967
[1mStep[0m  [100/106], [94mLoss[0m : 2.89755

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.397
====================================

Phase 1 - Evaluation MAE:  2.3969263490640893
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.60822
[1mStep[0m  [10/106], [94mLoss[0m : 2.73543
[1mStep[0m  [20/106], [94mLoss[0m : 2.61191
[1mStep[0m  [30/106], [94mLoss[0m : 2.59019
[1mStep[0m  [40/106], [94mLoss[0m : 2.91550
[1mStep[0m  [50/106], [94mLoss[0m : 2.35142
[1mStep[0m  [60/106], [94mLoss[0m : 2.71683
[1mStep[0m  [70/106], [94mLoss[0m : 2.64804
[1mStep[0m  [80/106], [94mLoss[0m : 2.57231
[1mStep[0m  [90/106], [94mLoss[0m : 2.55591
[1mStep[0m  [100/106], [94mLoss[0m : 2.23587

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36855
[1mStep[0m  [10/106], [94mLoss[0m : 2.43402
[1mStep[0m  [20/106], [94mLoss[0m : 2.19825
[1mStep[0m  [30/106], [94mLoss[0m : 2.73025
[1mStep[0m  [40/106], [94mLoss[0m : 2.73664
[1mStep[0m  [50/106], [94mLoss[0m : 2.53858
[1mStep[0m  [60/106], [94mLoss[0m : 2.45707
[1mStep[0m  [70/106], [94mLoss[0m : 2.41908
[1mStep[0m  [80/106], [94mLoss[0m : 2.35542
[1mStep[0m  [90/106], [94mLoss[0m : 2.77054
[1mStep[0m  [100/106], [94mLoss[0m : 2.48359

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.580, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70781
[1mStep[0m  [10/106], [94mLoss[0m : 2.34809
[1mStep[0m  [20/106], [94mLoss[0m : 2.39453
[1mStep[0m  [30/106], [94mLoss[0m : 2.30181
[1mStep[0m  [40/106], [94mLoss[0m : 2.59830
[1mStep[0m  [50/106], [94mLoss[0m : 2.38874
[1mStep[0m  [60/106], [94mLoss[0m : 2.18333
[1mStep[0m  [70/106], [94mLoss[0m : 2.71487
[1mStep[0m  [80/106], [94mLoss[0m : 2.84873
[1mStep[0m  [90/106], [94mLoss[0m : 2.83268
[1mStep[0m  [100/106], [94mLoss[0m : 2.46308

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62405
[1mStep[0m  [10/106], [94mLoss[0m : 2.31505
[1mStep[0m  [20/106], [94mLoss[0m : 2.14724
[1mStep[0m  [30/106], [94mLoss[0m : 2.28777
[1mStep[0m  [40/106], [94mLoss[0m : 2.20535
[1mStep[0m  [50/106], [94mLoss[0m : 2.70548
[1mStep[0m  [60/106], [94mLoss[0m : 2.60246
[1mStep[0m  [70/106], [94mLoss[0m : 2.59953
[1mStep[0m  [80/106], [94mLoss[0m : 2.61576
[1mStep[0m  [90/106], [94mLoss[0m : 2.61019
[1mStep[0m  [100/106], [94mLoss[0m : 2.46588

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.491, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27717
[1mStep[0m  [10/106], [94mLoss[0m : 2.81964
[1mStep[0m  [20/106], [94mLoss[0m : 2.22199
[1mStep[0m  [30/106], [94mLoss[0m : 2.13117
[1mStep[0m  [40/106], [94mLoss[0m : 2.42441
[1mStep[0m  [50/106], [94mLoss[0m : 2.31806
[1mStep[0m  [60/106], [94mLoss[0m : 2.45033
[1mStep[0m  [70/106], [94mLoss[0m : 2.27984
[1mStep[0m  [80/106], [94mLoss[0m : 2.66460
[1mStep[0m  [90/106], [94mLoss[0m : 2.62970
[1mStep[0m  [100/106], [94mLoss[0m : 2.28145

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07085
[1mStep[0m  [10/106], [94mLoss[0m : 2.35758
[1mStep[0m  [20/106], [94mLoss[0m : 2.32692
[1mStep[0m  [30/106], [94mLoss[0m : 2.62563
[1mStep[0m  [40/106], [94mLoss[0m : 2.35196
[1mStep[0m  [50/106], [94mLoss[0m : 2.50578
[1mStep[0m  [60/106], [94mLoss[0m : 2.37748
[1mStep[0m  [70/106], [94mLoss[0m : 2.89138
[1mStep[0m  [80/106], [94mLoss[0m : 1.86848
[1mStep[0m  [90/106], [94mLoss[0m : 2.84044
[1mStep[0m  [100/106], [94mLoss[0m : 2.39507

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20081
[1mStep[0m  [10/106], [94mLoss[0m : 2.21837
[1mStep[0m  [20/106], [94mLoss[0m : 2.45271
[1mStep[0m  [30/106], [94mLoss[0m : 2.35076
[1mStep[0m  [40/106], [94mLoss[0m : 2.60835
[1mStep[0m  [50/106], [94mLoss[0m : 2.51964
[1mStep[0m  [60/106], [94mLoss[0m : 2.47835
[1mStep[0m  [70/106], [94mLoss[0m : 2.33748
[1mStep[0m  [80/106], [94mLoss[0m : 2.56718
[1mStep[0m  [90/106], [94mLoss[0m : 2.36653
[1mStep[0m  [100/106], [94mLoss[0m : 2.43137

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38213
[1mStep[0m  [10/106], [94mLoss[0m : 2.56212
[1mStep[0m  [20/106], [94mLoss[0m : 2.35135
[1mStep[0m  [30/106], [94mLoss[0m : 2.51262
[1mStep[0m  [40/106], [94mLoss[0m : 2.33577
[1mStep[0m  [50/106], [94mLoss[0m : 2.23385
[1mStep[0m  [60/106], [94mLoss[0m : 2.40402
[1mStep[0m  [70/106], [94mLoss[0m : 2.31720
[1mStep[0m  [80/106], [94mLoss[0m : 2.09390
[1mStep[0m  [90/106], [94mLoss[0m : 2.33473
[1mStep[0m  [100/106], [94mLoss[0m : 2.12480

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.386, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42843
[1mStep[0m  [10/106], [94mLoss[0m : 2.57789
[1mStep[0m  [20/106], [94mLoss[0m : 2.10983
[1mStep[0m  [30/106], [94mLoss[0m : 2.09571
[1mStep[0m  [40/106], [94mLoss[0m : 2.36987
[1mStep[0m  [50/106], [94mLoss[0m : 2.28197
[1mStep[0m  [60/106], [94mLoss[0m : 2.23826
[1mStep[0m  [70/106], [94mLoss[0m : 2.08141
[1mStep[0m  [80/106], [94mLoss[0m : 2.15900
[1mStep[0m  [90/106], [94mLoss[0m : 2.24828
[1mStep[0m  [100/106], [94mLoss[0m : 2.17425

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19019
[1mStep[0m  [10/106], [94mLoss[0m : 2.23716
[1mStep[0m  [20/106], [94mLoss[0m : 2.11160
[1mStep[0m  [30/106], [94mLoss[0m : 2.05645
[1mStep[0m  [40/106], [94mLoss[0m : 2.31507
[1mStep[0m  [50/106], [94mLoss[0m : 2.39656
[1mStep[0m  [60/106], [94mLoss[0m : 2.14234
[1mStep[0m  [70/106], [94mLoss[0m : 2.12584
[1mStep[0m  [80/106], [94mLoss[0m : 2.34120
[1mStep[0m  [90/106], [94mLoss[0m : 1.97347
[1mStep[0m  [100/106], [94mLoss[0m : 2.29065

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.297, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34056
[1mStep[0m  [10/106], [94mLoss[0m : 2.15472
[1mStep[0m  [20/106], [94mLoss[0m : 2.11166
[1mStep[0m  [30/106], [94mLoss[0m : 2.35358
[1mStep[0m  [40/106], [94mLoss[0m : 2.12976
[1mStep[0m  [50/106], [94mLoss[0m : 2.08624
[1mStep[0m  [60/106], [94mLoss[0m : 2.75412
[1mStep[0m  [70/106], [94mLoss[0m : 2.24403
[1mStep[0m  [80/106], [94mLoss[0m : 2.04304
[1mStep[0m  [90/106], [94mLoss[0m : 2.38285
[1mStep[0m  [100/106], [94mLoss[0m : 2.15275

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.286, [92mTest[0m: 2.589, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37558
[1mStep[0m  [10/106], [94mLoss[0m : 2.49654
[1mStep[0m  [20/106], [94mLoss[0m : 2.11829
[1mStep[0m  [30/106], [94mLoss[0m : 2.10425
[1mStep[0m  [40/106], [94mLoss[0m : 2.13893
[1mStep[0m  [50/106], [94mLoss[0m : 2.15378
[1mStep[0m  [60/106], [94mLoss[0m : 2.08331
[1mStep[0m  [70/106], [94mLoss[0m : 2.16159
[1mStep[0m  [80/106], [94mLoss[0m : 2.38880
[1mStep[0m  [90/106], [94mLoss[0m : 2.24158
[1mStep[0m  [100/106], [94mLoss[0m : 2.26545

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.238, [92mTest[0m: 2.396, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17106
[1mStep[0m  [10/106], [94mLoss[0m : 2.14563
[1mStep[0m  [20/106], [94mLoss[0m : 2.13490
[1mStep[0m  [30/106], [94mLoss[0m : 2.08682
[1mStep[0m  [40/106], [94mLoss[0m : 2.04235
[1mStep[0m  [50/106], [94mLoss[0m : 2.35904
[1mStep[0m  [60/106], [94mLoss[0m : 2.09386
[1mStep[0m  [70/106], [94mLoss[0m : 2.27199
[1mStep[0m  [80/106], [94mLoss[0m : 1.97793
[1mStep[0m  [90/106], [94mLoss[0m : 1.99731
[1mStep[0m  [100/106], [94mLoss[0m : 2.10086

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.190, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.08454
[1mStep[0m  [10/106], [94mLoss[0m : 2.19966
[1mStep[0m  [20/106], [94mLoss[0m : 1.95747
[1mStep[0m  [30/106], [94mLoss[0m : 2.04654
[1mStep[0m  [40/106], [94mLoss[0m : 2.47567
[1mStep[0m  [50/106], [94mLoss[0m : 2.17533
[1mStep[0m  [60/106], [94mLoss[0m : 1.96756
[1mStep[0m  [70/106], [94mLoss[0m : 2.11205
[1mStep[0m  [80/106], [94mLoss[0m : 1.75651
[1mStep[0m  [90/106], [94mLoss[0m : 2.52079
[1mStep[0m  [100/106], [94mLoss[0m : 2.31728

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.167, [92mTest[0m: 2.464, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07967
[1mStep[0m  [10/106], [94mLoss[0m : 2.14373
[1mStep[0m  [20/106], [94mLoss[0m : 1.88023
[1mStep[0m  [30/106], [94mLoss[0m : 2.04948
[1mStep[0m  [40/106], [94mLoss[0m : 2.17679
[1mStep[0m  [50/106], [94mLoss[0m : 2.37883
[1mStep[0m  [60/106], [94mLoss[0m : 1.88005
[1mStep[0m  [70/106], [94mLoss[0m : 1.93183
[1mStep[0m  [80/106], [94mLoss[0m : 2.08314
[1mStep[0m  [90/106], [94mLoss[0m : 1.96952
[1mStep[0m  [100/106], [94mLoss[0m : 2.05224

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.126, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15445
[1mStep[0m  [10/106], [94mLoss[0m : 2.09935
[1mStep[0m  [20/106], [94mLoss[0m : 2.03963
[1mStep[0m  [30/106], [94mLoss[0m : 2.16952
[1mStep[0m  [40/106], [94mLoss[0m : 1.89502
[1mStep[0m  [50/106], [94mLoss[0m : 2.13969
[1mStep[0m  [60/106], [94mLoss[0m : 2.20431
[1mStep[0m  [70/106], [94mLoss[0m : 1.88852
[1mStep[0m  [80/106], [94mLoss[0m : 2.19171
[1mStep[0m  [90/106], [94mLoss[0m : 2.21227
[1mStep[0m  [100/106], [94mLoss[0m : 2.06784

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.090, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.10009
[1mStep[0m  [10/106], [94mLoss[0m : 2.09351
[1mStep[0m  [20/106], [94mLoss[0m : 1.91099
[1mStep[0m  [30/106], [94mLoss[0m : 2.07458
[1mStep[0m  [40/106], [94mLoss[0m : 1.99281
[1mStep[0m  [50/106], [94mLoss[0m : 2.01554
[1mStep[0m  [60/106], [94mLoss[0m : 1.87926
[1mStep[0m  [70/106], [94mLoss[0m : 2.00630
[1mStep[0m  [80/106], [94mLoss[0m : 1.73550
[1mStep[0m  [90/106], [94mLoss[0m : 1.91057
[1mStep[0m  [100/106], [94mLoss[0m : 1.94232

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.01512
[1mStep[0m  [10/106], [94mLoss[0m : 2.04306
[1mStep[0m  [20/106], [94mLoss[0m : 1.99558
[1mStep[0m  [30/106], [94mLoss[0m : 1.85196
[1mStep[0m  [40/106], [94mLoss[0m : 2.21797
[1mStep[0m  [50/106], [94mLoss[0m : 1.99584
[1mStep[0m  [60/106], [94mLoss[0m : 2.12407
[1mStep[0m  [70/106], [94mLoss[0m : 1.92060
[1mStep[0m  [80/106], [94mLoss[0m : 2.24588
[1mStep[0m  [90/106], [94mLoss[0m : 2.15183
[1mStep[0m  [100/106], [94mLoss[0m : 2.10508

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.99267
[1mStep[0m  [10/106], [94mLoss[0m : 2.05950
[1mStep[0m  [20/106], [94mLoss[0m : 2.03743
[1mStep[0m  [30/106], [94mLoss[0m : 1.80406
[1mStep[0m  [40/106], [94mLoss[0m : 1.74751
[1mStep[0m  [50/106], [94mLoss[0m : 2.07541
[1mStep[0m  [60/106], [94mLoss[0m : 2.16308
[1mStep[0m  [70/106], [94mLoss[0m : 1.85122
[1mStep[0m  [80/106], [94mLoss[0m : 2.02714
[1mStep[0m  [90/106], [94mLoss[0m : 2.24438
[1mStep[0m  [100/106], [94mLoss[0m : 1.86811

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.994, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.72122
[1mStep[0m  [10/106], [94mLoss[0m : 2.22156
[1mStep[0m  [20/106], [94mLoss[0m : 1.92434
[1mStep[0m  [30/106], [94mLoss[0m : 2.19764
[1mStep[0m  [40/106], [94mLoss[0m : 2.09467
[1mStep[0m  [50/106], [94mLoss[0m : 2.17658
[1mStep[0m  [60/106], [94mLoss[0m : 2.52791
[1mStep[0m  [70/106], [94mLoss[0m : 1.91142
[1mStep[0m  [80/106], [94mLoss[0m : 2.15961
[1mStep[0m  [90/106], [94mLoss[0m : 1.97133
[1mStep[0m  [100/106], [94mLoss[0m : 2.08691

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.988, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.84628
[1mStep[0m  [10/106], [94mLoss[0m : 1.96268
[1mStep[0m  [20/106], [94mLoss[0m : 2.07136
[1mStep[0m  [30/106], [94mLoss[0m : 1.79568
[1mStep[0m  [40/106], [94mLoss[0m : 1.77808
[1mStep[0m  [50/106], [94mLoss[0m : 1.73806
[1mStep[0m  [60/106], [94mLoss[0m : 1.94184
[1mStep[0m  [70/106], [94mLoss[0m : 2.23245
[1mStep[0m  [80/106], [94mLoss[0m : 2.14609
[1mStep[0m  [90/106], [94mLoss[0m : 1.82140
[1mStep[0m  [100/106], [94mLoss[0m : 2.01359

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.917, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92224
[1mStep[0m  [10/106], [94mLoss[0m : 1.79271
[1mStep[0m  [20/106], [94mLoss[0m : 1.85952
[1mStep[0m  [30/106], [94mLoss[0m : 1.81764
[1mStep[0m  [40/106], [94mLoss[0m : 1.90261
[1mStep[0m  [50/106], [94mLoss[0m : 2.13082
[1mStep[0m  [60/106], [94mLoss[0m : 1.98966
[1mStep[0m  [70/106], [94mLoss[0m : 1.79832
[1mStep[0m  [80/106], [94mLoss[0m : 2.03704
[1mStep[0m  [90/106], [94mLoss[0m : 1.71095
[1mStep[0m  [100/106], [94mLoss[0m : 1.96697

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.894, [92mTest[0m: 2.431, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.93631
[1mStep[0m  [10/106], [94mLoss[0m : 2.07130
[1mStep[0m  [20/106], [94mLoss[0m : 1.95985
[1mStep[0m  [30/106], [94mLoss[0m : 1.77766
[1mStep[0m  [40/106], [94mLoss[0m : 1.79218
[1mStep[0m  [50/106], [94mLoss[0m : 1.82850
[1mStep[0m  [60/106], [94mLoss[0m : 2.16682
[1mStep[0m  [70/106], [94mLoss[0m : 1.90512
[1mStep[0m  [80/106], [94mLoss[0m : 1.97217
[1mStep[0m  [90/106], [94mLoss[0m : 1.92562
[1mStep[0m  [100/106], [94mLoss[0m : 1.89495

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.452, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.03193
[1mStep[0m  [10/106], [94mLoss[0m : 1.77349
[1mStep[0m  [20/106], [94mLoss[0m : 1.98162
[1mStep[0m  [30/106], [94mLoss[0m : 1.59845
[1mStep[0m  [40/106], [94mLoss[0m : 1.88545
[1mStep[0m  [50/106], [94mLoss[0m : 1.86869
[1mStep[0m  [60/106], [94mLoss[0m : 2.03597
[1mStep[0m  [70/106], [94mLoss[0m : 1.92733
[1mStep[0m  [80/106], [94mLoss[0m : 1.94719
[1mStep[0m  [90/106], [94mLoss[0m : 2.11023
[1mStep[0m  [100/106], [94mLoss[0m : 1.97172

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.66108
[1mStep[0m  [10/106], [94mLoss[0m : 1.72360
[1mStep[0m  [20/106], [94mLoss[0m : 1.69856
[1mStep[0m  [30/106], [94mLoss[0m : 1.62106
[1mStep[0m  [40/106], [94mLoss[0m : 2.02382
[1mStep[0m  [50/106], [94mLoss[0m : 1.81958
[1mStep[0m  [60/106], [94mLoss[0m : 1.57619
[1mStep[0m  [70/106], [94mLoss[0m : 1.82875
[1mStep[0m  [80/106], [94mLoss[0m : 2.07742
[1mStep[0m  [90/106], [94mLoss[0m : 1.98706
[1mStep[0m  [100/106], [94mLoss[0m : 1.85662

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.823, [92mTest[0m: 2.425, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.75597
[1mStep[0m  [10/106], [94mLoss[0m : 1.99198
[1mStep[0m  [20/106], [94mLoss[0m : 1.68337
[1mStep[0m  [30/106], [94mLoss[0m : 1.96803
[1mStep[0m  [40/106], [94mLoss[0m : 1.92056
[1mStep[0m  [50/106], [94mLoss[0m : 1.80273
[1mStep[0m  [60/106], [94mLoss[0m : 1.73413
[1mStep[0m  [70/106], [94mLoss[0m : 1.98264
[1mStep[0m  [80/106], [94mLoss[0m : 1.81037
[1mStep[0m  [90/106], [94mLoss[0m : 1.98559
[1mStep[0m  [100/106], [94mLoss[0m : 2.06674

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.11814
[1mStep[0m  [10/106], [94mLoss[0m : 1.78203
[1mStep[0m  [20/106], [94mLoss[0m : 1.85915
[1mStep[0m  [30/106], [94mLoss[0m : 1.88689
[1mStep[0m  [40/106], [94mLoss[0m : 1.69126
[1mStep[0m  [50/106], [94mLoss[0m : 1.68192
[1mStep[0m  [60/106], [94mLoss[0m : 1.78855
[1mStep[0m  [70/106], [94mLoss[0m : 1.88078
[1mStep[0m  [80/106], [94mLoss[0m : 1.76364
[1mStep[0m  [90/106], [94mLoss[0m : 1.78784
[1mStep[0m  [100/106], [94mLoss[0m : 1.99134

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.517, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76959
[1mStep[0m  [10/106], [94mLoss[0m : 1.68371
[1mStep[0m  [20/106], [94mLoss[0m : 1.95733
[1mStep[0m  [30/106], [94mLoss[0m : 1.75851
[1mStep[0m  [40/106], [94mLoss[0m : 1.84798
[1mStep[0m  [50/106], [94mLoss[0m : 1.81087
[1mStep[0m  [60/106], [94mLoss[0m : 1.65803
[1mStep[0m  [70/106], [94mLoss[0m : 1.58853
[1mStep[0m  [80/106], [94mLoss[0m : 1.73778
[1mStep[0m  [90/106], [94mLoss[0m : 1.65506
[1mStep[0m  [100/106], [94mLoss[0m : 1.74451

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.744, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.68327
[1mStep[0m  [10/106], [94mLoss[0m : 1.65793
[1mStep[0m  [20/106], [94mLoss[0m : 1.74112
[1mStep[0m  [30/106], [94mLoss[0m : 1.67428
[1mStep[0m  [40/106], [94mLoss[0m : 1.64309
[1mStep[0m  [50/106], [94mLoss[0m : 1.72612
[1mStep[0m  [60/106], [94mLoss[0m : 1.63826
[1mStep[0m  [70/106], [94mLoss[0m : 1.88120
[1mStep[0m  [80/106], [94mLoss[0m : 2.04201
[1mStep[0m  [90/106], [94mLoss[0m : 1.76330
[1mStep[0m  [100/106], [94mLoss[0m : 1.66553

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.45223
[1mStep[0m  [10/106], [94mLoss[0m : 1.61454
[1mStep[0m  [20/106], [94mLoss[0m : 1.88616
[1mStep[0m  [30/106], [94mLoss[0m : 1.86637
[1mStep[0m  [40/106], [94mLoss[0m : 1.85641
[1mStep[0m  [50/106], [94mLoss[0m : 1.77208
[1mStep[0m  [60/106], [94mLoss[0m : 1.55070
[1mStep[0m  [70/106], [94mLoss[0m : 1.83130
[1mStep[0m  [80/106], [94mLoss[0m : 2.01133
[1mStep[0m  [90/106], [94mLoss[0m : 1.81553
[1mStep[0m  [100/106], [94mLoss[0m : 1.91196

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.486
====================================

Phase 2 - Evaluation MAE:  2.4857079712849743
MAE score P1        2.396926
MAE score P2        2.485708
loss                 1.71422
learning_rate       0.002575
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.5
weight_decay          0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 11.12369
[1mStep[0m  [21/213], [94mLoss[0m : 11.06281
[1mStep[0m  [42/213], [94mLoss[0m : 10.83799
[1mStep[0m  [63/213], [94mLoss[0m : 11.22776
[1mStep[0m  [84/213], [94mLoss[0m : 10.98509
[1mStep[0m  [105/213], [94mLoss[0m : 10.51661
[1mStep[0m  [126/213], [94mLoss[0m : 10.38345
[1mStep[0m  [147/213], [94mLoss[0m : 10.06232
[1mStep[0m  [168/213], [94mLoss[0m : 10.26020
[1mStep[0m  [189/213], [94mLoss[0m : 9.77725
[1mStep[0m  [210/213], [94mLoss[0m : 8.89371

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.327, [92mTest[0m: 10.971, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.29248
[1mStep[0m  [21/213], [94mLoss[0m : 9.61936
[1mStep[0m  [42/213], [94mLoss[0m : 9.67847
[1mStep[0m  [63/213], [94mLoss[0m : 9.01237
[1mStep[0m  [84/213], [94mLoss[0m : 8.91856
[1mStep[0m  [105/213], [94mLoss[0m : 8.83639
[1mStep[0m  [126/213], [94mLoss[0m : 8.79750
[1mStep[0m  [147/213], [94mLoss[0m : 8.68654
[1mStep[0m  [168/213], [94mLoss[0m : 8.95808
[1mStep[0m  [189/213], [94mLoss[0m : 7.13569
[1mStep[0m  [210/213], [94mLoss[0m : 7.75389

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 8.599, [92mTest[0m: 9.281, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 6.91793
[1mStep[0m  [21/213], [94mLoss[0m : 8.08636
[1mStep[0m  [42/213], [94mLoss[0m : 6.19878
[1mStep[0m  [63/213], [94mLoss[0m : 6.26096
[1mStep[0m  [84/213], [94mLoss[0m : 5.72439
[1mStep[0m  [105/213], [94mLoss[0m : 6.90090
[1mStep[0m  [126/213], [94mLoss[0m : 5.54517
[1mStep[0m  [147/213], [94mLoss[0m : 5.07416
[1mStep[0m  [168/213], [94mLoss[0m : 5.20660
[1mStep[0m  [189/213], [94mLoss[0m : 4.31162
[1mStep[0m  [210/213], [94mLoss[0m : 5.07505

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 6.050, [92mTest[0m: 6.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.39371
[1mStep[0m  [21/213], [94mLoss[0m : 4.35777
[1mStep[0m  [42/213], [94mLoss[0m : 3.63283
[1mStep[0m  [63/213], [94mLoss[0m : 4.82517
[1mStep[0m  [84/213], [94mLoss[0m : 4.09726
[1mStep[0m  [105/213], [94mLoss[0m : 3.54367
[1mStep[0m  [126/213], [94mLoss[0m : 3.68651
[1mStep[0m  [147/213], [94mLoss[0m : 3.93918
[1mStep[0m  [168/213], [94mLoss[0m : 3.38484
[1mStep[0m  [189/213], [94mLoss[0m : 2.85302
[1mStep[0m  [210/213], [94mLoss[0m : 2.31263

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.784, [92mTest[0m: 4.305, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63070
[1mStep[0m  [21/213], [94mLoss[0m : 2.46210
[1mStep[0m  [42/213], [94mLoss[0m : 2.90815
[1mStep[0m  [63/213], [94mLoss[0m : 3.10397
[1mStep[0m  [84/213], [94mLoss[0m : 3.01706
[1mStep[0m  [105/213], [94mLoss[0m : 2.34109
[1mStep[0m  [126/213], [94mLoss[0m : 2.94914
[1mStep[0m  [147/213], [94mLoss[0m : 2.68765
[1mStep[0m  [168/213], [94mLoss[0m : 2.86816
[1mStep[0m  [189/213], [94mLoss[0m : 2.32313
[1mStep[0m  [210/213], [94mLoss[0m : 2.80867

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.811, [92mTest[0m: 2.629, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.84347
[1mStep[0m  [21/213], [94mLoss[0m : 2.43092
[1mStep[0m  [42/213], [94mLoss[0m : 2.98381
[1mStep[0m  [63/213], [94mLoss[0m : 2.85309
[1mStep[0m  [84/213], [94mLoss[0m : 2.68273
[1mStep[0m  [105/213], [94mLoss[0m : 3.13206
[1mStep[0m  [126/213], [94mLoss[0m : 2.70088
[1mStep[0m  [147/213], [94mLoss[0m : 2.36861
[1mStep[0m  [168/213], [94mLoss[0m : 2.36752
[1mStep[0m  [189/213], [94mLoss[0m : 2.62371
[1mStep[0m  [210/213], [94mLoss[0m : 2.66081

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.737, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.82813
[1mStep[0m  [21/213], [94mLoss[0m : 2.47435
[1mStep[0m  [42/213], [94mLoss[0m : 2.53875
[1mStep[0m  [63/213], [94mLoss[0m : 2.66863
[1mStep[0m  [84/213], [94mLoss[0m : 2.77354
[1mStep[0m  [105/213], [94mLoss[0m : 2.60109
[1mStep[0m  [126/213], [94mLoss[0m : 2.25518
[1mStep[0m  [147/213], [94mLoss[0m : 3.22056
[1mStep[0m  [168/213], [94mLoss[0m : 2.46079
[1mStep[0m  [189/213], [94mLoss[0m : 3.00795
[1mStep[0m  [210/213], [94mLoss[0m : 2.62561

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.34048
[1mStep[0m  [21/213], [94mLoss[0m : 2.59193
[1mStep[0m  [42/213], [94mLoss[0m : 2.67283
[1mStep[0m  [63/213], [94mLoss[0m : 2.21429
[1mStep[0m  [84/213], [94mLoss[0m : 2.69981
[1mStep[0m  [105/213], [94mLoss[0m : 2.52681
[1mStep[0m  [126/213], [94mLoss[0m : 2.77933
[1mStep[0m  [147/213], [94mLoss[0m : 2.28018
[1mStep[0m  [168/213], [94mLoss[0m : 2.75396
[1mStep[0m  [189/213], [94mLoss[0m : 2.43994
[1mStep[0m  [210/213], [94mLoss[0m : 2.71143

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.678, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59582
[1mStep[0m  [21/213], [94mLoss[0m : 2.89953
[1mStep[0m  [42/213], [94mLoss[0m : 2.72311
[1mStep[0m  [63/213], [94mLoss[0m : 2.57971
[1mStep[0m  [84/213], [94mLoss[0m : 2.63507
[1mStep[0m  [105/213], [94mLoss[0m : 2.46282
[1mStep[0m  [126/213], [94mLoss[0m : 2.89645
[1mStep[0m  [147/213], [94mLoss[0m : 2.43582
[1mStep[0m  [168/213], [94mLoss[0m : 2.81873
[1mStep[0m  [189/213], [94mLoss[0m : 2.71171
[1mStep[0m  [210/213], [94mLoss[0m : 2.41658

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.660, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.74944
[1mStep[0m  [21/213], [94mLoss[0m : 2.46876
[1mStep[0m  [42/213], [94mLoss[0m : 2.82179
[1mStep[0m  [63/213], [94mLoss[0m : 2.51392
[1mStep[0m  [84/213], [94mLoss[0m : 2.65203
[1mStep[0m  [105/213], [94mLoss[0m : 3.00254
[1mStep[0m  [126/213], [94mLoss[0m : 1.99637
[1mStep[0m  [147/213], [94mLoss[0m : 2.74813
[1mStep[0m  [168/213], [94mLoss[0m : 2.70793
[1mStep[0m  [189/213], [94mLoss[0m : 2.78277
[1mStep[0m  [210/213], [94mLoss[0m : 2.35271

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.633, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54338
[1mStep[0m  [21/213], [94mLoss[0m : 2.39021
[1mStep[0m  [42/213], [94mLoss[0m : 2.22840
[1mStep[0m  [63/213], [94mLoss[0m : 2.33378
[1mStep[0m  [84/213], [94mLoss[0m : 2.69858
[1mStep[0m  [105/213], [94mLoss[0m : 2.69706
[1mStep[0m  [126/213], [94mLoss[0m : 2.88792
[1mStep[0m  [147/213], [94mLoss[0m : 2.44400
[1mStep[0m  [168/213], [94mLoss[0m : 2.50856
[1mStep[0m  [189/213], [94mLoss[0m : 2.72938
[1mStep[0m  [210/213], [94mLoss[0m : 2.38376

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.643, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31465
[1mStep[0m  [21/213], [94mLoss[0m : 2.59061
[1mStep[0m  [42/213], [94mLoss[0m : 2.40018
[1mStep[0m  [63/213], [94mLoss[0m : 2.28370
[1mStep[0m  [84/213], [94mLoss[0m : 2.58931
[1mStep[0m  [105/213], [94mLoss[0m : 2.54688
[1mStep[0m  [126/213], [94mLoss[0m : 2.39376
[1mStep[0m  [147/213], [94mLoss[0m : 2.40443
[1mStep[0m  [168/213], [94mLoss[0m : 2.84247
[1mStep[0m  [189/213], [94mLoss[0m : 2.64085
[1mStep[0m  [210/213], [94mLoss[0m : 2.72956

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.600, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59665
[1mStep[0m  [21/213], [94mLoss[0m : 2.99546
[1mStep[0m  [42/213], [94mLoss[0m : 2.55535
[1mStep[0m  [63/213], [94mLoss[0m : 2.69807
[1mStep[0m  [84/213], [94mLoss[0m : 2.51667
[1mStep[0m  [105/213], [94mLoss[0m : 2.88894
[1mStep[0m  [126/213], [94mLoss[0m : 2.66821
[1mStep[0m  [147/213], [94mLoss[0m : 2.23597
[1mStep[0m  [168/213], [94mLoss[0m : 2.26171
[1mStep[0m  [189/213], [94mLoss[0m : 2.50595
[1mStep[0m  [210/213], [94mLoss[0m : 2.37603

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.586, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.05312
[1mStep[0m  [21/213], [94mLoss[0m : 3.01042
[1mStep[0m  [42/213], [94mLoss[0m : 2.95888
[1mStep[0m  [63/213], [94mLoss[0m : 2.39911
[1mStep[0m  [84/213], [94mLoss[0m : 2.41623
[1mStep[0m  [105/213], [94mLoss[0m : 2.41455
[1mStep[0m  [126/213], [94mLoss[0m : 3.02346
[1mStep[0m  [147/213], [94mLoss[0m : 2.37565
[1mStep[0m  [168/213], [94mLoss[0m : 2.88936
[1mStep[0m  [189/213], [94mLoss[0m : 2.19403
[1mStep[0m  [210/213], [94mLoss[0m : 2.55498

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.390, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14456
[1mStep[0m  [21/213], [94mLoss[0m : 2.26291
[1mStep[0m  [42/213], [94mLoss[0m : 2.86349
[1mStep[0m  [63/213], [94mLoss[0m : 2.50359
[1mStep[0m  [84/213], [94mLoss[0m : 2.47112
[1mStep[0m  [105/213], [94mLoss[0m : 2.62592
[1mStep[0m  [126/213], [94mLoss[0m : 2.56781
[1mStep[0m  [147/213], [94mLoss[0m : 1.94249
[1mStep[0m  [168/213], [94mLoss[0m : 2.38144
[1mStep[0m  [189/213], [94mLoss[0m : 2.30298
[1mStep[0m  [210/213], [94mLoss[0m : 2.82218

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08231
[1mStep[0m  [21/213], [94mLoss[0m : 2.59098
[1mStep[0m  [42/213], [94mLoss[0m : 2.55755
[1mStep[0m  [63/213], [94mLoss[0m : 2.78565
[1mStep[0m  [84/213], [94mLoss[0m : 1.92390
[1mStep[0m  [105/213], [94mLoss[0m : 2.56147
[1mStep[0m  [126/213], [94mLoss[0m : 2.73339
[1mStep[0m  [147/213], [94mLoss[0m : 2.80228
[1mStep[0m  [168/213], [94mLoss[0m : 2.67774
[1mStep[0m  [189/213], [94mLoss[0m : 2.58879
[1mStep[0m  [210/213], [94mLoss[0m : 2.39884

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40133
[1mStep[0m  [21/213], [94mLoss[0m : 2.82884
[1mStep[0m  [42/213], [94mLoss[0m : 2.71924
[1mStep[0m  [63/213], [94mLoss[0m : 2.48252
[1mStep[0m  [84/213], [94mLoss[0m : 2.24872
[1mStep[0m  [105/213], [94mLoss[0m : 2.20278
[1mStep[0m  [126/213], [94mLoss[0m : 2.53021
[1mStep[0m  [147/213], [94mLoss[0m : 2.70795
[1mStep[0m  [168/213], [94mLoss[0m : 2.32663
[1mStep[0m  [189/213], [94mLoss[0m : 2.55188
[1mStep[0m  [210/213], [94mLoss[0m : 2.23513

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55698
[1mStep[0m  [21/213], [94mLoss[0m : 2.56198
[1mStep[0m  [42/213], [94mLoss[0m : 2.67760
[1mStep[0m  [63/213], [94mLoss[0m : 2.99461
[1mStep[0m  [84/213], [94mLoss[0m : 2.94758
[1mStep[0m  [105/213], [94mLoss[0m : 2.17353
[1mStep[0m  [126/213], [94mLoss[0m : 2.65176
[1mStep[0m  [147/213], [94mLoss[0m : 2.39665
[1mStep[0m  [168/213], [94mLoss[0m : 2.45329
[1mStep[0m  [189/213], [94mLoss[0m : 2.46022
[1mStep[0m  [210/213], [94mLoss[0m : 2.25777

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.551, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42351
[1mStep[0m  [21/213], [94mLoss[0m : 2.72278
[1mStep[0m  [42/213], [94mLoss[0m : 2.56459
[1mStep[0m  [63/213], [94mLoss[0m : 2.67727
[1mStep[0m  [84/213], [94mLoss[0m : 2.57107
[1mStep[0m  [105/213], [94mLoss[0m : 2.57161
[1mStep[0m  [126/213], [94mLoss[0m : 2.68767
[1mStep[0m  [147/213], [94mLoss[0m : 2.28810
[1mStep[0m  [168/213], [94mLoss[0m : 2.56580
[1mStep[0m  [189/213], [94mLoss[0m : 2.47627
[1mStep[0m  [210/213], [94mLoss[0m : 2.56060

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.90780
[1mStep[0m  [21/213], [94mLoss[0m : 3.22429
[1mStep[0m  [42/213], [94mLoss[0m : 2.88900
[1mStep[0m  [63/213], [94mLoss[0m : 2.63715
[1mStep[0m  [84/213], [94mLoss[0m : 2.14636
[1mStep[0m  [105/213], [94mLoss[0m : 2.07678
[1mStep[0m  [126/213], [94mLoss[0m : 2.46696
[1mStep[0m  [147/213], [94mLoss[0m : 2.04837
[1mStep[0m  [168/213], [94mLoss[0m : 2.35651
[1mStep[0m  [189/213], [94mLoss[0m : 2.35775
[1mStep[0m  [210/213], [94mLoss[0m : 2.44824

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.392, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.85404
[1mStep[0m  [21/213], [94mLoss[0m : 2.37537
[1mStep[0m  [42/213], [94mLoss[0m : 2.26736
[1mStep[0m  [63/213], [94mLoss[0m : 2.65937
[1mStep[0m  [84/213], [94mLoss[0m : 2.62343
[1mStep[0m  [105/213], [94mLoss[0m : 2.42141
[1mStep[0m  [126/213], [94mLoss[0m : 2.81346
[1mStep[0m  [147/213], [94mLoss[0m : 2.52170
[1mStep[0m  [168/213], [94mLoss[0m : 2.28568
[1mStep[0m  [189/213], [94mLoss[0m : 2.63906
[1mStep[0m  [210/213], [94mLoss[0m : 2.65288

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.376, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97898
[1mStep[0m  [21/213], [94mLoss[0m : 2.76964
[1mStep[0m  [42/213], [94mLoss[0m : 2.12049
[1mStep[0m  [63/213], [94mLoss[0m : 2.49610
[1mStep[0m  [84/213], [94mLoss[0m : 2.40197
[1mStep[0m  [105/213], [94mLoss[0m : 2.28844
[1mStep[0m  [126/213], [94mLoss[0m : 2.70239
[1mStep[0m  [147/213], [94mLoss[0m : 2.68122
[1mStep[0m  [168/213], [94mLoss[0m : 2.42045
[1mStep[0m  [189/213], [94mLoss[0m : 2.34065
[1mStep[0m  [210/213], [94mLoss[0m : 2.28572

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.389, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35642
[1mStep[0m  [21/213], [94mLoss[0m : 2.22708
[1mStep[0m  [42/213], [94mLoss[0m : 2.37190
[1mStep[0m  [63/213], [94mLoss[0m : 2.91605
[1mStep[0m  [84/213], [94mLoss[0m : 2.94694
[1mStep[0m  [105/213], [94mLoss[0m : 2.34430
[1mStep[0m  [126/213], [94mLoss[0m : 2.44989
[1mStep[0m  [147/213], [94mLoss[0m : 2.00731
[1mStep[0m  [168/213], [94mLoss[0m : 2.40220
[1mStep[0m  [189/213], [94mLoss[0m : 2.63738
[1mStep[0m  [210/213], [94mLoss[0m : 2.39366

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.386, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.85670
[1mStep[0m  [21/213], [94mLoss[0m : 2.52173
[1mStep[0m  [42/213], [94mLoss[0m : 2.31448
[1mStep[0m  [63/213], [94mLoss[0m : 2.94172
[1mStep[0m  [84/213], [94mLoss[0m : 2.63423
[1mStep[0m  [105/213], [94mLoss[0m : 2.48224
[1mStep[0m  [126/213], [94mLoss[0m : 2.20078
[1mStep[0m  [147/213], [94mLoss[0m : 2.39863
[1mStep[0m  [168/213], [94mLoss[0m : 2.47715
[1mStep[0m  [189/213], [94mLoss[0m : 2.86832
[1mStep[0m  [210/213], [94mLoss[0m : 2.64686

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.374, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.34244
[1mStep[0m  [21/213], [94mLoss[0m : 2.13565
[1mStep[0m  [42/213], [94mLoss[0m : 2.18111
[1mStep[0m  [63/213], [94mLoss[0m : 2.55774
[1mStep[0m  [84/213], [94mLoss[0m : 2.41141
[1mStep[0m  [105/213], [94mLoss[0m : 2.57841
[1mStep[0m  [126/213], [94mLoss[0m : 2.24378
[1mStep[0m  [147/213], [94mLoss[0m : 2.47017
[1mStep[0m  [168/213], [94mLoss[0m : 2.60473
[1mStep[0m  [189/213], [94mLoss[0m : 2.31235
[1mStep[0m  [210/213], [94mLoss[0m : 2.49539

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.380, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63246
[1mStep[0m  [21/213], [94mLoss[0m : 2.47271
[1mStep[0m  [42/213], [94mLoss[0m : 2.47687
[1mStep[0m  [63/213], [94mLoss[0m : 2.89505
[1mStep[0m  [84/213], [94mLoss[0m : 2.25888
[1mStep[0m  [105/213], [94mLoss[0m : 2.64353
[1mStep[0m  [126/213], [94mLoss[0m : 2.15318
[1mStep[0m  [147/213], [94mLoss[0m : 2.42713
[1mStep[0m  [168/213], [94mLoss[0m : 2.26551
[1mStep[0m  [189/213], [94mLoss[0m : 2.56324
[1mStep[0m  [210/213], [94mLoss[0m : 2.37568

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.395, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23235
[1mStep[0m  [21/213], [94mLoss[0m : 2.25959
[1mStep[0m  [42/213], [94mLoss[0m : 2.72045
[1mStep[0m  [63/213], [94mLoss[0m : 2.25717
[1mStep[0m  [84/213], [94mLoss[0m : 2.01137
[1mStep[0m  [105/213], [94mLoss[0m : 2.26467
[1mStep[0m  [126/213], [94mLoss[0m : 2.49142
[1mStep[0m  [147/213], [94mLoss[0m : 2.50520
[1mStep[0m  [168/213], [94mLoss[0m : 2.50080
[1mStep[0m  [189/213], [94mLoss[0m : 3.04004
[1mStep[0m  [210/213], [94mLoss[0m : 2.61253

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17065
[1mStep[0m  [21/213], [94mLoss[0m : 2.68902
[1mStep[0m  [42/213], [94mLoss[0m : 2.49211
[1mStep[0m  [63/213], [94mLoss[0m : 2.90574
[1mStep[0m  [84/213], [94mLoss[0m : 2.44804
[1mStep[0m  [105/213], [94mLoss[0m : 2.67490
[1mStep[0m  [126/213], [94mLoss[0m : 2.58239
[1mStep[0m  [147/213], [94mLoss[0m : 2.45019
[1mStep[0m  [168/213], [94mLoss[0m : 2.47863
[1mStep[0m  [189/213], [94mLoss[0m : 2.57309
[1mStep[0m  [210/213], [94mLoss[0m : 2.39753

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.391, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.95800
[1mStep[0m  [21/213], [94mLoss[0m : 2.90274
[1mStep[0m  [42/213], [94mLoss[0m : 2.73590
[1mStep[0m  [63/213], [94mLoss[0m : 2.30343
[1mStep[0m  [84/213], [94mLoss[0m : 2.78347
[1mStep[0m  [105/213], [94mLoss[0m : 2.87885
[1mStep[0m  [126/213], [94mLoss[0m : 2.54018
[1mStep[0m  [147/213], [94mLoss[0m : 2.35227
[1mStep[0m  [168/213], [94mLoss[0m : 2.31300
[1mStep[0m  [189/213], [94mLoss[0m : 2.43519
[1mStep[0m  [210/213], [94mLoss[0m : 2.63918

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.67774
[1mStep[0m  [21/213], [94mLoss[0m : 2.40045
[1mStep[0m  [42/213], [94mLoss[0m : 2.79951
[1mStep[0m  [63/213], [94mLoss[0m : 2.70744
[1mStep[0m  [84/213], [94mLoss[0m : 2.62079
[1mStep[0m  [105/213], [94mLoss[0m : 2.54467
[1mStep[0m  [126/213], [94mLoss[0m : 2.68273
[1mStep[0m  [147/213], [94mLoss[0m : 2.12395
[1mStep[0m  [168/213], [94mLoss[0m : 2.75426
[1mStep[0m  [189/213], [94mLoss[0m : 2.75994
[1mStep[0m  [210/213], [94mLoss[0m : 2.60461

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.379
====================================

Phase 1 - Evaluation MAE:  2.3788789578203886
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 2.55764
[1mStep[0m  [21/213], [94mLoss[0m : 2.64200
[1mStep[0m  [42/213], [94mLoss[0m : 2.09121
[1mStep[0m  [63/213], [94mLoss[0m : 2.43074
[1mStep[0m  [84/213], [94mLoss[0m : 2.12689
[1mStep[0m  [105/213], [94mLoss[0m : 2.93155
[1mStep[0m  [126/213], [94mLoss[0m : 2.68737
[1mStep[0m  [147/213], [94mLoss[0m : 2.82745
[1mStep[0m  [168/213], [94mLoss[0m : 2.34406
[1mStep[0m  [189/213], [94mLoss[0m : 3.00498
[1mStep[0m  [210/213], [94mLoss[0m : 2.61913

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32095
[1mStep[0m  [21/213], [94mLoss[0m : 2.34806
[1mStep[0m  [42/213], [94mLoss[0m : 2.69206
[1mStep[0m  [63/213], [94mLoss[0m : 2.60569
[1mStep[0m  [84/213], [94mLoss[0m : 2.81690
[1mStep[0m  [105/213], [94mLoss[0m : 2.34733
[1mStep[0m  [126/213], [94mLoss[0m : 2.30181
[1mStep[0m  [147/213], [94mLoss[0m : 2.56557
[1mStep[0m  [168/213], [94mLoss[0m : 2.16078
[1mStep[0m  [189/213], [94mLoss[0m : 2.01382
[1mStep[0m  [210/213], [94mLoss[0m : 2.71329

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.640, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19822
[1mStep[0m  [21/213], [94mLoss[0m : 2.40163
[1mStep[0m  [42/213], [94mLoss[0m : 2.38287
[1mStep[0m  [63/213], [94mLoss[0m : 2.26548
[1mStep[0m  [84/213], [94mLoss[0m : 2.11655
[1mStep[0m  [105/213], [94mLoss[0m : 2.49539
[1mStep[0m  [126/213], [94mLoss[0m : 2.22120
[1mStep[0m  [147/213], [94mLoss[0m : 2.35435
[1mStep[0m  [168/213], [94mLoss[0m : 2.37259
[1mStep[0m  [189/213], [94mLoss[0m : 2.42752
[1mStep[0m  [210/213], [94mLoss[0m : 2.04312

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62482
[1mStep[0m  [21/213], [94mLoss[0m : 2.36977
[1mStep[0m  [42/213], [94mLoss[0m : 2.29226
[1mStep[0m  [63/213], [94mLoss[0m : 2.28750
[1mStep[0m  [84/213], [94mLoss[0m : 1.97650
[1mStep[0m  [105/213], [94mLoss[0m : 2.28749
[1mStep[0m  [126/213], [94mLoss[0m : 2.78982
[1mStep[0m  [147/213], [94mLoss[0m : 2.80502
[1mStep[0m  [168/213], [94mLoss[0m : 2.13098
[1mStep[0m  [189/213], [94mLoss[0m : 2.25029
[1mStep[0m  [210/213], [94mLoss[0m : 2.54135

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18645
[1mStep[0m  [21/213], [94mLoss[0m : 2.07225
[1mStep[0m  [42/213], [94mLoss[0m : 2.11173
[1mStep[0m  [63/213], [94mLoss[0m : 2.20009
[1mStep[0m  [84/213], [94mLoss[0m : 2.44455
[1mStep[0m  [105/213], [94mLoss[0m : 2.50832
[1mStep[0m  [126/213], [94mLoss[0m : 2.26590
[1mStep[0m  [147/213], [94mLoss[0m : 2.22630
[1mStep[0m  [168/213], [94mLoss[0m : 3.09109
[1mStep[0m  [189/213], [94mLoss[0m : 2.32711
[1mStep[0m  [210/213], [94mLoss[0m : 2.51363

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.48037
[1mStep[0m  [21/213], [94mLoss[0m : 1.95988
[1mStep[0m  [42/213], [94mLoss[0m : 1.88627
[1mStep[0m  [63/213], [94mLoss[0m : 1.92661
[1mStep[0m  [84/213], [94mLoss[0m : 2.34990
[1mStep[0m  [105/213], [94mLoss[0m : 2.07361
[1mStep[0m  [126/213], [94mLoss[0m : 2.27665
[1mStep[0m  [147/213], [94mLoss[0m : 2.59685
[1mStep[0m  [168/213], [94mLoss[0m : 2.44275
[1mStep[0m  [189/213], [94mLoss[0m : 2.55524
[1mStep[0m  [210/213], [94mLoss[0m : 2.07668

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.300, [92mTest[0m: 2.378, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24664
[1mStep[0m  [21/213], [94mLoss[0m : 1.94461
[1mStep[0m  [42/213], [94mLoss[0m : 2.23447
[1mStep[0m  [63/213], [94mLoss[0m : 2.29855
[1mStep[0m  [84/213], [94mLoss[0m : 2.19552
[1mStep[0m  [105/213], [94mLoss[0m : 2.21418
[1mStep[0m  [126/213], [94mLoss[0m : 1.91402
[1mStep[0m  [147/213], [94mLoss[0m : 2.25512
[1mStep[0m  [168/213], [94mLoss[0m : 2.04655
[1mStep[0m  [189/213], [94mLoss[0m : 2.49646
[1mStep[0m  [210/213], [94mLoss[0m : 2.53937

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.252, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63371
[1mStep[0m  [21/213], [94mLoss[0m : 2.53446
[1mStep[0m  [42/213], [94mLoss[0m : 2.10235
[1mStep[0m  [63/213], [94mLoss[0m : 1.97926
[1mStep[0m  [84/213], [94mLoss[0m : 2.40935
[1mStep[0m  [105/213], [94mLoss[0m : 1.92375
[1mStep[0m  [126/213], [94mLoss[0m : 2.28535
[1mStep[0m  [147/213], [94mLoss[0m : 1.97007
[1mStep[0m  [168/213], [94mLoss[0m : 1.81367
[1mStep[0m  [189/213], [94mLoss[0m : 2.45284
[1mStep[0m  [210/213], [94mLoss[0m : 2.02527

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09093
[1mStep[0m  [21/213], [94mLoss[0m : 1.97977
[1mStep[0m  [42/213], [94mLoss[0m : 2.19382
[1mStep[0m  [63/213], [94mLoss[0m : 1.93895
[1mStep[0m  [84/213], [94mLoss[0m : 2.20331
[1mStep[0m  [105/213], [94mLoss[0m : 2.49298
[1mStep[0m  [126/213], [94mLoss[0m : 2.07267
[1mStep[0m  [147/213], [94mLoss[0m : 2.06832
[1mStep[0m  [168/213], [94mLoss[0m : 2.02915
[1mStep[0m  [189/213], [94mLoss[0m : 2.06019
[1mStep[0m  [210/213], [94mLoss[0m : 1.89579

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.171, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56385
[1mStep[0m  [21/213], [94mLoss[0m : 2.06456
[1mStep[0m  [42/213], [94mLoss[0m : 2.11629
[1mStep[0m  [63/213], [94mLoss[0m : 2.34061
[1mStep[0m  [84/213], [94mLoss[0m : 2.32238
[1mStep[0m  [105/213], [94mLoss[0m : 1.91150
[1mStep[0m  [126/213], [94mLoss[0m : 2.11941
[1mStep[0m  [147/213], [94mLoss[0m : 2.37991
[1mStep[0m  [168/213], [94mLoss[0m : 2.04612
[1mStep[0m  [189/213], [94mLoss[0m : 2.10264
[1mStep[0m  [210/213], [94mLoss[0m : 2.25623

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.363, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.90721
[1mStep[0m  [21/213], [94mLoss[0m : 1.88769
[1mStep[0m  [42/213], [94mLoss[0m : 2.06768
[1mStep[0m  [63/213], [94mLoss[0m : 2.03627
[1mStep[0m  [84/213], [94mLoss[0m : 2.37232
[1mStep[0m  [105/213], [94mLoss[0m : 2.34139
[1mStep[0m  [126/213], [94mLoss[0m : 2.04448
[1mStep[0m  [147/213], [94mLoss[0m : 1.87061
[1mStep[0m  [168/213], [94mLoss[0m : 1.87555
[1mStep[0m  [189/213], [94mLoss[0m : 1.86053
[1mStep[0m  [210/213], [94mLoss[0m : 1.78585

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.394, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.06238
[1mStep[0m  [21/213], [94mLoss[0m : 1.93528
[1mStep[0m  [42/213], [94mLoss[0m : 2.46186
[1mStep[0m  [63/213], [94mLoss[0m : 1.96044
[1mStep[0m  [84/213], [94mLoss[0m : 2.27429
[1mStep[0m  [105/213], [94mLoss[0m : 2.00744
[1mStep[0m  [126/213], [94mLoss[0m : 2.11847
[1mStep[0m  [147/213], [94mLoss[0m : 1.85361
[1mStep[0m  [168/213], [94mLoss[0m : 1.84176
[1mStep[0m  [189/213], [94mLoss[0m : 2.09928
[1mStep[0m  [210/213], [94mLoss[0m : 2.13142

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12709
[1mStep[0m  [21/213], [94mLoss[0m : 2.10865
[1mStep[0m  [42/213], [94mLoss[0m : 1.98702
[1mStep[0m  [63/213], [94mLoss[0m : 1.98901
[1mStep[0m  [84/213], [94mLoss[0m : 1.96513
[1mStep[0m  [105/213], [94mLoss[0m : 1.97403
[1mStep[0m  [126/213], [94mLoss[0m : 2.04971
[1mStep[0m  [147/213], [94mLoss[0m : 1.97113
[1mStep[0m  [168/213], [94mLoss[0m : 2.08667
[1mStep[0m  [189/213], [94mLoss[0m : 1.96016
[1mStep[0m  [210/213], [94mLoss[0m : 2.22936

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.391, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18399
[1mStep[0m  [21/213], [94mLoss[0m : 2.41531
[1mStep[0m  [42/213], [94mLoss[0m : 1.64852
[1mStep[0m  [63/213], [94mLoss[0m : 1.70054
[1mStep[0m  [84/213], [94mLoss[0m : 1.93789
[1mStep[0m  [105/213], [94mLoss[0m : 2.21678
[1mStep[0m  [126/213], [94mLoss[0m : 1.94680
[1mStep[0m  [147/213], [94mLoss[0m : 1.86882
[1mStep[0m  [168/213], [94mLoss[0m : 2.08033
[1mStep[0m  [189/213], [94mLoss[0m : 2.03010
[1mStep[0m  [210/213], [94mLoss[0m : 1.66111

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.77281
[1mStep[0m  [21/213], [94mLoss[0m : 1.76256
[1mStep[0m  [42/213], [94mLoss[0m : 1.74867
[1mStep[0m  [63/213], [94mLoss[0m : 1.61668
[1mStep[0m  [84/213], [94mLoss[0m : 2.13530
[1mStep[0m  [105/213], [94mLoss[0m : 1.80755
[1mStep[0m  [126/213], [94mLoss[0m : 2.07642
[1mStep[0m  [147/213], [94mLoss[0m : 1.91627
[1mStep[0m  [168/213], [94mLoss[0m : 1.83163
[1mStep[0m  [189/213], [94mLoss[0m : 2.00129
[1mStep[0m  [210/213], [94mLoss[0m : 1.69264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.95377
[1mStep[0m  [21/213], [94mLoss[0m : 1.89731
[1mStep[0m  [42/213], [94mLoss[0m : 2.06330
[1mStep[0m  [63/213], [94mLoss[0m : 2.25299
[1mStep[0m  [84/213], [94mLoss[0m : 1.98495
[1mStep[0m  [105/213], [94mLoss[0m : 1.86596
[1mStep[0m  [126/213], [94mLoss[0m : 2.14055
[1mStep[0m  [147/213], [94mLoss[0m : 1.56585
[1mStep[0m  [168/213], [94mLoss[0m : 2.21903
[1mStep[0m  [189/213], [94mLoss[0m : 2.13847
[1mStep[0m  [210/213], [94mLoss[0m : 1.96794

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.929, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.85953
[1mStep[0m  [21/213], [94mLoss[0m : 1.77189
[1mStep[0m  [42/213], [94mLoss[0m : 1.69066
[1mStep[0m  [63/213], [94mLoss[0m : 2.03048
[1mStep[0m  [84/213], [94mLoss[0m : 1.97127
[1mStep[0m  [105/213], [94mLoss[0m : 1.64489
[1mStep[0m  [126/213], [94mLoss[0m : 2.18238
[1mStep[0m  [147/213], [94mLoss[0m : 1.91967
[1mStep[0m  [168/213], [94mLoss[0m : 2.01929
[1mStep[0m  [189/213], [94mLoss[0m : 1.73081
[1mStep[0m  [210/213], [94mLoss[0m : 2.06391

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.897, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81397
[1mStep[0m  [21/213], [94mLoss[0m : 1.77549
[1mStep[0m  [42/213], [94mLoss[0m : 1.62274
[1mStep[0m  [63/213], [94mLoss[0m : 1.46262
[1mStep[0m  [84/213], [94mLoss[0m : 2.09763
[1mStep[0m  [105/213], [94mLoss[0m : 1.87506
[1mStep[0m  [126/213], [94mLoss[0m : 1.93254
[1mStep[0m  [147/213], [94mLoss[0m : 1.93240
[1mStep[0m  [168/213], [94mLoss[0m : 1.95696
[1mStep[0m  [189/213], [94mLoss[0m : 1.98460
[1mStep[0m  [210/213], [94mLoss[0m : 1.95406

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.79028
[1mStep[0m  [21/213], [94mLoss[0m : 1.79261
[1mStep[0m  [42/213], [94mLoss[0m : 1.75212
[1mStep[0m  [63/213], [94mLoss[0m : 1.80358
[1mStep[0m  [84/213], [94mLoss[0m : 1.89063
[1mStep[0m  [105/213], [94mLoss[0m : 1.84038
[1mStep[0m  [126/213], [94mLoss[0m : 2.08930
[1mStep[0m  [147/213], [94mLoss[0m : 1.24317
[1mStep[0m  [168/213], [94mLoss[0m : 1.90882
[1mStep[0m  [189/213], [94mLoss[0m : 1.81763
[1mStep[0m  [210/213], [94mLoss[0m : 1.63491

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.53283
[1mStep[0m  [21/213], [94mLoss[0m : 1.50799
[1mStep[0m  [42/213], [94mLoss[0m : 1.91852
[1mStep[0m  [63/213], [94mLoss[0m : 1.96041
[1mStep[0m  [84/213], [94mLoss[0m : 1.90583
[1mStep[0m  [105/213], [94mLoss[0m : 2.22471
[1mStep[0m  [126/213], [94mLoss[0m : 1.66664
[1mStep[0m  [147/213], [94mLoss[0m : 1.80145
[1mStep[0m  [168/213], [94mLoss[0m : 1.84329
[1mStep[0m  [189/213], [94mLoss[0m : 1.50590
[1mStep[0m  [210/213], [94mLoss[0m : 2.32891

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.806, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56070
[1mStep[0m  [21/213], [94mLoss[0m : 1.58763
[1mStep[0m  [42/213], [94mLoss[0m : 1.44769
[1mStep[0m  [63/213], [94mLoss[0m : 1.70239
[1mStep[0m  [84/213], [94mLoss[0m : 1.67517
[1mStep[0m  [105/213], [94mLoss[0m : 1.74011
[1mStep[0m  [126/213], [94mLoss[0m : 1.64248
[1mStep[0m  [147/213], [94mLoss[0m : 1.78819
[1mStep[0m  [168/213], [94mLoss[0m : 2.02771
[1mStep[0m  [189/213], [94mLoss[0m : 2.00156
[1mStep[0m  [210/213], [94mLoss[0m : 1.82821

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.766, [92mTest[0m: 2.444, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01874
[1mStep[0m  [21/213], [94mLoss[0m : 1.76408
[1mStep[0m  [42/213], [94mLoss[0m : 1.71602
[1mStep[0m  [63/213], [94mLoss[0m : 1.49626
[1mStep[0m  [84/213], [94mLoss[0m : 1.88151
[1mStep[0m  [105/213], [94mLoss[0m : 1.78997
[1mStep[0m  [126/213], [94mLoss[0m : 1.69089
[1mStep[0m  [147/213], [94mLoss[0m : 1.64384
[1mStep[0m  [168/213], [94mLoss[0m : 1.69737
[1mStep[0m  [189/213], [94mLoss[0m : 2.11636
[1mStep[0m  [210/213], [94mLoss[0m : 1.60628

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.84013
[1mStep[0m  [21/213], [94mLoss[0m : 1.46666
[1mStep[0m  [42/213], [94mLoss[0m : 2.02641
[1mStep[0m  [63/213], [94mLoss[0m : 1.68598
[1mStep[0m  [84/213], [94mLoss[0m : 1.42373
[1mStep[0m  [105/213], [94mLoss[0m : 1.43176
[1mStep[0m  [126/213], [94mLoss[0m : 1.77838
[1mStep[0m  [147/213], [94mLoss[0m : 1.72870
[1mStep[0m  [168/213], [94mLoss[0m : 1.33398
[1mStep[0m  [189/213], [94mLoss[0m : 1.47852
[1mStep[0m  [210/213], [94mLoss[0m : 1.77968

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.727, [92mTest[0m: 2.493, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.33602
[1mStep[0m  [21/213], [94mLoss[0m : 1.84637
[1mStep[0m  [42/213], [94mLoss[0m : 1.89062
[1mStep[0m  [63/213], [94mLoss[0m : 1.67242
[1mStep[0m  [84/213], [94mLoss[0m : 1.91411
[1mStep[0m  [105/213], [94mLoss[0m : 1.62268
[1mStep[0m  [126/213], [94mLoss[0m : 1.80771
[1mStep[0m  [147/213], [94mLoss[0m : 1.61082
[1mStep[0m  [168/213], [94mLoss[0m : 1.69722
[1mStep[0m  [189/213], [94mLoss[0m : 1.75469
[1mStep[0m  [210/213], [94mLoss[0m : 1.75902

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.490, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56248
[1mStep[0m  [21/213], [94mLoss[0m : 1.90044
[1mStep[0m  [42/213], [94mLoss[0m : 1.55192
[1mStep[0m  [63/213], [94mLoss[0m : 1.86647
[1mStep[0m  [84/213], [94mLoss[0m : 1.86798
[1mStep[0m  [105/213], [94mLoss[0m : 1.72465
[1mStep[0m  [126/213], [94mLoss[0m : 1.88836
[1mStep[0m  [147/213], [94mLoss[0m : 1.59968
[1mStep[0m  [168/213], [94mLoss[0m : 1.93976
[1mStep[0m  [189/213], [94mLoss[0m : 1.69519
[1mStep[0m  [210/213], [94mLoss[0m : 1.59760

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.672, [92mTest[0m: 2.519, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.60595
[1mStep[0m  [21/213], [94mLoss[0m : 1.58225
[1mStep[0m  [42/213], [94mLoss[0m : 2.06198
[1mStep[0m  [63/213], [94mLoss[0m : 1.52294
[1mStep[0m  [84/213], [94mLoss[0m : 1.75898
[1mStep[0m  [105/213], [94mLoss[0m : 1.44833
[1mStep[0m  [126/213], [94mLoss[0m : 1.70345
[1mStep[0m  [147/213], [94mLoss[0m : 1.27580
[1mStep[0m  [168/213], [94mLoss[0m : 1.69794
[1mStep[0m  [189/213], [94mLoss[0m : 1.70017
[1mStep[0m  [210/213], [94mLoss[0m : 1.79377

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.501, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.01575
[1mStep[0m  [21/213], [94mLoss[0m : 1.46341
[1mStep[0m  [42/213], [94mLoss[0m : 1.80912
[1mStep[0m  [63/213], [94mLoss[0m : 1.65668
[1mStep[0m  [84/213], [94mLoss[0m : 1.74455
[1mStep[0m  [105/213], [94mLoss[0m : 1.57801
[1mStep[0m  [126/213], [94mLoss[0m : 1.73845
[1mStep[0m  [147/213], [94mLoss[0m : 1.58665
[1mStep[0m  [168/213], [94mLoss[0m : 1.70393
[1mStep[0m  [189/213], [94mLoss[0m : 1.81671
[1mStep[0m  [210/213], [94mLoss[0m : 1.74695

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.646, [92mTest[0m: 2.489, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73453
[1mStep[0m  [21/213], [94mLoss[0m : 1.66006
[1mStep[0m  [42/213], [94mLoss[0m : 1.50313
[1mStep[0m  [63/213], [94mLoss[0m : 1.86872
[1mStep[0m  [84/213], [94mLoss[0m : 1.46322
[1mStep[0m  [105/213], [94mLoss[0m : 1.77284
[1mStep[0m  [126/213], [94mLoss[0m : 1.89655
[1mStep[0m  [147/213], [94mLoss[0m : 1.32809
[1mStep[0m  [168/213], [94mLoss[0m : 1.42313
[1mStep[0m  [189/213], [94mLoss[0m : 1.40294
[1mStep[0m  [210/213], [94mLoss[0m : 1.36947

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.610, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.73932
[1mStep[0m  [21/213], [94mLoss[0m : 1.51730
[1mStep[0m  [42/213], [94mLoss[0m : 1.67336
[1mStep[0m  [63/213], [94mLoss[0m : 1.57502
[1mStep[0m  [84/213], [94mLoss[0m : 1.84751
[1mStep[0m  [105/213], [94mLoss[0m : 1.76135
[1mStep[0m  [126/213], [94mLoss[0m : 1.86654
[1mStep[0m  [147/213], [94mLoss[0m : 1.47663
[1mStep[0m  [168/213], [94mLoss[0m : 1.72855
[1mStep[0m  [189/213], [94mLoss[0m : 1.63720
[1mStep[0m  [210/213], [94mLoss[0m : 1.54187

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.468, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45562
[1mStep[0m  [21/213], [94mLoss[0m : 1.78363
[1mStep[0m  [42/213], [94mLoss[0m : 1.49610
[1mStep[0m  [63/213], [94mLoss[0m : 1.24949
[1mStep[0m  [84/213], [94mLoss[0m : 1.51691
[1mStep[0m  [105/213], [94mLoss[0m : 1.73111
[1mStep[0m  [126/213], [94mLoss[0m : 1.85182
[1mStep[0m  [147/213], [94mLoss[0m : 1.36636
[1mStep[0m  [168/213], [94mLoss[0m : 1.34186
[1mStep[0m  [189/213], [94mLoss[0m : 1.67860
[1mStep[0m  [210/213], [94mLoss[0m : 1.74645

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.494
====================================

Phase 2 - Evaluation MAE:  2.494222782692819
MAE score P1       2.378879
MAE score P2       2.494223
loss                 1.5877
learning_rate      0.002575
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay         0.0001
Name: 5, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.01, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.08145
[1mStep[0m  [10/106], [94mLoss[0m : 11.06312
[1mStep[0m  [20/106], [94mLoss[0m : 11.20860
[1mStep[0m  [30/106], [94mLoss[0m : 10.81850
[1mStep[0m  [40/106], [94mLoss[0m : 10.41918
[1mStep[0m  [50/106], [94mLoss[0m : 10.42405
[1mStep[0m  [60/106], [94mLoss[0m : 11.01499
[1mStep[0m  [70/106], [94mLoss[0m : 10.54457
[1mStep[0m  [80/106], [94mLoss[0m : 11.20992
[1mStep[0m  [90/106], [94mLoss[0m : 10.42478
[1mStep[0m  [100/106], [94mLoss[0m : 10.30351

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.723, [92mTest[0m: 10.909, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.89685
[1mStep[0m  [10/106], [94mLoss[0m : 10.51349
[1mStep[0m  [20/106], [94mLoss[0m : 10.30711
[1mStep[0m  [30/106], [94mLoss[0m : 9.92202
[1mStep[0m  [40/106], [94mLoss[0m : 10.51403
[1mStep[0m  [50/106], [94mLoss[0m : 10.53971
[1mStep[0m  [60/106], [94mLoss[0m : 10.56575
[1mStep[0m  [70/106], [94mLoss[0m : 10.15682
[1mStep[0m  [80/106], [94mLoss[0m : 10.06842
[1mStep[0m  [90/106], [94mLoss[0m : 9.67225
[1mStep[0m  [100/106], [94mLoss[0m : 9.86650

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 10.340, [92mTest[0m: 10.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 10.41975
[1mStep[0m  [10/106], [94mLoss[0m : 10.16508
[1mStep[0m  [20/106], [94mLoss[0m : 10.02744
[1mStep[0m  [30/106], [94mLoss[0m : 10.28414
[1mStep[0m  [40/106], [94mLoss[0m : 9.17281
[1mStep[0m  [50/106], [94mLoss[0m : 9.38953
[1mStep[0m  [60/106], [94mLoss[0m : 9.95823
[1mStep[0m  [70/106], [94mLoss[0m : 10.23949
[1mStep[0m  [80/106], [94mLoss[0m : 9.66859
[1mStep[0m  [90/106], [94mLoss[0m : 9.70828
[1mStep[0m  [100/106], [94mLoss[0m : 9.80650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 9.930, [92mTest[0m: 9.872, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.80458
[1mStep[0m  [10/106], [94mLoss[0m : 9.26440
[1mStep[0m  [20/106], [94mLoss[0m : 9.76672
[1mStep[0m  [30/106], [94mLoss[0m : 9.46043
[1mStep[0m  [40/106], [94mLoss[0m : 9.24043
[1mStep[0m  [50/106], [94mLoss[0m : 9.31135
[1mStep[0m  [60/106], [94mLoss[0m : 9.39418
[1mStep[0m  [70/106], [94mLoss[0m : 9.29424
[1mStep[0m  [80/106], [94mLoss[0m : 9.70201
[1mStep[0m  [90/106], [94mLoss[0m : 9.75400
[1mStep[0m  [100/106], [94mLoss[0m : 9.10759

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 9.486, [92mTest[0m: 9.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 9.08585
[1mStep[0m  [10/106], [94mLoss[0m : 8.90249
[1mStep[0m  [20/106], [94mLoss[0m : 9.29179
[1mStep[0m  [30/106], [94mLoss[0m : 9.11727
[1mStep[0m  [40/106], [94mLoss[0m : 9.08758
[1mStep[0m  [50/106], [94mLoss[0m : 9.02719
[1mStep[0m  [60/106], [94mLoss[0m : 8.74296
[1mStep[0m  [70/106], [94mLoss[0m : 9.05850
[1mStep[0m  [80/106], [94mLoss[0m : 8.72095
[1mStep[0m  [90/106], [94mLoss[0m : 8.98078
[1mStep[0m  [100/106], [94mLoss[0m : 8.95856

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 8.964, [92mTest[0m: 8.728, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.64120
[1mStep[0m  [10/106], [94mLoss[0m : 8.52949
[1mStep[0m  [20/106], [94mLoss[0m : 8.35664
[1mStep[0m  [30/106], [94mLoss[0m : 8.58581
[1mStep[0m  [40/106], [94mLoss[0m : 8.34855
[1mStep[0m  [50/106], [94mLoss[0m : 8.52549
[1mStep[0m  [60/106], [94mLoss[0m : 8.32471
[1mStep[0m  [70/106], [94mLoss[0m : 7.69793
[1mStep[0m  [80/106], [94mLoss[0m : 8.52848
[1mStep[0m  [90/106], [94mLoss[0m : 8.10541
[1mStep[0m  [100/106], [94mLoss[0m : 7.72904

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 8.332, [92mTest[0m: 8.168, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.31873
[1mStep[0m  [10/106], [94mLoss[0m : 8.04948
[1mStep[0m  [20/106], [94mLoss[0m : 7.63847
[1mStep[0m  [30/106], [94mLoss[0m : 8.00754
[1mStep[0m  [40/106], [94mLoss[0m : 7.77589
[1mStep[0m  [50/106], [94mLoss[0m : 7.42514
[1mStep[0m  [60/106], [94mLoss[0m : 7.59815
[1mStep[0m  [70/106], [94mLoss[0m : 7.22970
[1mStep[0m  [80/106], [94mLoss[0m : 7.53017
[1mStep[0m  [90/106], [94mLoss[0m : 6.97403
[1mStep[0m  [100/106], [94mLoss[0m : 7.13812

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 7.581, [92mTest[0m: 7.361, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 7.62742
[1mStep[0m  [10/106], [94mLoss[0m : 7.41206
[1mStep[0m  [20/106], [94mLoss[0m : 7.10848
[1mStep[0m  [30/106], [94mLoss[0m : 6.89868
[1mStep[0m  [40/106], [94mLoss[0m : 6.85668
[1mStep[0m  [50/106], [94mLoss[0m : 6.45214
[1mStep[0m  [60/106], [94mLoss[0m : 6.48563
[1mStep[0m  [70/106], [94mLoss[0m : 6.75945
[1mStep[0m  [80/106], [94mLoss[0m : 6.52422
[1mStep[0m  [90/106], [94mLoss[0m : 6.48789
[1mStep[0m  [100/106], [94mLoss[0m : 6.79307

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 6.787, [92mTest[0m: 6.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 6.38076
[1mStep[0m  [10/106], [94mLoss[0m : 6.10147
[1mStep[0m  [20/106], [94mLoss[0m : 6.88194
[1mStep[0m  [30/106], [94mLoss[0m : 6.31717
[1mStep[0m  [40/106], [94mLoss[0m : 5.93266
[1mStep[0m  [50/106], [94mLoss[0m : 5.73723
[1mStep[0m  [60/106], [94mLoss[0m : 5.99458
[1mStep[0m  [70/106], [94mLoss[0m : 6.41608
[1mStep[0m  [80/106], [94mLoss[0m : 6.17599
[1mStep[0m  [90/106], [94mLoss[0m : 5.52952
[1mStep[0m  [100/106], [94mLoss[0m : 5.77676

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 6.072, [92mTest[0m: 5.648, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.87170
[1mStep[0m  [10/106], [94mLoss[0m : 5.39843
[1mStep[0m  [20/106], [94mLoss[0m : 5.64152
[1mStep[0m  [30/106], [94mLoss[0m : 5.23682
[1mStep[0m  [40/106], [94mLoss[0m : 5.07351
[1mStep[0m  [50/106], [94mLoss[0m : 6.04078
[1mStep[0m  [60/106], [94mLoss[0m : 5.17818
[1mStep[0m  [70/106], [94mLoss[0m : 4.73735
[1mStep[0m  [80/106], [94mLoss[0m : 4.71370
[1mStep[0m  [90/106], [94mLoss[0m : 5.40142
[1mStep[0m  [100/106], [94mLoss[0m : 5.20461

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 5.359, [92mTest[0m: 4.926, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.18602
[1mStep[0m  [10/106], [94mLoss[0m : 5.34606
[1mStep[0m  [20/106], [94mLoss[0m : 5.16071
[1mStep[0m  [30/106], [94mLoss[0m : 4.71010
[1mStep[0m  [40/106], [94mLoss[0m : 4.82656
[1mStep[0m  [50/106], [94mLoss[0m : 4.46733
[1mStep[0m  [60/106], [94mLoss[0m : 4.28876
[1mStep[0m  [70/106], [94mLoss[0m : 4.83285
[1mStep[0m  [80/106], [94mLoss[0m : 4.98804
[1mStep[0m  [90/106], [94mLoss[0m : 4.96971
[1mStep[0m  [100/106], [94mLoss[0m : 3.95762

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 4.738, [92mTest[0m: 4.345, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 4.41521
[1mStep[0m  [10/106], [94mLoss[0m : 4.37353
[1mStep[0m  [20/106], [94mLoss[0m : 4.22332
[1mStep[0m  [30/106], [94mLoss[0m : 3.94558
[1mStep[0m  [40/106], [94mLoss[0m : 4.33095
[1mStep[0m  [50/106], [94mLoss[0m : 4.25552
[1mStep[0m  [60/106], [94mLoss[0m : 3.71917
[1mStep[0m  [70/106], [94mLoss[0m : 4.02410
[1mStep[0m  [80/106], [94mLoss[0m : 4.32281
[1mStep[0m  [90/106], [94mLoss[0m : 3.86104
[1mStep[0m  [100/106], [94mLoss[0m : 3.87836

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 4.137, [92mTest[0m: 3.942, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.94296
[1mStep[0m  [10/106], [94mLoss[0m : 3.64107
[1mStep[0m  [20/106], [94mLoss[0m : 3.48589
[1mStep[0m  [30/106], [94mLoss[0m : 3.63091
[1mStep[0m  [40/106], [94mLoss[0m : 4.02421
[1mStep[0m  [50/106], [94mLoss[0m : 3.74686
[1mStep[0m  [60/106], [94mLoss[0m : 3.38882
[1mStep[0m  [70/106], [94mLoss[0m : 3.35455
[1mStep[0m  [80/106], [94mLoss[0m : 3.43020
[1mStep[0m  [90/106], [94mLoss[0m : 3.65768
[1mStep[0m  [100/106], [94mLoss[0m : 3.05206

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 3.596, [92mTest[0m: 3.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.51783
[1mStep[0m  [10/106], [94mLoss[0m : 2.76947
[1mStep[0m  [20/106], [94mLoss[0m : 3.60856
[1mStep[0m  [30/106], [94mLoss[0m : 3.04468
[1mStep[0m  [40/106], [94mLoss[0m : 3.08805
[1mStep[0m  [50/106], [94mLoss[0m : 2.97109
[1mStep[0m  [60/106], [94mLoss[0m : 3.30135
[1mStep[0m  [70/106], [94mLoss[0m : 3.13053
[1mStep[0m  [80/106], [94mLoss[0m : 2.90556
[1mStep[0m  [90/106], [94mLoss[0m : 2.99507
[1mStep[0m  [100/106], [94mLoss[0m : 3.01972

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 3.181, [92mTest[0m: 2.993, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90352
[1mStep[0m  [10/106], [94mLoss[0m : 2.64430
[1mStep[0m  [20/106], [94mLoss[0m : 3.41816
[1mStep[0m  [30/106], [94mLoss[0m : 3.01621
[1mStep[0m  [40/106], [94mLoss[0m : 2.88596
[1mStep[0m  [50/106], [94mLoss[0m : 3.06522
[1mStep[0m  [60/106], [94mLoss[0m : 2.70829
[1mStep[0m  [70/106], [94mLoss[0m : 3.22390
[1mStep[0m  [80/106], [94mLoss[0m : 2.68642
[1mStep[0m  [90/106], [94mLoss[0m : 2.66717
[1mStep[0m  [100/106], [94mLoss[0m : 2.59677

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.887, [92mTest[0m: 2.724, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35612
[1mStep[0m  [10/106], [94mLoss[0m : 2.71865
[1mStep[0m  [20/106], [94mLoss[0m : 2.72274
[1mStep[0m  [30/106], [94mLoss[0m : 2.61352
[1mStep[0m  [40/106], [94mLoss[0m : 2.60992
[1mStep[0m  [50/106], [94mLoss[0m : 2.52882
[1mStep[0m  [60/106], [94mLoss[0m : 2.52815
[1mStep[0m  [70/106], [94mLoss[0m : 2.39098
[1mStep[0m  [80/106], [94mLoss[0m : 2.81024
[1mStep[0m  [90/106], [94mLoss[0m : 2.55068
[1mStep[0m  [100/106], [94mLoss[0m : 2.74555

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.714, [92mTest[0m: 2.550, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91779
[1mStep[0m  [10/106], [94mLoss[0m : 3.14874
[1mStep[0m  [20/106], [94mLoss[0m : 2.84693
[1mStep[0m  [30/106], [94mLoss[0m : 2.32850
[1mStep[0m  [40/106], [94mLoss[0m : 2.58963
[1mStep[0m  [50/106], [94mLoss[0m : 2.67263
[1mStep[0m  [60/106], [94mLoss[0m : 2.59259
[1mStep[0m  [70/106], [94mLoss[0m : 2.68374
[1mStep[0m  [80/106], [94mLoss[0m : 2.85768
[1mStep[0m  [90/106], [94mLoss[0m : 2.82618
[1mStep[0m  [100/106], [94mLoss[0m : 2.78013

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.503, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72431
[1mStep[0m  [10/106], [94mLoss[0m : 2.28407
[1mStep[0m  [20/106], [94mLoss[0m : 2.88883
[1mStep[0m  [30/106], [94mLoss[0m : 2.98389
[1mStep[0m  [40/106], [94mLoss[0m : 2.43707
[1mStep[0m  [50/106], [94mLoss[0m : 2.69787
[1mStep[0m  [60/106], [94mLoss[0m : 2.48274
[1mStep[0m  [70/106], [94mLoss[0m : 2.73994
[1mStep[0m  [80/106], [94mLoss[0m : 2.48954
[1mStep[0m  [90/106], [94mLoss[0m : 2.39967
[1mStep[0m  [100/106], [94mLoss[0m : 2.91191

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66381
[1mStep[0m  [10/106], [94mLoss[0m : 2.43143
[1mStep[0m  [20/106], [94mLoss[0m : 2.50080
[1mStep[0m  [30/106], [94mLoss[0m : 2.60782
[1mStep[0m  [40/106], [94mLoss[0m : 2.64816
[1mStep[0m  [50/106], [94mLoss[0m : 2.67455
[1mStep[0m  [60/106], [94mLoss[0m : 2.66777
[1mStep[0m  [70/106], [94mLoss[0m : 2.57155
[1mStep[0m  [80/106], [94mLoss[0m : 2.35258
[1mStep[0m  [90/106], [94mLoss[0m : 2.92524
[1mStep[0m  [100/106], [94mLoss[0m : 2.81799

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.585, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52581
[1mStep[0m  [10/106], [94mLoss[0m : 2.61588
[1mStep[0m  [20/106], [94mLoss[0m : 2.30144
[1mStep[0m  [30/106], [94mLoss[0m : 2.53948
[1mStep[0m  [40/106], [94mLoss[0m : 2.79899
[1mStep[0m  [50/106], [94mLoss[0m : 2.49273
[1mStep[0m  [60/106], [94mLoss[0m : 2.36829
[1mStep[0m  [70/106], [94mLoss[0m : 2.48780
[1mStep[0m  [80/106], [94mLoss[0m : 2.61033
[1mStep[0m  [90/106], [94mLoss[0m : 2.61404
[1mStep[0m  [100/106], [94mLoss[0m : 2.64655

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58433
[1mStep[0m  [10/106], [94mLoss[0m : 2.54501
[1mStep[0m  [20/106], [94mLoss[0m : 2.72269
[1mStep[0m  [30/106], [94mLoss[0m : 2.60205
[1mStep[0m  [40/106], [94mLoss[0m : 2.76495
[1mStep[0m  [50/106], [94mLoss[0m : 2.56954
[1mStep[0m  [60/106], [94mLoss[0m : 2.61286
[1mStep[0m  [70/106], [94mLoss[0m : 2.47170
[1mStep[0m  [80/106], [94mLoss[0m : 2.93692
[1mStep[0m  [90/106], [94mLoss[0m : 2.75057
[1mStep[0m  [100/106], [94mLoss[0m : 2.46642

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63705
[1mStep[0m  [10/106], [94mLoss[0m : 2.72155
[1mStep[0m  [20/106], [94mLoss[0m : 2.66603
[1mStep[0m  [30/106], [94mLoss[0m : 2.58986
[1mStep[0m  [40/106], [94mLoss[0m : 2.55715
[1mStep[0m  [50/106], [94mLoss[0m : 2.72143
[1mStep[0m  [60/106], [94mLoss[0m : 2.78390
[1mStep[0m  [70/106], [94mLoss[0m : 2.60337
[1mStep[0m  [80/106], [94mLoss[0m : 2.64073
[1mStep[0m  [90/106], [94mLoss[0m : 2.77339
[1mStep[0m  [100/106], [94mLoss[0m : 2.82320

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73421
[1mStep[0m  [10/106], [94mLoss[0m : 2.59263
[1mStep[0m  [20/106], [94mLoss[0m : 2.72293
[1mStep[0m  [30/106], [94mLoss[0m : 2.57645
[1mStep[0m  [40/106], [94mLoss[0m : 3.10308
[1mStep[0m  [50/106], [94mLoss[0m : 2.67984
[1mStep[0m  [60/106], [94mLoss[0m : 2.62538
[1mStep[0m  [70/106], [94mLoss[0m : 2.69073
[1mStep[0m  [80/106], [94mLoss[0m : 2.64111
[1mStep[0m  [90/106], [94mLoss[0m : 2.91306
[1mStep[0m  [100/106], [94mLoss[0m : 2.64141

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.554, [92mTest[0m: 2.427, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.37873
[1mStep[0m  [10/106], [94mLoss[0m : 2.39483
[1mStep[0m  [20/106], [94mLoss[0m : 2.51121
[1mStep[0m  [30/106], [94mLoss[0m : 2.58670
[1mStep[0m  [40/106], [94mLoss[0m : 2.71967
[1mStep[0m  [50/106], [94mLoss[0m : 2.37721
[1mStep[0m  [60/106], [94mLoss[0m : 2.29452
[1mStep[0m  [70/106], [94mLoss[0m : 2.52642
[1mStep[0m  [80/106], [94mLoss[0m : 2.70875
[1mStep[0m  [90/106], [94mLoss[0m : 2.81075
[1mStep[0m  [100/106], [94mLoss[0m : 2.41217

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.425, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71512
[1mStep[0m  [10/106], [94mLoss[0m : 2.68189
[1mStep[0m  [20/106], [94mLoss[0m : 2.53864
[1mStep[0m  [30/106], [94mLoss[0m : 2.40589
[1mStep[0m  [40/106], [94mLoss[0m : 2.76604
[1mStep[0m  [50/106], [94mLoss[0m : 2.75165
[1mStep[0m  [60/106], [94mLoss[0m : 2.36733
[1mStep[0m  [70/106], [94mLoss[0m : 2.46458
[1mStep[0m  [80/106], [94mLoss[0m : 2.34313
[1mStep[0m  [90/106], [94mLoss[0m : 2.53082
[1mStep[0m  [100/106], [94mLoss[0m : 2.58038

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36503
[1mStep[0m  [10/106], [94mLoss[0m : 2.68071
[1mStep[0m  [20/106], [94mLoss[0m : 2.46317
[1mStep[0m  [30/106], [94mLoss[0m : 2.51248
[1mStep[0m  [40/106], [94mLoss[0m : 2.41036
[1mStep[0m  [50/106], [94mLoss[0m : 2.74949
[1mStep[0m  [60/106], [94mLoss[0m : 2.41192
[1mStep[0m  [70/106], [94mLoss[0m : 2.68981
[1mStep[0m  [80/106], [94mLoss[0m : 2.79618
[1mStep[0m  [90/106], [94mLoss[0m : 2.37846
[1mStep[0m  [100/106], [94mLoss[0m : 2.35928

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46559
[1mStep[0m  [10/106], [94mLoss[0m : 2.30569
[1mStep[0m  [20/106], [94mLoss[0m : 2.64827
[1mStep[0m  [30/106], [94mLoss[0m : 2.90465
[1mStep[0m  [40/106], [94mLoss[0m : 2.24837
[1mStep[0m  [50/106], [94mLoss[0m : 2.58231
[1mStep[0m  [60/106], [94mLoss[0m : 2.44396
[1mStep[0m  [70/106], [94mLoss[0m : 2.35773
[1mStep[0m  [80/106], [94mLoss[0m : 2.67124
[1mStep[0m  [90/106], [94mLoss[0m : 2.54480
[1mStep[0m  [100/106], [94mLoss[0m : 2.69021

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64540
[1mStep[0m  [10/106], [94mLoss[0m : 2.40730
[1mStep[0m  [20/106], [94mLoss[0m : 2.96333
[1mStep[0m  [30/106], [94mLoss[0m : 2.68210
[1mStep[0m  [40/106], [94mLoss[0m : 2.66501
[1mStep[0m  [50/106], [94mLoss[0m : 2.55984
[1mStep[0m  [60/106], [94mLoss[0m : 2.55402
[1mStep[0m  [70/106], [94mLoss[0m : 2.56544
[1mStep[0m  [80/106], [94mLoss[0m : 2.50897
[1mStep[0m  [90/106], [94mLoss[0m : 2.34987
[1mStep[0m  [100/106], [94mLoss[0m : 2.77829

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55664
[1mStep[0m  [10/106], [94mLoss[0m : 2.56419
[1mStep[0m  [20/106], [94mLoss[0m : 2.42144
[1mStep[0m  [30/106], [94mLoss[0m : 2.60195
[1mStep[0m  [40/106], [94mLoss[0m : 2.48771
[1mStep[0m  [50/106], [94mLoss[0m : 2.49049
[1mStep[0m  [60/106], [94mLoss[0m : 2.65241
[1mStep[0m  [70/106], [94mLoss[0m : 2.33237
[1mStep[0m  [80/106], [94mLoss[0m : 2.42182
[1mStep[0m  [90/106], [94mLoss[0m : 2.62342
[1mStep[0m  [100/106], [94mLoss[0m : 2.23390

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.415, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56294
[1mStep[0m  [10/106], [94mLoss[0m : 2.34599
[1mStep[0m  [20/106], [94mLoss[0m : 2.49563
[1mStep[0m  [30/106], [94mLoss[0m : 2.25746
[1mStep[0m  [40/106], [94mLoss[0m : 2.48930
[1mStep[0m  [50/106], [94mLoss[0m : 2.50119
[1mStep[0m  [60/106], [94mLoss[0m : 2.65694
[1mStep[0m  [70/106], [94mLoss[0m : 2.39068
[1mStep[0m  [80/106], [94mLoss[0m : 2.30258
[1mStep[0m  [90/106], [94mLoss[0m : 2.47250
[1mStep[0m  [100/106], [94mLoss[0m : 2.40934

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.414
====================================

Phase 1 - Evaluation MAE:  2.4139781133183895
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.48382
[1mStep[0m  [10/106], [94mLoss[0m : 2.47898
[1mStep[0m  [20/106], [94mLoss[0m : 2.50434
[1mStep[0m  [30/106], [94mLoss[0m : 2.62123
[1mStep[0m  [40/106], [94mLoss[0m : 2.58955
[1mStep[0m  [50/106], [94mLoss[0m : 2.88719
[1mStep[0m  [60/106], [94mLoss[0m : 2.66325
[1mStep[0m  [70/106], [94mLoss[0m : 2.60595
[1mStep[0m  [80/106], [94mLoss[0m : 2.40316
[1mStep[0m  [90/106], [94mLoss[0m : 2.55323
[1mStep[0m  [100/106], [94mLoss[0m : 2.61804

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45047
[1mStep[0m  [10/106], [94mLoss[0m : 2.55282
[1mStep[0m  [20/106], [94mLoss[0m : 2.74422
[1mStep[0m  [30/106], [94mLoss[0m : 2.60453
[1mStep[0m  [40/106], [94mLoss[0m : 2.92603
[1mStep[0m  [50/106], [94mLoss[0m : 2.58304
[1mStep[0m  [60/106], [94mLoss[0m : 2.44716
[1mStep[0m  [70/106], [94mLoss[0m : 2.40557
[1mStep[0m  [80/106], [94mLoss[0m : 2.53105
[1mStep[0m  [90/106], [94mLoss[0m : 2.26817
[1mStep[0m  [100/106], [94mLoss[0m : 2.28406

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.560, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31177
[1mStep[0m  [10/106], [94mLoss[0m : 2.48283
[1mStep[0m  [20/106], [94mLoss[0m : 2.66248
[1mStep[0m  [30/106], [94mLoss[0m : 2.31686
[1mStep[0m  [40/106], [94mLoss[0m : 2.26104
[1mStep[0m  [50/106], [94mLoss[0m : 2.37794
[1mStep[0m  [60/106], [94mLoss[0m : 2.12933
[1mStep[0m  [70/106], [94mLoss[0m : 2.31470
[1mStep[0m  [80/106], [94mLoss[0m : 2.48159
[1mStep[0m  [90/106], [94mLoss[0m : 3.00957
[1mStep[0m  [100/106], [94mLoss[0m : 2.11643

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.525, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61295
[1mStep[0m  [10/106], [94mLoss[0m : 2.50575
[1mStep[0m  [20/106], [94mLoss[0m : 2.52909
[1mStep[0m  [30/106], [94mLoss[0m : 2.53482
[1mStep[0m  [40/106], [94mLoss[0m : 2.42689
[1mStep[0m  [50/106], [94mLoss[0m : 2.60030
[1mStep[0m  [60/106], [94mLoss[0m : 2.48222
[1mStep[0m  [70/106], [94mLoss[0m : 2.35761
[1mStep[0m  [80/106], [94mLoss[0m : 2.34474
[1mStep[0m  [90/106], [94mLoss[0m : 2.27630
[1mStep[0m  [100/106], [94mLoss[0m : 2.32342

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.480, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66668
[1mStep[0m  [10/106], [94mLoss[0m : 2.37174
[1mStep[0m  [20/106], [94mLoss[0m : 2.32882
[1mStep[0m  [30/106], [94mLoss[0m : 2.54154
[1mStep[0m  [40/106], [94mLoss[0m : 2.31977
[1mStep[0m  [50/106], [94mLoss[0m : 2.55871
[1mStep[0m  [60/106], [94mLoss[0m : 2.54810
[1mStep[0m  [70/106], [94mLoss[0m : 2.63872
[1mStep[0m  [80/106], [94mLoss[0m : 2.30590
[1mStep[0m  [90/106], [94mLoss[0m : 2.68899
[1mStep[0m  [100/106], [94mLoss[0m : 2.18538

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.542, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30930
[1mStep[0m  [10/106], [94mLoss[0m : 2.73972
[1mStep[0m  [20/106], [94mLoss[0m : 2.29838
[1mStep[0m  [30/106], [94mLoss[0m : 2.43189
[1mStep[0m  [40/106], [94mLoss[0m : 2.52232
[1mStep[0m  [50/106], [94mLoss[0m : 2.57313
[1mStep[0m  [60/106], [94mLoss[0m : 3.00180
[1mStep[0m  [70/106], [94mLoss[0m : 2.79911
[1mStep[0m  [80/106], [94mLoss[0m : 2.44075
[1mStep[0m  [90/106], [94mLoss[0m : 2.70085
[1mStep[0m  [100/106], [94mLoss[0m : 2.33564

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.460, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51638
[1mStep[0m  [10/106], [94mLoss[0m : 2.47962
[1mStep[0m  [20/106], [94mLoss[0m : 2.24232
[1mStep[0m  [30/106], [94mLoss[0m : 2.29634
[1mStep[0m  [40/106], [94mLoss[0m : 2.06167
[1mStep[0m  [50/106], [94mLoss[0m : 2.54031
[1mStep[0m  [60/106], [94mLoss[0m : 2.34197
[1mStep[0m  [70/106], [94mLoss[0m : 2.56177
[1mStep[0m  [80/106], [94mLoss[0m : 2.89189
[1mStep[0m  [90/106], [94mLoss[0m : 2.50322
[1mStep[0m  [100/106], [94mLoss[0m : 2.57494

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41581
[1mStep[0m  [10/106], [94mLoss[0m : 2.47827
[1mStep[0m  [20/106], [94mLoss[0m : 2.38200
[1mStep[0m  [30/106], [94mLoss[0m : 2.69285
[1mStep[0m  [40/106], [94mLoss[0m : 1.95981
[1mStep[0m  [50/106], [94mLoss[0m : 2.61174
[1mStep[0m  [60/106], [94mLoss[0m : 2.68241
[1mStep[0m  [70/106], [94mLoss[0m : 2.21849
[1mStep[0m  [80/106], [94mLoss[0m : 2.31909
[1mStep[0m  [90/106], [94mLoss[0m : 2.16762
[1mStep[0m  [100/106], [94mLoss[0m : 2.47973

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.414, [92mTest[0m: 2.415, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34982
[1mStep[0m  [10/106], [94mLoss[0m : 2.34305
[1mStep[0m  [20/106], [94mLoss[0m : 2.36239
[1mStep[0m  [30/106], [94mLoss[0m : 2.68755
[1mStep[0m  [40/106], [94mLoss[0m : 2.28794
[1mStep[0m  [50/106], [94mLoss[0m : 1.94426
[1mStep[0m  [60/106], [94mLoss[0m : 2.23039
[1mStep[0m  [70/106], [94mLoss[0m : 2.45113
[1mStep[0m  [80/106], [94mLoss[0m : 2.39468
[1mStep[0m  [90/106], [94mLoss[0m : 2.18118
[1mStep[0m  [100/106], [94mLoss[0m : 2.69254

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26682
[1mStep[0m  [10/106], [94mLoss[0m : 2.59562
[1mStep[0m  [20/106], [94mLoss[0m : 2.24536
[1mStep[0m  [30/106], [94mLoss[0m : 2.24516
[1mStep[0m  [40/106], [94mLoss[0m : 2.33133
[1mStep[0m  [50/106], [94mLoss[0m : 2.32379
[1mStep[0m  [60/106], [94mLoss[0m : 2.38081
[1mStep[0m  [70/106], [94mLoss[0m : 2.43259
[1mStep[0m  [80/106], [94mLoss[0m : 2.60526
[1mStep[0m  [90/106], [94mLoss[0m : 2.48703
[1mStep[0m  [100/106], [94mLoss[0m : 2.67003

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60381
[1mStep[0m  [10/106], [94mLoss[0m : 2.28576
[1mStep[0m  [20/106], [94mLoss[0m : 2.40926
[1mStep[0m  [30/106], [94mLoss[0m : 2.65868
[1mStep[0m  [40/106], [94mLoss[0m : 2.06791
[1mStep[0m  [50/106], [94mLoss[0m : 2.16689
[1mStep[0m  [60/106], [94mLoss[0m : 2.21439
[1mStep[0m  [70/106], [94mLoss[0m : 2.35492
[1mStep[0m  [80/106], [94mLoss[0m : 2.64506
[1mStep[0m  [90/106], [94mLoss[0m : 2.15634
[1mStep[0m  [100/106], [94mLoss[0m : 2.18823

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35831
[1mStep[0m  [10/106], [94mLoss[0m : 2.23214
[1mStep[0m  [20/106], [94mLoss[0m : 2.43892
[1mStep[0m  [30/106], [94mLoss[0m : 2.17983
[1mStep[0m  [40/106], [94mLoss[0m : 2.38800
[1mStep[0m  [50/106], [94mLoss[0m : 2.45714
[1mStep[0m  [60/106], [94mLoss[0m : 2.13240
[1mStep[0m  [70/106], [94mLoss[0m : 2.40590
[1mStep[0m  [80/106], [94mLoss[0m : 2.02000
[1mStep[0m  [90/106], [94mLoss[0m : 2.33866
[1mStep[0m  [100/106], [94mLoss[0m : 2.42090

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.334, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.09270
[1mStep[0m  [10/106], [94mLoss[0m : 2.52499
[1mStep[0m  [20/106], [94mLoss[0m : 2.45270
[1mStep[0m  [30/106], [94mLoss[0m : 2.38082
[1mStep[0m  [40/106], [94mLoss[0m : 2.00731
[1mStep[0m  [50/106], [94mLoss[0m : 2.18144
[1mStep[0m  [60/106], [94mLoss[0m : 2.12792
[1mStep[0m  [70/106], [94mLoss[0m : 2.29477
[1mStep[0m  [80/106], [94mLoss[0m : 2.17788
[1mStep[0m  [90/106], [94mLoss[0m : 2.13660
[1mStep[0m  [100/106], [94mLoss[0m : 2.34430

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.521, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19496
[1mStep[0m  [10/106], [94mLoss[0m : 2.16754
[1mStep[0m  [20/106], [94mLoss[0m : 2.21243
[1mStep[0m  [30/106], [94mLoss[0m : 2.10040
[1mStep[0m  [40/106], [94mLoss[0m : 2.22706
[1mStep[0m  [50/106], [94mLoss[0m : 2.10431
[1mStep[0m  [60/106], [94mLoss[0m : 2.08566
[1mStep[0m  [70/106], [94mLoss[0m : 2.44658
[1mStep[0m  [80/106], [94mLoss[0m : 2.59417
[1mStep[0m  [90/106], [94mLoss[0m : 2.25910
[1mStep[0m  [100/106], [94mLoss[0m : 2.01997

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.266, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28442
[1mStep[0m  [10/106], [94mLoss[0m : 2.31046
[1mStep[0m  [20/106], [94mLoss[0m : 2.29913
[1mStep[0m  [30/106], [94mLoss[0m : 2.07428
[1mStep[0m  [40/106], [94mLoss[0m : 2.07083
[1mStep[0m  [50/106], [94mLoss[0m : 2.27814
[1mStep[0m  [60/106], [94mLoss[0m : 2.04403
[1mStep[0m  [70/106], [94mLoss[0m : 2.29835
[1mStep[0m  [80/106], [94mLoss[0m : 2.18044
[1mStep[0m  [90/106], [94mLoss[0m : 1.92267
[1mStep[0m  [100/106], [94mLoss[0m : 2.38373

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.250, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39992
[1mStep[0m  [10/106], [94mLoss[0m : 1.99030
[1mStep[0m  [20/106], [94mLoss[0m : 2.54207
[1mStep[0m  [30/106], [94mLoss[0m : 2.11608
[1mStep[0m  [40/106], [94mLoss[0m : 2.35667
[1mStep[0m  [50/106], [94mLoss[0m : 2.26952
[1mStep[0m  [60/106], [94mLoss[0m : 2.41629
[1mStep[0m  [70/106], [94mLoss[0m : 2.23692
[1mStep[0m  [80/106], [94mLoss[0m : 2.39631
[1mStep[0m  [90/106], [94mLoss[0m : 2.09485
[1mStep[0m  [100/106], [94mLoss[0m : 2.14639

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.212, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92822
[1mStep[0m  [10/106], [94mLoss[0m : 1.96366
[1mStep[0m  [20/106], [94mLoss[0m : 2.23966
[1mStep[0m  [30/106], [94mLoss[0m : 2.28508
[1mStep[0m  [40/106], [94mLoss[0m : 2.13228
[1mStep[0m  [50/106], [94mLoss[0m : 2.47562
[1mStep[0m  [60/106], [94mLoss[0m : 2.47386
[1mStep[0m  [70/106], [94mLoss[0m : 2.24619
[1mStep[0m  [80/106], [94mLoss[0m : 1.92665
[1mStep[0m  [90/106], [94mLoss[0m : 2.03226
[1mStep[0m  [100/106], [94mLoss[0m : 2.46185

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24086
[1mStep[0m  [10/106], [94mLoss[0m : 2.07689
[1mStep[0m  [20/106], [94mLoss[0m : 2.25160
[1mStep[0m  [30/106], [94mLoss[0m : 2.09244
[1mStep[0m  [40/106], [94mLoss[0m : 1.95112
[1mStep[0m  [50/106], [94mLoss[0m : 2.00102
[1mStep[0m  [60/106], [94mLoss[0m : 2.35910
[1mStep[0m  [70/106], [94mLoss[0m : 2.14158
[1mStep[0m  [80/106], [94mLoss[0m : 2.17943
[1mStep[0m  [90/106], [94mLoss[0m : 2.17197
[1mStep[0m  [100/106], [94mLoss[0m : 2.19180

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.499, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41992
[1mStep[0m  [10/106], [94mLoss[0m : 2.08554
[1mStep[0m  [20/106], [94mLoss[0m : 2.07145
[1mStep[0m  [30/106], [94mLoss[0m : 2.14722
[1mStep[0m  [40/106], [94mLoss[0m : 1.82830
[1mStep[0m  [50/106], [94mLoss[0m : 2.11016
[1mStep[0m  [60/106], [94mLoss[0m : 1.94214
[1mStep[0m  [70/106], [94mLoss[0m : 2.14482
[1mStep[0m  [80/106], [94mLoss[0m : 2.10339
[1mStep[0m  [90/106], [94mLoss[0m : 2.24222
[1mStep[0m  [100/106], [94mLoss[0m : 2.01378

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.128, [92mTest[0m: 2.500, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.09746
[1mStep[0m  [10/106], [94mLoss[0m : 2.02531
[1mStep[0m  [20/106], [94mLoss[0m : 2.17429
[1mStep[0m  [30/106], [94mLoss[0m : 2.20028
[1mStep[0m  [40/106], [94mLoss[0m : 1.77885
[1mStep[0m  [50/106], [94mLoss[0m : 2.29288
[1mStep[0m  [60/106], [94mLoss[0m : 2.33339
[1mStep[0m  [70/106], [94mLoss[0m : 2.38276
[1mStep[0m  [80/106], [94mLoss[0m : 2.29504
[1mStep[0m  [90/106], [94mLoss[0m : 1.87061
[1mStep[0m  [100/106], [94mLoss[0m : 1.68492

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.106, [92mTest[0m: 2.437, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.06418
[1mStep[0m  [10/106], [94mLoss[0m : 2.12466
[1mStep[0m  [20/106], [94mLoss[0m : 1.86323
[1mStep[0m  [30/106], [94mLoss[0m : 2.14133
[1mStep[0m  [40/106], [94mLoss[0m : 2.24157
[1mStep[0m  [50/106], [94mLoss[0m : 2.15529
[1mStep[0m  [60/106], [94mLoss[0m : 2.45144
[1mStep[0m  [70/106], [94mLoss[0m : 1.86566
[1mStep[0m  [80/106], [94mLoss[0m : 2.10768
[1mStep[0m  [90/106], [94mLoss[0m : 1.92841
[1mStep[0m  [100/106], [94mLoss[0m : 2.37182

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.414, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91808
[1mStep[0m  [10/106], [94mLoss[0m : 2.11459
[1mStep[0m  [20/106], [94mLoss[0m : 2.04719
[1mStep[0m  [30/106], [94mLoss[0m : 1.93629
[1mStep[0m  [40/106], [94mLoss[0m : 2.15283
[1mStep[0m  [50/106], [94mLoss[0m : 2.13734
[1mStep[0m  [60/106], [94mLoss[0m : 1.96639
[1mStep[0m  [70/106], [94mLoss[0m : 1.98932
[1mStep[0m  [80/106], [94mLoss[0m : 2.20834
[1mStep[0m  [90/106], [94mLoss[0m : 2.04115
[1mStep[0m  [100/106], [94mLoss[0m : 2.18949

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.042, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35897
[1mStep[0m  [10/106], [94mLoss[0m : 1.76466
[1mStep[0m  [20/106], [94mLoss[0m : 2.24702
[1mStep[0m  [30/106], [94mLoss[0m : 2.35950
[1mStep[0m  [40/106], [94mLoss[0m : 1.70798
[1mStep[0m  [50/106], [94mLoss[0m : 1.81387
[1mStep[0m  [60/106], [94mLoss[0m : 1.93137
[1mStep[0m  [70/106], [94mLoss[0m : 2.11524
[1mStep[0m  [80/106], [94mLoss[0m : 1.97877
[1mStep[0m  [90/106], [94mLoss[0m : 1.99733
[1mStep[0m  [100/106], [94mLoss[0m : 2.18855

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.017, [92mTest[0m: 2.424, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.07956
[1mStep[0m  [10/106], [94mLoss[0m : 1.92206
[1mStep[0m  [20/106], [94mLoss[0m : 1.99176
[1mStep[0m  [30/106], [94mLoss[0m : 2.24039
[1mStep[0m  [40/106], [94mLoss[0m : 2.06990
[1mStep[0m  [50/106], [94mLoss[0m : 2.06771
[1mStep[0m  [60/106], [94mLoss[0m : 1.98329
[1mStep[0m  [70/106], [94mLoss[0m : 1.83192
[1mStep[0m  [80/106], [94mLoss[0m : 2.13260
[1mStep[0m  [90/106], [94mLoss[0m : 2.04453
[1mStep[0m  [100/106], [94mLoss[0m : 1.93484

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.998, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.88292
[1mStep[0m  [10/106], [94mLoss[0m : 1.98333
[1mStep[0m  [20/106], [94mLoss[0m : 1.84517
[1mStep[0m  [30/106], [94mLoss[0m : 1.95063
[1mStep[0m  [40/106], [94mLoss[0m : 1.89204
[1mStep[0m  [50/106], [94mLoss[0m : 1.84592
[1mStep[0m  [60/106], [94mLoss[0m : 1.92487
[1mStep[0m  [70/106], [94mLoss[0m : 2.21319
[1mStep[0m  [80/106], [94mLoss[0m : 1.86668
[1mStep[0m  [90/106], [94mLoss[0m : 2.18824
[1mStep[0m  [100/106], [94mLoss[0m : 2.07651

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.974, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.22500
[1mStep[0m  [10/106], [94mLoss[0m : 1.81001
[1mStep[0m  [20/106], [94mLoss[0m : 2.12356
[1mStep[0m  [30/106], [94mLoss[0m : 2.00136
[1mStep[0m  [40/106], [94mLoss[0m : 2.09306
[1mStep[0m  [50/106], [94mLoss[0m : 1.69891
[1mStep[0m  [60/106], [94mLoss[0m : 1.99245
[1mStep[0m  [70/106], [94mLoss[0m : 2.06365
[1mStep[0m  [80/106], [94mLoss[0m : 2.10363
[1mStep[0m  [90/106], [94mLoss[0m : 1.85282
[1mStep[0m  [100/106], [94mLoss[0m : 1.86323

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.941, [92mTest[0m: 2.404, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78364
[1mStep[0m  [10/106], [94mLoss[0m : 1.93980
[1mStep[0m  [20/106], [94mLoss[0m : 1.87283
[1mStep[0m  [30/106], [94mLoss[0m : 2.04266
[1mStep[0m  [40/106], [94mLoss[0m : 2.00165
[1mStep[0m  [50/106], [94mLoss[0m : 2.01270
[1mStep[0m  [60/106], [94mLoss[0m : 2.02362
[1mStep[0m  [70/106], [94mLoss[0m : 2.09267
[1mStep[0m  [80/106], [94mLoss[0m : 2.01643
[1mStep[0m  [90/106], [94mLoss[0m : 2.07879
[1mStep[0m  [100/106], [94mLoss[0m : 1.93948

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.925, [92mTest[0m: 2.471, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.89977
[1mStep[0m  [10/106], [94mLoss[0m : 1.91915
[1mStep[0m  [20/106], [94mLoss[0m : 2.06293
[1mStep[0m  [30/106], [94mLoss[0m : 1.83793
[1mStep[0m  [40/106], [94mLoss[0m : 1.87102
[1mStep[0m  [50/106], [94mLoss[0m : 1.92054
[1mStep[0m  [60/106], [94mLoss[0m : 1.75397
[1mStep[0m  [70/106], [94mLoss[0m : 1.80991
[1mStep[0m  [80/106], [94mLoss[0m : 1.90473
[1mStep[0m  [90/106], [94mLoss[0m : 2.14803
[1mStep[0m  [100/106], [94mLoss[0m : 1.89118

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.63209
[1mStep[0m  [10/106], [94mLoss[0m : 1.88084
[1mStep[0m  [20/106], [94mLoss[0m : 2.13611
[1mStep[0m  [30/106], [94mLoss[0m : 2.05420
[1mStep[0m  [40/106], [94mLoss[0m : 2.13129
[1mStep[0m  [50/106], [94mLoss[0m : 1.71651
[1mStep[0m  [60/106], [94mLoss[0m : 1.88917
[1mStep[0m  [70/106], [94mLoss[0m : 1.85315
[1mStep[0m  [80/106], [94mLoss[0m : 1.66235
[1mStep[0m  [90/106], [94mLoss[0m : 2.13076
[1mStep[0m  [100/106], [94mLoss[0m : 2.00607

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.413, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91174
[1mStep[0m  [10/106], [94mLoss[0m : 2.03952
[1mStep[0m  [20/106], [94mLoss[0m : 1.76678
[1mStep[0m  [30/106], [94mLoss[0m : 1.94522
[1mStep[0m  [40/106], [94mLoss[0m : 1.77405
[1mStep[0m  [50/106], [94mLoss[0m : 1.80347
[1mStep[0m  [60/106], [94mLoss[0m : 1.80594
[1mStep[0m  [70/106], [94mLoss[0m : 1.79099
[1mStep[0m  [80/106], [94mLoss[0m : 1.78402
[1mStep[0m  [90/106], [94mLoss[0m : 1.75016
[1mStep[0m  [100/106], [94mLoss[0m : 2.03224

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.509, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.452
====================================

Phase 2 - Evaluation MAE:  2.4516014602949037
MAE score P1       2.413978
MAE score P2       2.451601
loss               1.883245
learning_rate      0.002575
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.1
weight_decay           0.01
Name: 6, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [200, 50], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.2, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 11.84747
[1mStep[0m  [21/213], [94mLoss[0m : 9.98541
[1mStep[0m  [42/213], [94mLoss[0m : 8.25665
[1mStep[0m  [63/213], [94mLoss[0m : 8.86122
[1mStep[0m  [84/213], [94mLoss[0m : 6.70763
[1mStep[0m  [105/213], [94mLoss[0m : 6.33284
[1mStep[0m  [126/213], [94mLoss[0m : 5.17031
[1mStep[0m  [147/213], [94mLoss[0m : 3.91998
[1mStep[0m  [168/213], [94mLoss[0m : 3.88226
[1mStep[0m  [189/213], [94mLoss[0m : 3.67753
[1mStep[0m  [210/213], [94mLoss[0m : 3.62550

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.417, [92mTest[0m: 10.937, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.33536
[1mStep[0m  [21/213], [94mLoss[0m : 2.79194
[1mStep[0m  [42/213], [94mLoss[0m : 3.05933
[1mStep[0m  [63/213], [94mLoss[0m : 3.20922
[1mStep[0m  [84/213], [94mLoss[0m : 2.75553
[1mStep[0m  [105/213], [94mLoss[0m : 2.71470
[1mStep[0m  [126/213], [94mLoss[0m : 2.87187
[1mStep[0m  [147/213], [94mLoss[0m : 2.71957
[1mStep[0m  [168/213], [94mLoss[0m : 2.53050
[1mStep[0m  [189/213], [94mLoss[0m : 3.05485
[1mStep[0m  [210/213], [94mLoss[0m : 2.42773

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.717, [92mTest[0m: 3.452, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46277
[1mStep[0m  [21/213], [94mLoss[0m : 2.66868
[1mStep[0m  [42/213], [94mLoss[0m : 2.49290
[1mStep[0m  [63/213], [94mLoss[0m : 2.83469
[1mStep[0m  [84/213], [94mLoss[0m : 2.54378
[1mStep[0m  [105/213], [94mLoss[0m : 2.10363
[1mStep[0m  [126/213], [94mLoss[0m : 2.93776
[1mStep[0m  [147/213], [94mLoss[0m : 2.57884
[1mStep[0m  [168/213], [94mLoss[0m : 2.78715
[1mStep[0m  [189/213], [94mLoss[0m : 2.86192
[1mStep[0m  [210/213], [94mLoss[0m : 2.85728

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.679, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.46255
[1mStep[0m  [21/213], [94mLoss[0m : 2.84767
[1mStep[0m  [42/213], [94mLoss[0m : 2.27424
[1mStep[0m  [63/213], [94mLoss[0m : 3.00891
[1mStep[0m  [84/213], [94mLoss[0m : 2.66871
[1mStep[0m  [105/213], [94mLoss[0m : 2.41651
[1mStep[0m  [126/213], [94mLoss[0m : 2.36500
[1mStep[0m  [147/213], [94mLoss[0m : 2.74237
[1mStep[0m  [168/213], [94mLoss[0m : 2.35666
[1mStep[0m  [189/213], [94mLoss[0m : 2.26801
[1mStep[0m  [210/213], [94mLoss[0m : 2.39031

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.504, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40177
[1mStep[0m  [21/213], [94mLoss[0m : 2.25864
[1mStep[0m  [42/213], [94mLoss[0m : 2.17133
[1mStep[0m  [63/213], [94mLoss[0m : 2.33460
[1mStep[0m  [84/213], [94mLoss[0m : 2.93223
[1mStep[0m  [105/213], [94mLoss[0m : 2.40663
[1mStep[0m  [126/213], [94mLoss[0m : 2.90125
[1mStep[0m  [147/213], [94mLoss[0m : 2.30692
[1mStep[0m  [168/213], [94mLoss[0m : 2.46142
[1mStep[0m  [189/213], [94mLoss[0m : 2.71375
[1mStep[0m  [210/213], [94mLoss[0m : 2.83523

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.545, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58449
[1mStep[0m  [21/213], [94mLoss[0m : 2.73733
[1mStep[0m  [42/213], [94mLoss[0m : 2.82667
[1mStep[0m  [63/213], [94mLoss[0m : 2.43779
[1mStep[0m  [84/213], [94mLoss[0m : 2.45612
[1mStep[0m  [105/213], [94mLoss[0m : 2.73836
[1mStep[0m  [126/213], [94mLoss[0m : 2.52219
[1mStep[0m  [147/213], [94mLoss[0m : 2.52314
[1mStep[0m  [168/213], [94mLoss[0m : 2.02017
[1mStep[0m  [189/213], [94mLoss[0m : 2.52263
[1mStep[0m  [210/213], [94mLoss[0m : 2.47744

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.531, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32754
[1mStep[0m  [21/213], [94mLoss[0m : 2.23097
[1mStep[0m  [42/213], [94mLoss[0m : 2.27580
[1mStep[0m  [63/213], [94mLoss[0m : 2.78633
[1mStep[0m  [84/213], [94mLoss[0m : 2.75640
[1mStep[0m  [105/213], [94mLoss[0m : 2.26479
[1mStep[0m  [126/213], [94mLoss[0m : 2.97533
[1mStep[0m  [147/213], [94mLoss[0m : 2.34685
[1mStep[0m  [168/213], [94mLoss[0m : 2.34740
[1mStep[0m  [189/213], [94mLoss[0m : 2.49424
[1mStep[0m  [210/213], [94mLoss[0m : 2.31908

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44963
[1mStep[0m  [21/213], [94mLoss[0m : 2.82655
[1mStep[0m  [42/213], [94mLoss[0m : 2.35539
[1mStep[0m  [63/213], [94mLoss[0m : 2.15327
[1mStep[0m  [84/213], [94mLoss[0m : 2.37545
[1mStep[0m  [105/213], [94mLoss[0m : 2.10430
[1mStep[0m  [126/213], [94mLoss[0m : 2.85754
[1mStep[0m  [147/213], [94mLoss[0m : 2.40486
[1mStep[0m  [168/213], [94mLoss[0m : 2.31501
[1mStep[0m  [189/213], [94mLoss[0m : 2.14973
[1mStep[0m  [210/213], [94mLoss[0m : 2.51652

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.496, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.66551
[1mStep[0m  [21/213], [94mLoss[0m : 2.86486
[1mStep[0m  [42/213], [94mLoss[0m : 2.26702
[1mStep[0m  [63/213], [94mLoss[0m : 2.71558
[1mStep[0m  [84/213], [94mLoss[0m : 2.33192
[1mStep[0m  [105/213], [94mLoss[0m : 2.30703
[1mStep[0m  [126/213], [94mLoss[0m : 2.55827
[1mStep[0m  [147/213], [94mLoss[0m : 2.45076
[1mStep[0m  [168/213], [94mLoss[0m : 2.74786
[1mStep[0m  [189/213], [94mLoss[0m : 2.53741
[1mStep[0m  [210/213], [94mLoss[0m : 2.40135

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.15701
[1mStep[0m  [21/213], [94mLoss[0m : 2.77112
[1mStep[0m  [42/213], [94mLoss[0m : 2.50430
[1mStep[0m  [63/213], [94mLoss[0m : 2.34032
[1mStep[0m  [84/213], [94mLoss[0m : 2.19988
[1mStep[0m  [105/213], [94mLoss[0m : 2.45556
[1mStep[0m  [126/213], [94mLoss[0m : 2.57015
[1mStep[0m  [147/213], [94mLoss[0m : 2.95367
[1mStep[0m  [168/213], [94mLoss[0m : 2.60855
[1mStep[0m  [189/213], [94mLoss[0m : 2.32150
[1mStep[0m  [210/213], [94mLoss[0m : 2.47355

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54283
[1mStep[0m  [21/213], [94mLoss[0m : 2.58282
[1mStep[0m  [42/213], [94mLoss[0m : 2.05902
[1mStep[0m  [63/213], [94mLoss[0m : 2.37780
[1mStep[0m  [84/213], [94mLoss[0m : 2.51285
[1mStep[0m  [105/213], [94mLoss[0m : 2.01125
[1mStep[0m  [126/213], [94mLoss[0m : 2.18451
[1mStep[0m  [147/213], [94mLoss[0m : 2.31103
[1mStep[0m  [168/213], [94mLoss[0m : 2.46457
[1mStep[0m  [189/213], [94mLoss[0m : 2.23998
[1mStep[0m  [210/213], [94mLoss[0m : 2.62866

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.434, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29409
[1mStep[0m  [21/213], [94mLoss[0m : 2.52525
[1mStep[0m  [42/213], [94mLoss[0m : 2.67443
[1mStep[0m  [63/213], [94mLoss[0m : 2.43283
[1mStep[0m  [84/213], [94mLoss[0m : 2.26440
[1mStep[0m  [105/213], [94mLoss[0m : 2.33933
[1mStep[0m  [126/213], [94mLoss[0m : 2.09075
[1mStep[0m  [147/213], [94mLoss[0m : 1.85477
[1mStep[0m  [168/213], [94mLoss[0m : 2.26523
[1mStep[0m  [189/213], [94mLoss[0m : 2.41707
[1mStep[0m  [210/213], [94mLoss[0m : 2.33208

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.459, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39565
[1mStep[0m  [21/213], [94mLoss[0m : 2.92359
[1mStep[0m  [42/213], [94mLoss[0m : 2.66012
[1mStep[0m  [63/213], [94mLoss[0m : 2.61079
[1mStep[0m  [84/213], [94mLoss[0m : 2.24143
[1mStep[0m  [105/213], [94mLoss[0m : 2.38990
[1mStep[0m  [126/213], [94mLoss[0m : 2.37985
[1mStep[0m  [147/213], [94mLoss[0m : 2.12527
[1mStep[0m  [168/213], [94mLoss[0m : 2.55286
[1mStep[0m  [189/213], [94mLoss[0m : 2.34698
[1mStep[0m  [210/213], [94mLoss[0m : 2.88982

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.22191
[1mStep[0m  [21/213], [94mLoss[0m : 2.54805
[1mStep[0m  [42/213], [94mLoss[0m : 2.65687
[1mStep[0m  [63/213], [94mLoss[0m : 2.27953
[1mStep[0m  [84/213], [94mLoss[0m : 2.59295
[1mStep[0m  [105/213], [94mLoss[0m : 2.29602
[1mStep[0m  [126/213], [94mLoss[0m : 2.36333
[1mStep[0m  [147/213], [94mLoss[0m : 2.72842
[1mStep[0m  [168/213], [94mLoss[0m : 2.37506
[1mStep[0m  [189/213], [94mLoss[0m : 2.72630
[1mStep[0m  [210/213], [94mLoss[0m : 2.52754

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.99752
[1mStep[0m  [21/213], [94mLoss[0m : 2.56287
[1mStep[0m  [42/213], [94mLoss[0m : 2.47872
[1mStep[0m  [63/213], [94mLoss[0m : 2.40440
[1mStep[0m  [84/213], [94mLoss[0m : 2.60826
[1mStep[0m  [105/213], [94mLoss[0m : 2.70430
[1mStep[0m  [126/213], [94mLoss[0m : 2.39819
[1mStep[0m  [147/213], [94mLoss[0m : 2.42971
[1mStep[0m  [168/213], [94mLoss[0m : 2.68588
[1mStep[0m  [189/213], [94mLoss[0m : 2.15273
[1mStep[0m  [210/213], [94mLoss[0m : 2.47686

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61273
[1mStep[0m  [21/213], [94mLoss[0m : 2.54140
[1mStep[0m  [42/213], [94mLoss[0m : 2.75590
[1mStep[0m  [63/213], [94mLoss[0m : 2.72355
[1mStep[0m  [84/213], [94mLoss[0m : 2.65284
[1mStep[0m  [105/213], [94mLoss[0m : 2.74480
[1mStep[0m  [126/213], [94mLoss[0m : 2.60003
[1mStep[0m  [147/213], [94mLoss[0m : 2.78054
[1mStep[0m  [168/213], [94mLoss[0m : 2.28285
[1mStep[0m  [189/213], [94mLoss[0m : 2.26686
[1mStep[0m  [210/213], [94mLoss[0m : 2.40609

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33703
[1mStep[0m  [21/213], [94mLoss[0m : 2.14528
[1mStep[0m  [42/213], [94mLoss[0m : 2.43884
[1mStep[0m  [63/213], [94mLoss[0m : 2.59249
[1mStep[0m  [84/213], [94mLoss[0m : 2.15484
[1mStep[0m  [105/213], [94mLoss[0m : 2.30559
[1mStep[0m  [126/213], [94mLoss[0m : 2.49443
[1mStep[0m  [147/213], [94mLoss[0m : 2.51158
[1mStep[0m  [168/213], [94mLoss[0m : 2.78078
[1mStep[0m  [189/213], [94mLoss[0m : 2.47497
[1mStep[0m  [210/213], [94mLoss[0m : 2.51653

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31167
[1mStep[0m  [21/213], [94mLoss[0m : 3.17330
[1mStep[0m  [42/213], [94mLoss[0m : 2.50508
[1mStep[0m  [63/213], [94mLoss[0m : 2.67656
[1mStep[0m  [84/213], [94mLoss[0m : 2.38056
[1mStep[0m  [105/213], [94mLoss[0m : 2.36489
[1mStep[0m  [126/213], [94mLoss[0m : 2.23985
[1mStep[0m  [147/213], [94mLoss[0m : 2.40714
[1mStep[0m  [168/213], [94mLoss[0m : 1.85483
[1mStep[0m  [189/213], [94mLoss[0m : 2.19932
[1mStep[0m  [210/213], [94mLoss[0m : 2.93123

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.407, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.43529
[1mStep[0m  [21/213], [94mLoss[0m : 2.49242
[1mStep[0m  [42/213], [94mLoss[0m : 2.01047
[1mStep[0m  [63/213], [94mLoss[0m : 2.46030
[1mStep[0m  [84/213], [94mLoss[0m : 2.71724
[1mStep[0m  [105/213], [94mLoss[0m : 2.83618
[1mStep[0m  [126/213], [94mLoss[0m : 2.23788
[1mStep[0m  [147/213], [94mLoss[0m : 2.38447
[1mStep[0m  [168/213], [94mLoss[0m : 2.74198
[1mStep[0m  [189/213], [94mLoss[0m : 2.60147
[1mStep[0m  [210/213], [94mLoss[0m : 2.18958

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.52989
[1mStep[0m  [21/213], [94mLoss[0m : 2.19711
[1mStep[0m  [42/213], [94mLoss[0m : 2.07440
[1mStep[0m  [63/213], [94mLoss[0m : 2.96716
[1mStep[0m  [84/213], [94mLoss[0m : 2.57658
[1mStep[0m  [105/213], [94mLoss[0m : 2.43100
[1mStep[0m  [126/213], [94mLoss[0m : 2.11504
[1mStep[0m  [147/213], [94mLoss[0m : 2.60956
[1mStep[0m  [168/213], [94mLoss[0m : 2.54969
[1mStep[0m  [189/213], [94mLoss[0m : 2.29548
[1mStep[0m  [210/213], [94mLoss[0m : 2.23650

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.395, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25408
[1mStep[0m  [21/213], [94mLoss[0m : 2.19058
[1mStep[0m  [42/213], [94mLoss[0m : 2.28091
[1mStep[0m  [63/213], [94mLoss[0m : 2.47337
[1mStep[0m  [84/213], [94mLoss[0m : 2.63103
[1mStep[0m  [105/213], [94mLoss[0m : 2.25989
[1mStep[0m  [126/213], [94mLoss[0m : 2.03897
[1mStep[0m  [147/213], [94mLoss[0m : 2.63672
[1mStep[0m  [168/213], [94mLoss[0m : 2.20139
[1mStep[0m  [189/213], [94mLoss[0m : 2.61881
[1mStep[0m  [210/213], [94mLoss[0m : 2.44217

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.398, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.29525
[1mStep[0m  [21/213], [94mLoss[0m : 2.36738
[1mStep[0m  [42/213], [94mLoss[0m : 3.03934
[1mStep[0m  [63/213], [94mLoss[0m : 2.39936
[1mStep[0m  [84/213], [94mLoss[0m : 2.26971
[1mStep[0m  [105/213], [94mLoss[0m : 1.96628
[1mStep[0m  [126/213], [94mLoss[0m : 2.55441
[1mStep[0m  [147/213], [94mLoss[0m : 2.26147
[1mStep[0m  [168/213], [94mLoss[0m : 2.91182
[1mStep[0m  [189/213], [94mLoss[0m : 2.67052
[1mStep[0m  [210/213], [94mLoss[0m : 2.52574

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.404, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39870
[1mStep[0m  [21/213], [94mLoss[0m : 2.90899
[1mStep[0m  [42/213], [94mLoss[0m : 2.47387
[1mStep[0m  [63/213], [94mLoss[0m : 2.30323
[1mStep[0m  [84/213], [94mLoss[0m : 2.69418
[1mStep[0m  [105/213], [94mLoss[0m : 2.68688
[1mStep[0m  [126/213], [94mLoss[0m : 2.22466
[1mStep[0m  [147/213], [94mLoss[0m : 2.33287
[1mStep[0m  [168/213], [94mLoss[0m : 2.34275
[1mStep[0m  [189/213], [94mLoss[0m : 2.10639
[1mStep[0m  [210/213], [94mLoss[0m : 2.48132

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21492
[1mStep[0m  [21/213], [94mLoss[0m : 2.26565
[1mStep[0m  [42/213], [94mLoss[0m : 2.11677
[1mStep[0m  [63/213], [94mLoss[0m : 2.36491
[1mStep[0m  [84/213], [94mLoss[0m : 2.09440
[1mStep[0m  [105/213], [94mLoss[0m : 2.62192
[1mStep[0m  [126/213], [94mLoss[0m : 2.03737
[1mStep[0m  [147/213], [94mLoss[0m : 2.88120
[1mStep[0m  [168/213], [94mLoss[0m : 2.39962
[1mStep[0m  [189/213], [94mLoss[0m : 2.62699
[1mStep[0m  [210/213], [94mLoss[0m : 2.50608

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.408, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.72274
[1mStep[0m  [21/213], [94mLoss[0m : 2.49568
[1mStep[0m  [42/213], [94mLoss[0m : 2.43273
[1mStep[0m  [63/213], [94mLoss[0m : 2.07769
[1mStep[0m  [84/213], [94mLoss[0m : 2.79205
[1mStep[0m  [105/213], [94mLoss[0m : 2.70253
[1mStep[0m  [126/213], [94mLoss[0m : 1.96461
[1mStep[0m  [147/213], [94mLoss[0m : 2.36847
[1mStep[0m  [168/213], [94mLoss[0m : 2.50311
[1mStep[0m  [189/213], [94mLoss[0m : 2.26062
[1mStep[0m  [210/213], [94mLoss[0m : 2.40456

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.390, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55289
[1mStep[0m  [21/213], [94mLoss[0m : 2.47874
[1mStep[0m  [42/213], [94mLoss[0m : 2.31203
[1mStep[0m  [63/213], [94mLoss[0m : 2.46497
[1mStep[0m  [84/213], [94mLoss[0m : 2.11967
[1mStep[0m  [105/213], [94mLoss[0m : 2.11213
[1mStep[0m  [126/213], [94mLoss[0m : 2.32079
[1mStep[0m  [147/213], [94mLoss[0m : 2.50862
[1mStep[0m  [168/213], [94mLoss[0m : 2.20238
[1mStep[0m  [189/213], [94mLoss[0m : 2.23887
[1mStep[0m  [210/213], [94mLoss[0m : 2.80944

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.389, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33200
[1mStep[0m  [21/213], [94mLoss[0m : 2.28675
[1mStep[0m  [42/213], [94mLoss[0m : 2.57659
[1mStep[0m  [63/213], [94mLoss[0m : 2.36147
[1mStep[0m  [84/213], [94mLoss[0m : 2.47721
[1mStep[0m  [105/213], [94mLoss[0m : 2.24020
[1mStep[0m  [126/213], [94mLoss[0m : 2.41957
[1mStep[0m  [147/213], [94mLoss[0m : 2.63563
[1mStep[0m  [168/213], [94mLoss[0m : 2.26394
[1mStep[0m  [189/213], [94mLoss[0m : 2.26658
[1mStep[0m  [210/213], [94mLoss[0m : 2.75777

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.386, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62209
[1mStep[0m  [21/213], [94mLoss[0m : 2.23485
[1mStep[0m  [42/213], [94mLoss[0m : 2.34533
[1mStep[0m  [63/213], [94mLoss[0m : 2.19928
[1mStep[0m  [84/213], [94mLoss[0m : 2.32948
[1mStep[0m  [105/213], [94mLoss[0m : 2.50944
[1mStep[0m  [126/213], [94mLoss[0m : 2.15532
[1mStep[0m  [147/213], [94mLoss[0m : 2.63988
[1mStep[0m  [168/213], [94mLoss[0m : 2.50815
[1mStep[0m  [189/213], [94mLoss[0m : 2.62987
[1mStep[0m  [210/213], [94mLoss[0m : 2.59115

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.385, [92mTest[0m: 2.382, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54096
[1mStep[0m  [21/213], [94mLoss[0m : 2.05248
[1mStep[0m  [42/213], [94mLoss[0m : 2.24328
[1mStep[0m  [63/213], [94mLoss[0m : 2.58296
[1mStep[0m  [84/213], [94mLoss[0m : 2.54190
[1mStep[0m  [105/213], [94mLoss[0m : 2.72161
[1mStep[0m  [126/213], [94mLoss[0m : 2.72097
[1mStep[0m  [147/213], [94mLoss[0m : 2.47694
[1mStep[0m  [168/213], [94mLoss[0m : 2.61367
[1mStep[0m  [189/213], [94mLoss[0m : 2.29417
[1mStep[0m  [210/213], [94mLoss[0m : 1.85173

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.379, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51953
[1mStep[0m  [21/213], [94mLoss[0m : 2.09362
[1mStep[0m  [42/213], [94mLoss[0m : 2.52486
[1mStep[0m  [63/213], [94mLoss[0m : 2.52392
[1mStep[0m  [84/213], [94mLoss[0m : 2.42310
[1mStep[0m  [105/213], [94mLoss[0m : 2.46141
[1mStep[0m  [126/213], [94mLoss[0m : 2.40028
[1mStep[0m  [147/213], [94mLoss[0m : 2.18267
[1mStep[0m  [168/213], [94mLoss[0m : 2.10852
[1mStep[0m  [189/213], [94mLoss[0m : 2.16279
[1mStep[0m  [210/213], [94mLoss[0m : 2.19845

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.365
====================================

Phase 1 - Evaluation MAE:  2.364897869667917
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 2.39831
[1mStep[0m  [21/213], [94mLoss[0m : 2.55889
[1mStep[0m  [42/213], [94mLoss[0m : 2.34206
[1mStep[0m  [63/213], [94mLoss[0m : 2.52070
[1mStep[0m  [84/213], [94mLoss[0m : 2.19453
[1mStep[0m  [105/213], [94mLoss[0m : 2.15959
[1mStep[0m  [126/213], [94mLoss[0m : 2.86867
[1mStep[0m  [147/213], [94mLoss[0m : 2.71920
[1mStep[0m  [168/213], [94mLoss[0m : 2.61398
[1mStep[0m  [189/213], [94mLoss[0m : 2.72198
[1mStep[0m  [210/213], [94mLoss[0m : 2.68195

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23883
[1mStep[0m  [21/213], [94mLoss[0m : 2.34845
[1mStep[0m  [42/213], [94mLoss[0m : 2.34162
[1mStep[0m  [63/213], [94mLoss[0m : 2.56490
[1mStep[0m  [84/213], [94mLoss[0m : 2.27585
[1mStep[0m  [105/213], [94mLoss[0m : 2.57624
[1mStep[0m  [126/213], [94mLoss[0m : 2.62727
[1mStep[0m  [147/213], [94mLoss[0m : 1.95039
[1mStep[0m  [168/213], [94mLoss[0m : 2.39717
[1mStep[0m  [189/213], [94mLoss[0m : 2.47226
[1mStep[0m  [210/213], [94mLoss[0m : 2.42711

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.419, [92mTest[0m: 3.264, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30335
[1mStep[0m  [21/213], [94mLoss[0m : 1.96744
[1mStep[0m  [42/213], [94mLoss[0m : 2.67744
[1mStep[0m  [63/213], [94mLoss[0m : 2.56319
[1mStep[0m  [84/213], [94mLoss[0m : 2.39790
[1mStep[0m  [105/213], [94mLoss[0m : 2.13380
[1mStep[0m  [126/213], [94mLoss[0m : 2.39696
[1mStep[0m  [147/213], [94mLoss[0m : 2.56358
[1mStep[0m  [168/213], [94mLoss[0m : 2.42490
[1mStep[0m  [189/213], [94mLoss[0m : 2.48362
[1mStep[0m  [210/213], [94mLoss[0m : 2.41352

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.392, [92mTest[0m: 3.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08769
[1mStep[0m  [21/213], [94mLoss[0m : 2.79839
[1mStep[0m  [42/213], [94mLoss[0m : 2.32338
[1mStep[0m  [63/213], [94mLoss[0m : 2.76180
[1mStep[0m  [84/213], [94mLoss[0m : 2.17758
[1mStep[0m  [105/213], [94mLoss[0m : 2.48455
[1mStep[0m  [126/213], [94mLoss[0m : 2.36888
[1mStep[0m  [147/213], [94mLoss[0m : 2.26204
[1mStep[0m  [168/213], [94mLoss[0m : 2.29747
[1mStep[0m  [189/213], [94mLoss[0m : 2.47921
[1mStep[0m  [210/213], [94mLoss[0m : 2.78367

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.975, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.51135
[1mStep[0m  [21/213], [94mLoss[0m : 2.12748
[1mStep[0m  [42/213], [94mLoss[0m : 2.10019
[1mStep[0m  [63/213], [94mLoss[0m : 2.29629
[1mStep[0m  [84/213], [94mLoss[0m : 2.48867
[1mStep[0m  [105/213], [94mLoss[0m : 2.44008
[1mStep[0m  [126/213], [94mLoss[0m : 2.51879
[1mStep[0m  [147/213], [94mLoss[0m : 2.49680
[1mStep[0m  [168/213], [94mLoss[0m : 2.08700
[1mStep[0m  [189/213], [94mLoss[0m : 2.15360
[1mStep[0m  [210/213], [94mLoss[0m : 2.37489

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.299, [92mTest[0m: 2.636, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17550
[1mStep[0m  [21/213], [94mLoss[0m : 2.29641
[1mStep[0m  [42/213], [94mLoss[0m : 2.15040
[1mStep[0m  [63/213], [94mLoss[0m : 2.40362
[1mStep[0m  [84/213], [94mLoss[0m : 2.69153
[1mStep[0m  [105/213], [94mLoss[0m : 2.42437
[1mStep[0m  [126/213], [94mLoss[0m : 2.29844
[1mStep[0m  [147/213], [94mLoss[0m : 2.19437
[1mStep[0m  [168/213], [94mLoss[0m : 1.95916
[1mStep[0m  [189/213], [94mLoss[0m : 2.46606
[1mStep[0m  [210/213], [94mLoss[0m : 2.37078

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.552, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.83259
[1mStep[0m  [21/213], [94mLoss[0m : 2.25679
[1mStep[0m  [42/213], [94mLoss[0m : 2.15484
[1mStep[0m  [63/213], [94mLoss[0m : 2.23248
[1mStep[0m  [84/213], [94mLoss[0m : 2.23934
[1mStep[0m  [105/213], [94mLoss[0m : 2.14551
[1mStep[0m  [126/213], [94mLoss[0m : 2.32713
[1mStep[0m  [147/213], [94mLoss[0m : 2.34611
[1mStep[0m  [168/213], [94mLoss[0m : 2.19038
[1mStep[0m  [189/213], [94mLoss[0m : 2.32134
[1mStep[0m  [210/213], [94mLoss[0m : 2.10266

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.247, [92mTest[0m: 2.495, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.03740
[1mStep[0m  [21/213], [94mLoss[0m : 2.28161
[1mStep[0m  [42/213], [94mLoss[0m : 2.34493
[1mStep[0m  [63/213], [94mLoss[0m : 1.90652
[1mStep[0m  [84/213], [94mLoss[0m : 2.24969
[1mStep[0m  [105/213], [94mLoss[0m : 2.11338
[1mStep[0m  [126/213], [94mLoss[0m : 2.09863
[1mStep[0m  [147/213], [94mLoss[0m : 2.57671
[1mStep[0m  [168/213], [94mLoss[0m : 2.33288
[1mStep[0m  [189/213], [94mLoss[0m : 1.90470
[1mStep[0m  [210/213], [94mLoss[0m : 2.29755

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.193, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64617
[1mStep[0m  [21/213], [94mLoss[0m : 2.32937
[1mStep[0m  [42/213], [94mLoss[0m : 1.95219
[1mStep[0m  [63/213], [94mLoss[0m : 2.01810
[1mStep[0m  [84/213], [94mLoss[0m : 2.31477
[1mStep[0m  [105/213], [94mLoss[0m : 2.68958
[1mStep[0m  [126/213], [94mLoss[0m : 2.09717
[1mStep[0m  [147/213], [94mLoss[0m : 2.52884
[1mStep[0m  [168/213], [94mLoss[0m : 2.37307
[1mStep[0m  [189/213], [94mLoss[0m : 2.04801
[1mStep[0m  [210/213], [94mLoss[0m : 1.88074

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.159, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50922
[1mStep[0m  [21/213], [94mLoss[0m : 2.08610
[1mStep[0m  [42/213], [94mLoss[0m : 2.28071
[1mStep[0m  [63/213], [94mLoss[0m : 1.80407
[1mStep[0m  [84/213], [94mLoss[0m : 2.27327
[1mStep[0m  [105/213], [94mLoss[0m : 1.67294
[1mStep[0m  [126/213], [94mLoss[0m : 1.90417
[1mStep[0m  [147/213], [94mLoss[0m : 2.31559
[1mStep[0m  [168/213], [94mLoss[0m : 2.07544
[1mStep[0m  [189/213], [94mLoss[0m : 2.32587
[1mStep[0m  [210/213], [94mLoss[0m : 2.17716

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89888
[1mStep[0m  [21/213], [94mLoss[0m : 2.08137
[1mStep[0m  [42/213], [94mLoss[0m : 1.97096
[1mStep[0m  [63/213], [94mLoss[0m : 2.01003
[1mStep[0m  [84/213], [94mLoss[0m : 2.07152
[1mStep[0m  [105/213], [94mLoss[0m : 2.05514
[1mStep[0m  [126/213], [94mLoss[0m : 1.95098
[1mStep[0m  [147/213], [94mLoss[0m : 2.26291
[1mStep[0m  [168/213], [94mLoss[0m : 2.06023
[1mStep[0m  [189/213], [94mLoss[0m : 2.59922
[1mStep[0m  [210/213], [94mLoss[0m : 1.91491

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.422, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97992
[1mStep[0m  [21/213], [94mLoss[0m : 2.45499
[1mStep[0m  [42/213], [94mLoss[0m : 2.11028
[1mStep[0m  [63/213], [94mLoss[0m : 1.95752
[1mStep[0m  [84/213], [94mLoss[0m : 2.31798
[1mStep[0m  [105/213], [94mLoss[0m : 2.31210
[1mStep[0m  [126/213], [94mLoss[0m : 1.57382
[1mStep[0m  [147/213], [94mLoss[0m : 2.10112
[1mStep[0m  [168/213], [94mLoss[0m : 1.90542
[1mStep[0m  [189/213], [94mLoss[0m : 2.20369
[1mStep[0m  [210/213], [94mLoss[0m : 1.79201

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.045, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.04381
[1mStep[0m  [21/213], [94mLoss[0m : 1.89371
[1mStep[0m  [42/213], [94mLoss[0m : 2.03308
[1mStep[0m  [63/213], [94mLoss[0m : 2.28056
[1mStep[0m  [84/213], [94mLoss[0m : 1.84322
[1mStep[0m  [105/213], [94mLoss[0m : 2.36483
[1mStep[0m  [126/213], [94mLoss[0m : 2.55016
[1mStep[0m  [147/213], [94mLoss[0m : 1.89149
[1mStep[0m  [168/213], [94mLoss[0m : 1.99332
[1mStep[0m  [189/213], [94mLoss[0m : 1.73410
[1mStep[0m  [210/213], [94mLoss[0m : 1.92276

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.79844
[1mStep[0m  [21/213], [94mLoss[0m : 1.87592
[1mStep[0m  [42/213], [94mLoss[0m : 1.86131
[1mStep[0m  [63/213], [94mLoss[0m : 2.10617
[1mStep[0m  [84/213], [94mLoss[0m : 2.11067
[1mStep[0m  [105/213], [94mLoss[0m : 2.14051
[1mStep[0m  [126/213], [94mLoss[0m : 1.91655
[1mStep[0m  [147/213], [94mLoss[0m : 1.96772
[1mStep[0m  [168/213], [94mLoss[0m : 2.32445
[1mStep[0m  [189/213], [94mLoss[0m : 2.30810
[1mStep[0m  [210/213], [94mLoss[0m : 1.83815

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.972, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.88894
[1mStep[0m  [21/213], [94mLoss[0m : 2.04935
[1mStep[0m  [42/213], [94mLoss[0m : 2.32257
[1mStep[0m  [63/213], [94mLoss[0m : 2.24153
[1mStep[0m  [84/213], [94mLoss[0m : 1.94701
[1mStep[0m  [105/213], [94mLoss[0m : 2.13498
[1mStep[0m  [126/213], [94mLoss[0m : 1.94004
[1mStep[0m  [147/213], [94mLoss[0m : 1.96503
[1mStep[0m  [168/213], [94mLoss[0m : 1.86762
[1mStep[0m  [189/213], [94mLoss[0m : 2.08159
[1mStep[0m  [210/213], [94mLoss[0m : 2.04758

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.931, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.86061
[1mStep[0m  [21/213], [94mLoss[0m : 1.72167
[1mStep[0m  [42/213], [94mLoss[0m : 1.98402
[1mStep[0m  [63/213], [94mLoss[0m : 2.10688
[1mStep[0m  [84/213], [94mLoss[0m : 1.81512
[1mStep[0m  [105/213], [94mLoss[0m : 1.96030
[1mStep[0m  [126/213], [94mLoss[0m : 1.59081
[1mStep[0m  [147/213], [94mLoss[0m : 1.40075
[1mStep[0m  [168/213], [94mLoss[0m : 1.96592
[1mStep[0m  [189/213], [94mLoss[0m : 1.90455
[1mStep[0m  [210/213], [94mLoss[0m : 1.75698

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.910, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80285
[1mStep[0m  [21/213], [94mLoss[0m : 2.04833
[1mStep[0m  [42/213], [94mLoss[0m : 1.89983
[1mStep[0m  [63/213], [94mLoss[0m : 1.84256
[1mStep[0m  [84/213], [94mLoss[0m : 1.74543
[1mStep[0m  [105/213], [94mLoss[0m : 1.92862
[1mStep[0m  [126/213], [94mLoss[0m : 1.82294
[1mStep[0m  [147/213], [94mLoss[0m : 1.57240
[1mStep[0m  [168/213], [94mLoss[0m : 1.93990
[1mStep[0m  [189/213], [94mLoss[0m : 2.00070
[1mStep[0m  [210/213], [94mLoss[0m : 2.23327

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.874, [92mTest[0m: 2.419, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25132
[1mStep[0m  [21/213], [94mLoss[0m : 1.73590
[1mStep[0m  [42/213], [94mLoss[0m : 1.91606
[1mStep[0m  [63/213], [94mLoss[0m : 2.30970
[1mStep[0m  [84/213], [94mLoss[0m : 1.74140
[1mStep[0m  [105/213], [94mLoss[0m : 1.75275
[1mStep[0m  [126/213], [94mLoss[0m : 1.90292
[1mStep[0m  [147/213], [94mLoss[0m : 1.54752
[1mStep[0m  [168/213], [94mLoss[0m : 1.46695
[1mStep[0m  [189/213], [94mLoss[0m : 1.91055
[1mStep[0m  [210/213], [94mLoss[0m : 1.93144

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.844, [92mTest[0m: 2.433, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24591
[1mStep[0m  [21/213], [94mLoss[0m : 1.67350
[1mStep[0m  [42/213], [94mLoss[0m : 1.82026
[1mStep[0m  [63/213], [94mLoss[0m : 1.89766
[1mStep[0m  [84/213], [94mLoss[0m : 1.97124
[1mStep[0m  [105/213], [94mLoss[0m : 1.81946
[1mStep[0m  [126/213], [94mLoss[0m : 1.65722
[1mStep[0m  [147/213], [94mLoss[0m : 1.76682
[1mStep[0m  [168/213], [94mLoss[0m : 1.47425
[1mStep[0m  [189/213], [94mLoss[0m : 1.74465
[1mStep[0m  [210/213], [94mLoss[0m : 1.82427

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.827, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.48218
[1mStep[0m  [21/213], [94mLoss[0m : 1.64046
[1mStep[0m  [42/213], [94mLoss[0m : 1.96100
[1mStep[0m  [63/213], [94mLoss[0m : 1.59105
[1mStep[0m  [84/213], [94mLoss[0m : 1.54716
[1mStep[0m  [105/213], [94mLoss[0m : 1.82295
[1mStep[0m  [126/213], [94mLoss[0m : 2.03431
[1mStep[0m  [147/213], [94mLoss[0m : 2.24863
[1mStep[0m  [168/213], [94mLoss[0m : 1.59053
[1mStep[0m  [189/213], [94mLoss[0m : 1.86673
[1mStep[0m  [210/213], [94mLoss[0m : 1.85477

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.804, [92mTest[0m: 2.427, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.17834
[1mStep[0m  [21/213], [94mLoss[0m : 1.73146
[1mStep[0m  [42/213], [94mLoss[0m : 1.67595
[1mStep[0m  [63/213], [94mLoss[0m : 1.82600
[1mStep[0m  [84/213], [94mLoss[0m : 2.16345
[1mStep[0m  [105/213], [94mLoss[0m : 1.73878
[1mStep[0m  [126/213], [94mLoss[0m : 1.72651
[1mStep[0m  [147/213], [94mLoss[0m : 1.91763
[1mStep[0m  [168/213], [94mLoss[0m : 1.57667
[1mStep[0m  [189/213], [94mLoss[0m : 1.92843
[1mStep[0m  [210/213], [94mLoss[0m : 1.49639

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.417, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.75821
[1mStep[0m  [21/213], [94mLoss[0m : 1.77106
[1mStep[0m  [42/213], [94mLoss[0m : 1.93221
[1mStep[0m  [63/213], [94mLoss[0m : 1.63744
[1mStep[0m  [84/213], [94mLoss[0m : 1.49890
[1mStep[0m  [105/213], [94mLoss[0m : 1.92812
[1mStep[0m  [126/213], [94mLoss[0m : 1.67051
[1mStep[0m  [147/213], [94mLoss[0m : 1.39905
[1mStep[0m  [168/213], [94mLoss[0m : 1.52933
[1mStep[0m  [189/213], [94mLoss[0m : 1.66426
[1mStep[0m  [210/213], [94mLoss[0m : 1.69176

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.748, [92mTest[0m: 2.438, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.23239
[1mStep[0m  [21/213], [94mLoss[0m : 1.47257
[1mStep[0m  [42/213], [94mLoss[0m : 1.91874
[1mStep[0m  [63/213], [94mLoss[0m : 1.62082
[1mStep[0m  [84/213], [94mLoss[0m : 1.53760
[1mStep[0m  [105/213], [94mLoss[0m : 1.74498
[1mStep[0m  [126/213], [94mLoss[0m : 1.54952
[1mStep[0m  [147/213], [94mLoss[0m : 1.67835
[1mStep[0m  [168/213], [94mLoss[0m : 1.64070
[1mStep[0m  [189/213], [94mLoss[0m : 2.00101
[1mStep[0m  [210/213], [94mLoss[0m : 1.76132

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.692, [92mTest[0m: 2.416, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66609
[1mStep[0m  [21/213], [94mLoss[0m : 1.76255
[1mStep[0m  [42/213], [94mLoss[0m : 1.70583
[1mStep[0m  [63/213], [94mLoss[0m : 1.69932
[1mStep[0m  [84/213], [94mLoss[0m : 1.31391
[1mStep[0m  [105/213], [94mLoss[0m : 1.58884
[1mStep[0m  [126/213], [94mLoss[0m : 1.59299
[1mStep[0m  [147/213], [94mLoss[0m : 1.83555
[1mStep[0m  [168/213], [94mLoss[0m : 2.09165
[1mStep[0m  [189/213], [94mLoss[0m : 1.78429
[1mStep[0m  [210/213], [94mLoss[0m : 1.80076

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.456, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.50492
[1mStep[0m  [21/213], [94mLoss[0m : 1.47993
[1mStep[0m  [42/213], [94mLoss[0m : 1.65148
[1mStep[0m  [63/213], [94mLoss[0m : 1.73533
[1mStep[0m  [84/213], [94mLoss[0m : 1.69621
[1mStep[0m  [105/213], [94mLoss[0m : 1.73116
[1mStep[0m  [126/213], [94mLoss[0m : 1.81115
[1mStep[0m  [147/213], [94mLoss[0m : 1.60799
[1mStep[0m  [168/213], [94mLoss[0m : 1.41854
[1mStep[0m  [189/213], [94mLoss[0m : 1.84991
[1mStep[0m  [210/213], [94mLoss[0m : 1.80238

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.678, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.59567
[1mStep[0m  [21/213], [94mLoss[0m : 1.86226
[1mStep[0m  [42/213], [94mLoss[0m : 1.52852
[1mStep[0m  [63/213], [94mLoss[0m : 1.87664
[1mStep[0m  [84/213], [94mLoss[0m : 1.75652
[1mStep[0m  [105/213], [94mLoss[0m : 2.06005
[1mStep[0m  [126/213], [94mLoss[0m : 1.76038
[1mStep[0m  [147/213], [94mLoss[0m : 2.00232
[1mStep[0m  [168/213], [94mLoss[0m : 1.90643
[1mStep[0m  [189/213], [94mLoss[0m : 1.42065
[1mStep[0m  [210/213], [94mLoss[0m : 1.94058

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.478, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74349
[1mStep[0m  [21/213], [94mLoss[0m : 1.39500
[1mStep[0m  [42/213], [94mLoss[0m : 1.47193
[1mStep[0m  [63/213], [94mLoss[0m : 1.68472
[1mStep[0m  [84/213], [94mLoss[0m : 1.41309
[1mStep[0m  [105/213], [94mLoss[0m : 1.72018
[1mStep[0m  [126/213], [94mLoss[0m : 1.51685
[1mStep[0m  [147/213], [94mLoss[0m : 1.66366
[1mStep[0m  [168/213], [94mLoss[0m : 1.54150
[1mStep[0m  [189/213], [94mLoss[0m : 1.95644
[1mStep[0m  [210/213], [94mLoss[0m : 1.86781

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.69864
[1mStep[0m  [21/213], [94mLoss[0m : 1.44313
[1mStep[0m  [42/213], [94mLoss[0m : 1.59343
[1mStep[0m  [63/213], [94mLoss[0m : 1.69338
[1mStep[0m  [84/213], [94mLoss[0m : 1.61800
[1mStep[0m  [105/213], [94mLoss[0m : 1.13330
[1mStep[0m  [126/213], [94mLoss[0m : 1.30554
[1mStep[0m  [147/213], [94mLoss[0m : 1.24288
[1mStep[0m  [168/213], [94mLoss[0m : 1.70389
[1mStep[0m  [189/213], [94mLoss[0m : 1.52387
[1mStep[0m  [210/213], [94mLoss[0m : 1.68654

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.57161
[1mStep[0m  [21/213], [94mLoss[0m : 1.98754
[1mStep[0m  [42/213], [94mLoss[0m : 1.51160
[1mStep[0m  [63/213], [94mLoss[0m : 1.50805
[1mStep[0m  [84/213], [94mLoss[0m : 1.40174
[1mStep[0m  [105/213], [94mLoss[0m : 1.45899
[1mStep[0m  [126/213], [94mLoss[0m : 1.45417
[1mStep[0m  [147/213], [94mLoss[0m : 1.54628
[1mStep[0m  [168/213], [94mLoss[0m : 1.41004
[1mStep[0m  [189/213], [94mLoss[0m : 1.32348
[1mStep[0m  [210/213], [94mLoss[0m : 1.59650

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.489, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63282
[1mStep[0m  [21/213], [94mLoss[0m : 1.75996
[1mStep[0m  [42/213], [94mLoss[0m : 1.52860
[1mStep[0m  [63/213], [94mLoss[0m : 1.59124
[1mStep[0m  [84/213], [94mLoss[0m : 1.80249
[1mStep[0m  [105/213], [94mLoss[0m : 1.74360
[1mStep[0m  [126/213], [94mLoss[0m : 1.49968
[1mStep[0m  [147/213], [94mLoss[0m : 1.49665
[1mStep[0m  [168/213], [94mLoss[0m : 1.56086
[1mStep[0m  [189/213], [94mLoss[0m : 1.52891
[1mStep[0m  [210/213], [94mLoss[0m : 1.57794

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.588, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.473
====================================

Phase 2 - Evaluation MAE:  2.4728329181671143
MAE score P1      2.364898
MAE score P2      2.472833
loss              1.585697
learning_rate     0.002575
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            relu
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 11.06883
[1mStep[0m  [21/213], [94mLoss[0m : 5.36285
[1mStep[0m  [42/213], [94mLoss[0m : 3.10083
[1mStep[0m  [63/213], [94mLoss[0m : 3.13595
[1mStep[0m  [84/213], [94mLoss[0m : 2.77676
[1mStep[0m  [105/213], [94mLoss[0m : 3.08723
[1mStep[0m  [126/213], [94mLoss[0m : 3.36954
[1mStep[0m  [147/213], [94mLoss[0m : 2.69974
[1mStep[0m  [168/213], [94mLoss[0m : 2.62266
[1mStep[0m  [189/213], [94mLoss[0m : 2.91178
[1mStep[0m  [210/213], [94mLoss[0m : 2.50937

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.417, [92mTest[0m: 10.819, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.71286
[1mStep[0m  [21/213], [94mLoss[0m : 2.51524
[1mStep[0m  [42/213], [94mLoss[0m : 2.70003
[1mStep[0m  [63/213], [94mLoss[0m : 2.72363
[1mStep[0m  [84/213], [94mLoss[0m : 2.19580
[1mStep[0m  [105/213], [94mLoss[0m : 2.24892
[1mStep[0m  [126/213], [94mLoss[0m : 2.44600
[1mStep[0m  [147/213], [94mLoss[0m : 2.48641
[1mStep[0m  [168/213], [94mLoss[0m : 2.85772
[1mStep[0m  [189/213], [94mLoss[0m : 2.33312
[1mStep[0m  [210/213], [94mLoss[0m : 2.48903

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.658, [92mTest[0m: 2.485, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30236
[1mStep[0m  [21/213], [94mLoss[0m : 2.91931
[1mStep[0m  [42/213], [94mLoss[0m : 2.77681
[1mStep[0m  [63/213], [94mLoss[0m : 2.26157
[1mStep[0m  [84/213], [94mLoss[0m : 2.58097
[1mStep[0m  [105/213], [94mLoss[0m : 2.24732
[1mStep[0m  [126/213], [94mLoss[0m : 2.55110
[1mStep[0m  [147/213], [94mLoss[0m : 2.43142
[1mStep[0m  [168/213], [94mLoss[0m : 2.75657
[1mStep[0m  [189/213], [94mLoss[0m : 2.86446
[1mStep[0m  [210/213], [94mLoss[0m : 2.45977

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.590, [92mTest[0m: 2.494, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73264
[1mStep[0m  [21/213], [94mLoss[0m : 2.63243
[1mStep[0m  [42/213], [94mLoss[0m : 3.22075
[1mStep[0m  [63/213], [94mLoss[0m : 3.04173
[1mStep[0m  [84/213], [94mLoss[0m : 2.87865
[1mStep[0m  [105/213], [94mLoss[0m : 2.64913
[1mStep[0m  [126/213], [94mLoss[0m : 2.59186
[1mStep[0m  [147/213], [94mLoss[0m : 2.41500
[1mStep[0m  [168/213], [94mLoss[0m : 2.53515
[1mStep[0m  [189/213], [94mLoss[0m : 2.66697
[1mStep[0m  [210/213], [94mLoss[0m : 2.48505

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.59449
[1mStep[0m  [21/213], [94mLoss[0m : 2.76031
[1mStep[0m  [42/213], [94mLoss[0m : 2.51334
[1mStep[0m  [63/213], [94mLoss[0m : 2.76394
[1mStep[0m  [84/213], [94mLoss[0m : 2.75169
[1mStep[0m  [105/213], [94mLoss[0m : 2.81096
[1mStep[0m  [126/213], [94mLoss[0m : 2.24510
[1mStep[0m  [147/213], [94mLoss[0m : 3.18745
[1mStep[0m  [168/213], [94mLoss[0m : 1.96859
[1mStep[0m  [189/213], [94mLoss[0m : 2.50329
[1mStep[0m  [210/213], [94mLoss[0m : 2.46155

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44503
[1mStep[0m  [21/213], [94mLoss[0m : 2.55785
[1mStep[0m  [42/213], [94mLoss[0m : 2.46600
[1mStep[0m  [63/213], [94mLoss[0m : 2.20508
[1mStep[0m  [84/213], [94mLoss[0m : 2.59365
[1mStep[0m  [105/213], [94mLoss[0m : 2.45516
[1mStep[0m  [126/213], [94mLoss[0m : 2.56203
[1mStep[0m  [147/213], [94mLoss[0m : 2.19533
[1mStep[0m  [168/213], [94mLoss[0m : 2.71216
[1mStep[0m  [189/213], [94mLoss[0m : 2.36267
[1mStep[0m  [210/213], [94mLoss[0m : 2.37940

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.409, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.79934
[1mStep[0m  [21/213], [94mLoss[0m : 2.51124
[1mStep[0m  [42/213], [94mLoss[0m : 2.64161
[1mStep[0m  [63/213], [94mLoss[0m : 2.54828
[1mStep[0m  [84/213], [94mLoss[0m : 2.62213
[1mStep[0m  [105/213], [94mLoss[0m : 2.58829
[1mStep[0m  [126/213], [94mLoss[0m : 3.06155
[1mStep[0m  [147/213], [94mLoss[0m : 2.30371
[1mStep[0m  [168/213], [94mLoss[0m : 2.30584
[1mStep[0m  [189/213], [94mLoss[0m : 2.77342
[1mStep[0m  [210/213], [94mLoss[0m : 2.49650

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.34614
[1mStep[0m  [21/213], [94mLoss[0m : 2.56027
[1mStep[0m  [42/213], [94mLoss[0m : 2.38910
[1mStep[0m  [63/213], [94mLoss[0m : 2.79965
[1mStep[0m  [84/213], [94mLoss[0m : 2.45797
[1mStep[0m  [105/213], [94mLoss[0m : 2.62065
[1mStep[0m  [126/213], [94mLoss[0m : 2.48871
[1mStep[0m  [147/213], [94mLoss[0m : 2.27256
[1mStep[0m  [168/213], [94mLoss[0m : 2.07672
[1mStep[0m  [189/213], [94mLoss[0m : 2.46673
[1mStep[0m  [210/213], [94mLoss[0m : 2.88129

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.37515
[1mStep[0m  [21/213], [94mLoss[0m : 2.30037
[1mStep[0m  [42/213], [94mLoss[0m : 2.33620
[1mStep[0m  [63/213], [94mLoss[0m : 2.21546
[1mStep[0m  [84/213], [94mLoss[0m : 2.51537
[1mStep[0m  [105/213], [94mLoss[0m : 2.02289
[1mStep[0m  [126/213], [94mLoss[0m : 2.25094
[1mStep[0m  [147/213], [94mLoss[0m : 2.61493
[1mStep[0m  [168/213], [94mLoss[0m : 2.11357
[1mStep[0m  [189/213], [94mLoss[0m : 2.71838
[1mStep[0m  [210/213], [94mLoss[0m : 2.55373

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.16157
[1mStep[0m  [21/213], [94mLoss[0m : 2.69387
[1mStep[0m  [42/213], [94mLoss[0m : 2.60269
[1mStep[0m  [63/213], [94mLoss[0m : 2.31567
[1mStep[0m  [84/213], [94mLoss[0m : 2.87686
[1mStep[0m  [105/213], [94mLoss[0m : 2.35086
[1mStep[0m  [126/213], [94mLoss[0m : 2.07513
[1mStep[0m  [147/213], [94mLoss[0m : 2.49394
[1mStep[0m  [168/213], [94mLoss[0m : 2.54220
[1mStep[0m  [189/213], [94mLoss[0m : 2.12132
[1mStep[0m  [210/213], [94mLoss[0m : 2.43694

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.377, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31636
[1mStep[0m  [21/213], [94mLoss[0m : 2.38432
[1mStep[0m  [42/213], [94mLoss[0m : 2.22778
[1mStep[0m  [63/213], [94mLoss[0m : 2.08425
[1mStep[0m  [84/213], [94mLoss[0m : 2.28785
[1mStep[0m  [105/213], [94mLoss[0m : 2.57437
[1mStep[0m  [126/213], [94mLoss[0m : 2.87130
[1mStep[0m  [147/213], [94mLoss[0m : 2.13530
[1mStep[0m  [168/213], [94mLoss[0m : 2.39456
[1mStep[0m  [189/213], [94mLoss[0m : 2.35533
[1mStep[0m  [210/213], [94mLoss[0m : 2.45210

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.371, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.20559
[1mStep[0m  [21/213], [94mLoss[0m : 2.43235
[1mStep[0m  [42/213], [94mLoss[0m : 3.01511
[1mStep[0m  [63/213], [94mLoss[0m : 2.48576
[1mStep[0m  [84/213], [94mLoss[0m : 2.03614
[1mStep[0m  [105/213], [94mLoss[0m : 2.26244
[1mStep[0m  [126/213], [94mLoss[0m : 2.45249
[1mStep[0m  [147/213], [94mLoss[0m : 2.57756
[1mStep[0m  [168/213], [94mLoss[0m : 2.55271
[1mStep[0m  [189/213], [94mLoss[0m : 2.46719
[1mStep[0m  [210/213], [94mLoss[0m : 2.69345

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61757
[1mStep[0m  [21/213], [94mLoss[0m : 2.33177
[1mStep[0m  [42/213], [94mLoss[0m : 2.48452
[1mStep[0m  [63/213], [94mLoss[0m : 2.71821
[1mStep[0m  [84/213], [94mLoss[0m : 2.55163
[1mStep[0m  [105/213], [94mLoss[0m : 2.45021
[1mStep[0m  [126/213], [94mLoss[0m : 2.69278
[1mStep[0m  [147/213], [94mLoss[0m : 2.36857
[1mStep[0m  [168/213], [94mLoss[0m : 2.27230
[1mStep[0m  [189/213], [94mLoss[0m : 2.17829
[1mStep[0m  [210/213], [94mLoss[0m : 2.71347

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.364, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30026
[1mStep[0m  [21/213], [94mLoss[0m : 2.15380
[1mStep[0m  [42/213], [94mLoss[0m : 2.48928
[1mStep[0m  [63/213], [94mLoss[0m : 2.38210
[1mStep[0m  [84/213], [94mLoss[0m : 2.38449
[1mStep[0m  [105/213], [94mLoss[0m : 2.41633
[1mStep[0m  [126/213], [94mLoss[0m : 2.17737
[1mStep[0m  [147/213], [94mLoss[0m : 2.26473
[1mStep[0m  [168/213], [94mLoss[0m : 2.59537
[1mStep[0m  [189/213], [94mLoss[0m : 2.44343
[1mStep[0m  [210/213], [94mLoss[0m : 2.49492

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.358, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44172
[1mStep[0m  [21/213], [94mLoss[0m : 2.43294
[1mStep[0m  [42/213], [94mLoss[0m : 2.40508
[1mStep[0m  [63/213], [94mLoss[0m : 2.81140
[1mStep[0m  [84/213], [94mLoss[0m : 2.14690
[1mStep[0m  [105/213], [94mLoss[0m : 2.47285
[1mStep[0m  [126/213], [94mLoss[0m : 2.43378
[1mStep[0m  [147/213], [94mLoss[0m : 2.06423
[1mStep[0m  [168/213], [94mLoss[0m : 2.18343
[1mStep[0m  [189/213], [94mLoss[0m : 2.02385
[1mStep[0m  [210/213], [94mLoss[0m : 2.14259

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.335, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12321
[1mStep[0m  [21/213], [94mLoss[0m : 2.23327
[1mStep[0m  [42/213], [94mLoss[0m : 2.69423
[1mStep[0m  [63/213], [94mLoss[0m : 2.34923
[1mStep[0m  [84/213], [94mLoss[0m : 2.13712
[1mStep[0m  [105/213], [94mLoss[0m : 2.24274
[1mStep[0m  [126/213], [94mLoss[0m : 2.51118
[1mStep[0m  [147/213], [94mLoss[0m : 2.32391
[1mStep[0m  [168/213], [94mLoss[0m : 2.63766
[1mStep[0m  [189/213], [94mLoss[0m : 2.21924
[1mStep[0m  [210/213], [94mLoss[0m : 2.55297

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.355, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65620
[1mStep[0m  [21/213], [94mLoss[0m : 2.23185
[1mStep[0m  [42/213], [94mLoss[0m : 2.54190
[1mStep[0m  [63/213], [94mLoss[0m : 2.23727
[1mStep[0m  [84/213], [94mLoss[0m : 2.90167
[1mStep[0m  [105/213], [94mLoss[0m : 2.15616
[1mStep[0m  [126/213], [94mLoss[0m : 2.48322
[1mStep[0m  [147/213], [94mLoss[0m : 2.27656
[1mStep[0m  [168/213], [94mLoss[0m : 2.49389
[1mStep[0m  [189/213], [94mLoss[0m : 2.30369
[1mStep[0m  [210/213], [94mLoss[0m : 2.71529

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.339, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42475
[1mStep[0m  [21/213], [94mLoss[0m : 2.00531
[1mStep[0m  [42/213], [94mLoss[0m : 2.23180
[1mStep[0m  [63/213], [94mLoss[0m : 2.53925
[1mStep[0m  [84/213], [94mLoss[0m : 2.08128
[1mStep[0m  [105/213], [94mLoss[0m : 2.52405
[1mStep[0m  [126/213], [94mLoss[0m : 2.51561
[1mStep[0m  [147/213], [94mLoss[0m : 2.81090
[1mStep[0m  [168/213], [94mLoss[0m : 2.32084
[1mStep[0m  [189/213], [94mLoss[0m : 2.59506
[1mStep[0m  [210/213], [94mLoss[0m : 2.47467

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.350, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.56213
[1mStep[0m  [21/213], [94mLoss[0m : 2.32683
[1mStep[0m  [42/213], [94mLoss[0m : 2.16351
[1mStep[0m  [63/213], [94mLoss[0m : 2.39127
[1mStep[0m  [84/213], [94mLoss[0m : 2.59910
[1mStep[0m  [105/213], [94mLoss[0m : 2.52693
[1mStep[0m  [126/213], [94mLoss[0m : 2.23759
[1mStep[0m  [147/213], [94mLoss[0m : 2.04949
[1mStep[0m  [168/213], [94mLoss[0m : 2.38331
[1mStep[0m  [189/213], [94mLoss[0m : 1.97590
[1mStep[0m  [210/213], [94mLoss[0m : 1.99161

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.367, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39443
[1mStep[0m  [21/213], [94mLoss[0m : 2.66737
[1mStep[0m  [42/213], [94mLoss[0m : 2.30391
[1mStep[0m  [63/213], [94mLoss[0m : 2.57334
[1mStep[0m  [84/213], [94mLoss[0m : 2.43253
[1mStep[0m  [105/213], [94mLoss[0m : 1.78731
[1mStep[0m  [126/213], [94mLoss[0m : 2.30449
[1mStep[0m  [147/213], [94mLoss[0m : 2.31716
[1mStep[0m  [168/213], [94mLoss[0m : 2.46111
[1mStep[0m  [189/213], [94mLoss[0m : 2.44601
[1mStep[0m  [210/213], [94mLoss[0m : 2.07947

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.367, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.34239
[1mStep[0m  [21/213], [94mLoss[0m : 2.28490
[1mStep[0m  [42/213], [94mLoss[0m : 2.55878
[1mStep[0m  [63/213], [94mLoss[0m : 2.11326
[1mStep[0m  [84/213], [94mLoss[0m : 2.38887
[1mStep[0m  [105/213], [94mLoss[0m : 2.76256
[1mStep[0m  [126/213], [94mLoss[0m : 2.19430
[1mStep[0m  [147/213], [94mLoss[0m : 2.71064
[1mStep[0m  [168/213], [94mLoss[0m : 2.73238
[1mStep[0m  [189/213], [94mLoss[0m : 2.40597
[1mStep[0m  [210/213], [94mLoss[0m : 2.43767

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25428
[1mStep[0m  [21/213], [94mLoss[0m : 2.50765
[1mStep[0m  [42/213], [94mLoss[0m : 2.14805
[1mStep[0m  [63/213], [94mLoss[0m : 2.24363
[1mStep[0m  [84/213], [94mLoss[0m : 2.57087
[1mStep[0m  [105/213], [94mLoss[0m : 2.73525
[1mStep[0m  [126/213], [94mLoss[0m : 2.81960
[1mStep[0m  [147/213], [94mLoss[0m : 2.61338
[1mStep[0m  [168/213], [94mLoss[0m : 2.67163
[1mStep[0m  [189/213], [94mLoss[0m : 2.29700
[1mStep[0m  [210/213], [94mLoss[0m : 2.46343

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.347, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28390
[1mStep[0m  [21/213], [94mLoss[0m : 2.60392
[1mStep[0m  [42/213], [94mLoss[0m : 2.64861
[1mStep[0m  [63/213], [94mLoss[0m : 2.00401
[1mStep[0m  [84/213], [94mLoss[0m : 2.48649
[1mStep[0m  [105/213], [94mLoss[0m : 2.75281
[1mStep[0m  [126/213], [94mLoss[0m : 2.40897
[1mStep[0m  [147/213], [94mLoss[0m : 2.20711
[1mStep[0m  [168/213], [94mLoss[0m : 2.48017
[1mStep[0m  [189/213], [94mLoss[0m : 2.37499
[1mStep[0m  [210/213], [94mLoss[0m : 2.00802

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.362, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92757
[1mStep[0m  [21/213], [94mLoss[0m : 2.34811
[1mStep[0m  [42/213], [94mLoss[0m : 2.40538
[1mStep[0m  [63/213], [94mLoss[0m : 2.63687
[1mStep[0m  [84/213], [94mLoss[0m : 2.31825
[1mStep[0m  [105/213], [94mLoss[0m : 1.92655
[1mStep[0m  [126/213], [94mLoss[0m : 2.60778
[1mStep[0m  [147/213], [94mLoss[0m : 2.27956
[1mStep[0m  [168/213], [94mLoss[0m : 2.09483
[1mStep[0m  [189/213], [94mLoss[0m : 2.76397
[1mStep[0m  [210/213], [94mLoss[0m : 2.46307

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.06506
[1mStep[0m  [21/213], [94mLoss[0m : 2.23447
[1mStep[0m  [42/213], [94mLoss[0m : 2.51734
[1mStep[0m  [63/213], [94mLoss[0m : 2.49018
[1mStep[0m  [84/213], [94mLoss[0m : 2.10823
[1mStep[0m  [105/213], [94mLoss[0m : 2.68553
[1mStep[0m  [126/213], [94mLoss[0m : 2.36919
[1mStep[0m  [147/213], [94mLoss[0m : 2.69247
[1mStep[0m  [168/213], [94mLoss[0m : 2.51857
[1mStep[0m  [189/213], [94mLoss[0m : 2.37740
[1mStep[0m  [210/213], [94mLoss[0m : 2.56670

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.361, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87114
[1mStep[0m  [21/213], [94mLoss[0m : 2.14509
[1mStep[0m  [42/213], [94mLoss[0m : 2.23904
[1mStep[0m  [63/213], [94mLoss[0m : 2.64987
[1mStep[0m  [84/213], [94mLoss[0m : 1.95998
[1mStep[0m  [105/213], [94mLoss[0m : 2.41869
[1mStep[0m  [126/213], [94mLoss[0m : 2.29733
[1mStep[0m  [147/213], [94mLoss[0m : 2.39524
[1mStep[0m  [168/213], [94mLoss[0m : 2.55264
[1mStep[0m  [189/213], [94mLoss[0m : 2.44479
[1mStep[0m  [210/213], [94mLoss[0m : 2.73641

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44314
[1mStep[0m  [21/213], [94mLoss[0m : 2.63466
[1mStep[0m  [42/213], [94mLoss[0m : 2.46992
[1mStep[0m  [63/213], [94mLoss[0m : 2.38242
[1mStep[0m  [84/213], [94mLoss[0m : 2.54203
[1mStep[0m  [105/213], [94mLoss[0m : 2.32490
[1mStep[0m  [126/213], [94mLoss[0m : 2.28308
[1mStep[0m  [147/213], [94mLoss[0m : 2.34292
[1mStep[0m  [168/213], [94mLoss[0m : 2.12510
[1mStep[0m  [189/213], [94mLoss[0m : 2.35856
[1mStep[0m  [210/213], [94mLoss[0m : 2.57933

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.333, [92mTest[0m: 2.337, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.39355
[1mStep[0m  [21/213], [94mLoss[0m : 2.27719
[1mStep[0m  [42/213], [94mLoss[0m : 2.51058
[1mStep[0m  [63/213], [94mLoss[0m : 2.59335
[1mStep[0m  [84/213], [94mLoss[0m : 2.37539
[1mStep[0m  [105/213], [94mLoss[0m : 2.92192
[1mStep[0m  [126/213], [94mLoss[0m : 2.12086
[1mStep[0m  [147/213], [94mLoss[0m : 2.46629
[1mStep[0m  [168/213], [94mLoss[0m : 1.82815
[1mStep[0m  [189/213], [94mLoss[0m : 1.91172
[1mStep[0m  [210/213], [94mLoss[0m : 2.31418

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.326, [92mTest[0m: 2.353, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.76872
[1mStep[0m  [21/213], [94mLoss[0m : 2.51374
[1mStep[0m  [42/213], [94mLoss[0m : 2.03304
[1mStep[0m  [63/213], [94mLoss[0m : 2.11292
[1mStep[0m  [84/213], [94mLoss[0m : 2.16437
[1mStep[0m  [105/213], [94mLoss[0m : 2.02626
[1mStep[0m  [126/213], [94mLoss[0m : 2.63944
[1mStep[0m  [147/213], [94mLoss[0m : 2.21465
[1mStep[0m  [168/213], [94mLoss[0m : 2.27322
[1mStep[0m  [189/213], [94mLoss[0m : 2.73481
[1mStep[0m  [210/213], [94mLoss[0m : 1.97237

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.312, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.24449
[1mStep[0m  [21/213], [94mLoss[0m : 2.40031
[1mStep[0m  [42/213], [94mLoss[0m : 2.26684
[1mStep[0m  [63/213], [94mLoss[0m : 2.45237
[1mStep[0m  [84/213], [94mLoss[0m : 2.22193
[1mStep[0m  [105/213], [94mLoss[0m : 2.22117
[1mStep[0m  [126/213], [94mLoss[0m : 2.57830
[1mStep[0m  [147/213], [94mLoss[0m : 2.15783
[1mStep[0m  [168/213], [94mLoss[0m : 2.75032
[1mStep[0m  [189/213], [94mLoss[0m : 2.34660
[1mStep[0m  [210/213], [94mLoss[0m : 2.08452

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.321, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.318
====================================

Phase 1 - Evaluation MAE:  2.3180770975238874
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 2.42902
[1mStep[0m  [21/213], [94mLoss[0m : 2.27570
[1mStep[0m  [42/213], [94mLoss[0m : 2.22545
[1mStep[0m  [63/213], [94mLoss[0m : 2.53581
[1mStep[0m  [84/213], [94mLoss[0m : 2.60692
[1mStep[0m  [105/213], [94mLoss[0m : 2.41843
[1mStep[0m  [126/213], [94mLoss[0m : 2.23491
[1mStep[0m  [147/213], [94mLoss[0m : 3.00162
[1mStep[0m  [168/213], [94mLoss[0m : 2.22925
[1mStep[0m  [189/213], [94mLoss[0m : 2.79845
[1mStep[0m  [210/213], [94mLoss[0m : 2.81937

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26732
[1mStep[0m  [21/213], [94mLoss[0m : 2.46539
[1mStep[0m  [42/213], [94mLoss[0m : 2.19268
[1mStep[0m  [63/213], [94mLoss[0m : 3.12913
[1mStep[0m  [84/213], [94mLoss[0m : 2.72915
[1mStep[0m  [105/213], [94mLoss[0m : 2.90881
[1mStep[0m  [126/213], [94mLoss[0m : 2.36598
[1mStep[0m  [147/213], [94mLoss[0m : 2.79828
[1mStep[0m  [168/213], [94mLoss[0m : 2.35048
[1mStep[0m  [189/213], [94mLoss[0m : 2.52137
[1mStep[0m  [210/213], [94mLoss[0m : 2.45781

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54323
[1mStep[0m  [21/213], [94mLoss[0m : 2.67874
[1mStep[0m  [42/213], [94mLoss[0m : 2.00185
[1mStep[0m  [63/213], [94mLoss[0m : 2.54686
[1mStep[0m  [84/213], [94mLoss[0m : 2.26471
[1mStep[0m  [105/213], [94mLoss[0m : 2.16246
[1mStep[0m  [126/213], [94mLoss[0m : 2.43164
[1mStep[0m  [147/213], [94mLoss[0m : 2.24994
[1mStep[0m  [168/213], [94mLoss[0m : 1.81859
[1mStep[0m  [189/213], [94mLoss[0m : 2.59071
[1mStep[0m  [210/213], [94mLoss[0m : 2.20421

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.306, [92mTest[0m: 2.498, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25230
[1mStep[0m  [21/213], [94mLoss[0m : 2.18194
[1mStep[0m  [42/213], [94mLoss[0m : 2.37151
[1mStep[0m  [63/213], [94mLoss[0m : 2.47790
[1mStep[0m  [84/213], [94mLoss[0m : 2.27501
[1mStep[0m  [105/213], [94mLoss[0m : 2.32360
[1mStep[0m  [126/213], [94mLoss[0m : 2.00722
[1mStep[0m  [147/213], [94mLoss[0m : 2.35558
[1mStep[0m  [168/213], [94mLoss[0m : 1.95744
[1mStep[0m  [189/213], [94mLoss[0m : 2.28973
[1mStep[0m  [210/213], [94mLoss[0m : 2.13841

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.03273
[1mStep[0m  [21/213], [94mLoss[0m : 1.86245
[1mStep[0m  [42/213], [94mLoss[0m : 1.81919
[1mStep[0m  [63/213], [94mLoss[0m : 2.22159
[1mStep[0m  [84/213], [94mLoss[0m : 2.06135
[1mStep[0m  [105/213], [94mLoss[0m : 2.21565
[1mStep[0m  [126/213], [94mLoss[0m : 2.30269
[1mStep[0m  [147/213], [94mLoss[0m : 2.43123
[1mStep[0m  [168/213], [94mLoss[0m : 1.80238
[1mStep[0m  [189/213], [94mLoss[0m : 2.19883
[1mStep[0m  [210/213], [94mLoss[0m : 1.92563

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.164, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30871
[1mStep[0m  [21/213], [94mLoss[0m : 1.96171
[1mStep[0m  [42/213], [94mLoss[0m : 2.01514
[1mStep[0m  [63/213], [94mLoss[0m : 2.01217
[1mStep[0m  [84/213], [94mLoss[0m : 1.90273
[1mStep[0m  [105/213], [94mLoss[0m : 2.18247
[1mStep[0m  [126/213], [94mLoss[0m : 2.08283
[1mStep[0m  [147/213], [94mLoss[0m : 1.76487
[1mStep[0m  [168/213], [94mLoss[0m : 2.25166
[1mStep[0m  [189/213], [94mLoss[0m : 2.07490
[1mStep[0m  [210/213], [94mLoss[0m : 1.98841

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.385, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.35819
[1mStep[0m  [21/213], [94mLoss[0m : 2.13394
[1mStep[0m  [42/213], [94mLoss[0m : 1.94226
[1mStep[0m  [63/213], [94mLoss[0m : 2.00926
[1mStep[0m  [84/213], [94mLoss[0m : 2.15897
[1mStep[0m  [105/213], [94mLoss[0m : 1.93707
[1mStep[0m  [126/213], [94mLoss[0m : 2.21976
[1mStep[0m  [147/213], [94mLoss[0m : 2.17547
[1mStep[0m  [168/213], [94mLoss[0m : 2.26914
[1mStep[0m  [189/213], [94mLoss[0m : 2.11691
[1mStep[0m  [210/213], [94mLoss[0m : 2.17649

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.057, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13655
[1mStep[0m  [21/213], [94mLoss[0m : 1.93454
[1mStep[0m  [42/213], [94mLoss[0m : 1.97551
[1mStep[0m  [63/213], [94mLoss[0m : 1.82364
[1mStep[0m  [84/213], [94mLoss[0m : 2.14322
[1mStep[0m  [105/213], [94mLoss[0m : 1.81454
[1mStep[0m  [126/213], [94mLoss[0m : 1.69607
[1mStep[0m  [147/213], [94mLoss[0m : 1.95294
[1mStep[0m  [168/213], [94mLoss[0m : 2.25249
[1mStep[0m  [189/213], [94mLoss[0m : 1.87143
[1mStep[0m  [210/213], [94mLoss[0m : 2.09581

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14192
[1mStep[0m  [21/213], [94mLoss[0m : 2.22054
[1mStep[0m  [42/213], [94mLoss[0m : 1.86489
[1mStep[0m  [63/213], [94mLoss[0m : 1.69039
[1mStep[0m  [84/213], [94mLoss[0m : 2.06480
[1mStep[0m  [105/213], [94mLoss[0m : 2.03242
[1mStep[0m  [126/213], [94mLoss[0m : 2.16679
[1mStep[0m  [147/213], [94mLoss[0m : 1.98946
[1mStep[0m  [168/213], [94mLoss[0m : 1.50436
[1mStep[0m  [189/213], [94mLoss[0m : 1.89384
[1mStep[0m  [210/213], [94mLoss[0m : 1.72187

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.955, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63652
[1mStep[0m  [21/213], [94mLoss[0m : 2.07510
[1mStep[0m  [42/213], [94mLoss[0m : 2.02137
[1mStep[0m  [63/213], [94mLoss[0m : 1.99483
[1mStep[0m  [84/213], [94mLoss[0m : 1.96937
[1mStep[0m  [105/213], [94mLoss[0m : 1.73964
[1mStep[0m  [126/213], [94mLoss[0m : 1.93413
[1mStep[0m  [147/213], [94mLoss[0m : 2.37029
[1mStep[0m  [168/213], [94mLoss[0m : 1.91594
[1mStep[0m  [189/213], [94mLoss[0m : 2.01632
[1mStep[0m  [210/213], [94mLoss[0m : 1.98579

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.907, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.10179
[1mStep[0m  [21/213], [94mLoss[0m : 2.03645
[1mStep[0m  [42/213], [94mLoss[0m : 1.71202
[1mStep[0m  [63/213], [94mLoss[0m : 1.95779
[1mStep[0m  [84/213], [94mLoss[0m : 1.91495
[1mStep[0m  [105/213], [94mLoss[0m : 1.83948
[1mStep[0m  [126/213], [94mLoss[0m : 1.98548
[1mStep[0m  [147/213], [94mLoss[0m : 1.69134
[1mStep[0m  [168/213], [94mLoss[0m : 1.98701
[1mStep[0m  [189/213], [94mLoss[0m : 1.65625
[1mStep[0m  [210/213], [94mLoss[0m : 2.07380

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.472, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.62048
[1mStep[0m  [21/213], [94mLoss[0m : 1.99755
[1mStep[0m  [42/213], [94mLoss[0m : 1.56840
[1mStep[0m  [63/213], [94mLoss[0m : 1.45321
[1mStep[0m  [84/213], [94mLoss[0m : 1.71450
[1mStep[0m  [105/213], [94mLoss[0m : 2.01722
[1mStep[0m  [126/213], [94mLoss[0m : 1.74933
[1mStep[0m  [147/213], [94mLoss[0m : 1.69797
[1mStep[0m  [168/213], [94mLoss[0m : 1.82266
[1mStep[0m  [189/213], [94mLoss[0m : 1.78532
[1mStep[0m  [210/213], [94mLoss[0m : 1.95531

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.851, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.51937
[1mStep[0m  [21/213], [94mLoss[0m : 1.74315
[1mStep[0m  [42/213], [94mLoss[0m : 1.79861
[1mStep[0m  [63/213], [94mLoss[0m : 1.89027
[1mStep[0m  [84/213], [94mLoss[0m : 1.74582
[1mStep[0m  [105/213], [94mLoss[0m : 1.92411
[1mStep[0m  [126/213], [94mLoss[0m : 1.53642
[1mStep[0m  [147/213], [94mLoss[0m : 1.93391
[1mStep[0m  [168/213], [94mLoss[0m : 1.77888
[1mStep[0m  [189/213], [94mLoss[0m : 2.01555
[1mStep[0m  [210/213], [94mLoss[0m : 1.94562

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.431, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.63449
[1mStep[0m  [21/213], [94mLoss[0m : 1.87736
[1mStep[0m  [42/213], [94mLoss[0m : 2.18919
[1mStep[0m  [63/213], [94mLoss[0m : 1.59448
[1mStep[0m  [84/213], [94mLoss[0m : 1.50991
[1mStep[0m  [105/213], [94mLoss[0m : 2.16218
[1mStep[0m  [126/213], [94mLoss[0m : 1.83444
[1mStep[0m  [147/213], [94mLoss[0m : 2.02588
[1mStep[0m  [168/213], [94mLoss[0m : 1.48283
[1mStep[0m  [189/213], [94mLoss[0m : 1.88350
[1mStep[0m  [210/213], [94mLoss[0m : 2.05612

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.764, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.62029
[1mStep[0m  [21/213], [94mLoss[0m : 1.51320
[1mStep[0m  [42/213], [94mLoss[0m : 1.61831
[1mStep[0m  [63/213], [94mLoss[0m : 1.68705
[1mStep[0m  [84/213], [94mLoss[0m : 1.78733
[1mStep[0m  [105/213], [94mLoss[0m : 1.64530
[1mStep[0m  [126/213], [94mLoss[0m : 1.74134
[1mStep[0m  [147/213], [94mLoss[0m : 1.82226
[1mStep[0m  [168/213], [94mLoss[0m : 2.15977
[1mStep[0m  [189/213], [94mLoss[0m : 1.29360
[1mStep[0m  [210/213], [94mLoss[0m : 1.84762

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47740
[1mStep[0m  [21/213], [94mLoss[0m : 1.73569
[1mStep[0m  [42/213], [94mLoss[0m : 1.70760
[1mStep[0m  [63/213], [94mLoss[0m : 2.11275
[1mStep[0m  [84/213], [94mLoss[0m : 1.71209
[1mStep[0m  [105/213], [94mLoss[0m : 1.78708
[1mStep[0m  [126/213], [94mLoss[0m : 1.86925
[1mStep[0m  [147/213], [94mLoss[0m : 2.03004
[1mStep[0m  [168/213], [94mLoss[0m : 1.97438
[1mStep[0m  [189/213], [94mLoss[0m : 1.78810
[1mStep[0m  [210/213], [94mLoss[0m : 1.91583

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.455, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66217
[1mStep[0m  [21/213], [94mLoss[0m : 1.82188
[1mStep[0m  [42/213], [94mLoss[0m : 1.77805
[1mStep[0m  [63/213], [94mLoss[0m : 1.44254
[1mStep[0m  [84/213], [94mLoss[0m : 1.59925
[1mStep[0m  [105/213], [94mLoss[0m : 1.55385
[1mStep[0m  [126/213], [94mLoss[0m : 1.99880
[1mStep[0m  [147/213], [94mLoss[0m : 1.61046
[1mStep[0m  [168/213], [94mLoss[0m : 1.62438
[1mStep[0m  [189/213], [94mLoss[0m : 1.78343
[1mStep[0m  [210/213], [94mLoss[0m : 1.46969

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.487, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.57844
[1mStep[0m  [21/213], [94mLoss[0m : 1.89017
[1mStep[0m  [42/213], [94mLoss[0m : 1.42885
[1mStep[0m  [63/213], [94mLoss[0m : 1.43081
[1mStep[0m  [84/213], [94mLoss[0m : 1.39854
[1mStep[0m  [105/213], [94mLoss[0m : 1.74648
[1mStep[0m  [126/213], [94mLoss[0m : 1.49980
[1mStep[0m  [147/213], [94mLoss[0m : 1.52969
[1mStep[0m  [168/213], [94mLoss[0m : 1.53937
[1mStep[0m  [189/213], [94mLoss[0m : 1.84068
[1mStep[0m  [210/213], [94mLoss[0m : 1.54941

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.637, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.64161
[1mStep[0m  [21/213], [94mLoss[0m : 1.61537
[1mStep[0m  [42/213], [94mLoss[0m : 1.31260
[1mStep[0m  [63/213], [94mLoss[0m : 1.51966
[1mStep[0m  [84/213], [94mLoss[0m : 1.90335
[1mStep[0m  [105/213], [94mLoss[0m : 1.91357
[1mStep[0m  [126/213], [94mLoss[0m : 1.42775
[1mStep[0m  [147/213], [94mLoss[0m : 1.55496
[1mStep[0m  [168/213], [94mLoss[0m : 1.79740
[1mStep[0m  [189/213], [94mLoss[0m : 1.67579
[1mStep[0m  [210/213], [94mLoss[0m : 1.70232

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.597, [92mTest[0m: 2.489, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.74663
[1mStep[0m  [21/213], [94mLoss[0m : 1.65866
[1mStep[0m  [42/213], [94mLoss[0m : 1.46477
[1mStep[0m  [63/213], [94mLoss[0m : 1.27901
[1mStep[0m  [84/213], [94mLoss[0m : 1.52523
[1mStep[0m  [105/213], [94mLoss[0m : 1.73885
[1mStep[0m  [126/213], [94mLoss[0m : 1.41484
[1mStep[0m  [147/213], [94mLoss[0m : 1.49618
[1mStep[0m  [168/213], [94mLoss[0m : 1.55468
[1mStep[0m  [189/213], [94mLoss[0m : 1.61116
[1mStep[0m  [210/213], [94mLoss[0m : 1.58845

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.569, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.58535
[1mStep[0m  [21/213], [94mLoss[0m : 1.49837
[1mStep[0m  [42/213], [94mLoss[0m : 1.57573
[1mStep[0m  [63/213], [94mLoss[0m : 1.79668
[1mStep[0m  [84/213], [94mLoss[0m : 1.37293
[1mStep[0m  [105/213], [94mLoss[0m : 1.61753
[1mStep[0m  [126/213], [94mLoss[0m : 1.48550
[1mStep[0m  [147/213], [94mLoss[0m : 1.79107
[1mStep[0m  [168/213], [94mLoss[0m : 1.67712
[1mStep[0m  [189/213], [94mLoss[0m : 1.30079
[1mStep[0m  [210/213], [94mLoss[0m : 1.28968

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.526, [92mTest[0m: 2.495, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.36921
[1mStep[0m  [21/213], [94mLoss[0m : 1.49234
[1mStep[0m  [42/213], [94mLoss[0m : 1.34727
[1mStep[0m  [63/213], [94mLoss[0m : 1.41835
[1mStep[0m  [84/213], [94mLoss[0m : 1.15872
[1mStep[0m  [105/213], [94mLoss[0m : 1.52384
[1mStep[0m  [126/213], [94mLoss[0m : 1.24115
[1mStep[0m  [147/213], [94mLoss[0m : 1.65523
[1mStep[0m  [168/213], [94mLoss[0m : 1.70921
[1mStep[0m  [189/213], [94mLoss[0m : 1.60641
[1mStep[0m  [210/213], [94mLoss[0m : 1.46764

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.506, [92mTest[0m: 2.560, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.65030
[1mStep[0m  [21/213], [94mLoss[0m : 1.51782
[1mStep[0m  [42/213], [94mLoss[0m : 1.66763
[1mStep[0m  [63/213], [94mLoss[0m : 1.46596
[1mStep[0m  [84/213], [94mLoss[0m : 1.58723
[1mStep[0m  [105/213], [94mLoss[0m : 1.53757
[1mStep[0m  [126/213], [94mLoss[0m : 1.77919
[1mStep[0m  [147/213], [94mLoss[0m : 1.48281
[1mStep[0m  [168/213], [94mLoss[0m : 1.29134
[1mStep[0m  [189/213], [94mLoss[0m : 1.58242
[1mStep[0m  [210/213], [94mLoss[0m : 1.68549

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.476, [92mTest[0m: 2.454, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.33523
[1mStep[0m  [21/213], [94mLoss[0m : 1.40958
[1mStep[0m  [42/213], [94mLoss[0m : 1.28683
[1mStep[0m  [63/213], [94mLoss[0m : 1.52051
[1mStep[0m  [84/213], [94mLoss[0m : 1.27961
[1mStep[0m  [105/213], [94mLoss[0m : 1.14031
[1mStep[0m  [126/213], [94mLoss[0m : 1.36186
[1mStep[0m  [147/213], [94mLoss[0m : 1.27889
[1mStep[0m  [168/213], [94mLoss[0m : 1.22964
[1mStep[0m  [189/213], [94mLoss[0m : 1.36272
[1mStep[0m  [210/213], [94mLoss[0m : 1.25442

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.445, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.33510
[1mStep[0m  [21/213], [94mLoss[0m : 1.24724
[1mStep[0m  [42/213], [94mLoss[0m : 1.45265
[1mStep[0m  [63/213], [94mLoss[0m : 1.55125
[1mStep[0m  [84/213], [94mLoss[0m : 1.61454
[1mStep[0m  [105/213], [94mLoss[0m : 1.26956
[1mStep[0m  [126/213], [94mLoss[0m : 1.38202
[1mStep[0m  [147/213], [94mLoss[0m : 1.27047
[1mStep[0m  [168/213], [94mLoss[0m : 1.57820
[1mStep[0m  [189/213], [94mLoss[0m : 1.47482
[1mStep[0m  [210/213], [94mLoss[0m : 1.26831

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.435, [92mTest[0m: 2.496, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47581
[1mStep[0m  [21/213], [94mLoss[0m : 1.20627
[1mStep[0m  [42/213], [94mLoss[0m : 1.41495
[1mStep[0m  [63/213], [94mLoss[0m : 1.35564
[1mStep[0m  [84/213], [94mLoss[0m : 1.39552
[1mStep[0m  [105/213], [94mLoss[0m : 1.40857
[1mStep[0m  [126/213], [94mLoss[0m : 1.68500
[1mStep[0m  [147/213], [94mLoss[0m : 1.26194
[1mStep[0m  [168/213], [94mLoss[0m : 1.26795
[1mStep[0m  [189/213], [94mLoss[0m : 1.39907
[1mStep[0m  [210/213], [94mLoss[0m : 1.70651

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.422, [92mTest[0m: 2.467, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.34870
[1mStep[0m  [21/213], [94mLoss[0m : 1.38901
[1mStep[0m  [42/213], [94mLoss[0m : 1.28549
[1mStep[0m  [63/213], [94mLoss[0m : 1.48234
[1mStep[0m  [84/213], [94mLoss[0m : 1.38362
[1mStep[0m  [105/213], [94mLoss[0m : 1.56747
[1mStep[0m  [126/213], [94mLoss[0m : 1.62387
[1mStep[0m  [147/213], [94mLoss[0m : 1.82809
[1mStep[0m  [168/213], [94mLoss[0m : 1.35418
[1mStep[0m  [189/213], [94mLoss[0m : 1.52845
[1mStep[0m  [210/213], [94mLoss[0m : 1.15890

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.403, [92mTest[0m: 2.472, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.442
====================================

Phase 2 - Evaluation MAE:  2.441982274910189
MAE score P1        2.318077
MAE score P2        2.441982
loss                1.402564
learning_rate       0.002575
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping          True
dropout                  0.3
momentum                 0.9
weight_decay          0.0001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 11.72821
[1mStep[0m  [10/106], [94mLoss[0m : 9.61604
[1mStep[0m  [20/106], [94mLoss[0m : 6.25582
[1mStep[0m  [30/106], [94mLoss[0m : 3.36586
[1mStep[0m  [40/106], [94mLoss[0m : 2.76722
[1mStep[0m  [50/106], [94mLoss[0m : 2.86101
[1mStep[0m  [60/106], [94mLoss[0m : 2.85225
[1mStep[0m  [70/106], [94mLoss[0m : 2.83610
[1mStep[0m  [80/106], [94mLoss[0m : 2.94283
[1mStep[0m  [90/106], [94mLoss[0m : 2.74833
[1mStep[0m  [100/106], [94mLoss[0m : 2.90255

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.320, [92mTest[0m: 11.001, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.71652
[1mStep[0m  [10/106], [94mLoss[0m : 2.78427
[1mStep[0m  [20/106], [94mLoss[0m : 2.58827
[1mStep[0m  [30/106], [94mLoss[0m : 2.68707
[1mStep[0m  [40/106], [94mLoss[0m : 2.70504
[1mStep[0m  [50/106], [94mLoss[0m : 2.92131
[1mStep[0m  [60/106], [94mLoss[0m : 2.97039
[1mStep[0m  [70/106], [94mLoss[0m : 2.96833
[1mStep[0m  [80/106], [94mLoss[0m : 2.57000
[1mStep[0m  [90/106], [94mLoss[0m : 2.73182
[1mStep[0m  [100/106], [94mLoss[0m : 2.80584

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.756, [92mTest[0m: 3.105, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76610
[1mStep[0m  [10/106], [94mLoss[0m : 2.73374
[1mStep[0m  [20/106], [94mLoss[0m : 2.42405
[1mStep[0m  [30/106], [94mLoss[0m : 2.86511
[1mStep[0m  [40/106], [94mLoss[0m : 2.79724
[1mStep[0m  [50/106], [94mLoss[0m : 2.84646
[1mStep[0m  [60/106], [94mLoss[0m : 2.27521
[1mStep[0m  [70/106], [94mLoss[0m : 3.06758
[1mStep[0m  [80/106], [94mLoss[0m : 2.65573
[1mStep[0m  [90/106], [94mLoss[0m : 2.99193
[1mStep[0m  [100/106], [94mLoss[0m : 3.07973

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.689, [92mTest[0m: 2.649, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48935
[1mStep[0m  [10/106], [94mLoss[0m : 2.46983
[1mStep[0m  [20/106], [94mLoss[0m : 2.50015
[1mStep[0m  [30/106], [94mLoss[0m : 2.57849
[1mStep[0m  [40/106], [94mLoss[0m : 2.92024
[1mStep[0m  [50/106], [94mLoss[0m : 2.41473
[1mStep[0m  [60/106], [94mLoss[0m : 2.88121
[1mStep[0m  [70/106], [94mLoss[0m : 2.36817
[1mStep[0m  [80/106], [94mLoss[0m : 2.43725
[1mStep[0m  [90/106], [94mLoss[0m : 2.72919
[1mStep[0m  [100/106], [94mLoss[0m : 2.81888

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.672, [92mTest[0m: 2.616, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72983
[1mStep[0m  [10/106], [94mLoss[0m : 2.67853
[1mStep[0m  [20/106], [94mLoss[0m : 2.29562
[1mStep[0m  [30/106], [94mLoss[0m : 2.84118
[1mStep[0m  [40/106], [94mLoss[0m : 2.73804
[1mStep[0m  [50/106], [94mLoss[0m : 2.91515
[1mStep[0m  [60/106], [94mLoss[0m : 2.83036
[1mStep[0m  [70/106], [94mLoss[0m : 2.58440
[1mStep[0m  [80/106], [94mLoss[0m : 2.52974
[1mStep[0m  [90/106], [94mLoss[0m : 2.51050
[1mStep[0m  [100/106], [94mLoss[0m : 2.63319

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45902
[1mStep[0m  [10/106], [94mLoss[0m : 2.36330
[1mStep[0m  [20/106], [94mLoss[0m : 2.45678
[1mStep[0m  [30/106], [94mLoss[0m : 2.41600
[1mStep[0m  [40/106], [94mLoss[0m : 2.99519
[1mStep[0m  [50/106], [94mLoss[0m : 2.84609
[1mStep[0m  [60/106], [94mLoss[0m : 2.78203
[1mStep[0m  [70/106], [94mLoss[0m : 2.59435
[1mStep[0m  [80/106], [94mLoss[0m : 2.47005
[1mStep[0m  [90/106], [94mLoss[0m : 2.79852
[1mStep[0m  [100/106], [94mLoss[0m : 2.34115

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.624, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50149
[1mStep[0m  [10/106], [94mLoss[0m : 2.64356
[1mStep[0m  [20/106], [94mLoss[0m : 2.67146
[1mStep[0m  [30/106], [94mLoss[0m : 2.68855
[1mStep[0m  [40/106], [94mLoss[0m : 2.52785
[1mStep[0m  [50/106], [94mLoss[0m : 2.46352
[1mStep[0m  [60/106], [94mLoss[0m : 2.46280
[1mStep[0m  [70/106], [94mLoss[0m : 2.82115
[1mStep[0m  [80/106], [94mLoss[0m : 2.57330
[1mStep[0m  [90/106], [94mLoss[0m : 2.77909
[1mStep[0m  [100/106], [94mLoss[0m : 2.66754

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.617, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.01668
[1mStep[0m  [10/106], [94mLoss[0m : 2.29267
[1mStep[0m  [20/106], [94mLoss[0m : 2.11557
[1mStep[0m  [30/106], [94mLoss[0m : 2.54742
[1mStep[0m  [40/106], [94mLoss[0m : 2.54027
[1mStep[0m  [50/106], [94mLoss[0m : 2.24224
[1mStep[0m  [60/106], [94mLoss[0m : 2.75231
[1mStep[0m  [70/106], [94mLoss[0m : 2.69265
[1mStep[0m  [80/106], [94mLoss[0m : 2.24728
[1mStep[0m  [90/106], [94mLoss[0m : 2.66598
[1mStep[0m  [100/106], [94mLoss[0m : 2.63852

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.563, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68878
[1mStep[0m  [10/106], [94mLoss[0m : 2.69435
[1mStep[0m  [20/106], [94mLoss[0m : 2.73560
[1mStep[0m  [30/106], [94mLoss[0m : 2.62139
[1mStep[0m  [40/106], [94mLoss[0m : 2.49362
[1mStep[0m  [50/106], [94mLoss[0m : 2.44696
[1mStep[0m  [60/106], [94mLoss[0m : 2.73723
[1mStep[0m  [70/106], [94mLoss[0m : 2.73829
[1mStep[0m  [80/106], [94mLoss[0m : 2.55111
[1mStep[0m  [90/106], [94mLoss[0m : 2.61191
[1mStep[0m  [100/106], [94mLoss[0m : 2.33720

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49701
[1mStep[0m  [10/106], [94mLoss[0m : 2.64914
[1mStep[0m  [20/106], [94mLoss[0m : 2.46248
[1mStep[0m  [30/106], [94mLoss[0m : 2.98164
[1mStep[0m  [40/106], [94mLoss[0m : 2.55956
[1mStep[0m  [50/106], [94mLoss[0m : 2.76367
[1mStep[0m  [60/106], [94mLoss[0m : 2.97698
[1mStep[0m  [70/106], [94mLoss[0m : 2.63681
[1mStep[0m  [80/106], [94mLoss[0m : 2.03947
[1mStep[0m  [90/106], [94mLoss[0m : 2.68509
[1mStep[0m  [100/106], [94mLoss[0m : 2.58873

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58532
[1mStep[0m  [10/106], [94mLoss[0m : 2.62614
[1mStep[0m  [20/106], [94mLoss[0m : 2.19228
[1mStep[0m  [30/106], [94mLoss[0m : 2.48797
[1mStep[0m  [40/106], [94mLoss[0m : 2.45936
[1mStep[0m  [50/106], [94mLoss[0m : 2.06286
[1mStep[0m  [60/106], [94mLoss[0m : 2.41151
[1mStep[0m  [70/106], [94mLoss[0m : 2.70277
[1mStep[0m  [80/106], [94mLoss[0m : 2.51315
[1mStep[0m  [90/106], [94mLoss[0m : 2.23478
[1mStep[0m  [100/106], [94mLoss[0m : 2.33558

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53929
[1mStep[0m  [10/106], [94mLoss[0m : 2.51711
[1mStep[0m  [20/106], [94mLoss[0m : 2.92376
[1mStep[0m  [30/106], [94mLoss[0m : 2.56027
[1mStep[0m  [40/106], [94mLoss[0m : 2.25860
[1mStep[0m  [50/106], [94mLoss[0m : 2.73965
[1mStep[0m  [60/106], [94mLoss[0m : 2.33576
[1mStep[0m  [70/106], [94mLoss[0m : 2.51348
[1mStep[0m  [80/106], [94mLoss[0m : 2.53849
[1mStep[0m  [90/106], [94mLoss[0m : 2.74178
[1mStep[0m  [100/106], [94mLoss[0m : 2.55471

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67503
[1mStep[0m  [10/106], [94mLoss[0m : 2.58874
[1mStep[0m  [20/106], [94mLoss[0m : 2.47914
[1mStep[0m  [30/106], [94mLoss[0m : 2.63012
[1mStep[0m  [40/106], [94mLoss[0m : 2.54808
[1mStep[0m  [50/106], [94mLoss[0m : 2.58578
[1mStep[0m  [60/106], [94mLoss[0m : 2.53281
[1mStep[0m  [70/106], [94mLoss[0m : 2.14635
[1mStep[0m  [80/106], [94mLoss[0m : 2.70885
[1mStep[0m  [90/106], [94mLoss[0m : 2.35171
[1mStep[0m  [100/106], [94mLoss[0m : 2.36333

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.357, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40838
[1mStep[0m  [10/106], [94mLoss[0m : 2.52990
[1mStep[0m  [20/106], [94mLoss[0m : 2.61028
[1mStep[0m  [30/106], [94mLoss[0m : 2.55514
[1mStep[0m  [40/106], [94mLoss[0m : 2.77441
[1mStep[0m  [50/106], [94mLoss[0m : 2.41745
[1mStep[0m  [60/106], [94mLoss[0m : 2.36966
[1mStep[0m  [70/106], [94mLoss[0m : 2.49896
[1mStep[0m  [80/106], [94mLoss[0m : 2.08535
[1mStep[0m  [90/106], [94mLoss[0m : 2.29530
[1mStep[0m  [100/106], [94mLoss[0m : 2.61847

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.32072
[1mStep[0m  [10/106], [94mLoss[0m : 2.47217
[1mStep[0m  [20/106], [94mLoss[0m : 2.28849
[1mStep[0m  [30/106], [94mLoss[0m : 2.66572
[1mStep[0m  [40/106], [94mLoss[0m : 2.58632
[1mStep[0m  [50/106], [94mLoss[0m : 2.41981
[1mStep[0m  [60/106], [94mLoss[0m : 2.14293
[1mStep[0m  [70/106], [94mLoss[0m : 2.33056
[1mStep[0m  [80/106], [94mLoss[0m : 2.24683
[1mStep[0m  [90/106], [94mLoss[0m : 2.63179
[1mStep[0m  [100/106], [94mLoss[0m : 2.02340

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.369, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25667
[1mStep[0m  [10/106], [94mLoss[0m : 2.41405
[1mStep[0m  [20/106], [94mLoss[0m : 2.50398
[1mStep[0m  [30/106], [94mLoss[0m : 2.31068
[1mStep[0m  [40/106], [94mLoss[0m : 2.31127
[1mStep[0m  [50/106], [94mLoss[0m : 2.79742
[1mStep[0m  [60/106], [94mLoss[0m : 2.16851
[1mStep[0m  [70/106], [94mLoss[0m : 2.29865
[1mStep[0m  [80/106], [94mLoss[0m : 2.35413
[1mStep[0m  [90/106], [94mLoss[0m : 2.68253
[1mStep[0m  [100/106], [94mLoss[0m : 2.29723

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61405
[1mStep[0m  [10/106], [94mLoss[0m : 2.24096
[1mStep[0m  [20/106], [94mLoss[0m : 2.56785
[1mStep[0m  [30/106], [94mLoss[0m : 2.16422
[1mStep[0m  [40/106], [94mLoss[0m : 2.43021
[1mStep[0m  [50/106], [94mLoss[0m : 2.49375
[1mStep[0m  [60/106], [94mLoss[0m : 2.62918
[1mStep[0m  [70/106], [94mLoss[0m : 2.66429
[1mStep[0m  [80/106], [94mLoss[0m : 2.35280
[1mStep[0m  [90/106], [94mLoss[0m : 2.43708
[1mStep[0m  [100/106], [94mLoss[0m : 2.50993

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53552
[1mStep[0m  [10/106], [94mLoss[0m : 2.26969
[1mStep[0m  [20/106], [94mLoss[0m : 2.44113
[1mStep[0m  [30/106], [94mLoss[0m : 2.45450
[1mStep[0m  [40/106], [94mLoss[0m : 2.45518
[1mStep[0m  [50/106], [94mLoss[0m : 2.76915
[1mStep[0m  [60/106], [94mLoss[0m : 2.20584
[1mStep[0m  [70/106], [94mLoss[0m : 2.32786
[1mStep[0m  [80/106], [94mLoss[0m : 2.19064
[1mStep[0m  [90/106], [94mLoss[0m : 2.40074
[1mStep[0m  [100/106], [94mLoss[0m : 2.59627

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51433
[1mStep[0m  [10/106], [94mLoss[0m : 2.20590
[1mStep[0m  [20/106], [94mLoss[0m : 2.67549
[1mStep[0m  [30/106], [94mLoss[0m : 2.30069
[1mStep[0m  [40/106], [94mLoss[0m : 2.28353
[1mStep[0m  [50/106], [94mLoss[0m : 2.27395
[1mStep[0m  [60/106], [94mLoss[0m : 2.62128
[1mStep[0m  [70/106], [94mLoss[0m : 2.47766
[1mStep[0m  [80/106], [94mLoss[0m : 2.31356
[1mStep[0m  [90/106], [94mLoss[0m : 2.36539
[1mStep[0m  [100/106], [94mLoss[0m : 2.38343

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.337, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45321
[1mStep[0m  [10/106], [94mLoss[0m : 2.45329
[1mStep[0m  [20/106], [94mLoss[0m : 2.62246
[1mStep[0m  [30/106], [94mLoss[0m : 2.58577
[1mStep[0m  [40/106], [94mLoss[0m : 2.26050
[1mStep[0m  [50/106], [94mLoss[0m : 2.38303
[1mStep[0m  [60/106], [94mLoss[0m : 2.60327
[1mStep[0m  [70/106], [94mLoss[0m : 2.31088
[1mStep[0m  [80/106], [94mLoss[0m : 2.39121
[1mStep[0m  [90/106], [94mLoss[0m : 2.58055
[1mStep[0m  [100/106], [94mLoss[0m : 2.28440

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49046
[1mStep[0m  [10/106], [94mLoss[0m : 2.61737
[1mStep[0m  [20/106], [94mLoss[0m : 2.38456
[1mStep[0m  [30/106], [94mLoss[0m : 2.11650
[1mStep[0m  [40/106], [94mLoss[0m : 2.21401
[1mStep[0m  [50/106], [94mLoss[0m : 2.47866
[1mStep[0m  [60/106], [94mLoss[0m : 2.68266
[1mStep[0m  [70/106], [94mLoss[0m : 2.51447
[1mStep[0m  [80/106], [94mLoss[0m : 2.51404
[1mStep[0m  [90/106], [94mLoss[0m : 2.31211
[1mStep[0m  [100/106], [94mLoss[0m : 2.74401

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.378, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30768
[1mStep[0m  [10/106], [94mLoss[0m : 2.25556
[1mStep[0m  [20/106], [94mLoss[0m : 2.40471
[1mStep[0m  [30/106], [94mLoss[0m : 2.34945
[1mStep[0m  [40/106], [94mLoss[0m : 2.37161
[1mStep[0m  [50/106], [94mLoss[0m : 2.28471
[1mStep[0m  [60/106], [94mLoss[0m : 2.03056
[1mStep[0m  [70/106], [94mLoss[0m : 2.35401
[1mStep[0m  [80/106], [94mLoss[0m : 2.52617
[1mStep[0m  [90/106], [94mLoss[0m : 2.52997
[1mStep[0m  [100/106], [94mLoss[0m : 2.56866

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.355, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16655
[1mStep[0m  [10/106], [94mLoss[0m : 2.26679
[1mStep[0m  [20/106], [94mLoss[0m : 2.55440
[1mStep[0m  [30/106], [94mLoss[0m : 2.41363
[1mStep[0m  [40/106], [94mLoss[0m : 2.35626
[1mStep[0m  [50/106], [94mLoss[0m : 2.69928
[1mStep[0m  [60/106], [94mLoss[0m : 2.71826
[1mStep[0m  [70/106], [94mLoss[0m : 2.73897
[1mStep[0m  [80/106], [94mLoss[0m : 2.60733
[1mStep[0m  [90/106], [94mLoss[0m : 2.43212
[1mStep[0m  [100/106], [94mLoss[0m : 2.36703

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.384, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51643
[1mStep[0m  [10/106], [94mLoss[0m : 2.15108
[1mStep[0m  [20/106], [94mLoss[0m : 2.25361
[1mStep[0m  [30/106], [94mLoss[0m : 2.31788
[1mStep[0m  [40/106], [94mLoss[0m : 2.40717
[1mStep[0m  [50/106], [94mLoss[0m : 2.60981
[1mStep[0m  [60/106], [94mLoss[0m : 2.36580
[1mStep[0m  [70/106], [94mLoss[0m : 2.58098
[1mStep[0m  [80/106], [94mLoss[0m : 2.42682
[1mStep[0m  [90/106], [94mLoss[0m : 2.44300
[1mStep[0m  [100/106], [94mLoss[0m : 2.41640

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.35846
[1mStep[0m  [10/106], [94mLoss[0m : 2.65321
[1mStep[0m  [20/106], [94mLoss[0m : 2.57836
[1mStep[0m  [30/106], [94mLoss[0m : 2.30662
[1mStep[0m  [40/106], [94mLoss[0m : 2.03940
[1mStep[0m  [50/106], [94mLoss[0m : 2.31006
[1mStep[0m  [60/106], [94mLoss[0m : 2.23994
[1mStep[0m  [70/106], [94mLoss[0m : 2.50319
[1mStep[0m  [80/106], [94mLoss[0m : 2.07773
[1mStep[0m  [90/106], [94mLoss[0m : 2.39089
[1mStep[0m  [100/106], [94mLoss[0m : 3.01454

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.325, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17087
[1mStep[0m  [10/106], [94mLoss[0m : 2.31959
[1mStep[0m  [20/106], [94mLoss[0m : 1.98092
[1mStep[0m  [30/106], [94mLoss[0m : 1.97901
[1mStep[0m  [40/106], [94mLoss[0m : 2.50359
[1mStep[0m  [50/106], [94mLoss[0m : 2.07399
[1mStep[0m  [60/106], [94mLoss[0m : 2.76847
[1mStep[0m  [70/106], [94mLoss[0m : 2.21768
[1mStep[0m  [80/106], [94mLoss[0m : 2.55512
[1mStep[0m  [90/106], [94mLoss[0m : 2.61694
[1mStep[0m  [100/106], [94mLoss[0m : 2.75267

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65445
[1mStep[0m  [10/106], [94mLoss[0m : 2.25139
[1mStep[0m  [20/106], [94mLoss[0m : 2.48451
[1mStep[0m  [30/106], [94mLoss[0m : 2.61916
[1mStep[0m  [40/106], [94mLoss[0m : 2.46803
[1mStep[0m  [50/106], [94mLoss[0m : 2.51827
[1mStep[0m  [60/106], [94mLoss[0m : 2.39645
[1mStep[0m  [70/106], [94mLoss[0m : 2.39710
[1mStep[0m  [80/106], [94mLoss[0m : 2.22162
[1mStep[0m  [90/106], [94mLoss[0m : 2.58702
[1mStep[0m  [100/106], [94mLoss[0m : 2.65786

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.418, [92mTest[0m: 2.330, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33772
[1mStep[0m  [10/106], [94mLoss[0m : 2.10080
[1mStep[0m  [20/106], [94mLoss[0m : 2.36128
[1mStep[0m  [30/106], [94mLoss[0m : 2.40062
[1mStep[0m  [40/106], [94mLoss[0m : 2.31676
[1mStep[0m  [50/106], [94mLoss[0m : 2.32552
[1mStep[0m  [60/106], [94mLoss[0m : 2.37549
[1mStep[0m  [70/106], [94mLoss[0m : 2.32118
[1mStep[0m  [80/106], [94mLoss[0m : 2.49194
[1mStep[0m  [90/106], [94mLoss[0m : 2.22291
[1mStep[0m  [100/106], [94mLoss[0m : 2.23742

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30942
[1mStep[0m  [10/106], [94mLoss[0m : 2.11969
[1mStep[0m  [20/106], [94mLoss[0m : 2.80902
[1mStep[0m  [30/106], [94mLoss[0m : 2.33796
[1mStep[0m  [40/106], [94mLoss[0m : 2.48594
[1mStep[0m  [50/106], [94mLoss[0m : 2.33355
[1mStep[0m  [60/106], [94mLoss[0m : 2.33552
[1mStep[0m  [70/106], [94mLoss[0m : 2.49502
[1mStep[0m  [80/106], [94mLoss[0m : 2.49566
[1mStep[0m  [90/106], [94mLoss[0m : 2.59497
[1mStep[0m  [100/106], [94mLoss[0m : 2.13359

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.331, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31315
[1mStep[0m  [10/106], [94mLoss[0m : 2.22939
[1mStep[0m  [20/106], [94mLoss[0m : 2.36908
[1mStep[0m  [30/106], [94mLoss[0m : 2.33612
[1mStep[0m  [40/106], [94mLoss[0m : 2.39089
[1mStep[0m  [50/106], [94mLoss[0m : 2.70148
[1mStep[0m  [60/106], [94mLoss[0m : 2.57671
[1mStep[0m  [70/106], [94mLoss[0m : 2.31148
[1mStep[0m  [80/106], [94mLoss[0m : 2.63182
[1mStep[0m  [90/106], [94mLoss[0m : 2.26891
[1mStep[0m  [100/106], [94mLoss[0m : 2.39307

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.327, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.344
====================================

Phase 1 - Evaluation MAE:  2.3439469607371204
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/106], [94mLoss[0m : 2.25311
[1mStep[0m  [10/106], [94mLoss[0m : 2.92153
[1mStep[0m  [20/106], [94mLoss[0m : 2.71398
[1mStep[0m  [30/106], [94mLoss[0m : 2.26829
[1mStep[0m  [40/106], [94mLoss[0m : 2.64878
[1mStep[0m  [50/106], [94mLoss[0m : 2.74673
[1mStep[0m  [60/106], [94mLoss[0m : 2.48577
[1mStep[0m  [70/106], [94mLoss[0m : 2.08740
[1mStep[0m  [80/106], [94mLoss[0m : 2.59385
[1mStep[0m  [90/106], [94mLoss[0m : 2.67806
[1mStep[0m  [100/106], [94mLoss[0m : 2.26187

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.340, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50195
[1mStep[0m  [10/106], [94mLoss[0m : 2.58265
[1mStep[0m  [20/106], [94mLoss[0m : 2.61087
[1mStep[0m  [30/106], [94mLoss[0m : 2.45417
[1mStep[0m  [40/106], [94mLoss[0m : 2.11597
[1mStep[0m  [50/106], [94mLoss[0m : 2.71017
[1mStep[0m  [60/106], [94mLoss[0m : 2.62595
[1mStep[0m  [70/106], [94mLoss[0m : 2.98147
[1mStep[0m  [80/106], [94mLoss[0m : 2.72335
[1mStep[0m  [90/106], [94mLoss[0m : 2.50968
[1mStep[0m  [100/106], [94mLoss[0m : 2.68626

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.383, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19252
[1mStep[0m  [10/106], [94mLoss[0m : 2.10537
[1mStep[0m  [20/106], [94mLoss[0m : 2.55234
[1mStep[0m  [30/106], [94mLoss[0m : 2.06298
[1mStep[0m  [40/106], [94mLoss[0m : 1.97903
[1mStep[0m  [50/106], [94mLoss[0m : 2.41048
[1mStep[0m  [60/106], [94mLoss[0m : 2.15333
[1mStep[0m  [70/106], [94mLoss[0m : 2.25641
[1mStep[0m  [80/106], [94mLoss[0m : 2.41212
[1mStep[0m  [90/106], [94mLoss[0m : 2.64304
[1mStep[0m  [100/106], [94mLoss[0m : 2.05565

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.363, [92mTest[0m: 2.360, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13139
[1mStep[0m  [10/106], [94mLoss[0m : 2.06464
[1mStep[0m  [20/106], [94mLoss[0m : 2.06365
[1mStep[0m  [30/106], [94mLoss[0m : 2.27290
[1mStep[0m  [40/106], [94mLoss[0m : 2.30158
[1mStep[0m  [50/106], [94mLoss[0m : 1.93620
[1mStep[0m  [60/106], [94mLoss[0m : 2.48973
[1mStep[0m  [70/106], [94mLoss[0m : 2.18190
[1mStep[0m  [80/106], [94mLoss[0m : 2.20971
[1mStep[0m  [90/106], [94mLoss[0m : 2.09794
[1mStep[0m  [100/106], [94mLoss[0m : 2.26517

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.373, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34302
[1mStep[0m  [10/106], [94mLoss[0m : 2.02769
[1mStep[0m  [20/106], [94mLoss[0m : 2.08297
[1mStep[0m  [30/106], [94mLoss[0m : 2.02535
[1mStep[0m  [40/106], [94mLoss[0m : 2.04206
[1mStep[0m  [50/106], [94mLoss[0m : 2.69019
[1mStep[0m  [60/106], [94mLoss[0m : 2.36152
[1mStep[0m  [70/106], [94mLoss[0m : 2.25448
[1mStep[0m  [80/106], [94mLoss[0m : 2.39168
[1mStep[0m  [90/106], [94mLoss[0m : 2.33352
[1mStep[0m  [100/106], [94mLoss[0m : 2.40668

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.379, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.04681
[1mStep[0m  [10/106], [94mLoss[0m : 1.99402
[1mStep[0m  [20/106], [94mLoss[0m : 1.98387
[1mStep[0m  [30/106], [94mLoss[0m : 1.96483
[1mStep[0m  [40/106], [94mLoss[0m : 2.17575
[1mStep[0m  [50/106], [94mLoss[0m : 2.07234
[1mStep[0m  [60/106], [94mLoss[0m : 1.93896
[1mStep[0m  [70/106], [94mLoss[0m : 2.17060
[1mStep[0m  [80/106], [94mLoss[0m : 2.20379
[1mStep[0m  [90/106], [94mLoss[0m : 2.28884
[1mStep[0m  [100/106], [94mLoss[0m : 2.33972

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30758
[1mStep[0m  [10/106], [94mLoss[0m : 2.00629
[1mStep[0m  [20/106], [94mLoss[0m : 1.94939
[1mStep[0m  [30/106], [94mLoss[0m : 2.21668
[1mStep[0m  [40/106], [94mLoss[0m : 1.64224
[1mStep[0m  [50/106], [94mLoss[0m : 2.24236
[1mStep[0m  [60/106], [94mLoss[0m : 2.15940
[1mStep[0m  [70/106], [94mLoss[0m : 2.36150
[1mStep[0m  [80/106], [94mLoss[0m : 2.23855
[1mStep[0m  [90/106], [94mLoss[0m : 2.20373
[1mStep[0m  [100/106], [94mLoss[0m : 2.20321

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.68173
[1mStep[0m  [10/106], [94mLoss[0m : 1.84453
[1mStep[0m  [20/106], [94mLoss[0m : 2.26036
[1mStep[0m  [30/106], [94mLoss[0m : 1.92085
[1mStep[0m  [40/106], [94mLoss[0m : 2.15356
[1mStep[0m  [50/106], [94mLoss[0m : 2.01340
[1mStep[0m  [60/106], [94mLoss[0m : 2.11834
[1mStep[0m  [70/106], [94mLoss[0m : 1.90936
[1mStep[0m  [80/106], [94mLoss[0m : 1.79439
[1mStep[0m  [90/106], [94mLoss[0m : 2.11571
[1mStep[0m  [100/106], [94mLoss[0m : 1.89470

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.87953
[1mStep[0m  [10/106], [94mLoss[0m : 1.96008
[1mStep[0m  [20/106], [94mLoss[0m : 1.94239
[1mStep[0m  [30/106], [94mLoss[0m : 2.15021
[1mStep[0m  [40/106], [94mLoss[0m : 1.94759
[1mStep[0m  [50/106], [94mLoss[0m : 2.23132
[1mStep[0m  [60/106], [94mLoss[0m : 2.06858
[1mStep[0m  [70/106], [94mLoss[0m : 2.09937
[1mStep[0m  [80/106], [94mLoss[0m : 2.26147
[1mStep[0m  [90/106], [94mLoss[0m : 1.83014
[1mStep[0m  [100/106], [94mLoss[0m : 1.93732

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.039, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.80119
[1mStep[0m  [10/106], [94mLoss[0m : 2.24684
[1mStep[0m  [20/106], [94mLoss[0m : 1.94155
[1mStep[0m  [30/106], [94mLoss[0m : 2.05950
[1mStep[0m  [40/106], [94mLoss[0m : 1.75874
[1mStep[0m  [50/106], [94mLoss[0m : 2.00920
[1mStep[0m  [60/106], [94mLoss[0m : 1.98845
[1mStep[0m  [70/106], [94mLoss[0m : 1.95126
[1mStep[0m  [80/106], [94mLoss[0m : 2.06024
[1mStep[0m  [90/106], [94mLoss[0m : 1.94509
[1mStep[0m  [100/106], [94mLoss[0m : 2.01945

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96727
[1mStep[0m  [10/106], [94mLoss[0m : 2.18407
[1mStep[0m  [20/106], [94mLoss[0m : 1.94313
[1mStep[0m  [30/106], [94mLoss[0m : 1.85518
[1mStep[0m  [40/106], [94mLoss[0m : 2.08053
[1mStep[0m  [50/106], [94mLoss[0m : 1.91635
[1mStep[0m  [60/106], [94mLoss[0m : 2.38086
[1mStep[0m  [70/106], [94mLoss[0m : 1.87290
[1mStep[0m  [80/106], [94mLoss[0m : 2.04029
[1mStep[0m  [90/106], [94mLoss[0m : 2.02589
[1mStep[0m  [100/106], [94mLoss[0m : 1.96198

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.964, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.09205
[1mStep[0m  [10/106], [94mLoss[0m : 1.93915
[1mStep[0m  [20/106], [94mLoss[0m : 1.61981
[1mStep[0m  [30/106], [94mLoss[0m : 2.00361
[1mStep[0m  [40/106], [94mLoss[0m : 2.15128
[1mStep[0m  [50/106], [94mLoss[0m : 2.11706
[1mStep[0m  [60/106], [94mLoss[0m : 1.88314
[1mStep[0m  [70/106], [94mLoss[0m : 2.09822
[1mStep[0m  [80/106], [94mLoss[0m : 1.98769
[1mStep[0m  [90/106], [94mLoss[0m : 1.87347
[1mStep[0m  [100/106], [94mLoss[0m : 1.79460

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.949, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92969
[1mStep[0m  [10/106], [94mLoss[0m : 1.79985
[1mStep[0m  [20/106], [94mLoss[0m : 1.78515
[1mStep[0m  [30/106], [94mLoss[0m : 2.51447
[1mStep[0m  [40/106], [94mLoss[0m : 1.80886
[1mStep[0m  [50/106], [94mLoss[0m : 2.04216
[1mStep[0m  [60/106], [94mLoss[0m : 1.72079
[1mStep[0m  [70/106], [94mLoss[0m : 2.00939
[1mStep[0m  [80/106], [94mLoss[0m : 2.05136
[1mStep[0m  [90/106], [94mLoss[0m : 1.91387
[1mStep[0m  [100/106], [94mLoss[0m : 2.00909

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16006
[1mStep[0m  [10/106], [94mLoss[0m : 1.77145
[1mStep[0m  [20/106], [94mLoss[0m : 1.87705
[1mStep[0m  [30/106], [94mLoss[0m : 1.71620
[1mStep[0m  [40/106], [94mLoss[0m : 1.81873
[1mStep[0m  [50/106], [94mLoss[0m : 2.09038
[1mStep[0m  [60/106], [94mLoss[0m : 1.95190
[1mStep[0m  [70/106], [94mLoss[0m : 2.07141
[1mStep[0m  [80/106], [94mLoss[0m : 2.09643
[1mStep[0m  [90/106], [94mLoss[0m : 1.86748
[1mStep[0m  [100/106], [94mLoss[0m : 1.99707

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.879, [92mTest[0m: 2.440, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79409
[1mStep[0m  [10/106], [94mLoss[0m : 1.67691
[1mStep[0m  [20/106], [94mLoss[0m : 1.98886
[1mStep[0m  [30/106], [94mLoss[0m : 2.03586
[1mStep[0m  [40/106], [94mLoss[0m : 1.77273
[1mStep[0m  [50/106], [94mLoss[0m : 1.69818
[1mStep[0m  [60/106], [94mLoss[0m : 1.95471
[1mStep[0m  [70/106], [94mLoss[0m : 2.07233
[1mStep[0m  [80/106], [94mLoss[0m : 2.10713
[1mStep[0m  [90/106], [94mLoss[0m : 1.84747
[1mStep[0m  [100/106], [94mLoss[0m : 1.91135

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.859, [92mTest[0m: 2.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70550
[1mStep[0m  [10/106], [94mLoss[0m : 1.90298
[1mStep[0m  [20/106], [94mLoss[0m : 1.61657
[1mStep[0m  [30/106], [94mLoss[0m : 1.98389
[1mStep[0m  [40/106], [94mLoss[0m : 1.69560
[1mStep[0m  [50/106], [94mLoss[0m : 1.84013
[1mStep[0m  [60/106], [94mLoss[0m : 1.95318
[1mStep[0m  [70/106], [94mLoss[0m : 2.03603
[1mStep[0m  [80/106], [94mLoss[0m : 1.91558
[1mStep[0m  [90/106], [94mLoss[0m : 1.94084
[1mStep[0m  [100/106], [94mLoss[0m : 1.96800

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74313
[1mStep[0m  [10/106], [94mLoss[0m : 1.99379
[1mStep[0m  [20/106], [94mLoss[0m : 1.75331
[1mStep[0m  [30/106], [94mLoss[0m : 2.00303
[1mStep[0m  [40/106], [94mLoss[0m : 1.92915
[1mStep[0m  [50/106], [94mLoss[0m : 1.96362
[1mStep[0m  [60/106], [94mLoss[0m : 2.02615
[1mStep[0m  [70/106], [94mLoss[0m : 1.84712
[1mStep[0m  [80/106], [94mLoss[0m : 1.99333
[1mStep[0m  [90/106], [94mLoss[0m : 1.85752
[1mStep[0m  [100/106], [94mLoss[0m : 1.90511

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.818, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82013
[1mStep[0m  [10/106], [94mLoss[0m : 1.97077
[1mStep[0m  [20/106], [94mLoss[0m : 1.63232
[1mStep[0m  [30/106], [94mLoss[0m : 1.76011
[1mStep[0m  [40/106], [94mLoss[0m : 1.78550
[1mStep[0m  [50/106], [94mLoss[0m : 1.66818
[1mStep[0m  [60/106], [94mLoss[0m : 1.87746
[1mStep[0m  [70/106], [94mLoss[0m : 1.83154
[1mStep[0m  [80/106], [94mLoss[0m : 1.80700
[1mStep[0m  [90/106], [94mLoss[0m : 1.76387
[1mStep[0m  [100/106], [94mLoss[0m : 2.09692

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.56847
[1mStep[0m  [10/106], [94mLoss[0m : 1.59256
[1mStep[0m  [20/106], [94mLoss[0m : 1.64492
[1mStep[0m  [30/106], [94mLoss[0m : 1.87430
[1mStep[0m  [40/106], [94mLoss[0m : 1.67661
[1mStep[0m  [50/106], [94mLoss[0m : 1.58973
[1mStep[0m  [60/106], [94mLoss[0m : 1.81330
[1mStep[0m  [70/106], [94mLoss[0m : 1.76823
[1mStep[0m  [80/106], [94mLoss[0m : 1.82434
[1mStep[0m  [90/106], [94mLoss[0m : 1.99080
[1mStep[0m  [100/106], [94mLoss[0m : 2.00286

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.783, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.76093
[1mStep[0m  [10/106], [94mLoss[0m : 1.70407
[1mStep[0m  [20/106], [94mLoss[0m : 1.62421
[1mStep[0m  [30/106], [94mLoss[0m : 1.81492
[1mStep[0m  [40/106], [94mLoss[0m : 1.86695
[1mStep[0m  [50/106], [94mLoss[0m : 1.68090
[1mStep[0m  [60/106], [94mLoss[0m : 1.52678
[1mStep[0m  [70/106], [94mLoss[0m : 1.92994
[1mStep[0m  [80/106], [94mLoss[0m : 1.89273
[1mStep[0m  [90/106], [94mLoss[0m : 1.82378
[1mStep[0m  [100/106], [94mLoss[0m : 1.64527

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.742, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62337
[1mStep[0m  [10/106], [94mLoss[0m : 1.80467
[1mStep[0m  [20/106], [94mLoss[0m : 1.67916
[1mStep[0m  [30/106], [94mLoss[0m : 1.74028
[1mStep[0m  [40/106], [94mLoss[0m : 1.79180
[1mStep[0m  [50/106], [94mLoss[0m : 1.76785
[1mStep[0m  [60/106], [94mLoss[0m : 1.67195
[1mStep[0m  [70/106], [94mLoss[0m : 1.76562
[1mStep[0m  [80/106], [94mLoss[0m : 1.49955
[1mStep[0m  [90/106], [94mLoss[0m : 1.99173
[1mStep[0m  [100/106], [94mLoss[0m : 1.85730

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.709, [92mTest[0m: 2.479, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54901
[1mStep[0m  [10/106], [94mLoss[0m : 1.47836
[1mStep[0m  [20/106], [94mLoss[0m : 1.48394
[1mStep[0m  [30/106], [94mLoss[0m : 1.57134
[1mStep[0m  [40/106], [94mLoss[0m : 1.67342
[1mStep[0m  [50/106], [94mLoss[0m : 1.78760
[1mStep[0m  [60/106], [94mLoss[0m : 1.76663
[1mStep[0m  [70/106], [94mLoss[0m : 1.60438
[1mStep[0m  [80/106], [94mLoss[0m : 1.63379
[1mStep[0m  [90/106], [94mLoss[0m : 1.67842
[1mStep[0m  [100/106], [94mLoss[0m : 1.75605

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.691, [92mTest[0m: 2.487, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.70163
[1mStep[0m  [10/106], [94mLoss[0m : 1.71748
[1mStep[0m  [20/106], [94mLoss[0m : 1.41318
[1mStep[0m  [30/106], [94mLoss[0m : 1.60361
[1mStep[0m  [40/106], [94mLoss[0m : 1.79689
[1mStep[0m  [50/106], [94mLoss[0m : 1.78195
[1mStep[0m  [60/106], [94mLoss[0m : 1.64892
[1mStep[0m  [70/106], [94mLoss[0m : 1.76191
[1mStep[0m  [80/106], [94mLoss[0m : 1.60551
[1mStep[0m  [90/106], [94mLoss[0m : 1.77378
[1mStep[0m  [100/106], [94mLoss[0m : 1.72407

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.52443
[1mStep[0m  [10/106], [94mLoss[0m : 1.78075
[1mStep[0m  [20/106], [94mLoss[0m : 1.52119
[1mStep[0m  [30/106], [94mLoss[0m : 1.72581
[1mStep[0m  [40/106], [94mLoss[0m : 1.58353
[1mStep[0m  [50/106], [94mLoss[0m : 1.55869
[1mStep[0m  [60/106], [94mLoss[0m : 1.59927
[1mStep[0m  [70/106], [94mLoss[0m : 1.64777
[1mStep[0m  [80/106], [94mLoss[0m : 1.87439
[1mStep[0m  [90/106], [94mLoss[0m : 1.65293
[1mStep[0m  [100/106], [94mLoss[0m : 1.62703

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.59859
[1mStep[0m  [10/106], [94mLoss[0m : 1.70456
[1mStep[0m  [20/106], [94mLoss[0m : 1.95481
[1mStep[0m  [30/106], [94mLoss[0m : 1.63273
[1mStep[0m  [40/106], [94mLoss[0m : 1.74013
[1mStep[0m  [50/106], [94mLoss[0m : 1.78591
[1mStep[0m  [60/106], [94mLoss[0m : 1.64841
[1mStep[0m  [70/106], [94mLoss[0m : 1.56228
[1mStep[0m  [80/106], [94mLoss[0m : 1.71434
[1mStep[0m  [90/106], [94mLoss[0m : 1.83653
[1mStep[0m  [100/106], [94mLoss[0m : 1.88374

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86448
[1mStep[0m  [10/106], [94mLoss[0m : 1.58075
[1mStep[0m  [20/106], [94mLoss[0m : 1.46482
[1mStep[0m  [30/106], [94mLoss[0m : 1.66837
[1mStep[0m  [40/106], [94mLoss[0m : 1.63289
[1mStep[0m  [50/106], [94mLoss[0m : 1.85898
[1mStep[0m  [60/106], [94mLoss[0m : 1.58560
[1mStep[0m  [70/106], [94mLoss[0m : 1.71239
[1mStep[0m  [80/106], [94mLoss[0m : 1.88745
[1mStep[0m  [90/106], [94mLoss[0m : 1.71151
[1mStep[0m  [100/106], [94mLoss[0m : 1.53865

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.631, [92mTest[0m: 2.482, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.62009
[1mStep[0m  [10/106], [94mLoss[0m : 1.78808
[1mStep[0m  [20/106], [94mLoss[0m : 1.40903
[1mStep[0m  [30/106], [94mLoss[0m : 1.54994
[1mStep[0m  [40/106], [94mLoss[0m : 1.65946
[1mStep[0m  [50/106], [94mLoss[0m : 1.41289
[1mStep[0m  [60/106], [94mLoss[0m : 1.56102
[1mStep[0m  [70/106], [94mLoss[0m : 1.59593
[1mStep[0m  [80/106], [94mLoss[0m : 1.80140
[1mStep[0m  [90/106], [94mLoss[0m : 1.70438
[1mStep[0m  [100/106], [94mLoss[0m : 1.65435

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.622, [92mTest[0m: 2.499, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.32336
[1mStep[0m  [10/106], [94mLoss[0m : 1.41840
[1mStep[0m  [20/106], [94mLoss[0m : 1.64859
[1mStep[0m  [30/106], [94mLoss[0m : 1.60502
[1mStep[0m  [40/106], [94mLoss[0m : 1.75879
[1mStep[0m  [50/106], [94mLoss[0m : 1.72677
[1mStep[0m  [60/106], [94mLoss[0m : 1.78515
[1mStep[0m  [70/106], [94mLoss[0m : 1.69741
[1mStep[0m  [80/106], [94mLoss[0m : 1.80263
[1mStep[0m  [90/106], [94mLoss[0m : 1.69817
[1mStep[0m  [100/106], [94mLoss[0m : 1.66784

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.607, [92mTest[0m: 2.485, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.55107
[1mStep[0m  [10/106], [94mLoss[0m : 1.37055
[1mStep[0m  [20/106], [94mLoss[0m : 1.57290
[1mStep[0m  [30/106], [94mLoss[0m : 1.66988
[1mStep[0m  [40/106], [94mLoss[0m : 1.53695
[1mStep[0m  [50/106], [94mLoss[0m : 1.64631
[1mStep[0m  [60/106], [94mLoss[0m : 1.57302
[1mStep[0m  [70/106], [94mLoss[0m : 1.62565
[1mStep[0m  [80/106], [94mLoss[0m : 1.57418
[1mStep[0m  [90/106], [94mLoss[0m : 1.53905
[1mStep[0m  [100/106], [94mLoss[0m : 1.58612

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54040
[1mStep[0m  [10/106], [94mLoss[0m : 1.59312
[1mStep[0m  [20/106], [94mLoss[0m : 1.58732
[1mStep[0m  [30/106], [94mLoss[0m : 1.48073
[1mStep[0m  [40/106], [94mLoss[0m : 1.38878
[1mStep[0m  [50/106], [94mLoss[0m : 1.60056
[1mStep[0m  [60/106], [94mLoss[0m : 1.50992
[1mStep[0m  [70/106], [94mLoss[0m : 1.47265
[1mStep[0m  [80/106], [94mLoss[0m : 1.74420
[1mStep[0m  [90/106], [94mLoss[0m : 1.46881
[1mStep[0m  [100/106], [94mLoss[0m : 1.58233

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.569, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.488
====================================

Phase 2 - Evaluation MAE:  2.4881982893314003
MAE score P1        2.343947
MAE score P2        2.488198
loss                1.569395
learning_rate       0.002575
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation              relu
optimizer                sgd
early stopping         False
dropout                  0.4
momentum                 0.9
weight_decay            0.01
Name: 9, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.001, 'momentum': 0.5, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 10.22092
[1mStep[0m  [21/213], [94mLoss[0m : 10.73748
[1mStep[0m  [42/213], [94mLoss[0m : 10.41386
[1mStep[0m  [63/213], [94mLoss[0m : 11.19652
[1mStep[0m  [84/213], [94mLoss[0m : 10.41750
[1mStep[0m  [105/213], [94mLoss[0m : 11.68759
[1mStep[0m  [126/213], [94mLoss[0m : 10.15692
[1mStep[0m  [147/213], [94mLoss[0m : 10.46308
[1mStep[0m  [168/213], [94mLoss[0m : 10.26324
[1mStep[0m  [189/213], [94mLoss[0m : 10.70253
[1mStep[0m  [210/213], [94mLoss[0m : 9.36891

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.571, [92mTest[0m: 11.058, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 10.07360
[1mStep[0m  [21/213], [94mLoss[0m : 10.07064
[1mStep[0m  [42/213], [94mLoss[0m : 9.68287
[1mStep[0m  [63/213], [94mLoss[0m : 9.86130
[1mStep[0m  [84/213], [94mLoss[0m : 9.80890
[1mStep[0m  [105/213], [94mLoss[0m : 10.39148
[1mStep[0m  [126/213], [94mLoss[0m : 9.57456
[1mStep[0m  [147/213], [94mLoss[0m : 10.16352
[1mStep[0m  [168/213], [94mLoss[0m : 9.98209
[1mStep[0m  [189/213], [94mLoss[0m : 9.93537
[1mStep[0m  [210/213], [94mLoss[0m : 9.06923

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 9.816, [92mTest[0m: 10.037, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 9.58872
[1mStep[0m  [21/213], [94mLoss[0m : 9.52939
[1mStep[0m  [42/213], [94mLoss[0m : 8.85709
[1mStep[0m  [63/213], [94mLoss[0m : 9.06749
[1mStep[0m  [84/213], [94mLoss[0m : 8.31249
[1mStep[0m  [105/213], [94mLoss[0m : 8.53660
[1mStep[0m  [126/213], [94mLoss[0m : 7.78758
[1mStep[0m  [147/213], [94mLoss[0m : 9.37046
[1mStep[0m  [168/213], [94mLoss[0m : 8.80329
[1mStep[0m  [189/213], [94mLoss[0m : 7.92220
[1mStep[0m  [210/213], [94mLoss[0m : 8.85101

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 8.919, [92mTest[0m: 9.085, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 8.13767
[1mStep[0m  [21/213], [94mLoss[0m : 8.50024
[1mStep[0m  [42/213], [94mLoss[0m : 8.46565
[1mStep[0m  [63/213], [94mLoss[0m : 8.52779
[1mStep[0m  [84/213], [94mLoss[0m : 7.85991
[1mStep[0m  [105/213], [94mLoss[0m : 7.02624
[1mStep[0m  [126/213], [94mLoss[0m : 7.58680
[1mStep[0m  [147/213], [94mLoss[0m : 7.89525
[1mStep[0m  [168/213], [94mLoss[0m : 7.20210
[1mStep[0m  [189/213], [94mLoss[0m : 7.47024
[1mStep[0m  [210/213], [94mLoss[0m : 6.71258

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 7.665, [92mTest[0m: 7.770, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 7.00430
[1mStep[0m  [21/213], [94mLoss[0m : 6.89165
[1mStep[0m  [42/213], [94mLoss[0m : 7.01157
[1mStep[0m  [63/213], [94mLoss[0m : 6.82783
[1mStep[0m  [84/213], [94mLoss[0m : 6.74210
[1mStep[0m  [105/213], [94mLoss[0m : 6.05896
[1mStep[0m  [126/213], [94mLoss[0m : 5.49774
[1mStep[0m  [147/213], [94mLoss[0m : 6.62795
[1mStep[0m  [168/213], [94mLoss[0m : 5.63465
[1mStep[0m  [189/213], [94mLoss[0m : 5.15607
[1mStep[0m  [210/213], [94mLoss[0m : 6.02901

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 6.151, [92mTest[0m: 6.044, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 5.55999
[1mStep[0m  [21/213], [94mLoss[0m : 5.40474
[1mStep[0m  [42/213], [94mLoss[0m : 4.84228
[1mStep[0m  [63/213], [94mLoss[0m : 4.43762
[1mStep[0m  [84/213], [94mLoss[0m : 5.64333
[1mStep[0m  [105/213], [94mLoss[0m : 4.34459
[1mStep[0m  [126/213], [94mLoss[0m : 4.71252
[1mStep[0m  [147/213], [94mLoss[0m : 4.16528
[1mStep[0m  [168/213], [94mLoss[0m : 4.10281
[1mStep[0m  [189/213], [94mLoss[0m : 4.48353
[1mStep[0m  [210/213], [94mLoss[0m : 3.84742

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 4.749, [92mTest[0m: 4.739, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.80592
[1mStep[0m  [21/213], [94mLoss[0m : 4.10795
[1mStep[0m  [42/213], [94mLoss[0m : 3.97674
[1mStep[0m  [63/213], [94mLoss[0m : 3.07895
[1mStep[0m  [84/213], [94mLoss[0m : 3.52837
[1mStep[0m  [105/213], [94mLoss[0m : 3.75251
[1mStep[0m  [126/213], [94mLoss[0m : 3.43796
[1mStep[0m  [147/213], [94mLoss[0m : 3.46002
[1mStep[0m  [168/213], [94mLoss[0m : 3.43708
[1mStep[0m  [189/213], [94mLoss[0m : 2.89957
[1mStep[0m  [210/213], [94mLoss[0m : 3.05872

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 3.595, [92mTest[0m: 3.513, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.12872
[1mStep[0m  [21/213], [94mLoss[0m : 4.01484
[1mStep[0m  [42/213], [94mLoss[0m : 2.66360
[1mStep[0m  [63/213], [94mLoss[0m : 3.42521
[1mStep[0m  [84/213], [94mLoss[0m : 2.79537
[1mStep[0m  [105/213], [94mLoss[0m : 2.88474
[1mStep[0m  [126/213], [94mLoss[0m : 2.84958
[1mStep[0m  [147/213], [94mLoss[0m : 2.92517
[1mStep[0m  [168/213], [94mLoss[0m : 2.51448
[1mStep[0m  [189/213], [94mLoss[0m : 2.69922
[1mStep[0m  [210/213], [94mLoss[0m : 2.94550

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.991, [92mTest[0m: 2.781, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.13637
[1mStep[0m  [21/213], [94mLoss[0m : 2.37861
[1mStep[0m  [42/213], [94mLoss[0m : 2.17150
[1mStep[0m  [63/213], [94mLoss[0m : 2.70070
[1mStep[0m  [84/213], [94mLoss[0m : 3.06907
[1mStep[0m  [105/213], [94mLoss[0m : 2.24775
[1mStep[0m  [126/213], [94mLoss[0m : 3.21508
[1mStep[0m  [147/213], [94mLoss[0m : 2.53275
[1mStep[0m  [168/213], [94mLoss[0m : 3.00400
[1mStep[0m  [189/213], [94mLoss[0m : 2.71606
[1mStep[0m  [210/213], [94mLoss[0m : 2.42576

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.783, [92mTest[0m: 2.538, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55376
[1mStep[0m  [21/213], [94mLoss[0m : 2.95269
[1mStep[0m  [42/213], [94mLoss[0m : 2.42600
[1mStep[0m  [63/213], [94mLoss[0m : 2.68935
[1mStep[0m  [84/213], [94mLoss[0m : 2.96190
[1mStep[0m  [105/213], [94mLoss[0m : 2.47369
[1mStep[0m  [126/213], [94mLoss[0m : 2.48488
[1mStep[0m  [147/213], [94mLoss[0m : 3.13219
[1mStep[0m  [168/213], [94mLoss[0m : 2.59771
[1mStep[0m  [189/213], [94mLoss[0m : 3.09131
[1mStep[0m  [210/213], [94mLoss[0m : 2.42566

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.732, [92mTest[0m: 2.482, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.98899
[1mStep[0m  [21/213], [94mLoss[0m : 2.98867
[1mStep[0m  [42/213], [94mLoss[0m : 2.27468
[1mStep[0m  [63/213], [94mLoss[0m : 2.98801
[1mStep[0m  [84/213], [94mLoss[0m : 2.37162
[1mStep[0m  [105/213], [94mLoss[0m : 2.97693
[1mStep[0m  [126/213], [94mLoss[0m : 2.93139
[1mStep[0m  [147/213], [94mLoss[0m : 2.74387
[1mStep[0m  [168/213], [94mLoss[0m : 2.94268
[1mStep[0m  [189/213], [94mLoss[0m : 2.66944
[1mStep[0m  [210/213], [94mLoss[0m : 2.74237

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.707, [92mTest[0m: 2.473, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.36294
[1mStep[0m  [21/213], [94mLoss[0m : 2.58629
[1mStep[0m  [42/213], [94mLoss[0m : 2.59461
[1mStep[0m  [63/213], [94mLoss[0m : 3.06701
[1mStep[0m  [84/213], [94mLoss[0m : 2.64937
[1mStep[0m  [105/213], [94mLoss[0m : 2.30672
[1mStep[0m  [126/213], [94mLoss[0m : 2.62404
[1mStep[0m  [147/213], [94mLoss[0m : 2.24727
[1mStep[0m  [168/213], [94mLoss[0m : 3.26818
[1mStep[0m  [189/213], [94mLoss[0m : 2.88678
[1mStep[0m  [210/213], [94mLoss[0m : 2.77080

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.710, [92mTest[0m: 2.481, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.95668
[1mStep[0m  [21/213], [94mLoss[0m : 2.82921
[1mStep[0m  [42/213], [94mLoss[0m : 2.71311
[1mStep[0m  [63/213], [94mLoss[0m : 2.97168
[1mStep[0m  [84/213], [94mLoss[0m : 2.34369
[1mStep[0m  [105/213], [94mLoss[0m : 2.62516
[1mStep[0m  [126/213], [94mLoss[0m : 2.74097
[1mStep[0m  [147/213], [94mLoss[0m : 2.75023
[1mStep[0m  [168/213], [94mLoss[0m : 2.91935
[1mStep[0m  [189/213], [94mLoss[0m : 2.93029
[1mStep[0m  [210/213], [94mLoss[0m : 2.95749

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.683, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.66006
[1mStep[0m  [21/213], [94mLoss[0m : 2.92503
[1mStep[0m  [42/213], [94mLoss[0m : 2.80236
[1mStep[0m  [63/213], [94mLoss[0m : 2.31498
[1mStep[0m  [84/213], [94mLoss[0m : 2.99510
[1mStep[0m  [105/213], [94mLoss[0m : 2.65804
[1mStep[0m  [126/213], [94mLoss[0m : 2.56562
[1mStep[0m  [147/213], [94mLoss[0m : 2.95289
[1mStep[0m  [168/213], [94mLoss[0m : 2.55717
[1mStep[0m  [189/213], [94mLoss[0m : 2.97472
[1mStep[0m  [210/213], [94mLoss[0m : 2.44779

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.445, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.95965
[1mStep[0m  [21/213], [94mLoss[0m : 2.19313
[1mStep[0m  [42/213], [94mLoss[0m : 2.64165
[1mStep[0m  [63/213], [94mLoss[0m : 2.77665
[1mStep[0m  [84/213], [94mLoss[0m : 2.51364
[1mStep[0m  [105/213], [94mLoss[0m : 2.51316
[1mStep[0m  [126/213], [94mLoss[0m : 2.40484
[1mStep[0m  [147/213], [94mLoss[0m : 2.85215
[1mStep[0m  [168/213], [94mLoss[0m : 2.11985
[1mStep[0m  [189/213], [94mLoss[0m : 2.87885
[1mStep[0m  [210/213], [94mLoss[0m : 2.48256

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.645, [92mTest[0m: 2.439, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63990
[1mStep[0m  [21/213], [94mLoss[0m : 2.55513
[1mStep[0m  [42/213], [94mLoss[0m : 2.41338
[1mStep[0m  [63/213], [94mLoss[0m : 2.55119
[1mStep[0m  [84/213], [94mLoss[0m : 2.54757
[1mStep[0m  [105/213], [94mLoss[0m : 2.35434
[1mStep[0m  [126/213], [94mLoss[0m : 3.24501
[1mStep[0m  [147/213], [94mLoss[0m : 2.52174
[1mStep[0m  [168/213], [94mLoss[0m : 2.37557
[1mStep[0m  [189/213], [94mLoss[0m : 3.13554
[1mStep[0m  [210/213], [94mLoss[0m : 2.47646

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.463, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42797
[1mStep[0m  [21/213], [94mLoss[0m : 2.82705
[1mStep[0m  [42/213], [94mLoss[0m : 3.25973
[1mStep[0m  [63/213], [94mLoss[0m : 2.62945
[1mStep[0m  [84/213], [94mLoss[0m : 2.94088
[1mStep[0m  [105/213], [94mLoss[0m : 2.69688
[1mStep[0m  [126/213], [94mLoss[0m : 2.03369
[1mStep[0m  [147/213], [94mLoss[0m : 2.84138
[1mStep[0m  [168/213], [94mLoss[0m : 2.77214
[1mStep[0m  [189/213], [94mLoss[0m : 2.53270
[1mStep[0m  [210/213], [94mLoss[0m : 2.28043

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.631, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.49861
[1mStep[0m  [21/213], [94mLoss[0m : 2.56807
[1mStep[0m  [42/213], [94mLoss[0m : 2.55612
[1mStep[0m  [63/213], [94mLoss[0m : 2.72651
[1mStep[0m  [84/213], [94mLoss[0m : 2.34225
[1mStep[0m  [105/213], [94mLoss[0m : 3.17125
[1mStep[0m  [126/213], [94mLoss[0m : 2.64725
[1mStep[0m  [147/213], [94mLoss[0m : 3.09713
[1mStep[0m  [168/213], [94mLoss[0m : 2.80172
[1mStep[0m  [189/213], [94mLoss[0m : 2.27883
[1mStep[0m  [210/213], [94mLoss[0m : 2.79590

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.615, [92mTest[0m: 2.418, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.89283
[1mStep[0m  [21/213], [94mLoss[0m : 2.76170
[1mStep[0m  [42/213], [94mLoss[0m : 2.74488
[1mStep[0m  [63/213], [94mLoss[0m : 3.23582
[1mStep[0m  [84/213], [94mLoss[0m : 2.52282
[1mStep[0m  [105/213], [94mLoss[0m : 2.61859
[1mStep[0m  [126/213], [94mLoss[0m : 2.66374
[1mStep[0m  [147/213], [94mLoss[0m : 2.73218
[1mStep[0m  [168/213], [94mLoss[0m : 2.46399
[1mStep[0m  [189/213], [94mLoss[0m : 2.68765
[1mStep[0m  [210/213], [94mLoss[0m : 2.70660

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.638, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.44100
[1mStep[0m  [21/213], [94mLoss[0m : 2.69265
[1mStep[0m  [42/213], [94mLoss[0m : 2.22435
[1mStep[0m  [63/213], [94mLoss[0m : 2.91721
[1mStep[0m  [84/213], [94mLoss[0m : 2.46000
[1mStep[0m  [105/213], [94mLoss[0m : 2.54814
[1mStep[0m  [126/213], [94mLoss[0m : 2.98506
[1mStep[0m  [147/213], [94mLoss[0m : 2.35972
[1mStep[0m  [168/213], [94mLoss[0m : 2.98001
[1mStep[0m  [189/213], [94mLoss[0m : 2.35795
[1mStep[0m  [210/213], [94mLoss[0m : 2.44326

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.596, [92mTest[0m: 2.419, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47888
[1mStep[0m  [21/213], [94mLoss[0m : 2.53103
[1mStep[0m  [42/213], [94mLoss[0m : 2.46523
[1mStep[0m  [63/213], [94mLoss[0m : 2.56881
[1mStep[0m  [84/213], [94mLoss[0m : 3.09865
[1mStep[0m  [105/213], [94mLoss[0m : 2.47497
[1mStep[0m  [126/213], [94mLoss[0m : 2.71192
[1mStep[0m  [147/213], [94mLoss[0m : 2.76894
[1mStep[0m  [168/213], [94mLoss[0m : 2.18742
[1mStep[0m  [189/213], [94mLoss[0m : 2.51758
[1mStep[0m  [210/213], [94mLoss[0m : 2.71232

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.621, [92mTest[0m: 2.430, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.82117
[1mStep[0m  [21/213], [94mLoss[0m : 2.99962
[1mStep[0m  [42/213], [94mLoss[0m : 2.48222
[1mStep[0m  [63/213], [94mLoss[0m : 3.05368
[1mStep[0m  [84/213], [94mLoss[0m : 1.91263
[1mStep[0m  [105/213], [94mLoss[0m : 2.47211
[1mStep[0m  [126/213], [94mLoss[0m : 2.61065
[1mStep[0m  [147/213], [94mLoss[0m : 2.06882
[1mStep[0m  [168/213], [94mLoss[0m : 2.68009
[1mStep[0m  [189/213], [94mLoss[0m : 3.03771
[1mStep[0m  [210/213], [94mLoss[0m : 2.50347

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.407, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.69516
[1mStep[0m  [21/213], [94mLoss[0m : 2.29992
[1mStep[0m  [42/213], [94mLoss[0m : 2.42480
[1mStep[0m  [63/213], [94mLoss[0m : 3.16403
[1mStep[0m  [84/213], [94mLoss[0m : 2.14904
[1mStep[0m  [105/213], [94mLoss[0m : 2.89068
[1mStep[0m  [126/213], [94mLoss[0m : 2.53977
[1mStep[0m  [147/213], [94mLoss[0m : 2.61808
[1mStep[0m  [168/213], [94mLoss[0m : 2.91018
[1mStep[0m  [189/213], [94mLoss[0m : 2.44541
[1mStep[0m  [210/213], [94mLoss[0m : 2.56228

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.413, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61926
[1mStep[0m  [21/213], [94mLoss[0m : 3.05772
[1mStep[0m  [42/213], [94mLoss[0m : 2.46533
[1mStep[0m  [63/213], [94mLoss[0m : 2.31273
[1mStep[0m  [84/213], [94mLoss[0m : 2.48903
[1mStep[0m  [105/213], [94mLoss[0m : 2.89637
[1mStep[0m  [126/213], [94mLoss[0m : 2.36978
[1mStep[0m  [147/213], [94mLoss[0m : 2.70438
[1mStep[0m  [168/213], [94mLoss[0m : 2.45857
[1mStep[0m  [189/213], [94mLoss[0m : 2.55426
[1mStep[0m  [210/213], [94mLoss[0m : 2.64410

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.566, [92mTest[0m: 2.411, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54370
[1mStep[0m  [21/213], [94mLoss[0m : 2.27097
[1mStep[0m  [42/213], [94mLoss[0m : 2.93270
[1mStep[0m  [63/213], [94mLoss[0m : 2.65123
[1mStep[0m  [84/213], [94mLoss[0m : 2.24083
[1mStep[0m  [105/213], [94mLoss[0m : 2.65154
[1mStep[0m  [126/213], [94mLoss[0m : 2.78769
[1mStep[0m  [147/213], [94mLoss[0m : 2.32141
[1mStep[0m  [168/213], [94mLoss[0m : 2.87331
[1mStep[0m  [189/213], [94mLoss[0m : 2.96611
[1mStep[0m  [210/213], [94mLoss[0m : 2.25086

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.583, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.73519
[1mStep[0m  [21/213], [94mLoss[0m : 2.33887
[1mStep[0m  [42/213], [94mLoss[0m : 2.54693
[1mStep[0m  [63/213], [94mLoss[0m : 2.59537
[1mStep[0m  [84/213], [94mLoss[0m : 2.55522
[1mStep[0m  [105/213], [94mLoss[0m : 2.78156
[1mStep[0m  [126/213], [94mLoss[0m : 2.51339
[1mStep[0m  [147/213], [94mLoss[0m : 2.77288
[1mStep[0m  [168/213], [94mLoss[0m : 2.93537
[1mStep[0m  [189/213], [94mLoss[0m : 2.42193
[1mStep[0m  [210/213], [94mLoss[0m : 2.98082

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.574, [92mTest[0m: 2.418, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.05111
[1mStep[0m  [21/213], [94mLoss[0m : 2.68227
[1mStep[0m  [42/213], [94mLoss[0m : 2.90716
[1mStep[0m  [63/213], [94mLoss[0m : 2.95644
[1mStep[0m  [84/213], [94mLoss[0m : 2.55612
[1mStep[0m  [105/213], [94mLoss[0m : 2.48765
[1mStep[0m  [126/213], [94mLoss[0m : 2.49216
[1mStep[0m  [147/213], [94mLoss[0m : 2.07831
[1mStep[0m  [168/213], [94mLoss[0m : 2.54073
[1mStep[0m  [189/213], [94mLoss[0m : 2.38333
[1mStep[0m  [210/213], [94mLoss[0m : 2.24705

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.395, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.62759
[1mStep[0m  [21/213], [94mLoss[0m : 2.57742
[1mStep[0m  [42/213], [94mLoss[0m : 3.01123
[1mStep[0m  [63/213], [94mLoss[0m : 2.47007
[1mStep[0m  [84/213], [94mLoss[0m : 2.32001
[1mStep[0m  [105/213], [94mLoss[0m : 2.56759
[1mStep[0m  [126/213], [94mLoss[0m : 2.23578
[1mStep[0m  [147/213], [94mLoss[0m : 2.82695
[1mStep[0m  [168/213], [94mLoss[0m : 2.41461
[1mStep[0m  [189/213], [94mLoss[0m : 2.35420
[1mStep[0m  [210/213], [94mLoss[0m : 2.97181

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.422, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.80060
[1mStep[0m  [21/213], [94mLoss[0m : 2.25290
[1mStep[0m  [42/213], [94mLoss[0m : 2.91768
[1mStep[0m  [63/213], [94mLoss[0m : 2.34496
[1mStep[0m  [84/213], [94mLoss[0m : 2.75489
[1mStep[0m  [105/213], [94mLoss[0m : 2.67815
[1mStep[0m  [126/213], [94mLoss[0m : 1.88993
[1mStep[0m  [147/213], [94mLoss[0m : 2.86166
[1mStep[0m  [168/213], [94mLoss[0m : 2.95179
[1mStep[0m  [189/213], [94mLoss[0m : 2.26835
[1mStep[0m  [210/213], [94mLoss[0m : 2.64751

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.405, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61682
[1mStep[0m  [21/213], [94mLoss[0m : 2.90459
[1mStep[0m  [42/213], [94mLoss[0m : 2.80974
[1mStep[0m  [63/213], [94mLoss[0m : 2.55023
[1mStep[0m  [84/213], [94mLoss[0m : 2.53296
[1mStep[0m  [105/213], [94mLoss[0m : 2.83409
[1mStep[0m  [126/213], [94mLoss[0m : 2.62084
[1mStep[0m  [147/213], [94mLoss[0m : 2.26388
[1mStep[0m  [168/213], [94mLoss[0m : 2.64816
[1mStep[0m  [189/213], [94mLoss[0m : 2.96528
[1mStep[0m  [210/213], [94mLoss[0m : 2.75481

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.391, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.404
====================================

Phase 1 - Evaluation MAE:  2.403519758638346
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/213], [94mLoss[0m : 2.56051
[1mStep[0m  [21/213], [94mLoss[0m : 2.20198
[1mStep[0m  [42/213], [94mLoss[0m : 2.70542
[1mStep[0m  [63/213], [94mLoss[0m : 2.19283
[1mStep[0m  [84/213], [94mLoss[0m : 2.89238
[1mStep[0m  [105/213], [94mLoss[0m : 2.49776
[1mStep[0m  [126/213], [94mLoss[0m : 3.06042
[1mStep[0m  [147/213], [94mLoss[0m : 2.97679
[1mStep[0m  [168/213], [94mLoss[0m : 2.37088
[1mStep[0m  [189/213], [94mLoss[0m : 2.54878
[1mStep[0m  [210/213], [94mLoss[0m : 2.50909

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.63293
[1mStep[0m  [21/213], [94mLoss[0m : 2.65399
[1mStep[0m  [42/213], [94mLoss[0m : 2.22474
[1mStep[0m  [63/213], [94mLoss[0m : 2.39728
[1mStep[0m  [84/213], [94mLoss[0m : 2.40753
[1mStep[0m  [105/213], [94mLoss[0m : 2.06407
[1mStep[0m  [126/213], [94mLoss[0m : 2.56927
[1mStep[0m  [147/213], [94mLoss[0m : 2.53989
[1mStep[0m  [168/213], [94mLoss[0m : 2.79426
[1mStep[0m  [189/213], [94mLoss[0m : 2.81348
[1mStep[0m  [210/213], [94mLoss[0m : 2.51712

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.541, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.78368
[1mStep[0m  [21/213], [94mLoss[0m : 2.53835
[1mStep[0m  [42/213], [94mLoss[0m : 2.34846
[1mStep[0m  [63/213], [94mLoss[0m : 2.63061
[1mStep[0m  [84/213], [94mLoss[0m : 2.69729
[1mStep[0m  [105/213], [94mLoss[0m : 2.97606
[1mStep[0m  [126/213], [94mLoss[0m : 2.08768
[1mStep[0m  [147/213], [94mLoss[0m : 2.43416
[1mStep[0m  [168/213], [94mLoss[0m : 2.26641
[1mStep[0m  [189/213], [94mLoss[0m : 2.64679
[1mStep[0m  [210/213], [94mLoss[0m : 2.66695

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.38918
[1mStep[0m  [21/213], [94mLoss[0m : 2.75095
[1mStep[0m  [42/213], [94mLoss[0m : 2.51040
[1mStep[0m  [63/213], [94mLoss[0m : 2.51748
[1mStep[0m  [84/213], [94mLoss[0m : 2.32279
[1mStep[0m  [105/213], [94mLoss[0m : 2.25761
[1mStep[0m  [126/213], [94mLoss[0m : 2.31546
[1mStep[0m  [147/213], [94mLoss[0m : 2.30740
[1mStep[0m  [168/213], [94mLoss[0m : 2.20106
[1mStep[0m  [189/213], [94mLoss[0m : 2.68331
[1mStep[0m  [210/213], [94mLoss[0m : 2.65037

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.72527
[1mStep[0m  [21/213], [94mLoss[0m : 2.28158
[1mStep[0m  [42/213], [94mLoss[0m : 2.94709
[1mStep[0m  [63/213], [94mLoss[0m : 2.71138
[1mStep[0m  [84/213], [94mLoss[0m : 1.78650
[1mStep[0m  [105/213], [94mLoss[0m : 2.52853
[1mStep[0m  [126/213], [94mLoss[0m : 2.26025
[1mStep[0m  [147/213], [94mLoss[0m : 2.47365
[1mStep[0m  [168/213], [94mLoss[0m : 2.76572
[1mStep[0m  [189/213], [94mLoss[0m : 2.49110
[1mStep[0m  [210/213], [94mLoss[0m : 2.07446

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.19897
[1mStep[0m  [21/213], [94mLoss[0m : 2.62721
[1mStep[0m  [42/213], [94mLoss[0m : 2.45955
[1mStep[0m  [63/213], [94mLoss[0m : 2.53354
[1mStep[0m  [84/213], [94mLoss[0m : 2.48051
[1mStep[0m  [105/213], [94mLoss[0m : 2.34628
[1mStep[0m  [126/213], [94mLoss[0m : 1.87561
[1mStep[0m  [147/213], [94mLoss[0m : 2.70259
[1mStep[0m  [168/213], [94mLoss[0m : 2.37652
[1mStep[0m  [189/213], [94mLoss[0m : 2.44711
[1mStep[0m  [210/213], [94mLoss[0m : 2.77402

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.400, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50216
[1mStep[0m  [21/213], [94mLoss[0m : 2.28054
[1mStep[0m  [42/213], [94mLoss[0m : 2.63695
[1mStep[0m  [63/213], [94mLoss[0m : 2.99927
[1mStep[0m  [84/213], [94mLoss[0m : 2.12438
[1mStep[0m  [105/213], [94mLoss[0m : 2.29818
[1mStep[0m  [126/213], [94mLoss[0m : 2.68276
[1mStep[0m  [147/213], [94mLoss[0m : 2.40465
[1mStep[0m  [168/213], [94mLoss[0m : 2.41183
[1mStep[0m  [189/213], [94mLoss[0m : 2.35020
[1mStep[0m  [210/213], [94mLoss[0m : 2.28614

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.20524
[1mStep[0m  [21/213], [94mLoss[0m : 2.04201
[1mStep[0m  [42/213], [94mLoss[0m : 1.86954
[1mStep[0m  [63/213], [94mLoss[0m : 2.37839
[1mStep[0m  [84/213], [94mLoss[0m : 2.46872
[1mStep[0m  [105/213], [94mLoss[0m : 2.17297
[1mStep[0m  [126/213], [94mLoss[0m : 2.50815
[1mStep[0m  [147/213], [94mLoss[0m : 2.39185
[1mStep[0m  [168/213], [94mLoss[0m : 2.21397
[1mStep[0m  [189/213], [94mLoss[0m : 1.90378
[1mStep[0m  [210/213], [94mLoss[0m : 2.34835

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.10084
[1mStep[0m  [21/213], [94mLoss[0m : 2.61160
[1mStep[0m  [42/213], [94mLoss[0m : 2.34100
[1mStep[0m  [63/213], [94mLoss[0m : 2.21376
[1mStep[0m  [84/213], [94mLoss[0m : 2.09309
[1mStep[0m  [105/213], [94mLoss[0m : 2.02498
[1mStep[0m  [126/213], [94mLoss[0m : 2.40379
[1mStep[0m  [147/213], [94mLoss[0m : 1.95536
[1mStep[0m  [168/213], [94mLoss[0m : 2.80552
[1mStep[0m  [189/213], [94mLoss[0m : 2.45806
[1mStep[0m  [210/213], [94mLoss[0m : 2.55275

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.393, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.54257
[1mStep[0m  [21/213], [94mLoss[0m : 1.99208
[1mStep[0m  [42/213], [94mLoss[0m : 2.23432
[1mStep[0m  [63/213], [94mLoss[0m : 2.07485
[1mStep[0m  [84/213], [94mLoss[0m : 2.54942
[1mStep[0m  [105/213], [94mLoss[0m : 2.21437
[1mStep[0m  [126/213], [94mLoss[0m : 2.33761
[1mStep[0m  [147/213], [94mLoss[0m : 2.45608
[1mStep[0m  [168/213], [94mLoss[0m : 2.58486
[1mStep[0m  [189/213], [94mLoss[0m : 2.33641
[1mStep[0m  [210/213], [94mLoss[0m : 2.63618

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.399, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08356
[1mStep[0m  [21/213], [94mLoss[0m : 2.39475
[1mStep[0m  [42/213], [94mLoss[0m : 2.20336
[1mStep[0m  [63/213], [94mLoss[0m : 2.28641
[1mStep[0m  [84/213], [94mLoss[0m : 2.40649
[1mStep[0m  [105/213], [94mLoss[0m : 2.69432
[1mStep[0m  [126/213], [94mLoss[0m : 1.87892
[1mStep[0m  [147/213], [94mLoss[0m : 2.39182
[1mStep[0m  [168/213], [94mLoss[0m : 2.05760
[1mStep[0m  [189/213], [94mLoss[0m : 2.15004
[1mStep[0m  [210/213], [94mLoss[0m : 2.44269

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.387, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.60208
[1mStep[0m  [21/213], [94mLoss[0m : 2.11473
[1mStep[0m  [42/213], [94mLoss[0m : 1.92973
[1mStep[0m  [63/213], [94mLoss[0m : 2.08667
[1mStep[0m  [84/213], [94mLoss[0m : 2.41144
[1mStep[0m  [105/213], [94mLoss[0m : 2.68865
[1mStep[0m  [126/213], [94mLoss[0m : 2.07974
[1mStep[0m  [147/213], [94mLoss[0m : 1.95952
[1mStep[0m  [168/213], [94mLoss[0m : 2.33336
[1mStep[0m  [189/213], [94mLoss[0m : 2.19243
[1mStep[0m  [210/213], [94mLoss[0m : 2.33210

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.240, [92mTest[0m: 2.432, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.26173
[1mStep[0m  [21/213], [94mLoss[0m : 2.07483
[1mStep[0m  [42/213], [94mLoss[0m : 2.23664
[1mStep[0m  [63/213], [94mLoss[0m : 2.36165
[1mStep[0m  [84/213], [94mLoss[0m : 1.96940
[1mStep[0m  [105/213], [94mLoss[0m : 1.89120
[1mStep[0m  [126/213], [94mLoss[0m : 2.22814
[1mStep[0m  [147/213], [94mLoss[0m : 2.55229
[1mStep[0m  [168/213], [94mLoss[0m : 1.99475
[1mStep[0m  [189/213], [94mLoss[0m : 2.05034
[1mStep[0m  [210/213], [94mLoss[0m : 2.22316

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.204, [92mTest[0m: 2.460, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.18525
[1mStep[0m  [21/213], [94mLoss[0m : 2.51145
[1mStep[0m  [42/213], [94mLoss[0m : 1.93312
[1mStep[0m  [63/213], [94mLoss[0m : 2.31720
[1mStep[0m  [84/213], [94mLoss[0m : 2.16988
[1mStep[0m  [105/213], [94mLoss[0m : 2.02545
[1mStep[0m  [126/213], [94mLoss[0m : 2.06158
[1mStep[0m  [147/213], [94mLoss[0m : 2.51622
[1mStep[0m  [168/213], [94mLoss[0m : 2.22351
[1mStep[0m  [189/213], [94mLoss[0m : 2.42440
[1mStep[0m  [210/213], [94mLoss[0m : 1.79625

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.147, [92mTest[0m: 2.505, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.08447
[1mStep[0m  [21/213], [94mLoss[0m : 2.50846
[1mStep[0m  [42/213], [94mLoss[0m : 1.83641
[1mStep[0m  [63/213], [94mLoss[0m : 2.08505
[1mStep[0m  [84/213], [94mLoss[0m : 1.80982
[1mStep[0m  [105/213], [94mLoss[0m : 1.88165
[1mStep[0m  [126/213], [94mLoss[0m : 2.15938
[1mStep[0m  [147/213], [94mLoss[0m : 2.12034
[1mStep[0m  [168/213], [94mLoss[0m : 2.19867
[1mStep[0m  [189/213], [94mLoss[0m : 2.15238
[1mStep[0m  [210/213], [94mLoss[0m : 2.34214

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.397, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.98472
[1mStep[0m  [21/213], [94mLoss[0m : 2.03962
[1mStep[0m  [42/213], [94mLoss[0m : 1.75004
[1mStep[0m  [63/213], [94mLoss[0m : 2.09233
[1mStep[0m  [84/213], [94mLoss[0m : 2.01471
[1mStep[0m  [105/213], [94mLoss[0m : 1.94455
[1mStep[0m  [126/213], [94mLoss[0m : 1.93910
[1mStep[0m  [147/213], [94mLoss[0m : 2.16571
[1mStep[0m  [168/213], [94mLoss[0m : 2.13429
[1mStep[0m  [189/213], [94mLoss[0m : 2.00267
[1mStep[0m  [210/213], [94mLoss[0m : 2.31647

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09047
[1mStep[0m  [21/213], [94mLoss[0m : 2.24666
[1mStep[0m  [42/213], [94mLoss[0m : 2.12583
[1mStep[0m  [63/213], [94mLoss[0m : 1.91270
[1mStep[0m  [84/213], [94mLoss[0m : 1.93610
[1mStep[0m  [105/213], [94mLoss[0m : 1.77454
[1mStep[0m  [126/213], [94mLoss[0m : 2.26813
[1mStep[0m  [147/213], [94mLoss[0m : 2.36641
[1mStep[0m  [168/213], [94mLoss[0m : 2.47880
[1mStep[0m  [189/213], [94mLoss[0m : 1.94148
[1mStep[0m  [210/213], [94mLoss[0m : 2.06212

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.062, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.12040
[1mStep[0m  [21/213], [94mLoss[0m : 2.27662
[1mStep[0m  [42/213], [94mLoss[0m : 1.83394
[1mStep[0m  [63/213], [94mLoss[0m : 1.89636
[1mStep[0m  [84/213], [94mLoss[0m : 1.80460
[1mStep[0m  [105/213], [94mLoss[0m : 1.93078
[1mStep[0m  [126/213], [94mLoss[0m : 2.30108
[1mStep[0m  [147/213], [94mLoss[0m : 2.27126
[1mStep[0m  [168/213], [94mLoss[0m : 2.47080
[1mStep[0m  [189/213], [94mLoss[0m : 2.36947
[1mStep[0m  [210/213], [94mLoss[0m : 2.07272

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.421, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.89850
[1mStep[0m  [21/213], [94mLoss[0m : 2.17496
[1mStep[0m  [42/213], [94mLoss[0m : 2.00640
[1mStep[0m  [63/213], [94mLoss[0m : 2.05686
[1mStep[0m  [84/213], [94mLoss[0m : 1.94305
[1mStep[0m  [105/213], [94mLoss[0m : 2.30992
[1mStep[0m  [126/213], [94mLoss[0m : 2.06540
[1mStep[0m  [147/213], [94mLoss[0m : 2.30424
[1mStep[0m  [168/213], [94mLoss[0m : 1.62123
[1mStep[0m  [189/213], [94mLoss[0m : 2.14377
[1mStep[0m  [210/213], [94mLoss[0m : 1.75896

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.438, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.79518
[1mStep[0m  [21/213], [94mLoss[0m : 1.80740
[1mStep[0m  [42/213], [94mLoss[0m : 2.20554
[1mStep[0m  [63/213], [94mLoss[0m : 2.01609
[1mStep[0m  [84/213], [94mLoss[0m : 1.96471
[1mStep[0m  [105/213], [94mLoss[0m : 1.59657
[1mStep[0m  [126/213], [94mLoss[0m : 1.74911
[1mStep[0m  [147/213], [94mLoss[0m : 2.31335
[1mStep[0m  [168/213], [94mLoss[0m : 1.80661
[1mStep[0m  [189/213], [94mLoss[0m : 1.89551
[1mStep[0m  [210/213], [94mLoss[0m : 2.16381

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.97898
[1mStep[0m  [21/213], [94mLoss[0m : 1.71698
[1mStep[0m  [42/213], [94mLoss[0m : 1.97289
[1mStep[0m  [63/213], [94mLoss[0m : 1.69825
[1mStep[0m  [84/213], [94mLoss[0m : 1.92725
[1mStep[0m  [105/213], [94mLoss[0m : 2.02711
[1mStep[0m  [126/213], [94mLoss[0m : 2.06943
[1mStep[0m  [147/213], [94mLoss[0m : 1.81386
[1mStep[0m  [168/213], [94mLoss[0m : 1.70383
[1mStep[0m  [189/213], [94mLoss[0m : 2.31996
[1mStep[0m  [210/213], [94mLoss[0m : 1.71373

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.69584
[1mStep[0m  [21/213], [94mLoss[0m : 1.94920
[1mStep[0m  [42/213], [94mLoss[0m : 1.76327
[1mStep[0m  [63/213], [94mLoss[0m : 1.91387
[1mStep[0m  [84/213], [94mLoss[0m : 1.58302
[1mStep[0m  [105/213], [94mLoss[0m : 2.03500
[1mStep[0m  [126/213], [94mLoss[0m : 2.06665
[1mStep[0m  [147/213], [94mLoss[0m : 2.01776
[1mStep[0m  [168/213], [94mLoss[0m : 1.87892
[1mStep[0m  [189/213], [94mLoss[0m : 2.30019
[1mStep[0m  [210/213], [94mLoss[0m : 1.69506

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.924, [92mTest[0m: 2.464, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.52886
[1mStep[0m  [21/213], [94mLoss[0m : 1.80890
[1mStep[0m  [42/213], [94mLoss[0m : 2.02180
[1mStep[0m  [63/213], [94mLoss[0m : 1.68700
[1mStep[0m  [84/213], [94mLoss[0m : 1.87582
[1mStep[0m  [105/213], [94mLoss[0m : 1.54735
[1mStep[0m  [126/213], [94mLoss[0m : 1.89019
[1mStep[0m  [147/213], [94mLoss[0m : 1.73235
[1mStep[0m  [168/213], [94mLoss[0m : 1.75709
[1mStep[0m  [189/213], [94mLoss[0m : 1.96258
[1mStep[0m  [210/213], [94mLoss[0m : 1.67062

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.898, [92mTest[0m: 2.444, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.80741
[1mStep[0m  [21/213], [94mLoss[0m : 1.96729
[1mStep[0m  [42/213], [94mLoss[0m : 1.86180
[1mStep[0m  [63/213], [94mLoss[0m : 2.05524
[1mStep[0m  [84/213], [94mLoss[0m : 1.79817
[1mStep[0m  [105/213], [94mLoss[0m : 1.65848
[1mStep[0m  [126/213], [94mLoss[0m : 1.69045
[1mStep[0m  [147/213], [94mLoss[0m : 1.89436
[1mStep[0m  [168/213], [94mLoss[0m : 1.82200
[1mStep[0m  [189/213], [94mLoss[0m : 1.74349
[1mStep[0m  [210/213], [94mLoss[0m : 1.94479

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.869, [92mTest[0m: 2.468, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.94555
[1mStep[0m  [21/213], [94mLoss[0m : 1.85225
[1mStep[0m  [42/213], [94mLoss[0m : 2.03086
[1mStep[0m  [63/213], [94mLoss[0m : 1.70487
[1mStep[0m  [84/213], [94mLoss[0m : 1.96120
[1mStep[0m  [105/213], [94mLoss[0m : 1.68925
[1mStep[0m  [126/213], [94mLoss[0m : 1.72139
[1mStep[0m  [147/213], [94mLoss[0m : 1.89519
[1mStep[0m  [168/213], [94mLoss[0m : 1.82861
[1mStep[0m  [189/213], [94mLoss[0m : 2.27144
[1mStep[0m  [210/213], [94mLoss[0m : 2.01041

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.453, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.67827
[1mStep[0m  [21/213], [94mLoss[0m : 1.76819
[1mStep[0m  [42/213], [94mLoss[0m : 1.62710
[1mStep[0m  [63/213], [94mLoss[0m : 1.55062
[1mStep[0m  [84/213], [94mLoss[0m : 2.06169
[1mStep[0m  [105/213], [94mLoss[0m : 1.57647
[1mStep[0m  [126/213], [94mLoss[0m : 1.73882
[1mStep[0m  [147/213], [94mLoss[0m : 1.70213
[1mStep[0m  [168/213], [94mLoss[0m : 2.09590
[1mStep[0m  [189/213], [94mLoss[0m : 1.63221
[1mStep[0m  [210/213], [94mLoss[0m : 2.17293

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.828, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.71353
[1mStep[0m  [21/213], [94mLoss[0m : 1.91949
[1mStep[0m  [42/213], [94mLoss[0m : 1.76410
[1mStep[0m  [63/213], [94mLoss[0m : 1.56813
[1mStep[0m  [84/213], [94mLoss[0m : 1.61341
[1mStep[0m  [105/213], [94mLoss[0m : 1.80688
[1mStep[0m  [126/213], [94mLoss[0m : 1.87899
[1mStep[0m  [147/213], [94mLoss[0m : 2.02373
[1mStep[0m  [168/213], [94mLoss[0m : 1.91235
[1mStep[0m  [189/213], [94mLoss[0m : 1.80723
[1mStep[0m  [210/213], [94mLoss[0m : 1.91032

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.447, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.87167
[1mStep[0m  [21/213], [94mLoss[0m : 1.52540
[1mStep[0m  [42/213], [94mLoss[0m : 1.64368
[1mStep[0m  [63/213], [94mLoss[0m : 1.62839
[1mStep[0m  [84/213], [94mLoss[0m : 1.57748
[1mStep[0m  [105/213], [94mLoss[0m : 1.98220
[1mStep[0m  [126/213], [94mLoss[0m : 1.60632
[1mStep[0m  [147/213], [94mLoss[0m : 1.62176
[1mStep[0m  [168/213], [94mLoss[0m : 1.87807
[1mStep[0m  [189/213], [94mLoss[0m : 1.57107
[1mStep[0m  [210/213], [94mLoss[0m : 1.84521

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.434, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.76918
[1mStep[0m  [21/213], [94mLoss[0m : 1.75521
[1mStep[0m  [42/213], [94mLoss[0m : 1.91211
[1mStep[0m  [63/213], [94mLoss[0m : 1.63590
[1mStep[0m  [84/213], [94mLoss[0m : 1.77138
[1mStep[0m  [105/213], [94mLoss[0m : 1.95493
[1mStep[0m  [126/213], [94mLoss[0m : 1.44676
[1mStep[0m  [147/213], [94mLoss[0m : 1.92404
[1mStep[0m  [168/213], [94mLoss[0m : 1.73983
[1mStep[0m  [189/213], [94mLoss[0m : 1.61954
[1mStep[0m  [210/213], [94mLoss[0m : 1.78913

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.449, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.90429
[1mStep[0m  [21/213], [94mLoss[0m : 1.65489
[1mStep[0m  [42/213], [94mLoss[0m : 1.97051
[1mStep[0m  [63/213], [94mLoss[0m : 1.25238
[1mStep[0m  [84/213], [94mLoss[0m : 1.74037
[1mStep[0m  [105/213], [94mLoss[0m : 1.77588
[1mStep[0m  [126/213], [94mLoss[0m : 1.56403
[1mStep[0m  [147/213], [94mLoss[0m : 1.71401
[1mStep[0m  [168/213], [94mLoss[0m : 2.20563
[1mStep[0m  [189/213], [94mLoss[0m : 1.88326
[1mStep[0m  [210/213], [94mLoss[0m : 2.15488

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.761, [92mTest[0m: 2.446, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.478
====================================

Phase 2 - Evaluation MAE:  2.4777057834391325
MAE score P1        2.40352
MAE score P2       2.477706
loss               1.761218
learning_rate      0.002575
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay          0.001
Name: 10, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.001, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 10.63229
[1mStep[0m  [21/213], [94mLoss[0m : 7.04105
[1mStep[0m  [42/213], [94mLoss[0m : 3.43477
[1mStep[0m  [63/213], [94mLoss[0m : 2.53222
[1mStep[0m  [84/213], [94mLoss[0m : 2.35705
[1mStep[0m  [105/213], [94mLoss[0m : 2.42800
[1mStep[0m  [126/213], [94mLoss[0m : 2.43585
[1mStep[0m  [147/213], [94mLoss[0m : 2.53759
[1mStep[0m  [168/213], [94mLoss[0m : 2.39791
[1mStep[0m  [189/213], [94mLoss[0m : 2.45102
[1mStep[0m  [210/213], [94mLoss[0m : 2.59738

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.739, [92mTest[0m: 10.723, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.42155
[1mStep[0m  [21/213], [94mLoss[0m : 3.19191
[1mStep[0m  [42/213], [94mLoss[0m : 3.09704
[1mStep[0m  [63/213], [94mLoss[0m : 2.24319
[1mStep[0m  [84/213], [94mLoss[0m : 3.22172
[1mStep[0m  [105/213], [94mLoss[0m : 2.59622
[1mStep[0m  [126/213], [94mLoss[0m : 2.74467
[1mStep[0m  [147/213], [94mLoss[0m : 2.74593
[1mStep[0m  [168/213], [94mLoss[0m : 3.09617
[1mStep[0m  [189/213], [94mLoss[0m : 2.63346
[1mStep[0m  [210/213], [94mLoss[0m : 2.77545

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.628, [92mTest[0m: 2.528, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.50467
[1mStep[0m  [21/213], [94mLoss[0m : 2.81610
[1mStep[0m  [42/213], [94mLoss[0m : 2.60038
[1mStep[0m  [63/213], [94mLoss[0m : 2.51972
[1mStep[0m  [84/213], [94mLoss[0m : 2.71698
[1mStep[0m  [105/213], [94mLoss[0m : 2.45700
[1mStep[0m  [126/213], [94mLoss[0m : 2.38663
[1mStep[0m  [147/213], [94mLoss[0m : 2.45982
[1mStep[0m  [168/213], [94mLoss[0m : 2.36394
[1mStep[0m  [189/213], [94mLoss[0m : 3.29145
[1mStep[0m  [210/213], [94mLoss[0m : 2.70694

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.457, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.33453
[1mStep[0m  [21/213], [94mLoss[0m : 2.35690
[1mStep[0m  [42/213], [94mLoss[0m : 2.77413
[1mStep[0m  [63/213], [94mLoss[0m : 2.91713
[1mStep[0m  [84/213], [94mLoss[0m : 2.49011
[1mStep[0m  [105/213], [94mLoss[0m : 3.01631
[1mStep[0m  [126/213], [94mLoss[0m : 2.57645
[1mStep[0m  [147/213], [94mLoss[0m : 2.52775
[1mStep[0m  [168/213], [94mLoss[0m : 2.34716
[1mStep[0m  [189/213], [94mLoss[0m : 3.04664
[1mStep[0m  [210/213], [94mLoss[0m : 2.04636

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.542, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32426
[1mStep[0m  [21/213], [94mLoss[0m : 2.15207
[1mStep[0m  [42/213], [94mLoss[0m : 2.54809
[1mStep[0m  [63/213], [94mLoss[0m : 2.77230
[1mStep[0m  [84/213], [94mLoss[0m : 2.63962
[1mStep[0m  [105/213], [94mLoss[0m : 2.69024
[1mStep[0m  [126/213], [94mLoss[0m : 1.95570
[1mStep[0m  [147/213], [94mLoss[0m : 2.31525
[1mStep[0m  [168/213], [94mLoss[0m : 3.05117
[1mStep[0m  [189/213], [94mLoss[0m : 2.01125
[1mStep[0m  [210/213], [94mLoss[0m : 2.10554

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.61860
[1mStep[0m  [21/213], [94mLoss[0m : 2.53106
[1mStep[0m  [42/213], [94mLoss[0m : 2.27955
[1mStep[0m  [63/213], [94mLoss[0m : 2.35962
[1mStep[0m  [84/213], [94mLoss[0m : 2.29941
[1mStep[0m  [105/213], [94mLoss[0m : 2.88154
[1mStep[0m  [126/213], [94mLoss[0m : 2.56677
[1mStep[0m  [147/213], [94mLoss[0m : 2.03767
[1mStep[0m  [168/213], [94mLoss[0m : 2.10229
[1mStep[0m  [189/213], [94mLoss[0m : 2.56126
[1mStep[0m  [210/213], [94mLoss[0m : 2.43868

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.15689
[1mStep[0m  [21/213], [94mLoss[0m : 2.22166
[1mStep[0m  [42/213], [94mLoss[0m : 2.17958
[1mStep[0m  [63/213], [94mLoss[0m : 2.14398
[1mStep[0m  [84/213], [94mLoss[0m : 2.21655
[1mStep[0m  [105/213], [94mLoss[0m : 2.70052
[1mStep[0m  [126/213], [94mLoss[0m : 2.18025
[1mStep[0m  [147/213], [94mLoss[0m : 2.76256
[1mStep[0m  [168/213], [94mLoss[0m : 2.23952
[1mStep[0m  [189/213], [94mLoss[0m : 2.48259
[1mStep[0m  [210/213], [94mLoss[0m : 2.38317

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.64001
[1mStep[0m  [21/213], [94mLoss[0m : 2.34807
[1mStep[0m  [42/213], [94mLoss[0m : 2.41101
[1mStep[0m  [63/213], [94mLoss[0m : 2.41901
[1mStep[0m  [84/213], [94mLoss[0m : 2.47056
[1mStep[0m  [105/213], [94mLoss[0m : 2.45876
[1mStep[0m  [126/213], [94mLoss[0m : 2.45061
[1mStep[0m  [147/213], [94mLoss[0m : 2.88904
[1mStep[0m  [168/213], [94mLoss[0m : 2.56437
[1mStep[0m  [189/213], [94mLoss[0m : 2.45594
[1mStep[0m  [210/213], [94mLoss[0m : 2.74365

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 3.02635
[1mStep[0m  [21/213], [94mLoss[0m : 2.50464
[1mStep[0m  [42/213], [94mLoss[0m : 2.49467
[1mStep[0m  [63/213], [94mLoss[0m : 2.56126
[1mStep[0m  [84/213], [94mLoss[0m : 2.49622
[1mStep[0m  [105/213], [94mLoss[0m : 2.54917
[1mStep[0m  [126/213], [94mLoss[0m : 2.65887
[1mStep[0m  [147/213], [94mLoss[0m : 1.94741
[1mStep[0m  [168/213], [94mLoss[0m : 2.04092
[1mStep[0m  [189/213], [94mLoss[0m : 2.38911
[1mStep[0m  [210/213], [94mLoss[0m : 2.17479

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.386, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23045
[1mStep[0m  [21/213], [94mLoss[0m : 2.88398
[1mStep[0m  [42/213], [94mLoss[0m : 2.47073
[1mStep[0m  [63/213], [94mLoss[0m : 2.53523
[1mStep[0m  [84/213], [94mLoss[0m : 2.37348
[1mStep[0m  [105/213], [94mLoss[0m : 2.39850
[1mStep[0m  [126/213], [94mLoss[0m : 2.56470
[1mStep[0m  [147/213], [94mLoss[0m : 2.00419
[1mStep[0m  [168/213], [94mLoss[0m : 2.10107
[1mStep[0m  [189/213], [94mLoss[0m : 2.77112
[1mStep[0m  [210/213], [94mLoss[0m : 2.43075

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32330
[1mStep[0m  [21/213], [94mLoss[0m : 2.44735
[1mStep[0m  [42/213], [94mLoss[0m : 2.72950
[1mStep[0m  [63/213], [94mLoss[0m : 2.35337
[1mStep[0m  [84/213], [94mLoss[0m : 2.58402
[1mStep[0m  [105/213], [94mLoss[0m : 2.34874
[1mStep[0m  [126/213], [94mLoss[0m : 2.95082
[1mStep[0m  [147/213], [94mLoss[0m : 2.29398
[1mStep[0m  [168/213], [94mLoss[0m : 2.41209
[1mStep[0m  [189/213], [94mLoss[0m : 2.17061
[1mStep[0m  [210/213], [94mLoss[0m : 2.13090

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.405, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.09349
[1mStep[0m  [21/213], [94mLoss[0m : 2.21733
[1mStep[0m  [42/213], [94mLoss[0m : 2.26830
[1mStep[0m  [63/213], [94mLoss[0m : 2.22744
[1mStep[0m  [84/213], [94mLoss[0m : 2.66613
[1mStep[0m  [105/213], [94mLoss[0m : 2.46706
[1mStep[0m  [126/213], [94mLoss[0m : 1.98320
[1mStep[0m  [147/213], [94mLoss[0m : 2.63632
[1mStep[0m  [168/213], [94mLoss[0m : 2.20484
[1mStep[0m  [189/213], [94mLoss[0m : 2.88627
[1mStep[0m  [210/213], [94mLoss[0m : 2.39886

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.376, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.58035
[1mStep[0m  [21/213], [94mLoss[0m : 2.03022
[1mStep[0m  [42/213], [94mLoss[0m : 2.44980
[1mStep[0m  [63/213], [94mLoss[0m : 2.12768
[1mStep[0m  [84/213], [94mLoss[0m : 2.25935
[1mStep[0m  [105/213], [94mLoss[0m : 2.46118
[1mStep[0m  [126/213], [94mLoss[0m : 2.41260
[1mStep[0m  [147/213], [94mLoss[0m : 2.43360
[1mStep[0m  [168/213], [94mLoss[0m : 2.68644
[1mStep[0m  [189/213], [94mLoss[0m : 2.86333
[1mStep[0m  [210/213], [94mLoss[0m : 2.26092

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.343, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55539
[1mStep[0m  [21/213], [94mLoss[0m : 2.41117
[1mStep[0m  [42/213], [94mLoss[0m : 1.98752
[1mStep[0m  [63/213], [94mLoss[0m : 2.54796
[1mStep[0m  [84/213], [94mLoss[0m : 2.18092
[1mStep[0m  [105/213], [94mLoss[0m : 2.55262
[1mStep[0m  [126/213], [94mLoss[0m : 2.36194
[1mStep[0m  [147/213], [94mLoss[0m : 2.44181
[1mStep[0m  [168/213], [94mLoss[0m : 2.64054
[1mStep[0m  [189/213], [94mLoss[0m : 2.20299
[1mStep[0m  [210/213], [94mLoss[0m : 2.50102

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.380, [92mTest[0m: 2.354, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.31586
[1mStep[0m  [21/213], [94mLoss[0m : 2.69342
[1mStep[0m  [42/213], [94mLoss[0m : 2.50690
[1mStep[0m  [63/213], [94mLoss[0m : 2.08064
[1mStep[0m  [84/213], [94mLoss[0m : 1.78788
[1mStep[0m  [105/213], [94mLoss[0m : 2.65571
[1mStep[0m  [126/213], [94mLoss[0m : 2.22369
[1mStep[0m  [147/213], [94mLoss[0m : 2.39446
[1mStep[0m  [168/213], [94mLoss[0m : 2.74996
[1mStep[0m  [189/213], [94mLoss[0m : 2.45832
[1mStep[0m  [210/213], [94mLoss[0m : 2.33665

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.356, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32564
[1mStep[0m  [21/213], [94mLoss[0m : 2.72734
[1mStep[0m  [42/213], [94mLoss[0m : 2.46951
[1mStep[0m  [63/213], [94mLoss[0m : 2.70032
[1mStep[0m  [84/213], [94mLoss[0m : 2.18625
[1mStep[0m  [105/213], [94mLoss[0m : 2.31080
[1mStep[0m  [126/213], [94mLoss[0m : 2.20285
[1mStep[0m  [147/213], [94mLoss[0m : 2.10466
[1mStep[0m  [168/213], [94mLoss[0m : 2.35906
[1mStep[0m  [189/213], [94mLoss[0m : 2.25532
[1mStep[0m  [210/213], [94mLoss[0m : 2.55280

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.331, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.67824
[1mStep[0m  [21/213], [94mLoss[0m : 2.13961
[1mStep[0m  [42/213], [94mLoss[0m : 1.94605
[1mStep[0m  [63/213], [94mLoss[0m : 2.01791
[1mStep[0m  [84/213], [94mLoss[0m : 2.23663
[1mStep[0m  [105/213], [94mLoss[0m : 2.25991
[1mStep[0m  [126/213], [94mLoss[0m : 2.57068
[1mStep[0m  [147/213], [94mLoss[0m : 2.88683
[1mStep[0m  [168/213], [94mLoss[0m : 2.79014
[1mStep[0m  [189/213], [94mLoss[0m : 2.01148
[1mStep[0m  [210/213], [94mLoss[0m : 2.11688

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.353, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.65444
[1mStep[0m  [21/213], [94mLoss[0m : 2.28327
[1mStep[0m  [42/213], [94mLoss[0m : 2.61714
[1mStep[0m  [63/213], [94mLoss[0m : 2.41459
[1mStep[0m  [84/213], [94mLoss[0m : 2.33317
[1mStep[0m  [105/213], [94mLoss[0m : 1.97344
[1mStep[0m  [126/213], [94mLoss[0m : 2.33516
[1mStep[0m  [147/213], [94mLoss[0m : 1.99099
[1mStep[0m  [168/213], [94mLoss[0m : 2.48443
[1mStep[0m  [189/213], [94mLoss[0m : 2.30064
[1mStep[0m  [210/213], [94mLoss[0m : 2.13947

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.365, [92mTest[0m: 2.318, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.14784
[1mStep[0m  [21/213], [94mLoss[0m : 1.90666
[1mStep[0m  [42/213], [94mLoss[0m : 2.30372
[1mStep[0m  [63/213], [94mLoss[0m : 1.81467
[1mStep[0m  [84/213], [94mLoss[0m : 2.15618
[1mStep[0m  [105/213], [94mLoss[0m : 2.41282
[1mStep[0m  [126/213], [94mLoss[0m : 2.29146
[1mStep[0m  [147/213], [94mLoss[0m : 2.71352
[1mStep[0m  [168/213], [94mLoss[0m : 2.58961
[1mStep[0m  [189/213], [94mLoss[0m : 2.36486
[1mStep[0m  [210/213], [94mLoss[0m : 1.95776

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.347, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.32716
[1mStep[0m  [21/213], [94mLoss[0m : 2.15096
[1mStep[0m  [42/213], [94mLoss[0m : 2.07316
[1mStep[0m  [63/213], [94mLoss[0m : 2.06065
[1mStep[0m  [84/213], [94mLoss[0m : 2.02410
[1mStep[0m  [105/213], [94mLoss[0m : 2.48596
[1mStep[0m  [126/213], [94mLoss[0m : 2.34361
[1mStep[0m  [147/213], [94mLoss[0m : 2.63387
[1mStep[0m  [168/213], [94mLoss[0m : 2.15546
[1mStep[0m  [189/213], [94mLoss[0m : 2.51124
[1mStep[0m  [210/213], [94mLoss[0m : 2.15212

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.47665
[1mStep[0m  [21/213], [94mLoss[0m : 2.35636
[1mStep[0m  [42/213], [94mLoss[0m : 2.29831
[1mStep[0m  [63/213], [94mLoss[0m : 2.37901
[1mStep[0m  [84/213], [94mLoss[0m : 2.22624
[1mStep[0m  [105/213], [94mLoss[0m : 2.43061
[1mStep[0m  [126/213], [94mLoss[0m : 2.27124
[1mStep[0m  [147/213], [94mLoss[0m : 2.59098
[1mStep[0m  [168/213], [94mLoss[0m : 2.20509
[1mStep[0m  [189/213], [94mLoss[0m : 2.40276
[1mStep[0m  [210/213], [94mLoss[0m : 1.90881

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.346, [92mTest[0m: 2.334, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.34940
[1mStep[0m  [21/213], [94mLoss[0m : 2.15221
[1mStep[0m  [42/213], [94mLoss[0m : 2.19081
[1mStep[0m  [63/213], [94mLoss[0m : 2.33957
[1mStep[0m  [84/213], [94mLoss[0m : 2.23834
[1mStep[0m  [105/213], [94mLoss[0m : 2.33920
[1mStep[0m  [126/213], [94mLoss[0m : 2.20815
[1mStep[0m  [147/213], [94mLoss[0m : 2.37637
[1mStep[0m  [168/213], [94mLoss[0m : 2.39018
[1mStep[0m  [189/213], [94mLoss[0m : 2.79681
[1mStep[0m  [210/213], [94mLoss[0m : 2.27534

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.344, [92mTest[0m: 2.335, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.05584
[1mStep[0m  [21/213], [94mLoss[0m : 1.98728
[1mStep[0m  [42/213], [94mLoss[0m : 2.12925
[1mStep[0m  [63/213], [94mLoss[0m : 2.16238
[1mStep[0m  [84/213], [94mLoss[0m : 2.05789
[1mStep[0m  [105/213], [94mLoss[0m : 2.42761
[1mStep[0m  [126/213], [94mLoss[0m : 2.07609
[1mStep[0m  [147/213], [94mLoss[0m : 1.93626
[1mStep[0m  [168/213], [94mLoss[0m : 2.44489
[1mStep[0m  [189/213], [94mLoss[0m : 2.72835
[1mStep[0m  [210/213], [94mLoss[0m : 2.28365

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.341, [92mTest[0m: 2.348, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.36160
[1mStep[0m  [21/213], [94mLoss[0m : 2.15814
[1mStep[0m  [42/213], [94mLoss[0m : 2.32663
[1mStep[0m  [63/213], [94mLoss[0m : 2.44678
[1mStep[0m  [84/213], [94mLoss[0m : 2.38909
[1mStep[0m  [105/213], [94mLoss[0m : 2.63118
[1mStep[0m  [126/213], [94mLoss[0m : 2.07409
[1mStep[0m  [147/213], [94mLoss[0m : 2.05661
[1mStep[0m  [168/213], [94mLoss[0m : 2.35241
[1mStep[0m  [189/213], [94mLoss[0m : 2.50598
[1mStep[0m  [210/213], [94mLoss[0m : 1.99584

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.310, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.28614
[1mStep[0m  [21/213], [94mLoss[0m : 2.47459
[1mStep[0m  [42/213], [94mLoss[0m : 2.47662
[1mStep[0m  [63/213], [94mLoss[0m : 2.70357
[1mStep[0m  [84/213], [94mLoss[0m : 2.33971
[1mStep[0m  [105/213], [94mLoss[0m : 2.39954
[1mStep[0m  [126/213], [94mLoss[0m : 2.33580
[1mStep[0m  [147/213], [94mLoss[0m : 2.35940
[1mStep[0m  [168/213], [94mLoss[0m : 1.95282
[1mStep[0m  [189/213], [94mLoss[0m : 2.16885
[1mStep[0m  [210/213], [94mLoss[0m : 2.53731

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.323, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.40117
[1mStep[0m  [21/213], [94mLoss[0m : 2.14474
[1mStep[0m  [42/213], [94mLoss[0m : 2.29083
[1mStep[0m  [63/213], [94mLoss[0m : 2.36962
[1mStep[0m  [84/213], [94mLoss[0m : 2.27679
[1mStep[0m  [105/213], [94mLoss[0m : 2.28529
[1mStep[0m  [126/213], [94mLoss[0m : 2.49885
[1mStep[0m  [147/213], [94mLoss[0m : 2.55298
[1mStep[0m  [168/213], [94mLoss[0m : 2.50244
[1mStep[0m  [189/213], [94mLoss[0m : 2.13393
[1mStep[0m  [210/213], [94mLoss[0m : 2.15346

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.326, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.30834
[1mStep[0m  [21/213], [94mLoss[0m : 2.10156
[1mStep[0m  [42/213], [94mLoss[0m : 2.49302
[1mStep[0m  [63/213], [94mLoss[0m : 2.16128
[1mStep[0m  [84/213], [94mLoss[0m : 2.09613
[1mStep[0m  [105/213], [94mLoss[0m : 2.81741
[1mStep[0m  [126/213], [94mLoss[0m : 2.85442
[1mStep[0m  [147/213], [94mLoss[0m : 2.16145
[1mStep[0m  [168/213], [94mLoss[0m : 2.09505
[1mStep[0m  [189/213], [94mLoss[0m : 2.39571
[1mStep[0m  [210/213], [94mLoss[0m : 2.73830

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.333, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92767
[1mStep[0m  [21/213], [94mLoss[0m : 2.52890
[1mStep[0m  [42/213], [94mLoss[0m : 2.03451
[1mStep[0m  [63/213], [94mLoss[0m : 2.03716
[1mStep[0m  [84/213], [94mLoss[0m : 2.07100
[1mStep[0m  [105/213], [94mLoss[0m : 2.25090
[1mStep[0m  [126/213], [94mLoss[0m : 1.92672
[1mStep[0m  [147/213], [94mLoss[0m : 2.43957
[1mStep[0m  [168/213], [94mLoss[0m : 2.63732
[1mStep[0m  [189/213], [94mLoss[0m : 2.33572
[1mStep[0m  [210/213], [94mLoss[0m : 2.18948

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.308, [92mTest[0m: 2.343, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.23352
[1mStep[0m  [21/213], [94mLoss[0m : 2.36348
[1mStep[0m  [42/213], [94mLoss[0m : 2.47890
[1mStep[0m  [63/213], [94mLoss[0m : 2.30461
[1mStep[0m  [84/213], [94mLoss[0m : 2.31730
[1mStep[0m  [105/213], [94mLoss[0m : 2.44246
[1mStep[0m  [126/213], [94mLoss[0m : 2.33505
[1mStep[0m  [147/213], [94mLoss[0m : 2.16657
[1mStep[0m  [168/213], [94mLoss[0m : 2.25122
[1mStep[0m  [189/213], [94mLoss[0m : 2.31100
[1mStep[0m  [210/213], [94mLoss[0m : 2.05576

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.383, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.55041
[1mStep[0m  [21/213], [94mLoss[0m : 2.51794
[1mStep[0m  [42/213], [94mLoss[0m : 2.54739
[1mStep[0m  [63/213], [94mLoss[0m : 1.98347
[1mStep[0m  [84/213], [94mLoss[0m : 2.25374
[1mStep[0m  [105/213], [94mLoss[0m : 2.32485
[1mStep[0m  [126/213], [94mLoss[0m : 2.47268
[1mStep[0m  [147/213], [94mLoss[0m : 2.47926
[1mStep[0m  [168/213], [94mLoss[0m : 1.94826
[1mStep[0m  [189/213], [94mLoss[0m : 2.24315
[1mStep[0m  [210/213], [94mLoss[0m : 2.26605

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.298, [92mTest[0m: 2.328, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.325
====================================

Phase 1 - Evaluation MAE:  2.3251910333363517
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/213], [94mLoss[0m : 2.22367
[1mStep[0m  [21/213], [94mLoss[0m : 2.38448
[1mStep[0m  [42/213], [94mLoss[0m : 2.45797
[1mStep[0m  [63/213], [94mLoss[0m : 2.51191
[1mStep[0m  [84/213], [94mLoss[0m : 2.56449
[1mStep[0m  [105/213], [94mLoss[0m : 2.20326
[1mStep[0m  [126/213], [94mLoss[0m : 2.90493
[1mStep[0m  [147/213], [94mLoss[0m : 2.26317
[1mStep[0m  [168/213], [94mLoss[0m : 2.37354
[1mStep[0m  [189/213], [94mLoss[0m : 2.40084
[1mStep[0m  [210/213], [94mLoss[0m : 2.83170

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.326, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.25367
[1mStep[0m  [21/213], [94mLoss[0m : 2.42788
[1mStep[0m  [42/213], [94mLoss[0m : 2.13059
[1mStep[0m  [63/213], [94mLoss[0m : 2.34539
[1mStep[0m  [84/213], [94mLoss[0m : 1.84316
[1mStep[0m  [105/213], [94mLoss[0m : 2.19381
[1mStep[0m  [126/213], [94mLoss[0m : 2.23269
[1mStep[0m  [147/213], [94mLoss[0m : 2.24223
[1mStep[0m  [168/213], [94mLoss[0m : 2.33906
[1mStep[0m  [189/213], [94mLoss[0m : 1.86158
[1mStep[0m  [210/213], [94mLoss[0m : 2.10986

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.406, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.13219
[1mStep[0m  [21/213], [94mLoss[0m : 2.05688
[1mStep[0m  [42/213], [94mLoss[0m : 2.29742
[1mStep[0m  [63/213], [94mLoss[0m : 2.28770
[1mStep[0m  [84/213], [94mLoss[0m : 2.56194
[1mStep[0m  [105/213], [94mLoss[0m : 2.33722
[1mStep[0m  [126/213], [94mLoss[0m : 2.10192
[1mStep[0m  [147/213], [94mLoss[0m : 2.02765
[1mStep[0m  [168/213], [94mLoss[0m : 2.42478
[1mStep[0m  [189/213], [94mLoss[0m : 1.97499
[1mStep[0m  [210/213], [94mLoss[0m : 2.03535

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.21200
[1mStep[0m  [21/213], [94mLoss[0m : 2.21146
[1mStep[0m  [42/213], [94mLoss[0m : 2.15902
[1mStep[0m  [63/213], [94mLoss[0m : 1.99603
[1mStep[0m  [84/213], [94mLoss[0m : 2.43076
[1mStep[0m  [105/213], [94mLoss[0m : 2.13911
[1mStep[0m  [126/213], [94mLoss[0m : 2.33644
[1mStep[0m  [147/213], [94mLoss[0m : 2.17189
[1mStep[0m  [168/213], [94mLoss[0m : 2.42703
[1mStep[0m  [189/213], [94mLoss[0m : 2.36952
[1mStep[0m  [210/213], [94mLoss[0m : 2.28583

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.199, [92mTest[0m: 2.372, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.58761
[1mStep[0m  [21/213], [94mLoss[0m : 2.08396
[1mStep[0m  [42/213], [94mLoss[0m : 2.01827
[1mStep[0m  [63/213], [94mLoss[0m : 2.16316
[1mStep[0m  [84/213], [94mLoss[0m : 2.04036
[1mStep[0m  [105/213], [94mLoss[0m : 2.80996
[1mStep[0m  [126/213], [94mLoss[0m : 1.95174
[1mStep[0m  [147/213], [94mLoss[0m : 2.23825
[1mStep[0m  [168/213], [94mLoss[0m : 2.03158
[1mStep[0m  [189/213], [94mLoss[0m : 2.02187
[1mStep[0m  [210/213], [94mLoss[0m : 1.98168

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.424, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.10630
[1mStep[0m  [21/213], [94mLoss[0m : 2.00514
[1mStep[0m  [42/213], [94mLoss[0m : 2.14906
[1mStep[0m  [63/213], [94mLoss[0m : 2.11302
[1mStep[0m  [84/213], [94mLoss[0m : 2.42879
[1mStep[0m  [105/213], [94mLoss[0m : 2.21838
[1mStep[0m  [126/213], [94mLoss[0m : 1.94547
[1mStep[0m  [147/213], [94mLoss[0m : 2.17107
[1mStep[0m  [168/213], [94mLoss[0m : 2.04050
[1mStep[0m  [189/213], [94mLoss[0m : 2.77681
[1mStep[0m  [210/213], [94mLoss[0m : 2.15117

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.084, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 2.07682
[1mStep[0m  [21/213], [94mLoss[0m : 2.04339
[1mStep[0m  [42/213], [94mLoss[0m : 1.67410
[1mStep[0m  [63/213], [94mLoss[0m : 1.65291
[1mStep[0m  [84/213], [94mLoss[0m : 2.45884
[1mStep[0m  [105/213], [94mLoss[0m : 2.21771
[1mStep[0m  [126/213], [94mLoss[0m : 2.43124
[1mStep[0m  [147/213], [94mLoss[0m : 1.74598
[1mStep[0m  [168/213], [94mLoss[0m : 1.78848
[1mStep[0m  [189/213], [94mLoss[0m : 2.29208
[1mStep[0m  [210/213], [94mLoss[0m : 1.80193

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.024, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92447
[1mStep[0m  [21/213], [94mLoss[0m : 1.72455
[1mStep[0m  [42/213], [94mLoss[0m : 1.69855
[1mStep[0m  [63/213], [94mLoss[0m : 2.10864
[1mStep[0m  [84/213], [94mLoss[0m : 1.84955
[1mStep[0m  [105/213], [94mLoss[0m : 1.89455
[1mStep[0m  [126/213], [94mLoss[0m : 1.90950
[1mStep[0m  [147/213], [94mLoss[0m : 1.76152
[1mStep[0m  [168/213], [94mLoss[0m : 1.75496
[1mStep[0m  [189/213], [94mLoss[0m : 1.80363
[1mStep[0m  [210/213], [94mLoss[0m : 2.11716

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.966, [92mTest[0m: 2.389, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.92259
[1mStep[0m  [21/213], [94mLoss[0m : 1.87160
[1mStep[0m  [42/213], [94mLoss[0m : 1.84860
[1mStep[0m  [63/213], [94mLoss[0m : 1.83510
[1mStep[0m  [84/213], [94mLoss[0m : 1.81232
[1mStep[0m  [105/213], [94mLoss[0m : 2.05400
[1mStep[0m  [126/213], [94mLoss[0m : 1.90940
[1mStep[0m  [147/213], [94mLoss[0m : 1.68534
[1mStep[0m  [168/213], [94mLoss[0m : 2.31729
[1mStep[0m  [189/213], [94mLoss[0m : 2.07020
[1mStep[0m  [210/213], [94mLoss[0m : 2.20074

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.935, [92mTest[0m: 2.417, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.81530
[1mStep[0m  [21/213], [94mLoss[0m : 1.79253
[1mStep[0m  [42/213], [94mLoss[0m : 1.88806
[1mStep[0m  [63/213], [94mLoss[0m : 2.12947
[1mStep[0m  [84/213], [94mLoss[0m : 2.07895
[1mStep[0m  [105/213], [94mLoss[0m : 1.81843
[1mStep[0m  [126/213], [94mLoss[0m : 2.01748
[1mStep[0m  [147/213], [94mLoss[0m : 2.11920
[1mStep[0m  [168/213], [94mLoss[0m : 1.85651
[1mStep[0m  [189/213], [94mLoss[0m : 1.79440
[1mStep[0m  [210/213], [94mLoss[0m : 1.77893

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.896, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.66408
[1mStep[0m  [21/213], [94mLoss[0m : 1.56564
[1mStep[0m  [42/213], [94mLoss[0m : 1.77690
[1mStep[0m  [63/213], [94mLoss[0m : 1.76355
[1mStep[0m  [84/213], [94mLoss[0m : 1.77234
[1mStep[0m  [105/213], [94mLoss[0m : 1.84125
[1mStep[0m  [126/213], [94mLoss[0m : 1.88155
[1mStep[0m  [147/213], [94mLoss[0m : 1.61597
[1mStep[0m  [168/213], [94mLoss[0m : 1.72262
[1mStep[0m  [189/213], [94mLoss[0m : 1.89031
[1mStep[0m  [210/213], [94mLoss[0m : 2.04386

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.435, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.69198
[1mStep[0m  [21/213], [94mLoss[0m : 1.80723
[1mStep[0m  [42/213], [94mLoss[0m : 1.75585
[1mStep[0m  [63/213], [94mLoss[0m : 1.92469
[1mStep[0m  [84/213], [94mLoss[0m : 2.11553
[1mStep[0m  [105/213], [94mLoss[0m : 1.73456
[1mStep[0m  [126/213], [94mLoss[0m : 1.85440
[1mStep[0m  [147/213], [94mLoss[0m : 2.38482
[1mStep[0m  [168/213], [94mLoss[0m : 1.92290
[1mStep[0m  [189/213], [94mLoss[0m : 1.63943
[1mStep[0m  [210/213], [94mLoss[0m : 2.14935

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.797, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.62909
[1mStep[0m  [21/213], [94mLoss[0m : 1.45234
[1mStep[0m  [42/213], [94mLoss[0m : 2.10025
[1mStep[0m  [63/213], [94mLoss[0m : 1.25761
[1mStep[0m  [84/213], [94mLoss[0m : 1.71542
[1mStep[0m  [105/213], [94mLoss[0m : 1.85148
[1mStep[0m  [126/213], [94mLoss[0m : 1.76295
[1mStep[0m  [147/213], [94mLoss[0m : 1.65551
[1mStep[0m  [168/213], [94mLoss[0m : 1.87660
[1mStep[0m  [189/213], [94mLoss[0m : 1.72775
[1mStep[0m  [210/213], [94mLoss[0m : 2.08795

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.78822
[1mStep[0m  [21/213], [94mLoss[0m : 1.56114
[1mStep[0m  [42/213], [94mLoss[0m : 1.68526
[1mStep[0m  [63/213], [94mLoss[0m : 1.46768
[1mStep[0m  [84/213], [94mLoss[0m : 1.64824
[1mStep[0m  [105/213], [94mLoss[0m : 1.53964
[1mStep[0m  [126/213], [94mLoss[0m : 1.53036
[1mStep[0m  [147/213], [94mLoss[0m : 2.18116
[1mStep[0m  [168/213], [94mLoss[0m : 2.09145
[1mStep[0m  [189/213], [94mLoss[0m : 1.81175
[1mStep[0m  [210/213], [94mLoss[0m : 1.64723

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.72346
[1mStep[0m  [21/213], [94mLoss[0m : 1.19076
[1mStep[0m  [42/213], [94mLoss[0m : 1.85566
[1mStep[0m  [63/213], [94mLoss[0m : 1.48854
[1mStep[0m  [84/213], [94mLoss[0m : 1.79621
[1mStep[0m  [105/213], [94mLoss[0m : 1.28396
[1mStep[0m  [126/213], [94mLoss[0m : 1.57537
[1mStep[0m  [147/213], [94mLoss[0m : 1.76015
[1mStep[0m  [168/213], [94mLoss[0m : 1.83469
[1mStep[0m  [189/213], [94mLoss[0m : 1.97171
[1mStep[0m  [210/213], [94mLoss[0m : 1.95741

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.467, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.45978
[1mStep[0m  [21/213], [94mLoss[0m : 1.47754
[1mStep[0m  [42/213], [94mLoss[0m : 2.01159
[1mStep[0m  [63/213], [94mLoss[0m : 1.67331
[1mStep[0m  [84/213], [94mLoss[0m : 1.65175
[1mStep[0m  [105/213], [94mLoss[0m : 1.58501
[1mStep[0m  [126/213], [94mLoss[0m : 1.84557
[1mStep[0m  [147/213], [94mLoss[0m : 1.47487
[1mStep[0m  [168/213], [94mLoss[0m : 1.91582
[1mStep[0m  [189/213], [94mLoss[0m : 1.86003
[1mStep[0m  [210/213], [94mLoss[0m : 1.61741

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.460, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.47025
[1mStep[0m  [21/213], [94mLoss[0m : 1.58426
[1mStep[0m  [42/213], [94mLoss[0m : 1.94034
[1mStep[0m  [63/213], [94mLoss[0m : 1.61383
[1mStep[0m  [84/213], [94mLoss[0m : 1.81364
[1mStep[0m  [105/213], [94mLoss[0m : 1.63419
[1mStep[0m  [126/213], [94mLoss[0m : 1.80590
[1mStep[0m  [147/213], [94mLoss[0m : 1.36566
[1mStep[0m  [168/213], [94mLoss[0m : 1.37450
[1mStep[0m  [189/213], [94mLoss[0m : 1.83900
[1mStep[0m  [210/213], [94mLoss[0m : 1.95616

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.642, [92mTest[0m: 2.536, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.34700
[1mStep[0m  [21/213], [94mLoss[0m : 1.59848
[1mStep[0m  [42/213], [94mLoss[0m : 1.33181
[1mStep[0m  [63/213], [94mLoss[0m : 1.92447
[1mStep[0m  [84/213], [94mLoss[0m : 1.46883
[1mStep[0m  [105/213], [94mLoss[0m : 1.42972
[1mStep[0m  [126/213], [94mLoss[0m : 1.38294
[1mStep[0m  [147/213], [94mLoss[0m : 1.64959
[1mStep[0m  [168/213], [94mLoss[0m : 1.60920
[1mStep[0m  [189/213], [94mLoss[0m : 1.47692
[1mStep[0m  [210/213], [94mLoss[0m : 1.57805

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.605, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.39443
[1mStep[0m  [21/213], [94mLoss[0m : 1.99554
[1mStep[0m  [42/213], [94mLoss[0m : 1.35137
[1mStep[0m  [63/213], [94mLoss[0m : 1.57359
[1mStep[0m  [84/213], [94mLoss[0m : 1.70691
[1mStep[0m  [105/213], [94mLoss[0m : 1.85548
[1mStep[0m  [126/213], [94mLoss[0m : 2.08174
[1mStep[0m  [147/213], [94mLoss[0m : 1.55372
[1mStep[0m  [168/213], [94mLoss[0m : 1.91871
[1mStep[0m  [189/213], [94mLoss[0m : 1.76075
[1mStep[0m  [210/213], [94mLoss[0m : 1.59195

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.585, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.56732
[1mStep[0m  [21/213], [94mLoss[0m : 1.33232
[1mStep[0m  [42/213], [94mLoss[0m : 1.37655
[1mStep[0m  [63/213], [94mLoss[0m : 1.76905
[1mStep[0m  [84/213], [94mLoss[0m : 1.37939
[1mStep[0m  [105/213], [94mLoss[0m : 1.66065
[1mStep[0m  [126/213], [94mLoss[0m : 1.35225
[1mStep[0m  [147/213], [94mLoss[0m : 1.49409
[1mStep[0m  [168/213], [94mLoss[0m : 1.54284
[1mStep[0m  [189/213], [94mLoss[0m : 1.52262
[1mStep[0m  [210/213], [94mLoss[0m : 1.93301

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.543, [92mTest[0m: 2.451, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.55863
[1mStep[0m  [21/213], [94mLoss[0m : 1.58332
[1mStep[0m  [42/213], [94mLoss[0m : 1.64850
[1mStep[0m  [63/213], [94mLoss[0m : 1.62072
[1mStep[0m  [84/213], [94mLoss[0m : 1.74530
[1mStep[0m  [105/213], [94mLoss[0m : 1.37805
[1mStep[0m  [126/213], [94mLoss[0m : 1.41472
[1mStep[0m  [147/213], [94mLoss[0m : 1.50240
[1mStep[0m  [168/213], [94mLoss[0m : 1.62447
[1mStep[0m  [189/213], [94mLoss[0m : 1.70807
[1mStep[0m  [210/213], [94mLoss[0m : 1.42139

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.557, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.40293
[1mStep[0m  [21/213], [94mLoss[0m : 1.41295
[1mStep[0m  [42/213], [94mLoss[0m : 1.24628
[1mStep[0m  [63/213], [94mLoss[0m : 1.32786
[1mStep[0m  [84/213], [94mLoss[0m : 1.71175
[1mStep[0m  [105/213], [94mLoss[0m : 1.46239
[1mStep[0m  [126/213], [94mLoss[0m : 1.60527
[1mStep[0m  [147/213], [94mLoss[0m : 1.45385
[1mStep[0m  [168/213], [94mLoss[0m : 1.35828
[1mStep[0m  [189/213], [94mLoss[0m : 1.28693
[1mStep[0m  [210/213], [94mLoss[0m : 1.68862

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.477, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.57723
[1mStep[0m  [21/213], [94mLoss[0m : 1.27845
[1mStep[0m  [42/213], [94mLoss[0m : 1.67027
[1mStep[0m  [63/213], [94mLoss[0m : 1.51168
[1mStep[0m  [84/213], [94mLoss[0m : 1.90584
[1mStep[0m  [105/213], [94mLoss[0m : 1.41248
[1mStep[0m  [126/213], [94mLoss[0m : 1.57215
[1mStep[0m  [147/213], [94mLoss[0m : 1.46059
[1mStep[0m  [168/213], [94mLoss[0m : 1.46310
[1mStep[0m  [189/213], [94mLoss[0m : 1.36665
[1mStep[0m  [210/213], [94mLoss[0m : 1.19216

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.460, [92mTest[0m: 2.458, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.55740
[1mStep[0m  [21/213], [94mLoss[0m : 1.26607
[1mStep[0m  [42/213], [94mLoss[0m : 1.40009
[1mStep[0m  [63/213], [94mLoss[0m : 1.96633
[1mStep[0m  [84/213], [94mLoss[0m : 1.52921
[1mStep[0m  [105/213], [94mLoss[0m : 1.71974
[1mStep[0m  [126/213], [94mLoss[0m : 1.53665
[1mStep[0m  [147/213], [94mLoss[0m : 1.53573
[1mStep[0m  [168/213], [94mLoss[0m : 1.67641
[1mStep[0m  [189/213], [94mLoss[0m : 1.36425
[1mStep[0m  [210/213], [94mLoss[0m : 1.37355

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.435, [92mTest[0m: 2.465, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.12413
[1mStep[0m  [21/213], [94mLoss[0m : 1.23108
[1mStep[0m  [42/213], [94mLoss[0m : 1.65118
[1mStep[0m  [63/213], [94mLoss[0m : 1.33425
[1mStep[0m  [84/213], [94mLoss[0m : 1.31399
[1mStep[0m  [105/213], [94mLoss[0m : 1.67580
[1mStep[0m  [126/213], [94mLoss[0m : 1.72808
[1mStep[0m  [147/213], [94mLoss[0m : 1.32460
[1mStep[0m  [168/213], [94mLoss[0m : 1.09816
[1mStep[0m  [189/213], [94mLoss[0m : 1.19757
[1mStep[0m  [210/213], [94mLoss[0m : 1.48555

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.415, [92mTest[0m: 2.497, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.26898
[1mStep[0m  [21/213], [94mLoss[0m : 1.51914
[1mStep[0m  [42/213], [94mLoss[0m : 1.67001
[1mStep[0m  [63/213], [94mLoss[0m : 1.57015
[1mStep[0m  [84/213], [94mLoss[0m : 1.63394
[1mStep[0m  [105/213], [94mLoss[0m : 1.35742
[1mStep[0m  [126/213], [94mLoss[0m : 1.37337
[1mStep[0m  [147/213], [94mLoss[0m : 1.53469
[1mStep[0m  [168/213], [94mLoss[0m : 1.09869
[1mStep[0m  [189/213], [94mLoss[0m : 1.31252
[1mStep[0m  [210/213], [94mLoss[0m : 1.26320

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.394, [92mTest[0m: 2.480, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/213], [94mLoss[0m : 1.35317
[1mStep[0m  [21/213], [94mLoss[0m : 1.39320
[1mStep[0m  [42/213], [94mLoss[0m : 1.32513
[1mStep[0m  [63/213], [94mLoss[0m : 1.54409
[1mStep[0m  [84/213], [94mLoss[0m : 1.18279
[1mStep[0m  [105/213], [94mLoss[0m : 1.34154
[1mStep[0m  [126/213], [94mLoss[0m : 1.47120
[1mStep[0m  [147/213], [94mLoss[0m : 1.40232
[1mStep[0m  [168/213], [94mLoss[0m : 1.56113
[1mStep[0m  [189/213], [94mLoss[0m : 1.44021
[1mStep[0m  [210/213], [94mLoss[0m : 1.35715

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.381, [92mTest[0m: 2.488, [96mlr[0m: 0.0023175
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.507
====================================

Phase 2 - Evaluation MAE:  2.5073808251686818
MAE score P1       2.325191
MAE score P2       2.507381
loss               1.381176
learning_rate      0.002575
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.9
weight_decay         0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 11.23748
[1mStep[0m  [10/106], [94mLoss[0m : 10.61721
[1mStep[0m  [20/106], [94mLoss[0m : 9.05726
[1mStep[0m  [30/106], [94mLoss[0m : 9.17625
[1mStep[0m  [40/106], [94mLoss[0m : 7.84148
[1mStep[0m  [50/106], [94mLoss[0m : 6.84270
[1mStep[0m  [60/106], [94mLoss[0m : 6.58734
[1mStep[0m  [70/106], [94mLoss[0m : 6.07695
[1mStep[0m  [80/106], [94mLoss[0m : 4.81216
[1mStep[0m  [90/106], [94mLoss[0m : 4.67118
[1mStep[0m  [100/106], [94mLoss[0m : 4.32923

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 7.327, [92mTest[0m: 11.119, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.79053
[1mStep[0m  [10/106], [94mLoss[0m : 4.05942
[1mStep[0m  [20/106], [94mLoss[0m : 3.07903
[1mStep[0m  [30/106], [94mLoss[0m : 3.06825
[1mStep[0m  [40/106], [94mLoss[0m : 3.08628
[1mStep[0m  [50/106], [94mLoss[0m : 3.27576
[1mStep[0m  [60/106], [94mLoss[0m : 2.83621
[1mStep[0m  [70/106], [94mLoss[0m : 3.13592
[1mStep[0m  [80/106], [94mLoss[0m : 2.87426
[1mStep[0m  [90/106], [94mLoss[0m : 2.92999
[1mStep[0m  [100/106], [94mLoss[0m : 2.97912

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 3.147, [92mTest[0m: 3.942, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44354
[1mStep[0m  [10/106], [94mLoss[0m : 2.76124
[1mStep[0m  [20/106], [94mLoss[0m : 2.83314
[1mStep[0m  [30/106], [94mLoss[0m : 2.72572
[1mStep[0m  [40/106], [94mLoss[0m : 2.78760
[1mStep[0m  [50/106], [94mLoss[0m : 2.70480
[1mStep[0m  [60/106], [94mLoss[0m : 2.47111
[1mStep[0m  [70/106], [94mLoss[0m : 2.62659
[1mStep[0m  [80/106], [94mLoss[0m : 2.40661
[1mStep[0m  [90/106], [94mLoss[0m : 2.55169
[1mStep[0m  [100/106], [94mLoss[0m : 2.42346

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.690, [92mTest[0m: 2.646, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.91954
[1mStep[0m  [10/106], [94mLoss[0m : 2.52158
[1mStep[0m  [20/106], [94mLoss[0m : 2.34399
[1mStep[0m  [30/106], [94mLoss[0m : 2.92734
[1mStep[0m  [40/106], [94mLoss[0m : 2.68749
[1mStep[0m  [50/106], [94mLoss[0m : 2.43330
[1mStep[0m  [60/106], [94mLoss[0m : 2.64531
[1mStep[0m  [70/106], [94mLoss[0m : 2.41331
[1mStep[0m  [80/106], [94mLoss[0m : 2.77335
[1mStep[0m  [90/106], [94mLoss[0m : 2.68438
[1mStep[0m  [100/106], [94mLoss[0m : 2.90329

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.517, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61559
[1mStep[0m  [10/106], [94mLoss[0m : 2.56236
[1mStep[0m  [20/106], [94mLoss[0m : 2.48987
[1mStep[0m  [30/106], [94mLoss[0m : 2.70476
[1mStep[0m  [40/106], [94mLoss[0m : 2.41471
[1mStep[0m  [50/106], [94mLoss[0m : 2.36484
[1mStep[0m  [60/106], [94mLoss[0m : 2.33771
[1mStep[0m  [70/106], [94mLoss[0m : 2.78089
[1mStep[0m  [80/106], [94mLoss[0m : 2.52688
[1mStep[0m  [90/106], [94mLoss[0m : 2.29349
[1mStep[0m  [100/106], [94mLoss[0m : 2.47504

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.70797
[1mStep[0m  [10/106], [94mLoss[0m : 2.45634
[1mStep[0m  [20/106], [94mLoss[0m : 2.23819
[1mStep[0m  [30/106], [94mLoss[0m : 2.76185
[1mStep[0m  [40/106], [94mLoss[0m : 2.69928
[1mStep[0m  [50/106], [94mLoss[0m : 2.47262
[1mStep[0m  [60/106], [94mLoss[0m : 2.44216
[1mStep[0m  [70/106], [94mLoss[0m : 2.31499
[1mStep[0m  [80/106], [94mLoss[0m : 2.22087
[1mStep[0m  [90/106], [94mLoss[0m : 2.49832
[1mStep[0m  [100/106], [94mLoss[0m : 2.71958

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.565, [92mTest[0m: 2.490, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49233
[1mStep[0m  [10/106], [94mLoss[0m : 2.27868
[1mStep[0m  [20/106], [94mLoss[0m : 2.51659
[1mStep[0m  [30/106], [94mLoss[0m : 2.85459
[1mStep[0m  [40/106], [94mLoss[0m : 2.32998
[1mStep[0m  [50/106], [94mLoss[0m : 2.42009
[1mStep[0m  [60/106], [94mLoss[0m : 2.44272
[1mStep[0m  [70/106], [94mLoss[0m : 2.29214
[1mStep[0m  [80/106], [94mLoss[0m : 2.76719
[1mStep[0m  [90/106], [94mLoss[0m : 2.62704
[1mStep[0m  [100/106], [94mLoss[0m : 2.51358

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46111
[1mStep[0m  [10/106], [94mLoss[0m : 2.71025
[1mStep[0m  [20/106], [94mLoss[0m : 2.48715
[1mStep[0m  [30/106], [94mLoss[0m : 2.59123
[1mStep[0m  [40/106], [94mLoss[0m : 2.25643
[1mStep[0m  [50/106], [94mLoss[0m : 2.63769
[1mStep[0m  [60/106], [94mLoss[0m : 2.32333
[1mStep[0m  [70/106], [94mLoss[0m : 2.46837
[1mStep[0m  [80/106], [94mLoss[0m : 2.82987
[1mStep[0m  [90/106], [94mLoss[0m : 2.53724
[1mStep[0m  [100/106], [94mLoss[0m : 2.71879

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.462, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50936
[1mStep[0m  [10/106], [94mLoss[0m : 2.46663
[1mStep[0m  [20/106], [94mLoss[0m : 2.44306
[1mStep[0m  [30/106], [94mLoss[0m : 2.74763
[1mStep[0m  [40/106], [94mLoss[0m : 2.74403
[1mStep[0m  [50/106], [94mLoss[0m : 2.55718
[1mStep[0m  [60/106], [94mLoss[0m : 2.31892
[1mStep[0m  [70/106], [94mLoss[0m : 2.53576
[1mStep[0m  [80/106], [94mLoss[0m : 2.73232
[1mStep[0m  [90/106], [94mLoss[0m : 2.43382
[1mStep[0m  [100/106], [94mLoss[0m : 2.39198

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.548, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39495
[1mStep[0m  [10/106], [94mLoss[0m : 2.36737
[1mStep[0m  [20/106], [94mLoss[0m : 2.35612
[1mStep[0m  [30/106], [94mLoss[0m : 2.21363
[1mStep[0m  [40/106], [94mLoss[0m : 2.14802
[1mStep[0m  [50/106], [94mLoss[0m : 2.67897
[1mStep[0m  [60/106], [94mLoss[0m : 2.58399
[1mStep[0m  [70/106], [94mLoss[0m : 2.98836
[1mStep[0m  [80/106], [94mLoss[0m : 2.38202
[1mStep[0m  [90/106], [94mLoss[0m : 2.38530
[1mStep[0m  [100/106], [94mLoss[0m : 2.68743

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.458, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59151
[1mStep[0m  [10/106], [94mLoss[0m : 2.39927
[1mStep[0m  [20/106], [94mLoss[0m : 2.51457
[1mStep[0m  [30/106], [94mLoss[0m : 2.57818
[1mStep[0m  [40/106], [94mLoss[0m : 2.39089
[1mStep[0m  [50/106], [94mLoss[0m : 2.45614
[1mStep[0m  [60/106], [94mLoss[0m : 2.38008
[1mStep[0m  [70/106], [94mLoss[0m : 2.79456
[1mStep[0m  [80/106], [94mLoss[0m : 2.30535
[1mStep[0m  [90/106], [94mLoss[0m : 2.50412
[1mStep[0m  [100/106], [94mLoss[0m : 2.62727

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.531, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.77462
[1mStep[0m  [10/106], [94mLoss[0m : 2.58186
[1mStep[0m  [20/106], [94mLoss[0m : 2.39164
[1mStep[0m  [30/106], [94mLoss[0m : 2.55467
[1mStep[0m  [40/106], [94mLoss[0m : 2.44842
[1mStep[0m  [50/106], [94mLoss[0m : 2.15186
[1mStep[0m  [60/106], [94mLoss[0m : 2.41675
[1mStep[0m  [70/106], [94mLoss[0m : 2.71449
[1mStep[0m  [80/106], [94mLoss[0m : 2.71791
[1mStep[0m  [90/106], [94mLoss[0m : 2.61981
[1mStep[0m  [100/106], [94mLoss[0m : 2.43913

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.537, [92mTest[0m: 2.447, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62842
[1mStep[0m  [10/106], [94mLoss[0m : 2.54489
[1mStep[0m  [20/106], [94mLoss[0m : 2.56861
[1mStep[0m  [30/106], [94mLoss[0m : 2.23734
[1mStep[0m  [40/106], [94mLoss[0m : 2.35008
[1mStep[0m  [50/106], [94mLoss[0m : 2.76800
[1mStep[0m  [60/106], [94mLoss[0m : 2.46887
[1mStep[0m  [70/106], [94mLoss[0m : 2.45755
[1mStep[0m  [80/106], [94mLoss[0m : 2.98032
[1mStep[0m  [90/106], [94mLoss[0m : 2.37414
[1mStep[0m  [100/106], [94mLoss[0m : 2.13130

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60021
[1mStep[0m  [10/106], [94mLoss[0m : 2.34139
[1mStep[0m  [20/106], [94mLoss[0m : 2.40881
[1mStep[0m  [30/106], [94mLoss[0m : 2.63387
[1mStep[0m  [40/106], [94mLoss[0m : 2.58574
[1mStep[0m  [50/106], [94mLoss[0m : 2.41443
[1mStep[0m  [60/106], [94mLoss[0m : 2.56494
[1mStep[0m  [70/106], [94mLoss[0m : 2.73674
[1mStep[0m  [80/106], [94mLoss[0m : 2.27159
[1mStep[0m  [90/106], [94mLoss[0m : 2.35158
[1mStep[0m  [100/106], [94mLoss[0m : 2.51612

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.451, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58695
[1mStep[0m  [10/106], [94mLoss[0m : 2.63307
[1mStep[0m  [20/106], [94mLoss[0m : 2.45010
[1mStep[0m  [30/106], [94mLoss[0m : 2.77423
[1mStep[0m  [40/106], [94mLoss[0m : 2.47321
[1mStep[0m  [50/106], [94mLoss[0m : 2.46726
[1mStep[0m  [60/106], [94mLoss[0m : 2.47869
[1mStep[0m  [70/106], [94mLoss[0m : 2.43742
[1mStep[0m  [80/106], [94mLoss[0m : 2.60819
[1mStep[0m  [90/106], [94mLoss[0m : 2.34588
[1mStep[0m  [100/106], [94mLoss[0m : 2.58340

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.456, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56690
[1mStep[0m  [10/106], [94mLoss[0m : 2.37180
[1mStep[0m  [20/106], [94mLoss[0m : 2.58568
[1mStep[0m  [30/106], [94mLoss[0m : 2.28057
[1mStep[0m  [40/106], [94mLoss[0m : 2.64619
[1mStep[0m  [50/106], [94mLoss[0m : 2.50840
[1mStep[0m  [60/106], [94mLoss[0m : 2.81593
[1mStep[0m  [70/106], [94mLoss[0m : 2.38733
[1mStep[0m  [80/106], [94mLoss[0m : 2.70771
[1mStep[0m  [90/106], [94mLoss[0m : 2.25912
[1mStep[0m  [100/106], [94mLoss[0m : 2.11409

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.446, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.89508
[1mStep[0m  [10/106], [94mLoss[0m : 2.40526
[1mStep[0m  [20/106], [94mLoss[0m : 2.68213
[1mStep[0m  [30/106], [94mLoss[0m : 2.63611
[1mStep[0m  [40/106], [94mLoss[0m : 2.64925
[1mStep[0m  [50/106], [94mLoss[0m : 2.42022
[1mStep[0m  [60/106], [94mLoss[0m : 2.47217
[1mStep[0m  [70/106], [94mLoss[0m : 2.75078
[1mStep[0m  [80/106], [94mLoss[0m : 2.34567
[1mStep[0m  [90/106], [94mLoss[0m : 2.48900
[1mStep[0m  [100/106], [94mLoss[0m : 2.46591

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46857
[1mStep[0m  [10/106], [94mLoss[0m : 2.56235
[1mStep[0m  [20/106], [94mLoss[0m : 2.48044
[1mStep[0m  [30/106], [94mLoss[0m : 2.45708
[1mStep[0m  [40/106], [94mLoss[0m : 2.42666
[1mStep[0m  [50/106], [94mLoss[0m : 2.48638
[1mStep[0m  [60/106], [94mLoss[0m : 2.58713
[1mStep[0m  [70/106], [94mLoss[0m : 2.58907
[1mStep[0m  [80/106], [94mLoss[0m : 2.65651
[1mStep[0m  [90/106], [94mLoss[0m : 2.44158
[1mStep[0m  [100/106], [94mLoss[0m : 2.14962

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45688
[1mStep[0m  [10/106], [94mLoss[0m : 2.40407
[1mStep[0m  [20/106], [94mLoss[0m : 2.61446
[1mStep[0m  [30/106], [94mLoss[0m : 2.44977
[1mStep[0m  [40/106], [94mLoss[0m : 2.64059
[1mStep[0m  [50/106], [94mLoss[0m : 2.45074
[1mStep[0m  [60/106], [94mLoss[0m : 2.37669
[1mStep[0m  [70/106], [94mLoss[0m : 2.14750
[1mStep[0m  [80/106], [94mLoss[0m : 2.82543
[1mStep[0m  [90/106], [94mLoss[0m : 2.65589
[1mStep[0m  [100/106], [94mLoss[0m : 2.31971

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.448, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48224
[1mStep[0m  [10/106], [94mLoss[0m : 2.66119
[1mStep[0m  [20/106], [94mLoss[0m : 2.32362
[1mStep[0m  [30/106], [94mLoss[0m : 2.86655
[1mStep[0m  [40/106], [94mLoss[0m : 2.52601
[1mStep[0m  [50/106], [94mLoss[0m : 2.56873
[1mStep[0m  [60/106], [94mLoss[0m : 2.44468
[1mStep[0m  [70/106], [94mLoss[0m : 2.37244
[1mStep[0m  [80/106], [94mLoss[0m : 2.31074
[1mStep[0m  [90/106], [94mLoss[0m : 2.50325
[1mStep[0m  [100/106], [94mLoss[0m : 2.35954

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.443, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33507
[1mStep[0m  [10/106], [94mLoss[0m : 2.24087
[1mStep[0m  [20/106], [94mLoss[0m : 2.38414
[1mStep[0m  [30/106], [94mLoss[0m : 2.53703
[1mStep[0m  [40/106], [94mLoss[0m : 2.53800
[1mStep[0m  [50/106], [94mLoss[0m : 2.49407
[1mStep[0m  [60/106], [94mLoss[0m : 2.26166
[1mStep[0m  [70/106], [94mLoss[0m : 2.45384
[1mStep[0m  [80/106], [94mLoss[0m : 2.40052
[1mStep[0m  [90/106], [94mLoss[0m : 2.11806
[1mStep[0m  [100/106], [94mLoss[0m : 2.93313

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.445, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40595
[1mStep[0m  [10/106], [94mLoss[0m : 2.60181
[1mStep[0m  [20/106], [94mLoss[0m : 2.53946
[1mStep[0m  [30/106], [94mLoss[0m : 2.96919
[1mStep[0m  [40/106], [94mLoss[0m : 2.53270
[1mStep[0m  [50/106], [94mLoss[0m : 2.33185
[1mStep[0m  [60/106], [94mLoss[0m : 2.41976
[1mStep[0m  [70/106], [94mLoss[0m : 2.42811
[1mStep[0m  [80/106], [94mLoss[0m : 2.36003
[1mStep[0m  [90/106], [94mLoss[0m : 2.35045
[1mStep[0m  [100/106], [94mLoss[0m : 2.66867

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.439, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42537
[1mStep[0m  [10/106], [94mLoss[0m : 2.48480
[1mStep[0m  [20/106], [94mLoss[0m : 2.35229
[1mStep[0m  [30/106], [94mLoss[0m : 2.46766
[1mStep[0m  [40/106], [94mLoss[0m : 2.28824
[1mStep[0m  [50/106], [94mLoss[0m : 2.75550
[1mStep[0m  [60/106], [94mLoss[0m : 2.26047
[1mStep[0m  [70/106], [94mLoss[0m : 2.40672
[1mStep[0m  [80/106], [94mLoss[0m : 2.32764
[1mStep[0m  [90/106], [94mLoss[0m : 2.73540
[1mStep[0m  [100/106], [94mLoss[0m : 2.50033

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.438, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52328
[1mStep[0m  [10/106], [94mLoss[0m : 2.54934
[1mStep[0m  [20/106], [94mLoss[0m : 2.56290
[1mStep[0m  [30/106], [94mLoss[0m : 2.68467
[1mStep[0m  [40/106], [94mLoss[0m : 2.29057
[1mStep[0m  [50/106], [94mLoss[0m : 2.50803
[1mStep[0m  [60/106], [94mLoss[0m : 2.26180
[1mStep[0m  [70/106], [94mLoss[0m : 2.70601
[1mStep[0m  [80/106], [94mLoss[0m : 2.42407
[1mStep[0m  [90/106], [94mLoss[0m : 2.30939
[1mStep[0m  [100/106], [94mLoss[0m : 2.43685

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25482
[1mStep[0m  [10/106], [94mLoss[0m : 2.74767
[1mStep[0m  [20/106], [94mLoss[0m : 2.75410
[1mStep[0m  [30/106], [94mLoss[0m : 2.56848
[1mStep[0m  [40/106], [94mLoss[0m : 2.57502
[1mStep[0m  [50/106], [94mLoss[0m : 2.53029
[1mStep[0m  [60/106], [94mLoss[0m : 2.62445
[1mStep[0m  [70/106], [94mLoss[0m : 2.51684
[1mStep[0m  [80/106], [94mLoss[0m : 2.76537
[1mStep[0m  [90/106], [94mLoss[0m : 2.80815
[1mStep[0m  [100/106], [94mLoss[0m : 2.70133

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.432, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17765
[1mStep[0m  [10/106], [94mLoss[0m : 2.49564
[1mStep[0m  [20/106], [94mLoss[0m : 2.63040
[1mStep[0m  [30/106], [94mLoss[0m : 2.64602
[1mStep[0m  [40/106], [94mLoss[0m : 2.30141
[1mStep[0m  [50/106], [94mLoss[0m : 2.21500
[1mStep[0m  [60/106], [94mLoss[0m : 2.75881
[1mStep[0m  [70/106], [94mLoss[0m : 2.69568
[1mStep[0m  [80/106], [94mLoss[0m : 2.42716
[1mStep[0m  [90/106], [94mLoss[0m : 2.55164
[1mStep[0m  [100/106], [94mLoss[0m : 2.82688

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.433, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57021
[1mStep[0m  [10/106], [94mLoss[0m : 2.65957
[1mStep[0m  [20/106], [94mLoss[0m : 2.47138
[1mStep[0m  [30/106], [94mLoss[0m : 2.74943
[1mStep[0m  [40/106], [94mLoss[0m : 2.61137
[1mStep[0m  [50/106], [94mLoss[0m : 2.35233
[1mStep[0m  [60/106], [94mLoss[0m : 2.60873
[1mStep[0m  [70/106], [94mLoss[0m : 2.40985
[1mStep[0m  [80/106], [94mLoss[0m : 2.67934
[1mStep[0m  [90/106], [94mLoss[0m : 2.19292
[1mStep[0m  [100/106], [94mLoss[0m : 2.51053

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.435, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.26673
[1mStep[0m  [10/106], [94mLoss[0m : 2.50131
[1mStep[0m  [20/106], [94mLoss[0m : 2.67905
[1mStep[0m  [30/106], [94mLoss[0m : 2.27698
[1mStep[0m  [40/106], [94mLoss[0m : 2.21580
[1mStep[0m  [50/106], [94mLoss[0m : 2.73119
[1mStep[0m  [60/106], [94mLoss[0m : 2.40218
[1mStep[0m  [70/106], [94mLoss[0m : 2.45944
[1mStep[0m  [80/106], [94mLoss[0m : 2.66859
[1mStep[0m  [90/106], [94mLoss[0m : 2.78825
[1mStep[0m  [100/106], [94mLoss[0m : 2.36051

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.426, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54464
[1mStep[0m  [10/106], [94mLoss[0m : 2.00754
[1mStep[0m  [20/106], [94mLoss[0m : 2.37724
[1mStep[0m  [30/106], [94mLoss[0m : 2.42902
[1mStep[0m  [40/106], [94mLoss[0m : 2.54891
[1mStep[0m  [50/106], [94mLoss[0m : 2.53145
[1mStep[0m  [60/106], [94mLoss[0m : 2.28581
[1mStep[0m  [70/106], [94mLoss[0m : 2.54585
[1mStep[0m  [80/106], [94mLoss[0m : 2.43303
[1mStep[0m  [90/106], [94mLoss[0m : 2.54638
[1mStep[0m  [100/106], [94mLoss[0m : 2.60316

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.436, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62001
[1mStep[0m  [10/106], [94mLoss[0m : 2.59646
[1mStep[0m  [20/106], [94mLoss[0m : 2.70009
[1mStep[0m  [30/106], [94mLoss[0m : 2.54651
[1mStep[0m  [40/106], [94mLoss[0m : 2.42914
[1mStep[0m  [50/106], [94mLoss[0m : 2.68419
[1mStep[0m  [60/106], [94mLoss[0m : 2.61151
[1mStep[0m  [70/106], [94mLoss[0m : 2.37936
[1mStep[0m  [80/106], [94mLoss[0m : 2.70756
[1mStep[0m  [90/106], [94mLoss[0m : 2.53243
[1mStep[0m  [100/106], [94mLoss[0m : 2.34455

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.428, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.434
====================================

Phase 1 - Evaluation MAE:  2.434444193570119
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.58055
[1mStep[0m  [10/106], [94mLoss[0m : 2.53613
[1mStep[0m  [20/106], [94mLoss[0m : 2.37576
[1mStep[0m  [30/106], [94mLoss[0m : 2.78082
[1mStep[0m  [40/106], [94mLoss[0m : 2.43040
[1mStep[0m  [50/106], [94mLoss[0m : 2.66630
[1mStep[0m  [60/106], [94mLoss[0m : 2.49908
[1mStep[0m  [70/106], [94mLoss[0m : 2.66277
[1mStep[0m  [80/106], [94mLoss[0m : 2.71117
[1mStep[0m  [90/106], [94mLoss[0m : 2.71546
[1mStep[0m  [100/106], [94mLoss[0m : 2.60788

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60476
[1mStep[0m  [10/106], [94mLoss[0m : 2.63674
[1mStep[0m  [20/106], [94mLoss[0m : 2.36279
[1mStep[0m  [30/106], [94mLoss[0m : 2.49389
[1mStep[0m  [40/106], [94mLoss[0m : 2.59496
[1mStep[0m  [50/106], [94mLoss[0m : 2.71629
[1mStep[0m  [60/106], [94mLoss[0m : 2.55399
[1mStep[0m  [70/106], [94mLoss[0m : 2.37367
[1mStep[0m  [80/106], [94mLoss[0m : 2.38289
[1mStep[0m  [90/106], [94mLoss[0m : 2.39642
[1mStep[0m  [100/106], [94mLoss[0m : 2.17254

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.533, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42647
[1mStep[0m  [10/106], [94mLoss[0m : 2.48383
[1mStep[0m  [20/106], [94mLoss[0m : 2.79083
[1mStep[0m  [30/106], [94mLoss[0m : 3.01850
[1mStep[0m  [40/106], [94mLoss[0m : 2.46945
[1mStep[0m  [50/106], [94mLoss[0m : 2.54842
[1mStep[0m  [60/106], [94mLoss[0m : 2.48431
[1mStep[0m  [70/106], [94mLoss[0m : 2.29387
[1mStep[0m  [80/106], [94mLoss[0m : 2.33706
[1mStep[0m  [90/106], [94mLoss[0m : 2.38638
[1mStep[0m  [100/106], [94mLoss[0m : 2.25391

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.47538
[1mStep[0m  [10/106], [94mLoss[0m : 2.42024
[1mStep[0m  [20/106], [94mLoss[0m : 2.68130
[1mStep[0m  [30/106], [94mLoss[0m : 2.48565
[1mStep[0m  [40/106], [94mLoss[0m : 2.39330
[1mStep[0m  [50/106], [94mLoss[0m : 2.35484
[1mStep[0m  [60/106], [94mLoss[0m : 2.60642
[1mStep[0m  [70/106], [94mLoss[0m : 2.38277
[1mStep[0m  [80/106], [94mLoss[0m : 2.35429
[1mStep[0m  [90/106], [94mLoss[0m : 2.44634
[1mStep[0m  [100/106], [94mLoss[0m : 2.51571

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.483, [92mTest[0m: 2.577, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69414
[1mStep[0m  [10/106], [94mLoss[0m : 2.97262
[1mStep[0m  [20/106], [94mLoss[0m : 2.48879
[1mStep[0m  [30/106], [94mLoss[0m : 2.42225
[1mStep[0m  [40/106], [94mLoss[0m : 2.48748
[1mStep[0m  [50/106], [94mLoss[0m : 2.49348
[1mStep[0m  [60/106], [94mLoss[0m : 2.31200
[1mStep[0m  [70/106], [94mLoss[0m : 2.17503
[1mStep[0m  [80/106], [94mLoss[0m : 2.23046
[1mStep[0m  [90/106], [94mLoss[0m : 2.69646
[1mStep[0m  [100/106], [94mLoss[0m : 2.52372

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.673, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31986
[1mStep[0m  [10/106], [94mLoss[0m : 2.76725
[1mStep[0m  [20/106], [94mLoss[0m : 2.60122
[1mStep[0m  [30/106], [94mLoss[0m : 2.70659
[1mStep[0m  [40/106], [94mLoss[0m : 2.39511
[1mStep[0m  [50/106], [94mLoss[0m : 2.34041
[1mStep[0m  [60/106], [94mLoss[0m : 2.74247
[1mStep[0m  [70/106], [94mLoss[0m : 1.98277
[1mStep[0m  [80/106], [94mLoss[0m : 2.45501
[1mStep[0m  [90/106], [94mLoss[0m : 2.75491
[1mStep[0m  [100/106], [94mLoss[0m : 2.49069

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.715, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29215
[1mStep[0m  [10/106], [94mLoss[0m : 2.24111
[1mStep[0m  [20/106], [94mLoss[0m : 2.41109
[1mStep[0m  [30/106], [94mLoss[0m : 2.58694
[1mStep[0m  [40/106], [94mLoss[0m : 2.51214
[1mStep[0m  [50/106], [94mLoss[0m : 2.49143
[1mStep[0m  [60/106], [94mLoss[0m : 2.58648
[1mStep[0m  [70/106], [94mLoss[0m : 2.35846
[1mStep[0m  [80/106], [94mLoss[0m : 2.50683
[1mStep[0m  [90/106], [94mLoss[0m : 2.48674
[1mStep[0m  [100/106], [94mLoss[0m : 2.71273

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.694, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46342
[1mStep[0m  [10/106], [94mLoss[0m : 2.33902
[1mStep[0m  [20/106], [94mLoss[0m : 2.57312
[1mStep[0m  [30/106], [94mLoss[0m : 2.33043
[1mStep[0m  [40/106], [94mLoss[0m : 2.88787
[1mStep[0m  [50/106], [94mLoss[0m : 2.31988
[1mStep[0m  [60/106], [94mLoss[0m : 2.33363
[1mStep[0m  [70/106], [94mLoss[0m : 2.48339
[1mStep[0m  [80/106], [94mLoss[0m : 2.18013
[1mStep[0m  [90/106], [94mLoss[0m : 2.37146
[1mStep[0m  [100/106], [94mLoss[0m : 2.28294

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.712, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31243
[1mStep[0m  [10/106], [94mLoss[0m : 2.81775
[1mStep[0m  [20/106], [94mLoss[0m : 2.29923
[1mStep[0m  [30/106], [94mLoss[0m : 2.33437
[1mStep[0m  [40/106], [94mLoss[0m : 2.30455
[1mStep[0m  [50/106], [94mLoss[0m : 2.14617
[1mStep[0m  [60/106], [94mLoss[0m : 2.56863
[1mStep[0m  [70/106], [94mLoss[0m : 2.44463
[1mStep[0m  [80/106], [94mLoss[0m : 2.23462
[1mStep[0m  [90/106], [94mLoss[0m : 2.36863
[1mStep[0m  [100/106], [94mLoss[0m : 2.42231

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.721, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65157
[1mStep[0m  [10/106], [94mLoss[0m : 2.56870
[1mStep[0m  [20/106], [94mLoss[0m : 2.52628
[1mStep[0m  [30/106], [94mLoss[0m : 2.29699
[1mStep[0m  [40/106], [94mLoss[0m : 2.53191
[1mStep[0m  [50/106], [94mLoss[0m : 2.51124
[1mStep[0m  [60/106], [94mLoss[0m : 2.62231
[1mStep[0m  [70/106], [94mLoss[0m : 2.45721
[1mStep[0m  [80/106], [94mLoss[0m : 2.06743
[1mStep[0m  [90/106], [94mLoss[0m : 2.58804
[1mStep[0m  [100/106], [94mLoss[0m : 2.08293

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.771, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29029
[1mStep[0m  [10/106], [94mLoss[0m : 2.25007
[1mStep[0m  [20/106], [94mLoss[0m : 2.53180
[1mStep[0m  [30/106], [94mLoss[0m : 2.24992
[1mStep[0m  [40/106], [94mLoss[0m : 2.73863
[1mStep[0m  [50/106], [94mLoss[0m : 2.27418
[1mStep[0m  [60/106], [94mLoss[0m : 2.44352
[1mStep[0m  [70/106], [94mLoss[0m : 2.54969
[1mStep[0m  [80/106], [94mLoss[0m : 2.20338
[1mStep[0m  [90/106], [94mLoss[0m : 2.57926
[1mStep[0m  [100/106], [94mLoss[0m : 2.36113

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.369, [92mTest[0m: 2.668, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17982
[1mStep[0m  [10/106], [94mLoss[0m : 2.28032
[1mStep[0m  [20/106], [94mLoss[0m : 2.47055
[1mStep[0m  [30/106], [94mLoss[0m : 2.73993
[1mStep[0m  [40/106], [94mLoss[0m : 2.10076
[1mStep[0m  [50/106], [94mLoss[0m : 2.52294
[1mStep[0m  [60/106], [94mLoss[0m : 2.67733
[1mStep[0m  [70/106], [94mLoss[0m : 2.43362
[1mStep[0m  [80/106], [94mLoss[0m : 2.42819
[1mStep[0m  [90/106], [94mLoss[0m : 2.47463
[1mStep[0m  [100/106], [94mLoss[0m : 2.56660

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.714, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.34939
[1mStep[0m  [10/106], [94mLoss[0m : 2.21724
[1mStep[0m  [20/106], [94mLoss[0m : 2.21022
[1mStep[0m  [30/106], [94mLoss[0m : 2.23363
[1mStep[0m  [40/106], [94mLoss[0m : 2.08504
[1mStep[0m  [50/106], [94mLoss[0m : 2.46764
[1mStep[0m  [60/106], [94mLoss[0m : 2.47575
[1mStep[0m  [70/106], [94mLoss[0m : 2.16603
[1mStep[0m  [80/106], [94mLoss[0m : 2.20091
[1mStep[0m  [90/106], [94mLoss[0m : 2.33348
[1mStep[0m  [100/106], [94mLoss[0m : 2.26420

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.717, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23087
[1mStep[0m  [10/106], [94mLoss[0m : 2.35245
[1mStep[0m  [20/106], [94mLoss[0m : 2.31459
[1mStep[0m  [30/106], [94mLoss[0m : 2.31695
[1mStep[0m  [40/106], [94mLoss[0m : 2.49086
[1mStep[0m  [50/106], [94mLoss[0m : 2.44529
[1mStep[0m  [60/106], [94mLoss[0m : 2.04648
[1mStep[0m  [70/106], [94mLoss[0m : 2.33169
[1mStep[0m  [80/106], [94mLoss[0m : 2.17909
[1mStep[0m  [90/106], [94mLoss[0m : 2.26567
[1mStep[0m  [100/106], [94mLoss[0m : 2.29774

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.319, [92mTest[0m: 2.763, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59878
[1mStep[0m  [10/106], [94mLoss[0m : 1.97868
[1mStep[0m  [20/106], [94mLoss[0m : 1.98851
[1mStep[0m  [30/106], [94mLoss[0m : 2.12749
[1mStep[0m  [40/106], [94mLoss[0m : 2.15768
[1mStep[0m  [50/106], [94mLoss[0m : 2.30379
[1mStep[0m  [60/106], [94mLoss[0m : 2.23969
[1mStep[0m  [70/106], [94mLoss[0m : 2.54476
[1mStep[0m  [80/106], [94mLoss[0m : 2.07863
[1mStep[0m  [90/106], [94mLoss[0m : 2.37593
[1mStep[0m  [100/106], [94mLoss[0m : 2.09430

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.312, [92mTest[0m: 2.752, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.95256
[1mStep[0m  [10/106], [94mLoss[0m : 2.34702
[1mStep[0m  [20/106], [94mLoss[0m : 2.11914
[1mStep[0m  [30/106], [94mLoss[0m : 2.14681
[1mStep[0m  [40/106], [94mLoss[0m : 2.26555
[1mStep[0m  [50/106], [94mLoss[0m : 2.53884
[1mStep[0m  [60/106], [94mLoss[0m : 2.47930
[1mStep[0m  [70/106], [94mLoss[0m : 2.10859
[1mStep[0m  [80/106], [94mLoss[0m : 2.17875
[1mStep[0m  [90/106], [94mLoss[0m : 2.22023
[1mStep[0m  [100/106], [94mLoss[0m : 2.27594

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.746, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33341
[1mStep[0m  [10/106], [94mLoss[0m : 2.33139
[1mStep[0m  [20/106], [94mLoss[0m : 2.03797
[1mStep[0m  [30/106], [94mLoss[0m : 1.95214
[1mStep[0m  [40/106], [94mLoss[0m : 2.19214
[1mStep[0m  [50/106], [94mLoss[0m : 2.12294
[1mStep[0m  [60/106], [94mLoss[0m : 2.27308
[1mStep[0m  [70/106], [94mLoss[0m : 2.14399
[1mStep[0m  [80/106], [94mLoss[0m : 2.13537
[1mStep[0m  [90/106], [94mLoss[0m : 2.48134
[1mStep[0m  [100/106], [94mLoss[0m : 2.13801

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.795, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.20177
[1mStep[0m  [10/106], [94mLoss[0m : 2.21243
[1mStep[0m  [20/106], [94mLoss[0m : 2.20669
[1mStep[0m  [30/106], [94mLoss[0m : 2.10395
[1mStep[0m  [40/106], [94mLoss[0m : 2.20238
[1mStep[0m  [50/106], [94mLoss[0m : 1.99583
[1mStep[0m  [60/106], [94mLoss[0m : 2.27346
[1mStep[0m  [70/106], [94mLoss[0m : 2.38535
[1mStep[0m  [80/106], [94mLoss[0m : 2.17710
[1mStep[0m  [90/106], [94mLoss[0m : 2.26208
[1mStep[0m  [100/106], [94mLoss[0m : 2.07289

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.249, [92mTest[0m: 2.751, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16006
[1mStep[0m  [10/106], [94mLoss[0m : 2.05470
[1mStep[0m  [20/106], [94mLoss[0m : 2.28833
[1mStep[0m  [30/106], [94mLoss[0m : 2.03895
[1mStep[0m  [40/106], [94mLoss[0m : 2.04899
[1mStep[0m  [50/106], [94mLoss[0m : 2.14691
[1mStep[0m  [60/106], [94mLoss[0m : 2.24339
[1mStep[0m  [70/106], [94mLoss[0m : 2.52365
[1mStep[0m  [80/106], [94mLoss[0m : 2.20823
[1mStep[0m  [90/106], [94mLoss[0m : 2.17757
[1mStep[0m  [100/106], [94mLoss[0m : 2.20356

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.744, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36969
[1mStep[0m  [10/106], [94mLoss[0m : 2.49356
[1mStep[0m  [20/106], [94mLoss[0m : 2.17493
[1mStep[0m  [30/106], [94mLoss[0m : 2.13417
[1mStep[0m  [40/106], [94mLoss[0m : 2.10043
[1mStep[0m  [50/106], [94mLoss[0m : 2.08366
[1mStep[0m  [60/106], [94mLoss[0m : 2.07185
[1mStep[0m  [70/106], [94mLoss[0m : 2.27670
[1mStep[0m  [80/106], [94mLoss[0m : 1.96712
[1mStep[0m  [90/106], [94mLoss[0m : 2.09539
[1mStep[0m  [100/106], [94mLoss[0m : 2.17199

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.207, [92mTest[0m: 2.822, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29439
[1mStep[0m  [10/106], [94mLoss[0m : 1.91050
[1mStep[0m  [20/106], [94mLoss[0m : 2.21568
[1mStep[0m  [30/106], [94mLoss[0m : 2.07995
[1mStep[0m  [40/106], [94mLoss[0m : 2.20019
[1mStep[0m  [50/106], [94mLoss[0m : 2.33157
[1mStep[0m  [60/106], [94mLoss[0m : 2.64410
[1mStep[0m  [70/106], [94mLoss[0m : 2.21337
[1mStep[0m  [80/106], [94mLoss[0m : 2.10217
[1mStep[0m  [90/106], [94mLoss[0m : 2.37961
[1mStep[0m  [100/106], [94mLoss[0m : 2.15592

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.188, [92mTest[0m: 2.763, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.05338
[1mStep[0m  [10/106], [94mLoss[0m : 2.05024
[1mStep[0m  [20/106], [94mLoss[0m : 2.45526
[1mStep[0m  [30/106], [94mLoss[0m : 1.75149
[1mStep[0m  [40/106], [94mLoss[0m : 2.40678
[1mStep[0m  [50/106], [94mLoss[0m : 2.29127
[1mStep[0m  [60/106], [94mLoss[0m : 2.19681
[1mStep[0m  [70/106], [94mLoss[0m : 2.12443
[1mStep[0m  [80/106], [94mLoss[0m : 2.04377
[1mStep[0m  [90/106], [94mLoss[0m : 1.96325
[1mStep[0m  [100/106], [94mLoss[0m : 2.10357

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.667, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.13598
[1mStep[0m  [10/106], [94mLoss[0m : 2.02593
[1mStep[0m  [20/106], [94mLoss[0m : 2.13088
[1mStep[0m  [30/106], [94mLoss[0m : 1.85883
[1mStep[0m  [40/106], [94mLoss[0m : 2.28144
[1mStep[0m  [50/106], [94mLoss[0m : 1.97261
[1mStep[0m  [60/106], [94mLoss[0m : 2.26212
[1mStep[0m  [70/106], [94mLoss[0m : 2.14629
[1mStep[0m  [80/106], [94mLoss[0m : 2.24029
[1mStep[0m  [90/106], [94mLoss[0m : 2.15788
[1mStep[0m  [100/106], [94mLoss[0m : 1.96169

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.718, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25617
[1mStep[0m  [10/106], [94mLoss[0m : 1.96607
[1mStep[0m  [20/106], [94mLoss[0m : 2.30054
[1mStep[0m  [30/106], [94mLoss[0m : 2.17945
[1mStep[0m  [40/106], [94mLoss[0m : 2.48933
[1mStep[0m  [50/106], [94mLoss[0m : 2.26121
[1mStep[0m  [60/106], [94mLoss[0m : 2.05098
[1mStep[0m  [70/106], [94mLoss[0m : 2.06500
[1mStep[0m  [80/106], [94mLoss[0m : 2.36720
[1mStep[0m  [90/106], [94mLoss[0m : 2.04636
[1mStep[0m  [100/106], [94mLoss[0m : 2.34443

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.130, [92mTest[0m: 2.699, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.90726
[1mStep[0m  [10/106], [94mLoss[0m : 2.04223
[1mStep[0m  [20/106], [94mLoss[0m : 2.06286
[1mStep[0m  [30/106], [94mLoss[0m : 2.05952
[1mStep[0m  [40/106], [94mLoss[0m : 1.93598
[1mStep[0m  [50/106], [94mLoss[0m : 1.99295
[1mStep[0m  [60/106], [94mLoss[0m : 2.13723
[1mStep[0m  [70/106], [94mLoss[0m : 2.18045
[1mStep[0m  [80/106], [94mLoss[0m : 2.00118
[1mStep[0m  [90/106], [94mLoss[0m : 2.29829
[1mStep[0m  [100/106], [94mLoss[0m : 2.10836

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.571, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.11917
[1mStep[0m  [10/106], [94mLoss[0m : 1.86247
[1mStep[0m  [20/106], [94mLoss[0m : 2.12752
[1mStep[0m  [30/106], [94mLoss[0m : 2.14924
[1mStep[0m  [40/106], [94mLoss[0m : 2.30053
[1mStep[0m  [50/106], [94mLoss[0m : 2.13853
[1mStep[0m  [60/106], [94mLoss[0m : 2.05063
[1mStep[0m  [70/106], [94mLoss[0m : 2.25598
[1mStep[0m  [80/106], [94mLoss[0m : 2.15627
[1mStep[0m  [90/106], [94mLoss[0m : 2.25303
[1mStep[0m  [100/106], [94mLoss[0m : 2.12282

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.637, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16144
[1mStep[0m  [10/106], [94mLoss[0m : 1.98591
[1mStep[0m  [20/106], [94mLoss[0m : 2.44557
[1mStep[0m  [30/106], [94mLoss[0m : 2.11668
[1mStep[0m  [40/106], [94mLoss[0m : 1.92338
[1mStep[0m  [50/106], [94mLoss[0m : 1.95336
[1mStep[0m  [60/106], [94mLoss[0m : 2.12575
[1mStep[0m  [70/106], [94mLoss[0m : 2.01433
[1mStep[0m  [80/106], [94mLoss[0m : 2.00704
[1mStep[0m  [90/106], [94mLoss[0m : 2.04038
[1mStep[0m  [100/106], [94mLoss[0m : 2.06394

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.049, [92mTest[0m: 2.631, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.02975
[1mStep[0m  [10/106], [94mLoss[0m : 2.16293
[1mStep[0m  [20/106], [94mLoss[0m : 1.94044
[1mStep[0m  [30/106], [94mLoss[0m : 2.00063
[1mStep[0m  [40/106], [94mLoss[0m : 1.92701
[1mStep[0m  [50/106], [94mLoss[0m : 2.17988
[1mStep[0m  [60/106], [94mLoss[0m : 2.29293
[1mStep[0m  [70/106], [94mLoss[0m : 1.99660
[1mStep[0m  [80/106], [94mLoss[0m : 2.19354
[1mStep[0m  [90/106], [94mLoss[0m : 2.28776
[1mStep[0m  [100/106], [94mLoss[0m : 2.22314

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.647, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17151
[1mStep[0m  [10/106], [94mLoss[0m : 2.06131
[1mStep[0m  [20/106], [94mLoss[0m : 1.91240
[1mStep[0m  [30/106], [94mLoss[0m : 1.69171
[1mStep[0m  [40/106], [94mLoss[0m : 2.11176
[1mStep[0m  [50/106], [94mLoss[0m : 1.97414
[1mStep[0m  [60/106], [94mLoss[0m : 2.06694
[1mStep[0m  [70/106], [94mLoss[0m : 1.84310
[1mStep[0m  [80/106], [94mLoss[0m : 2.26885
[1mStep[0m  [90/106], [94mLoss[0m : 2.07275
[1mStep[0m  [100/106], [94mLoss[0m : 1.93683

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.014, [92mTest[0m: 2.537, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92045
[1mStep[0m  [10/106], [94mLoss[0m : 1.82327
[1mStep[0m  [20/106], [94mLoss[0m : 1.94217
[1mStep[0m  [30/106], [94mLoss[0m : 1.86464
[1mStep[0m  [40/106], [94mLoss[0m : 2.24800
[1mStep[0m  [50/106], [94mLoss[0m : 1.94996
[1mStep[0m  [60/106], [94mLoss[0m : 2.06987
[1mStep[0m  [70/106], [94mLoss[0m : 2.09777
[1mStep[0m  [80/106], [94mLoss[0m : 2.04679
[1mStep[0m  [90/106], [94mLoss[0m : 2.07027
[1mStep[0m  [100/106], [94mLoss[0m : 1.89512

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.576, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.580
====================================

Phase 2 - Evaluation MAE:  2.5803100253051183
MAE score P1        2.434444
MAE score P2         2.58031
loss                1.992461
learning_rate       0.002575
batch_size               128
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.3
momentum                 0.1
weight_decay          0.0001
Name: 12, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [250, 50], 'learning_rate': 0.002575, 'batch_size': 64, 'epochs': 30, 'activation': 'tanh', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.3, 'weight_decay': 0.01, 'momentum': 0.9, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 10.37773
[1mStep[0m  [10/106], [94mLoss[0m : 9.94741
[1mStep[0m  [20/106], [94mLoss[0m : 10.03102
[1mStep[0m  [30/106], [94mLoss[0m : 10.02335
[1mStep[0m  [40/106], [94mLoss[0m : 9.45149
[1mStep[0m  [50/106], [94mLoss[0m : 9.73658
[1mStep[0m  [60/106], [94mLoss[0m : 9.43537
[1mStep[0m  [70/106], [94mLoss[0m : 9.01110
[1mStep[0m  [80/106], [94mLoss[0m : 8.46571
[1mStep[0m  [90/106], [94mLoss[0m : 8.94908
[1mStep[0m  [100/106], [94mLoss[0m : 8.21738

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.309, [92mTest[0m: 10.900, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 8.16675
[1mStep[0m  [10/106], [94mLoss[0m : 8.00879
[1mStep[0m  [20/106], [94mLoss[0m : 7.18691
[1mStep[0m  [30/106], [94mLoss[0m : 7.33036
[1mStep[0m  [40/106], [94mLoss[0m : 7.12563
[1mStep[0m  [50/106], [94mLoss[0m : 6.30653
[1mStep[0m  [60/106], [94mLoss[0m : 6.14551
[1mStep[0m  [70/106], [94mLoss[0m : 5.68324
[1mStep[0m  [80/106], [94mLoss[0m : 5.47258
[1mStep[0m  [90/106], [94mLoss[0m : 5.47050
[1mStep[0m  [100/106], [94mLoss[0m : 5.44630

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 6.526, [92mTest[0m: 8.921, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 5.36101
[1mStep[0m  [10/106], [94mLoss[0m : 4.45324
[1mStep[0m  [20/106], [94mLoss[0m : 4.31443
[1mStep[0m  [30/106], [94mLoss[0m : 4.43319
[1mStep[0m  [40/106], [94mLoss[0m : 4.15818
[1mStep[0m  [50/106], [94mLoss[0m : 3.84616
[1mStep[0m  [60/106], [94mLoss[0m : 3.85911
[1mStep[0m  [70/106], [94mLoss[0m : 3.67518
[1mStep[0m  [80/106], [94mLoss[0m : 4.31718
[1mStep[0m  [90/106], [94mLoss[0m : 3.52499
[1mStep[0m  [100/106], [94mLoss[0m : 3.64496

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 4.203, [92mTest[0m: 6.607, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.53543
[1mStep[0m  [10/106], [94mLoss[0m : 3.61179
[1mStep[0m  [20/106], [94mLoss[0m : 3.38138
[1mStep[0m  [30/106], [94mLoss[0m : 3.29186
[1mStep[0m  [40/106], [94mLoss[0m : 3.10297
[1mStep[0m  [50/106], [94mLoss[0m : 3.38371
[1mStep[0m  [60/106], [94mLoss[0m : 3.25556
[1mStep[0m  [70/106], [94mLoss[0m : 2.89691
[1mStep[0m  [80/106], [94mLoss[0m : 3.04634
[1mStep[0m  [90/106], [94mLoss[0m : 3.26384
[1mStep[0m  [100/106], [94mLoss[0m : 3.56829

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 3.258, [92mTest[0m: 4.589, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.17044
[1mStep[0m  [10/106], [94mLoss[0m : 3.14450
[1mStep[0m  [20/106], [94mLoss[0m : 2.61218
[1mStep[0m  [30/106], [94mLoss[0m : 3.05976
[1mStep[0m  [40/106], [94mLoss[0m : 3.08872
[1mStep[0m  [50/106], [94mLoss[0m : 3.15601
[1mStep[0m  [60/106], [94mLoss[0m : 3.30682
[1mStep[0m  [70/106], [94mLoss[0m : 2.77503
[1mStep[0m  [80/106], [94mLoss[0m : 2.87210
[1mStep[0m  [90/106], [94mLoss[0m : 2.94332
[1mStep[0m  [100/106], [94mLoss[0m : 2.93709

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 3.019, [92mTest[0m: 3.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.16541
[1mStep[0m  [10/106], [94mLoss[0m : 2.75895
[1mStep[0m  [20/106], [94mLoss[0m : 3.16492
[1mStep[0m  [30/106], [94mLoss[0m : 2.71464
[1mStep[0m  [40/106], [94mLoss[0m : 3.06783
[1mStep[0m  [50/106], [94mLoss[0m : 2.88867
[1mStep[0m  [60/106], [94mLoss[0m : 3.27283
[1mStep[0m  [70/106], [94mLoss[0m : 3.11755
[1mStep[0m  [80/106], [94mLoss[0m : 3.48668
[1mStep[0m  [90/106], [94mLoss[0m : 3.03560
[1mStep[0m  [100/106], [94mLoss[0m : 2.91735

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.994, [92mTest[0m: 3.047, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72049
[1mStep[0m  [10/106], [94mLoss[0m : 3.00522
[1mStep[0m  [20/106], [94mLoss[0m : 3.06720
[1mStep[0m  [30/106], [94mLoss[0m : 3.23506
[1mStep[0m  [40/106], [94mLoss[0m : 2.96858
[1mStep[0m  [50/106], [94mLoss[0m : 2.73858
[1mStep[0m  [60/106], [94mLoss[0m : 2.93264
[1mStep[0m  [70/106], [94mLoss[0m : 3.07518
[1mStep[0m  [80/106], [94mLoss[0m : 2.91580
[1mStep[0m  [90/106], [94mLoss[0m : 2.90435
[1mStep[0m  [100/106], [94mLoss[0m : 2.66135

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.947, [92mTest[0m: 3.010, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.87045
[1mStep[0m  [10/106], [94mLoss[0m : 2.73155
[1mStep[0m  [20/106], [94mLoss[0m : 2.58317
[1mStep[0m  [30/106], [94mLoss[0m : 3.01203
[1mStep[0m  [40/106], [94mLoss[0m : 2.99893
[1mStep[0m  [50/106], [94mLoss[0m : 2.91253
[1mStep[0m  [60/106], [94mLoss[0m : 2.97052
[1mStep[0m  [70/106], [94mLoss[0m : 3.03281
[1mStep[0m  [80/106], [94mLoss[0m : 2.89630
[1mStep[0m  [90/106], [94mLoss[0m : 2.99063
[1mStep[0m  [100/106], [94mLoss[0m : 3.15080

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.932, [92mTest[0m: 2.933, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.04075
[1mStep[0m  [10/106], [94mLoss[0m : 2.88416
[1mStep[0m  [20/106], [94mLoss[0m : 2.78892
[1mStep[0m  [30/106], [94mLoss[0m : 2.80756
[1mStep[0m  [40/106], [94mLoss[0m : 3.07408
[1mStep[0m  [50/106], [94mLoss[0m : 3.04763
[1mStep[0m  [60/106], [94mLoss[0m : 2.76449
[1mStep[0m  [70/106], [94mLoss[0m : 3.01597
[1mStep[0m  [80/106], [94mLoss[0m : 2.55337
[1mStep[0m  [90/106], [94mLoss[0m : 2.80600
[1mStep[0m  [100/106], [94mLoss[0m : 3.23577

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.920, [92mTest[0m: 2.820, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61447
[1mStep[0m  [10/106], [94mLoss[0m : 3.14023
[1mStep[0m  [20/106], [94mLoss[0m : 2.97660
[1mStep[0m  [30/106], [94mLoss[0m : 2.80149
[1mStep[0m  [40/106], [94mLoss[0m : 2.72871
[1mStep[0m  [50/106], [94mLoss[0m : 3.12568
[1mStep[0m  [60/106], [94mLoss[0m : 3.02743
[1mStep[0m  [70/106], [94mLoss[0m : 2.61237
[1mStep[0m  [80/106], [94mLoss[0m : 2.72632
[1mStep[0m  [90/106], [94mLoss[0m : 2.76466
[1mStep[0m  [100/106], [94mLoss[0m : 2.73826

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.882, [92mTest[0m: 2.802, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.76539
[1mStep[0m  [10/106], [94mLoss[0m : 3.11975
[1mStep[0m  [20/106], [94mLoss[0m : 2.83398
[1mStep[0m  [30/106], [94mLoss[0m : 2.53850
[1mStep[0m  [40/106], [94mLoss[0m : 2.74442
[1mStep[0m  [50/106], [94mLoss[0m : 2.94391
[1mStep[0m  [60/106], [94mLoss[0m : 3.08451
[1mStep[0m  [70/106], [94mLoss[0m : 2.56299
[1mStep[0m  [80/106], [94mLoss[0m : 2.66915
[1mStep[0m  [90/106], [94mLoss[0m : 2.86317
[1mStep[0m  [100/106], [94mLoss[0m : 2.97084

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.888, [92mTest[0m: 2.772, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.93955
[1mStep[0m  [10/106], [94mLoss[0m : 3.01870
[1mStep[0m  [20/106], [94mLoss[0m : 2.93343
[1mStep[0m  [30/106], [94mLoss[0m : 3.06129
[1mStep[0m  [40/106], [94mLoss[0m : 2.73946
[1mStep[0m  [50/106], [94mLoss[0m : 2.94805
[1mStep[0m  [60/106], [94mLoss[0m : 2.62676
[1mStep[0m  [70/106], [94mLoss[0m : 3.01870
[1mStep[0m  [80/106], [94mLoss[0m : 3.02576
[1mStep[0m  [90/106], [94mLoss[0m : 3.06566
[1mStep[0m  [100/106], [94mLoss[0m : 3.11580

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.876, [92mTest[0m: 2.743, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65481
[1mStep[0m  [10/106], [94mLoss[0m : 2.88343
[1mStep[0m  [20/106], [94mLoss[0m : 2.82766
[1mStep[0m  [30/106], [94mLoss[0m : 2.68568
[1mStep[0m  [40/106], [94mLoss[0m : 3.00259
[1mStep[0m  [50/106], [94mLoss[0m : 2.77849
[1mStep[0m  [60/106], [94mLoss[0m : 2.98565
[1mStep[0m  [70/106], [94mLoss[0m : 2.71967
[1mStep[0m  [80/106], [94mLoss[0m : 2.35035
[1mStep[0m  [90/106], [94mLoss[0m : 2.71621
[1mStep[0m  [100/106], [94mLoss[0m : 3.02452

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.827, [92mTest[0m: 2.665, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61126
[1mStep[0m  [10/106], [94mLoss[0m : 2.57295
[1mStep[0m  [20/106], [94mLoss[0m : 3.21133
[1mStep[0m  [30/106], [94mLoss[0m : 2.81031
[1mStep[0m  [40/106], [94mLoss[0m : 2.57781
[1mStep[0m  [50/106], [94mLoss[0m : 2.87939
[1mStep[0m  [60/106], [94mLoss[0m : 2.99001
[1mStep[0m  [70/106], [94mLoss[0m : 2.88624
[1mStep[0m  [80/106], [94mLoss[0m : 3.18292
[1mStep[0m  [90/106], [94mLoss[0m : 2.88961
[1mStep[0m  [100/106], [94mLoss[0m : 2.77282

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.860, [92mTest[0m: 2.658, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.13569
[1mStep[0m  [10/106], [94mLoss[0m : 2.61462
[1mStep[0m  [20/106], [94mLoss[0m : 2.68140
[1mStep[0m  [30/106], [94mLoss[0m : 2.84985
[1mStep[0m  [40/106], [94mLoss[0m : 2.33849
[1mStep[0m  [50/106], [94mLoss[0m : 2.66951
[1mStep[0m  [60/106], [94mLoss[0m : 2.72675
[1mStep[0m  [70/106], [94mLoss[0m : 3.03619
[1mStep[0m  [80/106], [94mLoss[0m : 2.92879
[1mStep[0m  [90/106], [94mLoss[0m : 2.54249
[1mStep[0m  [100/106], [94mLoss[0m : 2.97084

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.819, [92mTest[0m: 2.637, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.90407
[1mStep[0m  [10/106], [94mLoss[0m : 3.11339
[1mStep[0m  [20/106], [94mLoss[0m : 2.99640
[1mStep[0m  [30/106], [94mLoss[0m : 2.68638
[1mStep[0m  [40/106], [94mLoss[0m : 2.82088
[1mStep[0m  [50/106], [94mLoss[0m : 2.73761
[1mStep[0m  [60/106], [94mLoss[0m : 2.83098
[1mStep[0m  [70/106], [94mLoss[0m : 2.51381
[1mStep[0m  [80/106], [94mLoss[0m : 2.72309
[1mStep[0m  [90/106], [94mLoss[0m : 2.75472
[1mStep[0m  [100/106], [94mLoss[0m : 2.53189

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.820, [92mTest[0m: 2.633, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.96038
[1mStep[0m  [10/106], [94mLoss[0m : 2.44855
[1mStep[0m  [20/106], [94mLoss[0m : 2.78660
[1mStep[0m  [30/106], [94mLoss[0m : 2.89818
[1mStep[0m  [40/106], [94mLoss[0m : 2.90601
[1mStep[0m  [50/106], [94mLoss[0m : 2.66524
[1mStep[0m  [60/106], [94mLoss[0m : 2.63610
[1mStep[0m  [70/106], [94mLoss[0m : 2.78525
[1mStep[0m  [80/106], [94mLoss[0m : 2.82765
[1mStep[0m  [90/106], [94mLoss[0m : 2.85908
[1mStep[0m  [100/106], [94mLoss[0m : 2.83743

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.772, [92mTest[0m: 2.630, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.94892
[1mStep[0m  [10/106], [94mLoss[0m : 2.87418
[1mStep[0m  [20/106], [94mLoss[0m : 2.92668
[1mStep[0m  [30/106], [94mLoss[0m : 2.84766
[1mStep[0m  [40/106], [94mLoss[0m : 2.93631
[1mStep[0m  [50/106], [94mLoss[0m : 2.80257
[1mStep[0m  [60/106], [94mLoss[0m : 2.77367
[1mStep[0m  [70/106], [94mLoss[0m : 2.88863
[1mStep[0m  [80/106], [94mLoss[0m : 2.85629
[1mStep[0m  [90/106], [94mLoss[0m : 2.56217
[1mStep[0m  [100/106], [94mLoss[0m : 2.44236

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.820, [92mTest[0m: 2.581, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 3.05169
[1mStep[0m  [10/106], [94mLoss[0m : 2.88798
[1mStep[0m  [20/106], [94mLoss[0m : 2.67204
[1mStep[0m  [30/106], [94mLoss[0m : 2.74163
[1mStep[0m  [40/106], [94mLoss[0m : 2.98490
[1mStep[0m  [50/106], [94mLoss[0m : 2.78065
[1mStep[0m  [60/106], [94mLoss[0m : 2.96355
[1mStep[0m  [70/106], [94mLoss[0m : 2.83411
[1mStep[0m  [80/106], [94mLoss[0m : 2.85042
[1mStep[0m  [90/106], [94mLoss[0m : 2.86179
[1mStep[0m  [100/106], [94mLoss[0m : 2.98466

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.777, [92mTest[0m: 2.557, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64186
[1mStep[0m  [10/106], [94mLoss[0m : 2.62958
[1mStep[0m  [20/106], [94mLoss[0m : 2.47339
[1mStep[0m  [30/106], [94mLoss[0m : 3.41319
[1mStep[0m  [40/106], [94mLoss[0m : 2.93218
[1mStep[0m  [50/106], [94mLoss[0m : 2.88923
[1mStep[0m  [60/106], [94mLoss[0m : 2.76263
[1mStep[0m  [70/106], [94mLoss[0m : 2.68771
[1mStep[0m  [80/106], [94mLoss[0m : 2.79396
[1mStep[0m  [90/106], [94mLoss[0m : 2.59870
[1mStep[0m  [100/106], [94mLoss[0m : 2.87008

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.582, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66490
[1mStep[0m  [10/106], [94mLoss[0m : 2.85726
[1mStep[0m  [20/106], [94mLoss[0m : 2.54046
[1mStep[0m  [30/106], [94mLoss[0m : 2.61469
[1mStep[0m  [40/106], [94mLoss[0m : 2.57816
[1mStep[0m  [50/106], [94mLoss[0m : 2.78879
[1mStep[0m  [60/106], [94mLoss[0m : 3.21303
[1mStep[0m  [70/106], [94mLoss[0m : 2.96127
[1mStep[0m  [80/106], [94mLoss[0m : 2.86648
[1mStep[0m  [90/106], [94mLoss[0m : 2.78525
[1mStep[0m  [100/106], [94mLoss[0m : 2.73829

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.798, [92mTest[0m: 2.554, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.95917
[1mStep[0m  [10/106], [94mLoss[0m : 3.07759
[1mStep[0m  [20/106], [94mLoss[0m : 2.92016
[1mStep[0m  [30/106], [94mLoss[0m : 2.39537
[1mStep[0m  [40/106], [94mLoss[0m : 2.58058
[1mStep[0m  [50/106], [94mLoss[0m : 2.69080
[1mStep[0m  [60/106], [94mLoss[0m : 2.56278
[1mStep[0m  [70/106], [94mLoss[0m : 2.97606
[1mStep[0m  [80/106], [94mLoss[0m : 2.56728
[1mStep[0m  [90/106], [94mLoss[0m : 2.81132
[1mStep[0m  [100/106], [94mLoss[0m : 2.99546

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.796, [92mTest[0m: 2.556, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.57443
[1mStep[0m  [10/106], [94mLoss[0m : 2.98676
[1mStep[0m  [20/106], [94mLoss[0m : 2.73906
[1mStep[0m  [30/106], [94mLoss[0m : 2.92966
[1mStep[0m  [40/106], [94mLoss[0m : 3.15828
[1mStep[0m  [50/106], [94mLoss[0m : 2.95185
[1mStep[0m  [60/106], [94mLoss[0m : 3.06192
[1mStep[0m  [70/106], [94mLoss[0m : 2.53513
[1mStep[0m  [80/106], [94mLoss[0m : 2.75884
[1mStep[0m  [90/106], [94mLoss[0m : 3.24654
[1mStep[0m  [100/106], [94mLoss[0m : 2.93321

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.782, [92mTest[0m: 2.512, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.92298
[1mStep[0m  [10/106], [94mLoss[0m : 2.59899
[1mStep[0m  [20/106], [94mLoss[0m : 3.10974
[1mStep[0m  [30/106], [94mLoss[0m : 2.53023
[1mStep[0m  [40/106], [94mLoss[0m : 2.56652
[1mStep[0m  [50/106], [94mLoss[0m : 2.77316
[1mStep[0m  [60/106], [94mLoss[0m : 2.23790
[1mStep[0m  [70/106], [94mLoss[0m : 2.73096
[1mStep[0m  [80/106], [94mLoss[0m : 2.91673
[1mStep[0m  [90/106], [94mLoss[0m : 2.85129
[1mStep[0m  [100/106], [94mLoss[0m : 2.42882

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.776, [92mTest[0m: 2.519, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.87415
[1mStep[0m  [10/106], [94mLoss[0m : 2.60473
[1mStep[0m  [20/106], [94mLoss[0m : 2.65429
[1mStep[0m  [30/106], [94mLoss[0m : 2.76206
[1mStep[0m  [40/106], [94mLoss[0m : 2.38987
[1mStep[0m  [50/106], [94mLoss[0m : 2.56092
[1mStep[0m  [60/106], [94mLoss[0m : 2.86589
[1mStep[0m  [70/106], [94mLoss[0m : 2.53290
[1mStep[0m  [80/106], [94mLoss[0m : 2.23562
[1mStep[0m  [90/106], [94mLoss[0m : 2.80378
[1mStep[0m  [100/106], [94mLoss[0m : 2.62876

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.754, [92mTest[0m: 2.511, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.66909
[1mStep[0m  [10/106], [94mLoss[0m : 2.67589
[1mStep[0m  [20/106], [94mLoss[0m : 2.88362
[1mStep[0m  [30/106], [94mLoss[0m : 2.65513
[1mStep[0m  [40/106], [94mLoss[0m : 2.77494
[1mStep[0m  [50/106], [94mLoss[0m : 2.42706
[1mStep[0m  [60/106], [94mLoss[0m : 3.02343
[1mStep[0m  [70/106], [94mLoss[0m : 2.93219
[1mStep[0m  [80/106], [94mLoss[0m : 2.73391
[1mStep[0m  [90/106], [94mLoss[0m : 2.81689
[1mStep[0m  [100/106], [94mLoss[0m : 2.90958

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.752, [92mTest[0m: 2.500, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62361
[1mStep[0m  [10/106], [94mLoss[0m : 2.64988
[1mStep[0m  [20/106], [94mLoss[0m : 2.77991
[1mStep[0m  [30/106], [94mLoss[0m : 2.83331
[1mStep[0m  [40/106], [94mLoss[0m : 2.59866
[1mStep[0m  [50/106], [94mLoss[0m : 3.03397
[1mStep[0m  [60/106], [94mLoss[0m : 2.80904
[1mStep[0m  [70/106], [94mLoss[0m : 2.68314
[1mStep[0m  [80/106], [94mLoss[0m : 2.72232
[1mStep[0m  [90/106], [94mLoss[0m : 2.74048
[1mStep[0m  [100/106], [94mLoss[0m : 2.86040

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.758, [92mTest[0m: 2.505, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54641
[1mStep[0m  [10/106], [94mLoss[0m : 2.78558
[1mStep[0m  [20/106], [94mLoss[0m : 2.80888
[1mStep[0m  [30/106], [94mLoss[0m : 2.25420
[1mStep[0m  [40/106], [94mLoss[0m : 2.97953
[1mStep[0m  [50/106], [94mLoss[0m : 2.85426
[1mStep[0m  [60/106], [94mLoss[0m : 2.55796
[1mStep[0m  [70/106], [94mLoss[0m : 2.91811
[1mStep[0m  [80/106], [94mLoss[0m : 2.91548
[1mStep[0m  [90/106], [94mLoss[0m : 2.67163
[1mStep[0m  [100/106], [94mLoss[0m : 2.53448

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.725, [92mTest[0m: 2.510, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48080
[1mStep[0m  [10/106], [94mLoss[0m : 2.69534
[1mStep[0m  [20/106], [94mLoss[0m : 2.78283
[1mStep[0m  [30/106], [94mLoss[0m : 2.63461
[1mStep[0m  [40/106], [94mLoss[0m : 2.69560
[1mStep[0m  [50/106], [94mLoss[0m : 2.99598
[1mStep[0m  [60/106], [94mLoss[0m : 3.11040
[1mStep[0m  [70/106], [94mLoss[0m : 2.69725
[1mStep[0m  [80/106], [94mLoss[0m : 2.86577
[1mStep[0m  [90/106], [94mLoss[0m : 2.56633
[1mStep[0m  [100/106], [94mLoss[0m : 2.45911

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.747, [92mTest[0m: 2.489, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63838
[1mStep[0m  [10/106], [94mLoss[0m : 3.08867
[1mStep[0m  [20/106], [94mLoss[0m : 2.74231
[1mStep[0m  [30/106], [94mLoss[0m : 2.66351
[1mStep[0m  [40/106], [94mLoss[0m : 2.86987
[1mStep[0m  [50/106], [94mLoss[0m : 2.64622
[1mStep[0m  [60/106], [94mLoss[0m : 2.67087
[1mStep[0m  [70/106], [94mLoss[0m : 3.21596
[1mStep[0m  [80/106], [94mLoss[0m : 3.12409
[1mStep[0m  [90/106], [94mLoss[0m : 2.55643
[1mStep[0m  [100/106], [94mLoss[0m : 2.73942

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.755, [92mTest[0m: 2.488, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.487
====================================

Phase 1 - Evaluation MAE:  2.4871835888556713
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/106], [94mLoss[0m : 2.74656
[1mStep[0m  [10/106], [94mLoss[0m : 2.86506
[1mStep[0m  [20/106], [94mLoss[0m : 2.86508
[1mStep[0m  [30/106], [94mLoss[0m : 2.50846
[1mStep[0m  [40/106], [94mLoss[0m : 2.76353
[1mStep[0m  [50/106], [94mLoss[0m : 2.82838
[1mStep[0m  [60/106], [94mLoss[0m : 3.02120
[1mStep[0m  [70/106], [94mLoss[0m : 2.70213
[1mStep[0m  [80/106], [94mLoss[0m : 2.99929
[1mStep[0m  [90/106], [94mLoss[0m : 2.79624
[1mStep[0m  [100/106], [94mLoss[0m : 2.92982

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.783, [92mTest[0m: 2.483, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63517
[1mStep[0m  [10/106], [94mLoss[0m : 2.51115
[1mStep[0m  [20/106], [94mLoss[0m : 2.78871
[1mStep[0m  [30/106], [94mLoss[0m : 2.44631
[1mStep[0m  [40/106], [94mLoss[0m : 2.63717
[1mStep[0m  [50/106], [94mLoss[0m : 2.87557
[1mStep[0m  [60/106], [94mLoss[0m : 2.89166
[1mStep[0m  [70/106], [94mLoss[0m : 2.62169
[1mStep[0m  [80/106], [94mLoss[0m : 2.95145
[1mStep[0m  [90/106], [94mLoss[0m : 2.95202
[1mStep[0m  [100/106], [94mLoss[0m : 2.53575

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.775, [92mTest[0m: 2.740, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73637
[1mStep[0m  [10/106], [94mLoss[0m : 2.94699
[1mStep[0m  [20/106], [94mLoss[0m : 2.98080
[1mStep[0m  [30/106], [94mLoss[0m : 2.80646
[1mStep[0m  [40/106], [94mLoss[0m : 2.74332
[1mStep[0m  [50/106], [94mLoss[0m : 2.61693
[1mStep[0m  [60/106], [94mLoss[0m : 2.81583
[1mStep[0m  [70/106], [94mLoss[0m : 2.79074
[1mStep[0m  [80/106], [94mLoss[0m : 2.67425
[1mStep[0m  [90/106], [94mLoss[0m : 2.39908
[1mStep[0m  [100/106], [94mLoss[0m : 2.91603

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.753, [92mTest[0m: 2.697, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56582
[1mStep[0m  [10/106], [94mLoss[0m : 2.81825
[1mStep[0m  [20/106], [94mLoss[0m : 2.98546
[1mStep[0m  [30/106], [94mLoss[0m : 2.78898
[1mStep[0m  [40/106], [94mLoss[0m : 2.90902
[1mStep[0m  [50/106], [94mLoss[0m : 2.68215
[1mStep[0m  [60/106], [94mLoss[0m : 2.67071
[1mStep[0m  [70/106], [94mLoss[0m : 2.42982
[1mStep[0m  [80/106], [94mLoss[0m : 2.80979
[1mStep[0m  [90/106], [94mLoss[0m : 2.36935
[1mStep[0m  [100/106], [94mLoss[0m : 2.80233

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.738, [92mTest[0m: 2.720, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.73155
[1mStep[0m  [10/106], [94mLoss[0m : 2.21252
[1mStep[0m  [20/106], [94mLoss[0m : 2.52735
[1mStep[0m  [30/106], [94mLoss[0m : 2.82979
[1mStep[0m  [40/106], [94mLoss[0m : 2.65210
[1mStep[0m  [50/106], [94mLoss[0m : 3.26985
[1mStep[0m  [60/106], [94mLoss[0m : 2.74518
[1mStep[0m  [70/106], [94mLoss[0m : 2.46916
[1mStep[0m  [80/106], [94mLoss[0m : 2.88451
[1mStep[0m  [90/106], [94mLoss[0m : 2.56823
[1mStep[0m  [100/106], [94mLoss[0m : 2.76640

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.755, [92mTest[0m: 2.715, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58768
[1mStep[0m  [10/106], [94mLoss[0m : 2.80513
[1mStep[0m  [20/106], [94mLoss[0m : 3.17761
[1mStep[0m  [30/106], [94mLoss[0m : 2.80437
[1mStep[0m  [40/106], [94mLoss[0m : 2.78904
[1mStep[0m  [50/106], [94mLoss[0m : 2.83100
[1mStep[0m  [60/106], [94mLoss[0m : 2.69162
[1mStep[0m  [70/106], [94mLoss[0m : 2.42025
[1mStep[0m  [80/106], [94mLoss[0m : 2.80433
[1mStep[0m  [90/106], [94mLoss[0m : 2.66098
[1mStep[0m  [100/106], [94mLoss[0m : 2.48524

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.694, [92mTest[0m: 2.659, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.75893
[1mStep[0m  [10/106], [94mLoss[0m : 2.68184
[1mStep[0m  [20/106], [94mLoss[0m : 3.02830
[1mStep[0m  [30/106], [94mLoss[0m : 2.67185
[1mStep[0m  [40/106], [94mLoss[0m : 2.64378
[1mStep[0m  [50/106], [94mLoss[0m : 2.43201
[1mStep[0m  [60/106], [94mLoss[0m : 2.55432
[1mStep[0m  [70/106], [94mLoss[0m : 2.67904
[1mStep[0m  [80/106], [94mLoss[0m : 2.58832
[1mStep[0m  [90/106], [94mLoss[0m : 2.87809
[1mStep[0m  [100/106], [94mLoss[0m : 2.80436

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.703, [92mTest[0m: 2.642, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.59516
[1mStep[0m  [10/106], [94mLoss[0m : 2.72648
[1mStep[0m  [20/106], [94mLoss[0m : 2.73139
[1mStep[0m  [30/106], [94mLoss[0m : 2.69889
[1mStep[0m  [40/106], [94mLoss[0m : 2.68278
[1mStep[0m  [50/106], [94mLoss[0m : 2.92198
[1mStep[0m  [60/106], [94mLoss[0m : 2.73369
[1mStep[0m  [70/106], [94mLoss[0m : 2.67759
[1mStep[0m  [80/106], [94mLoss[0m : 2.65477
[1mStep[0m  [90/106], [94mLoss[0m : 2.85233
[1mStep[0m  [100/106], [94mLoss[0m : 2.77292

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.719, [92mTest[0m: 2.582, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50774
[1mStep[0m  [10/106], [94mLoss[0m : 2.35418
[1mStep[0m  [20/106], [94mLoss[0m : 2.98479
[1mStep[0m  [30/106], [94mLoss[0m : 3.12488
[1mStep[0m  [40/106], [94mLoss[0m : 2.45477
[1mStep[0m  [50/106], [94mLoss[0m : 2.43570
[1mStep[0m  [60/106], [94mLoss[0m : 2.60313
[1mStep[0m  [70/106], [94mLoss[0m : 2.74088
[1mStep[0m  [80/106], [94mLoss[0m : 2.56376
[1mStep[0m  [90/106], [94mLoss[0m : 2.44170
[1mStep[0m  [100/106], [94mLoss[0m : 2.90830

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.670, [92mTest[0m: 2.539, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.98283
[1mStep[0m  [10/106], [94mLoss[0m : 2.57831
[1mStep[0m  [20/106], [94mLoss[0m : 2.84743
[1mStep[0m  [30/106], [94mLoss[0m : 2.70406
[1mStep[0m  [40/106], [94mLoss[0m : 2.75005
[1mStep[0m  [50/106], [94mLoss[0m : 2.66103
[1mStep[0m  [60/106], [94mLoss[0m : 2.67690
[1mStep[0m  [70/106], [94mLoss[0m : 2.62432
[1mStep[0m  [80/106], [94mLoss[0m : 2.54038
[1mStep[0m  [90/106], [94mLoss[0m : 2.70712
[1mStep[0m  [100/106], [94mLoss[0m : 2.57311

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.665, [92mTest[0m: 2.540, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67090
[1mStep[0m  [10/106], [94mLoss[0m : 2.98919
[1mStep[0m  [20/106], [94mLoss[0m : 2.48968
[1mStep[0m  [30/106], [94mLoss[0m : 2.35017
[1mStep[0m  [40/106], [94mLoss[0m : 2.56575
[1mStep[0m  [50/106], [94mLoss[0m : 2.60698
[1mStep[0m  [60/106], [94mLoss[0m : 2.80821
[1mStep[0m  [70/106], [94mLoss[0m : 2.81331
[1mStep[0m  [80/106], [94mLoss[0m : 2.36342
[1mStep[0m  [90/106], [94mLoss[0m : 2.50539
[1mStep[0m  [100/106], [94mLoss[0m : 2.40810

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.640, [92mTest[0m: 2.556, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30645
[1mStep[0m  [10/106], [94mLoss[0m : 2.48587
[1mStep[0m  [20/106], [94mLoss[0m : 2.93894
[1mStep[0m  [30/106], [94mLoss[0m : 2.96685
[1mStep[0m  [40/106], [94mLoss[0m : 2.66769
[1mStep[0m  [50/106], [94mLoss[0m : 2.43598
[1mStep[0m  [60/106], [94mLoss[0m : 2.64599
[1mStep[0m  [70/106], [94mLoss[0m : 2.21617
[1mStep[0m  [80/106], [94mLoss[0m : 2.50585
[1mStep[0m  [90/106], [94mLoss[0m : 2.76155
[1mStep[0m  [100/106], [94mLoss[0m : 2.73559

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.642, [92mTest[0m: 2.548, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64073
[1mStep[0m  [10/106], [94mLoss[0m : 2.77303
[1mStep[0m  [20/106], [94mLoss[0m : 2.77289
[1mStep[0m  [30/106], [94mLoss[0m : 2.68353
[1mStep[0m  [40/106], [94mLoss[0m : 2.30595
[1mStep[0m  [50/106], [94mLoss[0m : 2.73468
[1mStep[0m  [60/106], [94mLoss[0m : 2.67026
[1mStep[0m  [70/106], [94mLoss[0m : 2.57071
[1mStep[0m  [80/106], [94mLoss[0m : 2.88153
[1mStep[0m  [90/106], [94mLoss[0m : 2.51632
[1mStep[0m  [100/106], [94mLoss[0m : 2.56844

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.620, [92mTest[0m: 2.497, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49016
[1mStep[0m  [10/106], [94mLoss[0m : 2.72596
[1mStep[0m  [20/106], [94mLoss[0m : 2.63402
[1mStep[0m  [30/106], [94mLoss[0m : 2.69781
[1mStep[0m  [40/106], [94mLoss[0m : 2.33540
[1mStep[0m  [50/106], [94mLoss[0m : 2.42344
[1mStep[0m  [60/106], [94mLoss[0m : 2.68552
[1mStep[0m  [70/106], [94mLoss[0m : 2.79573
[1mStep[0m  [80/106], [94mLoss[0m : 2.73388
[1mStep[0m  [90/106], [94mLoss[0m : 2.60320
[1mStep[0m  [100/106], [94mLoss[0m : 2.63835

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.58576
[1mStep[0m  [10/106], [94mLoss[0m : 2.50603
[1mStep[0m  [20/106], [94mLoss[0m : 2.49372
[1mStep[0m  [30/106], [94mLoss[0m : 2.62236
[1mStep[0m  [40/106], [94mLoss[0m : 2.44876
[1mStep[0m  [50/106], [94mLoss[0m : 2.59785
[1mStep[0m  [60/106], [94mLoss[0m : 2.68296
[1mStep[0m  [70/106], [94mLoss[0m : 2.72731
[1mStep[0m  [80/106], [94mLoss[0m : 2.53776
[1mStep[0m  [90/106], [94mLoss[0m : 2.81685
[1mStep[0m  [100/106], [94mLoss[0m : 2.43381

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.582, [92mTest[0m: 2.553, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68622
[1mStep[0m  [10/106], [94mLoss[0m : 2.38443
[1mStep[0m  [20/106], [94mLoss[0m : 2.60272
[1mStep[0m  [30/106], [94mLoss[0m : 2.28306
[1mStep[0m  [40/106], [94mLoss[0m : 2.35760
[1mStep[0m  [50/106], [94mLoss[0m : 2.88363
[1mStep[0m  [60/106], [94mLoss[0m : 2.44696
[1mStep[0m  [70/106], [94mLoss[0m : 2.73677
[1mStep[0m  [80/106], [94mLoss[0m : 2.28794
[1mStep[0m  [90/106], [94mLoss[0m : 2.66817
[1mStep[0m  [100/106], [94mLoss[0m : 2.95370

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.56067
[1mStep[0m  [10/106], [94mLoss[0m : 2.48709
[1mStep[0m  [20/106], [94mLoss[0m : 2.42731
[1mStep[0m  [30/106], [94mLoss[0m : 3.02910
[1mStep[0m  [40/106], [94mLoss[0m : 2.44639
[1mStep[0m  [50/106], [94mLoss[0m : 2.60405
[1mStep[0m  [60/106], [94mLoss[0m : 2.49505
[1mStep[0m  [70/106], [94mLoss[0m : 2.69214
[1mStep[0m  [80/106], [94mLoss[0m : 2.58684
[1mStep[0m  [90/106], [94mLoss[0m : 2.64675
[1mStep[0m  [100/106], [94mLoss[0m : 2.56383

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.552, [92mTest[0m: 2.516, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.55131
[1mStep[0m  [10/106], [94mLoss[0m : 2.37810
[1mStep[0m  [20/106], [94mLoss[0m : 2.57498
[1mStep[0m  [30/106], [94mLoss[0m : 2.80887
[1mStep[0m  [40/106], [94mLoss[0m : 2.60259
[1mStep[0m  [50/106], [94mLoss[0m : 2.68644
[1mStep[0m  [60/106], [94mLoss[0m : 2.80233
[1mStep[0m  [70/106], [94mLoss[0m : 2.39098
[1mStep[0m  [80/106], [94mLoss[0m : 2.27244
[1mStep[0m  [90/106], [94mLoss[0m : 2.43931
[1mStep[0m  [100/106], [94mLoss[0m : 2.52455

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.532, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21779
[1mStep[0m  [10/106], [94mLoss[0m : 2.49877
[1mStep[0m  [20/106], [94mLoss[0m : 2.71457
[1mStep[0m  [30/106], [94mLoss[0m : 2.58701
[1mStep[0m  [40/106], [94mLoss[0m : 2.19872
[1mStep[0m  [50/106], [94mLoss[0m : 2.06863
[1mStep[0m  [60/106], [94mLoss[0m : 2.41295
[1mStep[0m  [70/106], [94mLoss[0m : 2.71997
[1mStep[0m  [80/106], [94mLoss[0m : 2.37067
[1mStep[0m  [90/106], [94mLoss[0m : 2.57298
[1mStep[0m  [100/106], [94mLoss[0m : 2.40884

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.515, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.45255
[1mStep[0m  [10/106], [94mLoss[0m : 2.24200
[1mStep[0m  [20/106], [94mLoss[0m : 2.38360
[1mStep[0m  [30/106], [94mLoss[0m : 2.31743
[1mStep[0m  [40/106], [94mLoss[0m : 2.76988
[1mStep[0m  [50/106], [94mLoss[0m : 2.58065
[1mStep[0m  [60/106], [94mLoss[0m : 2.61361
[1mStep[0m  [70/106], [94mLoss[0m : 2.33173
[1mStep[0m  [80/106], [94mLoss[0m : 2.42392
[1mStep[0m  [90/106], [94mLoss[0m : 2.65419
[1mStep[0m  [100/106], [94mLoss[0m : 2.61657

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.564, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65878
[1mStep[0m  [10/106], [94mLoss[0m : 2.24238
[1mStep[0m  [20/106], [94mLoss[0m : 2.43858
[1mStep[0m  [30/106], [94mLoss[0m : 2.41915
[1mStep[0m  [40/106], [94mLoss[0m : 2.45150
[1mStep[0m  [50/106], [94mLoss[0m : 2.36027
[1mStep[0m  [60/106], [94mLoss[0m : 2.39065
[1mStep[0m  [70/106], [94mLoss[0m : 2.51034
[1mStep[0m  [80/106], [94mLoss[0m : 2.13692
[1mStep[0m  [90/106], [94mLoss[0m : 2.35475
[1mStep[0m  [100/106], [94mLoss[0m : 2.20536

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.578, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27623
[1mStep[0m  [10/106], [94mLoss[0m : 3.02368
[1mStep[0m  [20/106], [94mLoss[0m : 2.39153
[1mStep[0m  [30/106], [94mLoss[0m : 2.60478
[1mStep[0m  [40/106], [94mLoss[0m : 2.82355
[1mStep[0m  [50/106], [94mLoss[0m : 2.88955
[1mStep[0m  [60/106], [94mLoss[0m : 2.54598
[1mStep[0m  [70/106], [94mLoss[0m : 2.68755
[1mStep[0m  [80/106], [94mLoss[0m : 2.42651
[1mStep[0m  [90/106], [94mLoss[0m : 2.74458
[1mStep[0m  [100/106], [94mLoss[0m : 2.35808

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.549, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40491
[1mStep[0m  [10/106], [94mLoss[0m : 2.15618
[1mStep[0m  [20/106], [94mLoss[0m : 2.45307
[1mStep[0m  [30/106], [94mLoss[0m : 2.76487
[1mStep[0m  [40/106], [94mLoss[0m : 2.31348
[1mStep[0m  [50/106], [94mLoss[0m : 2.45709
[1mStep[0m  [60/106], [94mLoss[0m : 2.95831
[1mStep[0m  [70/106], [94mLoss[0m : 2.31921
[1mStep[0m  [80/106], [94mLoss[0m : 2.46257
[1mStep[0m  [90/106], [94mLoss[0m : 2.40599
[1mStep[0m  [100/106], [94mLoss[0m : 2.74313

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.528, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52296
[1mStep[0m  [10/106], [94mLoss[0m : 2.41248
[1mStep[0m  [20/106], [94mLoss[0m : 2.38412
[1mStep[0m  [30/106], [94mLoss[0m : 2.48193
[1mStep[0m  [40/106], [94mLoss[0m : 2.49001
[1mStep[0m  [50/106], [94mLoss[0m : 2.46754
[1mStep[0m  [60/106], [94mLoss[0m : 2.51866
[1mStep[0m  [70/106], [94mLoss[0m : 2.26891
[1mStep[0m  [80/106], [94mLoss[0m : 2.47991
[1mStep[0m  [90/106], [94mLoss[0m : 2.25231
[1mStep[0m  [100/106], [94mLoss[0m : 2.54090

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.521, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.14035
[1mStep[0m  [10/106], [94mLoss[0m : 2.57204
[1mStep[0m  [20/106], [94mLoss[0m : 2.29825
[1mStep[0m  [30/106], [94mLoss[0m : 2.31789
[1mStep[0m  [40/106], [94mLoss[0m : 2.50914
[1mStep[0m  [50/106], [94mLoss[0m : 2.12588
[1mStep[0m  [60/106], [94mLoss[0m : 2.39235
[1mStep[0m  [70/106], [94mLoss[0m : 2.33822
[1mStep[0m  [80/106], [94mLoss[0m : 2.64262
[1mStep[0m  [90/106], [94mLoss[0m : 2.28160
[1mStep[0m  [100/106], [94mLoss[0m : 2.31297

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.508, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18156
[1mStep[0m  [10/106], [94mLoss[0m : 2.18853
[1mStep[0m  [20/106], [94mLoss[0m : 2.23073
[1mStep[0m  [30/106], [94mLoss[0m : 2.40562
[1mStep[0m  [40/106], [94mLoss[0m : 2.37725
[1mStep[0m  [50/106], [94mLoss[0m : 2.44501
[1mStep[0m  [60/106], [94mLoss[0m : 2.61875
[1mStep[0m  [70/106], [94mLoss[0m : 2.31147
[1mStep[0m  [80/106], [94mLoss[0m : 2.52660
[1mStep[0m  [90/106], [94mLoss[0m : 2.32393
[1mStep[0m  [100/106], [94mLoss[0m : 2.28700

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.504, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38482
[1mStep[0m  [10/106], [94mLoss[0m : 2.32390
[1mStep[0m  [20/106], [94mLoss[0m : 2.49867
[1mStep[0m  [30/106], [94mLoss[0m : 2.60481
[1mStep[0m  [40/106], [94mLoss[0m : 2.17501
[1mStep[0m  [50/106], [94mLoss[0m : 2.15322
[1mStep[0m  [60/106], [94mLoss[0m : 2.38734
[1mStep[0m  [70/106], [94mLoss[0m : 2.35955
[1mStep[0m  [80/106], [94mLoss[0m : 2.52580
[1mStep[0m  [90/106], [94mLoss[0m : 2.50218
[1mStep[0m  [100/106], [94mLoss[0m : 2.22568

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.526, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30632
[1mStep[0m  [10/106], [94mLoss[0m : 2.30646
[1mStep[0m  [20/106], [94mLoss[0m : 2.18492
[1mStep[0m  [30/106], [94mLoss[0m : 2.33004
[1mStep[0m  [40/106], [94mLoss[0m : 2.25388
[1mStep[0m  [50/106], [94mLoss[0m : 2.72617
[1mStep[0m  [60/106], [94mLoss[0m : 2.09597
[1mStep[0m  [70/106], [94mLoss[0m : 2.29430
[1mStep[0m  [80/106], [94mLoss[0m : 2.55358
[1mStep[0m  [90/106], [94mLoss[0m : 2.14515
[1mStep[0m  [100/106], [94mLoss[0m : 2.57754

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.514, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64804
[1mStep[0m  [10/106], [94mLoss[0m : 2.38557
[1mStep[0m  [20/106], [94mLoss[0m : 2.42029
[1mStep[0m  [30/106], [94mLoss[0m : 2.24748
[1mStep[0m  [40/106], [94mLoss[0m : 2.43232
[1mStep[0m  [50/106], [94mLoss[0m : 2.29567
[1mStep[0m  [60/106], [94mLoss[0m : 2.39548
[1mStep[0m  [70/106], [94mLoss[0m : 2.65540
[1mStep[0m  [80/106], [94mLoss[0m : 2.47855
[1mStep[0m  [90/106], [94mLoss[0m : 2.45733
[1mStep[0m  [100/106], [94mLoss[0m : 2.41840

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.348, [92mTest[0m: 2.527, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50807
[1mStep[0m  [10/106], [94mLoss[0m : 2.22461
[1mStep[0m  [20/106], [94mLoss[0m : 2.33756
[1mStep[0m  [30/106], [94mLoss[0m : 2.32470
[1mStep[0m  [40/106], [94mLoss[0m : 2.19976
[1mStep[0m  [50/106], [94mLoss[0m : 2.10583
[1mStep[0m  [60/106], [94mLoss[0m : 2.14914
[1mStep[0m  [70/106], [94mLoss[0m : 2.41957
[1mStep[0m  [80/106], [94mLoss[0m : 2.50842
[1mStep[0m  [90/106], [94mLoss[0m : 2.51809
[1mStep[0m  [100/106], [94mLoss[0m : 2.01109

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.572, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.533
====================================

Phase 2 - Evaluation MAE:  2.5329857367389605
MAE score P1       2.487184
MAE score P2       2.532986
loss               2.332397
learning_rate      0.002575
batch_size              128
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.1
weight_decay         0.0001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 10.68347
[1mStep[0m  [10/106], [94mLoss[0m : 9.11804
[1mStep[0m  [20/106], [94mLoss[0m : 7.84805
[1mStep[0m  [30/106], [94mLoss[0m : 6.51790
[1mStep[0m  [40/106], [94mLoss[0m : 5.10759
[1mStep[0m  [50/106], [94mLoss[0m : 4.48583
[1mStep[0m  [60/106], [94mLoss[0m : 3.43636
[1mStep[0m  [70/106], [94mLoss[0m : 3.21980
[1mStep[0m  [80/106], [94mLoss[0m : 2.77101
[1mStep[0m  [90/106], [94mLoss[0m : 2.76848
[1mStep[0m  [100/106], [94mLoss[0m : 2.28809

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 5.299, [92mTest[0m: 10.714, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67087
[1mStep[0m  [10/106], [94mLoss[0m : 2.38386
[1mStep[0m  [20/106], [94mLoss[0m : 2.62617
[1mStep[0m  [30/106], [94mLoss[0m : 2.59028
[1mStep[0m  [40/106], [94mLoss[0m : 2.37685
[1mStep[0m  [50/106], [94mLoss[0m : 2.43905
[1mStep[0m  [60/106], [94mLoss[0m : 2.62359
[1mStep[0m  [70/106], [94mLoss[0m : 2.36444
[1mStep[0m  [80/106], [94mLoss[0m : 2.50191
[1mStep[0m  [90/106], [94mLoss[0m : 2.88592
[1mStep[0m  [100/106], [94mLoss[0m : 2.55810

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.570, [92mTest[0m: 2.585, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49244
[1mStep[0m  [10/106], [94mLoss[0m : 2.78793
[1mStep[0m  [20/106], [94mLoss[0m : 2.39810
[1mStep[0m  [30/106], [94mLoss[0m : 2.54179
[1mStep[0m  [40/106], [94mLoss[0m : 2.49977
[1mStep[0m  [50/106], [94mLoss[0m : 2.35414
[1mStep[0m  [60/106], [94mLoss[0m : 2.48049
[1mStep[0m  [70/106], [94mLoss[0m : 2.52045
[1mStep[0m  [80/106], [94mLoss[0m : 2.39761
[1mStep[0m  [90/106], [94mLoss[0m : 2.50685
[1mStep[0m  [100/106], [94mLoss[0m : 2.50051

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.519, [92mTest[0m: 2.476, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.72807
[1mStep[0m  [10/106], [94mLoss[0m : 2.41381
[1mStep[0m  [20/106], [94mLoss[0m : 2.57841
[1mStep[0m  [30/106], [94mLoss[0m : 2.66295
[1mStep[0m  [40/106], [94mLoss[0m : 2.77902
[1mStep[0m  [50/106], [94mLoss[0m : 2.50363
[1mStep[0m  [60/106], [94mLoss[0m : 2.39262
[1mStep[0m  [70/106], [94mLoss[0m : 2.64776
[1mStep[0m  [80/106], [94mLoss[0m : 2.53926
[1mStep[0m  [90/106], [94mLoss[0m : 2.41696
[1mStep[0m  [100/106], [94mLoss[0m : 2.53773

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.517, [92mTest[0m: 2.454, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38614
[1mStep[0m  [10/106], [94mLoss[0m : 2.62547
[1mStep[0m  [20/106], [94mLoss[0m : 2.41023
[1mStep[0m  [30/106], [94mLoss[0m : 2.32405
[1mStep[0m  [40/106], [94mLoss[0m : 2.69467
[1mStep[0m  [50/106], [94mLoss[0m : 2.71571
[1mStep[0m  [60/106], [94mLoss[0m : 2.67999
[1mStep[0m  [70/106], [94mLoss[0m : 2.55505
[1mStep[0m  [80/106], [94mLoss[0m : 2.33663
[1mStep[0m  [90/106], [94mLoss[0m : 2.25079
[1mStep[0m  [100/106], [94mLoss[0m : 2.29473

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.443, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46634
[1mStep[0m  [10/106], [94mLoss[0m : 2.57416
[1mStep[0m  [20/106], [94mLoss[0m : 2.63010
[1mStep[0m  [30/106], [94mLoss[0m : 2.54699
[1mStep[0m  [40/106], [94mLoss[0m : 2.75948
[1mStep[0m  [50/106], [94mLoss[0m : 2.73917
[1mStep[0m  [60/106], [94mLoss[0m : 2.34098
[1mStep[0m  [70/106], [94mLoss[0m : 2.56826
[1mStep[0m  [80/106], [94mLoss[0m : 2.36651
[1mStep[0m  [90/106], [94mLoss[0m : 2.64917
[1mStep[0m  [100/106], [94mLoss[0m : 2.37857

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30340
[1mStep[0m  [10/106], [94mLoss[0m : 2.44022
[1mStep[0m  [20/106], [94mLoss[0m : 2.51829
[1mStep[0m  [30/106], [94mLoss[0m : 2.34349
[1mStep[0m  [40/106], [94mLoss[0m : 2.37023
[1mStep[0m  [50/106], [94mLoss[0m : 2.69086
[1mStep[0m  [60/106], [94mLoss[0m : 2.29914
[1mStep[0m  [70/106], [94mLoss[0m : 2.24451
[1mStep[0m  [80/106], [94mLoss[0m : 2.67335
[1mStep[0m  [90/106], [94mLoss[0m : 2.57342
[1mStep[0m  [100/106], [94mLoss[0m : 2.39066

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.426, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.68182
[1mStep[0m  [10/106], [94mLoss[0m : 2.38680
[1mStep[0m  [20/106], [94mLoss[0m : 2.29939
[1mStep[0m  [30/106], [94mLoss[0m : 1.96216
[1mStep[0m  [40/106], [94mLoss[0m : 2.57622
[1mStep[0m  [50/106], [94mLoss[0m : 2.29280
[1mStep[0m  [60/106], [94mLoss[0m : 2.48271
[1mStep[0m  [70/106], [94mLoss[0m : 2.57789
[1mStep[0m  [80/106], [94mLoss[0m : 2.46637
[1mStep[0m  [90/106], [94mLoss[0m : 2.15981
[1mStep[0m  [100/106], [94mLoss[0m : 2.66663

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.427, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.28650
[1mStep[0m  [10/106], [94mLoss[0m : 2.30291
[1mStep[0m  [20/106], [94mLoss[0m : 2.33749
[1mStep[0m  [30/106], [94mLoss[0m : 2.39010
[1mStep[0m  [40/106], [94mLoss[0m : 2.17081
[1mStep[0m  [50/106], [94mLoss[0m : 2.27151
[1mStep[0m  [60/106], [94mLoss[0m : 2.68036
[1mStep[0m  [70/106], [94mLoss[0m : 2.62255
[1mStep[0m  [80/106], [94mLoss[0m : 2.30849
[1mStep[0m  [90/106], [94mLoss[0m : 2.33929
[1mStep[0m  [100/106], [94mLoss[0m : 2.48531

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.423, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.30295
[1mStep[0m  [10/106], [94mLoss[0m : 2.35270
[1mStep[0m  [20/106], [94mLoss[0m : 2.54786
[1mStep[0m  [30/106], [94mLoss[0m : 2.51157
[1mStep[0m  [40/106], [94mLoss[0m : 2.19082
[1mStep[0m  [50/106], [94mLoss[0m : 2.86892
[1mStep[0m  [60/106], [94mLoss[0m : 2.53855
[1mStep[0m  [70/106], [94mLoss[0m : 2.55945
[1mStep[0m  [80/106], [94mLoss[0m : 2.48313
[1mStep[0m  [90/106], [94mLoss[0m : 2.69644
[1mStep[0m  [100/106], [94mLoss[0m : 2.62013

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.60499
[1mStep[0m  [10/106], [94mLoss[0m : 2.37652
[1mStep[0m  [20/106], [94mLoss[0m : 2.53439
[1mStep[0m  [30/106], [94mLoss[0m : 2.52517
[1mStep[0m  [40/106], [94mLoss[0m : 2.40714
[1mStep[0m  [50/106], [94mLoss[0m : 2.93517
[1mStep[0m  [60/106], [94mLoss[0m : 2.58596
[1mStep[0m  [70/106], [94mLoss[0m : 2.79664
[1mStep[0m  [80/106], [94mLoss[0m : 2.49655
[1mStep[0m  [90/106], [94mLoss[0m : 2.60905
[1mStep[0m  [100/106], [94mLoss[0m : 2.41295

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.412, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.63892
[1mStep[0m  [10/106], [94mLoss[0m : 2.27939
[1mStep[0m  [20/106], [94mLoss[0m : 2.53421
[1mStep[0m  [30/106], [94mLoss[0m : 2.67967
[1mStep[0m  [40/106], [94mLoss[0m : 2.40350
[1mStep[0m  [50/106], [94mLoss[0m : 2.46402
[1mStep[0m  [60/106], [94mLoss[0m : 2.73365
[1mStep[0m  [70/106], [94mLoss[0m : 2.35559
[1mStep[0m  [80/106], [94mLoss[0m : 2.33742
[1mStep[0m  [90/106], [94mLoss[0m : 2.24520
[1mStep[0m  [100/106], [94mLoss[0m : 2.24986

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.413, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50965
[1mStep[0m  [10/106], [94mLoss[0m : 2.31814
[1mStep[0m  [20/106], [94mLoss[0m : 2.67790
[1mStep[0m  [30/106], [94mLoss[0m : 2.71723
[1mStep[0m  [40/106], [94mLoss[0m : 2.66550
[1mStep[0m  [50/106], [94mLoss[0m : 2.50524
[1mStep[0m  [60/106], [94mLoss[0m : 2.42700
[1mStep[0m  [70/106], [94mLoss[0m : 2.76354
[1mStep[0m  [80/106], [94mLoss[0m : 2.12020
[1mStep[0m  [90/106], [94mLoss[0m : 2.44189
[1mStep[0m  [100/106], [94mLoss[0m : 2.36328

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.425, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44688
[1mStep[0m  [10/106], [94mLoss[0m : 2.86990
[1mStep[0m  [20/106], [94mLoss[0m : 2.61468
[1mStep[0m  [30/106], [94mLoss[0m : 2.45570
[1mStep[0m  [40/106], [94mLoss[0m : 2.43125
[1mStep[0m  [50/106], [94mLoss[0m : 2.44034
[1mStep[0m  [60/106], [94mLoss[0m : 2.38442
[1mStep[0m  [70/106], [94mLoss[0m : 2.70745
[1mStep[0m  [80/106], [94mLoss[0m : 2.39451
[1mStep[0m  [90/106], [94mLoss[0m : 2.41120
[1mStep[0m  [100/106], [94mLoss[0m : 2.60312

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.48048
[1mStep[0m  [10/106], [94mLoss[0m : 2.53475
[1mStep[0m  [20/106], [94mLoss[0m : 2.59219
[1mStep[0m  [30/106], [94mLoss[0m : 2.23680
[1mStep[0m  [40/106], [94mLoss[0m : 2.58945
[1mStep[0m  [50/106], [94mLoss[0m : 2.56449
[1mStep[0m  [60/106], [94mLoss[0m : 2.78848
[1mStep[0m  [70/106], [94mLoss[0m : 2.43146
[1mStep[0m  [80/106], [94mLoss[0m : 2.31147
[1mStep[0m  [90/106], [94mLoss[0m : 2.73395
[1mStep[0m  [100/106], [94mLoss[0m : 2.47381

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52668
[1mStep[0m  [10/106], [94mLoss[0m : 2.45468
[1mStep[0m  [20/106], [94mLoss[0m : 2.33060
[1mStep[0m  [30/106], [94mLoss[0m : 2.55394
[1mStep[0m  [40/106], [94mLoss[0m : 2.32589
[1mStep[0m  [50/106], [94mLoss[0m : 2.58035
[1mStep[0m  [60/106], [94mLoss[0m : 2.39081
[1mStep[0m  [70/106], [94mLoss[0m : 2.33547
[1mStep[0m  [80/106], [94mLoss[0m : 2.35945
[1mStep[0m  [90/106], [94mLoss[0m : 2.38982
[1mStep[0m  [100/106], [94mLoss[0m : 2.56862

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.404, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36640
[1mStep[0m  [10/106], [94mLoss[0m : 2.53034
[1mStep[0m  [20/106], [94mLoss[0m : 2.17622
[1mStep[0m  [30/106], [94mLoss[0m : 2.42959
[1mStep[0m  [40/106], [94mLoss[0m : 2.42417
[1mStep[0m  [50/106], [94mLoss[0m : 2.43448
[1mStep[0m  [60/106], [94mLoss[0m : 2.56605
[1mStep[0m  [70/106], [94mLoss[0m : 2.49894
[1mStep[0m  [80/106], [94mLoss[0m : 2.45251
[1mStep[0m  [90/106], [94mLoss[0m : 2.64632
[1mStep[0m  [100/106], [94mLoss[0m : 2.58795

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.402, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.36178
[1mStep[0m  [10/106], [94mLoss[0m : 2.63636
[1mStep[0m  [20/106], [94mLoss[0m : 2.42370
[1mStep[0m  [30/106], [94mLoss[0m : 2.44324
[1mStep[0m  [40/106], [94mLoss[0m : 2.15449
[1mStep[0m  [50/106], [94mLoss[0m : 2.40515
[1mStep[0m  [60/106], [94mLoss[0m : 2.48519
[1mStep[0m  [70/106], [94mLoss[0m : 2.42837
[1mStep[0m  [80/106], [94mLoss[0m : 2.53725
[1mStep[0m  [90/106], [94mLoss[0m : 2.46791
[1mStep[0m  [100/106], [94mLoss[0m : 2.25384

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46180
[1mStep[0m  [10/106], [94mLoss[0m : 2.53485
[1mStep[0m  [20/106], [94mLoss[0m : 2.49981
[1mStep[0m  [30/106], [94mLoss[0m : 2.49845
[1mStep[0m  [40/106], [94mLoss[0m : 2.45414
[1mStep[0m  [50/106], [94mLoss[0m : 2.43262
[1mStep[0m  [60/106], [94mLoss[0m : 2.56332
[1mStep[0m  [70/106], [94mLoss[0m : 2.36338
[1mStep[0m  [80/106], [94mLoss[0m : 2.50644
[1mStep[0m  [90/106], [94mLoss[0m : 2.47095
[1mStep[0m  [100/106], [94mLoss[0m : 2.20940

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.401, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.52234
[1mStep[0m  [10/106], [94mLoss[0m : 2.46844
[1mStep[0m  [20/106], [94mLoss[0m : 2.20709
[1mStep[0m  [30/106], [94mLoss[0m : 2.52912
[1mStep[0m  [40/106], [94mLoss[0m : 2.49416
[1mStep[0m  [50/106], [94mLoss[0m : 2.20837
[1mStep[0m  [60/106], [94mLoss[0m : 2.34808
[1mStep[0m  [70/106], [94mLoss[0m : 2.37357
[1mStep[0m  [80/106], [94mLoss[0m : 2.63284
[1mStep[0m  [90/106], [94mLoss[0m : 2.50689
[1mStep[0m  [100/106], [94mLoss[0m : 2.62400

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.399, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38506
[1mStep[0m  [10/106], [94mLoss[0m : 2.43031
[1mStep[0m  [20/106], [94mLoss[0m : 2.37908
[1mStep[0m  [30/106], [94mLoss[0m : 2.67863
[1mStep[0m  [40/106], [94mLoss[0m : 2.36556
[1mStep[0m  [50/106], [94mLoss[0m : 2.45846
[1mStep[0m  [60/106], [94mLoss[0m : 2.57746
[1mStep[0m  [70/106], [94mLoss[0m : 2.70330
[1mStep[0m  [80/106], [94mLoss[0m : 2.31921
[1mStep[0m  [90/106], [94mLoss[0m : 2.35694
[1mStep[0m  [100/106], [94mLoss[0m : 2.69920

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.404, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67568
[1mStep[0m  [10/106], [94mLoss[0m : 2.39618
[1mStep[0m  [20/106], [94mLoss[0m : 2.71969
[1mStep[0m  [30/106], [94mLoss[0m : 2.65208
[1mStep[0m  [40/106], [94mLoss[0m : 2.62919
[1mStep[0m  [50/106], [94mLoss[0m : 2.61611
[1mStep[0m  [60/106], [94mLoss[0m : 2.30431
[1mStep[0m  [70/106], [94mLoss[0m : 2.35149
[1mStep[0m  [80/106], [94mLoss[0m : 2.69222
[1mStep[0m  [90/106], [94mLoss[0m : 2.49899
[1mStep[0m  [100/106], [94mLoss[0m : 2.33367

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.395, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49124
[1mStep[0m  [10/106], [94mLoss[0m : 2.12921
[1mStep[0m  [20/106], [94mLoss[0m : 2.63552
[1mStep[0m  [30/106], [94mLoss[0m : 2.47933
[1mStep[0m  [40/106], [94mLoss[0m : 2.31878
[1mStep[0m  [50/106], [94mLoss[0m : 2.54548
[1mStep[0m  [60/106], [94mLoss[0m : 2.72768
[1mStep[0m  [70/106], [94mLoss[0m : 2.40577
[1mStep[0m  [80/106], [94mLoss[0m : 2.06490
[1mStep[0m  [90/106], [94mLoss[0m : 2.46677
[1mStep[0m  [100/106], [94mLoss[0m : 2.56449

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.406, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65448
[1mStep[0m  [10/106], [94mLoss[0m : 2.39529
[1mStep[0m  [20/106], [94mLoss[0m : 2.26645
[1mStep[0m  [30/106], [94mLoss[0m : 2.40751
[1mStep[0m  [40/106], [94mLoss[0m : 2.43349
[1mStep[0m  [50/106], [94mLoss[0m : 2.37832
[1mStep[0m  [60/106], [94mLoss[0m : 2.32975
[1mStep[0m  [70/106], [94mLoss[0m : 2.14992
[1mStep[0m  [80/106], [94mLoss[0m : 2.46107
[1mStep[0m  [90/106], [94mLoss[0m : 2.35258
[1mStep[0m  [100/106], [94mLoss[0m : 2.20847

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.405, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.46990
[1mStep[0m  [10/106], [94mLoss[0m : 2.50674
[1mStep[0m  [20/106], [94mLoss[0m : 2.74096
[1mStep[0m  [30/106], [94mLoss[0m : 2.67577
[1mStep[0m  [40/106], [94mLoss[0m : 2.35671
[1mStep[0m  [50/106], [94mLoss[0m : 2.41358
[1mStep[0m  [60/106], [94mLoss[0m : 2.48288
[1mStep[0m  [70/106], [94mLoss[0m : 2.51261
[1mStep[0m  [80/106], [94mLoss[0m : 2.50775
[1mStep[0m  [90/106], [94mLoss[0m : 2.36728
[1mStep[0m  [100/106], [94mLoss[0m : 2.55411

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.397, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.65534
[1mStep[0m  [10/106], [94mLoss[0m : 2.56095
[1mStep[0m  [20/106], [94mLoss[0m : 2.45561
[1mStep[0m  [30/106], [94mLoss[0m : 2.69526
[1mStep[0m  [40/106], [94mLoss[0m : 2.46180
[1mStep[0m  [50/106], [94mLoss[0m : 2.61999
[1mStep[0m  [60/106], [94mLoss[0m : 2.55021
[1mStep[0m  [70/106], [94mLoss[0m : 2.28691
[1mStep[0m  [80/106], [94mLoss[0m : 2.70900
[1mStep[0m  [90/106], [94mLoss[0m : 2.24753
[1mStep[0m  [100/106], [94mLoss[0m : 2.64046

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.402, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.64840
[1mStep[0m  [10/106], [94mLoss[0m : 2.68678
[1mStep[0m  [20/106], [94mLoss[0m : 2.41214
[1mStep[0m  [30/106], [94mLoss[0m : 2.69917
[1mStep[0m  [40/106], [94mLoss[0m : 2.51233
[1mStep[0m  [50/106], [94mLoss[0m : 2.50852
[1mStep[0m  [60/106], [94mLoss[0m : 2.23119
[1mStep[0m  [70/106], [94mLoss[0m : 2.67005
[1mStep[0m  [80/106], [94mLoss[0m : 2.63197
[1mStep[0m  [90/106], [94mLoss[0m : 2.63189
[1mStep[0m  [100/106], [94mLoss[0m : 2.64814

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.401, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24140
[1mStep[0m  [10/106], [94mLoss[0m : 2.56454
[1mStep[0m  [20/106], [94mLoss[0m : 2.48434
[1mStep[0m  [30/106], [94mLoss[0m : 2.42150
[1mStep[0m  [40/106], [94mLoss[0m : 2.31387
[1mStep[0m  [50/106], [94mLoss[0m : 2.58219
[1mStep[0m  [60/106], [94mLoss[0m : 2.40486
[1mStep[0m  [70/106], [94mLoss[0m : 2.58537
[1mStep[0m  [80/106], [94mLoss[0m : 2.06819
[1mStep[0m  [90/106], [94mLoss[0m : 1.98288
[1mStep[0m  [100/106], [94mLoss[0m : 2.14999

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.395, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44033
[1mStep[0m  [10/106], [94mLoss[0m : 2.77606
[1mStep[0m  [20/106], [94mLoss[0m : 2.27144
[1mStep[0m  [30/106], [94mLoss[0m : 2.59298
[1mStep[0m  [40/106], [94mLoss[0m : 2.48459
[1mStep[0m  [50/106], [94mLoss[0m : 2.39008
[1mStep[0m  [60/106], [94mLoss[0m : 2.40907
[1mStep[0m  [70/106], [94mLoss[0m : 2.86448
[1mStep[0m  [80/106], [94mLoss[0m : 2.45319
[1mStep[0m  [90/106], [94mLoss[0m : 2.79412
[1mStep[0m  [100/106], [94mLoss[0m : 2.45565

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43637
[1mStep[0m  [10/106], [94mLoss[0m : 2.25024
[1mStep[0m  [20/106], [94mLoss[0m : 2.23492
[1mStep[0m  [30/106], [94mLoss[0m : 2.56585
[1mStep[0m  [40/106], [94mLoss[0m : 2.39652
[1mStep[0m  [50/106], [94mLoss[0m : 2.40409
[1mStep[0m  [60/106], [94mLoss[0m : 2.48265
[1mStep[0m  [70/106], [94mLoss[0m : 2.36972
[1mStep[0m  [80/106], [94mLoss[0m : 2.51136
[1mStep[0m  [90/106], [94mLoss[0m : 2.33486
[1mStep[0m  [100/106], [94mLoss[0m : 2.35700

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.396, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.393
====================================

Phase 1 - Evaluation MAE:  2.3932194619808556
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.39346
[1mStep[0m  [10/106], [94mLoss[0m : 2.22766
[1mStep[0m  [20/106], [94mLoss[0m : 2.37784
[1mStep[0m  [30/106], [94mLoss[0m : 2.40802
[1mStep[0m  [40/106], [94mLoss[0m : 2.66733
[1mStep[0m  [50/106], [94mLoss[0m : 2.45874
[1mStep[0m  [60/106], [94mLoss[0m : 2.60037
[1mStep[0m  [70/106], [94mLoss[0m : 2.61537
[1mStep[0m  [80/106], [94mLoss[0m : 2.56621
[1mStep[0m  [90/106], [94mLoss[0m : 2.49912
[1mStep[0m  [100/106], [94mLoss[0m : 2.27822

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.395, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.31895
[1mStep[0m  [10/106], [94mLoss[0m : 2.52229
[1mStep[0m  [20/106], [94mLoss[0m : 2.45592
[1mStep[0m  [30/106], [94mLoss[0m : 2.51875
[1mStep[0m  [40/106], [94mLoss[0m : 2.61988
[1mStep[0m  [50/106], [94mLoss[0m : 2.42558
[1mStep[0m  [60/106], [94mLoss[0m : 2.69972
[1mStep[0m  [70/106], [94mLoss[0m : 2.33818
[1mStep[0m  [80/106], [94mLoss[0m : 2.22921
[1mStep[0m  [90/106], [94mLoss[0m : 2.24128
[1mStep[0m  [100/106], [94mLoss[0m : 2.17110

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.40627
[1mStep[0m  [10/106], [94mLoss[0m : 2.16158
[1mStep[0m  [20/106], [94mLoss[0m : 2.29243
[1mStep[0m  [30/106], [94mLoss[0m : 2.36751
[1mStep[0m  [40/106], [94mLoss[0m : 2.46838
[1mStep[0m  [50/106], [94mLoss[0m : 2.32675
[1mStep[0m  [60/106], [94mLoss[0m : 2.50040
[1mStep[0m  [70/106], [94mLoss[0m : 2.51848
[1mStep[0m  [80/106], [94mLoss[0m : 2.30077
[1mStep[0m  [90/106], [94mLoss[0m : 2.29610
[1mStep[0m  [100/106], [94mLoss[0m : 2.43103

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.560, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.38184
[1mStep[0m  [10/106], [94mLoss[0m : 2.57286
[1mStep[0m  [20/106], [94mLoss[0m : 2.57381
[1mStep[0m  [30/106], [94mLoss[0m : 2.32310
[1mStep[0m  [40/106], [94mLoss[0m : 2.39307
[1mStep[0m  [50/106], [94mLoss[0m : 2.18353
[1mStep[0m  [60/106], [94mLoss[0m : 2.46110
[1mStep[0m  [70/106], [94mLoss[0m : 2.34982
[1mStep[0m  [80/106], [94mLoss[0m : 2.13543
[1mStep[0m  [90/106], [94mLoss[0m : 2.31014
[1mStep[0m  [100/106], [94mLoss[0m : 2.47241

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.594, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49847
[1mStep[0m  [10/106], [94mLoss[0m : 2.39265
[1mStep[0m  [20/106], [94mLoss[0m : 2.29500
[1mStep[0m  [30/106], [94mLoss[0m : 2.38844
[1mStep[0m  [40/106], [94mLoss[0m : 2.30309
[1mStep[0m  [50/106], [94mLoss[0m : 2.42799
[1mStep[0m  [60/106], [94mLoss[0m : 2.50390
[1mStep[0m  [70/106], [94mLoss[0m : 2.01107
[1mStep[0m  [80/106], [94mLoss[0m : 2.34440
[1mStep[0m  [90/106], [94mLoss[0m : 2.15282
[1mStep[0m  [100/106], [94mLoss[0m : 2.46918

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.508, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.15979
[1mStep[0m  [10/106], [94mLoss[0m : 2.29895
[1mStep[0m  [20/106], [94mLoss[0m : 2.37449
[1mStep[0m  [30/106], [94mLoss[0m : 2.37101
[1mStep[0m  [40/106], [94mLoss[0m : 2.36231
[1mStep[0m  [50/106], [94mLoss[0m : 2.68072
[1mStep[0m  [60/106], [94mLoss[0m : 2.24557
[1mStep[0m  [70/106], [94mLoss[0m : 2.49086
[1mStep[0m  [80/106], [94mLoss[0m : 2.13974
[1mStep[0m  [90/106], [94mLoss[0m : 2.47299
[1mStep[0m  [100/106], [94mLoss[0m : 2.10802

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.511, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.97846
[1mStep[0m  [10/106], [94mLoss[0m : 2.29718
[1mStep[0m  [20/106], [94mLoss[0m : 2.30868
[1mStep[0m  [30/106], [94mLoss[0m : 2.15681
[1mStep[0m  [40/106], [94mLoss[0m : 2.44559
[1mStep[0m  [50/106], [94mLoss[0m : 2.29858
[1mStep[0m  [60/106], [94mLoss[0m : 2.25928
[1mStep[0m  [70/106], [94mLoss[0m : 2.47077
[1mStep[0m  [80/106], [94mLoss[0m : 2.24791
[1mStep[0m  [90/106], [94mLoss[0m : 2.20423
[1mStep[0m  [100/106], [94mLoss[0m : 2.27998

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.283, [92mTest[0m: 2.488, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44950
[1mStep[0m  [10/106], [94mLoss[0m : 2.18551
[1mStep[0m  [20/106], [94mLoss[0m : 2.40552
[1mStep[0m  [30/106], [94mLoss[0m : 2.22600
[1mStep[0m  [40/106], [94mLoss[0m : 2.31164
[1mStep[0m  [50/106], [94mLoss[0m : 2.06765
[1mStep[0m  [60/106], [94mLoss[0m : 2.38606
[1mStep[0m  [70/106], [94mLoss[0m : 2.02021
[1mStep[0m  [80/106], [94mLoss[0m : 2.17945
[1mStep[0m  [90/106], [94mLoss[0m : 2.28030
[1mStep[0m  [100/106], [94mLoss[0m : 2.10694

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.468, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96018
[1mStep[0m  [10/106], [94mLoss[0m : 2.09369
[1mStep[0m  [20/106], [94mLoss[0m : 2.33777
[1mStep[0m  [30/106], [94mLoss[0m : 2.27608
[1mStep[0m  [40/106], [94mLoss[0m : 2.12750
[1mStep[0m  [50/106], [94mLoss[0m : 2.29470
[1mStep[0m  [60/106], [94mLoss[0m : 2.30386
[1mStep[0m  [70/106], [94mLoss[0m : 1.91261
[1mStep[0m  [80/106], [94mLoss[0m : 2.01911
[1mStep[0m  [90/106], [94mLoss[0m : 1.98741
[1mStep[0m  [100/106], [94mLoss[0m : 2.03881

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.206, [92mTest[0m: 2.437, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.97783
[1mStep[0m  [10/106], [94mLoss[0m : 2.20918
[1mStep[0m  [20/106], [94mLoss[0m : 2.33295
[1mStep[0m  [30/106], [94mLoss[0m : 1.93817
[1mStep[0m  [40/106], [94mLoss[0m : 2.22503
[1mStep[0m  [50/106], [94mLoss[0m : 2.20270
[1mStep[0m  [60/106], [94mLoss[0m : 2.12933
[1mStep[0m  [70/106], [94mLoss[0m : 1.94808
[1mStep[0m  [80/106], [94mLoss[0m : 1.93559
[1mStep[0m  [90/106], [94mLoss[0m : 2.15591
[1mStep[0m  [100/106], [94mLoss[0m : 2.08315

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.408, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92934
[1mStep[0m  [10/106], [94mLoss[0m : 2.29371
[1mStep[0m  [20/106], [94mLoss[0m : 1.98032
[1mStep[0m  [30/106], [94mLoss[0m : 2.13837
[1mStep[0m  [40/106], [94mLoss[0m : 1.94226
[1mStep[0m  [50/106], [94mLoss[0m : 1.92025
[1mStep[0m  [60/106], [94mLoss[0m : 1.90872
[1mStep[0m  [70/106], [94mLoss[0m : 2.05919
[1mStep[0m  [80/106], [94mLoss[0m : 2.18433
[1mStep[0m  [90/106], [94mLoss[0m : 2.23771
[1mStep[0m  [100/106], [94mLoss[0m : 2.13579

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.133, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96533
[1mStep[0m  [10/106], [94mLoss[0m : 2.08114
[1mStep[0m  [20/106], [94mLoss[0m : 2.27408
[1mStep[0m  [30/106], [94mLoss[0m : 2.21440
[1mStep[0m  [40/106], [94mLoss[0m : 2.19782
[1mStep[0m  [50/106], [94mLoss[0m : 1.81601
[1mStep[0m  [60/106], [94mLoss[0m : 2.10532
[1mStep[0m  [70/106], [94mLoss[0m : 2.02334
[1mStep[0m  [80/106], [94mLoss[0m : 2.01076
[1mStep[0m  [90/106], [94mLoss[0m : 1.99446
[1mStep[0m  [100/106], [94mLoss[0m : 2.17041

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.444, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.92288
[1mStep[0m  [10/106], [94mLoss[0m : 2.31675
[1mStep[0m  [20/106], [94mLoss[0m : 1.83949
[1mStep[0m  [30/106], [94mLoss[0m : 1.76268
[1mStep[0m  [40/106], [94mLoss[0m : 2.07019
[1mStep[0m  [50/106], [94mLoss[0m : 1.85905
[1mStep[0m  [60/106], [94mLoss[0m : 1.85659
[1mStep[0m  [70/106], [94mLoss[0m : 2.04155
[1mStep[0m  [80/106], [94mLoss[0m : 2.15568
[1mStep[0m  [90/106], [94mLoss[0m : 2.22039
[1mStep[0m  [100/106], [94mLoss[0m : 2.05010

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.056, [92mTest[0m: 2.453, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.12062
[1mStep[0m  [10/106], [94mLoss[0m : 2.15907
[1mStep[0m  [20/106], [94mLoss[0m : 2.02496
[1mStep[0m  [30/106], [94mLoss[0m : 1.88638
[1mStep[0m  [40/106], [94mLoss[0m : 2.19167
[1mStep[0m  [50/106], [94mLoss[0m : 1.94747
[1mStep[0m  [60/106], [94mLoss[0m : 1.92023
[1mStep[0m  [70/106], [94mLoss[0m : 2.23246
[1mStep[0m  [80/106], [94mLoss[0m : 1.78025
[1mStep[0m  [90/106], [94mLoss[0m : 1.91313
[1mStep[0m  [100/106], [94mLoss[0m : 2.25949

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.016, [92mTest[0m: 2.414, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.91042
[1mStep[0m  [10/106], [94mLoss[0m : 1.97762
[1mStep[0m  [20/106], [94mLoss[0m : 1.88563
[1mStep[0m  [30/106], [94mLoss[0m : 2.11113
[1mStep[0m  [40/106], [94mLoss[0m : 2.02831
[1mStep[0m  [50/106], [94mLoss[0m : 1.96900
[1mStep[0m  [60/106], [94mLoss[0m : 2.04984
[1mStep[0m  [70/106], [94mLoss[0m : 2.10849
[1mStep[0m  [80/106], [94mLoss[0m : 1.91726
[1mStep[0m  [90/106], [94mLoss[0m : 1.86356
[1mStep[0m  [100/106], [94mLoss[0m : 1.88115

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.986, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83836
[1mStep[0m  [10/106], [94mLoss[0m : 2.13940
[1mStep[0m  [20/106], [94mLoss[0m : 1.90698
[1mStep[0m  [30/106], [94mLoss[0m : 1.90206
[1mStep[0m  [40/106], [94mLoss[0m : 2.08482
[1mStep[0m  [50/106], [94mLoss[0m : 1.83205
[1mStep[0m  [60/106], [94mLoss[0m : 2.11164
[1mStep[0m  [70/106], [94mLoss[0m : 1.85914
[1mStep[0m  [80/106], [94mLoss[0m : 1.68341
[1mStep[0m  [90/106], [94mLoss[0m : 2.28796
[1mStep[0m  [100/106], [94mLoss[0m : 1.96520

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.478, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.82376
[1mStep[0m  [10/106], [94mLoss[0m : 1.75120
[1mStep[0m  [20/106], [94mLoss[0m : 2.14648
[1mStep[0m  [30/106], [94mLoss[0m : 1.64041
[1mStep[0m  [40/106], [94mLoss[0m : 1.75805
[1mStep[0m  [50/106], [94mLoss[0m : 1.85188
[1mStep[0m  [60/106], [94mLoss[0m : 1.95869
[1mStep[0m  [70/106], [94mLoss[0m : 2.04488
[1mStep[0m  [80/106], [94mLoss[0m : 1.90237
[1mStep[0m  [90/106], [94mLoss[0m : 1.99535
[1mStep[0m  [100/106], [94mLoss[0m : 2.06759

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.403, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.83462
[1mStep[0m  [10/106], [94mLoss[0m : 1.85689
[1mStep[0m  [20/106], [94mLoss[0m : 1.90853
[1mStep[0m  [30/106], [94mLoss[0m : 1.73826
[1mStep[0m  [40/106], [94mLoss[0m : 1.78570
[1mStep[0m  [50/106], [94mLoss[0m : 1.91322
[1mStep[0m  [60/106], [94mLoss[0m : 1.87011
[1mStep[0m  [70/106], [94mLoss[0m : 2.15419
[1mStep[0m  [80/106], [94mLoss[0m : 1.82405
[1mStep[0m  [90/106], [94mLoss[0m : 1.64892
[1mStep[0m  [100/106], [94mLoss[0m : 1.79208

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.885, [92mTest[0m: 2.436, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.78548
[1mStep[0m  [10/106], [94mLoss[0m : 1.83752
[1mStep[0m  [20/106], [94mLoss[0m : 1.83916
[1mStep[0m  [30/106], [94mLoss[0m : 1.72444
[1mStep[0m  [40/106], [94mLoss[0m : 1.90187
[1mStep[0m  [50/106], [94mLoss[0m : 1.73850
[1mStep[0m  [60/106], [94mLoss[0m : 1.84474
[1mStep[0m  [70/106], [94mLoss[0m : 1.74609
[1mStep[0m  [80/106], [94mLoss[0m : 2.33995
[1mStep[0m  [90/106], [94mLoss[0m : 2.06349
[1mStep[0m  [100/106], [94mLoss[0m : 1.85126

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.523, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.79680
[1mStep[0m  [10/106], [94mLoss[0m : 1.46777
[1mStep[0m  [20/106], [94mLoss[0m : 1.60757
[1mStep[0m  [30/106], [94mLoss[0m : 2.20425
[1mStep[0m  [40/106], [94mLoss[0m : 1.93792
[1mStep[0m  [50/106], [94mLoss[0m : 1.74348
[1mStep[0m  [60/106], [94mLoss[0m : 1.75423
[1mStep[0m  [70/106], [94mLoss[0m : 1.62727
[1mStep[0m  [80/106], [94mLoss[0m : 2.04791
[1mStep[0m  [90/106], [94mLoss[0m : 1.66396
[1mStep[0m  [100/106], [94mLoss[0m : 1.86673

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.824, [92mTest[0m: 2.463, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.71913
[1mStep[0m  [10/106], [94mLoss[0m : 1.68571
[1mStep[0m  [20/106], [94mLoss[0m : 1.86798
[1mStep[0m  [30/106], [94mLoss[0m : 1.84777
[1mStep[0m  [40/106], [94mLoss[0m : 1.63865
[1mStep[0m  [50/106], [94mLoss[0m : 1.84760
[1mStep[0m  [60/106], [94mLoss[0m : 1.64862
[1mStep[0m  [70/106], [94mLoss[0m : 1.79945
[1mStep[0m  [80/106], [94mLoss[0m : 1.72815
[1mStep[0m  [90/106], [94mLoss[0m : 1.65952
[1mStep[0m  [100/106], [94mLoss[0m : 2.06257

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.791, [92mTest[0m: 2.567, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.77654
[1mStep[0m  [10/106], [94mLoss[0m : 1.72817
[1mStep[0m  [20/106], [94mLoss[0m : 1.51462
[1mStep[0m  [30/106], [94mLoss[0m : 1.93514
[1mStep[0m  [40/106], [94mLoss[0m : 1.84500
[1mStep[0m  [50/106], [94mLoss[0m : 1.79091
[1mStep[0m  [60/106], [94mLoss[0m : 1.60662
[1mStep[0m  [70/106], [94mLoss[0m : 1.79859
[1mStep[0m  [80/106], [94mLoss[0m : 1.62310
[1mStep[0m  [90/106], [94mLoss[0m : 1.98008
[1mStep[0m  [100/106], [94mLoss[0m : 1.60602

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.440, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.74581
[1mStep[0m  [10/106], [94mLoss[0m : 1.50397
[1mStep[0m  [20/106], [94mLoss[0m : 1.68714
[1mStep[0m  [30/106], [94mLoss[0m : 1.92831
[1mStep[0m  [40/106], [94mLoss[0m : 1.49809
[1mStep[0m  [50/106], [94mLoss[0m : 1.72327
[1mStep[0m  [60/106], [94mLoss[0m : 1.71116
[1mStep[0m  [70/106], [94mLoss[0m : 1.61163
[1mStep[0m  [80/106], [94mLoss[0m : 1.84086
[1mStep[0m  [90/106], [94mLoss[0m : 1.83604
[1mStep[0m  [100/106], [94mLoss[0m : 1.74694

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.737, [92mTest[0m: 2.420, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.44308
[1mStep[0m  [10/106], [94mLoss[0m : 1.64288
[1mStep[0m  [20/106], [94mLoss[0m : 1.87653
[1mStep[0m  [30/106], [94mLoss[0m : 1.95332
[1mStep[0m  [40/106], [94mLoss[0m : 1.66565
[1mStep[0m  [50/106], [94mLoss[0m : 1.58101
[1mStep[0m  [60/106], [94mLoss[0m : 1.58360
[1mStep[0m  [70/106], [94mLoss[0m : 1.80096
[1mStep[0m  [80/106], [94mLoss[0m : 1.66303
[1mStep[0m  [90/106], [94mLoss[0m : 1.77545
[1mStep[0m  [100/106], [94mLoss[0m : 1.64654

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.699, [92mTest[0m: 2.458, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.54346
[1mStep[0m  [10/106], [94mLoss[0m : 1.43706
[1mStep[0m  [20/106], [94mLoss[0m : 1.51729
[1mStep[0m  [30/106], [94mLoss[0m : 1.70297
[1mStep[0m  [40/106], [94mLoss[0m : 1.68675
[1mStep[0m  [50/106], [94mLoss[0m : 1.73252
[1mStep[0m  [60/106], [94mLoss[0m : 1.59382
[1mStep[0m  [70/106], [94mLoss[0m : 1.57220
[1mStep[0m  [80/106], [94mLoss[0m : 1.43823
[1mStep[0m  [90/106], [94mLoss[0m : 1.59914
[1mStep[0m  [100/106], [94mLoss[0m : 1.50541

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.682, [92mTest[0m: 2.474, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.86363
[1mStep[0m  [10/106], [94mLoss[0m : 1.64425
[1mStep[0m  [20/106], [94mLoss[0m : 1.66175
[1mStep[0m  [30/106], [94mLoss[0m : 1.48791
[1mStep[0m  [40/106], [94mLoss[0m : 1.65065
[1mStep[0m  [50/106], [94mLoss[0m : 1.72499
[1mStep[0m  [60/106], [94mLoss[0m : 1.78613
[1mStep[0m  [70/106], [94mLoss[0m : 1.50494
[1mStep[0m  [80/106], [94mLoss[0m : 1.85166
[1mStep[0m  [90/106], [94mLoss[0m : 1.70359
[1mStep[0m  [100/106], [94mLoss[0m : 1.50242

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.671, [92mTest[0m: 2.423, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.50690
[1mStep[0m  [10/106], [94mLoss[0m : 1.47294
[1mStep[0m  [20/106], [94mLoss[0m : 1.55440
[1mStep[0m  [30/106], [94mLoss[0m : 1.85314
[1mStep[0m  [40/106], [94mLoss[0m : 1.70803
[1mStep[0m  [50/106], [94mLoss[0m : 1.61266
[1mStep[0m  [60/106], [94mLoss[0m : 1.52559
[1mStep[0m  [70/106], [94mLoss[0m : 1.63310
[1mStep[0m  [80/106], [94mLoss[0m : 1.41175
[1mStep[0m  [90/106], [94mLoss[0m : 1.85962
[1mStep[0m  [100/106], [94mLoss[0m : 1.60674

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.645, [92mTest[0m: 2.448, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.60789
[1mStep[0m  [10/106], [94mLoss[0m : 1.55871
[1mStep[0m  [20/106], [94mLoss[0m : 1.56449
[1mStep[0m  [30/106], [94mLoss[0m : 1.60155
[1mStep[0m  [40/106], [94mLoss[0m : 1.56324
[1mStep[0m  [50/106], [94mLoss[0m : 1.72654
[1mStep[0m  [60/106], [94mLoss[0m : 1.51595
[1mStep[0m  [70/106], [94mLoss[0m : 1.82529
[1mStep[0m  [80/106], [94mLoss[0m : 1.44844
[1mStep[0m  [90/106], [94mLoss[0m : 1.64457
[1mStep[0m  [100/106], [94mLoss[0m : 1.67176

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.467, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.50872
[1mStep[0m  [10/106], [94mLoss[0m : 1.31037
[1mStep[0m  [20/106], [94mLoss[0m : 1.83158
[1mStep[0m  [30/106], [94mLoss[0m : 1.78277
[1mStep[0m  [40/106], [94mLoss[0m : 1.59637
[1mStep[0m  [50/106], [94mLoss[0m : 1.63992
[1mStep[0m  [60/106], [94mLoss[0m : 1.68611
[1mStep[0m  [70/106], [94mLoss[0m : 1.39769
[1mStep[0m  [80/106], [94mLoss[0m : 1.77506
[1mStep[0m  [90/106], [94mLoss[0m : 1.46255
[1mStep[0m  [100/106], [94mLoss[0m : 1.45097

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.442, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.48289
[1mStep[0m  [10/106], [94mLoss[0m : 1.35644
[1mStep[0m  [20/106], [94mLoss[0m : 1.37624
[1mStep[0m  [30/106], [94mLoss[0m : 1.74024
[1mStep[0m  [40/106], [94mLoss[0m : 1.54967
[1mStep[0m  [50/106], [94mLoss[0m : 1.56701
[1mStep[0m  [60/106], [94mLoss[0m : 1.54360
[1mStep[0m  [70/106], [94mLoss[0m : 1.71914
[1mStep[0m  [80/106], [94mLoss[0m : 1.67836
[1mStep[0m  [90/106], [94mLoss[0m : 1.64923
[1mStep[0m  [100/106], [94mLoss[0m : 1.67409

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.492, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.440
====================================

Phase 2 - Evaluation MAE:  2.4397867220752643
MAE score P1      2.393219
MAE score P2      2.439787
loss              1.596468
learning_rate     0.002575
batch_size             128
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.3
momentum               0.5
weight_decay         0.001
Name: 14, dtype: object
Already used Hyperparameters:  {'hidden_sizes': [300, 100], 'learning_rate': 0.002575, 'batch_size': 128, 'epochs': 30, 'activation': 'sigmoid', 'optimizer': 'sgd', 'output_size': 1, 'dropout_p': 0.4, 'weight_decay': 0.0001, 'momentum': 0.1, 'sched_ss': 20, 'sched_g': 0.9}
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 11.43772
[1mStep[0m  [10/106], [94mLoss[0m : 5.95069
[1mStep[0m  [20/106], [94mLoss[0m : 3.14552
[1mStep[0m  [30/106], [94mLoss[0m : 2.91560
[1mStep[0m  [40/106], [94mLoss[0m : 2.78274
[1mStep[0m  [50/106], [94mLoss[0m : 2.89211
[1mStep[0m  [60/106], [94mLoss[0m : 2.41742
[1mStep[0m  [70/106], [94mLoss[0m : 2.75668
[1mStep[0m  [80/106], [94mLoss[0m : 2.59655
[1mStep[0m  [90/106], [94mLoss[0m : 2.42501
[1mStep[0m  [100/106], [94mLoss[0m : 2.63277

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.499, [92mTest[0m: 10.911, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39449
[1mStep[0m  [10/106], [94mLoss[0m : 2.56345
[1mStep[0m  [20/106], [94mLoss[0m : 2.70544
[1mStep[0m  [30/106], [94mLoss[0m : 2.65342
[1mStep[0m  [40/106], [94mLoss[0m : 2.50946
[1mStep[0m  [50/106], [94mLoss[0m : 2.74318
[1mStep[0m  [60/106], [94mLoss[0m : 2.67398
[1mStep[0m  [70/106], [94mLoss[0m : 2.44033
[1mStep[0m  [80/106], [94mLoss[0m : 2.54289
[1mStep[0m  [90/106], [94mLoss[0m : 2.45434
[1mStep[0m  [100/106], [94mLoss[0m : 2.71805

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.559, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.25307
[1mStep[0m  [10/106], [94mLoss[0m : 2.64927
[1mStep[0m  [20/106], [94mLoss[0m : 2.46117
[1mStep[0m  [30/106], [94mLoss[0m : 2.27616
[1mStep[0m  [40/106], [94mLoss[0m : 2.71885
[1mStep[0m  [50/106], [94mLoss[0m : 2.60997
[1mStep[0m  [60/106], [94mLoss[0m : 2.41761
[1mStep[0m  [70/106], [94mLoss[0m : 2.17508
[1mStep[0m  [80/106], [94mLoss[0m : 2.53127
[1mStep[0m  [90/106], [94mLoss[0m : 2.54112
[1mStep[0m  [100/106], [94mLoss[0m : 2.81572

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.553, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.50631
[1mStep[0m  [10/106], [94mLoss[0m : 2.60817
[1mStep[0m  [20/106], [94mLoss[0m : 2.56435
[1mStep[0m  [30/106], [94mLoss[0m : 2.37047
[1mStep[0m  [40/106], [94mLoss[0m : 2.71550
[1mStep[0m  [50/106], [94mLoss[0m : 2.40171
[1mStep[0m  [60/106], [94mLoss[0m : 2.61369
[1mStep[0m  [70/106], [94mLoss[0m : 2.46158
[1mStep[0m  [80/106], [94mLoss[0m : 2.41561
[1mStep[0m  [90/106], [94mLoss[0m : 2.45312
[1mStep[0m  [100/106], [94mLoss[0m : 2.36208

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.603, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.62334
[1mStep[0m  [10/106], [94mLoss[0m : 2.69463
[1mStep[0m  [20/106], [94mLoss[0m : 2.71818
[1mStep[0m  [30/106], [94mLoss[0m : 2.56276
[1mStep[0m  [40/106], [94mLoss[0m : 2.37036
[1mStep[0m  [50/106], [94mLoss[0m : 2.34391
[1mStep[0m  [60/106], [94mLoss[0m : 2.53481
[1mStep[0m  [70/106], [94mLoss[0m : 2.59682
[1mStep[0m  [80/106], [94mLoss[0m : 2.70661
[1mStep[0m  [90/106], [94mLoss[0m : 2.58773
[1mStep[0m  [100/106], [94mLoss[0m : 2.30953

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.542, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 1.96619
[1mStep[0m  [10/106], [94mLoss[0m : 2.11581
[1mStep[0m  [20/106], [94mLoss[0m : 2.22414
[1mStep[0m  [30/106], [94mLoss[0m : 2.58301
[1mStep[0m  [40/106], [94mLoss[0m : 2.21957
[1mStep[0m  [50/106], [94mLoss[0m : 2.37052
[1mStep[0m  [60/106], [94mLoss[0m : 2.82926
[1mStep[0m  [70/106], [94mLoss[0m : 2.31941
[1mStep[0m  [80/106], [94mLoss[0m : 2.29341
[1mStep[0m  [90/106], [94mLoss[0m : 2.29800
[1mStep[0m  [100/106], [94mLoss[0m : 2.33976

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.474, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.33895
[1mStep[0m  [10/106], [94mLoss[0m : 2.58321
[1mStep[0m  [20/106], [94mLoss[0m : 2.45596
[1mStep[0m  [30/106], [94mLoss[0m : 2.61309
[1mStep[0m  [40/106], [94mLoss[0m : 2.41854
[1mStep[0m  [50/106], [94mLoss[0m : 2.29128
[1mStep[0m  [60/106], [94mLoss[0m : 2.47793
[1mStep[0m  [70/106], [94mLoss[0m : 2.56759
[1mStep[0m  [80/106], [94mLoss[0m : 2.33308
[1mStep[0m  [90/106], [94mLoss[0m : 1.98383
[1mStep[0m  [100/106], [94mLoss[0m : 2.34425

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.465, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.51681
[1mStep[0m  [10/106], [94mLoss[0m : 2.33128
[1mStep[0m  [20/106], [94mLoss[0m : 2.69530
[1mStep[0m  [30/106], [94mLoss[0m : 2.76163
[1mStep[0m  [40/106], [94mLoss[0m : 2.54199
[1mStep[0m  [50/106], [94mLoss[0m : 2.43120
[1mStep[0m  [60/106], [94mLoss[0m : 2.09427
[1mStep[0m  [70/106], [94mLoss[0m : 2.32553
[1mStep[0m  [80/106], [94mLoss[0m : 2.27715
[1mStep[0m  [90/106], [94mLoss[0m : 2.15822
[1mStep[0m  [100/106], [94mLoss[0m : 2.49100

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.410, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16151
[1mStep[0m  [10/106], [94mLoss[0m : 2.20620
[1mStep[0m  [20/106], [94mLoss[0m : 2.54654
[1mStep[0m  [30/106], [94mLoss[0m : 2.66699
[1mStep[0m  [40/106], [94mLoss[0m : 2.19127
[1mStep[0m  [50/106], [94mLoss[0m : 2.30937
[1mStep[0m  [60/106], [94mLoss[0m : 2.67219
[1mStep[0m  [70/106], [94mLoss[0m : 2.84038
[1mStep[0m  [80/106], [94mLoss[0m : 2.26180
[1mStep[0m  [90/106], [94mLoss[0m : 2.41260
[1mStep[0m  [100/106], [94mLoss[0m : 2.35617

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.420, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39044
[1mStep[0m  [10/106], [94mLoss[0m : 2.76413
[1mStep[0m  [20/106], [94mLoss[0m : 2.15571
[1mStep[0m  [30/106], [94mLoss[0m : 2.37424
[1mStep[0m  [40/106], [94mLoss[0m : 2.47668
[1mStep[0m  [50/106], [94mLoss[0m : 2.37265
[1mStep[0m  [60/106], [94mLoss[0m : 2.13848
[1mStep[0m  [70/106], [94mLoss[0m : 2.51030
[1mStep[0m  [80/106], [94mLoss[0m : 2.61246
[1mStep[0m  [90/106], [94mLoss[0m : 2.19595
[1mStep[0m  [100/106], [94mLoss[0m : 2.50279

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.428, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.43130
[1mStep[0m  [10/106], [94mLoss[0m : 2.58741
[1mStep[0m  [20/106], [94mLoss[0m : 2.31082
[1mStep[0m  [30/106], [94mLoss[0m : 2.21109
[1mStep[0m  [40/106], [94mLoss[0m : 2.67238
[1mStep[0m  [50/106], [94mLoss[0m : 2.63368
[1mStep[0m  [60/106], [94mLoss[0m : 2.64077
[1mStep[0m  [70/106], [94mLoss[0m : 2.28896
[1mStep[0m  [80/106], [94mLoss[0m : 2.51693
[1mStep[0m  [90/106], [94mLoss[0m : 2.59659
[1mStep[0m  [100/106], [94mLoss[0m : 2.12903

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.416, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.27735
[1mStep[0m  [10/106], [94mLoss[0m : 2.33421
[1mStep[0m  [20/106], [94mLoss[0m : 2.55039
[1mStep[0m  [30/106], [94mLoss[0m : 2.41499
[1mStep[0m  [40/106], [94mLoss[0m : 2.94930
[1mStep[0m  [50/106], [94mLoss[0m : 2.60150
[1mStep[0m  [60/106], [94mLoss[0m : 2.54698
[1mStep[0m  [70/106], [94mLoss[0m : 2.22056
[1mStep[0m  [80/106], [94mLoss[0m : 2.32077
[1mStep[0m  [90/106], [94mLoss[0m : 2.57620
[1mStep[0m  [100/106], [94mLoss[0m : 2.56704

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.460, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.00606
[1mStep[0m  [10/106], [94mLoss[0m : 2.26095
[1mStep[0m  [20/106], [94mLoss[0m : 2.11871
[1mStep[0m  [30/106], [94mLoss[0m : 2.35205
[1mStep[0m  [40/106], [94mLoss[0m : 2.75493
[1mStep[0m  [50/106], [94mLoss[0m : 2.27583
[1mStep[0m  [60/106], [94mLoss[0m : 2.53206
[1mStep[0m  [70/106], [94mLoss[0m : 2.62576
[1mStep[0m  [80/106], [94mLoss[0m : 2.49730
[1mStep[0m  [90/106], [94mLoss[0m : 2.20400
[1mStep[0m  [100/106], [94mLoss[0m : 2.50671

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.388, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.39271
[1mStep[0m  [10/106], [94mLoss[0m : 2.51636
[1mStep[0m  [20/106], [94mLoss[0m : 2.32427
[1mStep[0m  [30/106], [94mLoss[0m : 2.26586
[1mStep[0m  [40/106], [94mLoss[0m : 2.20088
[1mStep[0m  [50/106], [94mLoss[0m : 2.19811
[1mStep[0m  [60/106], [94mLoss[0m : 2.66477
[1mStep[0m  [70/106], [94mLoss[0m : 2.65627
[1mStep[0m  [80/106], [94mLoss[0m : 2.10493
[1mStep[0m  [90/106], [94mLoss[0m : 2.30598
[1mStep[0m  [100/106], [94mLoss[0m : 2.66217

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.366, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.42140
[1mStep[0m  [10/106], [94mLoss[0m : 2.46204
[1mStep[0m  [20/106], [94mLoss[0m : 2.38262
[1mStep[0m  [30/106], [94mLoss[0m : 2.53702
[1mStep[0m  [40/106], [94mLoss[0m : 2.09849
[1mStep[0m  [50/106], [94mLoss[0m : 2.20636
[1mStep[0m  [60/106], [94mLoss[0m : 2.23517
[1mStep[0m  [70/106], [94mLoss[0m : 2.45758
[1mStep[0m  [80/106], [94mLoss[0m : 2.38000
[1mStep[0m  [90/106], [94mLoss[0m : 2.28137
[1mStep[0m  [100/106], [94mLoss[0m : 2.39136

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.21034
[1mStep[0m  [10/106], [94mLoss[0m : 2.42239
[1mStep[0m  [20/106], [94mLoss[0m : 2.50362
[1mStep[0m  [30/106], [94mLoss[0m : 2.20815
[1mStep[0m  [40/106], [94mLoss[0m : 2.50477
[1mStep[0m  [50/106], [94mLoss[0m : 2.25900
[1mStep[0m  [60/106], [94mLoss[0m : 2.64995
[1mStep[0m  [70/106], [94mLoss[0m : 2.25098
[1mStep[0m  [80/106], [94mLoss[0m : 2.28104
[1mStep[0m  [90/106], [94mLoss[0m : 2.45902
[1mStep[0m  [100/106], [94mLoss[0m : 2.44791

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.392, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.53127
[1mStep[0m  [10/106], [94mLoss[0m : 2.16899
[1mStep[0m  [20/106], [94mLoss[0m : 2.39327
[1mStep[0m  [30/106], [94mLoss[0m : 2.47658
[1mStep[0m  [40/106], [94mLoss[0m : 2.57671
[1mStep[0m  [50/106], [94mLoss[0m : 2.71207
[1mStep[0m  [60/106], [94mLoss[0m : 2.40927
[1mStep[0m  [70/106], [94mLoss[0m : 2.18948
[1mStep[0m  [80/106], [94mLoss[0m : 2.26133
[1mStep[0m  [90/106], [94mLoss[0m : 2.32883
[1mStep[0m  [100/106], [94mLoss[0m : 2.26024

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.362, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.49577
[1mStep[0m  [10/106], [94mLoss[0m : 2.12197
[1mStep[0m  [20/106], [94mLoss[0m : 2.19113
[1mStep[0m  [30/106], [94mLoss[0m : 2.50053
[1mStep[0m  [40/106], [94mLoss[0m : 2.27772
[1mStep[0m  [50/106], [94mLoss[0m : 2.30352
[1mStep[0m  [60/106], [94mLoss[0m : 2.78071
[1mStep[0m  [70/106], [94mLoss[0m : 2.35480
[1mStep[0m  [80/106], [94mLoss[0m : 2.32863
[1mStep[0m  [90/106], [94mLoss[0m : 2.10221
[1mStep[0m  [100/106], [94mLoss[0m : 2.34036

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.368, [92mTest[0m: 2.380, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.67041
[1mStep[0m  [10/106], [94mLoss[0m : 2.24091
[1mStep[0m  [20/106], [94mLoss[0m : 2.38993
[1mStep[0m  [30/106], [94mLoss[0m : 2.35148
[1mStep[0m  [40/106], [94mLoss[0m : 2.58245
[1mStep[0m  [50/106], [94mLoss[0m : 2.29241
[1mStep[0m  [60/106], [94mLoss[0m : 2.52289
[1mStep[0m  [70/106], [94mLoss[0m : 2.41432
[1mStep[0m  [80/106], [94mLoss[0m : 2.36113
[1mStep[0m  [90/106], [94mLoss[0m : 2.34534
[1mStep[0m  [100/106], [94mLoss[0m : 2.37634

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.383, [92mTest[0m: 2.382, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.74281
[1mStep[0m  [10/106], [94mLoss[0m : 2.40612
[1mStep[0m  [20/106], [94mLoss[0m : 2.43830
[1mStep[0m  [30/106], [94mLoss[0m : 2.30104
[1mStep[0m  [40/106], [94mLoss[0m : 2.14060
[1mStep[0m  [50/106], [94mLoss[0m : 2.57746
[1mStep[0m  [60/106], [94mLoss[0m : 2.24710
[1mStep[0m  [70/106], [94mLoss[0m : 2.02226
[1mStep[0m  [80/106], [94mLoss[0m : 2.57903
[1mStep[0m  [90/106], [94mLoss[0m : 2.15486
[1mStep[0m  [100/106], [94mLoss[0m : 2.40069

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.360, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.17120
[1mStep[0m  [10/106], [94mLoss[0m : 2.27769
[1mStep[0m  [20/106], [94mLoss[0m : 2.53946
[1mStep[0m  [30/106], [94mLoss[0m : 2.12967
[1mStep[0m  [40/106], [94mLoss[0m : 2.25365
[1mStep[0m  [50/106], [94mLoss[0m : 2.37551
[1mStep[0m  [60/106], [94mLoss[0m : 2.54649
[1mStep[0m  [70/106], [94mLoss[0m : 2.35539
[1mStep[0m  [80/106], [94mLoss[0m : 2.24224
[1mStep[0m  [90/106], [94mLoss[0m : 2.49785
[1mStep[0m  [100/106], [94mLoss[0m : 2.66451

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.354, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.29637
[1mStep[0m  [10/106], [94mLoss[0m : 2.26356
[1mStep[0m  [20/106], [94mLoss[0m : 2.25985
[1mStep[0m  [30/106], [94mLoss[0m : 2.59928
[1mStep[0m  [40/106], [94mLoss[0m : 2.26646
[1mStep[0m  [50/106], [94mLoss[0m : 2.01658
[1mStep[0m  [60/106], [94mLoss[0m : 2.58812
[1mStep[0m  [70/106], [94mLoss[0m : 2.51898
[1mStep[0m  [80/106], [94mLoss[0m : 2.26750
[1mStep[0m  [90/106], [94mLoss[0m : 2.45595
[1mStep[0m  [100/106], [94mLoss[0m : 2.27412

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.360, [92mTest[0m: 2.373, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.16404
[1mStep[0m  [10/106], [94mLoss[0m : 2.65765
[1mStep[0m  [20/106], [94mLoss[0m : 2.46950
[1mStep[0m  [30/106], [94mLoss[0m : 2.18944
[1mStep[0m  [40/106], [94mLoss[0m : 2.43878
[1mStep[0m  [50/106], [94mLoss[0m : 2.44132
[1mStep[0m  [60/106], [94mLoss[0m : 2.36247
[1mStep[0m  [70/106], [94mLoss[0m : 2.18086
[1mStep[0m  [80/106], [94mLoss[0m : 2.40142
[1mStep[0m  [90/106], [94mLoss[0m : 2.21982
[1mStep[0m  [100/106], [94mLoss[0m : 2.16854

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.360, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.54597
[1mStep[0m  [10/106], [94mLoss[0m : 2.41767
[1mStep[0m  [20/106], [94mLoss[0m : 2.04870
[1mStep[0m  [30/106], [94mLoss[0m : 2.27684
[1mStep[0m  [40/106], [94mLoss[0m : 2.29798
[1mStep[0m  [50/106], [94mLoss[0m : 2.39968
[1mStep[0m  [60/106], [94mLoss[0m : 2.48302
[1mStep[0m  [70/106], [94mLoss[0m : 2.29760
[1mStep[0m  [80/106], [94mLoss[0m : 2.69247
[1mStep[0m  [90/106], [94mLoss[0m : 2.53063
[1mStep[0m  [100/106], [94mLoss[0m : 2.38203

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.350, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.69708
[1mStep[0m  [10/106], [94mLoss[0m : 2.22918
[1mStep[0m  [20/106], [94mLoss[0m : 2.34526
[1mStep[0m  [30/106], [94mLoss[0m : 2.45578
[1mStep[0m  [40/106], [94mLoss[0m : 2.36436
[1mStep[0m  [50/106], [94mLoss[0m : 2.06700
[1mStep[0m  [60/106], [94mLoss[0m : 2.23834
[1mStep[0m  [70/106], [94mLoss[0m : 2.31066
[1mStep[0m  [80/106], [94mLoss[0m : 2.26426
[1mStep[0m  [90/106], [94mLoss[0m : 2.37462
[1mStep[0m  [100/106], [94mLoss[0m : 2.31746

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.19669
[1mStep[0m  [10/106], [94mLoss[0m : 2.52912
[1mStep[0m  [20/106], [94mLoss[0m : 2.19166
[1mStep[0m  [30/106], [94mLoss[0m : 2.05688
[1mStep[0m  [40/106], [94mLoss[0m : 2.48068
[1mStep[0m  [50/106], [94mLoss[0m : 2.30152
[1mStep[0m  [60/106], [94mLoss[0m : 2.57742
[1mStep[0m  [70/106], [94mLoss[0m : 2.63682
[1mStep[0m  [80/106], [94mLoss[0m : 2.42974
[1mStep[0m  [90/106], [94mLoss[0m : 2.57489
[1mStep[0m  [100/106], [94mLoss[0m : 2.30458

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18098
[1mStep[0m  [10/106], [94mLoss[0m : 2.52332
[1mStep[0m  [20/106], [94mLoss[0m : 2.28002
[1mStep[0m  [30/106], [94mLoss[0m : 2.22133
[1mStep[0m  [40/106], [94mLoss[0m : 2.12590
[1mStep[0m  [50/106], [94mLoss[0m : 2.24208
[1mStep[0m  [60/106], [94mLoss[0m : 2.16539
[1mStep[0m  [70/106], [94mLoss[0m : 2.29068
[1mStep[0m  [80/106], [94mLoss[0m : 2.37982
[1mStep[0m  [90/106], [94mLoss[0m : 2.35153
[1mStep[0m  [100/106], [94mLoss[0m : 2.22281

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.336, [92mTest[0m: 2.332, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.61311
[1mStep[0m  [10/106], [94mLoss[0m : 2.41477
[1mStep[0m  [20/106], [94mLoss[0m : 2.24094
[1mStep[0m  [30/106], [94mLoss[0m : 2.35091
[1mStep[0m  [40/106], [94mLoss[0m : 2.04218
[1mStep[0m  [50/106], [94mLoss[0m : 2.54286
[1mStep[0m  [60/106], [94mLoss[0m : 2.11390
[1mStep[0m  [70/106], [94mLoss[0m : 2.09365
[1mStep[0m  [80/106], [94mLoss[0m : 2.24238
[1mStep[0m  [90/106], [94mLoss[0m : 2.33398
[1mStep[0m  [100/106], [94mLoss[0m : 2.24966

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.342, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.24906
[1mStep[0m  [10/106], [94mLoss[0m : 2.38003
[1mStep[0m  [20/106], [94mLoss[0m : 2.33220
[1mStep[0m  [30/106], [94mLoss[0m : 2.04403
[1mStep[0m  [40/106], [94mLoss[0m : 2.53113
[1mStep[0m  [50/106], [94mLoss[0m : 2.12128
[1mStep[0m  [60/106], [94mLoss[0m : 2.34274
[1mStep[0m  [70/106], [94mLoss[0m : 2.47020
[1mStep[0m  [80/106], [94mLoss[0m : 2.68346
[1mStep[0m  [90/106], [94mLoss[0m : 2.41970
[1mStep[0m  [100/106], [94mLoss[0m : 1.99465

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.332, [92mTest[0m: 2.377, [96mlr[0m: 0.0023175
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.18561
[1mStep[0m  [10/106], [94mLoss[0m : 2.24763
[1mStep[0m  [20/106], [94mLoss[0m : 2.15183
[1mStep[0m  [30/106], [94mLoss[0m : 2.47040
[1mStep[0m  [40/106], [94mLoss[0m : 2.25182
[1mStep[0m  [50/106], [94mLoss[0m : 2.52100
[1mStep[0m  [60/106], [94mLoss[0m : 2.04938
[1mStep[0m  [70/106], [94mLoss[0m : 2.37162
[1mStep[0m  [80/106], [94mLoss[0m : 2.26866
[1mStep[0m  [90/106], [94mLoss[0m : 2.61127
[1mStep[0m  [100/106], [94mLoss[0m : 2.48946

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.344, [96mlr[0m: 0.0023175
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.365
====================================

Phase 1 - Evaluation MAE:  2.365320529577867
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.002575
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/106], [94mLoss[0m : 2.40574
[1mStep[0m  [10/106], [94mLoss[0m : 2.32725
[1mStep[0m  [20/106], [94mLoss[0m : 2.83036
[1mStep[0m  [30/106], [94mLoss[0m : 2.50096
[1mStep[0m  [40/106], [94mLoss[0m : 2.77383
[1mStep[0m  [50/106], [94mLoss[0m : 2.61786
[1mStep[0m  [60/106], [94mLoss[0m : 2.43554
[1mStep[0m  [70/106], [94mLoss[0m : 2.29759
[1mStep[0m  [80/106], [94mLoss[0m : 2.17585
[1mStep[0m  [90/106], [94mLoss[0m : 2.50221
[1mStep[0m  [100/106], [94mLoss[0m : 2.33426

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.370, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.23472
[1mStep[0m  [10/106], [94mLoss[0m : 2.43376
[1mStep[0m  [20/106], [94mLoss[0m : 2.28822
[1mStep[0m  [30/106], [94mLoss[0m : 2.44447
[1mStep[0m  [40/106], [94mLoss[0m : 2.18793
[1mStep[0m  [50/106], [94mLoss[0m : 2.40173
[1mStep[0m  [60/106], [94mLoss[0m : 2.31981
[1mStep[0m  [70/106], [94mLoss[0m : 2.15367
[1mStep[0m  [80/106], [94mLoss[0m : 2.48423
[1mStep[0m  [90/106], [94mLoss[0m : 2.47308
[1mStep[0m  [100/106], [94mLoss[0m : 2.51294

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.711, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.44334
[1mStep[0m  [10/106], [94mLoss[0m : 2.01236
[1mStep[0m  [20/106], [94mLoss[0m : 2.02735
[1mStep[0m  [30/106], [94mLoss[0m : 2.33909
[1mStep[0m  [40/106], [94mLoss[0m : 2.03520
[1mStep[0m  [50/106], [94mLoss[0m : 2.31985
[1mStep[0m  [60/106], [94mLoss[0m : 2.21124
[1mStep[0m  [70/106], [94mLoss[0m : 2.25295
[1mStep[0m  [80/106], [94mLoss[0m : 2.16107
[1mStep[0m  [90/106], [94mLoss[0m : 2.39035
[1mStep[0m  [100/106], [94mLoss[0m : 2.21869

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.267, [92mTest[0m: 2.441, [96mlr[0m: 0.002575
====================================

[1mStep[0m  [0/106], [94mLoss[0m : 2.41772
[1mStep[0m  [10/106], [94mLoss[0m : 2.35120
[1mStep[0m  [20/106], [94mLoss[0m : 2.50754
[1mStep[0m  [30/106], [94mLoss[0m : 1.90588
[1mStep[0m  [40/106], [94mLoss[0m : 2.27951
[1mStep[0m  [50/106], [94mLoss[0m : 1.90257
[1mStep[0m  [60/106], [94mLoss[0m : 2.11150
