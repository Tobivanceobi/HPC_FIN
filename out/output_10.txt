no change     /home/modelrep/sadiya/miniconda/condabin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda
no change     /home/modelrep/sadiya/miniconda/bin/conda-env
no change     /home/modelrep/sadiya/miniconda/bin/activate
no change     /home/modelrep/sadiya/miniconda/bin/deactivate
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.sh
no change     /home/modelrep/sadiya/miniconda/etc/fish/conf.d/conda.fish
no change     /home/modelrep/sadiya/miniconda/shell/condabin/Conda.psm1
no change     /home/modelrep/sadiya/miniconda/shell/condabin/conda-hook.ps1
no change     /home/modelrep/sadiya/miniconda/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /home/modelrep/sadiya/miniconda/etc/profile.d/conda.csh
no change     /home/modelrep/sadiya/.bashrc
No action taken.
-----------------
Process ID:  10
Number of Nodes:  1
Number of Array Tasks:  16
-----------------
Using device: cuda

AMD Instinct MI210
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
theta
whole_spec
delta
alpha
beta
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 11.38274
[1mStep[0m  [33/339], [94mLoss[0m : 6.37028
[1mStep[0m  [66/339], [94mLoss[0m : 4.05108
[1mStep[0m  [99/339], [94mLoss[0m : 2.96312
[1mStep[0m  [132/339], [94mLoss[0m : 3.07424
[1mStep[0m  [165/339], [94mLoss[0m : 2.54621
[1mStep[0m  [198/339], [94mLoss[0m : 2.67799
[1mStep[0m  [231/339], [94mLoss[0m : 3.10068
[1mStep[0m  [264/339], [94mLoss[0m : 2.48862
[1mStep[0m  [297/339], [94mLoss[0m : 2.28656
[1mStep[0m  [330/339], [94mLoss[0m : 2.50124

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.657, [92mTest[0m: 10.910, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.59259
[1mStep[0m  [33/339], [94mLoss[0m : 1.99508
[1mStep[0m  [66/339], [94mLoss[0m : 2.58468
[1mStep[0m  [99/339], [94mLoss[0m : 2.11655
[1mStep[0m  [132/339], [94mLoss[0m : 2.85438
[1mStep[0m  [165/339], [94mLoss[0m : 2.22333
[1mStep[0m  [198/339], [94mLoss[0m : 2.58320
[1mStep[0m  [231/339], [94mLoss[0m : 2.22853
[1mStep[0m  [264/339], [94mLoss[0m : 2.23985
[1mStep[0m  [297/339], [94mLoss[0m : 2.82260
[1mStep[0m  [330/339], [94mLoss[0m : 2.45213

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.97383
[1mStep[0m  [33/339], [94mLoss[0m : 2.52550
[1mStep[0m  [66/339], [94mLoss[0m : 2.21947
[1mStep[0m  [99/339], [94mLoss[0m : 2.76774
[1mStep[0m  [132/339], [94mLoss[0m : 2.59512
[1mStep[0m  [165/339], [94mLoss[0m : 2.68522
[1mStep[0m  [198/339], [94mLoss[0m : 3.12904
[1mStep[0m  [231/339], [94mLoss[0m : 2.54231
[1mStep[0m  [264/339], [94mLoss[0m : 2.69304
[1mStep[0m  [297/339], [94mLoss[0m : 2.62362
[1mStep[0m  [330/339], [94mLoss[0m : 2.02123

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.575, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88442
[1mStep[0m  [33/339], [94mLoss[0m : 2.75687
[1mStep[0m  [66/339], [94mLoss[0m : 2.44571
[1mStep[0m  [99/339], [94mLoss[0m : 2.38891
[1mStep[0m  [132/339], [94mLoss[0m : 2.62177
[1mStep[0m  [165/339], [94mLoss[0m : 2.13435
[1mStep[0m  [198/339], [94mLoss[0m : 2.55398
[1mStep[0m  [231/339], [94mLoss[0m : 2.62345
[1mStep[0m  [264/339], [94mLoss[0m : 2.04680
[1mStep[0m  [297/339], [94mLoss[0m : 2.55864
[1mStep[0m  [330/339], [94mLoss[0m : 2.60993

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.64361
[1mStep[0m  [33/339], [94mLoss[0m : 2.80952
[1mStep[0m  [66/339], [94mLoss[0m : 2.66144
[1mStep[0m  [99/339], [94mLoss[0m : 2.72492
[1mStep[0m  [132/339], [94mLoss[0m : 1.72859
[1mStep[0m  [165/339], [94mLoss[0m : 1.93319
[1mStep[0m  [198/339], [94mLoss[0m : 2.31638
[1mStep[0m  [231/339], [94mLoss[0m : 2.92867
[1mStep[0m  [264/339], [94mLoss[0m : 2.49291
[1mStep[0m  [297/339], [94mLoss[0m : 2.32535
[1mStep[0m  [330/339], [94mLoss[0m : 2.59040

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54406
[1mStep[0m  [33/339], [94mLoss[0m : 2.40800
[1mStep[0m  [66/339], [94mLoss[0m : 1.88629
[1mStep[0m  [99/339], [94mLoss[0m : 3.09809
[1mStep[0m  [132/339], [94mLoss[0m : 2.88401
[1mStep[0m  [165/339], [94mLoss[0m : 2.42273
[1mStep[0m  [198/339], [94mLoss[0m : 2.55355
[1mStep[0m  [231/339], [94mLoss[0m : 2.48100
[1mStep[0m  [264/339], [94mLoss[0m : 2.91765
[1mStep[0m  [297/339], [94mLoss[0m : 2.49340
[1mStep[0m  [330/339], [94mLoss[0m : 2.22313

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.578, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18980
[1mStep[0m  [33/339], [94mLoss[0m : 2.53727
[1mStep[0m  [66/339], [94mLoss[0m : 3.08491
[1mStep[0m  [99/339], [94mLoss[0m : 2.78354
[1mStep[0m  [132/339], [94mLoss[0m : 2.70414
[1mStep[0m  [165/339], [94mLoss[0m : 3.05944
[1mStep[0m  [198/339], [94mLoss[0m : 2.62998
[1mStep[0m  [231/339], [94mLoss[0m : 2.70253
[1mStep[0m  [264/339], [94mLoss[0m : 2.43587
[1mStep[0m  [297/339], [94mLoss[0m : 2.76037
[1mStep[0m  [330/339], [94mLoss[0m : 2.22439

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78172
[1mStep[0m  [33/339], [94mLoss[0m : 2.98903
[1mStep[0m  [66/339], [94mLoss[0m : 2.27258
[1mStep[0m  [99/339], [94mLoss[0m : 3.01017
[1mStep[0m  [132/339], [94mLoss[0m : 2.24063
[1mStep[0m  [165/339], [94mLoss[0m : 2.27989
[1mStep[0m  [198/339], [94mLoss[0m : 2.14093
[1mStep[0m  [231/339], [94mLoss[0m : 2.01879
[1mStep[0m  [264/339], [94mLoss[0m : 2.62950
[1mStep[0m  [297/339], [94mLoss[0m : 2.54313
[1mStep[0m  [330/339], [94mLoss[0m : 2.94186

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.557, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01368
[1mStep[0m  [33/339], [94mLoss[0m : 2.61356
[1mStep[0m  [66/339], [94mLoss[0m : 2.67977
[1mStep[0m  [99/339], [94mLoss[0m : 2.56132
[1mStep[0m  [132/339], [94mLoss[0m : 2.37488
[1mStep[0m  [165/339], [94mLoss[0m : 3.30043
[1mStep[0m  [198/339], [94mLoss[0m : 2.82247
[1mStep[0m  [231/339], [94mLoss[0m : 2.75337
[1mStep[0m  [264/339], [94mLoss[0m : 3.32033
[1mStep[0m  [297/339], [94mLoss[0m : 2.30530
[1mStep[0m  [330/339], [94mLoss[0m : 2.62229

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65847
[1mStep[0m  [33/339], [94mLoss[0m : 1.98253
[1mStep[0m  [66/339], [94mLoss[0m : 2.81986
[1mStep[0m  [99/339], [94mLoss[0m : 2.99085
[1mStep[0m  [132/339], [94mLoss[0m : 2.42284
[1mStep[0m  [165/339], [94mLoss[0m : 2.54087
[1mStep[0m  [198/339], [94mLoss[0m : 2.68409
[1mStep[0m  [231/339], [94mLoss[0m : 2.52207
[1mStep[0m  [264/339], [94mLoss[0m : 2.28535
[1mStep[0m  [297/339], [94mLoss[0m : 2.35490
[1mStep[0m  [330/339], [94mLoss[0m : 1.88720

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.96020
[1mStep[0m  [33/339], [94mLoss[0m : 2.28302
[1mStep[0m  [66/339], [94mLoss[0m : 2.35635
[1mStep[0m  [99/339], [94mLoss[0m : 2.93950
[1mStep[0m  [132/339], [94mLoss[0m : 2.27646
[1mStep[0m  [165/339], [94mLoss[0m : 2.20973
[1mStep[0m  [198/339], [94mLoss[0m : 2.88863
[1mStep[0m  [231/339], [94mLoss[0m : 2.21242
[1mStep[0m  [264/339], [94mLoss[0m : 2.43812
[1mStep[0m  [297/339], [94mLoss[0m : 2.95743
[1mStep[0m  [330/339], [94mLoss[0m : 2.61607

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.556, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.41706
[1mStep[0m  [33/339], [94mLoss[0m : 3.02512
[1mStep[0m  [66/339], [94mLoss[0m : 3.37039
[1mStep[0m  [99/339], [94mLoss[0m : 2.85653
[1mStep[0m  [132/339], [94mLoss[0m : 2.40111
[1mStep[0m  [165/339], [94mLoss[0m : 2.79453
[1mStep[0m  [198/339], [94mLoss[0m : 1.90361
[1mStep[0m  [231/339], [94mLoss[0m : 2.29213
[1mStep[0m  [264/339], [94mLoss[0m : 2.59024
[1mStep[0m  [297/339], [94mLoss[0m : 2.63487
[1mStep[0m  [330/339], [94mLoss[0m : 2.73837

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49404
[1mStep[0m  [33/339], [94mLoss[0m : 2.97294
[1mStep[0m  [66/339], [94mLoss[0m : 2.54095
[1mStep[0m  [99/339], [94mLoss[0m : 2.52667
[1mStep[0m  [132/339], [94mLoss[0m : 3.15187
[1mStep[0m  [165/339], [94mLoss[0m : 2.09653
[1mStep[0m  [198/339], [94mLoss[0m : 2.57368
[1mStep[0m  [231/339], [94mLoss[0m : 2.15790
[1mStep[0m  [264/339], [94mLoss[0m : 2.32062
[1mStep[0m  [297/339], [94mLoss[0m : 2.63974
[1mStep[0m  [330/339], [94mLoss[0m : 2.48869

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52017
[1mStep[0m  [33/339], [94mLoss[0m : 2.50001
[1mStep[0m  [66/339], [94mLoss[0m : 2.37977
[1mStep[0m  [99/339], [94mLoss[0m : 2.26662
[1mStep[0m  [132/339], [94mLoss[0m : 2.58737
[1mStep[0m  [165/339], [94mLoss[0m : 2.49064
[1mStep[0m  [198/339], [94mLoss[0m : 2.97119
[1mStep[0m  [231/339], [94mLoss[0m : 2.92682
[1mStep[0m  [264/339], [94mLoss[0m : 2.28985
[1mStep[0m  [297/339], [94mLoss[0m : 2.28912
[1mStep[0m  [330/339], [94mLoss[0m : 2.25263

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.80949
[1mStep[0m  [33/339], [94mLoss[0m : 2.36622
[1mStep[0m  [66/339], [94mLoss[0m : 2.46863
[1mStep[0m  [99/339], [94mLoss[0m : 2.58843
[1mStep[0m  [132/339], [94mLoss[0m : 2.56667
[1mStep[0m  [165/339], [94mLoss[0m : 2.41949
[1mStep[0m  [198/339], [94mLoss[0m : 2.01360
[1mStep[0m  [231/339], [94mLoss[0m : 2.42268
[1mStep[0m  [264/339], [94mLoss[0m : 3.40791
[1mStep[0m  [297/339], [94mLoss[0m : 2.49934
[1mStep[0m  [330/339], [94mLoss[0m : 2.35567

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.88224
[1mStep[0m  [33/339], [94mLoss[0m : 2.55276
[1mStep[0m  [66/339], [94mLoss[0m : 3.06143
[1mStep[0m  [99/339], [94mLoss[0m : 2.77161
[1mStep[0m  [132/339], [94mLoss[0m : 2.55020
[1mStep[0m  [165/339], [94mLoss[0m : 2.64051
[1mStep[0m  [198/339], [94mLoss[0m : 2.79102
[1mStep[0m  [231/339], [94mLoss[0m : 2.45347
[1mStep[0m  [264/339], [94mLoss[0m : 2.57142
[1mStep[0m  [297/339], [94mLoss[0m : 2.12478
[1mStep[0m  [330/339], [94mLoss[0m : 2.09648

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.529, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33735
[1mStep[0m  [33/339], [94mLoss[0m : 2.39808
[1mStep[0m  [66/339], [94mLoss[0m : 2.39867
[1mStep[0m  [99/339], [94mLoss[0m : 2.46582
[1mStep[0m  [132/339], [94mLoss[0m : 2.28889
[1mStep[0m  [165/339], [94mLoss[0m : 1.89423
[1mStep[0m  [198/339], [94mLoss[0m : 2.95125
[1mStep[0m  [231/339], [94mLoss[0m : 2.38876
[1mStep[0m  [264/339], [94mLoss[0m : 2.33543
[1mStep[0m  [297/339], [94mLoss[0m : 2.24018
[1mStep[0m  [330/339], [94mLoss[0m : 2.73671

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.524, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35109
[1mStep[0m  [33/339], [94mLoss[0m : 2.23846
[1mStep[0m  [66/339], [94mLoss[0m : 1.85953
[1mStep[0m  [99/339], [94mLoss[0m : 2.92066
[1mStep[0m  [132/339], [94mLoss[0m : 2.55451
[1mStep[0m  [165/339], [94mLoss[0m : 2.40433
[1mStep[0m  [198/339], [94mLoss[0m : 2.98488
[1mStep[0m  [231/339], [94mLoss[0m : 2.01818
[1mStep[0m  [264/339], [94mLoss[0m : 2.43834
[1mStep[0m  [297/339], [94mLoss[0m : 1.93978
[1mStep[0m  [330/339], [94mLoss[0m : 3.26531

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.509, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.16290
[1mStep[0m  [33/339], [94mLoss[0m : 3.24014
[1mStep[0m  [66/339], [94mLoss[0m : 2.15963
[1mStep[0m  [99/339], [94mLoss[0m : 2.77183
[1mStep[0m  [132/339], [94mLoss[0m : 1.98604
[1mStep[0m  [165/339], [94mLoss[0m : 2.94372
[1mStep[0m  [198/339], [94mLoss[0m : 2.43002
[1mStep[0m  [231/339], [94mLoss[0m : 2.86642
[1mStep[0m  [264/339], [94mLoss[0m : 2.28747
[1mStep[0m  [297/339], [94mLoss[0m : 2.52109
[1mStep[0m  [330/339], [94mLoss[0m : 2.18750

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.493, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43282
[1mStep[0m  [33/339], [94mLoss[0m : 2.11570
[1mStep[0m  [66/339], [94mLoss[0m : 1.86998
[1mStep[0m  [99/339], [94mLoss[0m : 3.21687
[1mStep[0m  [132/339], [94mLoss[0m : 3.10638
[1mStep[0m  [165/339], [94mLoss[0m : 2.36476
[1mStep[0m  [198/339], [94mLoss[0m : 2.56591
[1mStep[0m  [231/339], [94mLoss[0m : 2.94484
[1mStep[0m  [264/339], [94mLoss[0m : 2.70683
[1mStep[0m  [297/339], [94mLoss[0m : 2.27665
[1mStep[0m  [330/339], [94mLoss[0m : 2.37851

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.21903
[1mStep[0m  [33/339], [94mLoss[0m : 1.66973
[1mStep[0m  [66/339], [94mLoss[0m : 2.17375
[1mStep[0m  [99/339], [94mLoss[0m : 2.68070
[1mStep[0m  [132/339], [94mLoss[0m : 2.53331
[1mStep[0m  [165/339], [94mLoss[0m : 2.28778
[1mStep[0m  [198/339], [94mLoss[0m : 3.23222
[1mStep[0m  [231/339], [94mLoss[0m : 2.27462
[1mStep[0m  [264/339], [94mLoss[0m : 2.45209
[1mStep[0m  [297/339], [94mLoss[0m : 2.63996
[1mStep[0m  [330/339], [94mLoss[0m : 2.27149

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81576
[1mStep[0m  [33/339], [94mLoss[0m : 2.20473
[1mStep[0m  [66/339], [94mLoss[0m : 2.84608
[1mStep[0m  [99/339], [94mLoss[0m : 2.75311
[1mStep[0m  [132/339], [94mLoss[0m : 2.70057
[1mStep[0m  [165/339], [94mLoss[0m : 2.81526
[1mStep[0m  [198/339], [94mLoss[0m : 2.68245
[1mStep[0m  [231/339], [94mLoss[0m : 2.81476
[1mStep[0m  [264/339], [94mLoss[0m : 2.67087
[1mStep[0m  [297/339], [94mLoss[0m : 2.96027
[1mStep[0m  [330/339], [94mLoss[0m : 2.17802

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86774
[1mStep[0m  [33/339], [94mLoss[0m : 2.64369
[1mStep[0m  [66/339], [94mLoss[0m : 2.31012
[1mStep[0m  [99/339], [94mLoss[0m : 2.97345
[1mStep[0m  [132/339], [94mLoss[0m : 2.59266
[1mStep[0m  [165/339], [94mLoss[0m : 2.11699
[1mStep[0m  [198/339], [94mLoss[0m : 2.84617
[1mStep[0m  [231/339], [94mLoss[0m : 3.33855
[1mStep[0m  [264/339], [94mLoss[0m : 2.15881
[1mStep[0m  [297/339], [94mLoss[0m : 2.41545
[1mStep[0m  [330/339], [94mLoss[0m : 2.13298

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51181
[1mStep[0m  [33/339], [94mLoss[0m : 3.07068
[1mStep[0m  [66/339], [94mLoss[0m : 2.56869
[1mStep[0m  [99/339], [94mLoss[0m : 3.31427
[1mStep[0m  [132/339], [94mLoss[0m : 2.02921
[1mStep[0m  [165/339], [94mLoss[0m : 2.63300
[1mStep[0m  [198/339], [94mLoss[0m : 1.90277
[1mStep[0m  [231/339], [94mLoss[0m : 2.43551
[1mStep[0m  [264/339], [94mLoss[0m : 2.57213
[1mStep[0m  [297/339], [94mLoss[0m : 2.86278
[1mStep[0m  [330/339], [94mLoss[0m : 2.03152

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.86009
[1mStep[0m  [33/339], [94mLoss[0m : 2.23115
[1mStep[0m  [66/339], [94mLoss[0m : 2.27845
[1mStep[0m  [99/339], [94mLoss[0m : 3.07941
[1mStep[0m  [132/339], [94mLoss[0m : 2.27184
[1mStep[0m  [165/339], [94mLoss[0m : 2.30545
[1mStep[0m  [198/339], [94mLoss[0m : 2.87239
[1mStep[0m  [231/339], [94mLoss[0m : 2.61700
[1mStep[0m  [264/339], [94mLoss[0m : 2.66087
[1mStep[0m  [297/339], [94mLoss[0m : 2.41157
[1mStep[0m  [330/339], [94mLoss[0m : 2.48228

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21885
[1mStep[0m  [33/339], [94mLoss[0m : 2.79707
[1mStep[0m  [66/339], [94mLoss[0m : 2.34176
[1mStep[0m  [99/339], [94mLoss[0m : 2.01209
[1mStep[0m  [132/339], [94mLoss[0m : 3.44583
[1mStep[0m  [165/339], [94mLoss[0m : 2.57044
[1mStep[0m  [198/339], [94mLoss[0m : 3.11205
[1mStep[0m  [231/339], [94mLoss[0m : 1.79736
[1mStep[0m  [264/339], [94mLoss[0m : 2.31087
[1mStep[0m  [297/339], [94mLoss[0m : 2.15316
[1mStep[0m  [330/339], [94mLoss[0m : 2.77160

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93915
[1mStep[0m  [33/339], [94mLoss[0m : 2.55559
[1mStep[0m  [66/339], [94mLoss[0m : 2.28737
[1mStep[0m  [99/339], [94mLoss[0m : 2.51305
[1mStep[0m  [132/339], [94mLoss[0m : 2.07122
[1mStep[0m  [165/339], [94mLoss[0m : 2.87115
[1mStep[0m  [198/339], [94mLoss[0m : 2.17915
[1mStep[0m  [231/339], [94mLoss[0m : 2.31376
[1mStep[0m  [264/339], [94mLoss[0m : 2.44155
[1mStep[0m  [297/339], [94mLoss[0m : 2.40899
[1mStep[0m  [330/339], [94mLoss[0m : 2.75422

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.337, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63909
[1mStep[0m  [33/339], [94mLoss[0m : 2.60218
[1mStep[0m  [66/339], [94mLoss[0m : 1.97972
[1mStep[0m  [99/339], [94mLoss[0m : 1.97205
[1mStep[0m  [132/339], [94mLoss[0m : 2.40349
[1mStep[0m  [165/339], [94mLoss[0m : 2.09115
[1mStep[0m  [198/339], [94mLoss[0m : 2.49672
[1mStep[0m  [231/339], [94mLoss[0m : 2.60900
[1mStep[0m  [264/339], [94mLoss[0m : 2.85476
[1mStep[0m  [297/339], [94mLoss[0m : 2.67725
[1mStep[0m  [330/339], [94mLoss[0m : 1.95267

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47645
[1mStep[0m  [33/339], [94mLoss[0m : 2.94398
[1mStep[0m  [66/339], [94mLoss[0m : 2.33923
[1mStep[0m  [99/339], [94mLoss[0m : 2.40688
[1mStep[0m  [132/339], [94mLoss[0m : 1.97213
[1mStep[0m  [165/339], [94mLoss[0m : 2.96703
[1mStep[0m  [198/339], [94mLoss[0m : 2.71152
[1mStep[0m  [231/339], [94mLoss[0m : 2.53282
[1mStep[0m  [264/339], [94mLoss[0m : 2.86170
[1mStep[0m  [297/339], [94mLoss[0m : 2.28436
[1mStep[0m  [330/339], [94mLoss[0m : 2.18704

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63052
[1mStep[0m  [33/339], [94mLoss[0m : 2.42751
[1mStep[0m  [66/339], [94mLoss[0m : 2.15691
[1mStep[0m  [99/339], [94mLoss[0m : 2.61268
[1mStep[0m  [132/339], [94mLoss[0m : 2.60220
[1mStep[0m  [165/339], [94mLoss[0m : 3.11673
[1mStep[0m  [198/339], [94mLoss[0m : 1.90805
[1mStep[0m  [231/339], [94mLoss[0m : 2.44599
[1mStep[0m  [264/339], [94mLoss[0m : 2.68909
[1mStep[0m  [297/339], [94mLoss[0m : 2.40275
[1mStep[0m  [330/339], [94mLoss[0m : 2.80748

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.346
====================================

Phase 1 - Evaluation MAE:  2.345663380833854
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.80937
[1mStep[0m  [33/339], [94mLoss[0m : 2.33920
[1mStep[0m  [66/339], [94mLoss[0m : 2.96704
[1mStep[0m  [99/339], [94mLoss[0m : 2.23360
[1mStep[0m  [132/339], [94mLoss[0m : 2.92683
[1mStep[0m  [165/339], [94mLoss[0m : 2.15499
[1mStep[0m  [198/339], [94mLoss[0m : 2.22497
[1mStep[0m  [231/339], [94mLoss[0m : 3.01163
[1mStep[0m  [264/339], [94mLoss[0m : 2.66251
[1mStep[0m  [297/339], [94mLoss[0m : 2.49998
[1mStep[0m  [330/339], [94mLoss[0m : 2.73840

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.514, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.46720
[1mStep[0m  [33/339], [94mLoss[0m : 2.65636
[1mStep[0m  [66/339], [94mLoss[0m : 2.82521
[1mStep[0m  [99/339], [94mLoss[0m : 2.86269
[1mStep[0m  [132/339], [94mLoss[0m : 2.16422
[1mStep[0m  [165/339], [94mLoss[0m : 2.74345
[1mStep[0m  [198/339], [94mLoss[0m : 2.38898
[1mStep[0m  [231/339], [94mLoss[0m : 2.39728
[1mStep[0m  [264/339], [94mLoss[0m : 2.13808
[1mStep[0m  [297/339], [94mLoss[0m : 1.76526
[1mStep[0m  [330/339], [94mLoss[0m : 1.94552

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35183
[1mStep[0m  [33/339], [94mLoss[0m : 2.53865
[1mStep[0m  [66/339], [94mLoss[0m : 2.43791
[1mStep[0m  [99/339], [94mLoss[0m : 2.06032
[1mStep[0m  [132/339], [94mLoss[0m : 2.03813
[1mStep[0m  [165/339], [94mLoss[0m : 2.39017
[1mStep[0m  [198/339], [94mLoss[0m : 2.24519
[1mStep[0m  [231/339], [94mLoss[0m : 1.82265
[1mStep[0m  [264/339], [94mLoss[0m : 2.55213
[1mStep[0m  [297/339], [94mLoss[0m : 3.15127
[1mStep[0m  [330/339], [94mLoss[0m : 2.99272

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95300
[1mStep[0m  [33/339], [94mLoss[0m : 2.53137
[1mStep[0m  [66/339], [94mLoss[0m : 2.22807
[1mStep[0m  [99/339], [94mLoss[0m : 2.07046
[1mStep[0m  [132/339], [94mLoss[0m : 1.86155
[1mStep[0m  [165/339], [94mLoss[0m : 2.39591
[1mStep[0m  [198/339], [94mLoss[0m : 2.76573
[1mStep[0m  [231/339], [94mLoss[0m : 2.11534
[1mStep[0m  [264/339], [94mLoss[0m : 2.51368
[1mStep[0m  [297/339], [94mLoss[0m : 3.55532
[1mStep[0m  [330/339], [94mLoss[0m : 1.76270

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.328, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.67961
[1mStep[0m  [33/339], [94mLoss[0m : 2.40451
[1mStep[0m  [66/339], [94mLoss[0m : 2.40146
[1mStep[0m  [99/339], [94mLoss[0m : 2.30428
[1mStep[0m  [132/339], [94mLoss[0m : 2.58131
[1mStep[0m  [165/339], [94mLoss[0m : 2.62376
[1mStep[0m  [198/339], [94mLoss[0m : 3.02900
[1mStep[0m  [231/339], [94mLoss[0m : 2.71166
[1mStep[0m  [264/339], [94mLoss[0m : 1.88156
[1mStep[0m  [297/339], [94mLoss[0m : 2.43537
[1mStep[0m  [330/339], [94mLoss[0m : 2.19177

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.84016
[1mStep[0m  [33/339], [94mLoss[0m : 2.41500
[1mStep[0m  [66/339], [94mLoss[0m : 2.01821
[1mStep[0m  [99/339], [94mLoss[0m : 2.26364
[1mStep[0m  [132/339], [94mLoss[0m : 2.82989
[1mStep[0m  [165/339], [94mLoss[0m : 2.41984
[1mStep[0m  [198/339], [94mLoss[0m : 2.07799
[1mStep[0m  [231/339], [94mLoss[0m : 1.90275
[1mStep[0m  [264/339], [94mLoss[0m : 2.72354
[1mStep[0m  [297/339], [94mLoss[0m : 2.18552
[1mStep[0m  [330/339], [94mLoss[0m : 2.16709

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.214, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.53259
[1mStep[0m  [33/339], [94mLoss[0m : 1.83432
[1mStep[0m  [66/339], [94mLoss[0m : 1.77613
[1mStep[0m  [99/339], [94mLoss[0m : 2.21528
[1mStep[0m  [132/339], [94mLoss[0m : 2.60353
[1mStep[0m  [165/339], [94mLoss[0m : 2.53199
[1mStep[0m  [198/339], [94mLoss[0m : 1.98435
[1mStep[0m  [231/339], [94mLoss[0m : 1.92366
[1mStep[0m  [264/339], [94mLoss[0m : 1.94898
[1mStep[0m  [297/339], [94mLoss[0m : 1.45800
[1mStep[0m  [330/339], [94mLoss[0m : 2.58583

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91160
[1mStep[0m  [33/339], [94mLoss[0m : 2.32907
[1mStep[0m  [66/339], [94mLoss[0m : 1.91074
[1mStep[0m  [99/339], [94mLoss[0m : 1.73767
[1mStep[0m  [132/339], [94mLoss[0m : 2.61439
[1mStep[0m  [165/339], [94mLoss[0m : 1.96642
[1mStep[0m  [198/339], [94mLoss[0m : 2.28471
[1mStep[0m  [231/339], [94mLoss[0m : 2.12011
[1mStep[0m  [264/339], [94mLoss[0m : 2.13783
[1mStep[0m  [297/339], [94mLoss[0m : 2.58436
[1mStep[0m  [330/339], [94mLoss[0m : 1.56423

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.93013
[1mStep[0m  [33/339], [94mLoss[0m : 2.03161
[1mStep[0m  [66/339], [94mLoss[0m : 2.21003
[1mStep[0m  [99/339], [94mLoss[0m : 1.47875
[1mStep[0m  [132/339], [94mLoss[0m : 2.11629
[1mStep[0m  [165/339], [94mLoss[0m : 2.29408
[1mStep[0m  [198/339], [94mLoss[0m : 2.16928
[1mStep[0m  [231/339], [94mLoss[0m : 2.24821
[1mStep[0m  [264/339], [94mLoss[0m : 2.07754
[1mStep[0m  [297/339], [94mLoss[0m : 1.97751
[1mStep[0m  [330/339], [94mLoss[0m : 1.20997

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.72344
[1mStep[0m  [33/339], [94mLoss[0m : 2.02654
[1mStep[0m  [66/339], [94mLoss[0m : 2.11369
[1mStep[0m  [99/339], [94mLoss[0m : 2.36650
[1mStep[0m  [132/339], [94mLoss[0m : 2.23774
[1mStep[0m  [165/339], [94mLoss[0m : 2.07611
[1mStep[0m  [198/339], [94mLoss[0m : 2.07052
[1mStep[0m  [231/339], [94mLoss[0m : 2.18012
[1mStep[0m  [264/339], [94mLoss[0m : 1.79336
[1mStep[0m  [297/339], [94mLoss[0m : 1.94405
[1mStep[0m  [330/339], [94mLoss[0m : 2.00411

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.068, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10177
[1mStep[0m  [33/339], [94mLoss[0m : 1.99513
[1mStep[0m  [66/339], [94mLoss[0m : 2.16249
[1mStep[0m  [99/339], [94mLoss[0m : 2.51103
[1mStep[0m  [132/339], [94mLoss[0m : 2.94014
[1mStep[0m  [165/339], [94mLoss[0m : 1.65360
[1mStep[0m  [198/339], [94mLoss[0m : 1.85276
[1mStep[0m  [231/339], [94mLoss[0m : 2.12000
[1mStep[0m  [264/339], [94mLoss[0m : 2.25128
[1mStep[0m  [297/339], [94mLoss[0m : 2.10333
[1mStep[0m  [330/339], [94mLoss[0m : 2.01690

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.999, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90354
[1mStep[0m  [33/339], [94mLoss[0m : 1.61512
[1mStep[0m  [66/339], [94mLoss[0m : 1.63373
[1mStep[0m  [99/339], [94mLoss[0m : 2.17596
[1mStep[0m  [132/339], [94mLoss[0m : 1.78723
[1mStep[0m  [165/339], [94mLoss[0m : 1.48081
[1mStep[0m  [198/339], [94mLoss[0m : 1.80909
[1mStep[0m  [231/339], [94mLoss[0m : 1.85387
[1mStep[0m  [264/339], [94mLoss[0m : 3.16690
[1mStep[0m  [297/339], [94mLoss[0m : 2.29938
[1mStep[0m  [330/339], [94mLoss[0m : 1.82169

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.991, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57566
[1mStep[0m  [33/339], [94mLoss[0m : 1.59363
[1mStep[0m  [66/339], [94mLoss[0m : 1.67967
[1mStep[0m  [99/339], [94mLoss[0m : 2.45139
[1mStep[0m  [132/339], [94mLoss[0m : 1.64087
[1mStep[0m  [165/339], [94mLoss[0m : 2.39650
[1mStep[0m  [198/339], [94mLoss[0m : 2.21137
[1mStep[0m  [231/339], [94mLoss[0m : 2.06430
[1mStep[0m  [264/339], [94mLoss[0m : 2.34200
[1mStep[0m  [297/339], [94mLoss[0m : 2.27504
[1mStep[0m  [330/339], [94mLoss[0m : 1.38147

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58876
[1mStep[0m  [33/339], [94mLoss[0m : 1.93016
[1mStep[0m  [66/339], [94mLoss[0m : 1.60536
[1mStep[0m  [99/339], [94mLoss[0m : 1.98727
[1mStep[0m  [132/339], [94mLoss[0m : 2.27629
[1mStep[0m  [165/339], [94mLoss[0m : 1.63429
[1mStep[0m  [198/339], [94mLoss[0m : 2.30505
[1mStep[0m  [231/339], [94mLoss[0m : 1.82491
[1mStep[0m  [264/339], [94mLoss[0m : 2.37271
[1mStep[0m  [297/339], [94mLoss[0m : 1.61104
[1mStep[0m  [330/339], [94mLoss[0m : 1.63549

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.933, [92mTest[0m: 2.506, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47599
[1mStep[0m  [33/339], [94mLoss[0m : 1.86179
[1mStep[0m  [66/339], [94mLoss[0m : 2.42245
[1mStep[0m  [99/339], [94mLoss[0m : 2.01983
[1mStep[0m  [132/339], [94mLoss[0m : 1.53858
[1mStep[0m  [165/339], [94mLoss[0m : 1.80844
[1mStep[0m  [198/339], [94mLoss[0m : 2.23598
[1mStep[0m  [231/339], [94mLoss[0m : 1.48006
[1mStep[0m  [264/339], [94mLoss[0m : 1.76796
[1mStep[0m  [297/339], [94mLoss[0m : 1.51030
[1mStep[0m  [330/339], [94mLoss[0m : 2.06749

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.549, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.16740
[1mStep[0m  [33/339], [94mLoss[0m : 2.40028
[1mStep[0m  [66/339], [94mLoss[0m : 1.74753
[1mStep[0m  [99/339], [94mLoss[0m : 2.09956
[1mStep[0m  [132/339], [94mLoss[0m : 1.75329
[1mStep[0m  [165/339], [94mLoss[0m : 1.77798
[1mStep[0m  [198/339], [94mLoss[0m : 1.27033
[1mStep[0m  [231/339], [94mLoss[0m : 1.70577
[1mStep[0m  [264/339], [94mLoss[0m : 2.37849
[1mStep[0m  [297/339], [94mLoss[0m : 1.51952
[1mStep[0m  [330/339], [94mLoss[0m : 2.01497

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.858, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.30260
[1mStep[0m  [33/339], [94mLoss[0m : 1.18960
[1mStep[0m  [66/339], [94mLoss[0m : 1.36882
[1mStep[0m  [99/339], [94mLoss[0m : 2.17868
[1mStep[0m  [132/339], [94mLoss[0m : 1.95264
[1mStep[0m  [165/339], [94mLoss[0m : 1.76390
[1mStep[0m  [198/339], [94mLoss[0m : 2.17640
[1mStep[0m  [231/339], [94mLoss[0m : 2.02343
[1mStep[0m  [264/339], [94mLoss[0m : 2.08109
[1mStep[0m  [297/339], [94mLoss[0m : 1.82616
[1mStep[0m  [330/339], [94mLoss[0m : 2.14564

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.849, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91834
[1mStep[0m  [33/339], [94mLoss[0m : 1.46529
[1mStep[0m  [66/339], [94mLoss[0m : 2.02270
[1mStep[0m  [99/339], [94mLoss[0m : 1.86436
[1mStep[0m  [132/339], [94mLoss[0m : 1.92455
[1mStep[0m  [165/339], [94mLoss[0m : 2.29608
[1mStep[0m  [198/339], [94mLoss[0m : 1.86559
[1mStep[0m  [231/339], [94mLoss[0m : 1.54826
[1mStep[0m  [264/339], [94mLoss[0m : 1.81206
[1mStep[0m  [297/339], [94mLoss[0m : 2.12333
[1mStep[0m  [330/339], [94mLoss[0m : 1.63351

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.816, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.82162
[1mStep[0m  [33/339], [94mLoss[0m : 2.06873
[1mStep[0m  [66/339], [94mLoss[0m : 1.67982
[1mStep[0m  [99/339], [94mLoss[0m : 1.19438
[1mStep[0m  [132/339], [94mLoss[0m : 1.68874
[1mStep[0m  [165/339], [94mLoss[0m : 1.10707
[1mStep[0m  [198/339], [94mLoss[0m : 2.06769
[1mStep[0m  [231/339], [94mLoss[0m : 1.61211
[1mStep[0m  [264/339], [94mLoss[0m : 2.19669
[1mStep[0m  [297/339], [94mLoss[0m : 1.70733
[1mStep[0m  [330/339], [94mLoss[0m : 1.84079

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.789, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59752
[1mStep[0m  [33/339], [94mLoss[0m : 1.82115
[1mStep[0m  [66/339], [94mLoss[0m : 1.96688
[1mStep[0m  [99/339], [94mLoss[0m : 1.42656
[1mStep[0m  [132/339], [94mLoss[0m : 2.00791
[1mStep[0m  [165/339], [94mLoss[0m : 1.89506
[1mStep[0m  [198/339], [94mLoss[0m : 1.90785
[1mStep[0m  [231/339], [94mLoss[0m : 1.72458
[1mStep[0m  [264/339], [94mLoss[0m : 1.87308
[1mStep[0m  [297/339], [94mLoss[0m : 1.95798
[1mStep[0m  [330/339], [94mLoss[0m : 1.66353

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04690
[1mStep[0m  [33/339], [94mLoss[0m : 1.97487
[1mStep[0m  [66/339], [94mLoss[0m : 1.85771
[1mStep[0m  [99/339], [94mLoss[0m : 1.91633
[1mStep[0m  [132/339], [94mLoss[0m : 1.80265
[1mStep[0m  [165/339], [94mLoss[0m : 1.63723
[1mStep[0m  [198/339], [94mLoss[0m : 1.67247
[1mStep[0m  [231/339], [94mLoss[0m : 1.77193
[1mStep[0m  [264/339], [94mLoss[0m : 1.96899
[1mStep[0m  [297/339], [94mLoss[0m : 1.76468
[1mStep[0m  [330/339], [94mLoss[0m : 1.95012

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.464, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.01739
[1mStep[0m  [33/339], [94mLoss[0m : 1.66871
[1mStep[0m  [66/339], [94mLoss[0m : 1.70495
[1mStep[0m  [99/339], [94mLoss[0m : 1.52645
[1mStep[0m  [132/339], [94mLoss[0m : 2.14651
[1mStep[0m  [165/339], [94mLoss[0m : 1.91153
[1mStep[0m  [198/339], [94mLoss[0m : 1.42885
[1mStep[0m  [231/339], [94mLoss[0m : 1.21189
[1mStep[0m  [264/339], [94mLoss[0m : 1.47080
[1mStep[0m  [297/339], [94mLoss[0m : 1.72432
[1mStep[0m  [330/339], [94mLoss[0m : 1.72685

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.40063
[1mStep[0m  [33/339], [94mLoss[0m : 1.97047
[1mStep[0m  [66/339], [94mLoss[0m : 1.47232
[1mStep[0m  [99/339], [94mLoss[0m : 1.37407
[1mStep[0m  [132/339], [94mLoss[0m : 1.95338
[1mStep[0m  [165/339], [94mLoss[0m : 1.42981
[1mStep[0m  [198/339], [94mLoss[0m : 1.62757
[1mStep[0m  [231/339], [94mLoss[0m : 2.02778
[1mStep[0m  [264/339], [94mLoss[0m : 1.82205
[1mStep[0m  [297/339], [94mLoss[0m : 1.58641
[1mStep[0m  [330/339], [94mLoss[0m : 1.72737

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.660, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 0.95049
[1mStep[0m  [33/339], [94mLoss[0m : 1.99733
[1mStep[0m  [66/339], [94mLoss[0m : 1.81380
[1mStep[0m  [99/339], [94mLoss[0m : 1.30649
[1mStep[0m  [132/339], [94mLoss[0m : 1.80612
[1mStep[0m  [165/339], [94mLoss[0m : 1.89730
[1mStep[0m  [198/339], [94mLoss[0m : 1.73419
[1mStep[0m  [231/339], [94mLoss[0m : 1.58552
[1mStep[0m  [264/339], [94mLoss[0m : 1.53758
[1mStep[0m  [297/339], [94mLoss[0m : 1.82450
[1mStep[0m  [330/339], [94mLoss[0m : 1.59260

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52400
[1mStep[0m  [33/339], [94mLoss[0m : 1.79300
[1mStep[0m  [66/339], [94mLoss[0m : 1.84904
[1mStep[0m  [99/339], [94mLoss[0m : 1.46202
[1mStep[0m  [132/339], [94mLoss[0m : 1.78910
[1mStep[0m  [165/339], [94mLoss[0m : 1.98660
[1mStep[0m  [198/339], [94mLoss[0m : 1.98269
[1mStep[0m  [231/339], [94mLoss[0m : 1.82175
[1mStep[0m  [264/339], [94mLoss[0m : 1.51680
[1mStep[0m  [297/339], [94mLoss[0m : 1.58250
[1mStep[0m  [330/339], [94mLoss[0m : 1.39513

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.648, [92mTest[0m: 2.528, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17918
[1mStep[0m  [33/339], [94mLoss[0m : 1.19991
[1mStep[0m  [66/339], [94mLoss[0m : 1.54532
[1mStep[0m  [99/339], [94mLoss[0m : 1.46493
[1mStep[0m  [132/339], [94mLoss[0m : 1.50089
[1mStep[0m  [165/339], [94mLoss[0m : 1.61612
[1mStep[0m  [198/339], [94mLoss[0m : 2.27586
[1mStep[0m  [231/339], [94mLoss[0m : 1.71247
[1mStep[0m  [264/339], [94mLoss[0m : 1.76045
[1mStep[0m  [297/339], [94mLoss[0m : 2.08406
[1mStep[0m  [330/339], [94mLoss[0m : 1.63980

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.633, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70788
[1mStep[0m  [33/339], [94mLoss[0m : 1.40676
[1mStep[0m  [66/339], [94mLoss[0m : 1.49997
[1mStep[0m  [99/339], [94mLoss[0m : 1.56705
[1mStep[0m  [132/339], [94mLoss[0m : 1.85736
[1mStep[0m  [165/339], [94mLoss[0m : 2.02490
[1mStep[0m  [198/339], [94mLoss[0m : 1.39059
[1mStep[0m  [231/339], [94mLoss[0m : 1.45236
[1mStep[0m  [264/339], [94mLoss[0m : 1.53764
[1mStep[0m  [297/339], [94mLoss[0m : 1.55819
[1mStep[0m  [330/339], [94mLoss[0m : 2.11520

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.33855
[1mStep[0m  [33/339], [94mLoss[0m : 1.91554
[1mStep[0m  [66/339], [94mLoss[0m : 2.16670
[1mStep[0m  [99/339], [94mLoss[0m : 1.28904
[1mStep[0m  [132/339], [94mLoss[0m : 1.45091
[1mStep[0m  [165/339], [94mLoss[0m : 2.28825
[1mStep[0m  [198/339], [94mLoss[0m : 1.09256
[1mStep[0m  [231/339], [94mLoss[0m : 1.43247
[1mStep[0m  [264/339], [94mLoss[0m : 1.91542
[1mStep[0m  [297/339], [94mLoss[0m : 1.36133
[1mStep[0m  [330/339], [94mLoss[0m : 1.72147

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51902
[1mStep[0m  [33/339], [94mLoss[0m : 1.46636
[1mStep[0m  [66/339], [94mLoss[0m : 1.37157
[1mStep[0m  [99/339], [94mLoss[0m : 1.11985
[1mStep[0m  [132/339], [94mLoss[0m : 1.05559
[1mStep[0m  [165/339], [94mLoss[0m : 1.57272
[1mStep[0m  [198/339], [94mLoss[0m : 1.33588
[1mStep[0m  [231/339], [94mLoss[0m : 0.84507
[1mStep[0m  [264/339], [94mLoss[0m : 1.48261
[1mStep[0m  [297/339], [94mLoss[0m : 1.61043
[1mStep[0m  [330/339], [94mLoss[0m : 1.78285

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.595, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.62352
[1mStep[0m  [33/339], [94mLoss[0m : 1.94877
[1mStep[0m  [66/339], [94mLoss[0m : 1.47001
[1mStep[0m  [99/339], [94mLoss[0m : 1.30455
[1mStep[0m  [132/339], [94mLoss[0m : 1.62526
[1mStep[0m  [165/339], [94mLoss[0m : 1.38342
[1mStep[0m  [198/339], [94mLoss[0m : 1.54493
[1mStep[0m  [231/339], [94mLoss[0m : 1.28917
[1mStep[0m  [264/339], [94mLoss[0m : 1.68646
[1mStep[0m  [297/339], [94mLoss[0m : 1.78171
[1mStep[0m  [330/339], [94mLoss[0m : 1.36097

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.577, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.468
====================================

Phase 2 - Evaluation MAE:  2.467500439787333
MAE score P1       2.345663
MAE score P2         2.4675
loss               1.576684
learning_rate      0.007525
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.1
weight_decay         0.0001
Name: 0, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.14690
[1mStep[0m  [16/169], [94mLoss[0m : 9.21168
[1mStep[0m  [32/169], [94mLoss[0m : 7.73333
[1mStep[0m  [48/169], [94mLoss[0m : 6.20117
[1mStep[0m  [64/169], [94mLoss[0m : 4.51062
[1mStep[0m  [80/169], [94mLoss[0m : 2.67246
[1mStep[0m  [96/169], [94mLoss[0m : 2.36031
[1mStep[0m  [112/169], [94mLoss[0m : 3.29729
[1mStep[0m  [128/169], [94mLoss[0m : 2.41728
[1mStep[0m  [144/169], [94mLoss[0m : 3.24593
[1mStep[0m  [160/169], [94mLoss[0m : 2.49542

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.825, [92mTest[0m: 11.171, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31123
[1mStep[0m  [16/169], [94mLoss[0m : 2.72159
[1mStep[0m  [32/169], [94mLoss[0m : 2.53277
[1mStep[0m  [48/169], [94mLoss[0m : 3.04982
[1mStep[0m  [64/169], [94mLoss[0m : 2.46922
[1mStep[0m  [80/169], [94mLoss[0m : 2.52999
[1mStep[0m  [96/169], [94mLoss[0m : 2.82982
[1mStep[0m  [112/169], [94mLoss[0m : 2.90765
[1mStep[0m  [128/169], [94mLoss[0m : 2.89647
[1mStep[0m  [144/169], [94mLoss[0m : 2.68440
[1mStep[0m  [160/169], [94mLoss[0m : 2.24833

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.648, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55754
[1mStep[0m  [16/169], [94mLoss[0m : 2.42207
[1mStep[0m  [32/169], [94mLoss[0m : 2.38842
[1mStep[0m  [48/169], [94mLoss[0m : 2.83518
[1mStep[0m  [64/169], [94mLoss[0m : 2.61052
[1mStep[0m  [80/169], [94mLoss[0m : 2.60094
[1mStep[0m  [96/169], [94mLoss[0m : 2.60566
[1mStep[0m  [112/169], [94mLoss[0m : 2.16817
[1mStep[0m  [128/169], [94mLoss[0m : 2.42180
[1mStep[0m  [144/169], [94mLoss[0m : 2.54520
[1mStep[0m  [160/169], [94mLoss[0m : 2.12786

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38040
[1mStep[0m  [16/169], [94mLoss[0m : 2.25595
[1mStep[0m  [32/169], [94mLoss[0m : 2.38421
[1mStep[0m  [48/169], [94mLoss[0m : 2.36031
[1mStep[0m  [64/169], [94mLoss[0m : 2.77803
[1mStep[0m  [80/169], [94mLoss[0m : 2.32102
[1mStep[0m  [96/169], [94mLoss[0m : 2.14401
[1mStep[0m  [112/169], [94mLoss[0m : 2.24822
[1mStep[0m  [128/169], [94mLoss[0m : 2.10686
[1mStep[0m  [144/169], [94mLoss[0m : 2.27273
[1mStep[0m  [160/169], [94mLoss[0m : 2.28388

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55721
[1mStep[0m  [16/169], [94mLoss[0m : 2.67410
[1mStep[0m  [32/169], [94mLoss[0m : 2.56039
[1mStep[0m  [48/169], [94mLoss[0m : 3.03214
[1mStep[0m  [64/169], [94mLoss[0m : 2.72322
[1mStep[0m  [80/169], [94mLoss[0m : 2.64138
[1mStep[0m  [96/169], [94mLoss[0m : 2.27828
[1mStep[0m  [112/169], [94mLoss[0m : 2.43146
[1mStep[0m  [128/169], [94mLoss[0m : 2.51203
[1mStep[0m  [144/169], [94mLoss[0m : 2.38589
[1mStep[0m  [160/169], [94mLoss[0m : 2.88858

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48349
[1mStep[0m  [16/169], [94mLoss[0m : 2.63908
[1mStep[0m  [32/169], [94mLoss[0m : 2.51133
[1mStep[0m  [48/169], [94mLoss[0m : 2.61821
[1mStep[0m  [64/169], [94mLoss[0m : 2.35885
[1mStep[0m  [80/169], [94mLoss[0m : 2.54153
[1mStep[0m  [96/169], [94mLoss[0m : 2.85686
[1mStep[0m  [112/169], [94mLoss[0m : 2.37727
[1mStep[0m  [128/169], [94mLoss[0m : 2.40266
[1mStep[0m  [144/169], [94mLoss[0m : 2.31302
[1mStep[0m  [160/169], [94mLoss[0m : 2.54199

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81695
[1mStep[0m  [16/169], [94mLoss[0m : 2.30770
[1mStep[0m  [32/169], [94mLoss[0m : 2.80524
[1mStep[0m  [48/169], [94mLoss[0m : 2.70656
[1mStep[0m  [64/169], [94mLoss[0m : 2.57909
[1mStep[0m  [80/169], [94mLoss[0m : 2.38309
[1mStep[0m  [96/169], [94mLoss[0m : 2.71326
[1mStep[0m  [112/169], [94mLoss[0m : 2.64622
[1mStep[0m  [128/169], [94mLoss[0m : 2.31617
[1mStep[0m  [144/169], [94mLoss[0m : 2.47390
[1mStep[0m  [160/169], [94mLoss[0m : 2.31900

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.11795
[1mStep[0m  [16/169], [94mLoss[0m : 2.39704
[1mStep[0m  [32/169], [94mLoss[0m : 2.34090
[1mStep[0m  [48/169], [94mLoss[0m : 2.18567
[1mStep[0m  [64/169], [94mLoss[0m : 2.09741
[1mStep[0m  [80/169], [94mLoss[0m : 2.44684
[1mStep[0m  [96/169], [94mLoss[0m : 2.47386
[1mStep[0m  [112/169], [94mLoss[0m : 2.34064
[1mStep[0m  [128/169], [94mLoss[0m : 3.21060
[1mStep[0m  [144/169], [94mLoss[0m : 2.82568
[1mStep[0m  [160/169], [94mLoss[0m : 2.16512

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44182
[1mStep[0m  [16/169], [94mLoss[0m : 2.55094
[1mStep[0m  [32/169], [94mLoss[0m : 2.63073
[1mStep[0m  [48/169], [94mLoss[0m : 2.52664
[1mStep[0m  [64/169], [94mLoss[0m : 2.64044
[1mStep[0m  [80/169], [94mLoss[0m : 2.42232
[1mStep[0m  [96/169], [94mLoss[0m : 2.78924
[1mStep[0m  [112/169], [94mLoss[0m : 2.56291
[1mStep[0m  [128/169], [94mLoss[0m : 2.52447
[1mStep[0m  [144/169], [94mLoss[0m : 2.57515
[1mStep[0m  [160/169], [94mLoss[0m : 2.13529

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77872
[1mStep[0m  [16/169], [94mLoss[0m : 2.27581
[1mStep[0m  [32/169], [94mLoss[0m : 2.95156
[1mStep[0m  [48/169], [94mLoss[0m : 2.58482
[1mStep[0m  [64/169], [94mLoss[0m : 2.29343
[1mStep[0m  [80/169], [94mLoss[0m : 2.51060
[1mStep[0m  [96/169], [94mLoss[0m : 2.39915
[1mStep[0m  [112/169], [94mLoss[0m : 2.75798
[1mStep[0m  [128/169], [94mLoss[0m : 2.20431
[1mStep[0m  [144/169], [94mLoss[0m : 2.66726
[1mStep[0m  [160/169], [94mLoss[0m : 2.44303

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53172
[1mStep[0m  [16/169], [94mLoss[0m : 2.13378
[1mStep[0m  [32/169], [94mLoss[0m : 2.61526
[1mStep[0m  [48/169], [94mLoss[0m : 2.47390
[1mStep[0m  [64/169], [94mLoss[0m : 2.15469
[1mStep[0m  [80/169], [94mLoss[0m : 2.35107
[1mStep[0m  [96/169], [94mLoss[0m : 2.39122
[1mStep[0m  [112/169], [94mLoss[0m : 2.18512
[1mStep[0m  [128/169], [94mLoss[0m : 2.46341
[1mStep[0m  [144/169], [94mLoss[0m : 2.24876
[1mStep[0m  [160/169], [94mLoss[0m : 2.66730

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50329
[1mStep[0m  [16/169], [94mLoss[0m : 2.41344
[1mStep[0m  [32/169], [94mLoss[0m : 2.42725
[1mStep[0m  [48/169], [94mLoss[0m : 2.27080
[1mStep[0m  [64/169], [94mLoss[0m : 2.10529
[1mStep[0m  [80/169], [94mLoss[0m : 2.31106
[1mStep[0m  [96/169], [94mLoss[0m : 2.33117
[1mStep[0m  [112/169], [94mLoss[0m : 2.41053
[1mStep[0m  [128/169], [94mLoss[0m : 2.47930
[1mStep[0m  [144/169], [94mLoss[0m : 2.42096
[1mStep[0m  [160/169], [94mLoss[0m : 2.42728

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12398
[1mStep[0m  [16/169], [94mLoss[0m : 2.68523
[1mStep[0m  [32/169], [94mLoss[0m : 2.59982
[1mStep[0m  [48/169], [94mLoss[0m : 2.75175
[1mStep[0m  [64/169], [94mLoss[0m : 2.44555
[1mStep[0m  [80/169], [94mLoss[0m : 2.33976
[1mStep[0m  [96/169], [94mLoss[0m : 2.48334
[1mStep[0m  [112/169], [94mLoss[0m : 2.86615
[1mStep[0m  [128/169], [94mLoss[0m : 2.15396
[1mStep[0m  [144/169], [94mLoss[0m : 2.38596
[1mStep[0m  [160/169], [94mLoss[0m : 2.24162

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42745
[1mStep[0m  [16/169], [94mLoss[0m : 2.71725
[1mStep[0m  [32/169], [94mLoss[0m : 2.22852
[1mStep[0m  [48/169], [94mLoss[0m : 1.66112
[1mStep[0m  [64/169], [94mLoss[0m : 2.24772
[1mStep[0m  [80/169], [94mLoss[0m : 2.42919
[1mStep[0m  [96/169], [94mLoss[0m : 2.57755
[1mStep[0m  [112/169], [94mLoss[0m : 2.45756
[1mStep[0m  [128/169], [94mLoss[0m : 2.34178
[1mStep[0m  [144/169], [94mLoss[0m : 2.29573
[1mStep[0m  [160/169], [94mLoss[0m : 2.70010

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12109
[1mStep[0m  [16/169], [94mLoss[0m : 2.89310
[1mStep[0m  [32/169], [94mLoss[0m : 2.32669
[1mStep[0m  [48/169], [94mLoss[0m : 2.41669
[1mStep[0m  [64/169], [94mLoss[0m : 2.29074
[1mStep[0m  [80/169], [94mLoss[0m : 2.41778
[1mStep[0m  [96/169], [94mLoss[0m : 2.40926
[1mStep[0m  [112/169], [94mLoss[0m : 2.45403
[1mStep[0m  [128/169], [94mLoss[0m : 2.47266
[1mStep[0m  [144/169], [94mLoss[0m : 2.49367
[1mStep[0m  [160/169], [94mLoss[0m : 2.29756

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86395
[1mStep[0m  [16/169], [94mLoss[0m : 2.42285
[1mStep[0m  [32/169], [94mLoss[0m : 2.49364
[1mStep[0m  [48/169], [94mLoss[0m : 2.04535
[1mStep[0m  [64/169], [94mLoss[0m : 2.87203
[1mStep[0m  [80/169], [94mLoss[0m : 2.42074
[1mStep[0m  [96/169], [94mLoss[0m : 2.00707
[1mStep[0m  [112/169], [94mLoss[0m : 2.07193
[1mStep[0m  [128/169], [94mLoss[0m : 2.42886
[1mStep[0m  [144/169], [94mLoss[0m : 2.67550
[1mStep[0m  [160/169], [94mLoss[0m : 2.66147

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46728
[1mStep[0m  [16/169], [94mLoss[0m : 2.57441
[1mStep[0m  [32/169], [94mLoss[0m : 2.17885
[1mStep[0m  [48/169], [94mLoss[0m : 2.60446
[1mStep[0m  [64/169], [94mLoss[0m : 2.15559
[1mStep[0m  [80/169], [94mLoss[0m : 2.54646
[1mStep[0m  [96/169], [94mLoss[0m : 2.19744
[1mStep[0m  [112/169], [94mLoss[0m : 2.22871
[1mStep[0m  [128/169], [94mLoss[0m : 1.82988
[1mStep[0m  [144/169], [94mLoss[0m : 2.13150
[1mStep[0m  [160/169], [94mLoss[0m : 2.56925

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15557
[1mStep[0m  [16/169], [94mLoss[0m : 2.35350
[1mStep[0m  [32/169], [94mLoss[0m : 2.02010
[1mStep[0m  [48/169], [94mLoss[0m : 2.43009
[1mStep[0m  [64/169], [94mLoss[0m : 2.34252
[1mStep[0m  [80/169], [94mLoss[0m : 2.33952
[1mStep[0m  [96/169], [94mLoss[0m : 2.53466
[1mStep[0m  [112/169], [94mLoss[0m : 2.32572
[1mStep[0m  [128/169], [94mLoss[0m : 2.66965
[1mStep[0m  [144/169], [94mLoss[0m : 2.43185
[1mStep[0m  [160/169], [94mLoss[0m : 2.39292

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64412
[1mStep[0m  [16/169], [94mLoss[0m : 2.46050
[1mStep[0m  [32/169], [94mLoss[0m : 2.55000
[1mStep[0m  [48/169], [94mLoss[0m : 2.30839
[1mStep[0m  [64/169], [94mLoss[0m : 2.36705
[1mStep[0m  [80/169], [94mLoss[0m : 2.23427
[1mStep[0m  [96/169], [94mLoss[0m : 2.59669
[1mStep[0m  [112/169], [94mLoss[0m : 2.47697
[1mStep[0m  [128/169], [94mLoss[0m : 2.05688
[1mStep[0m  [144/169], [94mLoss[0m : 2.20868
[1mStep[0m  [160/169], [94mLoss[0m : 3.00983

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.99649
[1mStep[0m  [16/169], [94mLoss[0m : 2.41956
[1mStep[0m  [32/169], [94mLoss[0m : 2.13823
[1mStep[0m  [48/169], [94mLoss[0m : 2.63410
[1mStep[0m  [64/169], [94mLoss[0m : 2.65997
[1mStep[0m  [80/169], [94mLoss[0m : 2.29504
[1mStep[0m  [96/169], [94mLoss[0m : 2.59966
[1mStep[0m  [112/169], [94mLoss[0m : 2.41242
[1mStep[0m  [128/169], [94mLoss[0m : 2.24368
[1mStep[0m  [144/169], [94mLoss[0m : 2.43150
[1mStep[0m  [160/169], [94mLoss[0m : 2.60448

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40095
[1mStep[0m  [16/169], [94mLoss[0m : 2.37312
[1mStep[0m  [32/169], [94mLoss[0m : 2.30444
[1mStep[0m  [48/169], [94mLoss[0m : 2.44737
[1mStep[0m  [64/169], [94mLoss[0m : 2.24557
[1mStep[0m  [80/169], [94mLoss[0m : 2.61722
[1mStep[0m  [96/169], [94mLoss[0m : 2.47715
[1mStep[0m  [112/169], [94mLoss[0m : 2.27374
[1mStep[0m  [128/169], [94mLoss[0m : 2.36128
[1mStep[0m  [144/169], [94mLoss[0m : 3.01211
[1mStep[0m  [160/169], [94mLoss[0m : 2.42306

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.368, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.80396
[1mStep[0m  [16/169], [94mLoss[0m : 2.70531
[1mStep[0m  [32/169], [94mLoss[0m : 2.44131
[1mStep[0m  [48/169], [94mLoss[0m : 2.15293
[1mStep[0m  [64/169], [94mLoss[0m : 2.36243
[1mStep[0m  [80/169], [94mLoss[0m : 2.80813
[1mStep[0m  [96/169], [94mLoss[0m : 2.48366
[1mStep[0m  [112/169], [94mLoss[0m : 2.56210
[1mStep[0m  [128/169], [94mLoss[0m : 2.07888
[1mStep[0m  [144/169], [94mLoss[0m : 3.00557
[1mStep[0m  [160/169], [94mLoss[0m : 2.33721

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34952
[1mStep[0m  [16/169], [94mLoss[0m : 2.62103
[1mStep[0m  [32/169], [94mLoss[0m : 2.31021
[1mStep[0m  [48/169], [94mLoss[0m : 2.33804
[1mStep[0m  [64/169], [94mLoss[0m : 2.41757
[1mStep[0m  [80/169], [94mLoss[0m : 2.15014
[1mStep[0m  [96/169], [94mLoss[0m : 2.53000
[1mStep[0m  [112/169], [94mLoss[0m : 2.71308
[1mStep[0m  [128/169], [94mLoss[0m : 2.61332
[1mStep[0m  [144/169], [94mLoss[0m : 2.55186
[1mStep[0m  [160/169], [94mLoss[0m : 1.99468

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.413, [92mTest[0m: 2.380, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19307
[1mStep[0m  [16/169], [94mLoss[0m : 2.28737
[1mStep[0m  [32/169], [94mLoss[0m : 2.49424
[1mStep[0m  [48/169], [94mLoss[0m : 2.04905
[1mStep[0m  [64/169], [94mLoss[0m : 2.27697
[1mStep[0m  [80/169], [94mLoss[0m : 2.03513
[1mStep[0m  [96/169], [94mLoss[0m : 2.09444
[1mStep[0m  [112/169], [94mLoss[0m : 2.59180
[1mStep[0m  [128/169], [94mLoss[0m : 2.46127
[1mStep[0m  [144/169], [94mLoss[0m : 2.38251
[1mStep[0m  [160/169], [94mLoss[0m : 2.34161

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61273
[1mStep[0m  [16/169], [94mLoss[0m : 2.65113
[1mStep[0m  [32/169], [94mLoss[0m : 2.32396
[1mStep[0m  [48/169], [94mLoss[0m : 2.32135
[1mStep[0m  [64/169], [94mLoss[0m : 2.12501
[1mStep[0m  [80/169], [94mLoss[0m : 1.97088
[1mStep[0m  [96/169], [94mLoss[0m : 2.43963
[1mStep[0m  [112/169], [94mLoss[0m : 2.41703
[1mStep[0m  [128/169], [94mLoss[0m : 2.36818
[1mStep[0m  [144/169], [94mLoss[0m : 2.58688
[1mStep[0m  [160/169], [94mLoss[0m : 2.51460

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48085
[1mStep[0m  [16/169], [94mLoss[0m : 2.82820
[1mStep[0m  [32/169], [94mLoss[0m : 2.20920
[1mStep[0m  [48/169], [94mLoss[0m : 2.03377
[1mStep[0m  [64/169], [94mLoss[0m : 2.37659
[1mStep[0m  [80/169], [94mLoss[0m : 2.34397
[1mStep[0m  [96/169], [94mLoss[0m : 2.52197
[1mStep[0m  [112/169], [94mLoss[0m : 2.50558
[1mStep[0m  [128/169], [94mLoss[0m : 2.39295
[1mStep[0m  [144/169], [94mLoss[0m : 2.35078
[1mStep[0m  [160/169], [94mLoss[0m : 2.49806

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.379, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45396
[1mStep[0m  [16/169], [94mLoss[0m : 2.58285
[1mStep[0m  [32/169], [94mLoss[0m : 2.54984
[1mStep[0m  [48/169], [94mLoss[0m : 2.45586
[1mStep[0m  [64/169], [94mLoss[0m : 2.35699
[1mStep[0m  [80/169], [94mLoss[0m : 2.68051
[1mStep[0m  [96/169], [94mLoss[0m : 2.08306
[1mStep[0m  [112/169], [94mLoss[0m : 2.26905
[1mStep[0m  [128/169], [94mLoss[0m : 2.58743
[1mStep[0m  [144/169], [94mLoss[0m : 2.31528
[1mStep[0m  [160/169], [94mLoss[0m : 2.77545

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.72845
[1mStep[0m  [16/169], [94mLoss[0m : 2.56940
[1mStep[0m  [32/169], [94mLoss[0m : 2.12027
[1mStep[0m  [48/169], [94mLoss[0m : 2.28798
[1mStep[0m  [64/169], [94mLoss[0m : 1.91861
[1mStep[0m  [80/169], [94mLoss[0m : 2.34321
[1mStep[0m  [96/169], [94mLoss[0m : 3.17275
[1mStep[0m  [112/169], [94mLoss[0m : 2.31000
[1mStep[0m  [128/169], [94mLoss[0m : 2.37824
[1mStep[0m  [144/169], [94mLoss[0m : 2.73942
[1mStep[0m  [160/169], [94mLoss[0m : 2.49250

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.360, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02199
[1mStep[0m  [16/169], [94mLoss[0m : 2.42998
[1mStep[0m  [32/169], [94mLoss[0m : 2.29828
[1mStep[0m  [48/169], [94mLoss[0m : 2.44784
[1mStep[0m  [64/169], [94mLoss[0m : 2.24802
[1mStep[0m  [80/169], [94mLoss[0m : 2.29466
[1mStep[0m  [96/169], [94mLoss[0m : 2.26546
[1mStep[0m  [112/169], [94mLoss[0m : 2.63443
[1mStep[0m  [128/169], [94mLoss[0m : 2.05978
[1mStep[0m  [144/169], [94mLoss[0m : 2.84745
[1mStep[0m  [160/169], [94mLoss[0m : 2.28508

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35546
[1mStep[0m  [16/169], [94mLoss[0m : 2.01998
[1mStep[0m  [32/169], [94mLoss[0m : 2.04417
[1mStep[0m  [48/169], [94mLoss[0m : 1.94942
[1mStep[0m  [64/169], [94mLoss[0m : 1.90968
[1mStep[0m  [80/169], [94mLoss[0m : 2.37870
[1mStep[0m  [96/169], [94mLoss[0m : 2.70473
[1mStep[0m  [112/169], [94mLoss[0m : 2.56157
[1mStep[0m  [128/169], [94mLoss[0m : 2.46667
[1mStep[0m  [144/169], [94mLoss[0m : 2.55675
[1mStep[0m  [160/169], [94mLoss[0m : 2.59454

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.340
====================================

Phase 1 - Evaluation MAE:  2.3395166844129562
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.64059
[1mStep[0m  [16/169], [94mLoss[0m : 2.44619
[1mStep[0m  [32/169], [94mLoss[0m : 2.50640
[1mStep[0m  [48/169], [94mLoss[0m : 2.09321
[1mStep[0m  [64/169], [94mLoss[0m : 2.19175
[1mStep[0m  [80/169], [94mLoss[0m : 2.48532
[1mStep[0m  [96/169], [94mLoss[0m : 2.40623
[1mStep[0m  [112/169], [94mLoss[0m : 2.89789
[1mStep[0m  [128/169], [94mLoss[0m : 2.18775
[1mStep[0m  [144/169], [94mLoss[0m : 2.82857
[1mStep[0m  [160/169], [94mLoss[0m : 2.15944

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42654
[1mStep[0m  [16/169], [94mLoss[0m : 1.81692
[1mStep[0m  [32/169], [94mLoss[0m : 2.47345
[1mStep[0m  [48/169], [94mLoss[0m : 2.33496
[1mStep[0m  [64/169], [94mLoss[0m : 2.33310
[1mStep[0m  [80/169], [94mLoss[0m : 2.22483
[1mStep[0m  [96/169], [94mLoss[0m : 2.16357
[1mStep[0m  [112/169], [94mLoss[0m : 2.13756
[1mStep[0m  [128/169], [94mLoss[0m : 2.29216
[1mStep[0m  [144/169], [94mLoss[0m : 2.30269
[1mStep[0m  [160/169], [94mLoss[0m : 2.24016

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.347, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91210
[1mStep[0m  [16/169], [94mLoss[0m : 2.31400
[1mStep[0m  [32/169], [94mLoss[0m : 2.35300
[1mStep[0m  [48/169], [94mLoss[0m : 2.23262
[1mStep[0m  [64/169], [94mLoss[0m : 2.44555
[1mStep[0m  [80/169], [94mLoss[0m : 2.47003
[1mStep[0m  [96/169], [94mLoss[0m : 2.42177
[1mStep[0m  [112/169], [94mLoss[0m : 2.22453
[1mStep[0m  [128/169], [94mLoss[0m : 2.00673
[1mStep[0m  [144/169], [94mLoss[0m : 2.04975
[1mStep[0m  [160/169], [94mLoss[0m : 2.39170

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.265, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90246
[1mStep[0m  [16/169], [94mLoss[0m : 2.32355
[1mStep[0m  [32/169], [94mLoss[0m : 2.31593
[1mStep[0m  [48/169], [94mLoss[0m : 2.20774
[1mStep[0m  [64/169], [94mLoss[0m : 2.74469
[1mStep[0m  [80/169], [94mLoss[0m : 1.91664
[1mStep[0m  [96/169], [94mLoss[0m : 2.10854
[1mStep[0m  [112/169], [94mLoss[0m : 2.05873
[1mStep[0m  [128/169], [94mLoss[0m : 1.85820
[1mStep[0m  [144/169], [94mLoss[0m : 2.08709
[1mStep[0m  [160/169], [94mLoss[0m : 2.20210

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.202, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85572
[1mStep[0m  [16/169], [94mLoss[0m : 2.27246
[1mStep[0m  [32/169], [94mLoss[0m : 2.53245
[1mStep[0m  [48/169], [94mLoss[0m : 2.18510
[1mStep[0m  [64/169], [94mLoss[0m : 2.16630
[1mStep[0m  [80/169], [94mLoss[0m : 1.82940
[1mStep[0m  [96/169], [94mLoss[0m : 2.34970
[1mStep[0m  [112/169], [94mLoss[0m : 2.06491
[1mStep[0m  [128/169], [94mLoss[0m : 1.66961
[1mStep[0m  [144/169], [94mLoss[0m : 2.49690
[1mStep[0m  [160/169], [94mLoss[0m : 2.23506

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93200
[1mStep[0m  [16/169], [94mLoss[0m : 2.01968
[1mStep[0m  [32/169], [94mLoss[0m : 2.20920
[1mStep[0m  [48/169], [94mLoss[0m : 2.14661
[1mStep[0m  [64/169], [94mLoss[0m : 2.11384
[1mStep[0m  [80/169], [94mLoss[0m : 2.13652
[1mStep[0m  [96/169], [94mLoss[0m : 2.50373
[1mStep[0m  [112/169], [94mLoss[0m : 2.03936
[1mStep[0m  [128/169], [94mLoss[0m : 1.58967
[1mStep[0m  [144/169], [94mLoss[0m : 1.88124
[1mStep[0m  [160/169], [94mLoss[0m : 1.93860

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.083, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91462
[1mStep[0m  [16/169], [94mLoss[0m : 2.02822
[1mStep[0m  [32/169], [94mLoss[0m : 1.78986
[1mStep[0m  [48/169], [94mLoss[0m : 1.90504
[1mStep[0m  [64/169], [94mLoss[0m : 1.89249
[1mStep[0m  [80/169], [94mLoss[0m : 1.88144
[1mStep[0m  [96/169], [94mLoss[0m : 2.28025
[1mStep[0m  [112/169], [94mLoss[0m : 1.89236
[1mStep[0m  [128/169], [94mLoss[0m : 2.06987
[1mStep[0m  [144/169], [94mLoss[0m : 2.00846
[1mStep[0m  [160/169], [94mLoss[0m : 2.15027

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.040, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87478
[1mStep[0m  [16/169], [94mLoss[0m : 1.58885
[1mStep[0m  [32/169], [94mLoss[0m : 1.66309
[1mStep[0m  [48/169], [94mLoss[0m : 2.14964
[1mStep[0m  [64/169], [94mLoss[0m : 1.86453
[1mStep[0m  [80/169], [94mLoss[0m : 2.26873
[1mStep[0m  [96/169], [94mLoss[0m : 2.38626
[1mStep[0m  [112/169], [94mLoss[0m : 2.23691
[1mStep[0m  [128/169], [94mLoss[0m : 1.91609
[1mStep[0m  [144/169], [94mLoss[0m : 1.92954
[1mStep[0m  [160/169], [94mLoss[0m : 2.05180

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.980, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10764
[1mStep[0m  [16/169], [94mLoss[0m : 2.08171
[1mStep[0m  [32/169], [94mLoss[0m : 1.70117
[1mStep[0m  [48/169], [94mLoss[0m : 1.83628
[1mStep[0m  [64/169], [94mLoss[0m : 1.85890
[1mStep[0m  [80/169], [94mLoss[0m : 1.92871
[1mStep[0m  [96/169], [94mLoss[0m : 1.87495
[1mStep[0m  [112/169], [94mLoss[0m : 1.66936
[1mStep[0m  [128/169], [94mLoss[0m : 1.98449
[1mStep[0m  [144/169], [94mLoss[0m : 2.40643
[1mStep[0m  [160/169], [94mLoss[0m : 2.00250

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.961, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60205
[1mStep[0m  [16/169], [94mLoss[0m : 1.84183
[1mStep[0m  [32/169], [94mLoss[0m : 1.45506
[1mStep[0m  [48/169], [94mLoss[0m : 2.20012
[1mStep[0m  [64/169], [94mLoss[0m : 1.72296
[1mStep[0m  [80/169], [94mLoss[0m : 1.85220
[1mStep[0m  [96/169], [94mLoss[0m : 1.82233
[1mStep[0m  [112/169], [94mLoss[0m : 1.77021
[1mStep[0m  [128/169], [94mLoss[0m : 2.17869
[1mStep[0m  [144/169], [94mLoss[0m : 1.86230
[1mStep[0m  [160/169], [94mLoss[0m : 2.23959

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.65765
[1mStep[0m  [16/169], [94mLoss[0m : 1.92636
[1mStep[0m  [32/169], [94mLoss[0m : 1.86886
[1mStep[0m  [48/169], [94mLoss[0m : 2.28679
[1mStep[0m  [64/169], [94mLoss[0m : 1.78553
[1mStep[0m  [80/169], [94mLoss[0m : 1.49649
[1mStep[0m  [96/169], [94mLoss[0m : 1.87439
[1mStep[0m  [112/169], [94mLoss[0m : 2.27452
[1mStep[0m  [128/169], [94mLoss[0m : 1.90906
[1mStep[0m  [144/169], [94mLoss[0m : 1.86371
[1mStep[0m  [160/169], [94mLoss[0m : 1.98553

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.852, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95981
[1mStep[0m  [16/169], [94mLoss[0m : 1.62528
[1mStep[0m  [32/169], [94mLoss[0m : 1.86253
[1mStep[0m  [48/169], [94mLoss[0m : 1.70490
[1mStep[0m  [64/169], [94mLoss[0m : 1.94734
[1mStep[0m  [80/169], [94mLoss[0m : 1.56478
[1mStep[0m  [96/169], [94mLoss[0m : 1.44231
[1mStep[0m  [112/169], [94mLoss[0m : 2.66522
[1mStep[0m  [128/169], [94mLoss[0m : 1.90861
[1mStep[0m  [144/169], [94mLoss[0m : 2.12772
[1mStep[0m  [160/169], [94mLoss[0m : 2.05299

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.833, [92mTest[0m: 2.519, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87873
[1mStep[0m  [16/169], [94mLoss[0m : 1.62249
[1mStep[0m  [32/169], [94mLoss[0m : 1.83623
[1mStep[0m  [48/169], [94mLoss[0m : 1.79574
[1mStep[0m  [64/169], [94mLoss[0m : 1.44687
[1mStep[0m  [80/169], [94mLoss[0m : 1.76387
[1mStep[0m  [96/169], [94mLoss[0m : 1.98794
[1mStep[0m  [112/169], [94mLoss[0m : 2.25980
[1mStep[0m  [128/169], [94mLoss[0m : 2.19958
[1mStep[0m  [144/169], [94mLoss[0m : 1.63113
[1mStep[0m  [160/169], [94mLoss[0m : 1.75003

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.801, [92mTest[0m: 2.457, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74434
[1mStep[0m  [16/169], [94mLoss[0m : 2.14297
[1mStep[0m  [32/169], [94mLoss[0m : 1.62370
[1mStep[0m  [48/169], [94mLoss[0m : 1.66712
[1mStep[0m  [64/169], [94mLoss[0m : 1.58664
[1mStep[0m  [80/169], [94mLoss[0m : 1.56576
[1mStep[0m  [96/169], [94mLoss[0m : 2.06882
[1mStep[0m  [112/169], [94mLoss[0m : 1.56949
[1mStep[0m  [128/169], [94mLoss[0m : 1.54676
[1mStep[0m  [144/169], [94mLoss[0m : 1.82856
[1mStep[0m  [160/169], [94mLoss[0m : 1.52245

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.759, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72630
[1mStep[0m  [16/169], [94mLoss[0m : 1.45633
[1mStep[0m  [32/169], [94mLoss[0m : 1.64339
[1mStep[0m  [48/169], [94mLoss[0m : 1.54023
[1mStep[0m  [64/169], [94mLoss[0m : 1.72243
[1mStep[0m  [80/169], [94mLoss[0m : 1.52085
[1mStep[0m  [96/169], [94mLoss[0m : 1.51504
[1mStep[0m  [112/169], [94mLoss[0m : 1.92572
[1mStep[0m  [128/169], [94mLoss[0m : 1.87353
[1mStep[0m  [144/169], [94mLoss[0m : 1.80507
[1mStep[0m  [160/169], [94mLoss[0m : 1.79101

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.742, [92mTest[0m: 2.501, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.81890
[1mStep[0m  [16/169], [94mLoss[0m : 1.53572
[1mStep[0m  [32/169], [94mLoss[0m : 1.90838
[1mStep[0m  [48/169], [94mLoss[0m : 1.76974
[1mStep[0m  [64/169], [94mLoss[0m : 1.56955
[1mStep[0m  [80/169], [94mLoss[0m : 1.46492
[1mStep[0m  [96/169], [94mLoss[0m : 1.92392
[1mStep[0m  [112/169], [94mLoss[0m : 1.82641
[1mStep[0m  [128/169], [94mLoss[0m : 1.74504
[1mStep[0m  [144/169], [94mLoss[0m : 1.96359
[1mStep[0m  [160/169], [94mLoss[0m : 1.73089

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.719, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86556
[1mStep[0m  [16/169], [94mLoss[0m : 1.57645
[1mStep[0m  [32/169], [94mLoss[0m : 2.07669
[1mStep[0m  [48/169], [94mLoss[0m : 1.50692
[1mStep[0m  [64/169], [94mLoss[0m : 1.52652
[1mStep[0m  [80/169], [94mLoss[0m : 1.45082
[1mStep[0m  [96/169], [94mLoss[0m : 1.74338
[1mStep[0m  [112/169], [94mLoss[0m : 1.45983
[1mStep[0m  [128/169], [94mLoss[0m : 1.72021
[1mStep[0m  [144/169], [94mLoss[0m : 1.67163
[1mStep[0m  [160/169], [94mLoss[0m : 1.32323

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.529, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71506
[1mStep[0m  [16/169], [94mLoss[0m : 1.63882
[1mStep[0m  [32/169], [94mLoss[0m : 2.01741
[1mStep[0m  [48/169], [94mLoss[0m : 1.91368
[1mStep[0m  [64/169], [94mLoss[0m : 1.42354
[1mStep[0m  [80/169], [94mLoss[0m : 1.63801
[1mStep[0m  [96/169], [94mLoss[0m : 1.50102
[1mStep[0m  [112/169], [94mLoss[0m : 1.81408
[1mStep[0m  [128/169], [94mLoss[0m : 1.34056
[1mStep[0m  [144/169], [94mLoss[0m : 1.38954
[1mStep[0m  [160/169], [94mLoss[0m : 1.71014

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.640, [92mTest[0m: 2.540, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57980
[1mStep[0m  [16/169], [94mLoss[0m : 1.43406
[1mStep[0m  [32/169], [94mLoss[0m : 1.71121
[1mStep[0m  [48/169], [94mLoss[0m : 1.48822
[1mStep[0m  [64/169], [94mLoss[0m : 1.47294
[1mStep[0m  [80/169], [94mLoss[0m : 1.45310
[1mStep[0m  [96/169], [94mLoss[0m : 1.60394
[1mStep[0m  [112/169], [94mLoss[0m : 1.60053
[1mStep[0m  [128/169], [94mLoss[0m : 1.53675
[1mStep[0m  [144/169], [94mLoss[0m : 1.75793
[1mStep[0m  [160/169], [94mLoss[0m : 1.26669

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.616, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.16898
[1mStep[0m  [16/169], [94mLoss[0m : 1.60038
[1mStep[0m  [32/169], [94mLoss[0m : 1.29141
[1mStep[0m  [48/169], [94mLoss[0m : 1.85244
[1mStep[0m  [64/169], [94mLoss[0m : 1.54597
[1mStep[0m  [80/169], [94mLoss[0m : 1.43317
[1mStep[0m  [96/169], [94mLoss[0m : 1.37250
[1mStep[0m  [112/169], [94mLoss[0m : 1.44515
[1mStep[0m  [128/169], [94mLoss[0m : 2.09467
[1mStep[0m  [144/169], [94mLoss[0m : 1.57496
[1mStep[0m  [160/169], [94mLoss[0m : 1.43984

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.593, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38735
[1mStep[0m  [16/169], [94mLoss[0m : 1.45904
[1mStep[0m  [32/169], [94mLoss[0m : 1.42350
[1mStep[0m  [48/169], [94mLoss[0m : 1.49381
[1mStep[0m  [64/169], [94mLoss[0m : 1.26193
[1mStep[0m  [80/169], [94mLoss[0m : 1.83627
[1mStep[0m  [96/169], [94mLoss[0m : 1.54014
[1mStep[0m  [112/169], [94mLoss[0m : 1.62613
[1mStep[0m  [128/169], [94mLoss[0m : 1.66218
[1mStep[0m  [144/169], [94mLoss[0m : 1.60086
[1mStep[0m  [160/169], [94mLoss[0m : 1.42786

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.538, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51463
[1mStep[0m  [16/169], [94mLoss[0m : 1.42154
[1mStep[0m  [32/169], [94mLoss[0m : 1.25134
[1mStep[0m  [48/169], [94mLoss[0m : 1.31196
[1mStep[0m  [64/169], [94mLoss[0m : 1.34316
[1mStep[0m  [80/169], [94mLoss[0m : 1.50253
[1mStep[0m  [96/169], [94mLoss[0m : 1.54730
[1mStep[0m  [112/169], [94mLoss[0m : 1.59863
[1mStep[0m  [128/169], [94mLoss[0m : 1.49711
[1mStep[0m  [144/169], [94mLoss[0m : 1.48421
[1mStep[0m  [160/169], [94mLoss[0m : 1.66951

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.497, [92mTest[0m: 2.532, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46314
[1mStep[0m  [16/169], [94mLoss[0m : 1.55455
[1mStep[0m  [32/169], [94mLoss[0m : 1.65951
[1mStep[0m  [48/169], [94mLoss[0m : 1.80882
[1mStep[0m  [64/169], [94mLoss[0m : 1.44669
[1mStep[0m  [80/169], [94mLoss[0m : 1.59432
[1mStep[0m  [96/169], [94mLoss[0m : 1.27634
[1mStep[0m  [112/169], [94mLoss[0m : 1.54663
[1mStep[0m  [128/169], [94mLoss[0m : 1.51856
[1mStep[0m  [144/169], [94mLoss[0m : 1.45513
[1mStep[0m  [160/169], [94mLoss[0m : 1.57271

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.465, [92mTest[0m: 2.575, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41416
[1mStep[0m  [16/169], [94mLoss[0m : 1.07650
[1mStep[0m  [32/169], [94mLoss[0m : 1.85495
[1mStep[0m  [48/169], [94mLoss[0m : 1.61545
[1mStep[0m  [64/169], [94mLoss[0m : 1.36329
[1mStep[0m  [80/169], [94mLoss[0m : 1.29080
[1mStep[0m  [96/169], [94mLoss[0m : 1.44325
[1mStep[0m  [112/169], [94mLoss[0m : 1.75146
[1mStep[0m  [128/169], [94mLoss[0m : 1.50823
[1mStep[0m  [144/169], [94mLoss[0m : 1.58870
[1mStep[0m  [160/169], [94mLoss[0m : 1.50745

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41588
[1mStep[0m  [16/169], [94mLoss[0m : 1.52240
[1mStep[0m  [32/169], [94mLoss[0m : 1.60960
[1mStep[0m  [48/169], [94mLoss[0m : 1.37618
[1mStep[0m  [64/169], [94mLoss[0m : 1.15712
[1mStep[0m  [80/169], [94mLoss[0m : 1.12610
[1mStep[0m  [96/169], [94mLoss[0m : 1.27834
[1mStep[0m  [112/169], [94mLoss[0m : 1.35502
[1mStep[0m  [128/169], [94mLoss[0m : 1.30226
[1mStep[0m  [144/169], [94mLoss[0m : 1.56861
[1mStep[0m  [160/169], [94mLoss[0m : 1.31940

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.436, [92mTest[0m: 2.557, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36426
[1mStep[0m  [16/169], [94mLoss[0m : 1.34645
[1mStep[0m  [32/169], [94mLoss[0m : 1.32117
[1mStep[0m  [48/169], [94mLoss[0m : 1.49143
[1mStep[0m  [64/169], [94mLoss[0m : 1.26545
[1mStep[0m  [80/169], [94mLoss[0m : 1.29324
[1mStep[0m  [96/169], [94mLoss[0m : 1.61977
[1mStep[0m  [112/169], [94mLoss[0m : 1.38485
[1mStep[0m  [128/169], [94mLoss[0m : 1.31892
[1mStep[0m  [144/169], [94mLoss[0m : 1.50071
[1mStep[0m  [160/169], [94mLoss[0m : 1.12807

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.407, [92mTest[0m: 2.512, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.503
====================================

Phase 2 - Evaluation MAE:  2.5030731856822968
MAE score P1      2.339517
MAE score P2      2.503073
loss              1.407264
learning_rate     0.007525
batch_size              64
hidden_sizes         [250]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.9
weight_decay         0.001
Name: 1, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.74979
[1mStep[0m  [16/169], [94mLoss[0m : 3.31256
[1mStep[0m  [32/169], [94mLoss[0m : 2.92153
[1mStep[0m  [48/169], [94mLoss[0m : 3.18164
[1mStep[0m  [64/169], [94mLoss[0m : 2.65288
[1mStep[0m  [80/169], [94mLoss[0m : 2.17432
[1mStep[0m  [96/169], [94mLoss[0m : 2.29057
[1mStep[0m  [112/169], [94mLoss[0m : 2.52349
[1mStep[0m  [128/169], [94mLoss[0m : 2.84639
[1mStep[0m  [144/169], [94mLoss[0m : 2.52299
[1mStep[0m  [160/169], [94mLoss[0m : 2.28703

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.034, [92mTest[0m: 11.177, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35151
[1mStep[0m  [16/169], [94mLoss[0m : 2.64787
[1mStep[0m  [32/169], [94mLoss[0m : 2.51580
[1mStep[0m  [48/169], [94mLoss[0m : 2.89138
[1mStep[0m  [64/169], [94mLoss[0m : 2.22395
[1mStep[0m  [80/169], [94mLoss[0m : 2.37433
[1mStep[0m  [96/169], [94mLoss[0m : 3.05215
[1mStep[0m  [112/169], [94mLoss[0m : 2.06902
[1mStep[0m  [128/169], [94mLoss[0m : 2.10675
[1mStep[0m  [144/169], [94mLoss[0m : 2.62055
[1mStep[0m  [160/169], [94mLoss[0m : 2.40887

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51805
[1mStep[0m  [16/169], [94mLoss[0m : 2.24652
[1mStep[0m  [32/169], [94mLoss[0m : 2.50628
[1mStep[0m  [48/169], [94mLoss[0m : 2.30127
[1mStep[0m  [64/169], [94mLoss[0m : 2.60300
[1mStep[0m  [80/169], [94mLoss[0m : 2.11821
[1mStep[0m  [96/169], [94mLoss[0m : 2.55970
[1mStep[0m  [112/169], [94mLoss[0m : 2.62833
[1mStep[0m  [128/169], [94mLoss[0m : 2.44733
[1mStep[0m  [144/169], [94mLoss[0m : 2.41604
[1mStep[0m  [160/169], [94mLoss[0m : 2.08047

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68834
[1mStep[0m  [16/169], [94mLoss[0m : 2.72831
[1mStep[0m  [32/169], [94mLoss[0m : 2.55500
[1mStep[0m  [48/169], [94mLoss[0m : 2.16147
[1mStep[0m  [64/169], [94mLoss[0m : 2.60411
[1mStep[0m  [80/169], [94mLoss[0m : 2.49319
[1mStep[0m  [96/169], [94mLoss[0m : 2.75168
[1mStep[0m  [112/169], [94mLoss[0m : 2.39857
[1mStep[0m  [128/169], [94mLoss[0m : 2.21458
[1mStep[0m  [144/169], [94mLoss[0m : 2.34177
[1mStep[0m  [160/169], [94mLoss[0m : 2.63209

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.442, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10954
[1mStep[0m  [16/169], [94mLoss[0m : 2.47865
[1mStep[0m  [32/169], [94mLoss[0m : 1.88809
[1mStep[0m  [48/169], [94mLoss[0m : 2.13759
[1mStep[0m  [64/169], [94mLoss[0m : 2.37648
[1mStep[0m  [80/169], [94mLoss[0m : 2.26299
[1mStep[0m  [96/169], [94mLoss[0m : 2.63859
[1mStep[0m  [112/169], [94mLoss[0m : 2.46972
[1mStep[0m  [128/169], [94mLoss[0m : 2.14853
[1mStep[0m  [144/169], [94mLoss[0m : 2.26832
[1mStep[0m  [160/169], [94mLoss[0m : 2.39361

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52821
[1mStep[0m  [16/169], [94mLoss[0m : 2.22598
[1mStep[0m  [32/169], [94mLoss[0m : 2.64486
[1mStep[0m  [48/169], [94mLoss[0m : 2.91443
[1mStep[0m  [64/169], [94mLoss[0m : 2.11144
[1mStep[0m  [80/169], [94mLoss[0m : 2.19956
[1mStep[0m  [96/169], [94mLoss[0m : 2.52467
[1mStep[0m  [112/169], [94mLoss[0m : 2.12679
[1mStep[0m  [128/169], [94mLoss[0m : 2.52032
[1mStep[0m  [144/169], [94mLoss[0m : 1.92918
[1mStep[0m  [160/169], [94mLoss[0m : 2.37353

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29318
[1mStep[0m  [16/169], [94mLoss[0m : 2.40200
[1mStep[0m  [32/169], [94mLoss[0m : 2.33162
[1mStep[0m  [48/169], [94mLoss[0m : 2.13933
[1mStep[0m  [64/169], [94mLoss[0m : 2.13670
[1mStep[0m  [80/169], [94mLoss[0m : 2.35188
[1mStep[0m  [96/169], [94mLoss[0m : 2.66722
[1mStep[0m  [112/169], [94mLoss[0m : 2.46250
[1mStep[0m  [128/169], [94mLoss[0m : 2.77835
[1mStep[0m  [144/169], [94mLoss[0m : 2.31860
[1mStep[0m  [160/169], [94mLoss[0m : 2.57348

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36561
[1mStep[0m  [16/169], [94mLoss[0m : 2.40078
[1mStep[0m  [32/169], [94mLoss[0m : 2.19479
[1mStep[0m  [48/169], [94mLoss[0m : 2.19722
[1mStep[0m  [64/169], [94mLoss[0m : 2.17294
[1mStep[0m  [80/169], [94mLoss[0m : 2.10106
[1mStep[0m  [96/169], [94mLoss[0m : 2.72593
[1mStep[0m  [112/169], [94mLoss[0m : 2.70820
[1mStep[0m  [128/169], [94mLoss[0m : 2.37163
[1mStep[0m  [144/169], [94mLoss[0m : 2.41139
[1mStep[0m  [160/169], [94mLoss[0m : 2.62735

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19647
[1mStep[0m  [16/169], [94mLoss[0m : 2.26236
[1mStep[0m  [32/169], [94mLoss[0m : 2.71447
[1mStep[0m  [48/169], [94mLoss[0m : 2.67759
[1mStep[0m  [64/169], [94mLoss[0m : 2.60578
[1mStep[0m  [80/169], [94mLoss[0m : 2.25005
[1mStep[0m  [96/169], [94mLoss[0m : 2.57881
[1mStep[0m  [112/169], [94mLoss[0m : 1.95269
[1mStep[0m  [128/169], [94mLoss[0m : 2.47487
[1mStep[0m  [144/169], [94mLoss[0m : 2.29781
[1mStep[0m  [160/169], [94mLoss[0m : 2.59012

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70671
[1mStep[0m  [16/169], [94mLoss[0m : 2.62217
[1mStep[0m  [32/169], [94mLoss[0m : 2.32193
[1mStep[0m  [48/169], [94mLoss[0m : 2.44824
[1mStep[0m  [64/169], [94mLoss[0m : 2.84422
[1mStep[0m  [80/169], [94mLoss[0m : 2.25217
[1mStep[0m  [96/169], [94mLoss[0m : 2.66431
[1mStep[0m  [112/169], [94mLoss[0m : 2.18739
[1mStep[0m  [128/169], [94mLoss[0m : 2.28568
[1mStep[0m  [144/169], [94mLoss[0m : 2.24886
[1mStep[0m  [160/169], [94mLoss[0m : 2.34725

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.409, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04542
[1mStep[0m  [16/169], [94mLoss[0m : 2.70416
[1mStep[0m  [32/169], [94mLoss[0m : 2.81695
[1mStep[0m  [48/169], [94mLoss[0m : 2.63056
[1mStep[0m  [64/169], [94mLoss[0m : 2.60529
[1mStep[0m  [80/169], [94mLoss[0m : 2.44266
[1mStep[0m  [96/169], [94mLoss[0m : 2.23264
[1mStep[0m  [112/169], [94mLoss[0m : 1.98078
[1mStep[0m  [128/169], [94mLoss[0m : 2.53846
[1mStep[0m  [144/169], [94mLoss[0m : 2.53702
[1mStep[0m  [160/169], [94mLoss[0m : 2.49361

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52901
[1mStep[0m  [16/169], [94mLoss[0m : 2.43350
[1mStep[0m  [32/169], [94mLoss[0m : 2.47555
[1mStep[0m  [48/169], [94mLoss[0m : 2.26175
[1mStep[0m  [64/169], [94mLoss[0m : 2.50716
[1mStep[0m  [80/169], [94mLoss[0m : 2.36231
[1mStep[0m  [96/169], [94mLoss[0m : 2.20083
[1mStep[0m  [112/169], [94mLoss[0m : 2.30227
[1mStep[0m  [128/169], [94mLoss[0m : 2.38560
[1mStep[0m  [144/169], [94mLoss[0m : 2.03738
[1mStep[0m  [160/169], [94mLoss[0m : 2.27111

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.94882
[1mStep[0m  [16/169], [94mLoss[0m : 2.21895
[1mStep[0m  [32/169], [94mLoss[0m : 2.43949
[1mStep[0m  [48/169], [94mLoss[0m : 2.10387
[1mStep[0m  [64/169], [94mLoss[0m : 2.56528
[1mStep[0m  [80/169], [94mLoss[0m : 2.20196
[1mStep[0m  [96/169], [94mLoss[0m : 2.42243
[1mStep[0m  [112/169], [94mLoss[0m : 2.23092
[1mStep[0m  [128/169], [94mLoss[0m : 2.52197
[1mStep[0m  [144/169], [94mLoss[0m : 2.33473
[1mStep[0m  [160/169], [94mLoss[0m : 1.75365

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.408, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.95105
[1mStep[0m  [16/169], [94mLoss[0m : 2.71533
[1mStep[0m  [32/169], [94mLoss[0m : 2.66896
[1mStep[0m  [48/169], [94mLoss[0m : 2.71948
[1mStep[0m  [64/169], [94mLoss[0m : 2.23244
[1mStep[0m  [80/169], [94mLoss[0m : 2.31794
[1mStep[0m  [96/169], [94mLoss[0m : 2.81579
[1mStep[0m  [112/169], [94mLoss[0m : 2.38106
[1mStep[0m  [128/169], [94mLoss[0m : 2.64917
[1mStep[0m  [144/169], [94mLoss[0m : 2.59517
[1mStep[0m  [160/169], [94mLoss[0m : 2.57561

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10143
[1mStep[0m  [16/169], [94mLoss[0m : 2.34317
[1mStep[0m  [32/169], [94mLoss[0m : 2.42433
[1mStep[0m  [48/169], [94mLoss[0m : 2.39181
[1mStep[0m  [64/169], [94mLoss[0m : 2.28935
[1mStep[0m  [80/169], [94mLoss[0m : 2.53231
[1mStep[0m  [96/169], [94mLoss[0m : 2.80829
[1mStep[0m  [112/169], [94mLoss[0m : 2.50842
[1mStep[0m  [128/169], [94mLoss[0m : 2.23561
[1mStep[0m  [144/169], [94mLoss[0m : 2.28828
[1mStep[0m  [160/169], [94mLoss[0m : 2.26967

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22669
[1mStep[0m  [16/169], [94mLoss[0m : 2.26272
[1mStep[0m  [32/169], [94mLoss[0m : 2.52126
[1mStep[0m  [48/169], [94mLoss[0m : 2.02567
[1mStep[0m  [64/169], [94mLoss[0m : 2.17444
[1mStep[0m  [80/169], [94mLoss[0m : 2.21792
[1mStep[0m  [96/169], [94mLoss[0m : 2.24041
[1mStep[0m  [112/169], [94mLoss[0m : 2.50180
[1mStep[0m  [128/169], [94mLoss[0m : 2.19734
[1mStep[0m  [144/169], [94mLoss[0m : 2.15691
[1mStep[0m  [160/169], [94mLoss[0m : 2.89245

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13783
[1mStep[0m  [16/169], [94mLoss[0m : 2.06245
[1mStep[0m  [32/169], [94mLoss[0m : 2.44692
[1mStep[0m  [48/169], [94mLoss[0m : 2.43118
[1mStep[0m  [64/169], [94mLoss[0m : 3.02025
[1mStep[0m  [80/169], [94mLoss[0m : 2.31131
[1mStep[0m  [96/169], [94mLoss[0m : 2.63726
[1mStep[0m  [112/169], [94mLoss[0m : 2.33512
[1mStep[0m  [128/169], [94mLoss[0m : 2.51080
[1mStep[0m  [144/169], [94mLoss[0m : 2.52484
[1mStep[0m  [160/169], [94mLoss[0m : 2.55802

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29571
[1mStep[0m  [16/169], [94mLoss[0m : 2.73050
[1mStep[0m  [32/169], [94mLoss[0m : 2.03275
[1mStep[0m  [48/169], [94mLoss[0m : 2.15592
[1mStep[0m  [64/169], [94mLoss[0m : 2.27443
[1mStep[0m  [80/169], [94mLoss[0m : 1.97306
[1mStep[0m  [96/169], [94mLoss[0m : 2.27443
[1mStep[0m  [112/169], [94mLoss[0m : 2.17541
[1mStep[0m  [128/169], [94mLoss[0m : 2.20204
[1mStep[0m  [144/169], [94mLoss[0m : 2.64355
[1mStep[0m  [160/169], [94mLoss[0m : 2.34160

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.415, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15646
[1mStep[0m  [16/169], [94mLoss[0m : 2.27787
[1mStep[0m  [32/169], [94mLoss[0m : 2.40020
[1mStep[0m  [48/169], [94mLoss[0m : 2.45407
[1mStep[0m  [64/169], [94mLoss[0m : 2.14496
[1mStep[0m  [80/169], [94mLoss[0m : 2.37791
[1mStep[0m  [96/169], [94mLoss[0m : 2.39380
[1mStep[0m  [112/169], [94mLoss[0m : 2.41565
[1mStep[0m  [128/169], [94mLoss[0m : 2.31223
[1mStep[0m  [144/169], [94mLoss[0m : 2.81481
[1mStep[0m  [160/169], [94mLoss[0m : 2.42413

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.412, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41252
[1mStep[0m  [16/169], [94mLoss[0m : 2.45053
[1mStep[0m  [32/169], [94mLoss[0m : 2.17903
[1mStep[0m  [48/169], [94mLoss[0m : 2.49249
[1mStep[0m  [64/169], [94mLoss[0m : 3.07658
[1mStep[0m  [80/169], [94mLoss[0m : 2.41419
[1mStep[0m  [96/169], [94mLoss[0m : 2.17890
[1mStep[0m  [112/169], [94mLoss[0m : 2.64128
[1mStep[0m  [128/169], [94mLoss[0m : 2.30106
[1mStep[0m  [144/169], [94mLoss[0m : 2.56881
[1mStep[0m  [160/169], [94mLoss[0m : 2.55852

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35244
[1mStep[0m  [16/169], [94mLoss[0m : 2.31013
[1mStep[0m  [32/169], [94mLoss[0m : 2.18140
[1mStep[0m  [48/169], [94mLoss[0m : 2.92459
[1mStep[0m  [64/169], [94mLoss[0m : 2.59278
[1mStep[0m  [80/169], [94mLoss[0m : 2.64897
[1mStep[0m  [96/169], [94mLoss[0m : 2.29029
[1mStep[0m  [112/169], [94mLoss[0m : 1.99880
[1mStep[0m  [128/169], [94mLoss[0m : 2.30013
[1mStep[0m  [144/169], [94mLoss[0m : 2.68796
[1mStep[0m  [160/169], [94mLoss[0m : 2.67375

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.405, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35095
[1mStep[0m  [16/169], [94mLoss[0m : 2.73889
[1mStep[0m  [32/169], [94mLoss[0m : 2.22404
[1mStep[0m  [48/169], [94mLoss[0m : 2.16561
[1mStep[0m  [64/169], [94mLoss[0m : 2.47849
[1mStep[0m  [80/169], [94mLoss[0m : 2.32711
[1mStep[0m  [96/169], [94mLoss[0m : 2.52592
[1mStep[0m  [112/169], [94mLoss[0m : 2.20923
[1mStep[0m  [128/169], [94mLoss[0m : 2.39124
[1mStep[0m  [144/169], [94mLoss[0m : 2.34439
[1mStep[0m  [160/169], [94mLoss[0m : 2.27101

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21428
[1mStep[0m  [16/169], [94mLoss[0m : 2.48756
[1mStep[0m  [32/169], [94mLoss[0m : 2.10779
[1mStep[0m  [48/169], [94mLoss[0m : 2.49980
[1mStep[0m  [64/169], [94mLoss[0m : 2.63192
[1mStep[0m  [80/169], [94mLoss[0m : 2.69733
[1mStep[0m  [96/169], [94mLoss[0m : 2.39807
[1mStep[0m  [112/169], [94mLoss[0m : 2.17775
[1mStep[0m  [128/169], [94mLoss[0m : 2.47234
[1mStep[0m  [144/169], [94mLoss[0m : 2.23681
[1mStep[0m  [160/169], [94mLoss[0m : 2.38974

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18135
[1mStep[0m  [16/169], [94mLoss[0m : 2.56371
[1mStep[0m  [32/169], [94mLoss[0m : 2.12517
[1mStep[0m  [48/169], [94mLoss[0m : 2.50295
[1mStep[0m  [64/169], [94mLoss[0m : 2.42383
[1mStep[0m  [80/169], [94mLoss[0m : 2.39235
[1mStep[0m  [96/169], [94mLoss[0m : 2.22067
[1mStep[0m  [112/169], [94mLoss[0m : 2.09306
[1mStep[0m  [128/169], [94mLoss[0m : 2.30483
[1mStep[0m  [144/169], [94mLoss[0m : 2.50865
[1mStep[0m  [160/169], [94mLoss[0m : 2.34037

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15420
[1mStep[0m  [16/169], [94mLoss[0m : 2.93525
[1mStep[0m  [32/169], [94mLoss[0m : 2.14770
[1mStep[0m  [48/169], [94mLoss[0m : 2.60589
[1mStep[0m  [64/169], [94mLoss[0m : 2.53744
[1mStep[0m  [80/169], [94mLoss[0m : 2.49341
[1mStep[0m  [96/169], [94mLoss[0m : 2.44419
[1mStep[0m  [112/169], [94mLoss[0m : 2.47949
[1mStep[0m  [128/169], [94mLoss[0m : 2.44781
[1mStep[0m  [144/169], [94mLoss[0m : 2.67964
[1mStep[0m  [160/169], [94mLoss[0m : 2.14536

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47967
[1mStep[0m  [16/169], [94mLoss[0m : 1.97812
[1mStep[0m  [32/169], [94mLoss[0m : 2.42066
[1mStep[0m  [48/169], [94mLoss[0m : 2.04065
[1mStep[0m  [64/169], [94mLoss[0m : 2.47183
[1mStep[0m  [80/169], [94mLoss[0m : 2.07384
[1mStep[0m  [96/169], [94mLoss[0m : 2.43892
[1mStep[0m  [112/169], [94mLoss[0m : 2.19390
[1mStep[0m  [128/169], [94mLoss[0m : 2.41537
[1mStep[0m  [144/169], [94mLoss[0m : 2.22004
[1mStep[0m  [160/169], [94mLoss[0m : 2.54510

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.403, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87565
[1mStep[0m  [16/169], [94mLoss[0m : 2.75317
[1mStep[0m  [32/169], [94mLoss[0m : 2.97335
[1mStep[0m  [48/169], [94mLoss[0m : 2.40123
[1mStep[0m  [64/169], [94mLoss[0m : 2.36499
[1mStep[0m  [80/169], [94mLoss[0m : 2.37306
[1mStep[0m  [96/169], [94mLoss[0m : 2.65517
[1mStep[0m  [112/169], [94mLoss[0m : 3.12156
[1mStep[0m  [128/169], [94mLoss[0m : 2.13170
[1mStep[0m  [144/169], [94mLoss[0m : 2.49167
[1mStep[0m  [160/169], [94mLoss[0m : 2.22150

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41770
[1mStep[0m  [16/169], [94mLoss[0m : 2.48713
[1mStep[0m  [32/169], [94mLoss[0m : 2.40609
[1mStep[0m  [48/169], [94mLoss[0m : 2.16158
[1mStep[0m  [64/169], [94mLoss[0m : 2.46250
[1mStep[0m  [80/169], [94mLoss[0m : 2.96646
[1mStep[0m  [96/169], [94mLoss[0m : 2.35787
[1mStep[0m  [112/169], [94mLoss[0m : 2.42993
[1mStep[0m  [128/169], [94mLoss[0m : 2.33096
[1mStep[0m  [144/169], [94mLoss[0m : 2.42087
[1mStep[0m  [160/169], [94mLoss[0m : 2.37513

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47264
[1mStep[0m  [16/169], [94mLoss[0m : 2.53735
[1mStep[0m  [32/169], [94mLoss[0m : 2.28510
[1mStep[0m  [48/169], [94mLoss[0m : 2.29593
[1mStep[0m  [64/169], [94mLoss[0m : 2.17163
[1mStep[0m  [80/169], [94mLoss[0m : 2.24096
[1mStep[0m  [96/169], [94mLoss[0m : 2.68862
[1mStep[0m  [112/169], [94mLoss[0m : 2.31028
[1mStep[0m  [128/169], [94mLoss[0m : 2.47493
[1mStep[0m  [144/169], [94mLoss[0m : 2.31864
[1mStep[0m  [160/169], [94mLoss[0m : 2.10712

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50959
[1mStep[0m  [16/169], [94mLoss[0m : 2.43186
[1mStep[0m  [32/169], [94mLoss[0m : 2.28094
[1mStep[0m  [48/169], [94mLoss[0m : 1.94261
[1mStep[0m  [64/169], [94mLoss[0m : 2.04586
[1mStep[0m  [80/169], [94mLoss[0m : 2.21857
[1mStep[0m  [96/169], [94mLoss[0m : 2.74167
[1mStep[0m  [112/169], [94mLoss[0m : 2.14844
[1mStep[0m  [128/169], [94mLoss[0m : 2.63888
[1mStep[0m  [144/169], [94mLoss[0m : 2.53307
[1mStep[0m  [160/169], [94mLoss[0m : 2.90353

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.402, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.313
====================================

Phase 1 - Evaluation MAE:  2.3126777644668306
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.33840
[1mStep[0m  [16/169], [94mLoss[0m : 2.35993
[1mStep[0m  [32/169], [94mLoss[0m : 2.36414
[1mStep[0m  [48/169], [94mLoss[0m : 2.58791
[1mStep[0m  [64/169], [94mLoss[0m : 2.91278
[1mStep[0m  [80/169], [94mLoss[0m : 2.44807
[1mStep[0m  [96/169], [94mLoss[0m : 2.20494
[1mStep[0m  [112/169], [94mLoss[0m : 2.40938
[1mStep[0m  [128/169], [94mLoss[0m : 2.31154
[1mStep[0m  [144/169], [94mLoss[0m : 2.19508
[1mStep[0m  [160/169], [94mLoss[0m : 2.24384

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.311, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38342
[1mStep[0m  [16/169], [94mLoss[0m : 2.52325
[1mStep[0m  [32/169], [94mLoss[0m : 1.74161
[1mStep[0m  [48/169], [94mLoss[0m : 2.03013
[1mStep[0m  [64/169], [94mLoss[0m : 2.72951
[1mStep[0m  [80/169], [94mLoss[0m : 2.07869
[1mStep[0m  [96/169], [94mLoss[0m : 2.63137
[1mStep[0m  [112/169], [94mLoss[0m : 2.39669
[1mStep[0m  [128/169], [94mLoss[0m : 2.18700
[1mStep[0m  [144/169], [94mLoss[0m : 2.59960
[1mStep[0m  [160/169], [94mLoss[0m : 2.65178

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.523, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27766
[1mStep[0m  [16/169], [94mLoss[0m : 2.05441
[1mStep[0m  [32/169], [94mLoss[0m : 2.51847
[1mStep[0m  [48/169], [94mLoss[0m : 2.65603
[1mStep[0m  [64/169], [94mLoss[0m : 2.37575
[1mStep[0m  [80/169], [94mLoss[0m : 2.15798
[1mStep[0m  [96/169], [94mLoss[0m : 2.12844
[1mStep[0m  [112/169], [94mLoss[0m : 2.60595
[1mStep[0m  [128/169], [94mLoss[0m : 2.54884
[1mStep[0m  [144/169], [94mLoss[0m : 2.36356
[1mStep[0m  [160/169], [94mLoss[0m : 2.09763

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.529, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45979
[1mStep[0m  [16/169], [94mLoss[0m : 2.28426
[1mStep[0m  [32/169], [94mLoss[0m : 2.32670
[1mStep[0m  [48/169], [94mLoss[0m : 2.32290
[1mStep[0m  [64/169], [94mLoss[0m : 2.51123
[1mStep[0m  [80/169], [94mLoss[0m : 2.22729
[1mStep[0m  [96/169], [94mLoss[0m : 2.02346
[1mStep[0m  [112/169], [94mLoss[0m : 2.34586
[1mStep[0m  [128/169], [94mLoss[0m : 2.14298
[1mStep[0m  [144/169], [94mLoss[0m : 1.86593
[1mStep[0m  [160/169], [94mLoss[0m : 2.31534

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.231, [92mTest[0m: 2.459, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69635
[1mStep[0m  [16/169], [94mLoss[0m : 1.97268
[1mStep[0m  [32/169], [94mLoss[0m : 2.16920
[1mStep[0m  [48/169], [94mLoss[0m : 1.94812
[1mStep[0m  [64/169], [94mLoss[0m : 2.50664
[1mStep[0m  [80/169], [94mLoss[0m : 2.70737
[1mStep[0m  [96/169], [94mLoss[0m : 2.43366
[1mStep[0m  [112/169], [94mLoss[0m : 2.26549
[1mStep[0m  [128/169], [94mLoss[0m : 2.28533
[1mStep[0m  [144/169], [94mLoss[0m : 2.36715
[1mStep[0m  [160/169], [94mLoss[0m : 2.37581

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05652
[1mStep[0m  [16/169], [94mLoss[0m : 2.06309
[1mStep[0m  [32/169], [94mLoss[0m : 2.45466
[1mStep[0m  [48/169], [94mLoss[0m : 1.99734
[1mStep[0m  [64/169], [94mLoss[0m : 2.29141
[1mStep[0m  [80/169], [94mLoss[0m : 1.68407
[1mStep[0m  [96/169], [94mLoss[0m : 2.28256
[1mStep[0m  [112/169], [94mLoss[0m : 2.10968
[1mStep[0m  [128/169], [94mLoss[0m : 2.28055
[1mStep[0m  [144/169], [94mLoss[0m : 2.14434
[1mStep[0m  [160/169], [94mLoss[0m : 2.00062

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96852
[1mStep[0m  [16/169], [94mLoss[0m : 1.60366
[1mStep[0m  [32/169], [94mLoss[0m : 1.84201
[1mStep[0m  [48/169], [94mLoss[0m : 1.86524
[1mStep[0m  [64/169], [94mLoss[0m : 1.91384
[1mStep[0m  [80/169], [94mLoss[0m : 2.06912
[1mStep[0m  [96/169], [94mLoss[0m : 2.39981
[1mStep[0m  [112/169], [94mLoss[0m : 2.12462
[1mStep[0m  [128/169], [94mLoss[0m : 2.04171
[1mStep[0m  [144/169], [94mLoss[0m : 2.20907
[1mStep[0m  [160/169], [94mLoss[0m : 2.12862

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.046, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34433
[1mStep[0m  [16/169], [94mLoss[0m : 1.87585
[1mStep[0m  [32/169], [94mLoss[0m : 2.03112
[1mStep[0m  [48/169], [94mLoss[0m : 1.70814
[1mStep[0m  [64/169], [94mLoss[0m : 1.88949
[1mStep[0m  [80/169], [94mLoss[0m : 1.88726
[1mStep[0m  [96/169], [94mLoss[0m : 2.11461
[1mStep[0m  [112/169], [94mLoss[0m : 2.36151
[1mStep[0m  [128/169], [94mLoss[0m : 1.66202
[1mStep[0m  [144/169], [94mLoss[0m : 2.04955
[1mStep[0m  [160/169], [94mLoss[0m : 2.05589

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.992, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91809
[1mStep[0m  [16/169], [94mLoss[0m : 2.14862
[1mStep[0m  [32/169], [94mLoss[0m : 1.66187
[1mStep[0m  [48/169], [94mLoss[0m : 1.93590
[1mStep[0m  [64/169], [94mLoss[0m : 2.03005
[1mStep[0m  [80/169], [94mLoss[0m : 2.25309
[1mStep[0m  [96/169], [94mLoss[0m : 1.83915
[1mStep[0m  [112/169], [94mLoss[0m : 2.00404
[1mStep[0m  [128/169], [94mLoss[0m : 2.44001
[1mStep[0m  [144/169], [94mLoss[0m : 1.70048
[1mStep[0m  [160/169], [94mLoss[0m : 2.22347

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.950, [92mTest[0m: 2.636, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97018
[1mStep[0m  [16/169], [94mLoss[0m : 1.81842
[1mStep[0m  [32/169], [94mLoss[0m : 1.65868
[1mStep[0m  [48/169], [94mLoss[0m : 2.23826
[1mStep[0m  [64/169], [94mLoss[0m : 1.58113
[1mStep[0m  [80/169], [94mLoss[0m : 1.65347
[1mStep[0m  [96/169], [94mLoss[0m : 1.94330
[1mStep[0m  [112/169], [94mLoss[0m : 2.45166
[1mStep[0m  [128/169], [94mLoss[0m : 1.81666
[1mStep[0m  [144/169], [94mLoss[0m : 1.46417
[1mStep[0m  [160/169], [94mLoss[0m : 1.91661

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64976
[1mStep[0m  [16/169], [94mLoss[0m : 1.91334
[1mStep[0m  [32/169], [94mLoss[0m : 1.80394
[1mStep[0m  [48/169], [94mLoss[0m : 1.85560
[1mStep[0m  [64/169], [94mLoss[0m : 1.75607
[1mStep[0m  [80/169], [94mLoss[0m : 1.80810
[1mStep[0m  [96/169], [94mLoss[0m : 1.37247
[1mStep[0m  [112/169], [94mLoss[0m : 1.86257
[1mStep[0m  [128/169], [94mLoss[0m : 1.80236
[1mStep[0m  [144/169], [94mLoss[0m : 1.87509
[1mStep[0m  [160/169], [94mLoss[0m : 1.94138

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.842, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66694
[1mStep[0m  [16/169], [94mLoss[0m : 1.88964
[1mStep[0m  [32/169], [94mLoss[0m : 2.10359
[1mStep[0m  [48/169], [94mLoss[0m : 1.83251
[1mStep[0m  [64/169], [94mLoss[0m : 1.71803
[1mStep[0m  [80/169], [94mLoss[0m : 1.67966
[1mStep[0m  [96/169], [94mLoss[0m : 1.75145
[1mStep[0m  [112/169], [94mLoss[0m : 1.65942
[1mStep[0m  [128/169], [94mLoss[0m : 1.83969
[1mStep[0m  [144/169], [94mLoss[0m : 1.59612
[1mStep[0m  [160/169], [94mLoss[0m : 1.84726

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.798, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.91728
[1mStep[0m  [16/169], [94mLoss[0m : 1.40246
[1mStep[0m  [32/169], [94mLoss[0m : 1.69825
[1mStep[0m  [48/169], [94mLoss[0m : 1.69868
[1mStep[0m  [64/169], [94mLoss[0m : 1.68143
[1mStep[0m  [80/169], [94mLoss[0m : 1.92555
[1mStep[0m  [96/169], [94mLoss[0m : 1.72418
[1mStep[0m  [112/169], [94mLoss[0m : 1.56118
[1mStep[0m  [128/169], [94mLoss[0m : 1.96732
[1mStep[0m  [144/169], [94mLoss[0m : 1.62038
[1mStep[0m  [160/169], [94mLoss[0m : 1.81337

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.743, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68342
[1mStep[0m  [16/169], [94mLoss[0m : 1.61498
[1mStep[0m  [32/169], [94mLoss[0m : 1.76845
[1mStep[0m  [48/169], [94mLoss[0m : 2.10018
[1mStep[0m  [64/169], [94mLoss[0m : 1.84815
[1mStep[0m  [80/169], [94mLoss[0m : 1.83754
[1mStep[0m  [96/169], [94mLoss[0m : 1.92814
[1mStep[0m  [112/169], [94mLoss[0m : 2.19633
[1mStep[0m  [128/169], [94mLoss[0m : 1.63739
[1mStep[0m  [144/169], [94mLoss[0m : 1.67031
[1mStep[0m  [160/169], [94mLoss[0m : 1.64905

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.716, [92mTest[0m: 2.518, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72304
[1mStep[0m  [16/169], [94mLoss[0m : 1.39860
[1mStep[0m  [32/169], [94mLoss[0m : 1.61472
[1mStep[0m  [48/169], [94mLoss[0m : 1.54413
[1mStep[0m  [64/169], [94mLoss[0m : 1.38214
[1mStep[0m  [80/169], [94mLoss[0m : 1.88801
[1mStep[0m  [96/169], [94mLoss[0m : 1.60616
[1mStep[0m  [112/169], [94mLoss[0m : 1.48087
[1mStep[0m  [128/169], [94mLoss[0m : 1.88175
[1mStep[0m  [144/169], [94mLoss[0m : 1.81039
[1mStep[0m  [160/169], [94mLoss[0m : 1.39333

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.697, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.44573
[1mStep[0m  [16/169], [94mLoss[0m : 1.58855
[1mStep[0m  [32/169], [94mLoss[0m : 1.76682
[1mStep[0m  [48/169], [94mLoss[0m : 1.46110
[1mStep[0m  [64/169], [94mLoss[0m : 1.88408
[1mStep[0m  [80/169], [94mLoss[0m : 1.75493
[1mStep[0m  [96/169], [94mLoss[0m : 1.68166
[1mStep[0m  [112/169], [94mLoss[0m : 1.90168
[1mStep[0m  [128/169], [94mLoss[0m : 1.45946
[1mStep[0m  [144/169], [94mLoss[0m : 1.90235
[1mStep[0m  [160/169], [94mLoss[0m : 1.44359

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.647, [92mTest[0m: 2.467, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46286
[1mStep[0m  [16/169], [94mLoss[0m : 1.16092
[1mStep[0m  [32/169], [94mLoss[0m : 1.44365
[1mStep[0m  [48/169], [94mLoss[0m : 1.42516
[1mStep[0m  [64/169], [94mLoss[0m : 1.54363
[1mStep[0m  [80/169], [94mLoss[0m : 1.48084
[1mStep[0m  [96/169], [94mLoss[0m : 1.46666
[1mStep[0m  [112/169], [94mLoss[0m : 1.35876
[1mStep[0m  [128/169], [94mLoss[0m : 2.07126
[1mStep[0m  [144/169], [94mLoss[0m : 1.81100
[1mStep[0m  [160/169], [94mLoss[0m : 1.61049

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.627, [92mTest[0m: 2.629, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69182
[1mStep[0m  [16/169], [94mLoss[0m : 1.70562
[1mStep[0m  [32/169], [94mLoss[0m : 1.51067
[1mStep[0m  [48/169], [94mLoss[0m : 1.38087
[1mStep[0m  [64/169], [94mLoss[0m : 1.44582
[1mStep[0m  [80/169], [94mLoss[0m : 1.78560
[1mStep[0m  [96/169], [94mLoss[0m : 1.86416
[1mStep[0m  [112/169], [94mLoss[0m : 1.31825
[1mStep[0m  [128/169], [94mLoss[0m : 1.55960
[1mStep[0m  [144/169], [94mLoss[0m : 1.92520
[1mStep[0m  [160/169], [94mLoss[0m : 1.35491

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70946
[1mStep[0m  [16/169], [94mLoss[0m : 1.50443
[1mStep[0m  [32/169], [94mLoss[0m : 1.45215
[1mStep[0m  [48/169], [94mLoss[0m : 1.67209
[1mStep[0m  [64/169], [94mLoss[0m : 1.47616
[1mStep[0m  [80/169], [94mLoss[0m : 1.86896
[1mStep[0m  [96/169], [94mLoss[0m : 1.57572
[1mStep[0m  [112/169], [94mLoss[0m : 1.34475
[1mStep[0m  [128/169], [94mLoss[0m : 1.49802
[1mStep[0m  [144/169], [94mLoss[0m : 1.55651
[1mStep[0m  [160/169], [94mLoss[0m : 1.56483

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74537
[1mStep[0m  [16/169], [94mLoss[0m : 1.51581
[1mStep[0m  [32/169], [94mLoss[0m : 1.53976
[1mStep[0m  [48/169], [94mLoss[0m : 1.56854
[1mStep[0m  [64/169], [94mLoss[0m : 1.60842
[1mStep[0m  [80/169], [94mLoss[0m : 1.25458
[1mStep[0m  [96/169], [94mLoss[0m : 1.34070
[1mStep[0m  [112/169], [94mLoss[0m : 1.44720
[1mStep[0m  [128/169], [94mLoss[0m : 1.46863
[1mStep[0m  [144/169], [94mLoss[0m : 1.63127
[1mStep[0m  [160/169], [94mLoss[0m : 1.34879

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.539, [92mTest[0m: 2.499, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70105
[1mStep[0m  [16/169], [94mLoss[0m : 1.29576
[1mStep[0m  [32/169], [94mLoss[0m : 1.51059
[1mStep[0m  [48/169], [94mLoss[0m : 1.50971
[1mStep[0m  [64/169], [94mLoss[0m : 1.51858
[1mStep[0m  [80/169], [94mLoss[0m : 1.54253
[1mStep[0m  [96/169], [94mLoss[0m : 1.50368
[1mStep[0m  [112/169], [94mLoss[0m : 1.46159
[1mStep[0m  [128/169], [94mLoss[0m : 1.19051
[1mStep[0m  [144/169], [94mLoss[0m : 1.41896
[1mStep[0m  [160/169], [94mLoss[0m : 1.68127

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.482, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53386
[1mStep[0m  [16/169], [94mLoss[0m : 1.32761
[1mStep[0m  [32/169], [94mLoss[0m : 1.52211
[1mStep[0m  [48/169], [94mLoss[0m : 1.39351
[1mStep[0m  [64/169], [94mLoss[0m : 1.27924
[1mStep[0m  [80/169], [94mLoss[0m : 1.40780
[1mStep[0m  [96/169], [94mLoss[0m : 1.44298
[1mStep[0m  [112/169], [94mLoss[0m : 1.29483
[1mStep[0m  [128/169], [94mLoss[0m : 1.41444
[1mStep[0m  [144/169], [94mLoss[0m : 1.29954
[1mStep[0m  [160/169], [94mLoss[0m : 1.22471

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.454, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39890
[1mStep[0m  [16/169], [94mLoss[0m : 1.57686
[1mStep[0m  [32/169], [94mLoss[0m : 1.26524
[1mStep[0m  [48/169], [94mLoss[0m : 1.48310
[1mStep[0m  [64/169], [94mLoss[0m : 1.51443
[1mStep[0m  [80/169], [94mLoss[0m : 1.48721
[1mStep[0m  [96/169], [94mLoss[0m : 1.66617
[1mStep[0m  [112/169], [94mLoss[0m : 1.56807
[1mStep[0m  [128/169], [94mLoss[0m : 1.62745
[1mStep[0m  [144/169], [94mLoss[0m : 1.54621
[1mStep[0m  [160/169], [94mLoss[0m : 1.38115

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.433, [92mTest[0m: 2.581, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.46811
[1mStep[0m  [16/169], [94mLoss[0m : 1.50425
[1mStep[0m  [32/169], [94mLoss[0m : 1.36386
[1mStep[0m  [48/169], [94mLoss[0m : 1.61091
[1mStep[0m  [64/169], [94mLoss[0m : 1.26961
[1mStep[0m  [80/169], [94mLoss[0m : 1.32887
[1mStep[0m  [96/169], [94mLoss[0m : 1.28635
[1mStep[0m  [112/169], [94mLoss[0m : 1.62010
[1mStep[0m  [128/169], [94mLoss[0m : 1.24361
[1mStep[0m  [144/169], [94mLoss[0m : 1.41237
[1mStep[0m  [160/169], [94mLoss[0m : 1.37432

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.421, [92mTest[0m: 2.470, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.554
====================================

Phase 2 - Evaluation MAE:  2.55372245822634
MAE score P1      2.312678
MAE score P2      2.553722
loss              1.420523
learning_rate     0.007525
batch_size              64
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.1
weight_decay         0.001
Name: 2, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 10.81015
[1mStep[0m  [16/169], [94mLoss[0m : 9.28065
[1mStep[0m  [32/169], [94mLoss[0m : 7.49992
[1mStep[0m  [48/169], [94mLoss[0m : 5.49010
[1mStep[0m  [64/169], [94mLoss[0m : 4.46600
[1mStep[0m  [80/169], [94mLoss[0m : 2.93901
[1mStep[0m  [96/169], [94mLoss[0m : 2.55994
[1mStep[0m  [112/169], [94mLoss[0m : 3.05839
[1mStep[0m  [128/169], [94mLoss[0m : 2.41528
[1mStep[0m  [144/169], [94mLoss[0m : 2.67010
[1mStep[0m  [160/169], [94mLoss[0m : 2.06303

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.806, [92mTest[0m: 11.019, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.92801
[1mStep[0m  [16/169], [94mLoss[0m : 2.47007
[1mStep[0m  [32/169], [94mLoss[0m : 2.55985
[1mStep[0m  [48/169], [94mLoss[0m : 2.30367
[1mStep[0m  [64/169], [94mLoss[0m : 2.49337
[1mStep[0m  [80/169], [94mLoss[0m : 2.73569
[1mStep[0m  [96/169], [94mLoss[0m : 2.65640
[1mStep[0m  [112/169], [94mLoss[0m : 2.23234
[1mStep[0m  [128/169], [94mLoss[0m : 2.82051
[1mStep[0m  [144/169], [94mLoss[0m : 2.29604
[1mStep[0m  [160/169], [94mLoss[0m : 2.32790

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67714
[1mStep[0m  [16/169], [94mLoss[0m : 2.77132
[1mStep[0m  [32/169], [94mLoss[0m : 2.57847
[1mStep[0m  [48/169], [94mLoss[0m : 2.24078
[1mStep[0m  [64/169], [94mLoss[0m : 2.61086
[1mStep[0m  [80/169], [94mLoss[0m : 2.34275
[1mStep[0m  [96/169], [94mLoss[0m : 2.47988
[1mStep[0m  [112/169], [94mLoss[0m : 2.59630
[1mStep[0m  [128/169], [94mLoss[0m : 2.41973
[1mStep[0m  [144/169], [94mLoss[0m : 2.54858
[1mStep[0m  [160/169], [94mLoss[0m : 2.58515

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.68525
[1mStep[0m  [16/169], [94mLoss[0m : 1.84649
[1mStep[0m  [32/169], [94mLoss[0m : 2.21033
[1mStep[0m  [48/169], [94mLoss[0m : 2.21901
[1mStep[0m  [64/169], [94mLoss[0m : 2.50723
[1mStep[0m  [80/169], [94mLoss[0m : 2.37954
[1mStep[0m  [96/169], [94mLoss[0m : 2.54015
[1mStep[0m  [112/169], [94mLoss[0m : 2.53514
[1mStep[0m  [128/169], [94mLoss[0m : 2.67283
[1mStep[0m  [144/169], [94mLoss[0m : 2.84281
[1mStep[0m  [160/169], [94mLoss[0m : 2.25976

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.468, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55542
[1mStep[0m  [16/169], [94mLoss[0m : 2.50348
[1mStep[0m  [32/169], [94mLoss[0m : 2.47837
[1mStep[0m  [48/169], [94mLoss[0m : 2.12343
[1mStep[0m  [64/169], [94mLoss[0m : 2.34846
[1mStep[0m  [80/169], [94mLoss[0m : 2.67586
[1mStep[0m  [96/169], [94mLoss[0m : 2.40883
[1mStep[0m  [112/169], [94mLoss[0m : 2.42708
[1mStep[0m  [128/169], [94mLoss[0m : 2.35427
[1mStep[0m  [144/169], [94mLoss[0m : 2.22948
[1mStep[0m  [160/169], [94mLoss[0m : 2.66480

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25001
[1mStep[0m  [16/169], [94mLoss[0m : 2.27219
[1mStep[0m  [32/169], [94mLoss[0m : 2.50419
[1mStep[0m  [48/169], [94mLoss[0m : 1.92047
[1mStep[0m  [64/169], [94mLoss[0m : 2.36543
[1mStep[0m  [80/169], [94mLoss[0m : 2.84428
[1mStep[0m  [96/169], [94mLoss[0m : 2.22730
[1mStep[0m  [112/169], [94mLoss[0m : 2.30112
[1mStep[0m  [128/169], [94mLoss[0m : 2.36739
[1mStep[0m  [144/169], [94mLoss[0m : 2.25569
[1mStep[0m  [160/169], [94mLoss[0m : 2.43191

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33371
[1mStep[0m  [16/169], [94mLoss[0m : 2.14882
[1mStep[0m  [32/169], [94mLoss[0m : 2.59524
[1mStep[0m  [48/169], [94mLoss[0m : 2.67957
[1mStep[0m  [64/169], [94mLoss[0m : 2.26448
[1mStep[0m  [80/169], [94mLoss[0m : 2.84811
[1mStep[0m  [96/169], [94mLoss[0m : 2.82094
[1mStep[0m  [112/169], [94mLoss[0m : 2.76235
[1mStep[0m  [128/169], [94mLoss[0m : 2.63761
[1mStep[0m  [144/169], [94mLoss[0m : 2.08396
[1mStep[0m  [160/169], [94mLoss[0m : 2.47255

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44620
[1mStep[0m  [16/169], [94mLoss[0m : 2.57912
[1mStep[0m  [32/169], [94mLoss[0m : 2.82374
[1mStep[0m  [48/169], [94mLoss[0m : 2.28383
[1mStep[0m  [64/169], [94mLoss[0m : 2.45230
[1mStep[0m  [80/169], [94mLoss[0m : 2.67411
[1mStep[0m  [96/169], [94mLoss[0m : 2.44398
[1mStep[0m  [112/169], [94mLoss[0m : 1.98037
[1mStep[0m  [128/169], [94mLoss[0m : 2.04550
[1mStep[0m  [144/169], [94mLoss[0m : 2.60634
[1mStep[0m  [160/169], [94mLoss[0m : 2.62475

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83740
[1mStep[0m  [16/169], [94mLoss[0m : 2.53778
[1mStep[0m  [32/169], [94mLoss[0m : 1.99631
[1mStep[0m  [48/169], [94mLoss[0m : 2.05550
[1mStep[0m  [64/169], [94mLoss[0m : 2.40367
[1mStep[0m  [80/169], [94mLoss[0m : 2.45760
[1mStep[0m  [96/169], [94mLoss[0m : 2.35242
[1mStep[0m  [112/169], [94mLoss[0m : 2.39138
[1mStep[0m  [128/169], [94mLoss[0m : 2.40039
[1mStep[0m  [144/169], [94mLoss[0m : 2.06597
[1mStep[0m  [160/169], [94mLoss[0m : 3.00235

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33708
[1mStep[0m  [16/169], [94mLoss[0m : 2.28071
[1mStep[0m  [32/169], [94mLoss[0m : 2.18463
[1mStep[0m  [48/169], [94mLoss[0m : 2.51011
[1mStep[0m  [64/169], [94mLoss[0m : 2.13071
[1mStep[0m  [80/169], [94mLoss[0m : 2.44593
[1mStep[0m  [96/169], [94mLoss[0m : 2.47634
[1mStep[0m  [112/169], [94mLoss[0m : 2.73289
[1mStep[0m  [128/169], [94mLoss[0m : 2.17581
[1mStep[0m  [144/169], [94mLoss[0m : 2.27930
[1mStep[0m  [160/169], [94mLoss[0m : 2.62702

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71703
[1mStep[0m  [16/169], [94mLoss[0m : 2.73470
[1mStep[0m  [32/169], [94mLoss[0m : 2.70037
[1mStep[0m  [48/169], [94mLoss[0m : 2.22912
[1mStep[0m  [64/169], [94mLoss[0m : 2.56737
[1mStep[0m  [80/169], [94mLoss[0m : 2.12021
[1mStep[0m  [96/169], [94mLoss[0m : 2.27024
[1mStep[0m  [112/169], [94mLoss[0m : 2.50049
[1mStep[0m  [128/169], [94mLoss[0m : 2.35160
[1mStep[0m  [144/169], [94mLoss[0m : 2.42039
[1mStep[0m  [160/169], [94mLoss[0m : 2.19809

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.315, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59865
[1mStep[0m  [16/169], [94mLoss[0m : 2.65454
[1mStep[0m  [32/169], [94mLoss[0m : 2.49225
[1mStep[0m  [48/169], [94mLoss[0m : 2.02729
[1mStep[0m  [64/169], [94mLoss[0m : 2.21767
[1mStep[0m  [80/169], [94mLoss[0m : 2.22664
[1mStep[0m  [96/169], [94mLoss[0m : 2.82765
[1mStep[0m  [112/169], [94mLoss[0m : 2.89113
[1mStep[0m  [128/169], [94mLoss[0m : 2.39135
[1mStep[0m  [144/169], [94mLoss[0m : 2.81093
[1mStep[0m  [160/169], [94mLoss[0m : 2.44407

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28250
[1mStep[0m  [16/169], [94mLoss[0m : 2.43408
[1mStep[0m  [32/169], [94mLoss[0m : 2.68488
[1mStep[0m  [48/169], [94mLoss[0m : 2.45878
[1mStep[0m  [64/169], [94mLoss[0m : 2.34027
[1mStep[0m  [80/169], [94mLoss[0m : 2.31729
[1mStep[0m  [96/169], [94mLoss[0m : 2.47045
[1mStep[0m  [112/169], [94mLoss[0m : 2.06617
[1mStep[0m  [128/169], [94mLoss[0m : 2.93428
[1mStep[0m  [144/169], [94mLoss[0m : 2.27276
[1mStep[0m  [160/169], [94mLoss[0m : 2.73356

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66845
[1mStep[0m  [16/169], [94mLoss[0m : 2.30540
[1mStep[0m  [32/169], [94mLoss[0m : 2.36803
[1mStep[0m  [48/169], [94mLoss[0m : 2.24317
[1mStep[0m  [64/169], [94mLoss[0m : 2.28514
[1mStep[0m  [80/169], [94mLoss[0m : 2.33038
[1mStep[0m  [96/169], [94mLoss[0m : 1.86277
[1mStep[0m  [112/169], [94mLoss[0m : 2.67453
[1mStep[0m  [128/169], [94mLoss[0m : 2.66197
[1mStep[0m  [144/169], [94mLoss[0m : 2.18381
[1mStep[0m  [160/169], [94mLoss[0m : 2.17904

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64676
[1mStep[0m  [16/169], [94mLoss[0m : 2.86292
[1mStep[0m  [32/169], [94mLoss[0m : 2.57156
[1mStep[0m  [48/169], [94mLoss[0m : 2.75055
[1mStep[0m  [64/169], [94mLoss[0m : 2.67568
[1mStep[0m  [80/169], [94mLoss[0m : 2.62627
[1mStep[0m  [96/169], [94mLoss[0m : 2.07827
[1mStep[0m  [112/169], [94mLoss[0m : 2.45784
[1mStep[0m  [128/169], [94mLoss[0m : 2.40230
[1mStep[0m  [144/169], [94mLoss[0m : 2.42786
[1mStep[0m  [160/169], [94mLoss[0m : 2.53595

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41992
[1mStep[0m  [16/169], [94mLoss[0m : 2.57064
[1mStep[0m  [32/169], [94mLoss[0m : 2.79716
[1mStep[0m  [48/169], [94mLoss[0m : 1.97256
[1mStep[0m  [64/169], [94mLoss[0m : 2.28794
[1mStep[0m  [80/169], [94mLoss[0m : 2.39159
[1mStep[0m  [96/169], [94mLoss[0m : 2.83263
[1mStep[0m  [112/169], [94mLoss[0m : 3.06059
[1mStep[0m  [128/169], [94mLoss[0m : 2.26849
[1mStep[0m  [144/169], [94mLoss[0m : 2.50881
[1mStep[0m  [160/169], [94mLoss[0m : 2.61821

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37079
[1mStep[0m  [16/169], [94mLoss[0m : 2.43176
[1mStep[0m  [32/169], [94mLoss[0m : 2.46741
[1mStep[0m  [48/169], [94mLoss[0m : 2.37601
[1mStep[0m  [64/169], [94mLoss[0m : 2.44620
[1mStep[0m  [80/169], [94mLoss[0m : 2.54969
[1mStep[0m  [96/169], [94mLoss[0m : 2.62829
[1mStep[0m  [112/169], [94mLoss[0m : 2.24339
[1mStep[0m  [128/169], [94mLoss[0m : 2.17436
[1mStep[0m  [144/169], [94mLoss[0m : 2.64745
[1mStep[0m  [160/169], [94mLoss[0m : 2.38240

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28634
[1mStep[0m  [16/169], [94mLoss[0m : 2.03654
[1mStep[0m  [32/169], [94mLoss[0m : 2.56796
[1mStep[0m  [48/169], [94mLoss[0m : 2.43741
[1mStep[0m  [64/169], [94mLoss[0m : 2.62521
[1mStep[0m  [80/169], [94mLoss[0m : 2.42656
[1mStep[0m  [96/169], [94mLoss[0m : 2.43327
[1mStep[0m  [112/169], [94mLoss[0m : 2.32771
[1mStep[0m  [128/169], [94mLoss[0m : 2.28014
[1mStep[0m  [144/169], [94mLoss[0m : 2.02668
[1mStep[0m  [160/169], [94mLoss[0m : 2.40925

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16729
[1mStep[0m  [16/169], [94mLoss[0m : 2.06495
[1mStep[0m  [32/169], [94mLoss[0m : 2.25403
[1mStep[0m  [48/169], [94mLoss[0m : 2.41877
[1mStep[0m  [64/169], [94mLoss[0m : 2.47109
[1mStep[0m  [80/169], [94mLoss[0m : 2.37269
[1mStep[0m  [96/169], [94mLoss[0m : 2.30004
[1mStep[0m  [112/169], [94mLoss[0m : 2.08842
[1mStep[0m  [128/169], [94mLoss[0m : 2.23259
[1mStep[0m  [144/169], [94mLoss[0m : 2.28052
[1mStep[0m  [160/169], [94mLoss[0m : 2.30585

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29359
[1mStep[0m  [16/169], [94mLoss[0m : 2.47822
[1mStep[0m  [32/169], [94mLoss[0m : 2.57067
[1mStep[0m  [48/169], [94mLoss[0m : 2.57564
[1mStep[0m  [64/169], [94mLoss[0m : 2.70789
[1mStep[0m  [80/169], [94mLoss[0m : 2.68726
[1mStep[0m  [96/169], [94mLoss[0m : 2.20289
[1mStep[0m  [112/169], [94mLoss[0m : 2.15671
[1mStep[0m  [128/169], [94mLoss[0m : 2.09245
[1mStep[0m  [144/169], [94mLoss[0m : 2.99487
[1mStep[0m  [160/169], [94mLoss[0m : 2.41341

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17259
[1mStep[0m  [16/169], [94mLoss[0m : 2.98606
[1mStep[0m  [32/169], [94mLoss[0m : 2.42255
[1mStep[0m  [48/169], [94mLoss[0m : 2.51885
[1mStep[0m  [64/169], [94mLoss[0m : 2.82270
[1mStep[0m  [80/169], [94mLoss[0m : 2.83751
[1mStep[0m  [96/169], [94mLoss[0m : 2.37017
[1mStep[0m  [112/169], [94mLoss[0m : 2.67823
[1mStep[0m  [128/169], [94mLoss[0m : 2.80560
[1mStep[0m  [144/169], [94mLoss[0m : 2.40266
[1mStep[0m  [160/169], [94mLoss[0m : 2.64679

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.327, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47125
[1mStep[0m  [16/169], [94mLoss[0m : 2.81289
[1mStep[0m  [32/169], [94mLoss[0m : 2.15965
[1mStep[0m  [48/169], [94mLoss[0m : 2.14992
[1mStep[0m  [64/169], [94mLoss[0m : 2.55987
[1mStep[0m  [80/169], [94mLoss[0m : 2.20217
[1mStep[0m  [96/169], [94mLoss[0m : 2.14741
[1mStep[0m  [112/169], [94mLoss[0m : 2.62607
[1mStep[0m  [128/169], [94mLoss[0m : 2.65550
[1mStep[0m  [144/169], [94mLoss[0m : 2.59995
[1mStep[0m  [160/169], [94mLoss[0m : 2.50997

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63437
[1mStep[0m  [16/169], [94mLoss[0m : 2.34773
[1mStep[0m  [32/169], [94mLoss[0m : 1.87402
[1mStep[0m  [48/169], [94mLoss[0m : 1.96054
[1mStep[0m  [64/169], [94mLoss[0m : 2.57010
[1mStep[0m  [80/169], [94mLoss[0m : 2.56199
[1mStep[0m  [96/169], [94mLoss[0m : 2.46981
[1mStep[0m  [112/169], [94mLoss[0m : 2.20537
[1mStep[0m  [128/169], [94mLoss[0m : 2.35102
[1mStep[0m  [144/169], [94mLoss[0m : 2.31430
[1mStep[0m  [160/169], [94mLoss[0m : 2.08720

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28212
[1mStep[0m  [16/169], [94mLoss[0m : 3.06692
[1mStep[0m  [32/169], [94mLoss[0m : 2.45552
[1mStep[0m  [48/169], [94mLoss[0m : 2.46527
[1mStep[0m  [64/169], [94mLoss[0m : 2.58773
[1mStep[0m  [80/169], [94mLoss[0m : 2.61060
[1mStep[0m  [96/169], [94mLoss[0m : 2.74627
[1mStep[0m  [112/169], [94mLoss[0m : 2.23826
[1mStep[0m  [128/169], [94mLoss[0m : 1.99068
[1mStep[0m  [144/169], [94mLoss[0m : 2.73868
[1mStep[0m  [160/169], [94mLoss[0m : 2.07387

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.350, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03874
[1mStep[0m  [16/169], [94mLoss[0m : 2.53565
[1mStep[0m  [32/169], [94mLoss[0m : 2.66815
[1mStep[0m  [48/169], [94mLoss[0m : 2.05535
[1mStep[0m  [64/169], [94mLoss[0m : 2.41165
[1mStep[0m  [80/169], [94mLoss[0m : 2.51626
[1mStep[0m  [96/169], [94mLoss[0m : 2.40852
[1mStep[0m  [112/169], [94mLoss[0m : 1.85501
[1mStep[0m  [128/169], [94mLoss[0m : 2.30115
[1mStep[0m  [144/169], [94mLoss[0m : 2.03889
[1mStep[0m  [160/169], [94mLoss[0m : 2.96715

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60080
[1mStep[0m  [16/169], [94mLoss[0m : 2.38519
[1mStep[0m  [32/169], [94mLoss[0m : 2.56354
[1mStep[0m  [48/169], [94mLoss[0m : 2.45737
[1mStep[0m  [64/169], [94mLoss[0m : 2.42099
[1mStep[0m  [80/169], [94mLoss[0m : 2.93358
[1mStep[0m  [96/169], [94mLoss[0m : 2.21324
[1mStep[0m  [112/169], [94mLoss[0m : 2.50807
[1mStep[0m  [128/169], [94mLoss[0m : 2.10351
[1mStep[0m  [144/169], [94mLoss[0m : 2.65783
[1mStep[0m  [160/169], [94mLoss[0m : 2.80270

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.352, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34117
[1mStep[0m  [16/169], [94mLoss[0m : 2.10184
[1mStep[0m  [32/169], [94mLoss[0m : 2.43505
[1mStep[0m  [48/169], [94mLoss[0m : 2.44444
[1mStep[0m  [64/169], [94mLoss[0m : 2.43357
[1mStep[0m  [80/169], [94mLoss[0m : 1.84532
[1mStep[0m  [96/169], [94mLoss[0m : 2.27448
[1mStep[0m  [112/169], [94mLoss[0m : 2.42844
[1mStep[0m  [128/169], [94mLoss[0m : 2.35565
[1mStep[0m  [144/169], [94mLoss[0m : 2.06977
[1mStep[0m  [160/169], [94mLoss[0m : 2.60669

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84887
[1mStep[0m  [16/169], [94mLoss[0m : 2.57037
[1mStep[0m  [32/169], [94mLoss[0m : 2.45260
[1mStep[0m  [48/169], [94mLoss[0m : 2.47424
[1mStep[0m  [64/169], [94mLoss[0m : 2.51311
[1mStep[0m  [80/169], [94mLoss[0m : 2.24918
[1mStep[0m  [96/169], [94mLoss[0m : 2.91911
[1mStep[0m  [112/169], [94mLoss[0m : 2.90336
[1mStep[0m  [128/169], [94mLoss[0m : 2.56320
[1mStep[0m  [144/169], [94mLoss[0m : 2.61432
[1mStep[0m  [160/169], [94mLoss[0m : 2.23469

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60751
[1mStep[0m  [16/169], [94mLoss[0m : 3.15276
[1mStep[0m  [32/169], [94mLoss[0m : 2.50612
[1mStep[0m  [48/169], [94mLoss[0m : 2.27272
[1mStep[0m  [64/169], [94mLoss[0m : 2.15616
[1mStep[0m  [80/169], [94mLoss[0m : 2.56857
[1mStep[0m  [96/169], [94mLoss[0m : 2.13558
[1mStep[0m  [112/169], [94mLoss[0m : 2.52569
[1mStep[0m  [128/169], [94mLoss[0m : 2.38616
[1mStep[0m  [144/169], [94mLoss[0m : 2.09915
[1mStep[0m  [160/169], [94mLoss[0m : 2.49200

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64147
[1mStep[0m  [16/169], [94mLoss[0m : 2.55047
[1mStep[0m  [32/169], [94mLoss[0m : 2.44271
[1mStep[0m  [48/169], [94mLoss[0m : 2.42749
[1mStep[0m  [64/169], [94mLoss[0m : 2.35769
[1mStep[0m  [80/169], [94mLoss[0m : 3.11769
[1mStep[0m  [96/169], [94mLoss[0m : 2.37778
[1mStep[0m  [112/169], [94mLoss[0m : 2.29590
[1mStep[0m  [128/169], [94mLoss[0m : 2.61116
[1mStep[0m  [144/169], [94mLoss[0m : 2.79756
[1mStep[0m  [160/169], [94mLoss[0m : 2.81799

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.329
====================================

Phase 1 - Evaluation MAE:  2.3288827453340804
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/169], [94mLoss[0m : 2.80845
[1mStep[0m  [16/169], [94mLoss[0m : 2.44664
[1mStep[0m  [32/169], [94mLoss[0m : 2.39846
[1mStep[0m  [48/169], [94mLoss[0m : 2.92132
[1mStep[0m  [64/169], [94mLoss[0m : 2.82031
[1mStep[0m  [80/169], [94mLoss[0m : 3.28326
[1mStep[0m  [96/169], [94mLoss[0m : 2.57820
[1mStep[0m  [112/169], [94mLoss[0m : 2.82704
[1mStep[0m  [128/169], [94mLoss[0m : 2.70117
[1mStep[0m  [144/169], [94mLoss[0m : 2.77901
[1mStep[0m  [160/169], [94mLoss[0m : 2.40200

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.98114
[1mStep[0m  [16/169], [94mLoss[0m : 2.10078
[1mStep[0m  [32/169], [94mLoss[0m : 2.26512
[1mStep[0m  [48/169], [94mLoss[0m : 2.27482
[1mStep[0m  [64/169], [94mLoss[0m : 2.45142
[1mStep[0m  [80/169], [94mLoss[0m : 2.49199
[1mStep[0m  [96/169], [94mLoss[0m : 2.41333
[1mStep[0m  [112/169], [94mLoss[0m : 2.67233
[1mStep[0m  [128/169], [94mLoss[0m : 2.44857
[1mStep[0m  [144/169], [94mLoss[0m : 2.57256
[1mStep[0m  [160/169], [94mLoss[0m : 2.75771

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.390, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41941
[1mStep[0m  [16/169], [94mLoss[0m : 2.43121
[1mStep[0m  [32/169], [94mLoss[0m : 1.79104
[1mStep[0m  [48/169], [94mLoss[0m : 2.22557
[1mStep[0m  [64/169], [94mLoss[0m : 1.98354
[1mStep[0m  [80/169], [94mLoss[0m : 2.32589
[1mStep[0m  [96/169], [94mLoss[0m : 2.29741
[1mStep[0m  [112/169], [94mLoss[0m : 2.67902
[1mStep[0m  [128/169], [94mLoss[0m : 2.91337
[1mStep[0m  [144/169], [94mLoss[0m : 2.27221
[1mStep[0m  [160/169], [94mLoss[0m : 2.06302

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09240
[1mStep[0m  [16/169], [94mLoss[0m : 2.22258
[1mStep[0m  [32/169], [94mLoss[0m : 2.26762
[1mStep[0m  [48/169], [94mLoss[0m : 1.84162
[1mStep[0m  [64/169], [94mLoss[0m : 1.98466
[1mStep[0m  [80/169], [94mLoss[0m : 3.28224
[1mStep[0m  [96/169], [94mLoss[0m : 2.47473
[1mStep[0m  [112/169], [94mLoss[0m : 2.47412
[1mStep[0m  [128/169], [94mLoss[0m : 1.96304
[1mStep[0m  [144/169], [94mLoss[0m : 2.35436
[1mStep[0m  [160/169], [94mLoss[0m : 2.31195

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08805
[1mStep[0m  [16/169], [94mLoss[0m : 2.08387
[1mStep[0m  [32/169], [94mLoss[0m : 2.51131
[1mStep[0m  [48/169], [94mLoss[0m : 1.93185
[1mStep[0m  [64/169], [94mLoss[0m : 2.69298
[1mStep[0m  [80/169], [94mLoss[0m : 2.40379
[1mStep[0m  [96/169], [94mLoss[0m : 2.52850
[1mStep[0m  [112/169], [94mLoss[0m : 2.05787
[1mStep[0m  [128/169], [94mLoss[0m : 2.01065
[1mStep[0m  [144/169], [94mLoss[0m : 2.69880
[1mStep[0m  [160/169], [94mLoss[0m : 2.01926

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.243, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86377
[1mStep[0m  [16/169], [94mLoss[0m : 2.27432
[1mStep[0m  [32/169], [94mLoss[0m : 2.30530
[1mStep[0m  [48/169], [94mLoss[0m : 2.24156
[1mStep[0m  [64/169], [94mLoss[0m : 1.91317
[1mStep[0m  [80/169], [94mLoss[0m : 1.80103
[1mStep[0m  [96/169], [94mLoss[0m : 2.61841
[1mStep[0m  [112/169], [94mLoss[0m : 2.50953
[1mStep[0m  [128/169], [94mLoss[0m : 1.89238
[1mStep[0m  [144/169], [94mLoss[0m : 1.87200
[1mStep[0m  [160/169], [94mLoss[0m : 2.34577

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.229, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87582
[1mStep[0m  [16/169], [94mLoss[0m : 1.98631
[1mStep[0m  [32/169], [94mLoss[0m : 2.08946
[1mStep[0m  [48/169], [94mLoss[0m : 2.01301
[1mStep[0m  [64/169], [94mLoss[0m : 2.00881
[1mStep[0m  [80/169], [94mLoss[0m : 1.48708
[1mStep[0m  [96/169], [94mLoss[0m : 2.26041
[1mStep[0m  [112/169], [94mLoss[0m : 2.00916
[1mStep[0m  [128/169], [94mLoss[0m : 2.30028
[1mStep[0m  [144/169], [94mLoss[0m : 2.37435
[1mStep[0m  [160/169], [94mLoss[0m : 2.02328

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.198, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04221
[1mStep[0m  [16/169], [94mLoss[0m : 2.08571
[1mStep[0m  [32/169], [94mLoss[0m : 1.93848
[1mStep[0m  [48/169], [94mLoss[0m : 1.61651
[1mStep[0m  [64/169], [94mLoss[0m : 2.24279
[1mStep[0m  [80/169], [94mLoss[0m : 2.55303
[1mStep[0m  [96/169], [94mLoss[0m : 1.86158
[1mStep[0m  [112/169], [94mLoss[0m : 2.01609
[1mStep[0m  [128/169], [94mLoss[0m : 2.39968
[1mStep[0m  [144/169], [94mLoss[0m : 2.53797
[1mStep[0m  [160/169], [94mLoss[0m : 2.04074

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.197, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20467
[1mStep[0m  [16/169], [94mLoss[0m : 2.16370
[1mStep[0m  [32/169], [94mLoss[0m : 2.03082
[1mStep[0m  [48/169], [94mLoss[0m : 2.17896
[1mStep[0m  [64/169], [94mLoss[0m : 2.20456
[1mStep[0m  [80/169], [94mLoss[0m : 1.83775
[1mStep[0m  [96/169], [94mLoss[0m : 2.12541
[1mStep[0m  [112/169], [94mLoss[0m : 2.34139
[1mStep[0m  [128/169], [94mLoss[0m : 2.35683
[1mStep[0m  [144/169], [94mLoss[0m : 2.70798
[1mStep[0m  [160/169], [94mLoss[0m : 2.55989

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83501
[1mStep[0m  [16/169], [94mLoss[0m : 2.83827
[1mStep[0m  [32/169], [94mLoss[0m : 1.88911
[1mStep[0m  [48/169], [94mLoss[0m : 2.44656
[1mStep[0m  [64/169], [94mLoss[0m : 1.72621
[1mStep[0m  [80/169], [94mLoss[0m : 2.47019
[1mStep[0m  [96/169], [94mLoss[0m : 1.86381
[1mStep[0m  [112/169], [94mLoss[0m : 2.34482
[1mStep[0m  [128/169], [94mLoss[0m : 2.52073
[1mStep[0m  [144/169], [94mLoss[0m : 2.01393
[1mStep[0m  [160/169], [94mLoss[0m : 2.28622

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.181, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30813
[1mStep[0m  [16/169], [94mLoss[0m : 1.67690
[1mStep[0m  [32/169], [94mLoss[0m : 2.03835
[1mStep[0m  [48/169], [94mLoss[0m : 2.24444
[1mStep[0m  [64/169], [94mLoss[0m : 2.12943
[1mStep[0m  [80/169], [94mLoss[0m : 2.33192
[1mStep[0m  [96/169], [94mLoss[0m : 1.93259
[1mStep[0m  [112/169], [94mLoss[0m : 2.27176
[1mStep[0m  [128/169], [94mLoss[0m : 2.46103
[1mStep[0m  [144/169], [94mLoss[0m : 2.42191
[1mStep[0m  [160/169], [94mLoss[0m : 1.94951

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.175, [92mTest[0m: 2.421, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90596
[1mStep[0m  [16/169], [94mLoss[0m : 2.21203
[1mStep[0m  [32/169], [94mLoss[0m : 2.13933
[1mStep[0m  [48/169], [94mLoss[0m : 2.20024
[1mStep[0m  [64/169], [94mLoss[0m : 2.00583
[1mStep[0m  [80/169], [94mLoss[0m : 2.21895
[1mStep[0m  [96/169], [94mLoss[0m : 2.34242
[1mStep[0m  [112/169], [94mLoss[0m : 1.94391
[1mStep[0m  [128/169], [94mLoss[0m : 2.58407
[1mStep[0m  [144/169], [94mLoss[0m : 2.23014
[1mStep[0m  [160/169], [94mLoss[0m : 2.16110

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.158, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84429
[1mStep[0m  [16/169], [94mLoss[0m : 2.00489
[1mStep[0m  [32/169], [94mLoss[0m : 2.15319
[1mStep[0m  [48/169], [94mLoss[0m : 2.25320
[1mStep[0m  [64/169], [94mLoss[0m : 1.91753
[1mStep[0m  [80/169], [94mLoss[0m : 1.88054
[1mStep[0m  [96/169], [94mLoss[0m : 2.10816
[1mStep[0m  [112/169], [94mLoss[0m : 2.08756
[1mStep[0m  [128/169], [94mLoss[0m : 2.26619
[1mStep[0m  [144/169], [94mLoss[0m : 1.63935
[1mStep[0m  [160/169], [94mLoss[0m : 1.82955

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34657
[1mStep[0m  [16/169], [94mLoss[0m : 2.38356
[1mStep[0m  [32/169], [94mLoss[0m : 1.83817
[1mStep[0m  [48/169], [94mLoss[0m : 1.98172
[1mStep[0m  [64/169], [94mLoss[0m : 2.10147
[1mStep[0m  [80/169], [94mLoss[0m : 2.78845
[1mStep[0m  [96/169], [94mLoss[0m : 2.12729
[1mStep[0m  [112/169], [94mLoss[0m : 1.86234
[1mStep[0m  [128/169], [94mLoss[0m : 2.27213
[1mStep[0m  [144/169], [94mLoss[0m : 1.99669
[1mStep[0m  [160/169], [94mLoss[0m : 2.41532

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.425, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21451
[1mStep[0m  [16/169], [94mLoss[0m : 1.87392
[1mStep[0m  [32/169], [94mLoss[0m : 1.74471
[1mStep[0m  [48/169], [94mLoss[0m : 2.47685
[1mStep[0m  [64/169], [94mLoss[0m : 2.29409
[1mStep[0m  [80/169], [94mLoss[0m : 2.64141
[1mStep[0m  [96/169], [94mLoss[0m : 2.21227
[1mStep[0m  [112/169], [94mLoss[0m : 2.00599
[1mStep[0m  [128/169], [94mLoss[0m : 2.46664
[1mStep[0m  [144/169], [94mLoss[0m : 2.44654
[1mStep[0m  [160/169], [94mLoss[0m : 2.20585

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.114, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98137
[1mStep[0m  [16/169], [94mLoss[0m : 2.12610
[1mStep[0m  [32/169], [94mLoss[0m : 1.95123
[1mStep[0m  [48/169], [94mLoss[0m : 1.57943
[1mStep[0m  [64/169], [94mLoss[0m : 2.44026
[1mStep[0m  [80/169], [94mLoss[0m : 2.35634
[1mStep[0m  [96/169], [94mLoss[0m : 2.62639
[1mStep[0m  [112/169], [94mLoss[0m : 1.86579
[1mStep[0m  [128/169], [94mLoss[0m : 2.14915
[1mStep[0m  [144/169], [94mLoss[0m : 2.03670
[1mStep[0m  [160/169], [94mLoss[0m : 2.36698

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85640
[1mStep[0m  [16/169], [94mLoss[0m : 2.00134
[1mStep[0m  [32/169], [94mLoss[0m : 2.17891
[1mStep[0m  [48/169], [94mLoss[0m : 2.07604
[1mStep[0m  [64/169], [94mLoss[0m : 2.25670
[1mStep[0m  [80/169], [94mLoss[0m : 1.84761
[1mStep[0m  [96/169], [94mLoss[0m : 1.94327
[1mStep[0m  [112/169], [94mLoss[0m : 1.84008
[1mStep[0m  [128/169], [94mLoss[0m : 2.28251
[1mStep[0m  [144/169], [94mLoss[0m : 1.75920
[1mStep[0m  [160/169], [94mLoss[0m : 2.75954

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.105, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82413
[1mStep[0m  [16/169], [94mLoss[0m : 1.99151
[1mStep[0m  [32/169], [94mLoss[0m : 2.00766
[1mStep[0m  [48/169], [94mLoss[0m : 1.80567
[1mStep[0m  [64/169], [94mLoss[0m : 1.86206
[1mStep[0m  [80/169], [94mLoss[0m : 2.21923
[1mStep[0m  [96/169], [94mLoss[0m : 2.11166
[1mStep[0m  [112/169], [94mLoss[0m : 2.25769
[1mStep[0m  [128/169], [94mLoss[0m : 2.33781
[1mStep[0m  [144/169], [94mLoss[0m : 1.92043
[1mStep[0m  [160/169], [94mLoss[0m : 2.15804

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.096, [92mTest[0m: 2.456, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06201
[1mStep[0m  [16/169], [94mLoss[0m : 1.86736
[1mStep[0m  [32/169], [94mLoss[0m : 1.59076
[1mStep[0m  [48/169], [94mLoss[0m : 2.16183
[1mStep[0m  [64/169], [94mLoss[0m : 2.01919
[1mStep[0m  [80/169], [94mLoss[0m : 2.49612
[1mStep[0m  [96/169], [94mLoss[0m : 1.73702
[1mStep[0m  [112/169], [94mLoss[0m : 2.00118
[1mStep[0m  [128/169], [94mLoss[0m : 2.33632
[1mStep[0m  [144/169], [94mLoss[0m : 1.96323
[1mStep[0m  [160/169], [94mLoss[0m : 1.66979

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.085, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96908
[1mStep[0m  [16/169], [94mLoss[0m : 2.18689
[1mStep[0m  [32/169], [94mLoss[0m : 2.12003
[1mStep[0m  [48/169], [94mLoss[0m : 2.03436
[1mStep[0m  [64/169], [94mLoss[0m : 1.98868
[1mStep[0m  [80/169], [94mLoss[0m : 2.73163
[1mStep[0m  [96/169], [94mLoss[0m : 1.94974
[1mStep[0m  [112/169], [94mLoss[0m : 2.32879
[1mStep[0m  [128/169], [94mLoss[0m : 1.91591
[1mStep[0m  [144/169], [94mLoss[0m : 2.28135
[1mStep[0m  [160/169], [94mLoss[0m : 2.15943

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.061, [92mTest[0m: 2.424, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04422
[1mStep[0m  [16/169], [94mLoss[0m : 1.75474
[1mStep[0m  [32/169], [94mLoss[0m : 1.57599
[1mStep[0m  [48/169], [94mLoss[0m : 1.82242
[1mStep[0m  [64/169], [94mLoss[0m : 2.31010
[1mStep[0m  [80/169], [94mLoss[0m : 2.31119
[1mStep[0m  [96/169], [94mLoss[0m : 1.84186
[1mStep[0m  [112/169], [94mLoss[0m : 1.97402
[1mStep[0m  [128/169], [94mLoss[0m : 1.73909
[1mStep[0m  [144/169], [94mLoss[0m : 1.75119
[1mStep[0m  [160/169], [94mLoss[0m : 1.91711

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.044, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90930
[1mStep[0m  [16/169], [94mLoss[0m : 1.90589
[1mStep[0m  [32/169], [94mLoss[0m : 2.11870
[1mStep[0m  [48/169], [94mLoss[0m : 1.91775
[1mStep[0m  [64/169], [94mLoss[0m : 2.36061
[1mStep[0m  [80/169], [94mLoss[0m : 2.30410
[1mStep[0m  [96/169], [94mLoss[0m : 2.33806
[1mStep[0m  [112/169], [94mLoss[0m : 2.06818
[1mStep[0m  [128/169], [94mLoss[0m : 2.05266
[1mStep[0m  [144/169], [94mLoss[0m : 2.10655
[1mStep[0m  [160/169], [94mLoss[0m : 1.89065

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.451, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60956
[1mStep[0m  [16/169], [94mLoss[0m : 2.31187
[1mStep[0m  [32/169], [94mLoss[0m : 2.23066
[1mStep[0m  [48/169], [94mLoss[0m : 2.12271
[1mStep[0m  [64/169], [94mLoss[0m : 2.51138
[1mStep[0m  [80/169], [94mLoss[0m : 2.25382
[1mStep[0m  [96/169], [94mLoss[0m : 1.68528
[1mStep[0m  [112/169], [94mLoss[0m : 1.71854
[1mStep[0m  [128/169], [94mLoss[0m : 2.62362
[1mStep[0m  [144/169], [94mLoss[0m : 1.81388
[1mStep[0m  [160/169], [94mLoss[0m : 2.02332

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.038, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67418
[1mStep[0m  [16/169], [94mLoss[0m : 1.97769
[1mStep[0m  [32/169], [94mLoss[0m : 1.84739
[1mStep[0m  [48/169], [94mLoss[0m : 2.46268
[1mStep[0m  [64/169], [94mLoss[0m : 1.51877
[1mStep[0m  [80/169], [94mLoss[0m : 1.92692
[1mStep[0m  [96/169], [94mLoss[0m : 2.15549
[1mStep[0m  [112/169], [94mLoss[0m : 2.17094
[1mStep[0m  [128/169], [94mLoss[0m : 2.04339
[1mStep[0m  [144/169], [94mLoss[0m : 2.51181
[1mStep[0m  [160/169], [94mLoss[0m : 2.70405

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.026, [92mTest[0m: 2.451, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09665
[1mStep[0m  [16/169], [94mLoss[0m : 2.43012
[1mStep[0m  [32/169], [94mLoss[0m : 1.91084
[1mStep[0m  [48/169], [94mLoss[0m : 2.06392
[1mStep[0m  [64/169], [94mLoss[0m : 2.13710
[1mStep[0m  [80/169], [94mLoss[0m : 1.84278
[1mStep[0m  [96/169], [94mLoss[0m : 1.95053
[1mStep[0m  [112/169], [94mLoss[0m : 1.91823
[1mStep[0m  [128/169], [94mLoss[0m : 1.75902
[1mStep[0m  [144/169], [94mLoss[0m : 2.12607
[1mStep[0m  [160/169], [94mLoss[0m : 1.84967

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53366
[1mStep[0m  [16/169], [94mLoss[0m : 2.08152
[1mStep[0m  [32/169], [94mLoss[0m : 2.09700
[1mStep[0m  [48/169], [94mLoss[0m : 1.40075
[1mStep[0m  [64/169], [94mLoss[0m : 2.03153
[1mStep[0m  [80/169], [94mLoss[0m : 1.85253
[1mStep[0m  [96/169], [94mLoss[0m : 2.32270
[1mStep[0m  [112/169], [94mLoss[0m : 1.97696
[1mStep[0m  [128/169], [94mLoss[0m : 2.02130
[1mStep[0m  [144/169], [94mLoss[0m : 2.13130
[1mStep[0m  [160/169], [94mLoss[0m : 2.23967

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.483, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07861
[1mStep[0m  [16/169], [94mLoss[0m : 1.92772
[1mStep[0m  [32/169], [94mLoss[0m : 1.75788
[1mStep[0m  [48/169], [94mLoss[0m : 1.82221
[1mStep[0m  [64/169], [94mLoss[0m : 1.95551
[1mStep[0m  [80/169], [94mLoss[0m : 2.29848
[1mStep[0m  [96/169], [94mLoss[0m : 2.13809
[1mStep[0m  [112/169], [94mLoss[0m : 2.17892
[1mStep[0m  [128/169], [94mLoss[0m : 2.44236
[1mStep[0m  [144/169], [94mLoss[0m : 2.10933
[1mStep[0m  [160/169], [94mLoss[0m : 2.58779

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.029, [92mTest[0m: 2.470, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94339
[1mStep[0m  [16/169], [94mLoss[0m : 1.79579
[1mStep[0m  [32/169], [94mLoss[0m : 2.28575
[1mStep[0m  [48/169], [94mLoss[0m : 2.17851
[1mStep[0m  [64/169], [94mLoss[0m : 1.99908
[1mStep[0m  [80/169], [94mLoss[0m : 2.38842
[1mStep[0m  [96/169], [94mLoss[0m : 1.99474
[1mStep[0m  [112/169], [94mLoss[0m : 2.16627
[1mStep[0m  [128/169], [94mLoss[0m : 2.09497
[1mStep[0m  [144/169], [94mLoss[0m : 2.12165
[1mStep[0m  [160/169], [94mLoss[0m : 2.03224

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.432, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35389
[1mStep[0m  [16/169], [94mLoss[0m : 2.11942
[1mStep[0m  [32/169], [94mLoss[0m : 1.98289
[1mStep[0m  [48/169], [94mLoss[0m : 1.75482
[1mStep[0m  [64/169], [94mLoss[0m : 1.80354
[1mStep[0m  [80/169], [94mLoss[0m : 1.86766
[1mStep[0m  [96/169], [94mLoss[0m : 2.06882
[1mStep[0m  [112/169], [94mLoss[0m : 2.15955
[1mStep[0m  [128/169], [94mLoss[0m : 2.05756
[1mStep[0m  [144/169], [94mLoss[0m : 2.11746
[1mStep[0m  [160/169], [94mLoss[0m : 2.01064

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.050, [92mTest[0m: 2.457, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23295
[1mStep[0m  [16/169], [94mLoss[0m : 2.09368
[1mStep[0m  [32/169], [94mLoss[0m : 1.97435
[1mStep[0m  [48/169], [94mLoss[0m : 2.31959
[1mStep[0m  [64/169], [94mLoss[0m : 2.26087
[1mStep[0m  [80/169], [94mLoss[0m : 1.77521
[1mStep[0m  [96/169], [94mLoss[0m : 1.49518
[1mStep[0m  [112/169], [94mLoss[0m : 1.82174
[1mStep[0m  [128/169], [94mLoss[0m : 2.00995
[1mStep[0m  [144/169], [94mLoss[0m : 2.15601
[1mStep[0m  [160/169], [94mLoss[0m : 2.25623

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.503, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.504
====================================

Phase 2 - Evaluation MAE:  2.504285033260073
MAE score P1      2.328883
MAE score P2      2.504285
loss              2.012971
learning_rate     0.007525
batch_size              64
hidden_sizes         [100]
epochs                  30
activation            tanh
optimizer              sgd
early stopping       False
dropout                0.2
momentum               0.9
weight_decay          0.01
Name: 3, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.80976
[1mStep[0m  [16/169], [94mLoss[0m : 10.42247
[1mStep[0m  [32/169], [94mLoss[0m : 7.44578
[1mStep[0m  [48/169], [94mLoss[0m : 5.21571
[1mStep[0m  [64/169], [94mLoss[0m : 2.97914
[1mStep[0m  [80/169], [94mLoss[0m : 3.02775
[1mStep[0m  [96/169], [94mLoss[0m : 2.45273
[1mStep[0m  [112/169], [94mLoss[0m : 2.53719
[1mStep[0m  [128/169], [94mLoss[0m : 2.45017
[1mStep[0m  [144/169], [94mLoss[0m : 3.45101
[1mStep[0m  [160/169], [94mLoss[0m : 2.40959

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.807, [92mTest[0m: 11.109, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31902
[1mStep[0m  [16/169], [94mLoss[0m : 2.52767
[1mStep[0m  [32/169], [94mLoss[0m : 2.66720
[1mStep[0m  [48/169], [94mLoss[0m : 2.57026
[1mStep[0m  [64/169], [94mLoss[0m : 2.05887
[1mStep[0m  [80/169], [94mLoss[0m : 3.04255
[1mStep[0m  [96/169], [94mLoss[0m : 2.43510
[1mStep[0m  [112/169], [94mLoss[0m : 2.49075
[1mStep[0m  [128/169], [94mLoss[0m : 2.45838
[1mStep[0m  [144/169], [94mLoss[0m : 2.54733
[1mStep[0m  [160/169], [94mLoss[0m : 2.67489

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.573, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54708
[1mStep[0m  [16/169], [94mLoss[0m : 2.57516
[1mStep[0m  [32/169], [94mLoss[0m : 2.52152
[1mStep[0m  [48/169], [94mLoss[0m : 2.71869
[1mStep[0m  [64/169], [94mLoss[0m : 2.63840
[1mStep[0m  [80/169], [94mLoss[0m : 1.96900
[1mStep[0m  [96/169], [94mLoss[0m : 2.68122
[1mStep[0m  [112/169], [94mLoss[0m : 2.29838
[1mStep[0m  [128/169], [94mLoss[0m : 2.58671
[1mStep[0m  [144/169], [94mLoss[0m : 2.45504
[1mStep[0m  [160/169], [94mLoss[0m : 3.01656

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.508, [92mTest[0m: 2.431, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66889
[1mStep[0m  [16/169], [94mLoss[0m : 2.45162
[1mStep[0m  [32/169], [94mLoss[0m : 2.41525
[1mStep[0m  [48/169], [94mLoss[0m : 2.92966
[1mStep[0m  [64/169], [94mLoss[0m : 2.28588
[1mStep[0m  [80/169], [94mLoss[0m : 2.93547
[1mStep[0m  [96/169], [94mLoss[0m : 2.56333
[1mStep[0m  [112/169], [94mLoss[0m : 2.66963
[1mStep[0m  [128/169], [94mLoss[0m : 2.79620
[1mStep[0m  [144/169], [94mLoss[0m : 2.83852
[1mStep[0m  [160/169], [94mLoss[0m : 3.03559

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40341
[1mStep[0m  [16/169], [94mLoss[0m : 2.46760
[1mStep[0m  [32/169], [94mLoss[0m : 2.31478
[1mStep[0m  [48/169], [94mLoss[0m : 2.38927
[1mStep[0m  [64/169], [94mLoss[0m : 2.42826
[1mStep[0m  [80/169], [94mLoss[0m : 3.02950
[1mStep[0m  [96/169], [94mLoss[0m : 2.40351
[1mStep[0m  [112/169], [94mLoss[0m : 2.81888
[1mStep[0m  [128/169], [94mLoss[0m : 2.45970
[1mStep[0m  [144/169], [94mLoss[0m : 2.17844
[1mStep[0m  [160/169], [94mLoss[0m : 2.04954

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.368, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83045
[1mStep[0m  [16/169], [94mLoss[0m : 2.78546
[1mStep[0m  [32/169], [94mLoss[0m : 2.57250
[1mStep[0m  [48/169], [94mLoss[0m : 2.14917
[1mStep[0m  [64/169], [94mLoss[0m : 2.60265
[1mStep[0m  [80/169], [94mLoss[0m : 2.30922
[1mStep[0m  [96/169], [94mLoss[0m : 2.30757
[1mStep[0m  [112/169], [94mLoss[0m : 2.64426
[1mStep[0m  [128/169], [94mLoss[0m : 2.28805
[1mStep[0m  [144/169], [94mLoss[0m : 2.32842
[1mStep[0m  [160/169], [94mLoss[0m : 2.20011

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79092
[1mStep[0m  [16/169], [94mLoss[0m : 1.93365
[1mStep[0m  [32/169], [94mLoss[0m : 2.35754
[1mStep[0m  [48/169], [94mLoss[0m : 2.89273
[1mStep[0m  [64/169], [94mLoss[0m : 2.16182
[1mStep[0m  [80/169], [94mLoss[0m : 2.95654
[1mStep[0m  [96/169], [94mLoss[0m : 2.66825
[1mStep[0m  [112/169], [94mLoss[0m : 2.15915
[1mStep[0m  [128/169], [94mLoss[0m : 2.72910
[1mStep[0m  [144/169], [94mLoss[0m : 2.36101
[1mStep[0m  [160/169], [94mLoss[0m : 2.70503

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.427, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38246
[1mStep[0m  [16/169], [94mLoss[0m : 2.35509
[1mStep[0m  [32/169], [94mLoss[0m : 2.53476
[1mStep[0m  [48/169], [94mLoss[0m : 2.35550
[1mStep[0m  [64/169], [94mLoss[0m : 2.46619
[1mStep[0m  [80/169], [94mLoss[0m : 2.28943
[1mStep[0m  [96/169], [94mLoss[0m : 2.13854
[1mStep[0m  [112/169], [94mLoss[0m : 2.51466
[1mStep[0m  [128/169], [94mLoss[0m : 2.20792
[1mStep[0m  [144/169], [94mLoss[0m : 2.41064
[1mStep[0m  [160/169], [94mLoss[0m : 2.35903

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15272
[1mStep[0m  [16/169], [94mLoss[0m : 2.42441
[1mStep[0m  [32/169], [94mLoss[0m : 3.08896
[1mStep[0m  [48/169], [94mLoss[0m : 2.66032
[1mStep[0m  [64/169], [94mLoss[0m : 2.15346
[1mStep[0m  [80/169], [94mLoss[0m : 2.17515
[1mStep[0m  [96/169], [94mLoss[0m : 2.63689
[1mStep[0m  [112/169], [94mLoss[0m : 2.27626
[1mStep[0m  [128/169], [94mLoss[0m : 2.32986
[1mStep[0m  [144/169], [94mLoss[0m : 2.88165
[1mStep[0m  [160/169], [94mLoss[0m : 2.49571

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.429, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39723
[1mStep[0m  [16/169], [94mLoss[0m : 2.42184
[1mStep[0m  [32/169], [94mLoss[0m : 2.54639
[1mStep[0m  [48/169], [94mLoss[0m : 2.55805
[1mStep[0m  [64/169], [94mLoss[0m : 2.29235
[1mStep[0m  [80/169], [94mLoss[0m : 2.62389
[1mStep[0m  [96/169], [94mLoss[0m : 2.35823
[1mStep[0m  [112/169], [94mLoss[0m : 2.23444
[1mStep[0m  [128/169], [94mLoss[0m : 2.42105
[1mStep[0m  [144/169], [94mLoss[0m : 2.23480
[1mStep[0m  [160/169], [94mLoss[0m : 2.52353

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50273
[1mStep[0m  [16/169], [94mLoss[0m : 2.78994
[1mStep[0m  [32/169], [94mLoss[0m : 2.98415
[1mStep[0m  [48/169], [94mLoss[0m : 2.75434
[1mStep[0m  [64/169], [94mLoss[0m : 2.56937
[1mStep[0m  [80/169], [94mLoss[0m : 2.14895
[1mStep[0m  [96/169], [94mLoss[0m : 2.38123
[1mStep[0m  [112/169], [94mLoss[0m : 2.46279
[1mStep[0m  [128/169], [94mLoss[0m : 2.32570
[1mStep[0m  [144/169], [94mLoss[0m : 2.30063
[1mStep[0m  [160/169], [94mLoss[0m : 2.45963

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.411, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75251
[1mStep[0m  [16/169], [94mLoss[0m : 2.28107
[1mStep[0m  [32/169], [94mLoss[0m : 2.56689
[1mStep[0m  [48/169], [94mLoss[0m : 2.54280
[1mStep[0m  [64/169], [94mLoss[0m : 2.63189
[1mStep[0m  [80/169], [94mLoss[0m : 2.32441
[1mStep[0m  [96/169], [94mLoss[0m : 2.60226
[1mStep[0m  [112/169], [94mLoss[0m : 2.33454
[1mStep[0m  [128/169], [94mLoss[0m : 2.28297
[1mStep[0m  [144/169], [94mLoss[0m : 2.53238
[1mStep[0m  [160/169], [94mLoss[0m : 2.41929

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.433, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67974
[1mStep[0m  [16/169], [94mLoss[0m : 2.42587
[1mStep[0m  [32/169], [94mLoss[0m : 2.45424
[1mStep[0m  [48/169], [94mLoss[0m : 2.20020
[1mStep[0m  [64/169], [94mLoss[0m : 2.45939
[1mStep[0m  [80/169], [94mLoss[0m : 2.80013
[1mStep[0m  [96/169], [94mLoss[0m : 2.03225
[1mStep[0m  [112/169], [94mLoss[0m : 2.40203
[1mStep[0m  [128/169], [94mLoss[0m : 2.28500
[1mStep[0m  [144/169], [94mLoss[0m : 2.10542
[1mStep[0m  [160/169], [94mLoss[0m : 2.15262

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55151
[1mStep[0m  [16/169], [94mLoss[0m : 2.86101
[1mStep[0m  [32/169], [94mLoss[0m : 2.83018
[1mStep[0m  [48/169], [94mLoss[0m : 1.93064
[1mStep[0m  [64/169], [94mLoss[0m : 2.24222
[1mStep[0m  [80/169], [94mLoss[0m : 2.30660
[1mStep[0m  [96/169], [94mLoss[0m : 2.64479
[1mStep[0m  [112/169], [94mLoss[0m : 2.34657
[1mStep[0m  [128/169], [94mLoss[0m : 2.39598
[1mStep[0m  [144/169], [94mLoss[0m : 2.31997
[1mStep[0m  [160/169], [94mLoss[0m : 2.52504

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37123
[1mStep[0m  [16/169], [94mLoss[0m : 2.02566
[1mStep[0m  [32/169], [94mLoss[0m : 2.51219
[1mStep[0m  [48/169], [94mLoss[0m : 2.30255
[1mStep[0m  [64/169], [94mLoss[0m : 2.56570
[1mStep[0m  [80/169], [94mLoss[0m : 1.94906
[1mStep[0m  [96/169], [94mLoss[0m : 2.21219
[1mStep[0m  [112/169], [94mLoss[0m : 2.62397
[1mStep[0m  [128/169], [94mLoss[0m : 2.10541
[1mStep[0m  [144/169], [94mLoss[0m : 2.31507
[1mStep[0m  [160/169], [94mLoss[0m : 2.34264

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.78457
[1mStep[0m  [16/169], [94mLoss[0m : 2.24099
[1mStep[0m  [32/169], [94mLoss[0m : 2.04108
[1mStep[0m  [48/169], [94mLoss[0m : 2.44718
[1mStep[0m  [64/169], [94mLoss[0m : 2.34096
[1mStep[0m  [80/169], [94mLoss[0m : 2.43569
[1mStep[0m  [96/169], [94mLoss[0m : 2.49209
[1mStep[0m  [112/169], [94mLoss[0m : 2.16724
[1mStep[0m  [128/169], [94mLoss[0m : 2.18503
[1mStep[0m  [144/169], [94mLoss[0m : 2.89574
[1mStep[0m  [160/169], [94mLoss[0m : 2.57796

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.378, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23297
[1mStep[0m  [16/169], [94mLoss[0m : 2.57857
[1mStep[0m  [32/169], [94mLoss[0m : 2.66442
[1mStep[0m  [48/169], [94mLoss[0m : 1.96278
[1mStep[0m  [64/169], [94mLoss[0m : 2.57372
[1mStep[0m  [80/169], [94mLoss[0m : 2.40858
[1mStep[0m  [96/169], [94mLoss[0m : 2.41402
[1mStep[0m  [112/169], [94mLoss[0m : 2.64956
[1mStep[0m  [128/169], [94mLoss[0m : 2.24961
[1mStep[0m  [144/169], [94mLoss[0m : 2.37408
[1mStep[0m  [160/169], [94mLoss[0m : 2.35423

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.400, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60418
[1mStep[0m  [16/169], [94mLoss[0m : 2.16135
[1mStep[0m  [32/169], [94mLoss[0m : 2.19381
[1mStep[0m  [48/169], [94mLoss[0m : 2.33933
[1mStep[0m  [64/169], [94mLoss[0m : 2.65949
[1mStep[0m  [80/169], [94mLoss[0m : 2.73948
[1mStep[0m  [96/169], [94mLoss[0m : 2.35715
[1mStep[0m  [112/169], [94mLoss[0m : 2.48760
[1mStep[0m  [128/169], [94mLoss[0m : 2.30604
[1mStep[0m  [144/169], [94mLoss[0m : 2.57178
[1mStep[0m  [160/169], [94mLoss[0m : 2.35104

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38179
[1mStep[0m  [16/169], [94mLoss[0m : 2.03960
[1mStep[0m  [32/169], [94mLoss[0m : 2.29917
[1mStep[0m  [48/169], [94mLoss[0m : 2.42641
[1mStep[0m  [64/169], [94mLoss[0m : 2.62919
[1mStep[0m  [80/169], [94mLoss[0m : 2.53154
[1mStep[0m  [96/169], [94mLoss[0m : 2.35552
[1mStep[0m  [112/169], [94mLoss[0m : 2.37699
[1mStep[0m  [128/169], [94mLoss[0m : 2.13801
[1mStep[0m  [144/169], [94mLoss[0m : 2.27181
[1mStep[0m  [160/169], [94mLoss[0m : 2.49743

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41392
[1mStep[0m  [16/169], [94mLoss[0m : 2.36414
[1mStep[0m  [32/169], [94mLoss[0m : 2.46431
[1mStep[0m  [48/169], [94mLoss[0m : 2.52381
[1mStep[0m  [64/169], [94mLoss[0m : 1.92224
[1mStep[0m  [80/169], [94mLoss[0m : 2.28485
[1mStep[0m  [96/169], [94mLoss[0m : 2.37249
[1mStep[0m  [112/169], [94mLoss[0m : 2.29601
[1mStep[0m  [128/169], [94mLoss[0m : 2.56867
[1mStep[0m  [144/169], [94mLoss[0m : 2.42634
[1mStep[0m  [160/169], [94mLoss[0m : 2.24332

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.374, [92mTest[0m: 2.423, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01996
[1mStep[0m  [16/169], [94mLoss[0m : 2.26532
[1mStep[0m  [32/169], [94mLoss[0m : 2.87087
[1mStep[0m  [48/169], [94mLoss[0m : 2.40944
[1mStep[0m  [64/169], [94mLoss[0m : 2.10044
[1mStep[0m  [80/169], [94mLoss[0m : 2.17739
[1mStep[0m  [96/169], [94mLoss[0m : 2.02746
[1mStep[0m  [112/169], [94mLoss[0m : 2.75116
[1mStep[0m  [128/169], [94mLoss[0m : 2.12687
[1mStep[0m  [144/169], [94mLoss[0m : 2.58563
[1mStep[0m  [160/169], [94mLoss[0m : 2.61911

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.389, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08328
[1mStep[0m  [16/169], [94mLoss[0m : 2.58907
[1mStep[0m  [32/169], [94mLoss[0m : 2.39454
[1mStep[0m  [48/169], [94mLoss[0m : 2.07160
[1mStep[0m  [64/169], [94mLoss[0m : 2.24273
[1mStep[0m  [80/169], [94mLoss[0m : 2.45257
[1mStep[0m  [96/169], [94mLoss[0m : 2.22505
[1mStep[0m  [112/169], [94mLoss[0m : 2.20044
[1mStep[0m  [128/169], [94mLoss[0m : 2.34678
[1mStep[0m  [144/169], [94mLoss[0m : 2.45125
[1mStep[0m  [160/169], [94mLoss[0m : 2.31416

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47815
[1mStep[0m  [16/169], [94mLoss[0m : 2.42867
[1mStep[0m  [32/169], [94mLoss[0m : 2.36500
[1mStep[0m  [48/169], [94mLoss[0m : 2.27767
[1mStep[0m  [64/169], [94mLoss[0m : 2.12756
[1mStep[0m  [80/169], [94mLoss[0m : 2.27642
[1mStep[0m  [96/169], [94mLoss[0m : 2.74811
[1mStep[0m  [112/169], [94mLoss[0m : 2.14687
[1mStep[0m  [128/169], [94mLoss[0m : 2.03150
[1mStep[0m  [144/169], [94mLoss[0m : 2.31883
[1mStep[0m  [160/169], [94mLoss[0m : 2.60859

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.367, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59191
[1mStep[0m  [16/169], [94mLoss[0m : 2.55356
[1mStep[0m  [32/169], [94mLoss[0m : 2.45449
[1mStep[0m  [48/169], [94mLoss[0m : 2.66723
[1mStep[0m  [64/169], [94mLoss[0m : 2.97138
[1mStep[0m  [80/169], [94mLoss[0m : 2.13189
[1mStep[0m  [96/169], [94mLoss[0m : 2.13536
[1mStep[0m  [112/169], [94mLoss[0m : 2.78245
[1mStep[0m  [128/169], [94mLoss[0m : 2.29060
[1mStep[0m  [144/169], [94mLoss[0m : 2.20344
[1mStep[0m  [160/169], [94mLoss[0m : 2.11784

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.47369
[1mStep[0m  [16/169], [94mLoss[0m : 2.04345
[1mStep[0m  [32/169], [94mLoss[0m : 2.06335
[1mStep[0m  [48/169], [94mLoss[0m : 2.63697
[1mStep[0m  [64/169], [94mLoss[0m : 2.18361
[1mStep[0m  [80/169], [94mLoss[0m : 2.30184
[1mStep[0m  [96/169], [94mLoss[0m : 2.36162
[1mStep[0m  [112/169], [94mLoss[0m : 1.92690
[1mStep[0m  [128/169], [94mLoss[0m : 2.04117
[1mStep[0m  [144/169], [94mLoss[0m : 2.64600
[1mStep[0m  [160/169], [94mLoss[0m : 2.30249

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.372, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41322
[1mStep[0m  [16/169], [94mLoss[0m : 2.29696
[1mStep[0m  [32/169], [94mLoss[0m : 1.97733
[1mStep[0m  [48/169], [94mLoss[0m : 2.05152
[1mStep[0m  [64/169], [94mLoss[0m : 2.25210
[1mStep[0m  [80/169], [94mLoss[0m : 2.21428
[1mStep[0m  [96/169], [94mLoss[0m : 2.38900
[1mStep[0m  [112/169], [94mLoss[0m : 2.47097
[1mStep[0m  [128/169], [94mLoss[0m : 2.57751
[1mStep[0m  [144/169], [94mLoss[0m : 1.94157
[1mStep[0m  [160/169], [94mLoss[0m : 2.37675

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46198
[1mStep[0m  [16/169], [94mLoss[0m : 2.44285
[1mStep[0m  [32/169], [94mLoss[0m : 2.25727
[1mStep[0m  [48/169], [94mLoss[0m : 2.35380
[1mStep[0m  [64/169], [94mLoss[0m : 2.54493
[1mStep[0m  [80/169], [94mLoss[0m : 2.32458
[1mStep[0m  [96/169], [94mLoss[0m : 1.94366
[1mStep[0m  [112/169], [94mLoss[0m : 2.69409
[1mStep[0m  [128/169], [94mLoss[0m : 2.16192
[1mStep[0m  [144/169], [94mLoss[0m : 2.56598
[1mStep[0m  [160/169], [94mLoss[0m : 2.62755

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.353, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62281
[1mStep[0m  [16/169], [94mLoss[0m : 2.20381
[1mStep[0m  [32/169], [94mLoss[0m : 2.14751
[1mStep[0m  [48/169], [94mLoss[0m : 2.79105
[1mStep[0m  [64/169], [94mLoss[0m : 2.16173
[1mStep[0m  [80/169], [94mLoss[0m : 2.16937
[1mStep[0m  [96/169], [94mLoss[0m : 2.18681
[1mStep[0m  [112/169], [94mLoss[0m : 2.70126
[1mStep[0m  [128/169], [94mLoss[0m : 2.24451
[1mStep[0m  [144/169], [94mLoss[0m : 2.29427
[1mStep[0m  [160/169], [94mLoss[0m : 2.27173

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.364, [92mTest[0m: 2.375, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28135
[1mStep[0m  [16/169], [94mLoss[0m : 2.51173
[1mStep[0m  [32/169], [94mLoss[0m : 2.30514
[1mStep[0m  [48/169], [94mLoss[0m : 2.25212
[1mStep[0m  [64/169], [94mLoss[0m : 2.59776
[1mStep[0m  [80/169], [94mLoss[0m : 2.40695
[1mStep[0m  [96/169], [94mLoss[0m : 2.16947
[1mStep[0m  [112/169], [94mLoss[0m : 2.38400
[1mStep[0m  [128/169], [94mLoss[0m : 2.35026
[1mStep[0m  [144/169], [94mLoss[0m : 1.87307
[1mStep[0m  [160/169], [94mLoss[0m : 2.15858

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62955
[1mStep[0m  [16/169], [94mLoss[0m : 2.33787
[1mStep[0m  [32/169], [94mLoss[0m : 2.32592
[1mStep[0m  [48/169], [94mLoss[0m : 1.89891
[1mStep[0m  [64/169], [94mLoss[0m : 2.43398
[1mStep[0m  [80/169], [94mLoss[0m : 2.84197
[1mStep[0m  [96/169], [94mLoss[0m : 2.44087
[1mStep[0m  [112/169], [94mLoss[0m : 1.99334
[1mStep[0m  [128/169], [94mLoss[0m : 2.56127
[1mStep[0m  [144/169], [94mLoss[0m : 2.21287
[1mStep[0m  [160/169], [94mLoss[0m : 2.36701

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.3257913440465927
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=300, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.41187
[1mStep[0m  [16/169], [94mLoss[0m : 2.57608
[1mStep[0m  [32/169], [94mLoss[0m : 2.60054
[1mStep[0m  [48/169], [94mLoss[0m : 2.45481
[1mStep[0m  [64/169], [94mLoss[0m : 2.31794
[1mStep[0m  [80/169], [94mLoss[0m : 2.72384
[1mStep[0m  [96/169], [94mLoss[0m : 1.99390
[1mStep[0m  [112/169], [94mLoss[0m : 2.32227
[1mStep[0m  [128/169], [94mLoss[0m : 2.73146
[1mStep[0m  [144/169], [94mLoss[0m : 2.12970
[1mStep[0m  [160/169], [94mLoss[0m : 2.45066

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13675
[1mStep[0m  [16/169], [94mLoss[0m : 2.22123
[1mStep[0m  [32/169], [94mLoss[0m : 2.20471
[1mStep[0m  [48/169], [94mLoss[0m : 2.14499
[1mStep[0m  [64/169], [94mLoss[0m : 2.00385
[1mStep[0m  [80/169], [94mLoss[0m : 2.14525
[1mStep[0m  [96/169], [94mLoss[0m : 2.71793
[1mStep[0m  [112/169], [94mLoss[0m : 2.00161
[1mStep[0m  [128/169], [94mLoss[0m : 2.90650
[1mStep[0m  [144/169], [94mLoss[0m : 2.14808
[1mStep[0m  [160/169], [94mLoss[0m : 2.31386

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.356, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27625
[1mStep[0m  [16/169], [94mLoss[0m : 2.66369
[1mStep[0m  [32/169], [94mLoss[0m : 2.08622
[1mStep[0m  [48/169], [94mLoss[0m : 2.33768
[1mStep[0m  [64/169], [94mLoss[0m : 2.35504
[1mStep[0m  [80/169], [94mLoss[0m : 2.05326
[1mStep[0m  [96/169], [94mLoss[0m : 2.43784
[1mStep[0m  [112/169], [94mLoss[0m : 2.66368
[1mStep[0m  [128/169], [94mLoss[0m : 2.20842
[1mStep[0m  [144/169], [94mLoss[0m : 2.05436
[1mStep[0m  [160/169], [94mLoss[0m : 2.44450

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.279, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08462
[1mStep[0m  [16/169], [94mLoss[0m : 2.18451
[1mStep[0m  [32/169], [94mLoss[0m : 2.43959
[1mStep[0m  [48/169], [94mLoss[0m : 2.05906
[1mStep[0m  [64/169], [94mLoss[0m : 1.94785
[1mStep[0m  [80/169], [94mLoss[0m : 2.27182
[1mStep[0m  [96/169], [94mLoss[0m : 2.46716
[1mStep[0m  [112/169], [94mLoss[0m : 2.87646
[1mStep[0m  [128/169], [94mLoss[0m : 2.09006
[1mStep[0m  [144/169], [94mLoss[0m : 1.84068
[1mStep[0m  [160/169], [94mLoss[0m : 2.08134

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.195, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97326
[1mStep[0m  [16/169], [94mLoss[0m : 1.84519
[1mStep[0m  [32/169], [94mLoss[0m : 2.26171
[1mStep[0m  [48/169], [94mLoss[0m : 2.38963
[1mStep[0m  [64/169], [94mLoss[0m : 2.01918
[1mStep[0m  [80/169], [94mLoss[0m : 2.00003
[1mStep[0m  [96/169], [94mLoss[0m : 2.08286
[1mStep[0m  [112/169], [94mLoss[0m : 2.39859
[1mStep[0m  [128/169], [94mLoss[0m : 2.21884
[1mStep[0m  [144/169], [94mLoss[0m : 2.26685
[1mStep[0m  [160/169], [94mLoss[0m : 2.33316

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.141, [92mTest[0m: 2.448, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33458
[1mStep[0m  [16/169], [94mLoss[0m : 1.92918
[1mStep[0m  [32/169], [94mLoss[0m : 2.35474
[1mStep[0m  [48/169], [94mLoss[0m : 1.95916
[1mStep[0m  [64/169], [94mLoss[0m : 2.55021
[1mStep[0m  [80/169], [94mLoss[0m : 2.15280
[1mStep[0m  [96/169], [94mLoss[0m : 1.91414
[1mStep[0m  [112/169], [94mLoss[0m : 2.51386
[1mStep[0m  [128/169], [94mLoss[0m : 2.10625
[1mStep[0m  [144/169], [94mLoss[0m : 2.19797
[1mStep[0m  [160/169], [94mLoss[0m : 2.42919

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21665
[1mStep[0m  [16/169], [94mLoss[0m : 1.97798
[1mStep[0m  [32/169], [94mLoss[0m : 1.97363
[1mStep[0m  [48/169], [94mLoss[0m : 1.73195
[1mStep[0m  [64/169], [94mLoss[0m : 1.91808
[1mStep[0m  [80/169], [94mLoss[0m : 2.19210
[1mStep[0m  [96/169], [94mLoss[0m : 2.18907
[1mStep[0m  [112/169], [94mLoss[0m : 2.20605
[1mStep[0m  [128/169], [94mLoss[0m : 1.98239
[1mStep[0m  [144/169], [94mLoss[0m : 2.06922
[1mStep[0m  [160/169], [94mLoss[0m : 2.27816

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.048, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15455
[1mStep[0m  [16/169], [94mLoss[0m : 1.68959
[1mStep[0m  [32/169], [94mLoss[0m : 2.10928
[1mStep[0m  [48/169], [94mLoss[0m : 1.96757
[1mStep[0m  [64/169], [94mLoss[0m : 1.72742
[1mStep[0m  [80/169], [94mLoss[0m : 2.01787
[1mStep[0m  [96/169], [94mLoss[0m : 2.21547
[1mStep[0m  [112/169], [94mLoss[0m : 1.49291
[1mStep[0m  [128/169], [94mLoss[0m : 2.31899
[1mStep[0m  [144/169], [94mLoss[0m : 2.19891
[1mStep[0m  [160/169], [94mLoss[0m : 1.86323

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78606
[1mStep[0m  [16/169], [94mLoss[0m : 1.43795
[1mStep[0m  [32/169], [94mLoss[0m : 1.84525
[1mStep[0m  [48/169], [94mLoss[0m : 1.93854
[1mStep[0m  [64/169], [94mLoss[0m : 2.14790
[1mStep[0m  [80/169], [94mLoss[0m : 1.66679
[1mStep[0m  [96/169], [94mLoss[0m : 2.20582
[1mStep[0m  [112/169], [94mLoss[0m : 1.92751
[1mStep[0m  [128/169], [94mLoss[0m : 1.99372
[1mStep[0m  [144/169], [94mLoss[0m : 1.90842
[1mStep[0m  [160/169], [94mLoss[0m : 1.95321

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.932, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54382
[1mStep[0m  [16/169], [94mLoss[0m : 1.83150
[1mStep[0m  [32/169], [94mLoss[0m : 1.51953
[1mStep[0m  [48/169], [94mLoss[0m : 1.97594
[1mStep[0m  [64/169], [94mLoss[0m : 1.90015
[1mStep[0m  [80/169], [94mLoss[0m : 2.11842
[1mStep[0m  [96/169], [94mLoss[0m : 2.01691
[1mStep[0m  [112/169], [94mLoss[0m : 2.01814
[1mStep[0m  [128/169], [94mLoss[0m : 1.51793
[1mStep[0m  [144/169], [94mLoss[0m : 1.57119
[1mStep[0m  [160/169], [94mLoss[0m : 1.77787

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07678
[1mStep[0m  [16/169], [94mLoss[0m : 1.90169
[1mStep[0m  [32/169], [94mLoss[0m : 1.79645
[1mStep[0m  [48/169], [94mLoss[0m : 1.61890
[1mStep[0m  [64/169], [94mLoss[0m : 2.21218
[1mStep[0m  [80/169], [94mLoss[0m : 2.19064
[1mStep[0m  [96/169], [94mLoss[0m : 1.76942
[1mStep[0m  [112/169], [94mLoss[0m : 1.90085
[1mStep[0m  [128/169], [94mLoss[0m : 1.64901
[1mStep[0m  [144/169], [94mLoss[0m : 1.73630
[1mStep[0m  [160/169], [94mLoss[0m : 1.86858

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83198
[1mStep[0m  [16/169], [94mLoss[0m : 1.68011
[1mStep[0m  [32/169], [94mLoss[0m : 1.80300
[1mStep[0m  [48/169], [94mLoss[0m : 1.98368
[1mStep[0m  [64/169], [94mLoss[0m : 1.62636
[1mStep[0m  [80/169], [94mLoss[0m : 1.83896
[1mStep[0m  [96/169], [94mLoss[0m : 2.27392
[1mStep[0m  [112/169], [94mLoss[0m : 1.64659
[1mStep[0m  [128/169], [94mLoss[0m : 1.75141
[1mStep[0m  [144/169], [94mLoss[0m : 1.87393
[1mStep[0m  [160/169], [94mLoss[0m : 2.19293

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.475, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.43983
[1mStep[0m  [16/169], [94mLoss[0m : 1.56949
[1mStep[0m  [32/169], [94mLoss[0m : 1.91497
[1mStep[0m  [48/169], [94mLoss[0m : 1.81638
[1mStep[0m  [64/169], [94mLoss[0m : 1.90341
[1mStep[0m  [80/169], [94mLoss[0m : 1.51055
[1mStep[0m  [96/169], [94mLoss[0m : 1.68279
[1mStep[0m  [112/169], [94mLoss[0m : 1.96747
[1mStep[0m  [128/169], [94mLoss[0m : 1.68900
[1mStep[0m  [144/169], [94mLoss[0m : 1.48621
[1mStep[0m  [160/169], [94mLoss[0m : 1.84384

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.731, [92mTest[0m: 2.492, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34764
[1mStep[0m  [16/169], [94mLoss[0m : 1.33278
[1mStep[0m  [32/169], [94mLoss[0m : 2.06277
[1mStep[0m  [48/169], [94mLoss[0m : 2.00693
[1mStep[0m  [64/169], [94mLoss[0m : 2.00629
[1mStep[0m  [80/169], [94mLoss[0m : 1.56591
[1mStep[0m  [96/169], [94mLoss[0m : 1.55866
[1mStep[0m  [112/169], [94mLoss[0m : 1.63887
[1mStep[0m  [128/169], [94mLoss[0m : 1.54635
[1mStep[0m  [144/169], [94mLoss[0m : 1.50389
[1mStep[0m  [160/169], [94mLoss[0m : 1.90811

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.491, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57297
[1mStep[0m  [16/169], [94mLoss[0m : 1.86892
[1mStep[0m  [32/169], [94mLoss[0m : 1.37813
[1mStep[0m  [48/169], [94mLoss[0m : 1.34993
[1mStep[0m  [64/169], [94mLoss[0m : 1.83607
[1mStep[0m  [80/169], [94mLoss[0m : 1.51643
[1mStep[0m  [96/169], [94mLoss[0m : 1.66056
[1mStep[0m  [112/169], [94mLoss[0m : 1.50163
[1mStep[0m  [128/169], [94mLoss[0m : 1.84070
[1mStep[0m  [144/169], [94mLoss[0m : 1.57187
[1mStep[0m  [160/169], [94mLoss[0m : 2.17182

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.528, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41368
[1mStep[0m  [16/169], [94mLoss[0m : 1.35997
[1mStep[0m  [32/169], [94mLoss[0m : 1.53602
[1mStep[0m  [48/169], [94mLoss[0m : 1.76629
[1mStep[0m  [64/169], [94mLoss[0m : 1.47918
[1mStep[0m  [80/169], [94mLoss[0m : 1.48310
[1mStep[0m  [96/169], [94mLoss[0m : 1.53342
[1mStep[0m  [112/169], [94mLoss[0m : 1.52507
[1mStep[0m  [128/169], [94mLoss[0m : 1.41388
[1mStep[0m  [144/169], [94mLoss[0m : 1.25840
[1mStep[0m  [160/169], [94mLoss[0m : 1.68194

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.636, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60460
[1mStep[0m  [16/169], [94mLoss[0m : 1.39219
[1mStep[0m  [32/169], [94mLoss[0m : 1.34432
[1mStep[0m  [48/169], [94mLoss[0m : 1.59556
[1mStep[0m  [64/169], [94mLoss[0m : 1.23366
[1mStep[0m  [80/169], [94mLoss[0m : 1.49897
[1mStep[0m  [96/169], [94mLoss[0m : 1.56122
[1mStep[0m  [112/169], [94mLoss[0m : 1.98583
[1mStep[0m  [128/169], [94mLoss[0m : 1.72637
[1mStep[0m  [144/169], [94mLoss[0m : 1.40704
[1mStep[0m  [160/169], [94mLoss[0m : 1.53857

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54122
[1mStep[0m  [16/169], [94mLoss[0m : 1.87461
[1mStep[0m  [32/169], [94mLoss[0m : 1.18223
[1mStep[0m  [48/169], [94mLoss[0m : 1.58054
[1mStep[0m  [64/169], [94mLoss[0m : 1.83321
[1mStep[0m  [80/169], [94mLoss[0m : 1.56298
[1mStep[0m  [96/169], [94mLoss[0m : 1.38486
[1mStep[0m  [112/169], [94mLoss[0m : 1.59074
[1mStep[0m  [128/169], [94mLoss[0m : 1.66155
[1mStep[0m  [144/169], [94mLoss[0m : 1.35916
[1mStep[0m  [160/169], [94mLoss[0m : 1.40192

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.535, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75234
[1mStep[0m  [16/169], [94mLoss[0m : 1.38131
[1mStep[0m  [32/169], [94mLoss[0m : 1.09231
[1mStep[0m  [48/169], [94mLoss[0m : 1.64630
[1mStep[0m  [64/169], [94mLoss[0m : 1.36119
[1mStep[0m  [80/169], [94mLoss[0m : 1.51402
[1mStep[0m  [96/169], [94mLoss[0m : 1.67693
[1mStep[0m  [112/169], [94mLoss[0m : 1.29407
[1mStep[0m  [128/169], [94mLoss[0m : 1.70047
[1mStep[0m  [144/169], [94mLoss[0m : 1.53423
[1mStep[0m  [160/169], [94mLoss[0m : 1.76118

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51086
[1mStep[0m  [16/169], [94mLoss[0m : 1.84678
[1mStep[0m  [32/169], [94mLoss[0m : 1.45725
[1mStep[0m  [48/169], [94mLoss[0m : 1.45395
[1mStep[0m  [64/169], [94mLoss[0m : 1.46081
[1mStep[0m  [80/169], [94mLoss[0m : 1.61946
[1mStep[0m  [96/169], [94mLoss[0m : 1.53296
[1mStep[0m  [112/169], [94mLoss[0m : 1.45870
[1mStep[0m  [128/169], [94mLoss[0m : 1.69126
[1mStep[0m  [144/169], [94mLoss[0m : 1.45048
[1mStep[0m  [160/169], [94mLoss[0m : 1.94259

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.558, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.27267
[1mStep[0m  [16/169], [94mLoss[0m : 1.26131
[1mStep[0m  [32/169], [94mLoss[0m : 1.49317
[1mStep[0m  [48/169], [94mLoss[0m : 1.76845
[1mStep[0m  [64/169], [94mLoss[0m : 1.30596
[1mStep[0m  [80/169], [94mLoss[0m : 1.42565
[1mStep[0m  [96/169], [94mLoss[0m : 1.52818
[1mStep[0m  [112/169], [94mLoss[0m : 1.29213
[1mStep[0m  [128/169], [94mLoss[0m : 1.05806
[1mStep[0m  [144/169], [94mLoss[0m : 1.47613
[1mStep[0m  [160/169], [94mLoss[0m : 1.49994

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.444, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49584
[1mStep[0m  [16/169], [94mLoss[0m : 1.64380
[1mStep[0m  [32/169], [94mLoss[0m : 1.23327
[1mStep[0m  [48/169], [94mLoss[0m : 1.16792
[1mStep[0m  [64/169], [94mLoss[0m : 1.45475
[1mStep[0m  [80/169], [94mLoss[0m : 1.62888
[1mStep[0m  [96/169], [94mLoss[0m : 1.23012
[1mStep[0m  [112/169], [94mLoss[0m : 1.49307
[1mStep[0m  [128/169], [94mLoss[0m : 1.27549
[1mStep[0m  [144/169], [94mLoss[0m : 1.59298
[1mStep[0m  [160/169], [94mLoss[0m : 1.56633

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.418, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28699
[1mStep[0m  [16/169], [94mLoss[0m : 1.38229
[1mStep[0m  [32/169], [94mLoss[0m : 1.43260
[1mStep[0m  [48/169], [94mLoss[0m : 1.50650
[1mStep[0m  [64/169], [94mLoss[0m : 1.46312
[1mStep[0m  [80/169], [94mLoss[0m : 1.76935
[1mStep[0m  [96/169], [94mLoss[0m : 1.38398
[1mStep[0m  [112/169], [94mLoss[0m : 1.47471
[1mStep[0m  [128/169], [94mLoss[0m : 1.28442
[1mStep[0m  [144/169], [94mLoss[0m : 1.44959
[1mStep[0m  [160/169], [94mLoss[0m : 1.42748

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.409, [92mTest[0m: 2.483, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41339
[1mStep[0m  [16/169], [94mLoss[0m : 1.35676
[1mStep[0m  [32/169], [94mLoss[0m : 1.28895
[1mStep[0m  [48/169], [94mLoss[0m : 1.45600
[1mStep[0m  [64/169], [94mLoss[0m : 1.49188
[1mStep[0m  [80/169], [94mLoss[0m : 1.32760
[1mStep[0m  [96/169], [94mLoss[0m : 1.36398
[1mStep[0m  [112/169], [94mLoss[0m : 1.11330
[1mStep[0m  [128/169], [94mLoss[0m : 1.29153
[1mStep[0m  [144/169], [94mLoss[0m : 1.31201
[1mStep[0m  [160/169], [94mLoss[0m : 1.78988

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.371, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 23 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.486
====================================

Phase 2 - Evaluation MAE:  2.4861880157675063
MAE score P1      2.325791
MAE score P2      2.486188
loss              1.371185
learning_rate     0.007525
batch_size              64
hidden_sizes         [300]
epochs                  30
activation            tanh
optimizer              sgd
early stopping        True
dropout                0.2
momentum               0.9
weight_decay        0.0001
Name: 4, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.27772
[1mStep[0m  [33/339], [94mLoss[0m : 3.97252
[1mStep[0m  [66/339], [94mLoss[0m : 2.74076
[1mStep[0m  [99/339], [94mLoss[0m : 2.83826
[1mStep[0m  [132/339], [94mLoss[0m : 2.38551
[1mStep[0m  [165/339], [94mLoss[0m : 3.10648
[1mStep[0m  [198/339], [94mLoss[0m : 2.41112
[1mStep[0m  [231/339], [94mLoss[0m : 2.81496
[1mStep[0m  [264/339], [94mLoss[0m : 2.39886
[1mStep[0m  [297/339], [94mLoss[0m : 2.65665
[1mStep[0m  [330/339], [94mLoss[0m : 2.78978

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.118, [92mTest[0m: 11.029, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62298
[1mStep[0m  [33/339], [94mLoss[0m : 2.73188
[1mStep[0m  [66/339], [94mLoss[0m : 2.88646
[1mStep[0m  [99/339], [94mLoss[0m : 2.63985
[1mStep[0m  [132/339], [94mLoss[0m : 2.13087
[1mStep[0m  [165/339], [94mLoss[0m : 2.55470
[1mStep[0m  [198/339], [94mLoss[0m : 1.99408
[1mStep[0m  [231/339], [94mLoss[0m : 2.33815
[1mStep[0m  [264/339], [94mLoss[0m : 2.39860
[1mStep[0m  [297/339], [94mLoss[0m : 2.29467
[1mStep[0m  [330/339], [94mLoss[0m : 2.66399

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56123
[1mStep[0m  [33/339], [94mLoss[0m : 2.09915
[1mStep[0m  [66/339], [94mLoss[0m : 2.47358
[1mStep[0m  [99/339], [94mLoss[0m : 2.59100
[1mStep[0m  [132/339], [94mLoss[0m : 2.00981
[1mStep[0m  [165/339], [94mLoss[0m : 2.56267
[1mStep[0m  [198/339], [94mLoss[0m : 2.59387
[1mStep[0m  [231/339], [94mLoss[0m : 2.44070
[1mStep[0m  [264/339], [94mLoss[0m : 2.63355
[1mStep[0m  [297/339], [94mLoss[0m : 2.79665
[1mStep[0m  [330/339], [94mLoss[0m : 2.66122

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33478
[1mStep[0m  [33/339], [94mLoss[0m : 2.33436
[1mStep[0m  [66/339], [94mLoss[0m : 2.58627
[1mStep[0m  [99/339], [94mLoss[0m : 2.55787
[1mStep[0m  [132/339], [94mLoss[0m : 2.46207
[1mStep[0m  [165/339], [94mLoss[0m : 2.81245
[1mStep[0m  [198/339], [94mLoss[0m : 2.50405
[1mStep[0m  [231/339], [94mLoss[0m : 2.44353
[1mStep[0m  [264/339], [94mLoss[0m : 2.80430
[1mStep[0m  [297/339], [94mLoss[0m : 2.27897
[1mStep[0m  [330/339], [94mLoss[0m : 2.62749

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81384
[1mStep[0m  [33/339], [94mLoss[0m : 2.30672
[1mStep[0m  [66/339], [94mLoss[0m : 2.37941
[1mStep[0m  [99/339], [94mLoss[0m : 2.25477
[1mStep[0m  [132/339], [94mLoss[0m : 2.35082
[1mStep[0m  [165/339], [94mLoss[0m : 2.66666
[1mStep[0m  [198/339], [94mLoss[0m : 2.74592
[1mStep[0m  [231/339], [94mLoss[0m : 1.96419
[1mStep[0m  [264/339], [94mLoss[0m : 2.63509
[1mStep[0m  [297/339], [94mLoss[0m : 3.33954
[1mStep[0m  [330/339], [94mLoss[0m : 2.80026

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.23207
[1mStep[0m  [33/339], [94mLoss[0m : 1.99274
[1mStep[0m  [66/339], [94mLoss[0m : 2.45732
[1mStep[0m  [99/339], [94mLoss[0m : 2.22982
[1mStep[0m  [132/339], [94mLoss[0m : 3.06237
[1mStep[0m  [165/339], [94mLoss[0m : 2.92261
[1mStep[0m  [198/339], [94mLoss[0m : 1.97275
[1mStep[0m  [231/339], [94mLoss[0m : 2.36140
[1mStep[0m  [264/339], [94mLoss[0m : 2.72836
[1mStep[0m  [297/339], [94mLoss[0m : 2.57999
[1mStep[0m  [330/339], [94mLoss[0m : 3.05754

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32913
[1mStep[0m  [33/339], [94mLoss[0m : 2.74563
[1mStep[0m  [66/339], [94mLoss[0m : 2.07387
[1mStep[0m  [99/339], [94mLoss[0m : 2.35508
[1mStep[0m  [132/339], [94mLoss[0m : 3.04296
[1mStep[0m  [165/339], [94mLoss[0m : 3.06575
[1mStep[0m  [198/339], [94mLoss[0m : 2.34613
[1mStep[0m  [231/339], [94mLoss[0m : 1.78787
[1mStep[0m  [264/339], [94mLoss[0m : 2.46821
[1mStep[0m  [297/339], [94mLoss[0m : 2.28544
[1mStep[0m  [330/339], [94mLoss[0m : 2.40980

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04304
[1mStep[0m  [33/339], [94mLoss[0m : 2.54766
[1mStep[0m  [66/339], [94mLoss[0m : 2.47874
[1mStep[0m  [99/339], [94mLoss[0m : 2.09600
[1mStep[0m  [132/339], [94mLoss[0m : 2.29590
[1mStep[0m  [165/339], [94mLoss[0m : 2.31267
[1mStep[0m  [198/339], [94mLoss[0m : 2.20259
[1mStep[0m  [231/339], [94mLoss[0m : 2.60044
[1mStep[0m  [264/339], [94mLoss[0m : 2.38129
[1mStep[0m  [297/339], [94mLoss[0m : 2.04844
[1mStep[0m  [330/339], [94mLoss[0m : 2.51253

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70322
[1mStep[0m  [33/339], [94mLoss[0m : 2.35764
[1mStep[0m  [66/339], [94mLoss[0m : 2.63809
[1mStep[0m  [99/339], [94mLoss[0m : 2.50571
[1mStep[0m  [132/339], [94mLoss[0m : 1.51120
[1mStep[0m  [165/339], [94mLoss[0m : 2.58577
[1mStep[0m  [198/339], [94mLoss[0m : 3.40979
[1mStep[0m  [231/339], [94mLoss[0m : 3.17437
[1mStep[0m  [264/339], [94mLoss[0m : 2.81162
[1mStep[0m  [297/339], [94mLoss[0m : 2.39749
[1mStep[0m  [330/339], [94mLoss[0m : 2.28886

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.01226
[1mStep[0m  [33/339], [94mLoss[0m : 2.41067
[1mStep[0m  [66/339], [94mLoss[0m : 2.78396
[1mStep[0m  [99/339], [94mLoss[0m : 2.25789
[1mStep[0m  [132/339], [94mLoss[0m : 2.47436
[1mStep[0m  [165/339], [94mLoss[0m : 2.61792
[1mStep[0m  [198/339], [94mLoss[0m : 2.16468
[1mStep[0m  [231/339], [94mLoss[0m : 2.15669
[1mStep[0m  [264/339], [94mLoss[0m : 2.20049
[1mStep[0m  [297/339], [94mLoss[0m : 2.97015
[1mStep[0m  [330/339], [94mLoss[0m : 2.40229

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.00893
[1mStep[0m  [33/339], [94mLoss[0m : 2.39163
[1mStep[0m  [66/339], [94mLoss[0m : 2.49479
[1mStep[0m  [99/339], [94mLoss[0m : 2.78838
[1mStep[0m  [132/339], [94mLoss[0m : 2.28504
[1mStep[0m  [165/339], [94mLoss[0m : 2.41941
[1mStep[0m  [198/339], [94mLoss[0m : 2.35809
[1mStep[0m  [231/339], [94mLoss[0m : 2.45108
[1mStep[0m  [264/339], [94mLoss[0m : 2.28363
[1mStep[0m  [297/339], [94mLoss[0m : 1.95343
[1mStep[0m  [330/339], [94mLoss[0m : 2.88117

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88092
[1mStep[0m  [33/339], [94mLoss[0m : 2.37953
[1mStep[0m  [66/339], [94mLoss[0m : 2.14810
[1mStep[0m  [99/339], [94mLoss[0m : 2.65699
[1mStep[0m  [132/339], [94mLoss[0m : 2.55138
[1mStep[0m  [165/339], [94mLoss[0m : 3.16882
[1mStep[0m  [198/339], [94mLoss[0m : 2.51845
[1mStep[0m  [231/339], [94mLoss[0m : 2.33617
[1mStep[0m  [264/339], [94mLoss[0m : 2.51542
[1mStep[0m  [297/339], [94mLoss[0m : 2.61978
[1mStep[0m  [330/339], [94mLoss[0m : 2.04804

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.03790
[1mStep[0m  [33/339], [94mLoss[0m : 2.98568
[1mStep[0m  [66/339], [94mLoss[0m : 2.63456
[1mStep[0m  [99/339], [94mLoss[0m : 2.51797
[1mStep[0m  [132/339], [94mLoss[0m : 2.19035
[1mStep[0m  [165/339], [94mLoss[0m : 2.42360
[1mStep[0m  [198/339], [94mLoss[0m : 2.42822
[1mStep[0m  [231/339], [94mLoss[0m : 2.93011
[1mStep[0m  [264/339], [94mLoss[0m : 1.81135
[1mStep[0m  [297/339], [94mLoss[0m : 2.38652
[1mStep[0m  [330/339], [94mLoss[0m : 2.13660

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20516
[1mStep[0m  [33/339], [94mLoss[0m : 2.87030
[1mStep[0m  [66/339], [94mLoss[0m : 3.12882
[1mStep[0m  [99/339], [94mLoss[0m : 2.63941
[1mStep[0m  [132/339], [94mLoss[0m : 2.62815
[1mStep[0m  [165/339], [94mLoss[0m : 2.71391
[1mStep[0m  [198/339], [94mLoss[0m : 2.58535
[1mStep[0m  [231/339], [94mLoss[0m : 2.36541
[1mStep[0m  [264/339], [94mLoss[0m : 2.44272
[1mStep[0m  [297/339], [94mLoss[0m : 2.12346
[1mStep[0m  [330/339], [94mLoss[0m : 2.50609

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.319, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93294
[1mStep[0m  [33/339], [94mLoss[0m : 1.94856
[1mStep[0m  [66/339], [94mLoss[0m : 2.09155
[1mStep[0m  [99/339], [94mLoss[0m : 2.87734
[1mStep[0m  [132/339], [94mLoss[0m : 2.46870
[1mStep[0m  [165/339], [94mLoss[0m : 2.35043
[1mStep[0m  [198/339], [94mLoss[0m : 2.00265
[1mStep[0m  [231/339], [94mLoss[0m : 1.99449
[1mStep[0m  [264/339], [94mLoss[0m : 2.80405
[1mStep[0m  [297/339], [94mLoss[0m : 2.87569
[1mStep[0m  [330/339], [94mLoss[0m : 1.97679

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.22955
[1mStep[0m  [33/339], [94mLoss[0m : 2.16403
[1mStep[0m  [66/339], [94mLoss[0m : 2.63037
[1mStep[0m  [99/339], [94mLoss[0m : 2.28542
[1mStep[0m  [132/339], [94mLoss[0m : 2.25176
[1mStep[0m  [165/339], [94mLoss[0m : 2.29925
[1mStep[0m  [198/339], [94mLoss[0m : 2.29722
[1mStep[0m  [231/339], [94mLoss[0m : 2.33643
[1mStep[0m  [264/339], [94mLoss[0m : 2.47940
[1mStep[0m  [297/339], [94mLoss[0m : 2.50181
[1mStep[0m  [330/339], [94mLoss[0m : 2.04271

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39735
[1mStep[0m  [33/339], [94mLoss[0m : 2.53205
[1mStep[0m  [66/339], [94mLoss[0m : 2.57245
[1mStep[0m  [99/339], [94mLoss[0m : 2.59049
[1mStep[0m  [132/339], [94mLoss[0m : 2.30186
[1mStep[0m  [165/339], [94mLoss[0m : 2.59035
[1mStep[0m  [198/339], [94mLoss[0m : 1.96193
[1mStep[0m  [231/339], [94mLoss[0m : 2.52513
[1mStep[0m  [264/339], [94mLoss[0m : 2.14786
[1mStep[0m  [297/339], [94mLoss[0m : 2.41150
[1mStep[0m  [330/339], [94mLoss[0m : 2.54337

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.43196
[1mStep[0m  [33/339], [94mLoss[0m : 2.74016
[1mStep[0m  [66/339], [94mLoss[0m : 1.97259
[1mStep[0m  [99/339], [94mLoss[0m : 2.46952
[1mStep[0m  [132/339], [94mLoss[0m : 2.74854
[1mStep[0m  [165/339], [94mLoss[0m : 3.26950
[1mStep[0m  [198/339], [94mLoss[0m : 2.60982
[1mStep[0m  [231/339], [94mLoss[0m : 2.39937
[1mStep[0m  [264/339], [94mLoss[0m : 2.69721
[1mStep[0m  [297/339], [94mLoss[0m : 2.41020
[1mStep[0m  [330/339], [94mLoss[0m : 2.08490

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39960
[1mStep[0m  [33/339], [94mLoss[0m : 2.56160
[1mStep[0m  [66/339], [94mLoss[0m : 2.49291
[1mStep[0m  [99/339], [94mLoss[0m : 2.83458
[1mStep[0m  [132/339], [94mLoss[0m : 2.48490
[1mStep[0m  [165/339], [94mLoss[0m : 2.64663
[1mStep[0m  [198/339], [94mLoss[0m : 1.58981
[1mStep[0m  [231/339], [94mLoss[0m : 2.72801
[1mStep[0m  [264/339], [94mLoss[0m : 2.17597
[1mStep[0m  [297/339], [94mLoss[0m : 3.31625
[1mStep[0m  [330/339], [94mLoss[0m : 1.48472

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47877
[1mStep[0m  [33/339], [94mLoss[0m : 2.57365
[1mStep[0m  [66/339], [94mLoss[0m : 2.03535
[1mStep[0m  [99/339], [94mLoss[0m : 1.93321
[1mStep[0m  [132/339], [94mLoss[0m : 2.50425
[1mStep[0m  [165/339], [94mLoss[0m : 2.42287
[1mStep[0m  [198/339], [94mLoss[0m : 2.09401
[1mStep[0m  [231/339], [94mLoss[0m : 3.50088
[1mStep[0m  [264/339], [94mLoss[0m : 2.38123
[1mStep[0m  [297/339], [94mLoss[0m : 3.38382
[1mStep[0m  [330/339], [94mLoss[0m : 2.33188

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.63026
[1mStep[0m  [33/339], [94mLoss[0m : 2.90197
[1mStep[0m  [66/339], [94mLoss[0m : 2.30286
[1mStep[0m  [99/339], [94mLoss[0m : 1.87444
[1mStep[0m  [132/339], [94mLoss[0m : 2.16427
[1mStep[0m  [165/339], [94mLoss[0m : 2.78075
[1mStep[0m  [198/339], [94mLoss[0m : 2.04097
[1mStep[0m  [231/339], [94mLoss[0m : 3.24849
[1mStep[0m  [264/339], [94mLoss[0m : 2.82705
[1mStep[0m  [297/339], [94mLoss[0m : 2.37510
[1mStep[0m  [330/339], [94mLoss[0m : 2.51731

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.99650
[1mStep[0m  [33/339], [94mLoss[0m : 2.16444
[1mStep[0m  [66/339], [94mLoss[0m : 2.28786
[1mStep[0m  [99/339], [94mLoss[0m : 2.75960
[1mStep[0m  [132/339], [94mLoss[0m : 2.69419
[1mStep[0m  [165/339], [94mLoss[0m : 2.74990
[1mStep[0m  [198/339], [94mLoss[0m : 2.44764
[1mStep[0m  [231/339], [94mLoss[0m : 2.14771
[1mStep[0m  [264/339], [94mLoss[0m : 2.63509
[1mStep[0m  [297/339], [94mLoss[0m : 1.80059
[1mStep[0m  [330/339], [94mLoss[0m : 2.57284

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67948
[1mStep[0m  [33/339], [94mLoss[0m : 2.63895
[1mStep[0m  [66/339], [94mLoss[0m : 3.00106
[1mStep[0m  [99/339], [94mLoss[0m : 2.28366
[1mStep[0m  [132/339], [94mLoss[0m : 2.27147
[1mStep[0m  [165/339], [94mLoss[0m : 1.59540
[1mStep[0m  [198/339], [94mLoss[0m : 2.66049
[1mStep[0m  [231/339], [94mLoss[0m : 2.25971
[1mStep[0m  [264/339], [94mLoss[0m : 2.99022
[1mStep[0m  [297/339], [94mLoss[0m : 2.58614
[1mStep[0m  [330/339], [94mLoss[0m : 2.29833

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.60528
[1mStep[0m  [33/339], [94mLoss[0m : 2.05532
[1mStep[0m  [66/339], [94mLoss[0m : 2.40352
[1mStep[0m  [99/339], [94mLoss[0m : 2.67537
[1mStep[0m  [132/339], [94mLoss[0m : 2.49331
[1mStep[0m  [165/339], [94mLoss[0m : 2.73092
[1mStep[0m  [198/339], [94mLoss[0m : 2.32640
[1mStep[0m  [231/339], [94mLoss[0m : 2.25746
[1mStep[0m  [264/339], [94mLoss[0m : 2.11092
[1mStep[0m  [297/339], [94mLoss[0m : 2.16298
[1mStep[0m  [330/339], [94mLoss[0m : 2.99135

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.454, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.87677
[1mStep[0m  [33/339], [94mLoss[0m : 1.99153
[1mStep[0m  [66/339], [94mLoss[0m : 2.53016
[1mStep[0m  [99/339], [94mLoss[0m : 2.39084
[1mStep[0m  [132/339], [94mLoss[0m : 2.41493
[1mStep[0m  [165/339], [94mLoss[0m : 1.89652
[1mStep[0m  [198/339], [94mLoss[0m : 2.18290
[1mStep[0m  [231/339], [94mLoss[0m : 1.83129
[1mStep[0m  [264/339], [94mLoss[0m : 2.74574
[1mStep[0m  [297/339], [94mLoss[0m : 2.13777
[1mStep[0m  [330/339], [94mLoss[0m : 2.90173

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.19925
[1mStep[0m  [33/339], [94mLoss[0m : 2.94409
[1mStep[0m  [66/339], [94mLoss[0m : 2.50830
[1mStep[0m  [99/339], [94mLoss[0m : 2.09434
[1mStep[0m  [132/339], [94mLoss[0m : 2.69934
[1mStep[0m  [165/339], [94mLoss[0m : 2.62407
[1mStep[0m  [198/339], [94mLoss[0m : 2.32020
[1mStep[0m  [231/339], [94mLoss[0m : 1.96836
[1mStep[0m  [264/339], [94mLoss[0m : 2.51329
[1mStep[0m  [297/339], [94mLoss[0m : 3.05142
[1mStep[0m  [330/339], [94mLoss[0m : 1.84313

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17490
[1mStep[0m  [33/339], [94mLoss[0m : 3.00221
[1mStep[0m  [66/339], [94mLoss[0m : 2.72969
[1mStep[0m  [99/339], [94mLoss[0m : 2.39869
[1mStep[0m  [132/339], [94mLoss[0m : 2.15645
[1mStep[0m  [165/339], [94mLoss[0m : 2.57509
[1mStep[0m  [198/339], [94mLoss[0m : 2.37497
[1mStep[0m  [231/339], [94mLoss[0m : 2.61996
[1mStep[0m  [264/339], [94mLoss[0m : 2.72161
[1mStep[0m  [297/339], [94mLoss[0m : 2.31735
[1mStep[0m  [330/339], [94mLoss[0m : 2.90211

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21165
[1mStep[0m  [33/339], [94mLoss[0m : 2.58000
[1mStep[0m  [66/339], [94mLoss[0m : 2.76648
[1mStep[0m  [99/339], [94mLoss[0m : 2.69836
[1mStep[0m  [132/339], [94mLoss[0m : 2.03502
[1mStep[0m  [165/339], [94mLoss[0m : 3.25497
[1mStep[0m  [198/339], [94mLoss[0m : 2.08425
[1mStep[0m  [231/339], [94mLoss[0m : 2.26691
[1mStep[0m  [264/339], [94mLoss[0m : 2.56708
[1mStep[0m  [297/339], [94mLoss[0m : 2.58078
[1mStep[0m  [330/339], [94mLoss[0m : 1.77237

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.62786
[1mStep[0m  [33/339], [94mLoss[0m : 2.20275
[1mStep[0m  [66/339], [94mLoss[0m : 2.50859
[1mStep[0m  [99/339], [94mLoss[0m : 2.05703
[1mStep[0m  [132/339], [94mLoss[0m : 2.85711
[1mStep[0m  [165/339], [94mLoss[0m : 1.86336
[1mStep[0m  [198/339], [94mLoss[0m : 2.90367
[1mStep[0m  [231/339], [94mLoss[0m : 2.40014
[1mStep[0m  [264/339], [94mLoss[0m : 2.70336
[1mStep[0m  [297/339], [94mLoss[0m : 2.64459
[1mStep[0m  [330/339], [94mLoss[0m : 2.90775

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.15677
[1mStep[0m  [33/339], [94mLoss[0m : 2.74082
[1mStep[0m  [66/339], [94mLoss[0m : 2.12844
[1mStep[0m  [99/339], [94mLoss[0m : 2.43156
[1mStep[0m  [132/339], [94mLoss[0m : 2.68303
[1mStep[0m  [165/339], [94mLoss[0m : 2.81639
[1mStep[0m  [198/339], [94mLoss[0m : 2.15759
[1mStep[0m  [231/339], [94mLoss[0m : 2.76766
[1mStep[0m  [264/339], [94mLoss[0m : 2.71328
[1mStep[0m  [297/339], [94mLoss[0m : 3.30721
[1mStep[0m  [330/339], [94mLoss[0m : 2.77077

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.339
====================================

Phase 1 - Evaluation MAE:  2.3393994991758227
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 3.12756
[1mStep[0m  [33/339], [94mLoss[0m : 2.37817
[1mStep[0m  [66/339], [94mLoss[0m : 2.62330
[1mStep[0m  [99/339], [94mLoss[0m : 2.67758
[1mStep[0m  [132/339], [94mLoss[0m : 3.18360
[1mStep[0m  [165/339], [94mLoss[0m : 2.25809
[1mStep[0m  [198/339], [94mLoss[0m : 2.41288
[1mStep[0m  [231/339], [94mLoss[0m : 2.39301
[1mStep[0m  [264/339], [94mLoss[0m : 2.53918
[1mStep[0m  [297/339], [94mLoss[0m : 2.87654
[1mStep[0m  [330/339], [94mLoss[0m : 2.73345

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.25069
[1mStep[0m  [33/339], [94mLoss[0m : 2.39354
[1mStep[0m  [66/339], [94mLoss[0m : 2.48394
[1mStep[0m  [99/339], [94mLoss[0m : 2.43746
[1mStep[0m  [132/339], [94mLoss[0m : 2.02063
[1mStep[0m  [165/339], [94mLoss[0m : 2.65542
[1mStep[0m  [198/339], [94mLoss[0m : 3.32982
[1mStep[0m  [231/339], [94mLoss[0m : 2.38535
[1mStep[0m  [264/339], [94mLoss[0m : 2.58313
[1mStep[0m  [297/339], [94mLoss[0m : 2.46381
[1mStep[0m  [330/339], [94mLoss[0m : 2.87343

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.81233
[1mStep[0m  [33/339], [94mLoss[0m : 1.83164
[1mStep[0m  [66/339], [94mLoss[0m : 1.92758
[1mStep[0m  [99/339], [94mLoss[0m : 2.14108
[1mStep[0m  [132/339], [94mLoss[0m : 2.02261
[1mStep[0m  [165/339], [94mLoss[0m : 2.04403
[1mStep[0m  [198/339], [94mLoss[0m : 1.86068
[1mStep[0m  [231/339], [94mLoss[0m : 2.65555
[1mStep[0m  [264/339], [94mLoss[0m : 2.45372
[1mStep[0m  [297/339], [94mLoss[0m : 3.51326
[1mStep[0m  [330/339], [94mLoss[0m : 2.22373

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.335, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51870
[1mStep[0m  [33/339], [94mLoss[0m : 2.12676
[1mStep[0m  [66/339], [94mLoss[0m : 2.29365
[1mStep[0m  [99/339], [94mLoss[0m : 2.48578
[1mStep[0m  [132/339], [94mLoss[0m : 1.76512
[1mStep[0m  [165/339], [94mLoss[0m : 2.25299
[1mStep[0m  [198/339], [94mLoss[0m : 2.14414
[1mStep[0m  [231/339], [94mLoss[0m : 1.76016
[1mStep[0m  [264/339], [94mLoss[0m : 2.33380
[1mStep[0m  [297/339], [94mLoss[0m : 2.64477
[1mStep[0m  [330/339], [94mLoss[0m : 1.93824

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.268, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28363
[1mStep[0m  [33/339], [94mLoss[0m : 2.05712
[1mStep[0m  [66/339], [94mLoss[0m : 2.19294
[1mStep[0m  [99/339], [94mLoss[0m : 1.50643
[1mStep[0m  [132/339], [94mLoss[0m : 2.77558
[1mStep[0m  [165/339], [94mLoss[0m : 2.01799
[1mStep[0m  [198/339], [94mLoss[0m : 2.05098
[1mStep[0m  [231/339], [94mLoss[0m : 1.80638
[1mStep[0m  [264/339], [94mLoss[0m : 2.54132
[1mStep[0m  [297/339], [94mLoss[0m : 2.72961
[1mStep[0m  [330/339], [94mLoss[0m : 2.26255

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.42400
[1mStep[0m  [33/339], [94mLoss[0m : 2.68476
[1mStep[0m  [66/339], [94mLoss[0m : 2.08366
[1mStep[0m  [99/339], [94mLoss[0m : 2.35242
[1mStep[0m  [132/339], [94mLoss[0m : 2.19048
[1mStep[0m  [165/339], [94mLoss[0m : 1.89735
[1mStep[0m  [198/339], [94mLoss[0m : 2.53882
[1mStep[0m  [231/339], [94mLoss[0m : 2.09085
[1mStep[0m  [264/339], [94mLoss[0m : 1.93669
[1mStep[0m  [297/339], [94mLoss[0m : 2.79222
[1mStep[0m  [330/339], [94mLoss[0m : 2.42886

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.149, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57054
[1mStep[0m  [33/339], [94mLoss[0m : 1.82362
[1mStep[0m  [66/339], [94mLoss[0m : 2.28455
[1mStep[0m  [99/339], [94mLoss[0m : 2.17399
[1mStep[0m  [132/339], [94mLoss[0m : 1.99465
[1mStep[0m  [165/339], [94mLoss[0m : 2.66220
[1mStep[0m  [198/339], [94mLoss[0m : 2.30793
[1mStep[0m  [231/339], [94mLoss[0m : 2.56802
[1mStep[0m  [264/339], [94mLoss[0m : 1.91685
[1mStep[0m  [297/339], [94mLoss[0m : 1.67406
[1mStep[0m  [330/339], [94mLoss[0m : 2.60874

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.111, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04217
[1mStep[0m  [33/339], [94mLoss[0m : 2.29287
[1mStep[0m  [66/339], [94mLoss[0m : 2.07339
[1mStep[0m  [99/339], [94mLoss[0m : 2.21854
[1mStep[0m  [132/339], [94mLoss[0m : 1.78222
[1mStep[0m  [165/339], [94mLoss[0m : 2.21224
[1mStep[0m  [198/339], [94mLoss[0m : 2.29227
[1mStep[0m  [231/339], [94mLoss[0m : 2.22369
[1mStep[0m  [264/339], [94mLoss[0m : 2.16671
[1mStep[0m  [297/339], [94mLoss[0m : 1.84127
[1mStep[0m  [330/339], [94mLoss[0m : 1.80447

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.066, [92mTest[0m: 2.373, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14596
[1mStep[0m  [33/339], [94mLoss[0m : 1.63319
[1mStep[0m  [66/339], [94mLoss[0m : 1.83815
[1mStep[0m  [99/339], [94mLoss[0m : 1.79193
[1mStep[0m  [132/339], [94mLoss[0m : 1.83746
[1mStep[0m  [165/339], [94mLoss[0m : 1.97250
[1mStep[0m  [198/339], [94mLoss[0m : 2.23004
[1mStep[0m  [231/339], [94mLoss[0m : 1.83369
[1mStep[0m  [264/339], [94mLoss[0m : 2.66512
[1mStep[0m  [297/339], [94mLoss[0m : 2.88804
[1mStep[0m  [330/339], [94mLoss[0m : 2.33783

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.037, [92mTest[0m: 2.378, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09039
[1mStep[0m  [33/339], [94mLoss[0m : 2.23869
[1mStep[0m  [66/339], [94mLoss[0m : 2.80018
[1mStep[0m  [99/339], [94mLoss[0m : 2.10785
[1mStep[0m  [132/339], [94mLoss[0m : 2.04230
[1mStep[0m  [165/339], [94mLoss[0m : 1.57671
[1mStep[0m  [198/339], [94mLoss[0m : 2.18062
[1mStep[0m  [231/339], [94mLoss[0m : 2.36600
[1mStep[0m  [264/339], [94mLoss[0m : 2.54543
[1mStep[0m  [297/339], [94mLoss[0m : 2.52853
[1mStep[0m  [330/339], [94mLoss[0m : 1.91043

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.989, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.34693
[1mStep[0m  [33/339], [94mLoss[0m : 2.17083
[1mStep[0m  [66/339], [94mLoss[0m : 2.13400
[1mStep[0m  [99/339], [94mLoss[0m : 2.05953
[1mStep[0m  [132/339], [94mLoss[0m : 2.00344
[1mStep[0m  [165/339], [94mLoss[0m : 1.95144
[1mStep[0m  [198/339], [94mLoss[0m : 1.67885
[1mStep[0m  [231/339], [94mLoss[0m : 1.92074
[1mStep[0m  [264/339], [94mLoss[0m : 2.20358
[1mStep[0m  [297/339], [94mLoss[0m : 2.03526
[1mStep[0m  [330/339], [94mLoss[0m : 1.68642

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.951, [92mTest[0m: 2.489, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.58852
[1mStep[0m  [33/339], [94mLoss[0m : 2.00264
[1mStep[0m  [66/339], [94mLoss[0m : 2.26727
[1mStep[0m  [99/339], [94mLoss[0m : 2.11116
[1mStep[0m  [132/339], [94mLoss[0m : 1.61591
[1mStep[0m  [165/339], [94mLoss[0m : 2.07224
[1mStep[0m  [198/339], [94mLoss[0m : 1.48547
[1mStep[0m  [231/339], [94mLoss[0m : 1.45673
[1mStep[0m  [264/339], [94mLoss[0m : 1.65900
[1mStep[0m  [297/339], [94mLoss[0m : 1.59305
[1mStep[0m  [330/339], [94mLoss[0m : 1.79090

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.918, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85689
[1mStep[0m  [33/339], [94mLoss[0m : 1.77642
[1mStep[0m  [66/339], [94mLoss[0m : 1.47693
[1mStep[0m  [99/339], [94mLoss[0m : 2.16951
[1mStep[0m  [132/339], [94mLoss[0m : 1.76828
[1mStep[0m  [165/339], [94mLoss[0m : 1.98979
[1mStep[0m  [198/339], [94mLoss[0m : 1.53151
[1mStep[0m  [231/339], [94mLoss[0m : 2.19287
[1mStep[0m  [264/339], [94mLoss[0m : 2.19481
[1mStep[0m  [297/339], [94mLoss[0m : 2.02152
[1mStep[0m  [330/339], [94mLoss[0m : 1.80841

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.870, [92mTest[0m: 2.484, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.06347
[1mStep[0m  [33/339], [94mLoss[0m : 1.89303
[1mStep[0m  [66/339], [94mLoss[0m : 2.04247
[1mStep[0m  [99/339], [94mLoss[0m : 1.58326
[1mStep[0m  [132/339], [94mLoss[0m : 1.74953
[1mStep[0m  [165/339], [94mLoss[0m : 1.70637
[1mStep[0m  [198/339], [94mLoss[0m : 2.14654
[1mStep[0m  [231/339], [94mLoss[0m : 2.06128
[1mStep[0m  [264/339], [94mLoss[0m : 1.74502
[1mStep[0m  [297/339], [94mLoss[0m : 1.69907
[1mStep[0m  [330/339], [94mLoss[0m : 1.62873

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.862, [92mTest[0m: 2.399, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.61045
[1mStep[0m  [33/339], [94mLoss[0m : 1.82285
[1mStep[0m  [66/339], [94mLoss[0m : 1.50721
[1mStep[0m  [99/339], [94mLoss[0m : 1.65766
[1mStep[0m  [132/339], [94mLoss[0m : 1.96967
[1mStep[0m  [165/339], [94mLoss[0m : 1.91162
[1mStep[0m  [198/339], [94mLoss[0m : 1.18147
[1mStep[0m  [231/339], [94mLoss[0m : 1.63290
[1mStep[0m  [264/339], [94mLoss[0m : 1.45130
[1mStep[0m  [297/339], [94mLoss[0m : 1.81770
[1mStep[0m  [330/339], [94mLoss[0m : 2.01161

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.809, [92mTest[0m: 2.472, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.12651
[1mStep[0m  [33/339], [94mLoss[0m : 1.93333
[1mStep[0m  [66/339], [94mLoss[0m : 1.45663
[1mStep[0m  [99/339], [94mLoss[0m : 1.34904
[1mStep[0m  [132/339], [94mLoss[0m : 2.07977
[1mStep[0m  [165/339], [94mLoss[0m : 1.75750
[1mStep[0m  [198/339], [94mLoss[0m : 2.21769
[1mStep[0m  [231/339], [94mLoss[0m : 1.73461
[1mStep[0m  [264/339], [94mLoss[0m : 2.29098
[1mStep[0m  [297/339], [94mLoss[0m : 1.61425
[1mStep[0m  [330/339], [94mLoss[0m : 1.86995

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.36309
[1mStep[0m  [33/339], [94mLoss[0m : 1.99351
[1mStep[0m  [66/339], [94mLoss[0m : 1.39353
[1mStep[0m  [99/339], [94mLoss[0m : 1.46189
[1mStep[0m  [132/339], [94mLoss[0m : 2.12937
[1mStep[0m  [165/339], [94mLoss[0m : 1.76037
[1mStep[0m  [198/339], [94mLoss[0m : 2.25203
[1mStep[0m  [231/339], [94mLoss[0m : 1.85119
[1mStep[0m  [264/339], [94mLoss[0m : 1.48591
[1mStep[0m  [297/339], [94mLoss[0m : 1.25956
[1mStep[0m  [330/339], [94mLoss[0m : 1.65863

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.760, [92mTest[0m: 2.499, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65542
[1mStep[0m  [33/339], [94mLoss[0m : 2.19867
[1mStep[0m  [66/339], [94mLoss[0m : 1.12180
[1mStep[0m  [99/339], [94mLoss[0m : 2.64813
[1mStep[0m  [132/339], [94mLoss[0m : 1.62207
[1mStep[0m  [165/339], [94mLoss[0m : 1.52277
[1mStep[0m  [198/339], [94mLoss[0m : 1.31040
[1mStep[0m  [231/339], [94mLoss[0m : 1.97330
[1mStep[0m  [264/339], [94mLoss[0m : 1.79412
[1mStep[0m  [297/339], [94mLoss[0m : 1.67632
[1mStep[0m  [330/339], [94mLoss[0m : 1.43429

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.17176
[1mStep[0m  [33/339], [94mLoss[0m : 1.52359
[1mStep[0m  [66/339], [94mLoss[0m : 1.37614
[1mStep[0m  [99/339], [94mLoss[0m : 1.41226
[1mStep[0m  [132/339], [94mLoss[0m : 1.21776
[1mStep[0m  [165/339], [94mLoss[0m : 1.80261
[1mStep[0m  [198/339], [94mLoss[0m : 1.26590
[1mStep[0m  [231/339], [94mLoss[0m : 1.99534
[1mStep[0m  [264/339], [94mLoss[0m : 1.58013
[1mStep[0m  [297/339], [94mLoss[0m : 1.62733
[1mStep[0m  [330/339], [94mLoss[0m : 2.03609

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.726, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.40663
[1mStep[0m  [33/339], [94mLoss[0m : 1.84892
[1mStep[0m  [66/339], [94mLoss[0m : 1.18871
[1mStep[0m  [99/339], [94mLoss[0m : 1.60374
[1mStep[0m  [132/339], [94mLoss[0m : 1.33158
[1mStep[0m  [165/339], [94mLoss[0m : 1.76991
[1mStep[0m  [198/339], [94mLoss[0m : 1.98552
[1mStep[0m  [231/339], [94mLoss[0m : 2.07447
[1mStep[0m  [264/339], [94mLoss[0m : 2.14747
[1mStep[0m  [297/339], [94mLoss[0m : 1.30720
[1mStep[0m  [330/339], [94mLoss[0m : 1.70061

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.718, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.65646
[1mStep[0m  [33/339], [94mLoss[0m : 1.58563
[1mStep[0m  [66/339], [94mLoss[0m : 2.08739
[1mStep[0m  [99/339], [94mLoss[0m : 1.92020
[1mStep[0m  [132/339], [94mLoss[0m : 1.73171
[1mStep[0m  [165/339], [94mLoss[0m : 1.00258
[1mStep[0m  [198/339], [94mLoss[0m : 1.86642
[1mStep[0m  [231/339], [94mLoss[0m : 1.77044
[1mStep[0m  [264/339], [94mLoss[0m : 1.50214
[1mStep[0m  [297/339], [94mLoss[0m : 1.46812
[1mStep[0m  [330/339], [94mLoss[0m : 1.44088

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.655, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.26094
[1mStep[0m  [33/339], [94mLoss[0m : 2.01310
[1mStep[0m  [66/339], [94mLoss[0m : 2.19422
[1mStep[0m  [99/339], [94mLoss[0m : 1.75829
[1mStep[0m  [132/339], [94mLoss[0m : 1.52408
[1mStep[0m  [165/339], [94mLoss[0m : 1.71387
[1mStep[0m  [198/339], [94mLoss[0m : 1.54872
[1mStep[0m  [231/339], [94mLoss[0m : 1.35309
[1mStep[0m  [264/339], [94mLoss[0m : 1.95693
[1mStep[0m  [297/339], [94mLoss[0m : 1.94909
[1mStep[0m  [330/339], [94mLoss[0m : 1.66071

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.449, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.63605
[1mStep[0m  [33/339], [94mLoss[0m : 1.69774
[1mStep[0m  [66/339], [94mLoss[0m : 1.92923
[1mStep[0m  [99/339], [94mLoss[0m : 1.35285
[1mStep[0m  [132/339], [94mLoss[0m : 1.42223
[1mStep[0m  [165/339], [94mLoss[0m : 1.32811
[1mStep[0m  [198/339], [94mLoss[0m : 1.86701
[1mStep[0m  [231/339], [94mLoss[0m : 1.48555
[1mStep[0m  [264/339], [94mLoss[0m : 1.79754
[1mStep[0m  [297/339], [94mLoss[0m : 1.43139
[1mStep[0m  [330/339], [94mLoss[0m : 1.78318

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.598, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.47281
[1mStep[0m  [33/339], [94mLoss[0m : 1.79463
[1mStep[0m  [66/339], [94mLoss[0m : 1.36996
[1mStep[0m  [99/339], [94mLoss[0m : 1.24821
[1mStep[0m  [132/339], [94mLoss[0m : 1.15801
[1mStep[0m  [165/339], [94mLoss[0m : 1.47797
[1mStep[0m  [198/339], [94mLoss[0m : 1.78478
[1mStep[0m  [231/339], [94mLoss[0m : 1.69257
[1mStep[0m  [264/339], [94mLoss[0m : 1.65888
[1mStep[0m  [297/339], [94mLoss[0m : 1.98300
[1mStep[0m  [330/339], [94mLoss[0m : 1.55956

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.575, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.55848
[1mStep[0m  [33/339], [94mLoss[0m : 1.82024
[1mStep[0m  [66/339], [94mLoss[0m : 1.41680
[1mStep[0m  [99/339], [94mLoss[0m : 1.33158
[1mStep[0m  [132/339], [94mLoss[0m : 1.43944
[1mStep[0m  [165/339], [94mLoss[0m : 1.62694
[1mStep[0m  [198/339], [94mLoss[0m : 1.46800
[1mStep[0m  [231/339], [94mLoss[0m : 1.39250
[1mStep[0m  [264/339], [94mLoss[0m : 1.93174
[1mStep[0m  [297/339], [94mLoss[0m : 1.40501
[1mStep[0m  [330/339], [94mLoss[0m : 1.63081

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.581, [92mTest[0m: 2.517, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.66801
[1mStep[0m  [33/339], [94mLoss[0m : 1.43954
[1mStep[0m  [66/339], [94mLoss[0m : 1.34774
[1mStep[0m  [99/339], [94mLoss[0m : 1.28396
[1mStep[0m  [132/339], [94mLoss[0m : 1.55512
[1mStep[0m  [165/339], [94mLoss[0m : 1.16040
[1mStep[0m  [198/339], [94mLoss[0m : 1.90069
[1mStep[0m  [231/339], [94mLoss[0m : 1.40616
[1mStep[0m  [264/339], [94mLoss[0m : 2.02833
[1mStep[0m  [297/339], [94mLoss[0m : 1.13198
[1mStep[0m  [330/339], [94mLoss[0m : 1.59132

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.566, [92mTest[0m: 2.459, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64999
[1mStep[0m  [33/339], [94mLoss[0m : 1.11246
[1mStep[0m  [66/339], [94mLoss[0m : 1.47558
[1mStep[0m  [99/339], [94mLoss[0m : 1.12001
[1mStep[0m  [132/339], [94mLoss[0m : 1.53450
[1mStep[0m  [165/339], [94mLoss[0m : 2.30318
[1mStep[0m  [198/339], [94mLoss[0m : 1.34745
[1mStep[0m  [231/339], [94mLoss[0m : 1.17828
[1mStep[0m  [264/339], [94mLoss[0m : 1.92319
[1mStep[0m  [297/339], [94mLoss[0m : 1.67932
[1mStep[0m  [330/339], [94mLoss[0m : 1.31885

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.560, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41890
[1mStep[0m  [33/339], [94mLoss[0m : 1.04967
[1mStep[0m  [66/339], [94mLoss[0m : 1.61160
[1mStep[0m  [99/339], [94mLoss[0m : 2.23658
[1mStep[0m  [132/339], [94mLoss[0m : 1.75550
[1mStep[0m  [165/339], [94mLoss[0m : 1.73814
[1mStep[0m  [198/339], [94mLoss[0m : 2.23589
[1mStep[0m  [231/339], [94mLoss[0m : 1.95928
[1mStep[0m  [264/339], [94mLoss[0m : 1.67252
[1mStep[0m  [297/339], [94mLoss[0m : 1.47439
[1mStep[0m  [330/339], [94mLoss[0m : 1.83861

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.541, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73028
[1mStep[0m  [33/339], [94mLoss[0m : 1.96859
[1mStep[0m  [66/339], [94mLoss[0m : 1.52996
[1mStep[0m  [99/339], [94mLoss[0m : 1.68436
[1mStep[0m  [132/339], [94mLoss[0m : 1.75932
[1mStep[0m  [165/339], [94mLoss[0m : 1.48943
[1mStep[0m  [198/339], [94mLoss[0m : 1.75604
[1mStep[0m  [231/339], [94mLoss[0m : 1.31487
[1mStep[0m  [264/339], [94mLoss[0m : 1.26748
[1mStep[0m  [297/339], [94mLoss[0m : 1.29422
[1mStep[0m  [330/339], [94mLoss[0m : 1.38691

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.518, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73958
[1mStep[0m  [33/339], [94mLoss[0m : 1.22861
[1mStep[0m  [66/339], [94mLoss[0m : 1.51439
[1mStep[0m  [99/339], [94mLoss[0m : 1.55450
[1mStep[0m  [132/339], [94mLoss[0m : 1.14299
[1mStep[0m  [165/339], [94mLoss[0m : 1.40940
[1mStep[0m  [198/339], [94mLoss[0m : 1.56426
[1mStep[0m  [231/339], [94mLoss[0m : 1.37611
[1mStep[0m  [264/339], [94mLoss[0m : 1.49396
[1mStep[0m  [297/339], [94mLoss[0m : 1.79532
[1mStep[0m  [330/339], [94mLoss[0m : 1.41431

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.499, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.5163074035560133
MAE score P1        2.339399
MAE score P2        2.516307
loss                1.499466
learning_rate       0.007525
batch_size                32
hidden_sizes      [300, 100]
epochs                    30
activation           sigmoid
optimizer                sgd
early stopping         False
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 5, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.94633
[1mStep[0m  [16/169], [94mLoss[0m : 10.40768
[1mStep[0m  [32/169], [94mLoss[0m : 11.32279
[1mStep[0m  [48/169], [94mLoss[0m : 10.39622
[1mStep[0m  [64/169], [94mLoss[0m : 10.08182
[1mStep[0m  [80/169], [94mLoss[0m : 8.48839
[1mStep[0m  [96/169], [94mLoss[0m : 8.92903
[1mStep[0m  [112/169], [94mLoss[0m : 8.94358
[1mStep[0m  [128/169], [94mLoss[0m : 7.68709
[1mStep[0m  [144/169], [94mLoss[0m : 6.86300
[1mStep[0m  [160/169], [94mLoss[0m : 7.51614

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 9.230, [92mTest[0m: 10.884, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 6.62744
[1mStep[0m  [16/169], [94mLoss[0m : 6.14781
[1mStep[0m  [32/169], [94mLoss[0m : 5.47678
[1mStep[0m  [48/169], [94mLoss[0m : 5.10423
[1mStep[0m  [64/169], [94mLoss[0m : 4.89063
[1mStep[0m  [80/169], [94mLoss[0m : 3.93024
[1mStep[0m  [96/169], [94mLoss[0m : 3.36330
[1mStep[0m  [112/169], [94mLoss[0m : 2.99956
[1mStep[0m  [128/169], [94mLoss[0m : 3.20549
[1mStep[0m  [144/169], [94mLoss[0m : 3.08635
[1mStep[0m  [160/169], [94mLoss[0m : 3.09417

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 4.184, [92mTest[0m: 6.190, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.12636
[1mStep[0m  [16/169], [94mLoss[0m : 2.91010
[1mStep[0m  [32/169], [94mLoss[0m : 2.83638
[1mStep[0m  [48/169], [94mLoss[0m : 2.67913
[1mStep[0m  [64/169], [94mLoss[0m : 3.30401
[1mStep[0m  [80/169], [94mLoss[0m : 2.80454
[1mStep[0m  [96/169], [94mLoss[0m : 2.72794
[1mStep[0m  [112/169], [94mLoss[0m : 3.01201
[1mStep[0m  [128/169], [94mLoss[0m : 2.42108
[1mStep[0m  [144/169], [94mLoss[0m : 2.91390
[1mStep[0m  [160/169], [94mLoss[0m : 2.93286

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.860, [92mTest[0m: 2.439, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84057
[1mStep[0m  [16/169], [94mLoss[0m : 3.09470
[1mStep[0m  [32/169], [94mLoss[0m : 2.93470
[1mStep[0m  [48/169], [94mLoss[0m : 2.43032
[1mStep[0m  [64/169], [94mLoss[0m : 2.84891
[1mStep[0m  [80/169], [94mLoss[0m : 2.80603
[1mStep[0m  [96/169], [94mLoss[0m : 2.53465
[1mStep[0m  [112/169], [94mLoss[0m : 2.40359
[1mStep[0m  [128/169], [94mLoss[0m : 3.02858
[1mStep[0m  [144/169], [94mLoss[0m : 2.60014
[1mStep[0m  [160/169], [94mLoss[0m : 3.66364

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.772, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.83258
[1mStep[0m  [16/169], [94mLoss[0m : 2.69309
[1mStep[0m  [32/169], [94mLoss[0m : 2.70683
[1mStep[0m  [48/169], [94mLoss[0m : 2.53896
[1mStep[0m  [64/169], [94mLoss[0m : 2.50708
[1mStep[0m  [80/169], [94mLoss[0m : 2.98791
[1mStep[0m  [96/169], [94mLoss[0m : 3.08752
[1mStep[0m  [112/169], [94mLoss[0m : 2.79044
[1mStep[0m  [128/169], [94mLoss[0m : 3.10167
[1mStep[0m  [144/169], [94mLoss[0m : 3.29953
[1mStep[0m  [160/169], [94mLoss[0m : 2.95037

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.697, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.92465
[1mStep[0m  [16/169], [94mLoss[0m : 2.50687
[1mStep[0m  [32/169], [94mLoss[0m : 2.72715
[1mStep[0m  [48/169], [94mLoss[0m : 2.43841
[1mStep[0m  [64/169], [94mLoss[0m : 2.70770
[1mStep[0m  [80/169], [94mLoss[0m : 2.48483
[1mStep[0m  [96/169], [94mLoss[0m : 2.73788
[1mStep[0m  [112/169], [94mLoss[0m : 3.14395
[1mStep[0m  [128/169], [94mLoss[0m : 2.64162
[1mStep[0m  [144/169], [94mLoss[0m : 3.07498
[1mStep[0m  [160/169], [94mLoss[0m : 2.82803

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.673, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57744
[1mStep[0m  [16/169], [94mLoss[0m : 3.11754
[1mStep[0m  [32/169], [94mLoss[0m : 2.47704
[1mStep[0m  [48/169], [94mLoss[0m : 2.88847
[1mStep[0m  [64/169], [94mLoss[0m : 2.69682
[1mStep[0m  [80/169], [94mLoss[0m : 2.56520
[1mStep[0m  [96/169], [94mLoss[0m : 2.47732
[1mStep[0m  [112/169], [94mLoss[0m : 2.23750
[1mStep[0m  [128/169], [94mLoss[0m : 2.22260
[1mStep[0m  [144/169], [94mLoss[0m : 2.86581
[1mStep[0m  [160/169], [94mLoss[0m : 2.50139

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.08404
[1mStep[0m  [16/169], [94mLoss[0m : 2.44281
[1mStep[0m  [32/169], [94mLoss[0m : 2.48486
[1mStep[0m  [48/169], [94mLoss[0m : 2.54714
[1mStep[0m  [64/169], [94mLoss[0m : 2.87125
[1mStep[0m  [80/169], [94mLoss[0m : 2.70369
[1mStep[0m  [96/169], [94mLoss[0m : 2.40872
[1mStep[0m  [112/169], [94mLoss[0m : 2.35249
[1mStep[0m  [128/169], [94mLoss[0m : 2.50787
[1mStep[0m  [144/169], [94mLoss[0m : 2.87268
[1mStep[0m  [160/169], [94mLoss[0m : 2.55525

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.606, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67474
[1mStep[0m  [16/169], [94mLoss[0m : 2.91719
[1mStep[0m  [32/169], [94mLoss[0m : 2.47018
[1mStep[0m  [48/169], [94mLoss[0m : 2.34772
[1mStep[0m  [64/169], [94mLoss[0m : 2.79337
[1mStep[0m  [80/169], [94mLoss[0m : 2.52486
[1mStep[0m  [96/169], [94mLoss[0m : 3.30025
[1mStep[0m  [112/169], [94mLoss[0m : 2.79613
[1mStep[0m  [128/169], [94mLoss[0m : 2.76709
[1mStep[0m  [144/169], [94mLoss[0m : 2.69464
[1mStep[0m  [160/169], [94mLoss[0m : 2.49217

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.92374
[1mStep[0m  [16/169], [94mLoss[0m : 2.61918
[1mStep[0m  [32/169], [94mLoss[0m : 1.98223
[1mStep[0m  [48/169], [94mLoss[0m : 2.54431
[1mStep[0m  [64/169], [94mLoss[0m : 2.49548
[1mStep[0m  [80/169], [94mLoss[0m : 2.62213
[1mStep[0m  [96/169], [94mLoss[0m : 2.78678
[1mStep[0m  [112/169], [94mLoss[0m : 2.48496
[1mStep[0m  [128/169], [94mLoss[0m : 2.65095
[1mStep[0m  [144/169], [94mLoss[0m : 1.99883
[1mStep[0m  [160/169], [94mLoss[0m : 2.66556

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.564, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61581
[1mStep[0m  [16/169], [94mLoss[0m : 2.26546
[1mStep[0m  [32/169], [94mLoss[0m : 2.43605
[1mStep[0m  [48/169], [94mLoss[0m : 2.57812
[1mStep[0m  [64/169], [94mLoss[0m : 2.14856
[1mStep[0m  [80/169], [94mLoss[0m : 2.76730
[1mStep[0m  [96/169], [94mLoss[0m : 2.07909
[1mStep[0m  [112/169], [94mLoss[0m : 2.60672
[1mStep[0m  [128/169], [94mLoss[0m : 3.05075
[1mStep[0m  [144/169], [94mLoss[0m : 2.21736
[1mStep[0m  [160/169], [94mLoss[0m : 2.26410

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.532, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34496
[1mStep[0m  [16/169], [94mLoss[0m : 2.71217
[1mStep[0m  [32/169], [94mLoss[0m : 2.13246
[1mStep[0m  [48/169], [94mLoss[0m : 2.54030
[1mStep[0m  [64/169], [94mLoss[0m : 1.92370
[1mStep[0m  [80/169], [94mLoss[0m : 2.55088
[1mStep[0m  [96/169], [94mLoss[0m : 2.91578
[1mStep[0m  [112/169], [94mLoss[0m : 2.54139
[1mStep[0m  [128/169], [94mLoss[0m : 2.89349
[1mStep[0m  [144/169], [94mLoss[0m : 2.15744
[1mStep[0m  [160/169], [94mLoss[0m : 2.48867

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.89732
[1mStep[0m  [16/169], [94mLoss[0m : 2.14727
[1mStep[0m  [32/169], [94mLoss[0m : 2.27924
[1mStep[0m  [48/169], [94mLoss[0m : 2.68746
[1mStep[0m  [64/169], [94mLoss[0m : 2.65830
[1mStep[0m  [80/169], [94mLoss[0m : 2.54688
[1mStep[0m  [96/169], [94mLoss[0m : 2.09695
[1mStep[0m  [112/169], [94mLoss[0m : 2.36309
[1mStep[0m  [128/169], [94mLoss[0m : 2.46115
[1mStep[0m  [144/169], [94mLoss[0m : 2.70144
[1mStep[0m  [160/169], [94mLoss[0m : 2.51989

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.60083
[1mStep[0m  [16/169], [94mLoss[0m : 2.63261
[1mStep[0m  [32/169], [94mLoss[0m : 2.68994
[1mStep[0m  [48/169], [94mLoss[0m : 2.37580
[1mStep[0m  [64/169], [94mLoss[0m : 2.33033
[1mStep[0m  [80/169], [94mLoss[0m : 2.55636
[1mStep[0m  [96/169], [94mLoss[0m : 2.49835
[1mStep[0m  [112/169], [94mLoss[0m : 2.44022
[1mStep[0m  [128/169], [94mLoss[0m : 2.32809
[1mStep[0m  [144/169], [94mLoss[0m : 2.85841
[1mStep[0m  [160/169], [94mLoss[0m : 2.55994

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57631
[1mStep[0m  [16/169], [94mLoss[0m : 2.40791
[1mStep[0m  [32/169], [94mLoss[0m : 2.23254
[1mStep[0m  [48/169], [94mLoss[0m : 2.59718
[1mStep[0m  [64/169], [94mLoss[0m : 2.42981
[1mStep[0m  [80/169], [94mLoss[0m : 2.45077
[1mStep[0m  [96/169], [94mLoss[0m : 2.58342
[1mStep[0m  [112/169], [94mLoss[0m : 2.62484
[1mStep[0m  [128/169], [94mLoss[0m : 2.29037
[1mStep[0m  [144/169], [94mLoss[0m : 2.67241
[1mStep[0m  [160/169], [94mLoss[0m : 2.87709

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34992
[1mStep[0m  [16/169], [94mLoss[0m : 2.26419
[1mStep[0m  [32/169], [94mLoss[0m : 2.31728
[1mStep[0m  [48/169], [94mLoss[0m : 2.12202
[1mStep[0m  [64/169], [94mLoss[0m : 2.37422
[1mStep[0m  [80/169], [94mLoss[0m : 2.76011
[1mStep[0m  [96/169], [94mLoss[0m : 2.29811
[1mStep[0m  [112/169], [94mLoss[0m : 2.55577
[1mStep[0m  [128/169], [94mLoss[0m : 2.90301
[1mStep[0m  [144/169], [94mLoss[0m : 2.25644
[1mStep[0m  [160/169], [94mLoss[0m : 2.46582

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.502, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55656
[1mStep[0m  [16/169], [94mLoss[0m : 2.49218
[1mStep[0m  [32/169], [94mLoss[0m : 2.11076
[1mStep[0m  [48/169], [94mLoss[0m : 2.32422
[1mStep[0m  [64/169], [94mLoss[0m : 2.83673
[1mStep[0m  [80/169], [94mLoss[0m : 2.24772
[1mStep[0m  [96/169], [94mLoss[0m : 2.09373
[1mStep[0m  [112/169], [94mLoss[0m : 2.20066
[1mStep[0m  [128/169], [94mLoss[0m : 2.18788
[1mStep[0m  [144/169], [94mLoss[0m : 2.13097
[1mStep[0m  [160/169], [94mLoss[0m : 2.98802

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20425
[1mStep[0m  [16/169], [94mLoss[0m : 2.40496
[1mStep[0m  [32/169], [94mLoss[0m : 3.04586
[1mStep[0m  [48/169], [94mLoss[0m : 2.67638
[1mStep[0m  [64/169], [94mLoss[0m : 2.45208
[1mStep[0m  [80/169], [94mLoss[0m : 2.46235
[1mStep[0m  [96/169], [94mLoss[0m : 2.45983
[1mStep[0m  [112/169], [94mLoss[0m : 2.74700
[1mStep[0m  [128/169], [94mLoss[0m : 2.76499
[1mStep[0m  [144/169], [94mLoss[0m : 2.43013
[1mStep[0m  [160/169], [94mLoss[0m : 2.13272

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35482
[1mStep[0m  [16/169], [94mLoss[0m : 2.61360
[1mStep[0m  [32/169], [94mLoss[0m : 2.49050
[1mStep[0m  [48/169], [94mLoss[0m : 2.29366
[1mStep[0m  [64/169], [94mLoss[0m : 2.77053
[1mStep[0m  [80/169], [94mLoss[0m : 2.97341
[1mStep[0m  [96/169], [94mLoss[0m : 2.27026
[1mStep[0m  [112/169], [94mLoss[0m : 2.25447
[1mStep[0m  [128/169], [94mLoss[0m : 2.79154
[1mStep[0m  [144/169], [94mLoss[0m : 2.29829
[1mStep[0m  [160/169], [94mLoss[0m : 1.95874

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.315, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.79799
[1mStep[0m  [16/169], [94mLoss[0m : 2.37334
[1mStep[0m  [32/169], [94mLoss[0m : 2.63850
[1mStep[0m  [48/169], [94mLoss[0m : 2.56911
[1mStep[0m  [64/169], [94mLoss[0m : 2.20041
[1mStep[0m  [80/169], [94mLoss[0m : 2.54123
[1mStep[0m  [96/169], [94mLoss[0m : 2.81284
[1mStep[0m  [112/169], [94mLoss[0m : 2.27325
[1mStep[0m  [128/169], [94mLoss[0m : 2.62749
[1mStep[0m  [144/169], [94mLoss[0m : 1.86980
[1mStep[0m  [160/169], [94mLoss[0m : 2.63390

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22615
[1mStep[0m  [16/169], [94mLoss[0m : 2.69473
[1mStep[0m  [32/169], [94mLoss[0m : 2.88691
[1mStep[0m  [48/169], [94mLoss[0m : 2.36417
[1mStep[0m  [64/169], [94mLoss[0m : 2.45166
[1mStep[0m  [80/169], [94mLoss[0m : 2.44282
[1mStep[0m  [96/169], [94mLoss[0m : 2.98509
[1mStep[0m  [112/169], [94mLoss[0m : 2.26971
[1mStep[0m  [128/169], [94mLoss[0m : 2.38189
[1mStep[0m  [144/169], [94mLoss[0m : 2.37596
[1mStep[0m  [160/169], [94mLoss[0m : 2.48986

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17033
[1mStep[0m  [16/169], [94mLoss[0m : 2.16636
[1mStep[0m  [32/169], [94mLoss[0m : 2.55974
[1mStep[0m  [48/169], [94mLoss[0m : 1.98074
[1mStep[0m  [64/169], [94mLoss[0m : 2.58411
[1mStep[0m  [80/169], [94mLoss[0m : 2.72186
[1mStep[0m  [96/169], [94mLoss[0m : 2.37474
[1mStep[0m  [112/169], [94mLoss[0m : 2.21170
[1mStep[0m  [128/169], [94mLoss[0m : 2.54877
[1mStep[0m  [144/169], [94mLoss[0m : 2.80102
[1mStep[0m  [160/169], [94mLoss[0m : 2.49947

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.312, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41995
[1mStep[0m  [16/169], [94mLoss[0m : 2.37247
[1mStep[0m  [32/169], [94mLoss[0m : 2.57631
[1mStep[0m  [48/169], [94mLoss[0m : 2.36691
[1mStep[0m  [64/169], [94mLoss[0m : 2.11746
[1mStep[0m  [80/169], [94mLoss[0m : 2.46248
[1mStep[0m  [96/169], [94mLoss[0m : 2.48098
[1mStep[0m  [112/169], [94mLoss[0m : 2.78208
[1mStep[0m  [128/169], [94mLoss[0m : 2.18488
[1mStep[0m  [144/169], [94mLoss[0m : 2.99127
[1mStep[0m  [160/169], [94mLoss[0m : 2.38961

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.81624
[1mStep[0m  [16/169], [94mLoss[0m : 2.25371
[1mStep[0m  [32/169], [94mLoss[0m : 2.00110
[1mStep[0m  [48/169], [94mLoss[0m : 2.42715
[1mStep[0m  [64/169], [94mLoss[0m : 2.32023
[1mStep[0m  [80/169], [94mLoss[0m : 2.00676
[1mStep[0m  [96/169], [94mLoss[0m : 2.51068
[1mStep[0m  [112/169], [94mLoss[0m : 2.74831
[1mStep[0m  [128/169], [94mLoss[0m : 2.24703
[1mStep[0m  [144/169], [94mLoss[0m : 2.36125
[1mStep[0m  [160/169], [94mLoss[0m : 2.60752

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.449, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54296
[1mStep[0m  [16/169], [94mLoss[0m : 2.49864
[1mStep[0m  [32/169], [94mLoss[0m : 2.40149
[1mStep[0m  [48/169], [94mLoss[0m : 2.43082
[1mStep[0m  [64/169], [94mLoss[0m : 2.56569
[1mStep[0m  [80/169], [94mLoss[0m : 2.69536
[1mStep[0m  [96/169], [94mLoss[0m : 2.37910
[1mStep[0m  [112/169], [94mLoss[0m : 2.68794
[1mStep[0m  [128/169], [94mLoss[0m : 2.38032
[1mStep[0m  [144/169], [94mLoss[0m : 2.00522
[1mStep[0m  [160/169], [94mLoss[0m : 2.45950

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.444, [92mTest[0m: 2.309, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55205
[1mStep[0m  [16/169], [94mLoss[0m : 2.22934
[1mStep[0m  [32/169], [94mLoss[0m : 2.12845
[1mStep[0m  [48/169], [94mLoss[0m : 2.42846
[1mStep[0m  [64/169], [94mLoss[0m : 2.61039
[1mStep[0m  [80/169], [94mLoss[0m : 2.63579
[1mStep[0m  [96/169], [94mLoss[0m : 2.71943
[1mStep[0m  [112/169], [94mLoss[0m : 3.04083
[1mStep[0m  [128/169], [94mLoss[0m : 2.22239
[1mStep[0m  [144/169], [94mLoss[0m : 2.20401
[1mStep[0m  [160/169], [94mLoss[0m : 2.62483

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.310, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.91991
[1mStep[0m  [16/169], [94mLoss[0m : 2.80045
[1mStep[0m  [32/169], [94mLoss[0m : 2.04499
[1mStep[0m  [48/169], [94mLoss[0m : 2.45660
[1mStep[0m  [64/169], [94mLoss[0m : 2.17102
[1mStep[0m  [80/169], [94mLoss[0m : 2.47015
[1mStep[0m  [96/169], [94mLoss[0m : 2.39845
[1mStep[0m  [112/169], [94mLoss[0m : 2.78148
[1mStep[0m  [128/169], [94mLoss[0m : 2.18845
[1mStep[0m  [144/169], [94mLoss[0m : 2.07334
[1mStep[0m  [160/169], [94mLoss[0m : 2.70280

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.334, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41480
[1mStep[0m  [16/169], [94mLoss[0m : 1.98822
[1mStep[0m  [32/169], [94mLoss[0m : 2.65723
[1mStep[0m  [48/169], [94mLoss[0m : 2.19223
[1mStep[0m  [64/169], [94mLoss[0m : 1.96288
[1mStep[0m  [80/169], [94mLoss[0m : 2.56749
[1mStep[0m  [96/169], [94mLoss[0m : 2.73484
[1mStep[0m  [112/169], [94mLoss[0m : 2.76710
[1mStep[0m  [128/169], [94mLoss[0m : 2.06845
[1mStep[0m  [144/169], [94mLoss[0m : 2.46313
[1mStep[0m  [160/169], [94mLoss[0m : 2.53447

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19480
[1mStep[0m  [16/169], [94mLoss[0m : 2.68826
[1mStep[0m  [32/169], [94mLoss[0m : 2.39740
[1mStep[0m  [48/169], [94mLoss[0m : 2.62004
[1mStep[0m  [64/169], [94mLoss[0m : 2.39708
[1mStep[0m  [80/169], [94mLoss[0m : 2.09173
[1mStep[0m  [96/169], [94mLoss[0m : 2.58095
[1mStep[0m  [112/169], [94mLoss[0m : 2.28841
[1mStep[0m  [128/169], [94mLoss[0m : 1.94488
[1mStep[0m  [144/169], [94mLoss[0m : 2.37063
[1mStep[0m  [160/169], [94mLoss[0m : 2.94736

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.353, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51832
[1mStep[0m  [16/169], [94mLoss[0m : 2.24646
[1mStep[0m  [32/169], [94mLoss[0m : 2.56013
[1mStep[0m  [48/169], [94mLoss[0m : 2.28180
[1mStep[0m  [64/169], [94mLoss[0m : 2.48785
[1mStep[0m  [80/169], [94mLoss[0m : 2.56023
[1mStep[0m  [96/169], [94mLoss[0m : 2.45331
[1mStep[0m  [112/169], [94mLoss[0m : 2.61416
[1mStep[0m  [128/169], [94mLoss[0m : 2.67403
[1mStep[0m  [144/169], [94mLoss[0m : 2.49766
[1mStep[0m  [160/169], [94mLoss[0m : 2.74818

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.326369598507881
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.52536
[1mStep[0m  [16/169], [94mLoss[0m : 2.60335
[1mStep[0m  [32/169], [94mLoss[0m : 2.28506
[1mStep[0m  [48/169], [94mLoss[0m : 2.17153
[1mStep[0m  [64/169], [94mLoss[0m : 2.30586
[1mStep[0m  [80/169], [94mLoss[0m : 2.43686
[1mStep[0m  [96/169], [94mLoss[0m : 2.23090
[1mStep[0m  [112/169], [94mLoss[0m : 1.88979
[1mStep[0m  [128/169], [94mLoss[0m : 2.38623
[1mStep[0m  [144/169], [94mLoss[0m : 2.91909
[1mStep[0m  [160/169], [94mLoss[0m : 2.34741

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23414
[1mStep[0m  [16/169], [94mLoss[0m : 2.37258
[1mStep[0m  [32/169], [94mLoss[0m : 2.52375
[1mStep[0m  [48/169], [94mLoss[0m : 2.25793
[1mStep[0m  [64/169], [94mLoss[0m : 2.64451
[1mStep[0m  [80/169], [94mLoss[0m : 2.35305
[1mStep[0m  [96/169], [94mLoss[0m : 2.48672
[1mStep[0m  [112/169], [94mLoss[0m : 2.22550
[1mStep[0m  [128/169], [94mLoss[0m : 2.35842
[1mStep[0m  [144/169], [94mLoss[0m : 2.25378
[1mStep[0m  [160/169], [94mLoss[0m : 2.22362

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19372
[1mStep[0m  [16/169], [94mLoss[0m : 2.34003
[1mStep[0m  [32/169], [94mLoss[0m : 2.49949
[1mStep[0m  [48/169], [94mLoss[0m : 2.38765
[1mStep[0m  [64/169], [94mLoss[0m : 2.18425
[1mStep[0m  [80/169], [94mLoss[0m : 2.50152
[1mStep[0m  [96/169], [94mLoss[0m : 2.21370
[1mStep[0m  [112/169], [94mLoss[0m : 2.58758
[1mStep[0m  [128/169], [94mLoss[0m : 2.24670
[1mStep[0m  [144/169], [94mLoss[0m : 2.54557
[1mStep[0m  [160/169], [94mLoss[0m : 2.25796

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.318, [92mTest[0m: 2.377, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08449
[1mStep[0m  [16/169], [94mLoss[0m : 2.10910
[1mStep[0m  [32/169], [94mLoss[0m : 2.03945
[1mStep[0m  [48/169], [94mLoss[0m : 2.52491
[1mStep[0m  [64/169], [94mLoss[0m : 2.05230
[1mStep[0m  [80/169], [94mLoss[0m : 1.87730
[1mStep[0m  [96/169], [94mLoss[0m : 2.01354
[1mStep[0m  [112/169], [94mLoss[0m : 2.38062
[1mStep[0m  [128/169], [94mLoss[0m : 2.74559
[1mStep[0m  [144/169], [94mLoss[0m : 2.44290
[1mStep[0m  [160/169], [94mLoss[0m : 2.16502

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34174
[1mStep[0m  [16/169], [94mLoss[0m : 2.11128
[1mStep[0m  [32/169], [94mLoss[0m : 2.22924
[1mStep[0m  [48/169], [94mLoss[0m : 2.74026
[1mStep[0m  [64/169], [94mLoss[0m : 1.73945
[1mStep[0m  [80/169], [94mLoss[0m : 2.22638
[1mStep[0m  [96/169], [94mLoss[0m : 1.92197
[1mStep[0m  [112/169], [94mLoss[0m : 2.91736
[1mStep[0m  [128/169], [94mLoss[0m : 2.28818
[1mStep[0m  [144/169], [94mLoss[0m : 2.60996
[1mStep[0m  [160/169], [94mLoss[0m : 2.01761

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.201, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15887
[1mStep[0m  [16/169], [94mLoss[0m : 2.53455
[1mStep[0m  [32/169], [94mLoss[0m : 1.76519
[1mStep[0m  [48/169], [94mLoss[0m : 1.99691
[1mStep[0m  [64/169], [94mLoss[0m : 2.46758
[1mStep[0m  [80/169], [94mLoss[0m : 1.96267
[1mStep[0m  [96/169], [94mLoss[0m : 2.02684
[1mStep[0m  [112/169], [94mLoss[0m : 1.77804
[1mStep[0m  [128/169], [94mLoss[0m : 2.41036
[1mStep[0m  [144/169], [94mLoss[0m : 2.25247
[1mStep[0m  [160/169], [94mLoss[0m : 1.76633

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.148, [92mTest[0m: 2.420, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50189
[1mStep[0m  [16/169], [94mLoss[0m : 2.35120
[1mStep[0m  [32/169], [94mLoss[0m : 2.09218
[1mStep[0m  [48/169], [94mLoss[0m : 2.17811
[1mStep[0m  [64/169], [94mLoss[0m : 2.20307
[1mStep[0m  [80/169], [94mLoss[0m : 2.08880
[1mStep[0m  [96/169], [94mLoss[0m : 2.12162
[1mStep[0m  [112/169], [94mLoss[0m : 1.98518
[1mStep[0m  [128/169], [94mLoss[0m : 2.03642
[1mStep[0m  [144/169], [94mLoss[0m : 2.10852
[1mStep[0m  [160/169], [94mLoss[0m : 2.09091

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.086, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09151
[1mStep[0m  [16/169], [94mLoss[0m : 2.28553
[1mStep[0m  [32/169], [94mLoss[0m : 2.69264
[1mStep[0m  [48/169], [94mLoss[0m : 2.24681
[1mStep[0m  [64/169], [94mLoss[0m : 1.82235
[1mStep[0m  [80/169], [94mLoss[0m : 1.98000
[1mStep[0m  [96/169], [94mLoss[0m : 1.97958
[1mStep[0m  [112/169], [94mLoss[0m : 1.69120
[1mStep[0m  [128/169], [94mLoss[0m : 1.78489
[1mStep[0m  [144/169], [94mLoss[0m : 1.98991
[1mStep[0m  [160/169], [94mLoss[0m : 1.84825

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.072, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.75599
[1mStep[0m  [16/169], [94mLoss[0m : 1.73334
[1mStep[0m  [32/169], [94mLoss[0m : 1.79732
[1mStep[0m  [48/169], [94mLoss[0m : 2.32667
[1mStep[0m  [64/169], [94mLoss[0m : 2.13063
[1mStep[0m  [80/169], [94mLoss[0m : 1.87048
[1mStep[0m  [96/169], [94mLoss[0m : 1.91470
[1mStep[0m  [112/169], [94mLoss[0m : 2.02119
[1mStep[0m  [128/169], [94mLoss[0m : 1.71743
[1mStep[0m  [144/169], [94mLoss[0m : 2.00330
[1mStep[0m  [160/169], [94mLoss[0m : 2.25958

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.018, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16823
[1mStep[0m  [16/169], [94mLoss[0m : 1.89789
[1mStep[0m  [32/169], [94mLoss[0m : 2.21749
[1mStep[0m  [48/169], [94mLoss[0m : 1.87528
[1mStep[0m  [64/169], [94mLoss[0m : 2.49116
[1mStep[0m  [80/169], [94mLoss[0m : 2.03073
[1mStep[0m  [96/169], [94mLoss[0m : 1.88054
[1mStep[0m  [112/169], [94mLoss[0m : 2.08656
[1mStep[0m  [128/169], [94mLoss[0m : 2.16215
[1mStep[0m  [144/169], [94mLoss[0m : 1.97741
[1mStep[0m  [160/169], [94mLoss[0m : 2.27731

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69390
[1mStep[0m  [16/169], [94mLoss[0m : 1.83345
[1mStep[0m  [32/169], [94mLoss[0m : 2.17801
[1mStep[0m  [48/169], [94mLoss[0m : 1.82899
[1mStep[0m  [64/169], [94mLoss[0m : 1.93222
[1mStep[0m  [80/169], [94mLoss[0m : 2.13494
[1mStep[0m  [96/169], [94mLoss[0m : 1.82350
[1mStep[0m  [112/169], [94mLoss[0m : 2.46009
[1mStep[0m  [128/169], [94mLoss[0m : 1.81118
[1mStep[0m  [144/169], [94mLoss[0m : 1.71450
[1mStep[0m  [160/169], [94mLoss[0m : 1.90739

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.922, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08991
[1mStep[0m  [16/169], [94mLoss[0m : 1.79772
[1mStep[0m  [32/169], [94mLoss[0m : 1.63352
[1mStep[0m  [48/169], [94mLoss[0m : 1.58173
[1mStep[0m  [64/169], [94mLoss[0m : 1.88230
[1mStep[0m  [80/169], [94mLoss[0m : 2.13803
[1mStep[0m  [96/169], [94mLoss[0m : 2.01222
[1mStep[0m  [112/169], [94mLoss[0m : 2.29555
[1mStep[0m  [128/169], [94mLoss[0m : 1.87560
[1mStep[0m  [144/169], [94mLoss[0m : 2.09511
[1mStep[0m  [160/169], [94mLoss[0m : 1.86995

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.884, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86866
[1mStep[0m  [16/169], [94mLoss[0m : 1.55831
[1mStep[0m  [32/169], [94mLoss[0m : 1.68247
[1mStep[0m  [48/169], [94mLoss[0m : 1.52855
[1mStep[0m  [64/169], [94mLoss[0m : 2.21284
[1mStep[0m  [80/169], [94mLoss[0m : 1.66845
[1mStep[0m  [96/169], [94mLoss[0m : 1.68488
[1mStep[0m  [112/169], [94mLoss[0m : 1.76845
[1mStep[0m  [128/169], [94mLoss[0m : 1.56515
[1mStep[0m  [144/169], [94mLoss[0m : 1.96682
[1mStep[0m  [160/169], [94mLoss[0m : 1.72001

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.837, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00979
[1mStep[0m  [16/169], [94mLoss[0m : 2.03587
[1mStep[0m  [32/169], [94mLoss[0m : 2.06047
[1mStep[0m  [48/169], [94mLoss[0m : 1.95489
[1mStep[0m  [64/169], [94mLoss[0m : 1.77723
[1mStep[0m  [80/169], [94mLoss[0m : 1.69644
[1mStep[0m  [96/169], [94mLoss[0m : 1.96314
[1mStep[0m  [112/169], [94mLoss[0m : 2.04133
[1mStep[0m  [128/169], [94mLoss[0m : 1.98650
[1mStep[0m  [144/169], [94mLoss[0m : 1.90613
[1mStep[0m  [160/169], [94mLoss[0m : 1.85466

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.805, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54459
[1mStep[0m  [16/169], [94mLoss[0m : 1.80373
[1mStep[0m  [32/169], [94mLoss[0m : 1.66794
[1mStep[0m  [48/169], [94mLoss[0m : 1.78235
[1mStep[0m  [64/169], [94mLoss[0m : 1.75240
[1mStep[0m  [80/169], [94mLoss[0m : 1.51755
[1mStep[0m  [96/169], [94mLoss[0m : 2.05137
[1mStep[0m  [112/169], [94mLoss[0m : 2.07209
[1mStep[0m  [128/169], [94mLoss[0m : 1.84223
[1mStep[0m  [144/169], [94mLoss[0m : 1.69189
[1mStep[0m  [160/169], [94mLoss[0m : 2.25652

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.779, [92mTest[0m: 2.517, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01258
[1mStep[0m  [16/169], [94mLoss[0m : 1.54668
[1mStep[0m  [32/169], [94mLoss[0m : 1.93022
[1mStep[0m  [48/169], [94mLoss[0m : 1.90146
[1mStep[0m  [64/169], [94mLoss[0m : 1.76108
[1mStep[0m  [80/169], [94mLoss[0m : 1.47371
[1mStep[0m  [96/169], [94mLoss[0m : 1.48634
[1mStep[0m  [112/169], [94mLoss[0m : 1.81756
[1mStep[0m  [128/169], [94mLoss[0m : 1.71053
[1mStep[0m  [144/169], [94mLoss[0m : 1.70593
[1mStep[0m  [160/169], [94mLoss[0m : 1.93446

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.480, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57679
[1mStep[0m  [16/169], [94mLoss[0m : 1.63052
[1mStep[0m  [32/169], [94mLoss[0m : 1.55149
[1mStep[0m  [48/169], [94mLoss[0m : 1.91865
[1mStep[0m  [64/169], [94mLoss[0m : 1.67662
[1mStep[0m  [80/169], [94mLoss[0m : 1.72186
[1mStep[0m  [96/169], [94mLoss[0m : 1.65582
[1mStep[0m  [112/169], [94mLoss[0m : 1.83242
[1mStep[0m  [128/169], [94mLoss[0m : 1.95881
[1mStep[0m  [144/169], [94mLoss[0m : 1.84093
[1mStep[0m  [160/169], [94mLoss[0m : 2.08265

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.541, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72394
[1mStep[0m  [16/169], [94mLoss[0m : 1.54309
[1mStep[0m  [32/169], [94mLoss[0m : 1.73447
[1mStep[0m  [48/169], [94mLoss[0m : 1.49598
[1mStep[0m  [64/169], [94mLoss[0m : 1.93752
[1mStep[0m  [80/169], [94mLoss[0m : 1.40163
[1mStep[0m  [96/169], [94mLoss[0m : 1.59204
[1mStep[0m  [112/169], [94mLoss[0m : 1.36309
[1mStep[0m  [128/169], [94mLoss[0m : 1.54304
[1mStep[0m  [144/169], [94mLoss[0m : 1.40191
[1mStep[0m  [160/169], [94mLoss[0m : 1.31217

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.471, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40550
[1mStep[0m  [16/169], [94mLoss[0m : 1.59263
[1mStep[0m  [32/169], [94mLoss[0m : 1.77834
[1mStep[0m  [48/169], [94mLoss[0m : 1.46637
[1mStep[0m  [64/169], [94mLoss[0m : 1.41340
[1mStep[0m  [80/169], [94mLoss[0m : 1.47783
[1mStep[0m  [96/169], [94mLoss[0m : 1.41972
[1mStep[0m  [112/169], [94mLoss[0m : 1.82973
[1mStep[0m  [128/169], [94mLoss[0m : 1.52518
[1mStep[0m  [144/169], [94mLoss[0m : 1.37888
[1mStep[0m  [160/169], [94mLoss[0m : 1.86703

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.666, [92mTest[0m: 2.510, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09705
[1mStep[0m  [16/169], [94mLoss[0m : 1.73970
[1mStep[0m  [32/169], [94mLoss[0m : 1.78839
[1mStep[0m  [48/169], [94mLoss[0m : 1.60356
[1mStep[0m  [64/169], [94mLoss[0m : 1.92078
[1mStep[0m  [80/169], [94mLoss[0m : 1.78941
[1mStep[0m  [96/169], [94mLoss[0m : 1.94400
[1mStep[0m  [112/169], [94mLoss[0m : 1.68747
[1mStep[0m  [128/169], [94mLoss[0m : 1.74517
[1mStep[0m  [144/169], [94mLoss[0m : 1.90487
[1mStep[0m  [160/169], [94mLoss[0m : 1.51854

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.639, [92mTest[0m: 2.500, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51539
[1mStep[0m  [16/169], [94mLoss[0m : 1.15218
[1mStep[0m  [32/169], [94mLoss[0m : 1.69029
[1mStep[0m  [48/169], [94mLoss[0m : 1.29158
[1mStep[0m  [64/169], [94mLoss[0m : 1.40862
[1mStep[0m  [80/169], [94mLoss[0m : 1.73282
[1mStep[0m  [96/169], [94mLoss[0m : 1.37322
[1mStep[0m  [112/169], [94mLoss[0m : 1.73910
[1mStep[0m  [128/169], [94mLoss[0m : 1.74669
[1mStep[0m  [144/169], [94mLoss[0m : 1.53824
[1mStep[0m  [160/169], [94mLoss[0m : 1.48138

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.489, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69997
[1mStep[0m  [16/169], [94mLoss[0m : 1.56846
[1mStep[0m  [32/169], [94mLoss[0m : 1.56543
[1mStep[0m  [48/169], [94mLoss[0m : 1.63876
[1mStep[0m  [64/169], [94mLoss[0m : 1.76267
[1mStep[0m  [80/169], [94mLoss[0m : 1.42258
[1mStep[0m  [96/169], [94mLoss[0m : 1.47712
[1mStep[0m  [112/169], [94mLoss[0m : 1.36843
[1mStep[0m  [128/169], [94mLoss[0m : 2.12097
[1mStep[0m  [144/169], [94mLoss[0m : 1.64873
[1mStep[0m  [160/169], [94mLoss[0m : 1.53306

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.568, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32161
[1mStep[0m  [16/169], [94mLoss[0m : 1.49889
[1mStep[0m  [32/169], [94mLoss[0m : 1.90443
[1mStep[0m  [48/169], [94mLoss[0m : 1.65994
[1mStep[0m  [64/169], [94mLoss[0m : 1.61097
[1mStep[0m  [80/169], [94mLoss[0m : 1.35464
[1mStep[0m  [96/169], [94mLoss[0m : 1.38890
[1mStep[0m  [112/169], [94mLoss[0m : 1.59000
[1mStep[0m  [128/169], [94mLoss[0m : 1.49001
[1mStep[0m  [144/169], [94mLoss[0m : 1.53764
[1mStep[0m  [160/169], [94mLoss[0m : 1.42520

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.561, [92mTest[0m: 2.503, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66543
[1mStep[0m  [16/169], [94mLoss[0m : 1.57335
[1mStep[0m  [32/169], [94mLoss[0m : 1.40168
[1mStep[0m  [48/169], [94mLoss[0m : 1.46047
[1mStep[0m  [64/169], [94mLoss[0m : 1.52269
[1mStep[0m  [80/169], [94mLoss[0m : 1.61683
[1mStep[0m  [96/169], [94mLoss[0m : 1.53616
[1mStep[0m  [112/169], [94mLoss[0m : 1.29668
[1mStep[0m  [128/169], [94mLoss[0m : 1.71357
[1mStep[0m  [144/169], [94mLoss[0m : 1.92449
[1mStep[0m  [160/169], [94mLoss[0m : 1.68884

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.37116
[1mStep[0m  [16/169], [94mLoss[0m : 1.28703
[1mStep[0m  [32/169], [94mLoss[0m : 1.44466
[1mStep[0m  [48/169], [94mLoss[0m : 1.44767
[1mStep[0m  [64/169], [94mLoss[0m : 1.67814
[1mStep[0m  [80/169], [94mLoss[0m : 1.41950
[1mStep[0m  [96/169], [94mLoss[0m : 1.87548
[1mStep[0m  [112/169], [94mLoss[0m : 1.43708
[1mStep[0m  [128/169], [94mLoss[0m : 1.74215
[1mStep[0m  [144/169], [94mLoss[0m : 1.54120
[1mStep[0m  [160/169], [94mLoss[0m : 1.48381

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.531, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.37455
[1mStep[0m  [16/169], [94mLoss[0m : 1.33041
[1mStep[0m  [32/169], [94mLoss[0m : 1.45663
[1mStep[0m  [48/169], [94mLoss[0m : 1.55238
[1mStep[0m  [64/169], [94mLoss[0m : 1.19798
[1mStep[0m  [80/169], [94mLoss[0m : 1.55143
[1mStep[0m  [96/169], [94mLoss[0m : 1.35935
[1mStep[0m  [112/169], [94mLoss[0m : 1.59581
[1mStep[0m  [128/169], [94mLoss[0m : 1.38861
[1mStep[0m  [144/169], [94mLoss[0m : 1.29410
[1mStep[0m  [160/169], [94mLoss[0m : 1.52288

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.526, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55921
[1mStep[0m  [16/169], [94mLoss[0m : 1.97552
[1mStep[0m  [32/169], [94mLoss[0m : 1.12472
[1mStep[0m  [48/169], [94mLoss[0m : 1.46704
[1mStep[0m  [64/169], [94mLoss[0m : 1.46191
[1mStep[0m  [80/169], [94mLoss[0m : 1.44426
[1mStep[0m  [96/169], [94mLoss[0m : 1.72851
[1mStep[0m  [112/169], [94mLoss[0m : 1.49970
[1mStep[0m  [128/169], [94mLoss[0m : 1.20906
[1mStep[0m  [144/169], [94mLoss[0m : 1.51989
[1mStep[0m  [160/169], [94mLoss[0m : 1.50985

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.473, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36171
[1mStep[0m  [16/169], [94mLoss[0m : 1.46167
[1mStep[0m  [32/169], [94mLoss[0m : 1.45800
[1mStep[0m  [48/169], [94mLoss[0m : 1.87223
[1mStep[0m  [64/169], [94mLoss[0m : 1.47183
[1mStep[0m  [80/169], [94mLoss[0m : 1.77272
[1mStep[0m  [96/169], [94mLoss[0m : 1.66155
[1mStep[0m  [112/169], [94mLoss[0m : 1.50288
[1mStep[0m  [128/169], [94mLoss[0m : 1.41013
[1mStep[0m  [144/169], [94mLoss[0m : 1.88065
[1mStep[0m  [160/169], [94mLoss[0m : 1.28020

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.463, [92mTest[0m: 2.516, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42658
[1mStep[0m  [16/169], [94mLoss[0m : 1.52454
[1mStep[0m  [32/169], [94mLoss[0m : 1.59221
[1mStep[0m  [48/169], [94mLoss[0m : 1.26825
[1mStep[0m  [64/169], [94mLoss[0m : 1.58731
[1mStep[0m  [80/169], [94mLoss[0m : 1.49177
[1mStep[0m  [96/169], [94mLoss[0m : 1.35741
[1mStep[0m  [112/169], [94mLoss[0m : 1.40307
[1mStep[0m  [128/169], [94mLoss[0m : 1.47985
[1mStep[0m  [144/169], [94mLoss[0m : 1.58860
[1mStep[0m  [160/169], [94mLoss[0m : 1.39597

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.471, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.44614
[1mStep[0m  [16/169], [94mLoss[0m : 1.50141
[1mStep[0m  [32/169], [94mLoss[0m : 1.43809
[1mStep[0m  [48/169], [94mLoss[0m : 1.55115
[1mStep[0m  [64/169], [94mLoss[0m : 1.50930
[1mStep[0m  [80/169], [94mLoss[0m : 1.86266
[1mStep[0m  [96/169], [94mLoss[0m : 1.25046
[1mStep[0m  [112/169], [94mLoss[0m : 1.62120
[1mStep[0m  [128/169], [94mLoss[0m : 1.51793
[1mStep[0m  [144/169], [94mLoss[0m : 1.71669
[1mStep[0m  [160/169], [94mLoss[0m : 1.37653

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.440, [92mTest[0m: 2.511, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.473
====================================

Phase 2 - Evaluation MAE:  2.4732972596372877
MAE score P1        2.32637
MAE score P2       2.473297
loss               1.440198
learning_rate      0.007525
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping         True
dropout                 0.4
momentum                0.5
weight_decay         0.0001
Name: 6, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.32218
[1mStep[0m  [16/169], [94mLoss[0m : 10.75750
[1mStep[0m  [32/169], [94mLoss[0m : 11.52510
[1mStep[0m  [48/169], [94mLoss[0m : 10.23141
[1mStep[0m  [64/169], [94mLoss[0m : 10.21686
[1mStep[0m  [80/169], [94mLoss[0m : 10.46084
[1mStep[0m  [96/169], [94mLoss[0m : 9.21492
[1mStep[0m  [112/169], [94mLoss[0m : 9.40245
[1mStep[0m  [128/169], [94mLoss[0m : 9.25867
[1mStep[0m  [144/169], [94mLoss[0m : 9.88148
[1mStep[0m  [160/169], [94mLoss[0m : 9.18839

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 10.019, [92mTest[0m: 11.109, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 8.47689
[1mStep[0m  [16/169], [94mLoss[0m : 8.73953
[1mStep[0m  [32/169], [94mLoss[0m : 8.22059
[1mStep[0m  [48/169], [94mLoss[0m : 7.67511
[1mStep[0m  [64/169], [94mLoss[0m : 7.71600
[1mStep[0m  [80/169], [94mLoss[0m : 7.76413
[1mStep[0m  [96/169], [94mLoss[0m : 6.82253
[1mStep[0m  [112/169], [94mLoss[0m : 6.72613
[1mStep[0m  [128/169], [94mLoss[0m : 5.79011
[1mStep[0m  [144/169], [94mLoss[0m : 6.76309
[1mStep[0m  [160/169], [94mLoss[0m : 5.83932

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 7.115, [92mTest[0m: 8.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 5.52233
[1mStep[0m  [16/169], [94mLoss[0m : 5.53012
[1mStep[0m  [32/169], [94mLoss[0m : 4.38376
[1mStep[0m  [48/169], [94mLoss[0m : 4.13378
[1mStep[0m  [64/169], [94mLoss[0m : 4.29811
[1mStep[0m  [80/169], [94mLoss[0m : 3.79224
[1mStep[0m  [96/169], [94mLoss[0m : 3.64710
[1mStep[0m  [112/169], [94mLoss[0m : 3.62932
[1mStep[0m  [128/169], [94mLoss[0m : 3.34194
[1mStep[0m  [144/169], [94mLoss[0m : 2.60855
[1mStep[0m  [160/169], [94mLoss[0m : 2.89055

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 3.694, [92mTest[0m: 4.538, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66090
[1mStep[0m  [16/169], [94mLoss[0m : 2.95146
[1mStep[0m  [32/169], [94mLoss[0m : 2.48987
[1mStep[0m  [48/169], [94mLoss[0m : 2.39344
[1mStep[0m  [64/169], [94mLoss[0m : 2.48767
[1mStep[0m  [80/169], [94mLoss[0m : 2.52784
[1mStep[0m  [96/169], [94mLoss[0m : 2.52602
[1mStep[0m  [112/169], [94mLoss[0m : 2.50877
[1mStep[0m  [128/169], [94mLoss[0m : 2.82143
[1mStep[0m  [144/169], [94mLoss[0m : 2.84799
[1mStep[0m  [160/169], [94mLoss[0m : 2.87721

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.649, [92mTest[0m: 2.397, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73822
[1mStep[0m  [16/169], [94mLoss[0m : 2.72996
[1mStep[0m  [32/169], [94mLoss[0m : 2.41239
[1mStep[0m  [48/169], [94mLoss[0m : 2.28206
[1mStep[0m  [64/169], [94mLoss[0m : 2.39185
[1mStep[0m  [80/169], [94mLoss[0m : 2.53456
[1mStep[0m  [96/169], [94mLoss[0m : 2.17756
[1mStep[0m  [112/169], [94mLoss[0m : 2.48515
[1mStep[0m  [128/169], [94mLoss[0m : 2.72482
[1mStep[0m  [144/169], [94mLoss[0m : 2.55536
[1mStep[0m  [160/169], [94mLoss[0m : 2.71766

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.618, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85336
[1mStep[0m  [16/169], [94mLoss[0m : 2.71843
[1mStep[0m  [32/169], [94mLoss[0m : 2.71164
[1mStep[0m  [48/169], [94mLoss[0m : 3.02777
[1mStep[0m  [64/169], [94mLoss[0m : 2.53712
[1mStep[0m  [80/169], [94mLoss[0m : 2.80916
[1mStep[0m  [96/169], [94mLoss[0m : 2.25421
[1mStep[0m  [112/169], [94mLoss[0m : 2.66167
[1mStep[0m  [128/169], [94mLoss[0m : 2.36918
[1mStep[0m  [144/169], [94mLoss[0m : 2.36946
[1mStep[0m  [160/169], [94mLoss[0m : 2.65409

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.593, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27659
[1mStep[0m  [16/169], [94mLoss[0m : 2.67529
[1mStep[0m  [32/169], [94mLoss[0m : 2.03658
[1mStep[0m  [48/169], [94mLoss[0m : 2.71149
[1mStep[0m  [64/169], [94mLoss[0m : 2.82406
[1mStep[0m  [80/169], [94mLoss[0m : 2.57198
[1mStep[0m  [96/169], [94mLoss[0m : 2.36420
[1mStep[0m  [112/169], [94mLoss[0m : 2.30426
[1mStep[0m  [128/169], [94mLoss[0m : 2.45987
[1mStep[0m  [144/169], [94mLoss[0m : 2.74013
[1mStep[0m  [160/169], [94mLoss[0m : 3.33126

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.562, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71657
[1mStep[0m  [16/169], [94mLoss[0m : 2.52444
[1mStep[0m  [32/169], [94mLoss[0m : 2.75533
[1mStep[0m  [48/169], [94mLoss[0m : 2.57028
[1mStep[0m  [64/169], [94mLoss[0m : 2.39168
[1mStep[0m  [80/169], [94mLoss[0m : 2.87732
[1mStep[0m  [96/169], [94mLoss[0m : 2.76454
[1mStep[0m  [112/169], [94mLoss[0m : 2.02954
[1mStep[0m  [128/169], [94mLoss[0m : 2.63017
[1mStep[0m  [144/169], [94mLoss[0m : 2.72216
[1mStep[0m  [160/169], [94mLoss[0m : 2.85638

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.569, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.69903
[1mStep[0m  [16/169], [94mLoss[0m : 2.76920
[1mStep[0m  [32/169], [94mLoss[0m : 3.10083
[1mStep[0m  [48/169], [94mLoss[0m : 2.63001
[1mStep[0m  [64/169], [94mLoss[0m : 2.24163
[1mStep[0m  [80/169], [94mLoss[0m : 2.92161
[1mStep[0m  [96/169], [94mLoss[0m : 2.16297
[1mStep[0m  [112/169], [94mLoss[0m : 2.29100
[1mStep[0m  [128/169], [94mLoss[0m : 2.74506
[1mStep[0m  [144/169], [94mLoss[0m : 2.47967
[1mStep[0m  [160/169], [94mLoss[0m : 2.48446

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62756
[1mStep[0m  [16/169], [94mLoss[0m : 2.61310
[1mStep[0m  [32/169], [94mLoss[0m : 2.51720
[1mStep[0m  [48/169], [94mLoss[0m : 2.82348
[1mStep[0m  [64/169], [94mLoss[0m : 2.40486
[1mStep[0m  [80/169], [94mLoss[0m : 2.86818
[1mStep[0m  [96/169], [94mLoss[0m : 2.44679
[1mStep[0m  [112/169], [94mLoss[0m : 2.60088
[1mStep[0m  [128/169], [94mLoss[0m : 2.13279
[1mStep[0m  [144/169], [94mLoss[0m : 2.74303
[1mStep[0m  [160/169], [94mLoss[0m : 2.71394

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65426
[1mStep[0m  [16/169], [94mLoss[0m : 2.20394
[1mStep[0m  [32/169], [94mLoss[0m : 2.63546
[1mStep[0m  [48/169], [94mLoss[0m : 2.31173
[1mStep[0m  [64/169], [94mLoss[0m : 2.46008
[1mStep[0m  [80/169], [94mLoss[0m : 2.22124
[1mStep[0m  [96/169], [94mLoss[0m : 2.54770
[1mStep[0m  [112/169], [94mLoss[0m : 2.30558
[1mStep[0m  [128/169], [94mLoss[0m : 2.34092
[1mStep[0m  [144/169], [94mLoss[0m : 2.75563
[1mStep[0m  [160/169], [94mLoss[0m : 2.61182

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51636
[1mStep[0m  [16/169], [94mLoss[0m : 2.61629
[1mStep[0m  [32/169], [94mLoss[0m : 2.42395
[1mStep[0m  [48/169], [94mLoss[0m : 2.65033
[1mStep[0m  [64/169], [94mLoss[0m : 2.82682
[1mStep[0m  [80/169], [94mLoss[0m : 2.32006
[1mStep[0m  [96/169], [94mLoss[0m : 2.25172
[1mStep[0m  [112/169], [94mLoss[0m : 2.65286
[1mStep[0m  [128/169], [94mLoss[0m : 2.43613
[1mStep[0m  [144/169], [94mLoss[0m : 2.31695
[1mStep[0m  [160/169], [94mLoss[0m : 2.92143

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73959
[1mStep[0m  [16/169], [94mLoss[0m : 2.20675
[1mStep[0m  [32/169], [94mLoss[0m : 2.27129
[1mStep[0m  [48/169], [94mLoss[0m : 2.16700
[1mStep[0m  [64/169], [94mLoss[0m : 2.07798
[1mStep[0m  [80/169], [94mLoss[0m : 2.52481
[1mStep[0m  [96/169], [94mLoss[0m : 2.55797
[1mStep[0m  [112/169], [94mLoss[0m : 2.21175
[1mStep[0m  [128/169], [94mLoss[0m : 2.48318
[1mStep[0m  [144/169], [94mLoss[0m : 3.13862
[1mStep[0m  [160/169], [94mLoss[0m : 2.22357

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17828
[1mStep[0m  [16/169], [94mLoss[0m : 2.29264
[1mStep[0m  [32/169], [94mLoss[0m : 2.18485
[1mStep[0m  [48/169], [94mLoss[0m : 2.17278
[1mStep[0m  [64/169], [94mLoss[0m : 2.03031
[1mStep[0m  [80/169], [94mLoss[0m : 2.13398
[1mStep[0m  [96/169], [94mLoss[0m : 2.62757
[1mStep[0m  [112/169], [94mLoss[0m : 2.71531
[1mStep[0m  [128/169], [94mLoss[0m : 2.23332
[1mStep[0m  [144/169], [94mLoss[0m : 2.22455
[1mStep[0m  [160/169], [94mLoss[0m : 2.35453

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53739
[1mStep[0m  [16/169], [94mLoss[0m : 2.19904
[1mStep[0m  [32/169], [94mLoss[0m : 2.42007
[1mStep[0m  [48/169], [94mLoss[0m : 3.19581
[1mStep[0m  [64/169], [94mLoss[0m : 2.48953
[1mStep[0m  [80/169], [94mLoss[0m : 2.31155
[1mStep[0m  [96/169], [94mLoss[0m : 2.27789
[1mStep[0m  [112/169], [94mLoss[0m : 2.90454
[1mStep[0m  [128/169], [94mLoss[0m : 2.21444
[1mStep[0m  [144/169], [94mLoss[0m : 2.37108
[1mStep[0m  [160/169], [94mLoss[0m : 2.89181

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52219
[1mStep[0m  [16/169], [94mLoss[0m : 2.19547
[1mStep[0m  [32/169], [94mLoss[0m : 2.69592
[1mStep[0m  [48/169], [94mLoss[0m : 2.35820
[1mStep[0m  [64/169], [94mLoss[0m : 2.24217
[1mStep[0m  [80/169], [94mLoss[0m : 2.49704
[1mStep[0m  [96/169], [94mLoss[0m : 2.29250
[1mStep[0m  [112/169], [94mLoss[0m : 2.45568
[1mStep[0m  [128/169], [94mLoss[0m : 2.17237
[1mStep[0m  [144/169], [94mLoss[0m : 2.29578
[1mStep[0m  [160/169], [94mLoss[0m : 2.27840

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54034
[1mStep[0m  [16/169], [94mLoss[0m : 2.64319
[1mStep[0m  [32/169], [94mLoss[0m : 2.53169
[1mStep[0m  [48/169], [94mLoss[0m : 2.79653
[1mStep[0m  [64/169], [94mLoss[0m : 2.35245
[1mStep[0m  [80/169], [94mLoss[0m : 2.94887
[1mStep[0m  [96/169], [94mLoss[0m : 2.85049
[1mStep[0m  [112/169], [94mLoss[0m : 2.02402
[1mStep[0m  [128/169], [94mLoss[0m : 2.05882
[1mStep[0m  [144/169], [94mLoss[0m : 2.61243
[1mStep[0m  [160/169], [94mLoss[0m : 2.36210

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.455, [92mTest[0m: 2.315, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51207
[1mStep[0m  [16/169], [94mLoss[0m : 2.71672
[1mStep[0m  [32/169], [94mLoss[0m : 2.13904
[1mStep[0m  [48/169], [94mLoss[0m : 2.40592
[1mStep[0m  [64/169], [94mLoss[0m : 2.31630
[1mStep[0m  [80/169], [94mLoss[0m : 2.37208
[1mStep[0m  [96/169], [94mLoss[0m : 2.72962
[1mStep[0m  [112/169], [94mLoss[0m : 2.21525
[1mStep[0m  [128/169], [94mLoss[0m : 3.07038
[1mStep[0m  [144/169], [94mLoss[0m : 2.13060
[1mStep[0m  [160/169], [94mLoss[0m : 2.68381

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75556
[1mStep[0m  [16/169], [94mLoss[0m : 2.53131
[1mStep[0m  [32/169], [94mLoss[0m : 2.71424
[1mStep[0m  [48/169], [94mLoss[0m : 2.18048
[1mStep[0m  [64/169], [94mLoss[0m : 2.72217
[1mStep[0m  [80/169], [94mLoss[0m : 2.43638
[1mStep[0m  [96/169], [94mLoss[0m : 2.32692
[1mStep[0m  [112/169], [94mLoss[0m : 2.26109
[1mStep[0m  [128/169], [94mLoss[0m : 2.64430
[1mStep[0m  [144/169], [94mLoss[0m : 2.46302
[1mStep[0m  [160/169], [94mLoss[0m : 2.42623

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21396
[1mStep[0m  [16/169], [94mLoss[0m : 2.47302
[1mStep[0m  [32/169], [94mLoss[0m : 2.44675
[1mStep[0m  [48/169], [94mLoss[0m : 2.30218
[1mStep[0m  [64/169], [94mLoss[0m : 2.38749
[1mStep[0m  [80/169], [94mLoss[0m : 2.50649
[1mStep[0m  [96/169], [94mLoss[0m : 2.49135
[1mStep[0m  [112/169], [94mLoss[0m : 2.53087
[1mStep[0m  [128/169], [94mLoss[0m : 2.53743
[1mStep[0m  [144/169], [94mLoss[0m : 2.53909
[1mStep[0m  [160/169], [94mLoss[0m : 2.72190

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.435, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22780
[1mStep[0m  [16/169], [94mLoss[0m : 2.51497
[1mStep[0m  [32/169], [94mLoss[0m : 2.51642
[1mStep[0m  [48/169], [94mLoss[0m : 2.43538
[1mStep[0m  [64/169], [94mLoss[0m : 2.29114
[1mStep[0m  [80/169], [94mLoss[0m : 2.41233
[1mStep[0m  [96/169], [94mLoss[0m : 2.62811
[1mStep[0m  [112/169], [94mLoss[0m : 2.55818
[1mStep[0m  [128/169], [94mLoss[0m : 2.16432
[1mStep[0m  [144/169], [94mLoss[0m : 2.98651
[1mStep[0m  [160/169], [94mLoss[0m : 2.70404

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.30614
[1mStep[0m  [16/169], [94mLoss[0m : 1.98877
[1mStep[0m  [32/169], [94mLoss[0m : 2.63121
[1mStep[0m  [48/169], [94mLoss[0m : 2.87235
[1mStep[0m  [64/169], [94mLoss[0m : 2.69264
[1mStep[0m  [80/169], [94mLoss[0m : 2.59126
[1mStep[0m  [96/169], [94mLoss[0m : 2.57111
[1mStep[0m  [112/169], [94mLoss[0m : 2.62882
[1mStep[0m  [128/169], [94mLoss[0m : 2.46527
[1mStep[0m  [144/169], [94mLoss[0m : 2.50632
[1mStep[0m  [160/169], [94mLoss[0m : 2.12322

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.93014
[1mStep[0m  [16/169], [94mLoss[0m : 2.19639
[1mStep[0m  [32/169], [94mLoss[0m : 2.25079
[1mStep[0m  [48/169], [94mLoss[0m : 2.44991
[1mStep[0m  [64/169], [94mLoss[0m : 2.22682
[1mStep[0m  [80/169], [94mLoss[0m : 2.33941
[1mStep[0m  [96/169], [94mLoss[0m : 2.52196
[1mStep[0m  [112/169], [94mLoss[0m : 2.57392
[1mStep[0m  [128/169], [94mLoss[0m : 2.23704
[1mStep[0m  [144/169], [94mLoss[0m : 2.51798
[1mStep[0m  [160/169], [94mLoss[0m : 2.12842

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21379
[1mStep[0m  [16/169], [94mLoss[0m : 2.71722
[1mStep[0m  [32/169], [94mLoss[0m : 2.19024
[1mStep[0m  [48/169], [94mLoss[0m : 2.50003
[1mStep[0m  [64/169], [94mLoss[0m : 2.27887
[1mStep[0m  [80/169], [94mLoss[0m : 2.34399
[1mStep[0m  [96/169], [94mLoss[0m : 1.82571
[1mStep[0m  [112/169], [94mLoss[0m : 2.47653
[1mStep[0m  [128/169], [94mLoss[0m : 2.32584
[1mStep[0m  [144/169], [94mLoss[0m : 2.23176
[1mStep[0m  [160/169], [94mLoss[0m : 2.54037

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.419, [92mTest[0m: 2.321, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.02613
[1mStep[0m  [16/169], [94mLoss[0m : 2.47521
[1mStep[0m  [32/169], [94mLoss[0m : 2.33647
[1mStep[0m  [48/169], [94mLoss[0m : 2.43797
[1mStep[0m  [64/169], [94mLoss[0m : 2.23031
[1mStep[0m  [80/169], [94mLoss[0m : 2.24953
[1mStep[0m  [96/169], [94mLoss[0m : 2.16986
[1mStep[0m  [112/169], [94mLoss[0m : 2.20106
[1mStep[0m  [128/169], [94mLoss[0m : 2.05992
[1mStep[0m  [144/169], [94mLoss[0m : 2.59593
[1mStep[0m  [160/169], [94mLoss[0m : 2.84181

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.420, [92mTest[0m: 2.347, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44139
[1mStep[0m  [16/169], [94mLoss[0m : 2.55113
[1mStep[0m  [32/169], [94mLoss[0m : 2.47343
[1mStep[0m  [48/169], [94mLoss[0m : 2.09784
[1mStep[0m  [64/169], [94mLoss[0m : 1.81694
[1mStep[0m  [80/169], [94mLoss[0m : 2.45765
[1mStep[0m  [96/169], [94mLoss[0m : 2.46769
[1mStep[0m  [112/169], [94mLoss[0m : 2.45101
[1mStep[0m  [128/169], [94mLoss[0m : 2.51973
[1mStep[0m  [144/169], [94mLoss[0m : 2.63295
[1mStep[0m  [160/169], [94mLoss[0m : 1.96254

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.416, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25858
[1mStep[0m  [16/169], [94mLoss[0m : 2.67281
[1mStep[0m  [32/169], [94mLoss[0m : 2.63708
[1mStep[0m  [48/169], [94mLoss[0m : 2.34073
[1mStep[0m  [64/169], [94mLoss[0m : 3.00750
[1mStep[0m  [80/169], [94mLoss[0m : 2.67743
[1mStep[0m  [96/169], [94mLoss[0m : 2.15292
[1mStep[0m  [112/169], [94mLoss[0m : 2.44533
[1mStep[0m  [128/169], [94mLoss[0m : 2.70584
[1mStep[0m  [144/169], [94mLoss[0m : 2.37656
[1mStep[0m  [160/169], [94mLoss[0m : 2.69440

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.345, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26283
[1mStep[0m  [16/169], [94mLoss[0m : 2.26201
[1mStep[0m  [32/169], [94mLoss[0m : 2.46806
[1mStep[0m  [48/169], [94mLoss[0m : 2.43382
[1mStep[0m  [64/169], [94mLoss[0m : 2.44222
[1mStep[0m  [80/169], [94mLoss[0m : 2.01867
[1mStep[0m  [96/169], [94mLoss[0m : 2.39302
[1mStep[0m  [112/169], [94mLoss[0m : 2.14158
[1mStep[0m  [128/169], [94mLoss[0m : 3.03958
[1mStep[0m  [144/169], [94mLoss[0m : 2.29321
[1mStep[0m  [160/169], [94mLoss[0m : 2.18301

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.395, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08722
[1mStep[0m  [16/169], [94mLoss[0m : 2.47997
[1mStep[0m  [32/169], [94mLoss[0m : 2.39526
[1mStep[0m  [48/169], [94mLoss[0m : 2.29233
[1mStep[0m  [64/169], [94mLoss[0m : 2.63855
[1mStep[0m  [80/169], [94mLoss[0m : 2.87426
[1mStep[0m  [96/169], [94mLoss[0m : 2.40429
[1mStep[0m  [112/169], [94mLoss[0m : 2.25989
[1mStep[0m  [128/169], [94mLoss[0m : 2.40780
[1mStep[0m  [144/169], [94mLoss[0m : 2.31499
[1mStep[0m  [160/169], [94mLoss[0m : 2.58495

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.404, [92mTest[0m: 2.362, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18901
[1mStep[0m  [16/169], [94mLoss[0m : 2.91950
[1mStep[0m  [32/169], [94mLoss[0m : 2.26744
[1mStep[0m  [48/169], [94mLoss[0m : 2.08385
[1mStep[0m  [64/169], [94mLoss[0m : 2.01868
[1mStep[0m  [80/169], [94mLoss[0m : 2.12101
[1mStep[0m  [96/169], [94mLoss[0m : 2.10984
[1mStep[0m  [112/169], [94mLoss[0m : 2.75744
[1mStep[0m  [128/169], [94mLoss[0m : 2.66684
[1mStep[0m  [144/169], [94mLoss[0m : 2.07719
[1mStep[0m  [160/169], [94mLoss[0m : 2.52409

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.396, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.327
====================================

Phase 1 - Evaluation MAE:  2.3272181004285812
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=300, bias=True)
        (in_batch norm): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=300, out_features=100, bias=True)
        (0_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.71177
[1mStep[0m  [16/169], [94mLoss[0m : 2.26488
[1mStep[0m  [32/169], [94mLoss[0m : 2.67124
[1mStep[0m  [48/169], [94mLoss[0m : 2.63202
[1mStep[0m  [64/169], [94mLoss[0m : 2.67450
[1mStep[0m  [80/169], [94mLoss[0m : 2.39335
[1mStep[0m  [96/169], [94mLoss[0m : 2.80747
[1mStep[0m  [112/169], [94mLoss[0m : 2.43850
[1mStep[0m  [128/169], [94mLoss[0m : 2.50407
[1mStep[0m  [144/169], [94mLoss[0m : 2.85699
[1mStep[0m  [160/169], [94mLoss[0m : 2.29275

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83602
[1mStep[0m  [16/169], [94mLoss[0m : 2.17748
[1mStep[0m  [32/169], [94mLoss[0m : 2.20275
[1mStep[0m  [48/169], [94mLoss[0m : 2.66594
[1mStep[0m  [64/169], [94mLoss[0m : 2.51171
[1mStep[0m  [80/169], [94mLoss[0m : 2.19175
[1mStep[0m  [96/169], [94mLoss[0m : 2.13687
[1mStep[0m  [112/169], [94mLoss[0m : 2.46864
[1mStep[0m  [128/169], [94mLoss[0m : 2.66813
[1mStep[0m  [144/169], [94mLoss[0m : 2.07333
[1mStep[0m  [160/169], [94mLoss[0m : 2.65290

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.399, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35632
[1mStep[0m  [16/169], [94mLoss[0m : 2.35354
[1mStep[0m  [32/169], [94mLoss[0m : 2.45827
[1mStep[0m  [48/169], [94mLoss[0m : 2.24816
[1mStep[0m  [64/169], [94mLoss[0m : 2.22952
[1mStep[0m  [80/169], [94mLoss[0m : 1.88977
[1mStep[0m  [96/169], [94mLoss[0m : 2.34590
[1mStep[0m  [112/169], [94mLoss[0m : 1.97883
[1mStep[0m  [128/169], [94mLoss[0m : 2.25974
[1mStep[0m  [144/169], [94mLoss[0m : 2.57595
[1mStep[0m  [160/169], [94mLoss[0m : 2.38129

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.342, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.07874
[1mStep[0m  [16/169], [94mLoss[0m : 2.56080
[1mStep[0m  [32/169], [94mLoss[0m : 2.35177
[1mStep[0m  [48/169], [94mLoss[0m : 2.03806
[1mStep[0m  [64/169], [94mLoss[0m : 2.01490
[1mStep[0m  [80/169], [94mLoss[0m : 2.45580
[1mStep[0m  [96/169], [94mLoss[0m : 2.65646
[1mStep[0m  [112/169], [94mLoss[0m : 1.93603
[1mStep[0m  [128/169], [94mLoss[0m : 1.81335
[1mStep[0m  [144/169], [94mLoss[0m : 2.49062
[1mStep[0m  [160/169], [94mLoss[0m : 2.07935

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.282, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.92284
[1mStep[0m  [16/169], [94mLoss[0m : 2.65106
[1mStep[0m  [32/169], [94mLoss[0m : 1.97392
[1mStep[0m  [48/169], [94mLoss[0m : 2.27780
[1mStep[0m  [64/169], [94mLoss[0m : 2.33396
[1mStep[0m  [80/169], [94mLoss[0m : 2.65148
[1mStep[0m  [96/169], [94mLoss[0m : 2.48515
[1mStep[0m  [112/169], [94mLoss[0m : 2.16853
[1mStep[0m  [128/169], [94mLoss[0m : 1.78624
[1mStep[0m  [144/169], [94mLoss[0m : 2.94492
[1mStep[0m  [160/169], [94mLoss[0m : 2.19659

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.217, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22313
[1mStep[0m  [16/169], [94mLoss[0m : 2.19682
[1mStep[0m  [32/169], [94mLoss[0m : 2.03566
[1mStep[0m  [48/169], [94mLoss[0m : 1.92626
[1mStep[0m  [64/169], [94mLoss[0m : 2.12108
[1mStep[0m  [80/169], [94mLoss[0m : 1.84633
[1mStep[0m  [96/169], [94mLoss[0m : 2.05243
[1mStep[0m  [112/169], [94mLoss[0m : 2.44868
[1mStep[0m  [128/169], [94mLoss[0m : 2.23587
[1mStep[0m  [144/169], [94mLoss[0m : 2.31503
[1mStep[0m  [160/169], [94mLoss[0m : 1.60188

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86488
[1mStep[0m  [16/169], [94mLoss[0m : 1.98336
[1mStep[0m  [32/169], [94mLoss[0m : 2.29435
[1mStep[0m  [48/169], [94mLoss[0m : 1.83350
[1mStep[0m  [64/169], [94mLoss[0m : 2.12257
[1mStep[0m  [80/169], [94mLoss[0m : 2.10574
[1mStep[0m  [96/169], [94mLoss[0m : 1.92921
[1mStep[0m  [112/169], [94mLoss[0m : 1.71973
[1mStep[0m  [128/169], [94mLoss[0m : 2.26486
[1mStep[0m  [144/169], [94mLoss[0m : 1.74157
[1mStep[0m  [160/169], [94mLoss[0m : 1.77209

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.109, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26953
[1mStep[0m  [16/169], [94mLoss[0m : 1.82038
[1mStep[0m  [32/169], [94mLoss[0m : 1.85505
[1mStep[0m  [48/169], [94mLoss[0m : 1.86921
[1mStep[0m  [64/169], [94mLoss[0m : 2.51235
[1mStep[0m  [80/169], [94mLoss[0m : 2.25142
[1mStep[0m  [96/169], [94mLoss[0m : 2.07017
[1mStep[0m  [112/169], [94mLoss[0m : 1.93793
[1mStep[0m  [128/169], [94mLoss[0m : 2.18143
[1mStep[0m  [144/169], [94mLoss[0m : 2.07651
[1mStep[0m  [160/169], [94mLoss[0m : 2.38950

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.070, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98687
[1mStep[0m  [16/169], [94mLoss[0m : 1.99432
[1mStep[0m  [32/169], [94mLoss[0m : 1.97098
[1mStep[0m  [48/169], [94mLoss[0m : 2.26741
[1mStep[0m  [64/169], [94mLoss[0m : 1.61236
[1mStep[0m  [80/169], [94mLoss[0m : 2.13608
[1mStep[0m  [96/169], [94mLoss[0m : 1.83889
[1mStep[0m  [112/169], [94mLoss[0m : 1.82339
[1mStep[0m  [128/169], [94mLoss[0m : 2.12900
[1mStep[0m  [144/169], [94mLoss[0m : 1.93725
[1mStep[0m  [160/169], [94mLoss[0m : 2.32274

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.019, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.05364
[1mStep[0m  [16/169], [94mLoss[0m : 1.89596
[1mStep[0m  [32/169], [94mLoss[0m : 2.19504
[1mStep[0m  [48/169], [94mLoss[0m : 2.08699
[1mStep[0m  [64/169], [94mLoss[0m : 2.16258
[1mStep[0m  [80/169], [94mLoss[0m : 1.88804
[1mStep[0m  [96/169], [94mLoss[0m : 1.81907
[1mStep[0m  [112/169], [94mLoss[0m : 2.19330
[1mStep[0m  [128/169], [94mLoss[0m : 1.92290
[1mStep[0m  [144/169], [94mLoss[0m : 2.09253
[1mStep[0m  [160/169], [94mLoss[0m : 1.75608

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.958, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90585
[1mStep[0m  [16/169], [94mLoss[0m : 1.92630
[1mStep[0m  [32/169], [94mLoss[0m : 1.98206
[1mStep[0m  [48/169], [94mLoss[0m : 1.71194
[1mStep[0m  [64/169], [94mLoss[0m : 1.75954
[1mStep[0m  [80/169], [94mLoss[0m : 2.09598
[1mStep[0m  [96/169], [94mLoss[0m : 1.75356
[1mStep[0m  [112/169], [94mLoss[0m : 1.86022
[1mStep[0m  [128/169], [94mLoss[0m : 2.04040
[1mStep[0m  [144/169], [94mLoss[0m : 1.63672
[1mStep[0m  [160/169], [94mLoss[0m : 1.88641

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.914, [92mTest[0m: 2.406, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03039
[1mStep[0m  [16/169], [94mLoss[0m : 1.34755
[1mStep[0m  [32/169], [94mLoss[0m : 1.95274
[1mStep[0m  [48/169], [94mLoss[0m : 1.70380
[1mStep[0m  [64/169], [94mLoss[0m : 1.90304
[1mStep[0m  [80/169], [94mLoss[0m : 1.97326
[1mStep[0m  [96/169], [94mLoss[0m : 1.85012
[1mStep[0m  [112/169], [94mLoss[0m : 2.39943
[1mStep[0m  [128/169], [94mLoss[0m : 1.93622
[1mStep[0m  [144/169], [94mLoss[0m : 1.71226
[1mStep[0m  [160/169], [94mLoss[0m : 2.09467

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.860, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56575
[1mStep[0m  [16/169], [94mLoss[0m : 2.12858
[1mStep[0m  [32/169], [94mLoss[0m : 1.87332
[1mStep[0m  [48/169], [94mLoss[0m : 1.33065
[1mStep[0m  [64/169], [94mLoss[0m : 1.53322
[1mStep[0m  [80/169], [94mLoss[0m : 1.69512
[1mStep[0m  [96/169], [94mLoss[0m : 1.58791
[1mStep[0m  [112/169], [94mLoss[0m : 1.79404
[1mStep[0m  [128/169], [94mLoss[0m : 2.14786
[1mStep[0m  [144/169], [94mLoss[0m : 2.03299
[1mStep[0m  [160/169], [94mLoss[0m : 1.58824

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66211
[1mStep[0m  [16/169], [94mLoss[0m : 1.56403
[1mStep[0m  [32/169], [94mLoss[0m : 1.90898
[1mStep[0m  [48/169], [94mLoss[0m : 1.59652
[1mStep[0m  [64/169], [94mLoss[0m : 1.71895
[1mStep[0m  [80/169], [94mLoss[0m : 1.76964
[1mStep[0m  [96/169], [94mLoss[0m : 1.89317
[1mStep[0m  [112/169], [94mLoss[0m : 1.69610
[1mStep[0m  [128/169], [94mLoss[0m : 2.13839
[1mStep[0m  [144/169], [94mLoss[0m : 1.66141
[1mStep[0m  [160/169], [94mLoss[0m : 1.41444

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.437, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.60457
[1mStep[0m  [16/169], [94mLoss[0m : 1.59196
[1mStep[0m  [32/169], [94mLoss[0m : 1.59157
[1mStep[0m  [48/169], [94mLoss[0m : 1.43021
[1mStep[0m  [64/169], [94mLoss[0m : 1.95052
[1mStep[0m  [80/169], [94mLoss[0m : 1.83243
[1mStep[0m  [96/169], [94mLoss[0m : 1.57384
[1mStep[0m  [112/169], [94mLoss[0m : 2.07786
[1mStep[0m  [128/169], [94mLoss[0m : 1.99004
[1mStep[0m  [144/169], [94mLoss[0m : 1.29186
[1mStep[0m  [160/169], [94mLoss[0m : 1.78914

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.749, [92mTest[0m: 2.427, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.82645
[1mStep[0m  [16/169], [94mLoss[0m : 1.62173
[1mStep[0m  [32/169], [94mLoss[0m : 1.43573
[1mStep[0m  [48/169], [94mLoss[0m : 1.60817
[1mStep[0m  [64/169], [94mLoss[0m : 1.72736
[1mStep[0m  [80/169], [94mLoss[0m : 1.32751
[1mStep[0m  [96/169], [94mLoss[0m : 1.92363
[1mStep[0m  [112/169], [94mLoss[0m : 1.56752
[1mStep[0m  [128/169], [94mLoss[0m : 1.85960
[1mStep[0m  [144/169], [94mLoss[0m : 2.20273
[1mStep[0m  [160/169], [94mLoss[0m : 1.68455

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.706, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.71309
[1mStep[0m  [16/169], [94mLoss[0m : 2.10020
[1mStep[0m  [32/169], [94mLoss[0m : 1.83782
[1mStep[0m  [48/169], [94mLoss[0m : 1.55235
[1mStep[0m  [64/169], [94mLoss[0m : 1.38324
[1mStep[0m  [80/169], [94mLoss[0m : 1.76850
[1mStep[0m  [96/169], [94mLoss[0m : 1.64760
[1mStep[0m  [112/169], [94mLoss[0m : 1.91802
[1mStep[0m  [128/169], [94mLoss[0m : 1.91680
[1mStep[0m  [144/169], [94mLoss[0m : 1.48219
[1mStep[0m  [160/169], [94mLoss[0m : 1.68359

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.698, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.52756
[1mStep[0m  [16/169], [94mLoss[0m : 1.50972
[1mStep[0m  [32/169], [94mLoss[0m : 1.77588
[1mStep[0m  [48/169], [94mLoss[0m : 1.52427
[1mStep[0m  [64/169], [94mLoss[0m : 1.94623
[1mStep[0m  [80/169], [94mLoss[0m : 1.41697
[1mStep[0m  [96/169], [94mLoss[0m : 1.71576
[1mStep[0m  [112/169], [94mLoss[0m : 1.50184
[1mStep[0m  [128/169], [94mLoss[0m : 1.57212
[1mStep[0m  [144/169], [94mLoss[0m : 1.48165
[1mStep[0m  [160/169], [94mLoss[0m : 1.69813

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.654, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.56828
[1mStep[0m  [16/169], [94mLoss[0m : 1.55700
[1mStep[0m  [32/169], [94mLoss[0m : 1.70887
[1mStep[0m  [48/169], [94mLoss[0m : 1.75513
[1mStep[0m  [64/169], [94mLoss[0m : 1.74437
[1mStep[0m  [80/169], [94mLoss[0m : 1.43331
[1mStep[0m  [96/169], [94mLoss[0m : 1.60616
[1mStep[0m  [112/169], [94mLoss[0m : 1.70396
[1mStep[0m  [128/169], [94mLoss[0m : 1.91370
[1mStep[0m  [144/169], [94mLoss[0m : 1.43026
[1mStep[0m  [160/169], [94mLoss[0m : 1.57053

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.630, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39291
[1mStep[0m  [16/169], [94mLoss[0m : 1.60646
[1mStep[0m  [32/169], [94mLoss[0m : 2.01461
[1mStep[0m  [48/169], [94mLoss[0m : 1.48197
[1mStep[0m  [64/169], [94mLoss[0m : 1.50719
[1mStep[0m  [80/169], [94mLoss[0m : 1.26953
[1mStep[0m  [96/169], [94mLoss[0m : 1.94276
[1mStep[0m  [112/169], [94mLoss[0m : 1.56624
[1mStep[0m  [128/169], [94mLoss[0m : 1.34344
[1mStep[0m  [144/169], [94mLoss[0m : 1.58126
[1mStep[0m  [160/169], [94mLoss[0m : 1.83313

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.596, [92mTest[0m: 2.455, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32343
[1mStep[0m  [16/169], [94mLoss[0m : 1.24914
[1mStep[0m  [32/169], [94mLoss[0m : 1.81877
[1mStep[0m  [48/169], [94mLoss[0m : 1.36279
[1mStep[0m  [64/169], [94mLoss[0m : 1.51563
[1mStep[0m  [80/169], [94mLoss[0m : 1.74159
[1mStep[0m  [96/169], [94mLoss[0m : 1.56823
[1mStep[0m  [112/169], [94mLoss[0m : 1.57391
[1mStep[0m  [128/169], [94mLoss[0m : 1.47057
[1mStep[0m  [144/169], [94mLoss[0m : 1.76688
[1mStep[0m  [160/169], [94mLoss[0m : 1.68290

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.456, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51637
[1mStep[0m  [16/169], [94mLoss[0m : 1.31688
[1mStep[0m  [32/169], [94mLoss[0m : 1.24859
[1mStep[0m  [48/169], [94mLoss[0m : 1.60901
[1mStep[0m  [64/169], [94mLoss[0m : 1.78442
[1mStep[0m  [80/169], [94mLoss[0m : 1.37284
[1mStep[0m  [96/169], [94mLoss[0m : 1.20228
[1mStep[0m  [112/169], [94mLoss[0m : 1.57360
[1mStep[0m  [128/169], [94mLoss[0m : 1.21397
[1mStep[0m  [144/169], [94mLoss[0m : 1.63548
[1mStep[0m  [160/169], [94mLoss[0m : 1.38066

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57223
[1mStep[0m  [16/169], [94mLoss[0m : 1.30555
[1mStep[0m  [32/169], [94mLoss[0m : 1.40974
[1mStep[0m  [48/169], [94mLoss[0m : 1.67608
[1mStep[0m  [64/169], [94mLoss[0m : 1.41831
[1mStep[0m  [80/169], [94mLoss[0m : 1.80530
[1mStep[0m  [96/169], [94mLoss[0m : 1.15002
[1mStep[0m  [112/169], [94mLoss[0m : 1.86196
[1mStep[0m  [128/169], [94mLoss[0m : 1.41221
[1mStep[0m  [144/169], [94mLoss[0m : 1.67430
[1mStep[0m  [160/169], [94mLoss[0m : 1.54062

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.498, [92mTest[0m: 2.462, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40932
[1mStep[0m  [16/169], [94mLoss[0m : 1.36294
[1mStep[0m  [32/169], [94mLoss[0m : 1.37519
[1mStep[0m  [48/169], [94mLoss[0m : 1.30243
[1mStep[0m  [64/169], [94mLoss[0m : 1.57152
[1mStep[0m  [80/169], [94mLoss[0m : 1.65870
[1mStep[0m  [96/169], [94mLoss[0m : 1.29870
[1mStep[0m  [112/169], [94mLoss[0m : 1.49036
[1mStep[0m  [128/169], [94mLoss[0m : 1.81146
[1mStep[0m  [144/169], [94mLoss[0m : 1.49235
[1mStep[0m  [160/169], [94mLoss[0m : 1.50251

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.492, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.16254
[1mStep[0m  [16/169], [94mLoss[0m : 1.36091
[1mStep[0m  [32/169], [94mLoss[0m : 1.52530
[1mStep[0m  [48/169], [94mLoss[0m : 1.46271
[1mStep[0m  [64/169], [94mLoss[0m : 1.68542
[1mStep[0m  [80/169], [94mLoss[0m : 1.30310
[1mStep[0m  [96/169], [94mLoss[0m : 1.35460
[1mStep[0m  [112/169], [94mLoss[0m : 1.39642
[1mStep[0m  [128/169], [94mLoss[0m : 1.35137
[1mStep[0m  [144/169], [94mLoss[0m : 1.48706
[1mStep[0m  [160/169], [94mLoss[0m : 1.43679

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.472, [92mTest[0m: 2.487, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.33452
[1mStep[0m  [16/169], [94mLoss[0m : 1.26364
[1mStep[0m  [32/169], [94mLoss[0m : 1.24674
[1mStep[0m  [48/169], [94mLoss[0m : 1.50873
[1mStep[0m  [64/169], [94mLoss[0m : 1.66429
[1mStep[0m  [80/169], [94mLoss[0m : 1.64322
[1mStep[0m  [96/169], [94mLoss[0m : 1.16505
[1mStep[0m  [112/169], [94mLoss[0m : 1.94360
[1mStep[0m  [128/169], [94mLoss[0m : 1.55317
[1mStep[0m  [144/169], [94mLoss[0m : 1.36741
[1mStep[0m  [160/169], [94mLoss[0m : 1.48067

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.451, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.08620
[1mStep[0m  [16/169], [94mLoss[0m : 1.50833
[1mStep[0m  [32/169], [94mLoss[0m : 1.31318
[1mStep[0m  [48/169], [94mLoss[0m : 1.08879
[1mStep[0m  [64/169], [94mLoss[0m : 1.15491
[1mStep[0m  [80/169], [94mLoss[0m : 1.18555
[1mStep[0m  [96/169], [94mLoss[0m : 1.45669
[1mStep[0m  [112/169], [94mLoss[0m : 1.23704
[1mStep[0m  [128/169], [94mLoss[0m : 1.52185
[1mStep[0m  [144/169], [94mLoss[0m : 1.33607
[1mStep[0m  [160/169], [94mLoss[0m : 1.53784

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.428, [92mTest[0m: 2.463, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55048
[1mStep[0m  [16/169], [94mLoss[0m : 1.36143
[1mStep[0m  [32/169], [94mLoss[0m : 1.24479
[1mStep[0m  [48/169], [94mLoss[0m : 1.20362
[1mStep[0m  [64/169], [94mLoss[0m : 1.25992
[1mStep[0m  [80/169], [94mLoss[0m : 1.63802
[1mStep[0m  [96/169], [94mLoss[0m : 1.28513
[1mStep[0m  [112/169], [94mLoss[0m : 1.55076
[1mStep[0m  [128/169], [94mLoss[0m : 1.75442
[1mStep[0m  [144/169], [94mLoss[0m : 1.43683
[1mStep[0m  [160/169], [94mLoss[0m : 1.55410

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.423, [92mTest[0m: 2.485, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 27 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.492
====================================

Phase 2 - Evaluation MAE:  2.4915419199637006
MAE score P1        2.327218
MAE score P2        2.491542
loss                1.423416
learning_rate       0.007525
batch_size                64
hidden_sizes      [300, 100]
epochs                    30
activation              tanh
optimizer                sgd
early stopping          True
dropout                  0.2
momentum                 0.1
weight_decay           0.001
Name: 7, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 11.01408
[1mStep[0m  [16/169], [94mLoss[0m : 7.04108
[1mStep[0m  [32/169], [94mLoss[0m : 3.95490
[1mStep[0m  [48/169], [94mLoss[0m : 3.01996
[1mStep[0m  [64/169], [94mLoss[0m : 3.15665
[1mStep[0m  [80/169], [94mLoss[0m : 2.76284
[1mStep[0m  [96/169], [94mLoss[0m : 2.28596
[1mStep[0m  [112/169], [94mLoss[0m : 2.38327
[1mStep[0m  [128/169], [94mLoss[0m : 2.70968
[1mStep[0m  [144/169], [94mLoss[0m : 2.10430
[1mStep[0m  [160/169], [94mLoss[0m : 2.57741

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.632, [92mTest[0m: 10.776, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84786
[1mStep[0m  [16/169], [94mLoss[0m : 2.54852
[1mStep[0m  [32/169], [94mLoss[0m : 2.02741
[1mStep[0m  [48/169], [94mLoss[0m : 2.03023
[1mStep[0m  [64/169], [94mLoss[0m : 2.55283
[1mStep[0m  [80/169], [94mLoss[0m : 2.74183
[1mStep[0m  [96/169], [94mLoss[0m : 1.88574
[1mStep[0m  [112/169], [94mLoss[0m : 2.45912
[1mStep[0m  [128/169], [94mLoss[0m : 2.69324
[1mStep[0m  [144/169], [94mLoss[0m : 2.29638
[1mStep[0m  [160/169], [94mLoss[0m : 1.93456

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73047
[1mStep[0m  [16/169], [94mLoss[0m : 2.69170
[1mStep[0m  [32/169], [94mLoss[0m : 2.17868
[1mStep[0m  [48/169], [94mLoss[0m : 2.36897
[1mStep[0m  [64/169], [94mLoss[0m : 2.58992
[1mStep[0m  [80/169], [94mLoss[0m : 2.93792
[1mStep[0m  [96/169], [94mLoss[0m : 2.60121
[1mStep[0m  [112/169], [94mLoss[0m : 2.66871
[1mStep[0m  [128/169], [94mLoss[0m : 2.29920
[1mStep[0m  [144/169], [94mLoss[0m : 2.26392
[1mStep[0m  [160/169], [94mLoss[0m : 2.53772

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.533, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64470
[1mStep[0m  [16/169], [94mLoss[0m : 2.53327
[1mStep[0m  [32/169], [94mLoss[0m : 2.66908
[1mStep[0m  [48/169], [94mLoss[0m : 2.37042
[1mStep[0m  [64/169], [94mLoss[0m : 2.99280
[1mStep[0m  [80/169], [94mLoss[0m : 3.38832
[1mStep[0m  [96/169], [94mLoss[0m : 2.09689
[1mStep[0m  [112/169], [94mLoss[0m : 2.56056
[1mStep[0m  [128/169], [94mLoss[0m : 1.96630
[1mStep[0m  [144/169], [94mLoss[0m : 2.70669
[1mStep[0m  [160/169], [94mLoss[0m : 2.73937

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.538, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.02470
[1mStep[0m  [16/169], [94mLoss[0m : 2.88879
[1mStep[0m  [32/169], [94mLoss[0m : 2.18425
[1mStep[0m  [48/169], [94mLoss[0m : 2.29430
[1mStep[0m  [64/169], [94mLoss[0m : 2.14814
[1mStep[0m  [80/169], [94mLoss[0m : 1.99030
[1mStep[0m  [96/169], [94mLoss[0m : 2.14480
[1mStep[0m  [112/169], [94mLoss[0m : 2.32355
[1mStep[0m  [128/169], [94mLoss[0m : 2.41967
[1mStep[0m  [144/169], [94mLoss[0m : 2.51887
[1mStep[0m  [160/169], [94mLoss[0m : 2.47273

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.76425
[1mStep[0m  [16/169], [94mLoss[0m : 2.33183
[1mStep[0m  [32/169], [94mLoss[0m : 2.71084
[1mStep[0m  [48/169], [94mLoss[0m : 2.59496
[1mStep[0m  [64/169], [94mLoss[0m : 2.61121
[1mStep[0m  [80/169], [94mLoss[0m : 2.44214
[1mStep[0m  [96/169], [94mLoss[0m : 2.67217
[1mStep[0m  [112/169], [94mLoss[0m : 2.63477
[1mStep[0m  [128/169], [94mLoss[0m : 2.66115
[1mStep[0m  [144/169], [94mLoss[0m : 2.88322
[1mStep[0m  [160/169], [94mLoss[0m : 2.79423

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.500, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13439
[1mStep[0m  [16/169], [94mLoss[0m : 2.71916
[1mStep[0m  [32/169], [94mLoss[0m : 2.32016
[1mStep[0m  [48/169], [94mLoss[0m : 2.90480
[1mStep[0m  [64/169], [94mLoss[0m : 2.46051
[1mStep[0m  [80/169], [94mLoss[0m : 2.61859
[1mStep[0m  [96/169], [94mLoss[0m : 2.18084
[1mStep[0m  [112/169], [94mLoss[0m : 2.84602
[1mStep[0m  [128/169], [94mLoss[0m : 1.84527
[1mStep[0m  [144/169], [94mLoss[0m : 2.28174
[1mStep[0m  [160/169], [94mLoss[0m : 2.85688

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53157
[1mStep[0m  [16/169], [94mLoss[0m : 2.56624
[1mStep[0m  [32/169], [94mLoss[0m : 2.63611
[1mStep[0m  [48/169], [94mLoss[0m : 2.15405
[1mStep[0m  [64/169], [94mLoss[0m : 2.49879
[1mStep[0m  [80/169], [94mLoss[0m : 2.33078
[1mStep[0m  [96/169], [94mLoss[0m : 2.85731
[1mStep[0m  [112/169], [94mLoss[0m : 2.39999
[1mStep[0m  [128/169], [94mLoss[0m : 2.28096
[1mStep[0m  [144/169], [94mLoss[0m : 2.15497
[1mStep[0m  [160/169], [94mLoss[0m : 1.84401

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.490, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40125
[1mStep[0m  [16/169], [94mLoss[0m : 2.01441
[1mStep[0m  [32/169], [94mLoss[0m : 2.05487
[1mStep[0m  [48/169], [94mLoss[0m : 2.76548
[1mStep[0m  [64/169], [94mLoss[0m : 2.60883
[1mStep[0m  [80/169], [94mLoss[0m : 2.73949
[1mStep[0m  [96/169], [94mLoss[0m : 2.35535
[1mStep[0m  [112/169], [94mLoss[0m : 2.79929
[1mStep[0m  [128/169], [94mLoss[0m : 2.06429
[1mStep[0m  [144/169], [94mLoss[0m : 2.75067
[1mStep[0m  [160/169], [94mLoss[0m : 2.56207

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.484, [92mTest[0m: 2.324, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.06657
[1mStep[0m  [16/169], [94mLoss[0m : 2.32963
[1mStep[0m  [32/169], [94mLoss[0m : 2.50372
[1mStep[0m  [48/169], [94mLoss[0m : 2.10327
[1mStep[0m  [64/169], [94mLoss[0m : 2.31183
[1mStep[0m  [80/169], [94mLoss[0m : 2.45182
[1mStep[0m  [96/169], [94mLoss[0m : 2.73269
[1mStep[0m  [112/169], [94mLoss[0m : 2.05047
[1mStep[0m  [128/169], [94mLoss[0m : 2.96098
[1mStep[0m  [144/169], [94mLoss[0m : 2.52155
[1mStep[0m  [160/169], [94mLoss[0m : 2.15488

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.486, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.96606
[1mStep[0m  [16/169], [94mLoss[0m : 2.81896
[1mStep[0m  [32/169], [94mLoss[0m : 2.52241
[1mStep[0m  [48/169], [94mLoss[0m : 2.62898
[1mStep[0m  [64/169], [94mLoss[0m : 2.26414
[1mStep[0m  [80/169], [94mLoss[0m : 2.55380
[1mStep[0m  [96/169], [94mLoss[0m : 2.63482
[1mStep[0m  [112/169], [94mLoss[0m : 2.51118
[1mStep[0m  [128/169], [94mLoss[0m : 2.58512
[1mStep[0m  [144/169], [94mLoss[0m : 2.16077
[1mStep[0m  [160/169], [94mLoss[0m : 2.01813

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.505, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70076
[1mStep[0m  [16/169], [94mLoss[0m : 2.64365
[1mStep[0m  [32/169], [94mLoss[0m : 2.64002
[1mStep[0m  [48/169], [94mLoss[0m : 2.02686
[1mStep[0m  [64/169], [94mLoss[0m : 2.84097
[1mStep[0m  [80/169], [94mLoss[0m : 2.44496
[1mStep[0m  [96/169], [94mLoss[0m : 2.17292
[1mStep[0m  [112/169], [94mLoss[0m : 2.38173
[1mStep[0m  [128/169], [94mLoss[0m : 2.14395
[1mStep[0m  [144/169], [94mLoss[0m : 2.04683
[1mStep[0m  [160/169], [94mLoss[0m : 2.88132

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10440
[1mStep[0m  [16/169], [94mLoss[0m : 2.40466
[1mStep[0m  [32/169], [94mLoss[0m : 2.48089
[1mStep[0m  [48/169], [94mLoss[0m : 2.49153
[1mStep[0m  [64/169], [94mLoss[0m : 2.45608
[1mStep[0m  [80/169], [94mLoss[0m : 2.92875
[1mStep[0m  [96/169], [94mLoss[0m : 2.41055
[1mStep[0m  [112/169], [94mLoss[0m : 2.21511
[1mStep[0m  [128/169], [94mLoss[0m : 2.29978
[1mStep[0m  [144/169], [94mLoss[0m : 2.27670
[1mStep[0m  [160/169], [94mLoss[0m : 2.99444

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67922
[1mStep[0m  [16/169], [94mLoss[0m : 2.04760
[1mStep[0m  [32/169], [94mLoss[0m : 2.67612
[1mStep[0m  [48/169], [94mLoss[0m : 2.24509
[1mStep[0m  [64/169], [94mLoss[0m : 2.41375
[1mStep[0m  [80/169], [94mLoss[0m : 2.30478
[1mStep[0m  [96/169], [94mLoss[0m : 2.72049
[1mStep[0m  [112/169], [94mLoss[0m : 2.79122
[1mStep[0m  [128/169], [94mLoss[0m : 2.53034
[1mStep[0m  [144/169], [94mLoss[0m : 2.13026
[1mStep[0m  [160/169], [94mLoss[0m : 2.06649

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73737
[1mStep[0m  [16/169], [94mLoss[0m : 2.38356
[1mStep[0m  [32/169], [94mLoss[0m : 2.29061
[1mStep[0m  [48/169], [94mLoss[0m : 2.18164
[1mStep[0m  [64/169], [94mLoss[0m : 2.25649
[1mStep[0m  [80/169], [94mLoss[0m : 2.67496
[1mStep[0m  [96/169], [94mLoss[0m : 3.22770
[1mStep[0m  [112/169], [94mLoss[0m : 2.72603
[1mStep[0m  [128/169], [94mLoss[0m : 2.62055
[1mStep[0m  [144/169], [94mLoss[0m : 2.49727
[1mStep[0m  [160/169], [94mLoss[0m : 2.69838

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.482, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09541
[1mStep[0m  [16/169], [94mLoss[0m : 2.22827
[1mStep[0m  [32/169], [94mLoss[0m : 2.46329
[1mStep[0m  [48/169], [94mLoss[0m : 2.12871
[1mStep[0m  [64/169], [94mLoss[0m : 2.42959
[1mStep[0m  [80/169], [94mLoss[0m : 2.57607
[1mStep[0m  [96/169], [94mLoss[0m : 2.32965
[1mStep[0m  [112/169], [94mLoss[0m : 2.76371
[1mStep[0m  [128/169], [94mLoss[0m : 2.34953
[1mStep[0m  [144/169], [94mLoss[0m : 2.42349
[1mStep[0m  [160/169], [94mLoss[0m : 2.51043

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.56715
[1mStep[0m  [16/169], [94mLoss[0m : 2.31788
[1mStep[0m  [32/169], [94mLoss[0m : 2.84865
[1mStep[0m  [48/169], [94mLoss[0m : 2.62195
[1mStep[0m  [64/169], [94mLoss[0m : 2.66672
[1mStep[0m  [80/169], [94mLoss[0m : 2.07935
[1mStep[0m  [96/169], [94mLoss[0m : 2.51421
[1mStep[0m  [112/169], [94mLoss[0m : 2.59576
[1mStep[0m  [128/169], [94mLoss[0m : 3.05178
[1mStep[0m  [144/169], [94mLoss[0m : 2.08798
[1mStep[0m  [160/169], [94mLoss[0m : 2.30611

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37836
[1mStep[0m  [16/169], [94mLoss[0m : 2.31626
[1mStep[0m  [32/169], [94mLoss[0m : 2.49372
[1mStep[0m  [48/169], [94mLoss[0m : 2.02894
[1mStep[0m  [64/169], [94mLoss[0m : 2.18793
[1mStep[0m  [80/169], [94mLoss[0m : 2.72002
[1mStep[0m  [96/169], [94mLoss[0m : 1.94639
[1mStep[0m  [112/169], [94mLoss[0m : 2.46063
[1mStep[0m  [128/169], [94mLoss[0m : 2.60994
[1mStep[0m  [144/169], [94mLoss[0m : 3.01329
[1mStep[0m  [160/169], [94mLoss[0m : 2.71403

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71226
[1mStep[0m  [16/169], [94mLoss[0m : 2.51796
[1mStep[0m  [32/169], [94mLoss[0m : 2.57528
[1mStep[0m  [48/169], [94mLoss[0m : 2.47406
[1mStep[0m  [64/169], [94mLoss[0m : 2.58848
[1mStep[0m  [80/169], [94mLoss[0m : 2.19267
[1mStep[0m  [96/169], [94mLoss[0m : 2.58340
[1mStep[0m  [112/169], [94mLoss[0m : 2.30050
[1mStep[0m  [128/169], [94mLoss[0m : 2.99664
[1mStep[0m  [144/169], [94mLoss[0m : 2.36797
[1mStep[0m  [160/169], [94mLoss[0m : 2.22672

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64072
[1mStep[0m  [16/169], [94mLoss[0m : 2.60241
[1mStep[0m  [32/169], [94mLoss[0m : 2.71706
[1mStep[0m  [48/169], [94mLoss[0m : 2.25182
[1mStep[0m  [64/169], [94mLoss[0m : 2.16966
[1mStep[0m  [80/169], [94mLoss[0m : 2.52627
[1mStep[0m  [96/169], [94mLoss[0m : 2.54648
[1mStep[0m  [112/169], [94mLoss[0m : 2.60183
[1mStep[0m  [128/169], [94mLoss[0m : 2.95460
[1mStep[0m  [144/169], [94mLoss[0m : 2.74833
[1mStep[0m  [160/169], [94mLoss[0m : 2.83679

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.494, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14634
[1mStep[0m  [16/169], [94mLoss[0m : 2.23480
[1mStep[0m  [32/169], [94mLoss[0m : 2.36417
[1mStep[0m  [48/169], [94mLoss[0m : 3.11313
[1mStep[0m  [64/169], [94mLoss[0m : 2.30348
[1mStep[0m  [80/169], [94mLoss[0m : 2.28664
[1mStep[0m  [96/169], [94mLoss[0m : 2.30551
[1mStep[0m  [112/169], [94mLoss[0m : 2.38614
[1mStep[0m  [128/169], [94mLoss[0m : 2.63725
[1mStep[0m  [144/169], [94mLoss[0m : 2.77382
[1mStep[0m  [160/169], [94mLoss[0m : 2.42606

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.328, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.87206
[1mStep[0m  [16/169], [94mLoss[0m : 2.41959
[1mStep[0m  [32/169], [94mLoss[0m : 1.95857
[1mStep[0m  [48/169], [94mLoss[0m : 2.43053
[1mStep[0m  [64/169], [94mLoss[0m : 2.33686
[1mStep[0m  [80/169], [94mLoss[0m : 2.38968
[1mStep[0m  [96/169], [94mLoss[0m : 2.83758
[1mStep[0m  [112/169], [94mLoss[0m : 2.71802
[1mStep[0m  [128/169], [94mLoss[0m : 2.31197
[1mStep[0m  [144/169], [94mLoss[0m : 2.40631
[1mStep[0m  [160/169], [94mLoss[0m : 2.05865

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.57216
[1mStep[0m  [16/169], [94mLoss[0m : 2.32084
[1mStep[0m  [32/169], [94mLoss[0m : 2.22331
[1mStep[0m  [48/169], [94mLoss[0m : 2.44543
[1mStep[0m  [64/169], [94mLoss[0m : 2.40320
[1mStep[0m  [80/169], [94mLoss[0m : 2.02515
[1mStep[0m  [96/169], [94mLoss[0m : 2.38591
[1mStep[0m  [112/169], [94mLoss[0m : 2.24977
[1mStep[0m  [128/169], [94mLoss[0m : 2.46979
[1mStep[0m  [144/169], [94mLoss[0m : 2.51260
[1mStep[0m  [160/169], [94mLoss[0m : 2.27183

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25683
[1mStep[0m  [16/169], [94mLoss[0m : 2.10307
[1mStep[0m  [32/169], [94mLoss[0m : 2.73096
[1mStep[0m  [48/169], [94mLoss[0m : 2.23337
[1mStep[0m  [64/169], [94mLoss[0m : 2.60094
[1mStep[0m  [80/169], [94mLoss[0m : 2.06687
[1mStep[0m  [96/169], [94mLoss[0m : 2.22380
[1mStep[0m  [112/169], [94mLoss[0m : 2.98153
[1mStep[0m  [128/169], [94mLoss[0m : 2.40727
[1mStep[0m  [144/169], [94mLoss[0m : 2.89998
[1mStep[0m  [160/169], [94mLoss[0m : 2.77360

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.03831
[1mStep[0m  [16/169], [94mLoss[0m : 2.07323
[1mStep[0m  [32/169], [94mLoss[0m : 2.46224
[1mStep[0m  [48/169], [94mLoss[0m : 2.62038
[1mStep[0m  [64/169], [94mLoss[0m : 2.14897
[1mStep[0m  [80/169], [94mLoss[0m : 2.30671
[1mStep[0m  [96/169], [94mLoss[0m : 1.94093
[1mStep[0m  [112/169], [94mLoss[0m : 2.62799
[1mStep[0m  [128/169], [94mLoss[0m : 2.42168
[1mStep[0m  [144/169], [94mLoss[0m : 2.39328
[1mStep[0m  [160/169], [94mLoss[0m : 2.32618

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.480, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13427
[1mStep[0m  [16/169], [94mLoss[0m : 2.17488
[1mStep[0m  [32/169], [94mLoss[0m : 2.37831
[1mStep[0m  [48/169], [94mLoss[0m : 1.98816
[1mStep[0m  [64/169], [94mLoss[0m : 2.30467
[1mStep[0m  [80/169], [94mLoss[0m : 2.43889
[1mStep[0m  [96/169], [94mLoss[0m : 2.25175
[1mStep[0m  [112/169], [94mLoss[0m : 2.32698
[1mStep[0m  [128/169], [94mLoss[0m : 2.28245
[1mStep[0m  [144/169], [94mLoss[0m : 2.40662
[1mStep[0m  [160/169], [94mLoss[0m : 2.65796

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.315, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75539
[1mStep[0m  [16/169], [94mLoss[0m : 2.22980
[1mStep[0m  [32/169], [94mLoss[0m : 2.60827
[1mStep[0m  [48/169], [94mLoss[0m : 2.52713
[1mStep[0m  [64/169], [94mLoss[0m : 2.68149
[1mStep[0m  [80/169], [94mLoss[0m : 2.46925
[1mStep[0m  [96/169], [94mLoss[0m : 2.52839
[1mStep[0m  [112/169], [94mLoss[0m : 2.18300
[1mStep[0m  [128/169], [94mLoss[0m : 2.57845
[1mStep[0m  [144/169], [94mLoss[0m : 3.10114
[1mStep[0m  [160/169], [94mLoss[0m : 2.58253

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.316, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.66360
[1mStep[0m  [16/169], [94mLoss[0m : 2.33903
[1mStep[0m  [32/169], [94mLoss[0m : 2.64793
[1mStep[0m  [48/169], [94mLoss[0m : 2.93141
[1mStep[0m  [64/169], [94mLoss[0m : 2.59028
[1mStep[0m  [80/169], [94mLoss[0m : 2.33334
[1mStep[0m  [96/169], [94mLoss[0m : 2.39917
[1mStep[0m  [112/169], [94mLoss[0m : 2.27823
[1mStep[0m  [128/169], [94mLoss[0m : 2.36434
[1mStep[0m  [144/169], [94mLoss[0m : 2.04533
[1mStep[0m  [160/169], [94mLoss[0m : 2.94990

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.451, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.77907
[1mStep[0m  [16/169], [94mLoss[0m : 2.92699
[1mStep[0m  [32/169], [94mLoss[0m : 2.19713
[1mStep[0m  [48/169], [94mLoss[0m : 2.22557
[1mStep[0m  [64/169], [94mLoss[0m : 2.59587
[1mStep[0m  [80/169], [94mLoss[0m : 2.55579
[1mStep[0m  [96/169], [94mLoss[0m : 2.06808
[1mStep[0m  [112/169], [94mLoss[0m : 2.78660
[1mStep[0m  [128/169], [94mLoss[0m : 2.14586
[1mStep[0m  [144/169], [94mLoss[0m : 2.53488
[1mStep[0m  [160/169], [94mLoss[0m : 2.45169

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08631
[1mStep[0m  [16/169], [94mLoss[0m : 2.40177
[1mStep[0m  [32/169], [94mLoss[0m : 2.10872
[1mStep[0m  [48/169], [94mLoss[0m : 2.57137
[1mStep[0m  [64/169], [94mLoss[0m : 2.24705
[1mStep[0m  [80/169], [94mLoss[0m : 2.49696
[1mStep[0m  [96/169], [94mLoss[0m : 2.44363
[1mStep[0m  [112/169], [94mLoss[0m : 2.56236
[1mStep[0m  [128/169], [94mLoss[0m : 2.42654
[1mStep[0m  [144/169], [94mLoss[0m : 2.60990
[1mStep[0m  [160/169], [94mLoss[0m : 2.67511

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.319, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.326
====================================

Phase 1 - Evaluation MAE:  2.325688964554242
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=100, bias=True)
        (in_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=100, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.40787
[1mStep[0m  [16/169], [94mLoss[0m : 2.32365
[1mStep[0m  [32/169], [94mLoss[0m : 2.57182
[1mStep[0m  [48/169], [94mLoss[0m : 2.60011
[1mStep[0m  [64/169], [94mLoss[0m : 2.53293
[1mStep[0m  [80/169], [94mLoss[0m : 2.32375
[1mStep[0m  [96/169], [94mLoss[0m : 2.23336
[1mStep[0m  [112/169], [94mLoss[0m : 2.26152
[1mStep[0m  [128/169], [94mLoss[0m : 2.48557
[1mStep[0m  [144/169], [94mLoss[0m : 2.23846
[1mStep[0m  [160/169], [94mLoss[0m : 2.44479

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82485
[1mStep[0m  [16/169], [94mLoss[0m : 2.56339
[1mStep[0m  [32/169], [94mLoss[0m : 2.10778
[1mStep[0m  [48/169], [94mLoss[0m : 2.40137
[1mStep[0m  [64/169], [94mLoss[0m : 2.38050
[1mStep[0m  [80/169], [94mLoss[0m : 2.41888
[1mStep[0m  [96/169], [94mLoss[0m : 2.48782
[1mStep[0m  [112/169], [94mLoss[0m : 2.32264
[1mStep[0m  [128/169], [94mLoss[0m : 2.24598
[1mStep[0m  [144/169], [94mLoss[0m : 2.69413
[1mStep[0m  [160/169], [94mLoss[0m : 2.28108

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46719
[1mStep[0m  [16/169], [94mLoss[0m : 2.38059
[1mStep[0m  [32/169], [94mLoss[0m : 2.26704
[1mStep[0m  [48/169], [94mLoss[0m : 2.63477
[1mStep[0m  [64/169], [94mLoss[0m : 2.41984
[1mStep[0m  [80/169], [94mLoss[0m : 2.47613
[1mStep[0m  [96/169], [94mLoss[0m : 2.22099
[1mStep[0m  [112/169], [94mLoss[0m : 2.30670
[1mStep[0m  [128/169], [94mLoss[0m : 2.91354
[1mStep[0m  [144/169], [94mLoss[0m : 1.89284
[1mStep[0m  [160/169], [94mLoss[0m : 2.54730

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.350, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21761
[1mStep[0m  [16/169], [94mLoss[0m : 1.95187
[1mStep[0m  [32/169], [94mLoss[0m : 2.15430
[1mStep[0m  [48/169], [94mLoss[0m : 2.31052
[1mStep[0m  [64/169], [94mLoss[0m : 2.37827
[1mStep[0m  [80/169], [94mLoss[0m : 2.10451
[1mStep[0m  [96/169], [94mLoss[0m : 2.25322
[1mStep[0m  [112/169], [94mLoss[0m : 1.92804
[1mStep[0m  [128/169], [94mLoss[0m : 2.46314
[1mStep[0m  [144/169], [94mLoss[0m : 1.98726
[1mStep[0m  [160/169], [94mLoss[0m : 2.32404

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68303
[1mStep[0m  [16/169], [94mLoss[0m : 2.50524
[1mStep[0m  [32/169], [94mLoss[0m : 2.08268
[1mStep[0m  [48/169], [94mLoss[0m : 2.44504
[1mStep[0m  [64/169], [94mLoss[0m : 1.93645
[1mStep[0m  [80/169], [94mLoss[0m : 2.24185
[1mStep[0m  [96/169], [94mLoss[0m : 2.01176
[1mStep[0m  [112/169], [94mLoss[0m : 2.76254
[1mStep[0m  [128/169], [94mLoss[0m : 2.41962
[1mStep[0m  [144/169], [94mLoss[0m : 2.57018
[1mStep[0m  [160/169], [94mLoss[0m : 2.12297

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.256, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36787
[1mStep[0m  [16/169], [94mLoss[0m : 2.18869
[1mStep[0m  [32/169], [94mLoss[0m : 1.94914
[1mStep[0m  [48/169], [94mLoss[0m : 2.25566
[1mStep[0m  [64/169], [94mLoss[0m : 2.68049
[1mStep[0m  [80/169], [94mLoss[0m : 2.54168
[1mStep[0m  [96/169], [94mLoss[0m : 2.61640
[1mStep[0m  [112/169], [94mLoss[0m : 2.16875
[1mStep[0m  [128/169], [94mLoss[0m : 2.03754
[1mStep[0m  [144/169], [94mLoss[0m : 2.12702
[1mStep[0m  [160/169], [94mLoss[0m : 2.20691

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.190, [92mTest[0m: 2.393, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.35527
[1mStep[0m  [16/169], [94mLoss[0m : 2.33993
[1mStep[0m  [32/169], [94mLoss[0m : 2.14067
[1mStep[0m  [48/169], [94mLoss[0m : 2.23021
[1mStep[0m  [64/169], [94mLoss[0m : 1.85833
[1mStep[0m  [80/169], [94mLoss[0m : 2.38621
[1mStep[0m  [96/169], [94mLoss[0m : 2.17838
[1mStep[0m  [112/169], [94mLoss[0m : 1.80779
[1mStep[0m  [128/169], [94mLoss[0m : 2.01104
[1mStep[0m  [144/169], [94mLoss[0m : 2.40103
[1mStep[0m  [160/169], [94mLoss[0m : 2.11473

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.155, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.09134
[1mStep[0m  [16/169], [94mLoss[0m : 1.98970
[1mStep[0m  [32/169], [94mLoss[0m : 2.24534
[1mStep[0m  [48/169], [94mLoss[0m : 2.04052
[1mStep[0m  [64/169], [94mLoss[0m : 1.84211
[1mStep[0m  [80/169], [94mLoss[0m : 2.03839
[1mStep[0m  [96/169], [94mLoss[0m : 2.31401
[1mStep[0m  [112/169], [94mLoss[0m : 2.17121
[1mStep[0m  [128/169], [94mLoss[0m : 2.05805
[1mStep[0m  [144/169], [94mLoss[0m : 2.02853
[1mStep[0m  [160/169], [94mLoss[0m : 2.16181

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.063, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98438
[1mStep[0m  [16/169], [94mLoss[0m : 2.04990
[1mStep[0m  [32/169], [94mLoss[0m : 1.90680
[1mStep[0m  [48/169], [94mLoss[0m : 2.11790
[1mStep[0m  [64/169], [94mLoss[0m : 1.93589
[1mStep[0m  [80/169], [94mLoss[0m : 2.32581
[1mStep[0m  [96/169], [94mLoss[0m : 2.24489
[1mStep[0m  [112/169], [94mLoss[0m : 2.07107
[1mStep[0m  [128/169], [94mLoss[0m : 1.85598
[1mStep[0m  [144/169], [94mLoss[0m : 2.39791
[1mStep[0m  [160/169], [94mLoss[0m : 2.16300

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.034, [92mTest[0m: 2.414, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31959
[1mStep[0m  [16/169], [94mLoss[0m : 1.95302
[1mStep[0m  [32/169], [94mLoss[0m : 1.88912
[1mStep[0m  [48/169], [94mLoss[0m : 2.01328
[1mStep[0m  [64/169], [94mLoss[0m : 1.93340
[1mStep[0m  [80/169], [94mLoss[0m : 2.00563
[1mStep[0m  [96/169], [94mLoss[0m : 1.86139
[1mStep[0m  [112/169], [94mLoss[0m : 1.88023
[1mStep[0m  [128/169], [94mLoss[0m : 2.16291
[1mStep[0m  [144/169], [94mLoss[0m : 2.15244
[1mStep[0m  [160/169], [94mLoss[0m : 2.21792

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.015, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.72719
[1mStep[0m  [16/169], [94mLoss[0m : 2.26435
[1mStep[0m  [32/169], [94mLoss[0m : 1.70788
[1mStep[0m  [48/169], [94mLoss[0m : 1.75467
[1mStep[0m  [64/169], [94mLoss[0m : 2.59197
[1mStep[0m  [80/169], [94mLoss[0m : 2.02995
[1mStep[0m  [96/169], [94mLoss[0m : 2.30058
[1mStep[0m  [112/169], [94mLoss[0m : 1.77918
[1mStep[0m  [128/169], [94mLoss[0m : 2.31324
[1mStep[0m  [144/169], [94mLoss[0m : 2.14799
[1mStep[0m  [160/169], [94mLoss[0m : 1.96065

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.944, [92mTest[0m: 2.468, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.31601
[1mStep[0m  [16/169], [94mLoss[0m : 1.74679
[1mStep[0m  [32/169], [94mLoss[0m : 1.58130
[1mStep[0m  [48/169], [94mLoss[0m : 1.92031
[1mStep[0m  [64/169], [94mLoss[0m : 2.04959
[1mStep[0m  [80/169], [94mLoss[0m : 1.42829
[1mStep[0m  [96/169], [94mLoss[0m : 1.86011
[1mStep[0m  [112/169], [94mLoss[0m : 2.09947
[1mStep[0m  [128/169], [94mLoss[0m : 2.23850
[1mStep[0m  [144/169], [94mLoss[0m : 2.16101
[1mStep[0m  [160/169], [94mLoss[0m : 2.11915

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.921, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42851
[1mStep[0m  [16/169], [94mLoss[0m : 1.91170
[1mStep[0m  [32/169], [94mLoss[0m : 1.57989
[1mStep[0m  [48/169], [94mLoss[0m : 2.20293
[1mStep[0m  [64/169], [94mLoss[0m : 1.74188
[1mStep[0m  [80/169], [94mLoss[0m : 1.61238
[1mStep[0m  [96/169], [94mLoss[0m : 2.02499
[1mStep[0m  [112/169], [94mLoss[0m : 2.30980
[1mStep[0m  [128/169], [94mLoss[0m : 1.81825
[1mStep[0m  [144/169], [94mLoss[0m : 1.95962
[1mStep[0m  [160/169], [94mLoss[0m : 2.26313

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.493, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67407
[1mStep[0m  [16/169], [94mLoss[0m : 1.95669
[1mStep[0m  [32/169], [94mLoss[0m : 2.10177
[1mStep[0m  [48/169], [94mLoss[0m : 1.89173
[1mStep[0m  [64/169], [94mLoss[0m : 2.13145
[1mStep[0m  [80/169], [94mLoss[0m : 1.63560
[1mStep[0m  [96/169], [94mLoss[0m : 1.69615
[1mStep[0m  [112/169], [94mLoss[0m : 1.72851
[1mStep[0m  [128/169], [94mLoss[0m : 2.04942
[1mStep[0m  [144/169], [94mLoss[0m : 1.59566
[1mStep[0m  [160/169], [94mLoss[0m : 2.03959

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84479
[1mStep[0m  [16/169], [94mLoss[0m : 1.73082
[1mStep[0m  [32/169], [94mLoss[0m : 1.64659
[1mStep[0m  [48/169], [94mLoss[0m : 1.75591
[1mStep[0m  [64/169], [94mLoss[0m : 1.79746
[1mStep[0m  [80/169], [94mLoss[0m : 2.12686
[1mStep[0m  [96/169], [94mLoss[0m : 1.63858
[1mStep[0m  [112/169], [94mLoss[0m : 1.77328
[1mStep[0m  [128/169], [94mLoss[0m : 1.76466
[1mStep[0m  [144/169], [94mLoss[0m : 1.56151
[1mStep[0m  [160/169], [94mLoss[0m : 1.79617

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.813, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68619
[1mStep[0m  [16/169], [94mLoss[0m : 1.51835
[1mStep[0m  [32/169], [94mLoss[0m : 1.84668
[1mStep[0m  [48/169], [94mLoss[0m : 1.80609
[1mStep[0m  [64/169], [94mLoss[0m : 1.69400
[1mStep[0m  [80/169], [94mLoss[0m : 1.67428
[1mStep[0m  [96/169], [94mLoss[0m : 1.55320
[1mStep[0m  [112/169], [94mLoss[0m : 2.14102
[1mStep[0m  [128/169], [94mLoss[0m : 1.70668
[1mStep[0m  [144/169], [94mLoss[0m : 1.68337
[1mStep[0m  [160/169], [94mLoss[0m : 1.82539

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.773, [92mTest[0m: 2.478, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.70133
[1mStep[0m  [16/169], [94mLoss[0m : 1.59782
[1mStep[0m  [32/169], [94mLoss[0m : 1.68149
[1mStep[0m  [48/169], [94mLoss[0m : 1.91080
[1mStep[0m  [64/169], [94mLoss[0m : 1.70539
[1mStep[0m  [80/169], [94mLoss[0m : 1.71676
[1mStep[0m  [96/169], [94mLoss[0m : 2.02993
[1mStep[0m  [112/169], [94mLoss[0m : 1.61806
[1mStep[0m  [128/169], [94mLoss[0m : 1.87186
[1mStep[0m  [144/169], [94mLoss[0m : 1.58807
[1mStep[0m  [160/169], [94mLoss[0m : 1.66906

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.757, [92mTest[0m: 2.526, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51678
[1mStep[0m  [16/169], [94mLoss[0m : 1.78851
[1mStep[0m  [32/169], [94mLoss[0m : 1.58662
[1mStep[0m  [48/169], [94mLoss[0m : 1.64521
[1mStep[0m  [64/169], [94mLoss[0m : 2.03986
[1mStep[0m  [80/169], [94mLoss[0m : 1.71889
[1mStep[0m  [96/169], [94mLoss[0m : 1.98897
[1mStep[0m  [112/169], [94mLoss[0m : 1.84560
[1mStep[0m  [128/169], [94mLoss[0m : 1.79656
[1mStep[0m  [144/169], [94mLoss[0m : 1.59237
[1mStep[0m  [160/169], [94mLoss[0m : 1.82436

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.710, [92mTest[0m: 2.500, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28645
[1mStep[0m  [16/169], [94mLoss[0m : 1.51885
[1mStep[0m  [32/169], [94mLoss[0m : 1.60796
[1mStep[0m  [48/169], [94mLoss[0m : 1.54826
[1mStep[0m  [64/169], [94mLoss[0m : 1.83113
[1mStep[0m  [80/169], [94mLoss[0m : 1.68029
[1mStep[0m  [96/169], [94mLoss[0m : 1.96558
[1mStep[0m  [112/169], [94mLoss[0m : 1.94807
[1mStep[0m  [128/169], [94mLoss[0m : 2.07589
[1mStep[0m  [144/169], [94mLoss[0m : 1.54340
[1mStep[0m  [160/169], [94mLoss[0m : 1.63927

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.686, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69942
[1mStep[0m  [16/169], [94mLoss[0m : 1.70045
[1mStep[0m  [32/169], [94mLoss[0m : 1.84976
[1mStep[0m  [48/169], [94mLoss[0m : 1.58683
[1mStep[0m  [64/169], [94mLoss[0m : 1.91889
[1mStep[0m  [80/169], [94mLoss[0m : 1.69091
[1mStep[0m  [96/169], [94mLoss[0m : 1.69897
[1mStep[0m  [112/169], [94mLoss[0m : 1.69536
[1mStep[0m  [128/169], [94mLoss[0m : 1.98259
[1mStep[0m  [144/169], [94mLoss[0m : 1.91210
[1mStep[0m  [160/169], [94mLoss[0m : 1.67482

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.665, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.80576
[1mStep[0m  [16/169], [94mLoss[0m : 1.41518
[1mStep[0m  [32/169], [94mLoss[0m : 1.75082
[1mStep[0m  [48/169], [94mLoss[0m : 1.61676
[1mStep[0m  [64/169], [94mLoss[0m : 1.94684
[1mStep[0m  [80/169], [94mLoss[0m : 1.43509
[1mStep[0m  [96/169], [94mLoss[0m : 1.50350
[1mStep[0m  [112/169], [94mLoss[0m : 1.87582
[1mStep[0m  [128/169], [94mLoss[0m : 1.52371
[1mStep[0m  [144/169], [94mLoss[0m : 1.38368
[1mStep[0m  [160/169], [94mLoss[0m : 1.69935

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.626, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.48840
[1mStep[0m  [16/169], [94mLoss[0m : 1.41178
[1mStep[0m  [32/169], [94mLoss[0m : 1.62556
[1mStep[0m  [48/169], [94mLoss[0m : 1.37330
[1mStep[0m  [64/169], [94mLoss[0m : 1.62392
[1mStep[0m  [80/169], [94mLoss[0m : 1.52276
[1mStep[0m  [96/169], [94mLoss[0m : 1.93149
[1mStep[0m  [112/169], [94mLoss[0m : 1.52039
[1mStep[0m  [128/169], [94mLoss[0m : 1.58122
[1mStep[0m  [144/169], [94mLoss[0m : 1.35555
[1mStep[0m  [160/169], [94mLoss[0m : 1.40804

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.575, [92mTest[0m: 2.456, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51673
[1mStep[0m  [16/169], [94mLoss[0m : 1.54006
[1mStep[0m  [32/169], [94mLoss[0m : 1.54482
[1mStep[0m  [48/169], [94mLoss[0m : 1.42799
[1mStep[0m  [64/169], [94mLoss[0m : 1.63928
[1mStep[0m  [80/169], [94mLoss[0m : 1.34239
[1mStep[0m  [96/169], [94mLoss[0m : 1.44515
[1mStep[0m  [112/169], [94mLoss[0m : 1.92217
[1mStep[0m  [128/169], [94mLoss[0m : 1.71862
[1mStep[0m  [144/169], [94mLoss[0m : 1.58302
[1mStep[0m  [160/169], [94mLoss[0m : 1.40563

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49035
[1mStep[0m  [16/169], [94mLoss[0m : 1.20488
[1mStep[0m  [32/169], [94mLoss[0m : 1.65655
[1mStep[0m  [48/169], [94mLoss[0m : 1.64991
[1mStep[0m  [64/169], [94mLoss[0m : 1.74553
[1mStep[0m  [80/169], [94mLoss[0m : 1.76250
[1mStep[0m  [96/169], [94mLoss[0m : 1.60820
[1mStep[0m  [112/169], [94mLoss[0m : 1.50154
[1mStep[0m  [128/169], [94mLoss[0m : 1.53357
[1mStep[0m  [144/169], [94mLoss[0m : 1.41410
[1mStep[0m  [160/169], [94mLoss[0m : 1.67834

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.553, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.61716
[1mStep[0m  [16/169], [94mLoss[0m : 1.59591
[1mStep[0m  [32/169], [94mLoss[0m : 1.45755
[1mStep[0m  [48/169], [94mLoss[0m : 1.08167
[1mStep[0m  [64/169], [94mLoss[0m : 1.20524
[1mStep[0m  [80/169], [94mLoss[0m : 1.38884
[1mStep[0m  [96/169], [94mLoss[0m : 1.46414
[1mStep[0m  [112/169], [94mLoss[0m : 1.44475
[1mStep[0m  [128/169], [94mLoss[0m : 1.41974
[1mStep[0m  [144/169], [94mLoss[0m : 1.36964
[1mStep[0m  [160/169], [94mLoss[0m : 1.57388

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.554, [92mTest[0m: 2.474, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34577
[1mStep[0m  [16/169], [94mLoss[0m : 1.41821
[1mStep[0m  [32/169], [94mLoss[0m : 1.36201
[1mStep[0m  [48/169], [94mLoss[0m : 1.54548
[1mStep[0m  [64/169], [94mLoss[0m : 1.65921
[1mStep[0m  [80/169], [94mLoss[0m : 1.45176
[1mStep[0m  [96/169], [94mLoss[0m : 1.44830
[1mStep[0m  [112/169], [94mLoss[0m : 1.47173
[1mStep[0m  [128/169], [94mLoss[0m : 1.33934
[1mStep[0m  [144/169], [94mLoss[0m : 1.33560
[1mStep[0m  [160/169], [94mLoss[0m : 1.49469

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.521, [92mTest[0m: 2.514, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.62946
[1mStep[0m  [16/169], [94mLoss[0m : 1.61640
[1mStep[0m  [32/169], [94mLoss[0m : 1.61280
[1mStep[0m  [48/169], [94mLoss[0m : 1.45514
[1mStep[0m  [64/169], [94mLoss[0m : 1.55894
[1mStep[0m  [80/169], [94mLoss[0m : 1.76309
[1mStep[0m  [96/169], [94mLoss[0m : 1.25064
[1mStep[0m  [112/169], [94mLoss[0m : 1.47164
[1mStep[0m  [128/169], [94mLoss[0m : 1.60402
[1mStep[0m  [144/169], [94mLoss[0m : 1.73192
[1mStep[0m  [160/169], [94mLoss[0m : 1.49434

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.507, [92mTest[0m: 2.471, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.30869
[1mStep[0m  [16/169], [94mLoss[0m : 1.59860
[1mStep[0m  [32/169], [94mLoss[0m : 1.41267
[1mStep[0m  [48/169], [94mLoss[0m : 1.35239
[1mStep[0m  [64/169], [94mLoss[0m : 1.30665
[1mStep[0m  [80/169], [94mLoss[0m : 1.32290
[1mStep[0m  [96/169], [94mLoss[0m : 1.26768
[1mStep[0m  [112/169], [94mLoss[0m : 1.50909
[1mStep[0m  [128/169], [94mLoss[0m : 1.43284
[1mStep[0m  [144/169], [94mLoss[0m : 1.72980
[1mStep[0m  [160/169], [94mLoss[0m : 1.80610

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41008
[1mStep[0m  [16/169], [94mLoss[0m : 1.71484
[1mStep[0m  [32/169], [94mLoss[0m : 1.58431
[1mStep[0m  [48/169], [94mLoss[0m : 1.36508
[1mStep[0m  [64/169], [94mLoss[0m : 1.68832
[1mStep[0m  [80/169], [94mLoss[0m : 1.45963
[1mStep[0m  [96/169], [94mLoss[0m : 1.28511
[1mStep[0m  [112/169], [94mLoss[0m : 1.49432
[1mStep[0m  [128/169], [94mLoss[0m : 1.45280
[1mStep[0m  [144/169], [94mLoss[0m : 1.60566
[1mStep[0m  [160/169], [94mLoss[0m : 1.62466

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.459, [92mTest[0m: 2.492, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.47113
[1mStep[0m  [16/169], [94mLoss[0m : 1.32531
[1mStep[0m  [32/169], [94mLoss[0m : 1.52550
[1mStep[0m  [48/169], [94mLoss[0m : 1.47877
[1mStep[0m  [64/169], [94mLoss[0m : 1.25642
[1mStep[0m  [80/169], [94mLoss[0m : 1.21390
[1mStep[0m  [96/169], [94mLoss[0m : 1.59739
[1mStep[0m  [112/169], [94mLoss[0m : 1.69931
[1mStep[0m  [128/169], [94mLoss[0m : 1.35768
[1mStep[0m  [144/169], [94mLoss[0m : 1.51259
[1mStep[0m  [160/169], [94mLoss[0m : 1.43241

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.460, [92mTest[0m: 2.604, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.548
====================================

Phase 2 - Evaluation MAE:  2.548230992896216
MAE score P1      2.325689
MAE score P2      2.548231
loss              1.459426
learning_rate     0.007525
batch_size              64
hidden_sizes         [100]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping       False
dropout                0.4
momentum               0.1
weight_decay         0.001
Name: 8, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.01807
[1mStep[0m  [16/169], [94mLoss[0m : 3.82131
[1mStep[0m  [32/169], [94mLoss[0m : 2.95728
[1mStep[0m  [48/169], [94mLoss[0m : 2.91321
[1mStep[0m  [64/169], [94mLoss[0m : 2.70839
[1mStep[0m  [80/169], [94mLoss[0m : 3.08359
[1mStep[0m  [96/169], [94mLoss[0m : 2.44209
[1mStep[0m  [112/169], [94mLoss[0m : 2.19603
[1mStep[0m  [128/169], [94mLoss[0m : 2.68323
[1mStep[0m  [144/169], [94mLoss[0m : 2.93421
[1mStep[0m  [160/169], [94mLoss[0m : 2.48277

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.263, [92mTest[0m: 11.088, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48717
[1mStep[0m  [16/169], [94mLoss[0m : 2.67874
[1mStep[0m  [32/169], [94mLoss[0m : 2.24075
[1mStep[0m  [48/169], [94mLoss[0m : 2.30569
[1mStep[0m  [64/169], [94mLoss[0m : 2.35612
[1mStep[0m  [80/169], [94mLoss[0m : 2.79891
[1mStep[0m  [96/169], [94mLoss[0m : 2.44609
[1mStep[0m  [112/169], [94mLoss[0m : 3.16888
[1mStep[0m  [128/169], [94mLoss[0m : 2.38277
[1mStep[0m  [144/169], [94mLoss[0m : 2.62535
[1mStep[0m  [160/169], [94mLoss[0m : 2.31842

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.630, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54210
[1mStep[0m  [16/169], [94mLoss[0m : 2.38550
[1mStep[0m  [32/169], [94mLoss[0m : 3.19271
[1mStep[0m  [48/169], [94mLoss[0m : 2.54243
[1mStep[0m  [64/169], [94mLoss[0m : 2.03084
[1mStep[0m  [80/169], [94mLoss[0m : 2.73957
[1mStep[0m  [96/169], [94mLoss[0m : 2.47268
[1mStep[0m  [112/169], [94mLoss[0m : 2.49062
[1mStep[0m  [128/169], [94mLoss[0m : 2.90002
[1mStep[0m  [144/169], [94mLoss[0m : 2.75358
[1mStep[0m  [160/169], [94mLoss[0m : 2.55750

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.371, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53026
[1mStep[0m  [16/169], [94mLoss[0m : 2.41807
[1mStep[0m  [32/169], [94mLoss[0m : 3.12958
[1mStep[0m  [48/169], [94mLoss[0m : 2.71343
[1mStep[0m  [64/169], [94mLoss[0m : 2.47791
[1mStep[0m  [80/169], [94mLoss[0m : 2.57103
[1mStep[0m  [96/169], [94mLoss[0m : 2.91682
[1mStep[0m  [112/169], [94mLoss[0m : 2.45133
[1mStep[0m  [128/169], [94mLoss[0m : 2.79603
[1mStep[0m  [144/169], [94mLoss[0m : 2.48886
[1mStep[0m  [160/169], [94mLoss[0m : 2.31445

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.605, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33532
[1mStep[0m  [16/169], [94mLoss[0m : 3.12100
[1mStep[0m  [32/169], [94mLoss[0m : 2.41566
[1mStep[0m  [48/169], [94mLoss[0m : 2.81578
[1mStep[0m  [64/169], [94mLoss[0m : 2.60083
[1mStep[0m  [80/169], [94mLoss[0m : 2.82628
[1mStep[0m  [96/169], [94mLoss[0m : 2.99968
[1mStep[0m  [112/169], [94mLoss[0m : 2.45358
[1mStep[0m  [128/169], [94mLoss[0m : 2.77647
[1mStep[0m  [144/169], [94mLoss[0m : 2.09240
[1mStep[0m  [160/169], [94mLoss[0m : 2.66219

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62744
[1mStep[0m  [16/169], [94mLoss[0m : 2.35014
[1mStep[0m  [32/169], [94mLoss[0m : 2.56497
[1mStep[0m  [48/169], [94mLoss[0m : 2.53606
[1mStep[0m  [64/169], [94mLoss[0m : 2.68849
[1mStep[0m  [80/169], [94mLoss[0m : 2.19567
[1mStep[0m  [96/169], [94mLoss[0m : 2.19293
[1mStep[0m  [112/169], [94mLoss[0m : 2.67177
[1mStep[0m  [128/169], [94mLoss[0m : 2.54278
[1mStep[0m  [144/169], [94mLoss[0m : 2.11964
[1mStep[0m  [160/169], [94mLoss[0m : 2.81777

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61810
[1mStep[0m  [16/169], [94mLoss[0m : 2.64922
[1mStep[0m  [32/169], [94mLoss[0m : 2.25362
[1mStep[0m  [48/169], [94mLoss[0m : 2.67454
[1mStep[0m  [64/169], [94mLoss[0m : 2.32765
[1mStep[0m  [80/169], [94mLoss[0m : 2.14764
[1mStep[0m  [96/169], [94mLoss[0m : 2.64939
[1mStep[0m  [112/169], [94mLoss[0m : 2.82453
[1mStep[0m  [128/169], [94mLoss[0m : 2.69709
[1mStep[0m  [144/169], [94mLoss[0m : 2.65758
[1mStep[0m  [160/169], [94mLoss[0m : 2.87952

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98331
[1mStep[0m  [16/169], [94mLoss[0m : 2.46075
[1mStep[0m  [32/169], [94mLoss[0m : 2.47608
[1mStep[0m  [48/169], [94mLoss[0m : 2.65699
[1mStep[0m  [64/169], [94mLoss[0m : 2.61549
[1mStep[0m  [80/169], [94mLoss[0m : 2.52676
[1mStep[0m  [96/169], [94mLoss[0m : 2.98247
[1mStep[0m  [112/169], [94mLoss[0m : 3.03620
[1mStep[0m  [128/169], [94mLoss[0m : 2.44004
[1mStep[0m  [144/169], [94mLoss[0m : 2.62250
[1mStep[0m  [160/169], [94mLoss[0m : 2.78280

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41375
[1mStep[0m  [16/169], [94mLoss[0m : 2.83617
[1mStep[0m  [32/169], [94mLoss[0m : 2.23702
[1mStep[0m  [48/169], [94mLoss[0m : 2.45210
[1mStep[0m  [64/169], [94mLoss[0m : 2.54103
[1mStep[0m  [80/169], [94mLoss[0m : 2.31715
[1mStep[0m  [96/169], [94mLoss[0m : 2.34584
[1mStep[0m  [112/169], [94mLoss[0m : 2.56883
[1mStep[0m  [128/169], [94mLoss[0m : 2.47106
[1mStep[0m  [144/169], [94mLoss[0m : 2.34708
[1mStep[0m  [160/169], [94mLoss[0m : 2.27785

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.503, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.58259
[1mStep[0m  [16/169], [94mLoss[0m : 2.49246
[1mStep[0m  [32/169], [94mLoss[0m : 2.32942
[1mStep[0m  [48/169], [94mLoss[0m : 2.49927
[1mStep[0m  [64/169], [94mLoss[0m : 2.07411
[1mStep[0m  [80/169], [94mLoss[0m : 2.44527
[1mStep[0m  [96/169], [94mLoss[0m : 2.60306
[1mStep[0m  [112/169], [94mLoss[0m : 2.43849
[1mStep[0m  [128/169], [94mLoss[0m : 2.63942
[1mStep[0m  [144/169], [94mLoss[0m : 2.34262
[1mStep[0m  [160/169], [94mLoss[0m : 2.73714

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.507, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34148
[1mStep[0m  [16/169], [94mLoss[0m : 2.77247
[1mStep[0m  [32/169], [94mLoss[0m : 2.58142
[1mStep[0m  [48/169], [94mLoss[0m : 2.57201
[1mStep[0m  [64/169], [94mLoss[0m : 2.44173
[1mStep[0m  [80/169], [94mLoss[0m : 2.68512
[1mStep[0m  [96/169], [94mLoss[0m : 2.54460
[1mStep[0m  [112/169], [94mLoss[0m : 2.85744
[1mStep[0m  [128/169], [94mLoss[0m : 2.32319
[1mStep[0m  [144/169], [94mLoss[0m : 2.39734
[1mStep[0m  [160/169], [94mLoss[0m : 2.72855

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.498, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43225
[1mStep[0m  [16/169], [94mLoss[0m : 2.34078
[1mStep[0m  [32/169], [94mLoss[0m : 1.98179
[1mStep[0m  [48/169], [94mLoss[0m : 2.76568
[1mStep[0m  [64/169], [94mLoss[0m : 2.59800
[1mStep[0m  [80/169], [94mLoss[0m : 2.52189
[1mStep[0m  [96/169], [94mLoss[0m : 2.60879
[1mStep[0m  [112/169], [94mLoss[0m : 2.64165
[1mStep[0m  [128/169], [94mLoss[0m : 2.49503
[1mStep[0m  [144/169], [94mLoss[0m : 2.69595
[1mStep[0m  [160/169], [94mLoss[0m : 2.94247

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.499, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39799
[1mStep[0m  [16/169], [94mLoss[0m : 2.11247
[1mStep[0m  [32/169], [94mLoss[0m : 2.36832
[1mStep[0m  [48/169], [94mLoss[0m : 3.09529
[1mStep[0m  [64/169], [94mLoss[0m : 2.60547
[1mStep[0m  [80/169], [94mLoss[0m : 2.46620
[1mStep[0m  [96/169], [94mLoss[0m : 2.93674
[1mStep[0m  [112/169], [94mLoss[0m : 2.58442
[1mStep[0m  [128/169], [94mLoss[0m : 2.46467
[1mStep[0m  [144/169], [94mLoss[0m : 2.30962
[1mStep[0m  [160/169], [94mLoss[0m : 2.19878

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.470, [92mTest[0m: 2.376, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55577
[1mStep[0m  [16/169], [94mLoss[0m : 2.12375
[1mStep[0m  [32/169], [94mLoss[0m : 2.46074
[1mStep[0m  [48/169], [94mLoss[0m : 2.61956
[1mStep[0m  [64/169], [94mLoss[0m : 2.34486
[1mStep[0m  [80/169], [94mLoss[0m : 2.69868
[1mStep[0m  [96/169], [94mLoss[0m : 2.28614
[1mStep[0m  [112/169], [94mLoss[0m : 2.58847
[1mStep[0m  [128/169], [94mLoss[0m : 2.00869
[1mStep[0m  [144/169], [94mLoss[0m : 2.83207
[1mStep[0m  [160/169], [94mLoss[0m : 2.27692

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41782
[1mStep[0m  [16/169], [94mLoss[0m : 2.32160
[1mStep[0m  [32/169], [94mLoss[0m : 2.36296
[1mStep[0m  [48/169], [94mLoss[0m : 2.70999
[1mStep[0m  [64/169], [94mLoss[0m : 2.39779
[1mStep[0m  [80/169], [94mLoss[0m : 2.51676
[1mStep[0m  [96/169], [94mLoss[0m : 2.77412
[1mStep[0m  [112/169], [94mLoss[0m : 2.83463
[1mStep[0m  [128/169], [94mLoss[0m : 2.74463
[1mStep[0m  [144/169], [94mLoss[0m : 2.55074
[1mStep[0m  [160/169], [94mLoss[0m : 2.46834

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.70410
[1mStep[0m  [16/169], [94mLoss[0m : 2.43587
[1mStep[0m  [32/169], [94mLoss[0m : 2.10582
[1mStep[0m  [48/169], [94mLoss[0m : 2.35291
[1mStep[0m  [64/169], [94mLoss[0m : 2.04892
[1mStep[0m  [80/169], [94mLoss[0m : 2.32148
[1mStep[0m  [96/169], [94mLoss[0m : 2.19674
[1mStep[0m  [112/169], [94mLoss[0m : 2.58442
[1mStep[0m  [128/169], [94mLoss[0m : 2.54123
[1mStep[0m  [144/169], [94mLoss[0m : 2.85020
[1mStep[0m  [160/169], [94mLoss[0m : 2.13175

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.63535
[1mStep[0m  [16/169], [94mLoss[0m : 2.41569
[1mStep[0m  [32/169], [94mLoss[0m : 2.66952
[1mStep[0m  [48/169], [94mLoss[0m : 2.57942
[1mStep[0m  [64/169], [94mLoss[0m : 2.77355
[1mStep[0m  [80/169], [94mLoss[0m : 2.24506
[1mStep[0m  [96/169], [94mLoss[0m : 3.12953
[1mStep[0m  [112/169], [94mLoss[0m : 2.43773
[1mStep[0m  [128/169], [94mLoss[0m : 3.08062
[1mStep[0m  [144/169], [94mLoss[0m : 2.85515
[1mStep[0m  [160/169], [94mLoss[0m : 2.64607

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.71120
[1mStep[0m  [16/169], [94mLoss[0m : 2.44087
[1mStep[0m  [32/169], [94mLoss[0m : 2.86385
[1mStep[0m  [48/169], [94mLoss[0m : 2.16699
[1mStep[0m  [64/169], [94mLoss[0m : 2.53952
[1mStep[0m  [80/169], [94mLoss[0m : 2.31270
[1mStep[0m  [96/169], [94mLoss[0m : 2.58189
[1mStep[0m  [112/169], [94mLoss[0m : 2.26066
[1mStep[0m  [128/169], [94mLoss[0m : 2.48745
[1mStep[0m  [144/169], [94mLoss[0m : 2.60135
[1mStep[0m  [160/169], [94mLoss[0m : 2.44683

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89913
[1mStep[0m  [16/169], [94mLoss[0m : 2.62615
[1mStep[0m  [32/169], [94mLoss[0m : 2.36131
[1mStep[0m  [48/169], [94mLoss[0m : 2.39311
[1mStep[0m  [64/169], [94mLoss[0m : 3.21113
[1mStep[0m  [80/169], [94mLoss[0m : 2.07604
[1mStep[0m  [96/169], [94mLoss[0m : 2.81600
[1mStep[0m  [112/169], [94mLoss[0m : 2.42122
[1mStep[0m  [128/169], [94mLoss[0m : 2.23829
[1mStep[0m  [144/169], [94mLoss[0m : 1.97101
[1mStep[0m  [160/169], [94mLoss[0m : 2.52817

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.49431
[1mStep[0m  [16/169], [94mLoss[0m : 2.19589
[1mStep[0m  [32/169], [94mLoss[0m : 2.56261
[1mStep[0m  [48/169], [94mLoss[0m : 2.44263
[1mStep[0m  [64/169], [94mLoss[0m : 2.32126
[1mStep[0m  [80/169], [94mLoss[0m : 3.04436
[1mStep[0m  [96/169], [94mLoss[0m : 2.39906
[1mStep[0m  [112/169], [94mLoss[0m : 2.43673
[1mStep[0m  [128/169], [94mLoss[0m : 2.06969
[1mStep[0m  [144/169], [94mLoss[0m : 2.82748
[1mStep[0m  [160/169], [94mLoss[0m : 2.09040

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.341, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.31828
[1mStep[0m  [16/169], [94mLoss[0m : 2.62336
[1mStep[0m  [32/169], [94mLoss[0m : 2.50640
[1mStep[0m  [48/169], [94mLoss[0m : 2.74632
[1mStep[0m  [64/169], [94mLoss[0m : 2.11047
[1mStep[0m  [80/169], [94mLoss[0m : 2.21420
[1mStep[0m  [96/169], [94mLoss[0m : 2.33190
[1mStep[0m  [112/169], [94mLoss[0m : 2.73012
[1mStep[0m  [128/169], [94mLoss[0m : 2.14978
[1mStep[0m  [144/169], [94mLoss[0m : 2.58382
[1mStep[0m  [160/169], [94mLoss[0m : 2.34517

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53892
[1mStep[0m  [16/169], [94mLoss[0m : 2.78729
[1mStep[0m  [32/169], [94mLoss[0m : 2.39235
[1mStep[0m  [48/169], [94mLoss[0m : 2.29294
[1mStep[0m  [64/169], [94mLoss[0m : 2.37456
[1mStep[0m  [80/169], [94mLoss[0m : 2.35568
[1mStep[0m  [96/169], [94mLoss[0m : 2.66769
[1mStep[0m  [112/169], [94mLoss[0m : 2.54287
[1mStep[0m  [128/169], [94mLoss[0m : 2.74314
[1mStep[0m  [144/169], [94mLoss[0m : 2.70625
[1mStep[0m  [160/169], [94mLoss[0m : 2.28456

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28107
[1mStep[0m  [16/169], [94mLoss[0m : 2.60902
[1mStep[0m  [32/169], [94mLoss[0m : 2.42371
[1mStep[0m  [48/169], [94mLoss[0m : 2.42468
[1mStep[0m  [64/169], [94mLoss[0m : 2.58547
[1mStep[0m  [80/169], [94mLoss[0m : 2.29797
[1mStep[0m  [96/169], [94mLoss[0m : 2.28690
[1mStep[0m  [112/169], [94mLoss[0m : 2.73132
[1mStep[0m  [128/169], [94mLoss[0m : 2.03779
[1mStep[0m  [144/169], [94mLoss[0m : 2.43327
[1mStep[0m  [160/169], [94mLoss[0m : 2.00986

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.450, [92mTest[0m: 2.336, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21801
[1mStep[0m  [16/169], [94mLoss[0m : 2.75569
[1mStep[0m  [32/169], [94mLoss[0m : 2.44830
[1mStep[0m  [48/169], [94mLoss[0m : 2.35690
[1mStep[0m  [64/169], [94mLoss[0m : 2.44305
[1mStep[0m  [80/169], [94mLoss[0m : 2.43382
[1mStep[0m  [96/169], [94mLoss[0m : 2.54211
[1mStep[0m  [112/169], [94mLoss[0m : 2.50982
[1mStep[0m  [128/169], [94mLoss[0m : 2.26522
[1mStep[0m  [144/169], [94mLoss[0m : 3.07887
[1mStep[0m  [160/169], [94mLoss[0m : 2.53643

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86205
[1mStep[0m  [16/169], [94mLoss[0m : 2.62654
[1mStep[0m  [32/169], [94mLoss[0m : 2.74094
[1mStep[0m  [48/169], [94mLoss[0m : 2.46478
[1mStep[0m  [64/169], [94mLoss[0m : 2.26145
[1mStep[0m  [80/169], [94mLoss[0m : 2.94574
[1mStep[0m  [96/169], [94mLoss[0m : 2.73836
[1mStep[0m  [112/169], [94mLoss[0m : 2.63833
[1mStep[0m  [128/169], [94mLoss[0m : 2.41405
[1mStep[0m  [144/169], [94mLoss[0m : 2.35801
[1mStep[0m  [160/169], [94mLoss[0m : 2.36636

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23430
[1mStep[0m  [16/169], [94mLoss[0m : 2.66660
[1mStep[0m  [32/169], [94mLoss[0m : 2.23754
[1mStep[0m  [48/169], [94mLoss[0m : 2.61830
[1mStep[0m  [64/169], [94mLoss[0m : 2.42226
[1mStep[0m  [80/169], [94mLoss[0m : 2.19739
[1mStep[0m  [96/169], [94mLoss[0m : 2.46612
[1mStep[0m  [112/169], [94mLoss[0m : 2.28646
[1mStep[0m  [128/169], [94mLoss[0m : 2.38834
[1mStep[0m  [144/169], [94mLoss[0m : 2.46289
[1mStep[0m  [160/169], [94mLoss[0m : 2.11638

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.448, [92mTest[0m: 2.346, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39322
[1mStep[0m  [16/169], [94mLoss[0m : 2.23919
[1mStep[0m  [32/169], [94mLoss[0m : 2.31631
[1mStep[0m  [48/169], [94mLoss[0m : 2.17936
[1mStep[0m  [64/169], [94mLoss[0m : 2.32252
[1mStep[0m  [80/169], [94mLoss[0m : 2.43636
[1mStep[0m  [96/169], [94mLoss[0m : 2.39260
[1mStep[0m  [112/169], [94mLoss[0m : 2.46854
[1mStep[0m  [128/169], [94mLoss[0m : 2.58700
[1mStep[0m  [144/169], [94mLoss[0m : 2.73173
[1mStep[0m  [160/169], [94mLoss[0m : 2.24522

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.338, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41130
[1mStep[0m  [16/169], [94mLoss[0m : 2.46782
[1mStep[0m  [32/169], [94mLoss[0m : 2.44437
[1mStep[0m  [48/169], [94mLoss[0m : 2.41124
[1mStep[0m  [64/169], [94mLoss[0m : 2.02201
[1mStep[0m  [80/169], [94mLoss[0m : 2.35238
[1mStep[0m  [96/169], [94mLoss[0m : 2.50783
[1mStep[0m  [112/169], [94mLoss[0m : 2.22004
[1mStep[0m  [128/169], [94mLoss[0m : 2.64086
[1mStep[0m  [144/169], [94mLoss[0m : 2.69134
[1mStep[0m  [160/169], [94mLoss[0m : 2.16102

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.430, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48926
[1mStep[0m  [16/169], [94mLoss[0m : 2.34641
[1mStep[0m  [32/169], [94mLoss[0m : 2.81778
[1mStep[0m  [48/169], [94mLoss[0m : 2.78268
[1mStep[0m  [64/169], [94mLoss[0m : 2.68895
[1mStep[0m  [80/169], [94mLoss[0m : 2.20854
[1mStep[0m  [96/169], [94mLoss[0m : 2.38149
[1mStep[0m  [112/169], [94mLoss[0m : 2.25705
[1mStep[0m  [128/169], [94mLoss[0m : 2.55459
[1mStep[0m  [144/169], [94mLoss[0m : 2.17159
[1mStep[0m  [160/169], [94mLoss[0m : 2.26241

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.441, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08725
[1mStep[0m  [16/169], [94mLoss[0m : 2.64258
[1mStep[0m  [32/169], [94mLoss[0m : 2.63448
[1mStep[0m  [48/169], [94mLoss[0m : 2.34580
[1mStep[0m  [64/169], [94mLoss[0m : 2.91954
[1mStep[0m  [80/169], [94mLoss[0m : 2.39429
[1mStep[0m  [96/169], [94mLoss[0m : 2.12190
[1mStep[0m  [112/169], [94mLoss[0m : 2.40650
[1mStep[0m  [128/169], [94mLoss[0m : 2.60874
[1mStep[0m  [144/169], [94mLoss[0m : 2.20002
[1mStep[0m  [160/169], [94mLoss[0m : 2.47906

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.329
====================================

Phase 1 - Evaluation MAE:  2.3293983978884563
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.4, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 1.94011
[1mStep[0m  [16/169], [94mLoss[0m : 2.40235
[1mStep[0m  [32/169], [94mLoss[0m : 2.45032
[1mStep[0m  [48/169], [94mLoss[0m : 2.45536
[1mStep[0m  [64/169], [94mLoss[0m : 2.34173
[1mStep[0m  [80/169], [94mLoss[0m : 2.35629
[1mStep[0m  [96/169], [94mLoss[0m : 3.41507
[1mStep[0m  [112/169], [94mLoss[0m : 2.41386
[1mStep[0m  [128/169], [94mLoss[0m : 2.57395
[1mStep[0m  [144/169], [94mLoss[0m : 2.63305
[1mStep[0m  [160/169], [94mLoss[0m : 2.63202

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.495, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39128
[1mStep[0m  [16/169], [94mLoss[0m : 2.36070
[1mStep[0m  [32/169], [94mLoss[0m : 2.55835
[1mStep[0m  [48/169], [94mLoss[0m : 2.19609
[1mStep[0m  [64/169], [94mLoss[0m : 2.34450
[1mStep[0m  [80/169], [94mLoss[0m : 2.15888
[1mStep[0m  [96/169], [94mLoss[0m : 2.30455
[1mStep[0m  [112/169], [94mLoss[0m : 2.42488
[1mStep[0m  [128/169], [94mLoss[0m : 2.50579
[1mStep[0m  [144/169], [94mLoss[0m : 2.27766
[1mStep[0m  [160/169], [94mLoss[0m : 2.33477

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28953
[1mStep[0m  [16/169], [94mLoss[0m : 2.02160
[1mStep[0m  [32/169], [94mLoss[0m : 2.36720
[1mStep[0m  [48/169], [94mLoss[0m : 2.03100
[1mStep[0m  [64/169], [94mLoss[0m : 1.98219
[1mStep[0m  [80/169], [94mLoss[0m : 2.46213
[1mStep[0m  [96/169], [94mLoss[0m : 2.17837
[1mStep[0m  [112/169], [94mLoss[0m : 2.13737
[1mStep[0m  [128/169], [94mLoss[0m : 2.10259
[1mStep[0m  [144/169], [94mLoss[0m : 2.29820
[1mStep[0m  [160/169], [94mLoss[0m : 1.78859

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.391, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.88020
[1mStep[0m  [16/169], [94mLoss[0m : 2.36304
[1mStep[0m  [32/169], [94mLoss[0m : 2.22992
[1mStep[0m  [48/169], [94mLoss[0m : 2.04394
[1mStep[0m  [64/169], [94mLoss[0m : 2.28231
[1mStep[0m  [80/169], [94mLoss[0m : 2.17077
[1mStep[0m  [96/169], [94mLoss[0m : 2.34252
[1mStep[0m  [112/169], [94mLoss[0m : 2.22793
[1mStep[0m  [128/169], [94mLoss[0m : 2.47569
[1mStep[0m  [144/169], [94mLoss[0m : 1.98009
[1mStep[0m  [160/169], [94mLoss[0m : 2.45471

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.244, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17617
[1mStep[0m  [16/169], [94mLoss[0m : 2.38829
[1mStep[0m  [32/169], [94mLoss[0m : 2.01412
[1mStep[0m  [48/169], [94mLoss[0m : 1.55715
[1mStep[0m  [64/169], [94mLoss[0m : 2.28726
[1mStep[0m  [80/169], [94mLoss[0m : 2.18054
[1mStep[0m  [96/169], [94mLoss[0m : 2.23299
[1mStep[0m  [112/169], [94mLoss[0m : 2.11617
[1mStep[0m  [128/169], [94mLoss[0m : 2.10754
[1mStep[0m  [144/169], [94mLoss[0m : 2.02979
[1mStep[0m  [160/169], [94mLoss[0m : 2.78473

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.98597
[1mStep[0m  [16/169], [94mLoss[0m : 2.38293
[1mStep[0m  [32/169], [94mLoss[0m : 2.07975
[1mStep[0m  [48/169], [94mLoss[0m : 2.23557
[1mStep[0m  [64/169], [94mLoss[0m : 1.67872
[1mStep[0m  [80/169], [94mLoss[0m : 2.25310
[1mStep[0m  [96/169], [94mLoss[0m : 2.25102
[1mStep[0m  [112/169], [94mLoss[0m : 2.40884
[1mStep[0m  [128/169], [94mLoss[0m : 2.16651
[1mStep[0m  [144/169], [94mLoss[0m : 2.40872
[1mStep[0m  [160/169], [94mLoss[0m : 2.15982

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.145, [92mTest[0m: 2.375, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26729
[1mStep[0m  [16/169], [94mLoss[0m : 1.93604
[1mStep[0m  [32/169], [94mLoss[0m : 2.43666
[1mStep[0m  [48/169], [94mLoss[0m : 1.96498
[1mStep[0m  [64/169], [94mLoss[0m : 1.99280
[1mStep[0m  [80/169], [94mLoss[0m : 1.72703
[1mStep[0m  [96/169], [94mLoss[0m : 2.12524
[1mStep[0m  [112/169], [94mLoss[0m : 2.31750
[1mStep[0m  [128/169], [94mLoss[0m : 2.12128
[1mStep[0m  [144/169], [94mLoss[0m : 2.30920
[1mStep[0m  [160/169], [94mLoss[0m : 2.21089

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.088, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19567
[1mStep[0m  [16/169], [94mLoss[0m : 2.20654
[1mStep[0m  [32/169], [94mLoss[0m : 2.02373
[1mStep[0m  [48/169], [94mLoss[0m : 2.00737
[1mStep[0m  [64/169], [94mLoss[0m : 2.02592
[1mStep[0m  [80/169], [94mLoss[0m : 2.17258
[1mStep[0m  [96/169], [94mLoss[0m : 1.98147
[1mStep[0m  [112/169], [94mLoss[0m : 2.19385
[1mStep[0m  [128/169], [94mLoss[0m : 1.92460
[1mStep[0m  [144/169], [94mLoss[0m : 1.65194
[1mStep[0m  [160/169], [94mLoss[0m : 2.23948

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.054, [92mTest[0m: 2.400, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26700
[1mStep[0m  [16/169], [94mLoss[0m : 2.32080
[1mStep[0m  [32/169], [94mLoss[0m : 1.87951
[1mStep[0m  [48/169], [94mLoss[0m : 1.70642
[1mStep[0m  [64/169], [94mLoss[0m : 1.76658
[1mStep[0m  [80/169], [94mLoss[0m : 1.96340
[1mStep[0m  [96/169], [94mLoss[0m : 2.13871
[1mStep[0m  [112/169], [94mLoss[0m : 2.22745
[1mStep[0m  [128/169], [94mLoss[0m : 1.95745
[1mStep[0m  [144/169], [94mLoss[0m : 2.14670
[1mStep[0m  [160/169], [94mLoss[0m : 2.49527

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.012, [92mTest[0m: 2.411, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.13715
[1mStep[0m  [16/169], [94mLoss[0m : 2.09766
[1mStep[0m  [32/169], [94mLoss[0m : 1.91408
[1mStep[0m  [48/169], [94mLoss[0m : 1.61323
[1mStep[0m  [64/169], [94mLoss[0m : 1.93491
[1mStep[0m  [80/169], [94mLoss[0m : 1.95844
[1mStep[0m  [96/169], [94mLoss[0m : 2.05626
[1mStep[0m  [112/169], [94mLoss[0m : 2.02939
[1mStep[0m  [128/169], [94mLoss[0m : 1.86767
[1mStep[0m  [144/169], [94mLoss[0m : 1.82867
[1mStep[0m  [160/169], [94mLoss[0m : 2.25212

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.962, [92mTest[0m: 2.412, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.04330
[1mStep[0m  [16/169], [94mLoss[0m : 1.66272
[1mStep[0m  [32/169], [94mLoss[0m : 1.80683
[1mStep[0m  [48/169], [94mLoss[0m : 1.72893
[1mStep[0m  [64/169], [94mLoss[0m : 2.17374
[1mStep[0m  [80/169], [94mLoss[0m : 1.98908
[1mStep[0m  [96/169], [94mLoss[0m : 2.08640
[1mStep[0m  [112/169], [94mLoss[0m : 1.94823
[1mStep[0m  [128/169], [94mLoss[0m : 1.78840
[1mStep[0m  [144/169], [94mLoss[0m : 1.81053
[1mStep[0m  [160/169], [94mLoss[0m : 1.67497

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.913, [92mTest[0m: 2.435, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.54951
[1mStep[0m  [16/169], [94mLoss[0m : 1.83075
[1mStep[0m  [32/169], [94mLoss[0m : 1.80029
[1mStep[0m  [48/169], [94mLoss[0m : 1.88384
[1mStep[0m  [64/169], [94mLoss[0m : 2.27690
[1mStep[0m  [80/169], [94mLoss[0m : 1.76286
[1mStep[0m  [96/169], [94mLoss[0m : 1.47475
[1mStep[0m  [112/169], [94mLoss[0m : 2.14547
[1mStep[0m  [128/169], [94mLoss[0m : 2.02555
[1mStep[0m  [144/169], [94mLoss[0m : 2.28381
[1mStep[0m  [160/169], [94mLoss[0m : 1.93115

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.901, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.97633
[1mStep[0m  [16/169], [94mLoss[0m : 1.70313
[1mStep[0m  [32/169], [94mLoss[0m : 2.11965
[1mStep[0m  [48/169], [94mLoss[0m : 1.78750
[1mStep[0m  [64/169], [94mLoss[0m : 1.56805
[1mStep[0m  [80/169], [94mLoss[0m : 2.12929
[1mStep[0m  [96/169], [94mLoss[0m : 2.18139
[1mStep[0m  [112/169], [94mLoss[0m : 1.99375
[1mStep[0m  [128/169], [94mLoss[0m : 1.72474
[1mStep[0m  [144/169], [94mLoss[0m : 1.84412
[1mStep[0m  [160/169], [94mLoss[0m : 1.97426

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.865, [92mTest[0m: 2.405, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.93439
[1mStep[0m  [16/169], [94mLoss[0m : 1.92662
[1mStep[0m  [32/169], [94mLoss[0m : 1.72972
[1mStep[0m  [48/169], [94mLoss[0m : 1.93541
[1mStep[0m  [64/169], [94mLoss[0m : 1.64274
[1mStep[0m  [80/169], [94mLoss[0m : 1.79567
[1mStep[0m  [96/169], [94mLoss[0m : 1.56543
[1mStep[0m  [112/169], [94mLoss[0m : 1.70534
[1mStep[0m  [128/169], [94mLoss[0m : 1.88584
[1mStep[0m  [144/169], [94mLoss[0m : 1.78511
[1mStep[0m  [160/169], [94mLoss[0m : 1.63471

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.822, [92mTest[0m: 2.477, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59153
[1mStep[0m  [16/169], [94mLoss[0m : 1.93747
[1mStep[0m  [32/169], [94mLoss[0m : 1.59798
[1mStep[0m  [48/169], [94mLoss[0m : 1.61242
[1mStep[0m  [64/169], [94mLoss[0m : 1.68054
[1mStep[0m  [80/169], [94mLoss[0m : 1.74954
[1mStep[0m  [96/169], [94mLoss[0m : 1.47853
[1mStep[0m  [112/169], [94mLoss[0m : 1.68118
[1mStep[0m  [128/169], [94mLoss[0m : 2.10838
[1mStep[0m  [144/169], [94mLoss[0m : 2.02960
[1mStep[0m  [160/169], [94mLoss[0m : 1.79314

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.795, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50425
[1mStep[0m  [16/169], [94mLoss[0m : 1.58434
[1mStep[0m  [32/169], [94mLoss[0m : 1.72093
[1mStep[0m  [48/169], [94mLoss[0m : 1.85005
[1mStep[0m  [64/169], [94mLoss[0m : 1.54760
[1mStep[0m  [80/169], [94mLoss[0m : 1.85832
[1mStep[0m  [96/169], [94mLoss[0m : 1.91068
[1mStep[0m  [112/169], [94mLoss[0m : 1.86564
[1mStep[0m  [128/169], [94mLoss[0m : 1.84830
[1mStep[0m  [144/169], [94mLoss[0m : 1.64703
[1mStep[0m  [160/169], [94mLoss[0m : 1.96149

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51982
[1mStep[0m  [16/169], [94mLoss[0m : 1.51520
[1mStep[0m  [32/169], [94mLoss[0m : 1.60830
[1mStep[0m  [48/169], [94mLoss[0m : 1.44166
[1mStep[0m  [64/169], [94mLoss[0m : 1.97945
[1mStep[0m  [80/169], [94mLoss[0m : 1.70509
[1mStep[0m  [96/169], [94mLoss[0m : 1.76945
[1mStep[0m  [112/169], [94mLoss[0m : 1.64760
[1mStep[0m  [128/169], [94mLoss[0m : 1.90118
[1mStep[0m  [144/169], [94mLoss[0m : 1.67445
[1mStep[0m  [160/169], [94mLoss[0m : 2.09037

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.740, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66574
[1mStep[0m  [16/169], [94mLoss[0m : 1.55273
[1mStep[0m  [32/169], [94mLoss[0m : 1.42991
[1mStep[0m  [48/169], [94mLoss[0m : 1.79346
[1mStep[0m  [64/169], [94mLoss[0m : 1.88532
[1mStep[0m  [80/169], [94mLoss[0m : 1.72785
[1mStep[0m  [96/169], [94mLoss[0m : 2.17101
[1mStep[0m  [112/169], [94mLoss[0m : 1.62318
[1mStep[0m  [128/169], [94mLoss[0m : 2.02320
[1mStep[0m  [144/169], [94mLoss[0m : 1.79594
[1mStep[0m  [160/169], [94mLoss[0m : 1.56622

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.717, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.36251
[1mStep[0m  [16/169], [94mLoss[0m : 1.95054
[1mStep[0m  [32/169], [94mLoss[0m : 1.83444
[1mStep[0m  [48/169], [94mLoss[0m : 1.73367
[1mStep[0m  [64/169], [94mLoss[0m : 1.45445
[1mStep[0m  [80/169], [94mLoss[0m : 1.52506
[1mStep[0m  [96/169], [94mLoss[0m : 1.66026
[1mStep[0m  [112/169], [94mLoss[0m : 1.76813
[1mStep[0m  [128/169], [94mLoss[0m : 1.63582
[1mStep[0m  [144/169], [94mLoss[0m : 1.71583
[1mStep[0m  [160/169], [94mLoss[0m : 2.04476

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.673, [92mTest[0m: 2.450, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74933
[1mStep[0m  [16/169], [94mLoss[0m : 1.54494
[1mStep[0m  [32/169], [94mLoss[0m : 2.02191
[1mStep[0m  [48/169], [94mLoss[0m : 1.62208
[1mStep[0m  [64/169], [94mLoss[0m : 2.08721
[1mStep[0m  [80/169], [94mLoss[0m : 1.66851
[1mStep[0m  [96/169], [94mLoss[0m : 1.55875
[1mStep[0m  [112/169], [94mLoss[0m : 1.62206
[1mStep[0m  [128/169], [94mLoss[0m : 1.87380
[1mStep[0m  [144/169], [94mLoss[0m : 1.75797
[1mStep[0m  [160/169], [94mLoss[0m : 1.57152

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.650, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42719
[1mStep[0m  [16/169], [94mLoss[0m : 1.61248
[1mStep[0m  [32/169], [94mLoss[0m : 1.24847
[1mStep[0m  [48/169], [94mLoss[0m : 1.52457
[1mStep[0m  [64/169], [94mLoss[0m : 1.51723
[1mStep[0m  [80/169], [94mLoss[0m : 1.56150
[1mStep[0m  [96/169], [94mLoss[0m : 1.61834
[1mStep[0m  [112/169], [94mLoss[0m : 1.69207
[1mStep[0m  [128/169], [94mLoss[0m : 1.70519
[1mStep[0m  [144/169], [94mLoss[0m : 1.44650
[1mStep[0m  [160/169], [94mLoss[0m : 1.61625

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.617, [92mTest[0m: 2.493, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51753
[1mStep[0m  [16/169], [94mLoss[0m : 1.37624
[1mStep[0m  [32/169], [94mLoss[0m : 1.55775
[1mStep[0m  [48/169], [94mLoss[0m : 1.53222
[1mStep[0m  [64/169], [94mLoss[0m : 1.59750
[1mStep[0m  [80/169], [94mLoss[0m : 1.43191
[1mStep[0m  [96/169], [94mLoss[0m : 1.60418
[1mStep[0m  [112/169], [94mLoss[0m : 1.28643
[1mStep[0m  [128/169], [94mLoss[0m : 1.71280
[1mStep[0m  [144/169], [94mLoss[0m : 1.55877
[1mStep[0m  [160/169], [94mLoss[0m : 1.52750

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.586, [92mTest[0m: 2.531, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53229
[1mStep[0m  [16/169], [94mLoss[0m : 1.72592
[1mStep[0m  [32/169], [94mLoss[0m : 1.72773
[1mStep[0m  [48/169], [94mLoss[0m : 1.57020
[1mStep[0m  [64/169], [94mLoss[0m : 1.82812
[1mStep[0m  [80/169], [94mLoss[0m : 1.53517
[1mStep[0m  [96/169], [94mLoss[0m : 1.80266
[1mStep[0m  [112/169], [94mLoss[0m : 1.50064
[1mStep[0m  [128/169], [94mLoss[0m : 1.45098
[1mStep[0m  [144/169], [94mLoss[0m : 1.89600
[1mStep[0m  [160/169], [94mLoss[0m : 1.52396

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.583, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49292
[1mStep[0m  [16/169], [94mLoss[0m : 1.53078
[1mStep[0m  [32/169], [94mLoss[0m : 2.14282
[1mStep[0m  [48/169], [94mLoss[0m : 1.47215
[1mStep[0m  [64/169], [94mLoss[0m : 1.42482
[1mStep[0m  [80/169], [94mLoss[0m : 1.32559
[1mStep[0m  [96/169], [94mLoss[0m : 1.39337
[1mStep[0m  [112/169], [94mLoss[0m : 1.43601
[1mStep[0m  [128/169], [94mLoss[0m : 1.57634
[1mStep[0m  [144/169], [94mLoss[0m : 1.58088
[1mStep[0m  [160/169], [94mLoss[0m : 1.50304

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.525, [92mTest[0m: 2.507, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.78611
[1mStep[0m  [16/169], [94mLoss[0m : 1.62575
[1mStep[0m  [32/169], [94mLoss[0m : 1.64665
[1mStep[0m  [48/169], [94mLoss[0m : 1.35581
[1mStep[0m  [64/169], [94mLoss[0m : 1.69858
[1mStep[0m  [80/169], [94mLoss[0m : 1.31392
[1mStep[0m  [96/169], [94mLoss[0m : 1.59814
[1mStep[0m  [112/169], [94mLoss[0m : 1.48035
[1mStep[0m  [128/169], [94mLoss[0m : 1.67598
[1mStep[0m  [144/169], [94mLoss[0m : 1.52084
[1mStep[0m  [160/169], [94mLoss[0m : 1.45545

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.535, [92mTest[0m: 2.538, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59818
[1mStep[0m  [16/169], [94mLoss[0m : 1.47363
[1mStep[0m  [32/169], [94mLoss[0m : 1.42681
[1mStep[0m  [48/169], [94mLoss[0m : 1.45256
[1mStep[0m  [64/169], [94mLoss[0m : 1.41377
[1mStep[0m  [80/169], [94mLoss[0m : 1.31539
[1mStep[0m  [96/169], [94mLoss[0m : 1.46696
[1mStep[0m  [112/169], [94mLoss[0m : 1.47304
[1mStep[0m  [128/169], [94mLoss[0m : 1.64925
[1mStep[0m  [144/169], [94mLoss[0m : 1.35924
[1mStep[0m  [160/169], [94mLoss[0m : 1.51098

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.510, [92mTest[0m: 2.476, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53527
[1mStep[0m  [16/169], [94mLoss[0m : 1.20817
[1mStep[0m  [32/169], [94mLoss[0m : 1.47610
[1mStep[0m  [48/169], [94mLoss[0m : 1.39182
[1mStep[0m  [64/169], [94mLoss[0m : 1.64262
[1mStep[0m  [80/169], [94mLoss[0m : 1.55022
[1mStep[0m  [96/169], [94mLoss[0m : 1.52309
[1mStep[0m  [112/169], [94mLoss[0m : 1.33326
[1mStep[0m  [128/169], [94mLoss[0m : 1.26724
[1mStep[0m  [144/169], [94mLoss[0m : 1.48509
[1mStep[0m  [160/169], [94mLoss[0m : 1.45945

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.501, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.38659
[1mStep[0m  [16/169], [94mLoss[0m : 1.59950
[1mStep[0m  [32/169], [94mLoss[0m : 1.49948
[1mStep[0m  [48/169], [94mLoss[0m : 1.21576
[1mStep[0m  [64/169], [94mLoss[0m : 1.33947
[1mStep[0m  [80/169], [94mLoss[0m : 1.38633
[1mStep[0m  [96/169], [94mLoss[0m : 1.58835
[1mStep[0m  [112/169], [94mLoss[0m : 1.69753
[1mStep[0m  [128/169], [94mLoss[0m : 1.58866
[1mStep[0m  [144/169], [94mLoss[0m : 1.27863
[1mStep[0m  [160/169], [94mLoss[0m : 1.40067

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.485, [92mTest[0m: 2.498, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.39848
[1mStep[0m  [16/169], [94mLoss[0m : 1.73913
[1mStep[0m  [32/169], [94mLoss[0m : 1.35115
[1mStep[0m  [48/169], [94mLoss[0m : 1.22334
[1mStep[0m  [64/169], [94mLoss[0m : 1.26518
[1mStep[0m  [80/169], [94mLoss[0m : 1.25774
[1mStep[0m  [96/169], [94mLoss[0m : 1.51891
[1mStep[0m  [112/169], [94mLoss[0m : 1.36400
[1mStep[0m  [128/169], [94mLoss[0m : 1.48110
[1mStep[0m  [144/169], [94mLoss[0m : 1.20944
[1mStep[0m  [160/169], [94mLoss[0m : 1.61004

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.450, [92mTest[0m: 2.556, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.79626
[1mStep[0m  [16/169], [94mLoss[0m : 1.47258
[1mStep[0m  [32/169], [94mLoss[0m : 1.57563
[1mStep[0m  [48/169], [94mLoss[0m : 1.51421
[1mStep[0m  [64/169], [94mLoss[0m : 1.33788
[1mStep[0m  [80/169], [94mLoss[0m : 1.19770
[1mStep[0m  [96/169], [94mLoss[0m : 1.33593
[1mStep[0m  [112/169], [94mLoss[0m : 1.46215
[1mStep[0m  [128/169], [94mLoss[0m : 1.29046
[1mStep[0m  [144/169], [94mLoss[0m : 1.58747
[1mStep[0m  [160/169], [94mLoss[0m : 1.46181

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.461, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.489
====================================

Phase 2 - Evaluation MAE:  2.488851317337581
MAE score P1       2.329398
MAE score P2       2.488851
loss               1.450091
learning_rate      0.007525
batch_size               64
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.4
momentum                0.9
weight_decay         0.0001
Name: 9, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 9.75326
[1mStep[0m  [33/339], [94mLoss[0m : 5.01635
[1mStep[0m  [66/339], [94mLoss[0m : 3.48150
[1mStep[0m  [99/339], [94mLoss[0m : 3.08659
[1mStep[0m  [132/339], [94mLoss[0m : 3.27591
[1mStep[0m  [165/339], [94mLoss[0m : 2.38173
[1mStep[0m  [198/339], [94mLoss[0m : 2.28345
[1mStep[0m  [231/339], [94mLoss[0m : 2.70450
[1mStep[0m  [264/339], [94mLoss[0m : 2.88566
[1mStep[0m  [297/339], [94mLoss[0m : 2.72176
[1mStep[0m  [330/339], [94mLoss[0m : 2.37456

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.190, [92mTest[0m: 10.646, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77461
[1mStep[0m  [33/339], [94mLoss[0m : 2.54391
[1mStep[0m  [66/339], [94mLoss[0m : 2.28867
[1mStep[0m  [99/339], [94mLoss[0m : 3.01982
[1mStep[0m  [132/339], [94mLoss[0m : 2.95228
[1mStep[0m  [165/339], [94mLoss[0m : 2.56321
[1mStep[0m  [198/339], [94mLoss[0m : 2.65228
[1mStep[0m  [231/339], [94mLoss[0m : 3.39298
[1mStep[0m  [264/339], [94mLoss[0m : 2.03577
[1mStep[0m  [297/339], [94mLoss[0m : 2.38870
[1mStep[0m  [330/339], [94mLoss[0m : 2.21059

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.591, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.71567
[1mStep[0m  [33/339], [94mLoss[0m : 2.00155
[1mStep[0m  [66/339], [94mLoss[0m : 2.92252
[1mStep[0m  [99/339], [94mLoss[0m : 2.59282
[1mStep[0m  [132/339], [94mLoss[0m : 2.15943
[1mStep[0m  [165/339], [94mLoss[0m : 3.33548
[1mStep[0m  [198/339], [94mLoss[0m : 2.63845
[1mStep[0m  [231/339], [94mLoss[0m : 3.31111
[1mStep[0m  [264/339], [94mLoss[0m : 2.30294
[1mStep[0m  [297/339], [94mLoss[0m : 3.03889
[1mStep[0m  [330/339], [94mLoss[0m : 2.67871

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.581, [92mTest[0m: 2.367, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.69293
[1mStep[0m  [33/339], [94mLoss[0m : 2.08001
[1mStep[0m  [66/339], [94mLoss[0m : 2.46232
[1mStep[0m  [99/339], [94mLoss[0m : 2.78901
[1mStep[0m  [132/339], [94mLoss[0m : 1.87466
[1mStep[0m  [165/339], [94mLoss[0m : 2.45030
[1mStep[0m  [198/339], [94mLoss[0m : 2.39314
[1mStep[0m  [231/339], [94mLoss[0m : 2.49031
[1mStep[0m  [264/339], [94mLoss[0m : 1.81894
[1mStep[0m  [297/339], [94mLoss[0m : 2.75141
[1mStep[0m  [330/339], [94mLoss[0m : 2.77413

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.572, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.70630
[1mStep[0m  [33/339], [94mLoss[0m : 2.29138
[1mStep[0m  [66/339], [94mLoss[0m : 2.63539
[1mStep[0m  [99/339], [94mLoss[0m : 2.96290
[1mStep[0m  [132/339], [94mLoss[0m : 2.57785
[1mStep[0m  [165/339], [94mLoss[0m : 2.21344
[1mStep[0m  [198/339], [94mLoss[0m : 2.89121
[1mStep[0m  [231/339], [94mLoss[0m : 2.65270
[1mStep[0m  [264/339], [94mLoss[0m : 3.66332
[1mStep[0m  [297/339], [94mLoss[0m : 2.55610
[1mStep[0m  [330/339], [94mLoss[0m : 2.07375

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.571, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20442
[1mStep[0m  [33/339], [94mLoss[0m : 2.56218
[1mStep[0m  [66/339], [94mLoss[0m : 2.45809
[1mStep[0m  [99/339], [94mLoss[0m : 3.04316
[1mStep[0m  [132/339], [94mLoss[0m : 3.00184
[1mStep[0m  [165/339], [94mLoss[0m : 2.95105
[1mStep[0m  [198/339], [94mLoss[0m : 3.05523
[1mStep[0m  [231/339], [94mLoss[0m : 2.34023
[1mStep[0m  [264/339], [94mLoss[0m : 2.44696
[1mStep[0m  [297/339], [94mLoss[0m : 2.46622
[1mStep[0m  [330/339], [94mLoss[0m : 2.42795

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98332
[1mStep[0m  [33/339], [94mLoss[0m : 2.16986
[1mStep[0m  [66/339], [94mLoss[0m : 2.44490
[1mStep[0m  [99/339], [94mLoss[0m : 2.09039
[1mStep[0m  [132/339], [94mLoss[0m : 2.96314
[1mStep[0m  [165/339], [94mLoss[0m : 2.88294
[1mStep[0m  [198/339], [94mLoss[0m : 2.51904
[1mStep[0m  [231/339], [94mLoss[0m : 2.21597
[1mStep[0m  [264/339], [94mLoss[0m : 2.02379
[1mStep[0m  [297/339], [94mLoss[0m : 1.99212
[1mStep[0m  [330/339], [94mLoss[0m : 3.13747

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50556
[1mStep[0m  [33/339], [94mLoss[0m : 2.32832
[1mStep[0m  [66/339], [94mLoss[0m : 2.49131
[1mStep[0m  [99/339], [94mLoss[0m : 2.42433
[1mStep[0m  [132/339], [94mLoss[0m : 2.93983
[1mStep[0m  [165/339], [94mLoss[0m : 2.43154
[1mStep[0m  [198/339], [94mLoss[0m : 1.60117
[1mStep[0m  [231/339], [94mLoss[0m : 2.29375
[1mStep[0m  [264/339], [94mLoss[0m : 2.54626
[1mStep[0m  [297/339], [94mLoss[0m : 2.53575
[1mStep[0m  [330/339], [94mLoss[0m : 2.71905

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.543, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.77173
[1mStep[0m  [33/339], [94mLoss[0m : 2.39135
[1mStep[0m  [66/339], [94mLoss[0m : 2.20355
[1mStep[0m  [99/339], [94mLoss[0m : 1.90857
[1mStep[0m  [132/339], [94mLoss[0m : 2.57835
[1mStep[0m  [165/339], [94mLoss[0m : 2.44616
[1mStep[0m  [198/339], [94mLoss[0m : 2.30344
[1mStep[0m  [231/339], [94mLoss[0m : 2.97379
[1mStep[0m  [264/339], [94mLoss[0m : 2.64518
[1mStep[0m  [297/339], [94mLoss[0m : 2.62943
[1mStep[0m  [330/339], [94mLoss[0m : 2.57456

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.521, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65577
[1mStep[0m  [33/339], [94mLoss[0m : 2.69425
[1mStep[0m  [66/339], [94mLoss[0m : 2.04409
[1mStep[0m  [99/339], [94mLoss[0m : 3.41036
[1mStep[0m  [132/339], [94mLoss[0m : 2.38462
[1mStep[0m  [165/339], [94mLoss[0m : 2.50316
[1mStep[0m  [198/339], [94mLoss[0m : 2.74547
[1mStep[0m  [231/339], [94mLoss[0m : 3.10678
[1mStep[0m  [264/339], [94mLoss[0m : 2.76939
[1mStep[0m  [297/339], [94mLoss[0m : 2.55838
[1mStep[0m  [330/339], [94mLoss[0m : 2.51532

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.347, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36475
[1mStep[0m  [33/339], [94mLoss[0m : 2.93860
[1mStep[0m  [66/339], [94mLoss[0m : 2.88275
[1mStep[0m  [99/339], [94mLoss[0m : 2.59815
[1mStep[0m  [132/339], [94mLoss[0m : 2.78246
[1mStep[0m  [165/339], [94mLoss[0m : 2.11309
[1mStep[0m  [198/339], [94mLoss[0m : 1.94446
[1mStep[0m  [231/339], [94mLoss[0m : 3.02554
[1mStep[0m  [264/339], [94mLoss[0m : 2.82214
[1mStep[0m  [297/339], [94mLoss[0m : 2.18435
[1mStep[0m  [330/339], [94mLoss[0m : 2.62938

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.555, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47460
[1mStep[0m  [33/339], [94mLoss[0m : 2.28509
[1mStep[0m  [66/339], [94mLoss[0m : 2.95587
[1mStep[0m  [99/339], [94mLoss[0m : 2.14857
[1mStep[0m  [132/339], [94mLoss[0m : 2.52855
[1mStep[0m  [165/339], [94mLoss[0m : 2.90810
[1mStep[0m  [198/339], [94mLoss[0m : 2.89553
[1mStep[0m  [231/339], [94mLoss[0m : 2.20965
[1mStep[0m  [264/339], [94mLoss[0m : 2.50404
[1mStep[0m  [297/339], [94mLoss[0m : 2.40228
[1mStep[0m  [330/339], [94mLoss[0m : 2.38626

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.540, [92mTest[0m: 2.359, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.78146
[1mStep[0m  [33/339], [94mLoss[0m : 2.43806
[1mStep[0m  [66/339], [94mLoss[0m : 2.12172
[1mStep[0m  [99/339], [94mLoss[0m : 2.42834
[1mStep[0m  [132/339], [94mLoss[0m : 2.27124
[1mStep[0m  [165/339], [94mLoss[0m : 1.85982
[1mStep[0m  [198/339], [94mLoss[0m : 2.99504
[1mStep[0m  [231/339], [94mLoss[0m : 2.67450
[1mStep[0m  [264/339], [94mLoss[0m : 2.94766
[1mStep[0m  [297/339], [94mLoss[0m : 2.57487
[1mStep[0m  [330/339], [94mLoss[0m : 2.87549

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.528, [92mTest[0m: 2.350, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.23172
[1mStep[0m  [33/339], [94mLoss[0m : 3.20622
[1mStep[0m  [66/339], [94mLoss[0m : 2.77150
[1mStep[0m  [99/339], [94mLoss[0m : 2.84001
[1mStep[0m  [132/339], [94mLoss[0m : 3.04768
[1mStep[0m  [165/339], [94mLoss[0m : 2.55740
[1mStep[0m  [198/339], [94mLoss[0m : 2.70319
[1mStep[0m  [231/339], [94mLoss[0m : 2.65257
[1mStep[0m  [264/339], [94mLoss[0m : 2.32760
[1mStep[0m  [297/339], [94mLoss[0m : 2.06613
[1mStep[0m  [330/339], [94mLoss[0m : 2.51324

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.535, [92mTest[0m: 2.356, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.95540
[1mStep[0m  [33/339], [94mLoss[0m : 2.41207
[1mStep[0m  [66/339], [94mLoss[0m : 2.75538
[1mStep[0m  [99/339], [94mLoss[0m : 2.53148
[1mStep[0m  [132/339], [94mLoss[0m : 2.43627
[1mStep[0m  [165/339], [94mLoss[0m : 3.63546
[1mStep[0m  [198/339], [94mLoss[0m : 2.32134
[1mStep[0m  [231/339], [94mLoss[0m : 2.16449
[1mStep[0m  [264/339], [94mLoss[0m : 2.10889
[1mStep[0m  [297/339], [94mLoss[0m : 2.64456
[1mStep[0m  [330/339], [94mLoss[0m : 2.75837

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.32781
[1mStep[0m  [33/339], [94mLoss[0m : 2.70092
[1mStep[0m  [66/339], [94mLoss[0m : 2.50008
[1mStep[0m  [99/339], [94mLoss[0m : 2.81571
[1mStep[0m  [132/339], [94mLoss[0m : 2.34017
[1mStep[0m  [165/339], [94mLoss[0m : 2.92255
[1mStep[0m  [198/339], [94mLoss[0m : 2.27145
[1mStep[0m  [231/339], [94mLoss[0m : 2.10743
[1mStep[0m  [264/339], [94mLoss[0m : 2.67743
[1mStep[0m  [297/339], [94mLoss[0m : 2.31214
[1mStep[0m  [330/339], [94mLoss[0m : 2.71256

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.525, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28268
[1mStep[0m  [33/339], [94mLoss[0m : 2.08747
[1mStep[0m  [66/339], [94mLoss[0m : 2.65785
[1mStep[0m  [99/339], [94mLoss[0m : 2.38730
[1mStep[0m  [132/339], [94mLoss[0m : 2.46456
[1mStep[0m  [165/339], [94mLoss[0m : 2.39632
[1mStep[0m  [198/339], [94mLoss[0m : 2.46646
[1mStep[0m  [231/339], [94mLoss[0m : 2.25298
[1mStep[0m  [264/339], [94mLoss[0m : 2.94300
[1mStep[0m  [297/339], [94mLoss[0m : 1.80825
[1mStep[0m  [330/339], [94mLoss[0m : 2.42458

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.530, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68577
[1mStep[0m  [33/339], [94mLoss[0m : 2.40454
[1mStep[0m  [66/339], [94mLoss[0m : 2.42342
[1mStep[0m  [99/339], [94mLoss[0m : 3.04938
[1mStep[0m  [132/339], [94mLoss[0m : 2.79580
[1mStep[0m  [165/339], [94mLoss[0m : 2.11840
[1mStep[0m  [198/339], [94mLoss[0m : 2.22345
[1mStep[0m  [231/339], [94mLoss[0m : 2.55364
[1mStep[0m  [264/339], [94mLoss[0m : 2.32848
[1mStep[0m  [297/339], [94mLoss[0m : 2.88868
[1mStep[0m  [330/339], [94mLoss[0m : 2.37090

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.546, [92mTest[0m: 2.349, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49502
[1mStep[0m  [33/339], [94mLoss[0m : 2.95846
[1mStep[0m  [66/339], [94mLoss[0m : 2.38469
[1mStep[0m  [99/339], [94mLoss[0m : 2.65992
[1mStep[0m  [132/339], [94mLoss[0m : 1.97339
[1mStep[0m  [165/339], [94mLoss[0m : 2.59970
[1mStep[0m  [198/339], [94mLoss[0m : 2.60865
[1mStep[0m  [231/339], [94mLoss[0m : 2.63141
[1mStep[0m  [264/339], [94mLoss[0m : 2.36416
[1mStep[0m  [297/339], [94mLoss[0m : 2.58494
[1mStep[0m  [330/339], [94mLoss[0m : 2.40326

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54390
[1mStep[0m  [33/339], [94mLoss[0m : 2.22865
[1mStep[0m  [66/339], [94mLoss[0m : 2.85879
[1mStep[0m  [99/339], [94mLoss[0m : 2.35848
[1mStep[0m  [132/339], [94mLoss[0m : 2.21156
[1mStep[0m  [165/339], [94mLoss[0m : 2.60608
[1mStep[0m  [198/339], [94mLoss[0m : 2.51905
[1mStep[0m  [231/339], [94mLoss[0m : 2.84802
[1mStep[0m  [264/339], [94mLoss[0m : 2.57284
[1mStep[0m  [297/339], [94mLoss[0m : 2.13677
[1mStep[0m  [330/339], [94mLoss[0m : 2.14136

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56056
[1mStep[0m  [33/339], [94mLoss[0m : 2.34627
[1mStep[0m  [66/339], [94mLoss[0m : 2.68260
[1mStep[0m  [99/339], [94mLoss[0m : 1.96185
[1mStep[0m  [132/339], [94mLoss[0m : 2.17311
[1mStep[0m  [165/339], [94mLoss[0m : 2.94588
[1mStep[0m  [198/339], [94mLoss[0m : 2.33427
[1mStep[0m  [231/339], [94mLoss[0m : 2.19574
[1mStep[0m  [264/339], [94mLoss[0m : 2.42134
[1mStep[0m  [297/339], [94mLoss[0m : 2.53400
[1mStep[0m  [330/339], [94mLoss[0m : 2.05143

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.534, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.96826
[1mStep[0m  [33/339], [94mLoss[0m : 2.79274
[1mStep[0m  [66/339], [94mLoss[0m : 2.68378
[1mStep[0m  [99/339], [94mLoss[0m : 2.73993
[1mStep[0m  [132/339], [94mLoss[0m : 2.18671
[1mStep[0m  [165/339], [94mLoss[0m : 2.76143
[1mStep[0m  [198/339], [94mLoss[0m : 2.72657
[1mStep[0m  [231/339], [94mLoss[0m : 1.96853
[1mStep[0m  [264/339], [94mLoss[0m : 2.36881
[1mStep[0m  [297/339], [94mLoss[0m : 2.30090
[1mStep[0m  [330/339], [94mLoss[0m : 2.34516

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.558, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.75583
[1mStep[0m  [33/339], [94mLoss[0m : 2.29937
[1mStep[0m  [66/339], [94mLoss[0m : 2.61397
[1mStep[0m  [99/339], [94mLoss[0m : 2.19648
[1mStep[0m  [132/339], [94mLoss[0m : 2.97927
[1mStep[0m  [165/339], [94mLoss[0m : 2.90511
[1mStep[0m  [198/339], [94mLoss[0m : 2.61659
[1mStep[0m  [231/339], [94mLoss[0m : 2.02948
[1mStep[0m  [264/339], [94mLoss[0m : 3.02627
[1mStep[0m  [297/339], [94mLoss[0m : 2.04406
[1mStep[0m  [330/339], [94mLoss[0m : 3.00861

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.544, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.96468
[1mStep[0m  [33/339], [94mLoss[0m : 2.32501
[1mStep[0m  [66/339], [94mLoss[0m : 3.00654
[1mStep[0m  [99/339], [94mLoss[0m : 2.42860
[1mStep[0m  [132/339], [94mLoss[0m : 2.80768
[1mStep[0m  [165/339], [94mLoss[0m : 2.33992
[1mStep[0m  [198/339], [94mLoss[0m : 2.47890
[1mStep[0m  [231/339], [94mLoss[0m : 2.30231
[1mStep[0m  [264/339], [94mLoss[0m : 2.84661
[1mStep[0m  [297/339], [94mLoss[0m : 2.54682
[1mStep[0m  [330/339], [94mLoss[0m : 2.51065

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.92712
[1mStep[0m  [33/339], [94mLoss[0m : 3.36064
[1mStep[0m  [66/339], [94mLoss[0m : 2.33738
[1mStep[0m  [99/339], [94mLoss[0m : 1.94385
[1mStep[0m  [132/339], [94mLoss[0m : 2.57710
[1mStep[0m  [165/339], [94mLoss[0m : 2.63292
[1mStep[0m  [198/339], [94mLoss[0m : 3.03425
[1mStep[0m  [231/339], [94mLoss[0m : 2.81554
[1mStep[0m  [264/339], [94mLoss[0m : 2.96984
[1mStep[0m  [297/339], [94mLoss[0m : 2.67994
[1mStep[0m  [330/339], [94mLoss[0m : 2.63359

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.382, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47918
[1mStep[0m  [33/339], [94mLoss[0m : 2.58958
[1mStep[0m  [66/339], [94mLoss[0m : 2.01461
[1mStep[0m  [99/339], [94mLoss[0m : 2.66979
[1mStep[0m  [132/339], [94mLoss[0m : 2.21291
[1mStep[0m  [165/339], [94mLoss[0m : 1.87070
[1mStep[0m  [198/339], [94mLoss[0m : 3.10847
[1mStep[0m  [231/339], [94mLoss[0m : 2.23073
[1mStep[0m  [264/339], [94mLoss[0m : 2.78130
[1mStep[0m  [297/339], [94mLoss[0m : 2.96771
[1mStep[0m  [330/339], [94mLoss[0m : 2.19125

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.374, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.67946
[1mStep[0m  [33/339], [94mLoss[0m : 2.59795
[1mStep[0m  [66/339], [94mLoss[0m : 2.33679
[1mStep[0m  [99/339], [94mLoss[0m : 2.62457
[1mStep[0m  [132/339], [94mLoss[0m : 2.44402
[1mStep[0m  [165/339], [94mLoss[0m : 2.84637
[1mStep[0m  [198/339], [94mLoss[0m : 2.52506
[1mStep[0m  [231/339], [94mLoss[0m : 1.95463
[1mStep[0m  [264/339], [94mLoss[0m : 2.70819
[1mStep[0m  [297/339], [94mLoss[0m : 2.61980
[1mStep[0m  [330/339], [94mLoss[0m : 2.05620

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.547, [92mTest[0m: 2.367, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76085
[1mStep[0m  [33/339], [94mLoss[0m : 2.38677
[1mStep[0m  [66/339], [94mLoss[0m : 2.37334
[1mStep[0m  [99/339], [94mLoss[0m : 2.18294
[1mStep[0m  [132/339], [94mLoss[0m : 2.84762
[1mStep[0m  [165/339], [94mLoss[0m : 2.97794
[1mStep[0m  [198/339], [94mLoss[0m : 2.13234
[1mStep[0m  [231/339], [94mLoss[0m : 2.35849
[1mStep[0m  [264/339], [94mLoss[0m : 3.02899
[1mStep[0m  [297/339], [94mLoss[0m : 3.15337
[1mStep[0m  [330/339], [94mLoss[0m : 2.67010

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.553, [92mTest[0m: 2.373, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45495
[1mStep[0m  [33/339], [94mLoss[0m : 2.74916
[1mStep[0m  [66/339], [94mLoss[0m : 2.51617
[1mStep[0m  [99/339], [94mLoss[0m : 2.67389
[1mStep[0m  [132/339], [94mLoss[0m : 2.19182
[1mStep[0m  [165/339], [94mLoss[0m : 2.73668
[1mStep[0m  [198/339], [94mLoss[0m : 3.24175
[1mStep[0m  [231/339], [94mLoss[0m : 2.27738
[1mStep[0m  [264/339], [94mLoss[0m : 1.77294
[1mStep[0m  [297/339], [94mLoss[0m : 2.10490
[1mStep[0m  [330/339], [94mLoss[0m : 1.99259

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.550, [92mTest[0m: 2.373, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37307
[1mStep[0m  [33/339], [94mLoss[0m : 2.48335
[1mStep[0m  [66/339], [94mLoss[0m : 1.91960
[1mStep[0m  [99/339], [94mLoss[0m : 3.32097
[1mStep[0m  [132/339], [94mLoss[0m : 2.71179
[1mStep[0m  [165/339], [94mLoss[0m : 3.02821
[1mStep[0m  [198/339], [94mLoss[0m : 2.22726
[1mStep[0m  [231/339], [94mLoss[0m : 2.66123
[1mStep[0m  [264/339], [94mLoss[0m : 2.62058
[1mStep[0m  [297/339], [94mLoss[0m : 2.55281
[1mStep[0m  [330/339], [94mLoss[0m : 2.34147

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.559, [92mTest[0m: 2.381, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.382
====================================

Phase 1 - Evaluation MAE:  2.3819423544723377
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=200, bias=True)
        (in_batch norm): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=200, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.01
[1mStep[0m  [0/339], [94mLoss[0m : 3.06725
[1mStep[0m  [33/339], [94mLoss[0m : 2.04817
[1mStep[0m  [66/339], [94mLoss[0m : 2.31775
[1mStep[0m  [99/339], [94mLoss[0m : 2.29537
[1mStep[0m  [132/339], [94mLoss[0m : 2.67929
[1mStep[0m  [165/339], [94mLoss[0m : 2.33608
[1mStep[0m  [198/339], [94mLoss[0m : 2.71894
[1mStep[0m  [231/339], [94mLoss[0m : 2.30304
[1mStep[0m  [264/339], [94mLoss[0m : 2.20354
[1mStep[0m  [297/339], [94mLoss[0m : 2.77609
[1mStep[0m  [330/339], [94mLoss[0m : 2.31390

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.594, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.07426
[1mStep[0m  [33/339], [94mLoss[0m : 2.69600
[1mStep[0m  [66/339], [94mLoss[0m : 2.08179
[1mStep[0m  [99/339], [94mLoss[0m : 2.62792
[1mStep[0m  [132/339], [94mLoss[0m : 2.92983
[1mStep[0m  [165/339], [94mLoss[0m : 2.18493
[1mStep[0m  [198/339], [94mLoss[0m : 2.53039
[1mStep[0m  [231/339], [94mLoss[0m : 2.37283
[1mStep[0m  [264/339], [94mLoss[0m : 2.22864
[1mStep[0m  [297/339], [94mLoss[0m : 2.71643
[1mStep[0m  [330/339], [94mLoss[0m : 2.42583

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.515, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49137
[1mStep[0m  [33/339], [94mLoss[0m : 2.53856
[1mStep[0m  [66/339], [94mLoss[0m : 2.76511
[1mStep[0m  [99/339], [94mLoss[0m : 1.81100
[1mStep[0m  [132/339], [94mLoss[0m : 2.49833
[1mStep[0m  [165/339], [94mLoss[0m : 2.15684
[1mStep[0m  [198/339], [94mLoss[0m : 2.35870
[1mStep[0m  [231/339], [94mLoss[0m : 2.41484
[1mStep[0m  [264/339], [94mLoss[0m : 2.37334
[1mStep[0m  [297/339], [94mLoss[0m : 2.55030
[1mStep[0m  [330/339], [94mLoss[0m : 2.35779

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.68390
[1mStep[0m  [33/339], [94mLoss[0m : 2.47856
[1mStep[0m  [66/339], [94mLoss[0m : 2.03957
[1mStep[0m  [99/339], [94mLoss[0m : 2.53349
[1mStep[0m  [132/339], [94mLoss[0m : 2.00964
[1mStep[0m  [165/339], [94mLoss[0m : 2.93955
[1mStep[0m  [198/339], [94mLoss[0m : 2.71035
[1mStep[0m  [231/339], [94mLoss[0m : 2.76381
[1mStep[0m  [264/339], [94mLoss[0m : 2.27179
[1mStep[0m  [297/339], [94mLoss[0m : 2.50652
[1mStep[0m  [330/339], [94mLoss[0m : 2.27936

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33849
[1mStep[0m  [33/339], [94mLoss[0m : 2.22410
[1mStep[0m  [66/339], [94mLoss[0m : 3.01147
[1mStep[0m  [99/339], [94mLoss[0m : 3.04787
[1mStep[0m  [132/339], [94mLoss[0m : 3.06795
[1mStep[0m  [165/339], [94mLoss[0m : 2.45074
[1mStep[0m  [198/339], [94mLoss[0m : 2.49710
[1mStep[0m  [231/339], [94mLoss[0m : 2.58269
[1mStep[0m  [264/339], [94mLoss[0m : 2.73034
[1mStep[0m  [297/339], [94mLoss[0m : 2.27196
[1mStep[0m  [330/339], [94mLoss[0m : 2.28550

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16203
[1mStep[0m  [33/339], [94mLoss[0m : 2.10662
[1mStep[0m  [66/339], [94mLoss[0m : 2.74855
[1mStep[0m  [99/339], [94mLoss[0m : 2.04801
[1mStep[0m  [132/339], [94mLoss[0m : 2.09999
[1mStep[0m  [165/339], [94mLoss[0m : 2.23633
[1mStep[0m  [198/339], [94mLoss[0m : 2.65580
[1mStep[0m  [231/339], [94mLoss[0m : 2.02823
[1mStep[0m  [264/339], [94mLoss[0m : 3.15396
[1mStep[0m  [297/339], [94mLoss[0m : 2.01993
[1mStep[0m  [330/339], [94mLoss[0m : 2.55964

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.371, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35395
[1mStep[0m  [33/339], [94mLoss[0m : 2.06746
[1mStep[0m  [66/339], [94mLoss[0m : 2.66887
[1mStep[0m  [99/339], [94mLoss[0m : 1.89977
[1mStep[0m  [132/339], [94mLoss[0m : 2.42663
[1mStep[0m  [165/339], [94mLoss[0m : 2.62165
[1mStep[0m  [198/339], [94mLoss[0m : 2.40050
[1mStep[0m  [231/339], [94mLoss[0m : 2.50153
[1mStep[0m  [264/339], [94mLoss[0m : 2.45527
[1mStep[0m  [297/339], [94mLoss[0m : 2.25030
[1mStep[0m  [330/339], [94mLoss[0m : 2.21498

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.358, [92mTest[0m: 2.402, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.10802
[1mStep[0m  [33/339], [94mLoss[0m : 1.92336
[1mStep[0m  [66/339], [94mLoss[0m : 2.57434
[1mStep[0m  [99/339], [94mLoss[0m : 1.87706
[1mStep[0m  [132/339], [94mLoss[0m : 2.55302
[1mStep[0m  [165/339], [94mLoss[0m : 2.28347
[1mStep[0m  [198/339], [94mLoss[0m : 2.11933
[1mStep[0m  [231/339], [94mLoss[0m : 2.47735
[1mStep[0m  [264/339], [94mLoss[0m : 2.52152
[1mStep[0m  [297/339], [94mLoss[0m : 2.18348
[1mStep[0m  [330/339], [94mLoss[0m : 2.48624

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.320, [92mTest[0m: 2.395, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56585
[1mStep[0m  [33/339], [94mLoss[0m : 1.71201
[1mStep[0m  [66/339], [94mLoss[0m : 2.47478
[1mStep[0m  [99/339], [94mLoss[0m : 2.56680
[1mStep[0m  [132/339], [94mLoss[0m : 1.62489
[1mStep[0m  [165/339], [94mLoss[0m : 2.00578
[1mStep[0m  [198/339], [94mLoss[0m : 1.92543
[1mStep[0m  [231/339], [94mLoss[0m : 2.01248
[1mStep[0m  [264/339], [94mLoss[0m : 2.41347
[1mStep[0m  [297/339], [94mLoss[0m : 2.09994
[1mStep[0m  [330/339], [94mLoss[0m : 2.35442

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.293, [92mTest[0m: 2.413, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04275
[1mStep[0m  [33/339], [94mLoss[0m : 2.83765
[1mStep[0m  [66/339], [94mLoss[0m : 2.28813
[1mStep[0m  [99/339], [94mLoss[0m : 2.54044
[1mStep[0m  [132/339], [94mLoss[0m : 2.24275
[1mStep[0m  [165/339], [94mLoss[0m : 2.09602
[1mStep[0m  [198/339], [94mLoss[0m : 2.84232
[1mStep[0m  [231/339], [94mLoss[0m : 2.31839
[1mStep[0m  [264/339], [94mLoss[0m : 2.26901
[1mStep[0m  [297/339], [94mLoss[0m : 2.24843
[1mStep[0m  [330/339], [94mLoss[0m : 2.71250

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.281, [92mTest[0m: 2.396, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.90173
[1mStep[0m  [33/339], [94mLoss[0m : 2.32651
[1mStep[0m  [66/339], [94mLoss[0m : 2.52548
[1mStep[0m  [99/339], [94mLoss[0m : 2.48517
[1mStep[0m  [132/339], [94mLoss[0m : 2.42456
[1mStep[0m  [165/339], [94mLoss[0m : 2.34514
[1mStep[0m  [198/339], [94mLoss[0m : 2.64400
[1mStep[0m  [231/339], [94mLoss[0m : 2.48355
[1mStep[0m  [264/339], [94mLoss[0m : 1.96106
[1mStep[0m  [297/339], [94mLoss[0m : 2.37141
[1mStep[0m  [330/339], [94mLoss[0m : 2.78093

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.284, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08564
[1mStep[0m  [33/339], [94mLoss[0m : 2.19738
[1mStep[0m  [66/339], [94mLoss[0m : 1.94633
[1mStep[0m  [99/339], [94mLoss[0m : 2.05485
[1mStep[0m  [132/339], [94mLoss[0m : 2.35166
[1mStep[0m  [165/339], [94mLoss[0m : 2.67313
[1mStep[0m  [198/339], [94mLoss[0m : 2.84887
[1mStep[0m  [231/339], [94mLoss[0m : 2.56423
[1mStep[0m  [264/339], [94mLoss[0m : 2.13035
[1mStep[0m  [297/339], [94mLoss[0m : 1.90299
[1mStep[0m  [330/339], [94mLoss[0m : 2.37472

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.254, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37371
[1mStep[0m  [33/339], [94mLoss[0m : 2.25251
[1mStep[0m  [66/339], [94mLoss[0m : 1.65206
[1mStep[0m  [99/339], [94mLoss[0m : 2.74165
[1mStep[0m  [132/339], [94mLoss[0m : 2.51338
[1mStep[0m  [165/339], [94mLoss[0m : 1.96307
[1mStep[0m  [198/339], [94mLoss[0m : 2.45575
[1mStep[0m  [231/339], [94mLoss[0m : 2.53270
[1mStep[0m  [264/339], [94mLoss[0m : 2.34361
[1mStep[0m  [297/339], [94mLoss[0m : 2.21530
[1mStep[0m  [330/339], [94mLoss[0m : 2.44214

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.255, [92mTest[0m: 2.417, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04986
[1mStep[0m  [33/339], [94mLoss[0m : 2.21648
[1mStep[0m  [66/339], [94mLoss[0m : 2.01026
[1mStep[0m  [99/339], [94mLoss[0m : 2.08483
[1mStep[0m  [132/339], [94mLoss[0m : 2.77803
[1mStep[0m  [165/339], [94mLoss[0m : 1.97481
[1mStep[0m  [198/339], [94mLoss[0m : 2.70331
[1mStep[0m  [231/339], [94mLoss[0m : 2.44232
[1mStep[0m  [264/339], [94mLoss[0m : 2.44347
[1mStep[0m  [297/339], [94mLoss[0m : 2.58004
[1mStep[0m  [330/339], [94mLoss[0m : 2.09126

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.219, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34893
[1mStep[0m  [33/339], [94mLoss[0m : 2.59077
[1mStep[0m  [66/339], [94mLoss[0m : 2.26378
[1mStep[0m  [99/339], [94mLoss[0m : 1.91243
[1mStep[0m  [132/339], [94mLoss[0m : 1.95606
[1mStep[0m  [165/339], [94mLoss[0m : 2.18338
[1mStep[0m  [198/339], [94mLoss[0m : 1.93566
[1mStep[0m  [231/339], [94mLoss[0m : 1.97570
[1mStep[0m  [264/339], [94mLoss[0m : 2.39142
[1mStep[0m  [297/339], [94mLoss[0m : 2.78293
[1mStep[0m  [330/339], [94mLoss[0m : 2.54821

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.226, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.98838
[1mStep[0m  [33/339], [94mLoss[0m : 2.62226
[1mStep[0m  [66/339], [94mLoss[0m : 1.99546
[1mStep[0m  [99/339], [94mLoss[0m : 1.89613
[1mStep[0m  [132/339], [94mLoss[0m : 2.21065
[1mStep[0m  [165/339], [94mLoss[0m : 2.48599
[1mStep[0m  [198/339], [94mLoss[0m : 2.46788
[1mStep[0m  [231/339], [94mLoss[0m : 2.05484
[1mStep[0m  [264/339], [94mLoss[0m : 2.50193
[1mStep[0m  [297/339], [94mLoss[0m : 2.80458
[1mStep[0m  [330/339], [94mLoss[0m : 2.17612

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.211, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96911
[1mStep[0m  [33/339], [94mLoss[0m : 1.97011
[1mStep[0m  [66/339], [94mLoss[0m : 2.34086
[1mStep[0m  [99/339], [94mLoss[0m : 1.43815
[1mStep[0m  [132/339], [94mLoss[0m : 2.50004
[1mStep[0m  [165/339], [94mLoss[0m : 2.17941
[1mStep[0m  [198/339], [94mLoss[0m : 2.20972
[1mStep[0m  [231/339], [94mLoss[0m : 2.23005
[1mStep[0m  [264/339], [94mLoss[0m : 2.49559
[1mStep[0m  [297/339], [94mLoss[0m : 2.58668
[1mStep[0m  [330/339], [94mLoss[0m : 2.31918

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.191, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.48116
[1mStep[0m  [33/339], [94mLoss[0m : 2.01683
[1mStep[0m  [66/339], [94mLoss[0m : 2.88960
[1mStep[0m  [99/339], [94mLoss[0m : 2.02715
[1mStep[0m  [132/339], [94mLoss[0m : 1.95881
[1mStep[0m  [165/339], [94mLoss[0m : 2.17204
[1mStep[0m  [198/339], [94mLoss[0m : 2.24338
[1mStep[0m  [231/339], [94mLoss[0m : 2.27887
[1mStep[0m  [264/339], [94mLoss[0m : 2.07577
[1mStep[0m  [297/339], [94mLoss[0m : 2.74997
[1mStep[0m  [330/339], [94mLoss[0m : 2.27952

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.479, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21700
[1mStep[0m  [33/339], [94mLoss[0m : 2.07651
[1mStep[0m  [66/339], [94mLoss[0m : 1.72535
[1mStep[0m  [99/339], [94mLoss[0m : 2.28430
[1mStep[0m  [132/339], [94mLoss[0m : 1.75794
[1mStep[0m  [165/339], [94mLoss[0m : 2.12439
[1mStep[0m  [198/339], [94mLoss[0m : 2.43397
[1mStep[0m  [231/339], [94mLoss[0m : 1.84841
[1mStep[0m  [264/339], [94mLoss[0m : 1.86335
[1mStep[0m  [297/339], [94mLoss[0m : 2.74251
[1mStep[0m  [330/339], [94mLoss[0m : 1.49173

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.174, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.99044
[1mStep[0m  [33/339], [94mLoss[0m : 2.20816
[1mStep[0m  [66/339], [94mLoss[0m : 1.61706
[1mStep[0m  [99/339], [94mLoss[0m : 2.11736
[1mStep[0m  [132/339], [94mLoss[0m : 2.56879
[1mStep[0m  [165/339], [94mLoss[0m : 1.90334
[1mStep[0m  [198/339], [94mLoss[0m : 2.85438
[1mStep[0m  [231/339], [94mLoss[0m : 1.77606
[1mStep[0m  [264/339], [94mLoss[0m : 2.23084
[1mStep[0m  [297/339], [94mLoss[0m : 2.35331
[1mStep[0m  [330/339], [94mLoss[0m : 2.10659

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.182, [92mTest[0m: 2.457, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09310
[1mStep[0m  [33/339], [94mLoss[0m : 2.25072
[1mStep[0m  [66/339], [94mLoss[0m : 2.30276
[1mStep[0m  [99/339], [94mLoss[0m : 2.06725
[1mStep[0m  [132/339], [94mLoss[0m : 2.43119
[1mStep[0m  [165/339], [94mLoss[0m : 2.17126
[1mStep[0m  [198/339], [94mLoss[0m : 2.42768
[1mStep[0m  [231/339], [94mLoss[0m : 1.49188
[1mStep[0m  [264/339], [94mLoss[0m : 1.87630
[1mStep[0m  [297/339], [94mLoss[0m : 2.29003
[1mStep[0m  [330/339], [94mLoss[0m : 1.94715

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.150, [92mTest[0m: 2.509, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.51704
[1mStep[0m  [33/339], [94mLoss[0m : 2.08403
[1mStep[0m  [66/339], [94mLoss[0m : 2.30607
[1mStep[0m  [99/339], [94mLoss[0m : 2.27720
[1mStep[0m  [132/339], [94mLoss[0m : 1.48669
[1mStep[0m  [165/339], [94mLoss[0m : 2.61940
[1mStep[0m  [198/339], [94mLoss[0m : 2.04851
[1mStep[0m  [231/339], [94mLoss[0m : 1.74541
[1mStep[0m  [264/339], [94mLoss[0m : 2.21724
[1mStep[0m  [297/339], [94mLoss[0m : 2.13977
[1mStep[0m  [330/339], [94mLoss[0m : 2.15314

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.121, [92mTest[0m: 2.420, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85215
[1mStep[0m  [33/339], [94mLoss[0m : 1.95506
[1mStep[0m  [66/339], [94mLoss[0m : 2.22104
[1mStep[0m  [99/339], [94mLoss[0m : 1.75306
[1mStep[0m  [132/339], [94mLoss[0m : 1.53531
[1mStep[0m  [165/339], [94mLoss[0m : 1.81399
[1mStep[0m  [198/339], [94mLoss[0m : 1.50588
[1mStep[0m  [231/339], [94mLoss[0m : 1.97453
[1mStep[0m  [264/339], [94mLoss[0m : 1.79159
[1mStep[0m  [297/339], [94mLoss[0m : 2.57876
[1mStep[0m  [330/339], [94mLoss[0m : 1.69532

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.117, [92mTest[0m: 2.438, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00989
[1mStep[0m  [33/339], [94mLoss[0m : 2.09652
[1mStep[0m  [66/339], [94mLoss[0m : 1.72759
[1mStep[0m  [99/339], [94mLoss[0m : 2.31298
[1mStep[0m  [132/339], [94mLoss[0m : 2.04841
[1mStep[0m  [165/339], [94mLoss[0m : 1.99108
[1mStep[0m  [198/339], [94mLoss[0m : 2.10578
[1mStep[0m  [231/339], [94mLoss[0m : 2.34304
[1mStep[0m  [264/339], [94mLoss[0m : 1.92148
[1mStep[0m  [297/339], [94mLoss[0m : 2.00801
[1mStep[0m  [330/339], [94mLoss[0m : 1.85322

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.124, [92mTest[0m: 2.518, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.97602
[1mStep[0m  [33/339], [94mLoss[0m : 2.21360
[1mStep[0m  [66/339], [94mLoss[0m : 1.68448
[1mStep[0m  [99/339], [94mLoss[0m : 1.64381
[1mStep[0m  [132/339], [94mLoss[0m : 2.19190
[1mStep[0m  [165/339], [94mLoss[0m : 1.73009
[1mStep[0m  [198/339], [94mLoss[0m : 2.34011
[1mStep[0m  [231/339], [94mLoss[0m : 1.93126
[1mStep[0m  [264/339], [94mLoss[0m : 1.73399
[1mStep[0m  [297/339], [94mLoss[0m : 2.31904
[1mStep[0m  [330/339], [94mLoss[0m : 1.69830

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.469, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11748
[1mStep[0m  [33/339], [94mLoss[0m : 2.24305
[1mStep[0m  [66/339], [94mLoss[0m : 2.34946
[1mStep[0m  [99/339], [94mLoss[0m : 2.24695
[1mStep[0m  [132/339], [94mLoss[0m : 2.39616
[1mStep[0m  [165/339], [94mLoss[0m : 2.33942
[1mStep[0m  [198/339], [94mLoss[0m : 1.71092
[1mStep[0m  [231/339], [94mLoss[0m : 2.53348
[1mStep[0m  [264/339], [94mLoss[0m : 2.52087
[1mStep[0m  [297/339], [94mLoss[0m : 2.12318
[1mStep[0m  [330/339], [94mLoss[0m : 1.87597

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.099, [92mTest[0m: 2.461, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.36543
[1mStep[0m  [33/339], [94mLoss[0m : 1.67775
[1mStep[0m  [66/339], [94mLoss[0m : 1.82489
[1mStep[0m  [99/339], [94mLoss[0m : 1.92391
[1mStep[0m  [132/339], [94mLoss[0m : 2.56519
[1mStep[0m  [165/339], [94mLoss[0m : 1.87194
[1mStep[0m  [198/339], [94mLoss[0m : 1.88458
[1mStep[0m  [231/339], [94mLoss[0m : 2.32110
[1mStep[0m  [264/339], [94mLoss[0m : 1.79302
[1mStep[0m  [297/339], [94mLoss[0m : 2.08591
[1mStep[0m  [330/339], [94mLoss[0m : 2.75751

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.078, [92mTest[0m: 2.466, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75747
[1mStep[0m  [33/339], [94mLoss[0m : 2.08823
[1mStep[0m  [66/339], [94mLoss[0m : 1.85419
[1mStep[0m  [99/339], [94mLoss[0m : 2.20925
[1mStep[0m  [132/339], [94mLoss[0m : 2.20301
[1mStep[0m  [165/339], [94mLoss[0m : 1.86596
[1mStep[0m  [198/339], [94mLoss[0m : 2.48843
[1mStep[0m  [231/339], [94mLoss[0m : 1.71747
[1mStep[0m  [264/339], [94mLoss[0m : 2.30709
[1mStep[0m  [297/339], [94mLoss[0m : 2.41066
[1mStep[0m  [330/339], [94mLoss[0m : 1.95422

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.102, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.70744
[1mStep[0m  [33/339], [94mLoss[0m : 2.48894
[1mStep[0m  [66/339], [94mLoss[0m : 2.60199
[1mStep[0m  [99/339], [94mLoss[0m : 2.15400
[1mStep[0m  [132/339], [94mLoss[0m : 1.82702
[1mStep[0m  [165/339], [94mLoss[0m : 1.97695
[1mStep[0m  [198/339], [94mLoss[0m : 1.18124
[1mStep[0m  [231/339], [94mLoss[0m : 2.46482
[1mStep[0m  [264/339], [94mLoss[0m : 2.49847
[1mStep[0m  [297/339], [94mLoss[0m : 2.02950
[1mStep[0m  [330/339], [94mLoss[0m : 2.04070

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.103, [92mTest[0m: 2.501, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31790
[1mStep[0m  [33/339], [94mLoss[0m : 2.16556
[1mStep[0m  [66/339], [94mLoss[0m : 1.87851
[1mStep[0m  [99/339], [94mLoss[0m : 2.05958
[1mStep[0m  [132/339], [94mLoss[0m : 2.30728
[1mStep[0m  [165/339], [94mLoss[0m : 1.83390
[1mStep[0m  [198/339], [94mLoss[0m : 2.12273
[1mStep[0m  [231/339], [94mLoss[0m : 2.56311
[1mStep[0m  [264/339], [94mLoss[0m : 2.25247
[1mStep[0m  [297/339], [94mLoss[0m : 2.15084
[1mStep[0m  [330/339], [94mLoss[0m : 2.05058

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.074, [92mTest[0m: 2.496, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.436
====================================

Phase 2 - Evaluation MAE:  2.4357206599902264
MAE score P1       2.381942
MAE score P2       2.435721
loss               2.074001
learning_rate      0.007525
batch_size               32
hidden_sizes      [200, 50]
epochs                   30
activation          sigmoid
optimizer               sgd
early stopping        False
dropout                 0.3
momentum                0.5
weight_decay           0.01
Name: 10, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 11.07560
[1mStep[0m  [16/169], [94mLoss[0m : 3.78138
[1mStep[0m  [32/169], [94mLoss[0m : 2.40162
[1mStep[0m  [48/169], [94mLoss[0m : 2.82096
[1mStep[0m  [64/169], [94mLoss[0m : 2.56348
[1mStep[0m  [80/169], [94mLoss[0m : 2.55017
[1mStep[0m  [96/169], [94mLoss[0m : 2.40967
[1mStep[0m  [112/169], [94mLoss[0m : 2.35819
[1mStep[0m  [128/169], [94mLoss[0m : 3.00579
[1mStep[0m  [144/169], [94mLoss[0m : 2.30030
[1mStep[0m  [160/169], [94mLoss[0m : 2.78190

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.117, [92mTest[0m: 11.580, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43417
[1mStep[0m  [16/169], [94mLoss[0m : 2.70074
[1mStep[0m  [32/169], [94mLoss[0m : 2.53081
[1mStep[0m  [48/169], [94mLoss[0m : 2.85232
[1mStep[0m  [64/169], [94mLoss[0m : 2.52404
[1mStep[0m  [80/169], [94mLoss[0m : 2.40459
[1mStep[0m  [96/169], [94mLoss[0m : 2.34944
[1mStep[0m  [112/169], [94mLoss[0m : 2.57764
[1mStep[0m  [128/169], [94mLoss[0m : 2.96380
[1mStep[0m  [144/169], [94mLoss[0m : 2.49190
[1mStep[0m  [160/169], [94mLoss[0m : 2.72701

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.511, [92mTest[0m: 2.428, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.42397
[1mStep[0m  [16/169], [94mLoss[0m : 2.37700
[1mStep[0m  [32/169], [94mLoss[0m : 2.54150
[1mStep[0m  [48/169], [94mLoss[0m : 2.26268
[1mStep[0m  [64/169], [94mLoss[0m : 2.26810
[1mStep[0m  [80/169], [94mLoss[0m : 2.08889
[1mStep[0m  [96/169], [94mLoss[0m : 2.40739
[1mStep[0m  [112/169], [94mLoss[0m : 2.78575
[1mStep[0m  [128/169], [94mLoss[0m : 2.45875
[1mStep[0m  [144/169], [94mLoss[0m : 2.71297
[1mStep[0m  [160/169], [94mLoss[0m : 2.76213

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.382, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18885
[1mStep[0m  [16/169], [94mLoss[0m : 2.15771
[1mStep[0m  [32/169], [94mLoss[0m : 2.63817
[1mStep[0m  [48/169], [94mLoss[0m : 2.32262
[1mStep[0m  [64/169], [94mLoss[0m : 2.11588
[1mStep[0m  [80/169], [94mLoss[0m : 2.71846
[1mStep[0m  [96/169], [94mLoss[0m : 2.62324
[1mStep[0m  [112/169], [94mLoss[0m : 2.46866
[1mStep[0m  [128/169], [94mLoss[0m : 2.63693
[1mStep[0m  [144/169], [94mLoss[0m : 2.12878
[1mStep[0m  [160/169], [94mLoss[0m : 2.34765

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.362, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.85201
[1mStep[0m  [16/169], [94mLoss[0m : 2.56924
[1mStep[0m  [32/169], [94mLoss[0m : 2.37850
[1mStep[0m  [48/169], [94mLoss[0m : 2.31420
[1mStep[0m  [64/169], [94mLoss[0m : 2.46868
[1mStep[0m  [80/169], [94mLoss[0m : 2.57770
[1mStep[0m  [96/169], [94mLoss[0m : 2.78287
[1mStep[0m  [112/169], [94mLoss[0m : 2.45028
[1mStep[0m  [128/169], [94mLoss[0m : 2.45290
[1mStep[0m  [144/169], [94mLoss[0m : 2.74131
[1mStep[0m  [160/169], [94mLoss[0m : 2.21982

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21378
[1mStep[0m  [16/169], [94mLoss[0m : 2.21664
[1mStep[0m  [32/169], [94mLoss[0m : 2.48687
[1mStep[0m  [48/169], [94mLoss[0m : 2.18785
[1mStep[0m  [64/169], [94mLoss[0m : 2.34340
[1mStep[0m  [80/169], [94mLoss[0m : 2.44921
[1mStep[0m  [96/169], [94mLoss[0m : 2.06996
[1mStep[0m  [112/169], [94mLoss[0m : 2.69564
[1mStep[0m  [128/169], [94mLoss[0m : 2.69176
[1mStep[0m  [144/169], [94mLoss[0m : 2.54828
[1mStep[0m  [160/169], [94mLoss[0m : 2.18230

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.357, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.84696
[1mStep[0m  [16/169], [94mLoss[0m : 2.70966
[1mStep[0m  [32/169], [94mLoss[0m : 2.49238
[1mStep[0m  [48/169], [94mLoss[0m : 2.65342
[1mStep[0m  [64/169], [94mLoss[0m : 2.20302
[1mStep[0m  [80/169], [94mLoss[0m : 2.59928
[1mStep[0m  [96/169], [94mLoss[0m : 2.49236
[1mStep[0m  [112/169], [94mLoss[0m : 2.58608
[1mStep[0m  [128/169], [94mLoss[0m : 2.25339
[1mStep[0m  [144/169], [94mLoss[0m : 2.40554
[1mStep[0m  [160/169], [94mLoss[0m : 2.17401

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.457, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.82882
[1mStep[0m  [16/169], [94mLoss[0m : 2.63156
[1mStep[0m  [32/169], [94mLoss[0m : 2.81127
[1mStep[0m  [48/169], [94mLoss[0m : 2.57382
[1mStep[0m  [64/169], [94mLoss[0m : 2.47981
[1mStep[0m  [80/169], [94mLoss[0m : 2.95025
[1mStep[0m  [96/169], [94mLoss[0m : 2.24722
[1mStep[0m  [112/169], [94mLoss[0m : 2.72256
[1mStep[0m  [128/169], [94mLoss[0m : 2.28449
[1mStep[0m  [144/169], [94mLoss[0m : 2.07876
[1mStep[0m  [160/169], [94mLoss[0m : 2.54891

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.461, [92mTest[0m: 2.337, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.52455
[1mStep[0m  [16/169], [94mLoss[0m : 2.26157
[1mStep[0m  [32/169], [94mLoss[0m : 2.26728
[1mStep[0m  [48/169], [94mLoss[0m : 2.64051
[1mStep[0m  [64/169], [94mLoss[0m : 3.04653
[1mStep[0m  [80/169], [94mLoss[0m : 2.90927
[1mStep[0m  [96/169], [94mLoss[0m : 2.58131
[1mStep[0m  [112/169], [94mLoss[0m : 2.31804
[1mStep[0m  [128/169], [94mLoss[0m : 2.65462
[1mStep[0m  [144/169], [94mLoss[0m : 2.22893
[1mStep[0m  [160/169], [94mLoss[0m : 2.07908

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62635
[1mStep[0m  [16/169], [94mLoss[0m : 2.32076
[1mStep[0m  [32/169], [94mLoss[0m : 2.77985
[1mStep[0m  [48/169], [94mLoss[0m : 1.76730
[1mStep[0m  [64/169], [94mLoss[0m : 2.08504
[1mStep[0m  [80/169], [94mLoss[0m : 1.98319
[1mStep[0m  [96/169], [94mLoss[0m : 2.67684
[1mStep[0m  [112/169], [94mLoss[0m : 2.50832
[1mStep[0m  [128/169], [94mLoss[0m : 2.64584
[1mStep[0m  [144/169], [94mLoss[0m : 2.49887
[1mStep[0m  [160/169], [94mLoss[0m : 2.38469

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.458, [92mTest[0m: 2.336, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27989
[1mStep[0m  [16/169], [94mLoss[0m : 2.20486
[1mStep[0m  [32/169], [94mLoss[0m : 2.60856
[1mStep[0m  [48/169], [94mLoss[0m : 2.44941
[1mStep[0m  [64/169], [94mLoss[0m : 2.09883
[1mStep[0m  [80/169], [94mLoss[0m : 2.66462
[1mStep[0m  [96/169], [94mLoss[0m : 2.57092
[1mStep[0m  [112/169], [94mLoss[0m : 2.19658
[1mStep[0m  [128/169], [94mLoss[0m : 2.42801
[1mStep[0m  [144/169], [94mLoss[0m : 2.18733
[1mStep[0m  [160/169], [94mLoss[0m : 2.43021

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.443, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.07956
[1mStep[0m  [16/169], [94mLoss[0m : 2.35702
[1mStep[0m  [32/169], [94mLoss[0m : 2.49148
[1mStep[0m  [48/169], [94mLoss[0m : 2.26839
[1mStep[0m  [64/169], [94mLoss[0m : 2.14812
[1mStep[0m  [80/169], [94mLoss[0m : 2.34004
[1mStep[0m  [96/169], [94mLoss[0m : 2.36089
[1mStep[0m  [112/169], [94mLoss[0m : 2.55176
[1mStep[0m  [128/169], [94mLoss[0m : 2.02356
[1mStep[0m  [144/169], [94mLoss[0m : 2.38003
[1mStep[0m  [160/169], [94mLoss[0m : 2.52737

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.453, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23135
[1mStep[0m  [16/169], [94mLoss[0m : 2.26044
[1mStep[0m  [32/169], [94mLoss[0m : 2.28102
[1mStep[0m  [48/169], [94mLoss[0m : 2.33972
[1mStep[0m  [64/169], [94mLoss[0m : 2.43502
[1mStep[0m  [80/169], [94mLoss[0m : 2.76201
[1mStep[0m  [96/169], [94mLoss[0m : 2.44275
[1mStep[0m  [112/169], [94mLoss[0m : 2.37013
[1mStep[0m  [128/169], [94mLoss[0m : 2.51495
[1mStep[0m  [144/169], [94mLoss[0m : 2.57065
[1mStep[0m  [160/169], [94mLoss[0m : 2.32094

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55613
[1mStep[0m  [16/169], [94mLoss[0m : 3.11336
[1mStep[0m  [32/169], [94mLoss[0m : 2.22476
[1mStep[0m  [48/169], [94mLoss[0m : 2.44385
[1mStep[0m  [64/169], [94mLoss[0m : 2.27213
[1mStep[0m  [80/169], [94mLoss[0m : 2.55320
[1mStep[0m  [96/169], [94mLoss[0m : 2.40631
[1mStep[0m  [112/169], [94mLoss[0m : 2.21279
[1mStep[0m  [128/169], [94mLoss[0m : 2.32427
[1mStep[0m  [144/169], [94mLoss[0m : 2.57891
[1mStep[0m  [160/169], [94mLoss[0m : 2.62237

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18315
[1mStep[0m  [16/169], [94mLoss[0m : 2.22346
[1mStep[0m  [32/169], [94mLoss[0m : 2.35899
[1mStep[0m  [48/169], [94mLoss[0m : 2.33138
[1mStep[0m  [64/169], [94mLoss[0m : 2.96629
[1mStep[0m  [80/169], [94mLoss[0m : 2.45777
[1mStep[0m  [96/169], [94mLoss[0m : 2.33737
[1mStep[0m  [112/169], [94mLoss[0m : 2.16414
[1mStep[0m  [128/169], [94mLoss[0m : 2.10000
[1mStep[0m  [144/169], [94mLoss[0m : 2.05283
[1mStep[0m  [160/169], [94mLoss[0m : 2.62532

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75347
[1mStep[0m  [16/169], [94mLoss[0m : 2.31680
[1mStep[0m  [32/169], [94mLoss[0m : 3.01376
[1mStep[0m  [48/169], [94mLoss[0m : 2.47382
[1mStep[0m  [64/169], [94mLoss[0m : 2.33736
[1mStep[0m  [80/169], [94mLoss[0m : 2.37905
[1mStep[0m  [96/169], [94mLoss[0m : 2.29335
[1mStep[0m  [112/169], [94mLoss[0m : 2.52238
[1mStep[0m  [128/169], [94mLoss[0m : 2.15772
[1mStep[0m  [144/169], [94mLoss[0m : 2.05395
[1mStep[0m  [160/169], [94mLoss[0m : 2.29479

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.432, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67390
[1mStep[0m  [16/169], [94mLoss[0m : 2.68017
[1mStep[0m  [32/169], [94mLoss[0m : 2.66514
[1mStep[0m  [48/169], [94mLoss[0m : 2.66978
[1mStep[0m  [64/169], [94mLoss[0m : 2.15323
[1mStep[0m  [80/169], [94mLoss[0m : 2.74439
[1mStep[0m  [96/169], [94mLoss[0m : 2.75459
[1mStep[0m  [112/169], [94mLoss[0m : 2.37287
[1mStep[0m  [128/169], [94mLoss[0m : 2.43553
[1mStep[0m  [144/169], [94mLoss[0m : 2.84395
[1mStep[0m  [160/169], [94mLoss[0m : 2.21567

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19352
[1mStep[0m  [16/169], [94mLoss[0m : 2.79485
[1mStep[0m  [32/169], [94mLoss[0m : 2.65972
[1mStep[0m  [48/169], [94mLoss[0m : 2.58920
[1mStep[0m  [64/169], [94mLoss[0m : 2.51896
[1mStep[0m  [80/169], [94mLoss[0m : 2.56697
[1mStep[0m  [96/169], [94mLoss[0m : 2.35937
[1mStep[0m  [112/169], [94mLoss[0m : 2.68922
[1mStep[0m  [128/169], [94mLoss[0m : 2.36183
[1mStep[0m  [144/169], [94mLoss[0m : 2.39942
[1mStep[0m  [160/169], [94mLoss[0m : 2.60628

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.440, [92mTest[0m: 2.327, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 3.01070
[1mStep[0m  [16/169], [94mLoss[0m : 2.42296
[1mStep[0m  [32/169], [94mLoss[0m : 2.37104
[1mStep[0m  [48/169], [94mLoss[0m : 2.39608
[1mStep[0m  [64/169], [94mLoss[0m : 2.22943
[1mStep[0m  [80/169], [94mLoss[0m : 2.50945
[1mStep[0m  [96/169], [94mLoss[0m : 2.61857
[1mStep[0m  [112/169], [94mLoss[0m : 2.62473
[1mStep[0m  [128/169], [94mLoss[0m : 2.61232
[1mStep[0m  [144/169], [94mLoss[0m : 2.17551
[1mStep[0m  [160/169], [94mLoss[0m : 2.22462

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34869
[1mStep[0m  [16/169], [94mLoss[0m : 2.88537
[1mStep[0m  [32/169], [94mLoss[0m : 2.02701
[1mStep[0m  [48/169], [94mLoss[0m : 2.62880
[1mStep[0m  [64/169], [94mLoss[0m : 2.31680
[1mStep[0m  [80/169], [94mLoss[0m : 2.13955
[1mStep[0m  [96/169], [94mLoss[0m : 2.20187
[1mStep[0m  [112/169], [94mLoss[0m : 2.57260
[1mStep[0m  [128/169], [94mLoss[0m : 1.91570
[1mStep[0m  [144/169], [94mLoss[0m : 2.45058
[1mStep[0m  [160/169], [94mLoss[0m : 2.68257

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39291
[1mStep[0m  [16/169], [94mLoss[0m : 2.34096
[1mStep[0m  [32/169], [94mLoss[0m : 2.27811
[1mStep[0m  [48/169], [94mLoss[0m : 2.46606
[1mStep[0m  [64/169], [94mLoss[0m : 2.41392
[1mStep[0m  [80/169], [94mLoss[0m : 2.07252
[1mStep[0m  [96/169], [94mLoss[0m : 2.58690
[1mStep[0m  [112/169], [94mLoss[0m : 2.78326
[1mStep[0m  [128/169], [94mLoss[0m : 2.28400
[1mStep[0m  [144/169], [94mLoss[0m : 2.66080
[1mStep[0m  [160/169], [94mLoss[0m : 2.18877

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.437, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50980
[1mStep[0m  [16/169], [94mLoss[0m : 2.61710
[1mStep[0m  [32/169], [94mLoss[0m : 2.18900
[1mStep[0m  [48/169], [94mLoss[0m : 2.31301
[1mStep[0m  [64/169], [94mLoss[0m : 2.17947
[1mStep[0m  [80/169], [94mLoss[0m : 2.60272
[1mStep[0m  [96/169], [94mLoss[0m : 2.47224
[1mStep[0m  [112/169], [94mLoss[0m : 2.43597
[1mStep[0m  [128/169], [94mLoss[0m : 2.64265
[1mStep[0m  [144/169], [94mLoss[0m : 2.49438
[1mStep[0m  [160/169], [94mLoss[0m : 2.08583

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.99182
[1mStep[0m  [16/169], [94mLoss[0m : 2.54295
[1mStep[0m  [32/169], [94mLoss[0m : 2.40203
[1mStep[0m  [48/169], [94mLoss[0m : 2.17250
[1mStep[0m  [64/169], [94mLoss[0m : 2.36686
[1mStep[0m  [80/169], [94mLoss[0m : 2.44324
[1mStep[0m  [96/169], [94mLoss[0m : 2.32254
[1mStep[0m  [112/169], [94mLoss[0m : 2.72840
[1mStep[0m  [128/169], [94mLoss[0m : 2.29663
[1mStep[0m  [144/169], [94mLoss[0m : 2.33415
[1mStep[0m  [160/169], [94mLoss[0m : 2.29150

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.344, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15032
[1mStep[0m  [16/169], [94mLoss[0m : 2.30882
[1mStep[0m  [32/169], [94mLoss[0m : 2.32661
[1mStep[0m  [48/169], [94mLoss[0m : 2.74004
[1mStep[0m  [64/169], [94mLoss[0m : 2.34511
[1mStep[0m  [80/169], [94mLoss[0m : 2.29192
[1mStep[0m  [96/169], [94mLoss[0m : 2.21575
[1mStep[0m  [112/169], [94mLoss[0m : 2.72073
[1mStep[0m  [128/169], [94mLoss[0m : 2.35797
[1mStep[0m  [144/169], [94mLoss[0m : 2.49883
[1mStep[0m  [160/169], [94mLoss[0m : 2.53126

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.428, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27592
[1mStep[0m  [16/169], [94mLoss[0m : 2.72237
[1mStep[0m  [32/169], [94mLoss[0m : 2.70367
[1mStep[0m  [48/169], [94mLoss[0m : 2.64823
[1mStep[0m  [64/169], [94mLoss[0m : 2.30683
[1mStep[0m  [80/169], [94mLoss[0m : 2.23041
[1mStep[0m  [96/169], [94mLoss[0m : 2.28117
[1mStep[0m  [112/169], [94mLoss[0m : 2.60156
[1mStep[0m  [128/169], [94mLoss[0m : 2.33644
[1mStep[0m  [144/169], [94mLoss[0m : 2.28804
[1mStep[0m  [160/169], [94mLoss[0m : 2.40422

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.331, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55745
[1mStep[0m  [16/169], [94mLoss[0m : 3.07999
[1mStep[0m  [32/169], [94mLoss[0m : 2.32143
[1mStep[0m  [48/169], [94mLoss[0m : 2.53571
[1mStep[0m  [64/169], [94mLoss[0m : 2.30944
[1mStep[0m  [80/169], [94mLoss[0m : 2.35011
[1mStep[0m  [96/169], [94mLoss[0m : 2.05198
[1mStep[0m  [112/169], [94mLoss[0m : 2.38444
[1mStep[0m  [128/169], [94mLoss[0m : 2.58800
[1mStep[0m  [144/169], [94mLoss[0m : 2.39268
[1mStep[0m  [160/169], [94mLoss[0m : 2.14351

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.433, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33724
[1mStep[0m  [16/169], [94mLoss[0m : 2.03705
[1mStep[0m  [32/169], [94mLoss[0m : 2.62518
[1mStep[0m  [48/169], [94mLoss[0m : 2.61597
[1mStep[0m  [64/169], [94mLoss[0m : 2.56167
[1mStep[0m  [80/169], [94mLoss[0m : 2.81631
[1mStep[0m  [96/169], [94mLoss[0m : 2.57577
[1mStep[0m  [112/169], [94mLoss[0m : 2.19602
[1mStep[0m  [128/169], [94mLoss[0m : 2.28900
[1mStep[0m  [144/169], [94mLoss[0m : 2.45571
[1mStep[0m  [160/169], [94mLoss[0m : 2.35651

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.431, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44864
[1mStep[0m  [16/169], [94mLoss[0m : 2.43901
[1mStep[0m  [32/169], [94mLoss[0m : 2.64497
[1mStep[0m  [48/169], [94mLoss[0m : 2.56756
[1mStep[0m  [64/169], [94mLoss[0m : 2.14433
[1mStep[0m  [80/169], [94mLoss[0m : 2.48557
[1mStep[0m  [96/169], [94mLoss[0m : 2.13098
[1mStep[0m  [112/169], [94mLoss[0m : 2.10855
[1mStep[0m  [128/169], [94mLoss[0m : 2.43794
[1mStep[0m  [144/169], [94mLoss[0m : 2.81327
[1mStep[0m  [160/169], [94mLoss[0m : 2.61948

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.426, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44844
[1mStep[0m  [16/169], [94mLoss[0m : 2.50126
[1mStep[0m  [32/169], [94mLoss[0m : 2.15405
[1mStep[0m  [48/169], [94mLoss[0m : 2.22435
[1mStep[0m  [64/169], [94mLoss[0m : 2.57971
[1mStep[0m  [80/169], [94mLoss[0m : 2.68779
[1mStep[0m  [96/169], [94mLoss[0m : 2.83057
[1mStep[0m  [112/169], [94mLoss[0m : 2.46191
[1mStep[0m  [128/169], [94mLoss[0m : 2.39133
[1mStep[0m  [144/169], [94mLoss[0m : 2.63824
[1mStep[0m  [160/169], [94mLoss[0m : 2.15152

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.351, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53160
[1mStep[0m  [16/169], [94mLoss[0m : 2.45256
[1mStep[0m  [32/169], [94mLoss[0m : 2.44985
[1mStep[0m  [48/169], [94mLoss[0m : 2.40463
[1mStep[0m  [64/169], [94mLoss[0m : 2.80445
[1mStep[0m  [80/169], [94mLoss[0m : 2.43068
[1mStep[0m  [96/169], [94mLoss[0m : 1.96413
[1mStep[0m  [112/169], [94mLoss[0m : 2.87408
[1mStep[0m  [128/169], [94mLoss[0m : 2.55983
[1mStep[0m  [144/169], [94mLoss[0m : 2.33048
[1mStep[0m  [160/169], [94mLoss[0m : 2.03639

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.434, [92mTest[0m: 2.324, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.317
====================================

Phase 1 - Evaluation MAE:  2.31670121209962
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.4, inplace=False)
      )
      (1): Sequential(
        (out_linear): Linear(in_features=250, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.28049
[1mStep[0m  [16/169], [94mLoss[0m : 2.38569
[1mStep[0m  [32/169], [94mLoss[0m : 2.36592
[1mStep[0m  [48/169], [94mLoss[0m : 2.15432
[1mStep[0m  [64/169], [94mLoss[0m : 2.74218
[1mStep[0m  [80/169], [94mLoss[0m : 3.03434
[1mStep[0m  [96/169], [94mLoss[0m : 2.26910
[1mStep[0m  [112/169], [94mLoss[0m : 2.42224
[1mStep[0m  [128/169], [94mLoss[0m : 2.31233
[1mStep[0m  [144/169], [94mLoss[0m : 2.40080
[1mStep[0m  [160/169], [94mLoss[0m : 2.61922

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.456, [92mTest[0m: 2.317, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21098
[1mStep[0m  [16/169], [94mLoss[0m : 2.15336
[1mStep[0m  [32/169], [94mLoss[0m : 2.18703
[1mStep[0m  [48/169], [94mLoss[0m : 1.91845
[1mStep[0m  [64/169], [94mLoss[0m : 2.27189
[1mStep[0m  [80/169], [94mLoss[0m : 2.56191
[1mStep[0m  [96/169], [94mLoss[0m : 2.40627
[1mStep[0m  [112/169], [94mLoss[0m : 2.57895
[1mStep[0m  [128/169], [94mLoss[0m : 1.98712
[1mStep[0m  [144/169], [94mLoss[0m : 2.54027
[1mStep[0m  [160/169], [94mLoss[0m : 2.63448

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.379, [92mTest[0m: 2.594, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28515
[1mStep[0m  [16/169], [94mLoss[0m : 2.22027
[1mStep[0m  [32/169], [94mLoss[0m : 2.41962
[1mStep[0m  [48/169], [94mLoss[0m : 2.48172
[1mStep[0m  [64/169], [94mLoss[0m : 2.60083
[1mStep[0m  [80/169], [94mLoss[0m : 2.16260
[1mStep[0m  [96/169], [94mLoss[0m : 2.47813
[1mStep[0m  [112/169], [94mLoss[0m : 2.16943
[1mStep[0m  [128/169], [94mLoss[0m : 1.96662
[1mStep[0m  [144/169], [94mLoss[0m : 2.50282
[1mStep[0m  [160/169], [94mLoss[0m : 2.13620

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.323, [92mTest[0m: 2.415, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.37538
[1mStep[0m  [16/169], [94mLoss[0m : 2.02055
[1mStep[0m  [32/169], [94mLoss[0m : 2.42072
[1mStep[0m  [48/169], [94mLoss[0m : 2.62045
[1mStep[0m  [64/169], [94mLoss[0m : 2.10423
[1mStep[0m  [80/169], [94mLoss[0m : 2.52318
[1mStep[0m  [96/169], [94mLoss[0m : 2.23960
[1mStep[0m  [112/169], [94mLoss[0m : 2.11772
[1mStep[0m  [128/169], [94mLoss[0m : 2.00034
[1mStep[0m  [144/169], [94mLoss[0m : 2.41155
[1mStep[0m  [160/169], [94mLoss[0m : 2.21294

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.269, [92mTest[0m: 2.455, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.27590
[1mStep[0m  [16/169], [94mLoss[0m : 2.13872
[1mStep[0m  [32/169], [94mLoss[0m : 1.93941
[1mStep[0m  [48/169], [94mLoss[0m : 2.44970
[1mStep[0m  [64/169], [94mLoss[0m : 2.21882
[1mStep[0m  [80/169], [94mLoss[0m : 2.18074
[1mStep[0m  [96/169], [94mLoss[0m : 2.41717
[1mStep[0m  [112/169], [94mLoss[0m : 2.19274
[1mStep[0m  [128/169], [94mLoss[0m : 2.70034
[1mStep[0m  [144/169], [94mLoss[0m : 2.01065
[1mStep[0m  [160/169], [94mLoss[0m : 2.44845

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.215, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34139
[1mStep[0m  [16/169], [94mLoss[0m : 2.46822
[1mStep[0m  [32/169], [94mLoss[0m : 2.08899
[1mStep[0m  [48/169], [94mLoss[0m : 2.33001
[1mStep[0m  [64/169], [94mLoss[0m : 2.73407
[1mStep[0m  [80/169], [94mLoss[0m : 1.96746
[1mStep[0m  [96/169], [94mLoss[0m : 2.04243
[1mStep[0m  [112/169], [94mLoss[0m : 2.01534
[1mStep[0m  [128/169], [94mLoss[0m : 2.00687
[1mStep[0m  [144/169], [94mLoss[0m : 2.28683
[1mStep[0m  [160/169], [94mLoss[0m : 2.25044

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.156, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.86523
[1mStep[0m  [16/169], [94mLoss[0m : 1.89558
[1mStep[0m  [32/169], [94mLoss[0m : 1.97929
[1mStep[0m  [48/169], [94mLoss[0m : 2.35488
[1mStep[0m  [64/169], [94mLoss[0m : 2.21167
[1mStep[0m  [80/169], [94mLoss[0m : 2.16961
[1mStep[0m  [96/169], [94mLoss[0m : 2.21576
[1mStep[0m  [112/169], [94mLoss[0m : 1.84397
[1mStep[0m  [128/169], [94mLoss[0m : 1.86361
[1mStep[0m  [144/169], [94mLoss[0m : 2.04318
[1mStep[0m  [160/169], [94mLoss[0m : 2.10931

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.108, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90227
[1mStep[0m  [16/169], [94mLoss[0m : 2.28489
[1mStep[0m  [32/169], [94mLoss[0m : 2.44939
[1mStep[0m  [48/169], [94mLoss[0m : 2.35674
[1mStep[0m  [64/169], [94mLoss[0m : 2.16390
[1mStep[0m  [80/169], [94mLoss[0m : 1.78934
[1mStep[0m  [96/169], [94mLoss[0m : 2.06831
[1mStep[0m  [112/169], [94mLoss[0m : 2.21223
[1mStep[0m  [128/169], [94mLoss[0m : 1.90088
[1mStep[0m  [144/169], [94mLoss[0m : 2.47713
[1mStep[0m  [160/169], [94mLoss[0m : 2.42974

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.036, [92mTest[0m: 2.441, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21961
[1mStep[0m  [16/169], [94mLoss[0m : 2.24506
[1mStep[0m  [32/169], [94mLoss[0m : 2.56239
[1mStep[0m  [48/169], [94mLoss[0m : 2.15987
[1mStep[0m  [64/169], [94mLoss[0m : 2.10746
[1mStep[0m  [80/169], [94mLoss[0m : 1.98200
[1mStep[0m  [96/169], [94mLoss[0m : 1.96203
[1mStep[0m  [112/169], [94mLoss[0m : 1.75399
[1mStep[0m  [128/169], [94mLoss[0m : 2.32111
[1mStep[0m  [144/169], [94mLoss[0m : 2.38442
[1mStep[0m  [160/169], [94mLoss[0m : 1.95436

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.997, [92mTest[0m: 2.404, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44471
[1mStep[0m  [16/169], [94mLoss[0m : 2.04129
[1mStep[0m  [32/169], [94mLoss[0m : 2.01369
[1mStep[0m  [48/169], [94mLoss[0m : 1.95734
[1mStep[0m  [64/169], [94mLoss[0m : 1.81567
[1mStep[0m  [80/169], [94mLoss[0m : 2.46877
[1mStep[0m  [96/169], [94mLoss[0m : 2.22641
[1mStep[0m  [112/169], [94mLoss[0m : 1.65982
[1mStep[0m  [128/169], [94mLoss[0m : 2.43475
[1mStep[0m  [144/169], [94mLoss[0m : 2.08255
[1mStep[0m  [160/169], [94mLoss[0m : 2.13608

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.956, [92mTest[0m: 2.503, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.74091
[1mStep[0m  [16/169], [94mLoss[0m : 1.90798
[1mStep[0m  [32/169], [94mLoss[0m : 1.92408
[1mStep[0m  [48/169], [94mLoss[0m : 1.97295
[1mStep[0m  [64/169], [94mLoss[0m : 2.38927
[1mStep[0m  [80/169], [94mLoss[0m : 1.70769
[1mStep[0m  [96/169], [94mLoss[0m : 2.00236
[1mStep[0m  [112/169], [94mLoss[0m : 1.74155
[1mStep[0m  [128/169], [94mLoss[0m : 2.08419
[1mStep[0m  [144/169], [94mLoss[0m : 1.60949
[1mStep[0m  [160/169], [94mLoss[0m : 2.03219

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.892, [92mTest[0m: 2.423, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.89600
[1mStep[0m  [16/169], [94mLoss[0m : 1.40261
[1mStep[0m  [32/169], [94mLoss[0m : 1.83503
[1mStep[0m  [48/169], [94mLoss[0m : 1.68151
[1mStep[0m  [64/169], [94mLoss[0m : 1.86399
[1mStep[0m  [80/169], [94mLoss[0m : 1.62358
[1mStep[0m  [96/169], [94mLoss[0m : 1.58259
[1mStep[0m  [112/169], [94mLoss[0m : 1.72737
[1mStep[0m  [128/169], [94mLoss[0m : 2.15714
[1mStep[0m  [144/169], [94mLoss[0m : 2.04757
[1mStep[0m  [160/169], [94mLoss[0m : 1.84833

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94180
[1mStep[0m  [16/169], [94mLoss[0m : 2.02144
[1mStep[0m  [32/169], [94mLoss[0m : 1.36080
[1mStep[0m  [48/169], [94mLoss[0m : 1.43873
[1mStep[0m  [64/169], [94mLoss[0m : 1.73639
[1mStep[0m  [80/169], [94mLoss[0m : 1.93640
[1mStep[0m  [96/169], [94mLoss[0m : 2.02047
[1mStep[0m  [112/169], [94mLoss[0m : 1.69351
[1mStep[0m  [128/169], [94mLoss[0m : 1.49121
[1mStep[0m  [144/169], [94mLoss[0m : 1.81241
[1mStep[0m  [160/169], [94mLoss[0m : 1.80455

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.823, [92mTest[0m: 2.494, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.69368
[1mStep[0m  [16/169], [94mLoss[0m : 1.53680
[1mStep[0m  [32/169], [94mLoss[0m : 1.51011
[1mStep[0m  [48/169], [94mLoss[0m : 1.61002
[1mStep[0m  [64/169], [94mLoss[0m : 1.65048
[1mStep[0m  [80/169], [94mLoss[0m : 1.51330
[1mStep[0m  [96/169], [94mLoss[0m : 2.20992
[1mStep[0m  [112/169], [94mLoss[0m : 1.55845
[1mStep[0m  [128/169], [94mLoss[0m : 1.94724
[1mStep[0m  [144/169], [94mLoss[0m : 1.45083
[1mStep[0m  [160/169], [94mLoss[0m : 1.99328

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.775, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.51501
[1mStep[0m  [16/169], [94mLoss[0m : 1.54543
[1mStep[0m  [32/169], [94mLoss[0m : 1.62509
[1mStep[0m  [48/169], [94mLoss[0m : 1.84469
[1mStep[0m  [64/169], [94mLoss[0m : 1.62126
[1mStep[0m  [80/169], [94mLoss[0m : 1.30844
[1mStep[0m  [96/169], [94mLoss[0m : 1.88911
[1mStep[0m  [112/169], [94mLoss[0m : 1.40550
[1mStep[0m  [128/169], [94mLoss[0m : 1.68498
[1mStep[0m  [144/169], [94mLoss[0m : 1.70895
[1mStep[0m  [160/169], [94mLoss[0m : 1.80843

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.741, [92mTest[0m: 2.446, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.64738
[1mStep[0m  [16/169], [94mLoss[0m : 1.75639
[1mStep[0m  [32/169], [94mLoss[0m : 1.43135
[1mStep[0m  [48/169], [94mLoss[0m : 1.55139
[1mStep[0m  [64/169], [94mLoss[0m : 1.76399
[1mStep[0m  [80/169], [94mLoss[0m : 1.68496
[1mStep[0m  [96/169], [94mLoss[0m : 1.80034
[1mStep[0m  [112/169], [94mLoss[0m : 1.67162
[1mStep[0m  [128/169], [94mLoss[0m : 2.02934
[1mStep[0m  [144/169], [94mLoss[0m : 1.69002
[1mStep[0m  [160/169], [94mLoss[0m : 1.75294

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.703, [92mTest[0m: 2.463, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50561
[1mStep[0m  [16/169], [94mLoss[0m : 1.49474
[1mStep[0m  [32/169], [94mLoss[0m : 1.94662
[1mStep[0m  [48/169], [94mLoss[0m : 1.59729
[1mStep[0m  [64/169], [94mLoss[0m : 1.60148
[1mStep[0m  [80/169], [94mLoss[0m : 1.69862
[1mStep[0m  [96/169], [94mLoss[0m : 1.78247
[1mStep[0m  [112/169], [94mLoss[0m : 1.93457
[1mStep[0m  [128/169], [94mLoss[0m : 1.66496
[1mStep[0m  [144/169], [94mLoss[0m : 1.51674
[1mStep[0m  [160/169], [94mLoss[0m : 1.56378

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.684, [92mTest[0m: 2.540, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.57011
[1mStep[0m  [16/169], [94mLoss[0m : 2.15946
[1mStep[0m  [32/169], [94mLoss[0m : 1.59436
[1mStep[0m  [48/169], [94mLoss[0m : 1.58284
[1mStep[0m  [64/169], [94mLoss[0m : 1.83111
[1mStep[0m  [80/169], [94mLoss[0m : 1.56748
[1mStep[0m  [96/169], [94mLoss[0m : 2.14641
[1mStep[0m  [112/169], [94mLoss[0m : 1.56948
[1mStep[0m  [128/169], [94mLoss[0m : 1.38743
[1mStep[0m  [144/169], [94mLoss[0m : 1.80277
[1mStep[0m  [160/169], [94mLoss[0m : 1.73493

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.632, [92mTest[0m: 2.470, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.32087
[1mStep[0m  [16/169], [94mLoss[0m : 1.45054
[1mStep[0m  [32/169], [94mLoss[0m : 1.81701
[1mStep[0m  [48/169], [94mLoss[0m : 1.74121
[1mStep[0m  [64/169], [94mLoss[0m : 1.54491
[1mStep[0m  [80/169], [94mLoss[0m : 1.79391
[1mStep[0m  [96/169], [94mLoss[0m : 1.62961
[1mStep[0m  [112/169], [94mLoss[0m : 1.40476
[1mStep[0m  [128/169], [94mLoss[0m : 1.52740
[1mStep[0m  [144/169], [94mLoss[0m : 1.74034
[1mStep[0m  [160/169], [94mLoss[0m : 1.29561

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.611, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.31080
[1mStep[0m  [16/169], [94mLoss[0m : 1.55745
[1mStep[0m  [32/169], [94mLoss[0m : 1.69931
[1mStep[0m  [48/169], [94mLoss[0m : 1.59975
[1mStep[0m  [64/169], [94mLoss[0m : 1.66790
[1mStep[0m  [80/169], [94mLoss[0m : 1.49869
[1mStep[0m  [96/169], [94mLoss[0m : 1.67850
[1mStep[0m  [112/169], [94mLoss[0m : 1.82947
[1mStep[0m  [128/169], [94mLoss[0m : 1.42556
[1mStep[0m  [144/169], [94mLoss[0m : 1.23822
[1mStep[0m  [160/169], [94mLoss[0m : 1.76006

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.602, [92mTest[0m: 2.477, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.90720
[1mStep[0m  [16/169], [94mLoss[0m : 1.67172
[1mStep[0m  [32/169], [94mLoss[0m : 1.54337
[1mStep[0m  [48/169], [94mLoss[0m : 1.36388
[1mStep[0m  [64/169], [94mLoss[0m : 1.44420
[1mStep[0m  [80/169], [94mLoss[0m : 1.92475
[1mStep[0m  [96/169], [94mLoss[0m : 1.33118
[1mStep[0m  [112/169], [94mLoss[0m : 1.51688
[1mStep[0m  [128/169], [94mLoss[0m : 1.50923
[1mStep[0m  [144/169], [94mLoss[0m : 1.74310
[1mStep[0m  [160/169], [94mLoss[0m : 1.23806

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.556, [92mTest[0m: 2.596, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.28188
[1mStep[0m  [16/169], [94mLoss[0m : 1.56621
[1mStep[0m  [32/169], [94mLoss[0m : 1.55910
[1mStep[0m  [48/169], [94mLoss[0m : 1.49847
[1mStep[0m  [64/169], [94mLoss[0m : 2.19393
[1mStep[0m  [80/169], [94mLoss[0m : 1.64188
[1mStep[0m  [96/169], [94mLoss[0m : 1.59773
[1mStep[0m  [112/169], [94mLoss[0m : 1.58614
[1mStep[0m  [128/169], [94mLoss[0m : 1.60179
[1mStep[0m  [144/169], [94mLoss[0m : 1.75662
[1mStep[0m  [160/169], [94mLoss[0m : 1.57030

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.467, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.23609
[1mStep[0m  [16/169], [94mLoss[0m : 1.83339
[1mStep[0m  [32/169], [94mLoss[0m : 1.60775
[1mStep[0m  [48/169], [94mLoss[0m : 1.73034
[1mStep[0m  [64/169], [94mLoss[0m : 1.33083
[1mStep[0m  [80/169], [94mLoss[0m : 1.58065
[1mStep[0m  [96/169], [94mLoss[0m : 1.37467
[1mStep[0m  [112/169], [94mLoss[0m : 1.88744
[1mStep[0m  [128/169], [94mLoss[0m : 1.76478
[1mStep[0m  [144/169], [94mLoss[0m : 1.55284
[1mStep[0m  [160/169], [94mLoss[0m : 1.73331

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.514, [92mTest[0m: 2.505, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.42389
[1mStep[0m  [16/169], [94mLoss[0m : 1.25525
[1mStep[0m  [32/169], [94mLoss[0m : 1.35442
[1mStep[0m  [48/169], [94mLoss[0m : 1.22936
[1mStep[0m  [64/169], [94mLoss[0m : 1.64866
[1mStep[0m  [80/169], [94mLoss[0m : 1.68791
[1mStep[0m  [96/169], [94mLoss[0m : 1.66006
[1mStep[0m  [112/169], [94mLoss[0m : 1.50455
[1mStep[0m  [128/169], [94mLoss[0m : 1.17632
[1mStep[0m  [144/169], [94mLoss[0m : 1.44139
[1mStep[0m  [160/169], [94mLoss[0m : 1.38209

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.484, [92mTest[0m: 2.530, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.23787
[1mStep[0m  [16/169], [94mLoss[0m : 1.46043
[1mStep[0m  [32/169], [94mLoss[0m : 1.60336
[1mStep[0m  [48/169], [94mLoss[0m : 1.39682
[1mStep[0m  [64/169], [94mLoss[0m : 1.13969
[1mStep[0m  [80/169], [94mLoss[0m : 1.66506
[1mStep[0m  [96/169], [94mLoss[0m : 1.38362
[1mStep[0m  [112/169], [94mLoss[0m : 1.37370
[1mStep[0m  [128/169], [94mLoss[0m : 1.17087
[1mStep[0m  [144/169], [94mLoss[0m : 1.52294
[1mStep[0m  [160/169], [94mLoss[0m : 1.60220

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.449, [92mTest[0m: 2.479, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.53677
[1mStep[0m  [16/169], [94mLoss[0m : 1.36946
[1mStep[0m  [32/169], [94mLoss[0m : 1.73319
[1mStep[0m  [48/169], [94mLoss[0m : 1.48869
[1mStep[0m  [64/169], [94mLoss[0m : 1.29342
[1mStep[0m  [80/169], [94mLoss[0m : 1.70063
[1mStep[0m  [96/169], [94mLoss[0m : 1.58244
[1mStep[0m  [112/169], [94mLoss[0m : 1.65938
[1mStep[0m  [128/169], [94mLoss[0m : 1.76951
[1mStep[0m  [144/169], [94mLoss[0m : 1.57869
[1mStep[0m  [160/169], [94mLoss[0m : 1.14855

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.442, [92mTest[0m: 2.519, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.55029
[1mStep[0m  [16/169], [94mLoss[0m : 1.39049
[1mStep[0m  [32/169], [94mLoss[0m : 1.02047
[1mStep[0m  [48/169], [94mLoss[0m : 1.42774
[1mStep[0m  [64/169], [94mLoss[0m : 1.53245
[1mStep[0m  [80/169], [94mLoss[0m : 1.33845
[1mStep[0m  [96/169], [94mLoss[0m : 1.30586
[1mStep[0m  [112/169], [94mLoss[0m : 1.22028
[1mStep[0m  [128/169], [94mLoss[0m : 1.47041
[1mStep[0m  [144/169], [94mLoss[0m : 1.22981
[1mStep[0m  [160/169], [94mLoss[0m : 1.26201

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.420, [92mTest[0m: 2.504, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 26 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.595
====================================

Phase 2 - Evaluation MAE:  2.5950852079050883
MAE score P1      2.316701
MAE score P2      2.595085
loss                  1.42
learning_rate     0.007525
batch_size              64
hidden_sizes         [250]
epochs                  30
activation         sigmoid
optimizer              sgd
early stopping        True
dropout                0.4
momentum               0.1
weight_decay        0.0001
Name: 11, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 10.89914
[1mStep[0m  [16/169], [94mLoss[0m : 5.75845
[1mStep[0m  [32/169], [94mLoss[0m : 3.25876
[1mStep[0m  [48/169], [94mLoss[0m : 3.15858
[1mStep[0m  [64/169], [94mLoss[0m : 3.43840
[1mStep[0m  [80/169], [94mLoss[0m : 2.57700
[1mStep[0m  [96/169], [94mLoss[0m : 2.48954
[1mStep[0m  [112/169], [94mLoss[0m : 2.64654
[1mStep[0m  [128/169], [94mLoss[0m : 3.03620
[1mStep[0m  [144/169], [94mLoss[0m : 2.92588
[1mStep[0m  [160/169], [94mLoss[0m : 2.57166

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.503, [92mTest[0m: 10.982, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.67305
[1mStep[0m  [16/169], [94mLoss[0m : 2.74721
[1mStep[0m  [32/169], [94mLoss[0m : 2.39535
[1mStep[0m  [48/169], [94mLoss[0m : 2.35400
[1mStep[0m  [64/169], [94mLoss[0m : 2.82676
[1mStep[0m  [80/169], [94mLoss[0m : 2.87000
[1mStep[0m  [96/169], [94mLoss[0m : 2.59078
[1mStep[0m  [112/169], [94mLoss[0m : 2.95416
[1mStep[0m  [128/169], [94mLoss[0m : 2.89544
[1mStep[0m  [144/169], [94mLoss[0m : 2.71064
[1mStep[0m  [160/169], [94mLoss[0m : 2.39832

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.664, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64570
[1mStep[0m  [16/169], [94mLoss[0m : 2.54522
[1mStep[0m  [32/169], [94mLoss[0m : 2.70743
[1mStep[0m  [48/169], [94mLoss[0m : 2.32452
[1mStep[0m  [64/169], [94mLoss[0m : 2.35076
[1mStep[0m  [80/169], [94mLoss[0m : 2.54818
[1mStep[0m  [96/169], [94mLoss[0m : 2.74275
[1mStep[0m  [112/169], [94mLoss[0m : 2.47444
[1mStep[0m  [128/169], [94mLoss[0m : 2.85117
[1mStep[0m  [144/169], [94mLoss[0m : 2.23800
[1mStep[0m  [160/169], [94mLoss[0m : 2.62928

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.603, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76647
[1mStep[0m  [16/169], [94mLoss[0m : 2.38238
[1mStep[0m  [32/169], [94mLoss[0m : 2.45994
[1mStep[0m  [48/169], [94mLoss[0m : 2.28616
[1mStep[0m  [64/169], [94mLoss[0m : 2.89113
[1mStep[0m  [80/169], [94mLoss[0m : 2.20831
[1mStep[0m  [96/169], [94mLoss[0m : 2.87659
[1mStep[0m  [112/169], [94mLoss[0m : 2.84902
[1mStep[0m  [128/169], [94mLoss[0m : 2.60270
[1mStep[0m  [144/169], [94mLoss[0m : 2.54568
[1mStep[0m  [160/169], [94mLoss[0m : 2.18271

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.536, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.14949
[1mStep[0m  [16/169], [94mLoss[0m : 2.63777
[1mStep[0m  [32/169], [94mLoss[0m : 2.43023
[1mStep[0m  [48/169], [94mLoss[0m : 2.34883
[1mStep[0m  [64/169], [94mLoss[0m : 2.18556
[1mStep[0m  [80/169], [94mLoss[0m : 2.45024
[1mStep[0m  [96/169], [94mLoss[0m : 2.41425
[1mStep[0m  [112/169], [94mLoss[0m : 2.78248
[1mStep[0m  [128/169], [94mLoss[0m : 2.26274
[1mStep[0m  [144/169], [94mLoss[0m : 2.38836
[1mStep[0m  [160/169], [94mLoss[0m : 2.69482

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.512, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.99011
[1mStep[0m  [16/169], [94mLoss[0m : 2.75505
[1mStep[0m  [32/169], [94mLoss[0m : 2.33341
[1mStep[0m  [48/169], [94mLoss[0m : 2.32737
[1mStep[0m  [64/169], [94mLoss[0m : 2.62530
[1mStep[0m  [80/169], [94mLoss[0m : 2.14544
[1mStep[0m  [96/169], [94mLoss[0m : 2.92534
[1mStep[0m  [112/169], [94mLoss[0m : 2.50584
[1mStep[0m  [128/169], [94mLoss[0m : 2.64576
[1mStep[0m  [144/169], [94mLoss[0m : 2.26858
[1mStep[0m  [160/169], [94mLoss[0m : 2.80017

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.473, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53292
[1mStep[0m  [16/169], [94mLoss[0m : 2.44280
[1mStep[0m  [32/169], [94mLoss[0m : 2.36035
[1mStep[0m  [48/169], [94mLoss[0m : 2.51955
[1mStep[0m  [64/169], [94mLoss[0m : 1.86116
[1mStep[0m  [80/169], [94mLoss[0m : 2.22349
[1mStep[0m  [96/169], [94mLoss[0m : 2.21739
[1mStep[0m  [112/169], [94mLoss[0m : 2.78771
[1mStep[0m  [128/169], [94mLoss[0m : 2.28824
[1mStep[0m  [144/169], [94mLoss[0m : 2.50209
[1mStep[0m  [160/169], [94mLoss[0m : 2.71742

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.464, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.75714
[1mStep[0m  [16/169], [94mLoss[0m : 2.59400
[1mStep[0m  [32/169], [94mLoss[0m : 2.16945
[1mStep[0m  [48/169], [94mLoss[0m : 2.23394
[1mStep[0m  [64/169], [94mLoss[0m : 2.36239
[1mStep[0m  [80/169], [94mLoss[0m : 2.44631
[1mStep[0m  [96/169], [94mLoss[0m : 2.80787
[1mStep[0m  [112/169], [94mLoss[0m : 2.56436
[1mStep[0m  [128/169], [94mLoss[0m : 2.34943
[1mStep[0m  [144/169], [94mLoss[0m : 2.98357
[1mStep[0m  [160/169], [94mLoss[0m : 2.28567

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.438, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.39667
[1mStep[0m  [16/169], [94mLoss[0m : 2.37700
[1mStep[0m  [32/169], [94mLoss[0m : 2.36445
[1mStep[0m  [48/169], [94mLoss[0m : 2.85302
[1mStep[0m  [64/169], [94mLoss[0m : 2.68067
[1mStep[0m  [80/169], [94mLoss[0m : 2.89472
[1mStep[0m  [96/169], [94mLoss[0m : 2.24350
[1mStep[0m  [112/169], [94mLoss[0m : 2.61633
[1mStep[0m  [128/169], [94mLoss[0m : 2.05494
[1mStep[0m  [144/169], [94mLoss[0m : 2.67656
[1mStep[0m  [160/169], [94mLoss[0m : 2.61092

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.320, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.46644
[1mStep[0m  [16/169], [94mLoss[0m : 2.79603
[1mStep[0m  [32/169], [94mLoss[0m : 2.46509
[1mStep[0m  [48/169], [94mLoss[0m : 2.38661
[1mStep[0m  [64/169], [94mLoss[0m : 1.95113
[1mStep[0m  [80/169], [94mLoss[0m : 2.51950
[1mStep[0m  [96/169], [94mLoss[0m : 2.20597
[1mStep[0m  [112/169], [94mLoss[0m : 2.61552
[1mStep[0m  [128/169], [94mLoss[0m : 2.66642
[1mStep[0m  [144/169], [94mLoss[0m : 2.09921
[1mStep[0m  [160/169], [94mLoss[0m : 2.89603

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.342, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.59479
[1mStep[0m  [16/169], [94mLoss[0m : 2.25655
[1mStep[0m  [32/169], [94mLoss[0m : 2.86397
[1mStep[0m  [48/169], [94mLoss[0m : 2.00419
[1mStep[0m  [64/169], [94mLoss[0m : 2.41306
[1mStep[0m  [80/169], [94mLoss[0m : 2.40683
[1mStep[0m  [96/169], [94mLoss[0m : 2.58500
[1mStep[0m  [112/169], [94mLoss[0m : 2.36922
[1mStep[0m  [128/169], [94mLoss[0m : 2.86153
[1mStep[0m  [144/169], [94mLoss[0m : 2.18056
[1mStep[0m  [160/169], [94mLoss[0m : 2.39241

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45410
[1mStep[0m  [16/169], [94mLoss[0m : 1.95253
[1mStep[0m  [32/169], [94mLoss[0m : 2.24025
[1mStep[0m  [48/169], [94mLoss[0m : 1.94907
[1mStep[0m  [64/169], [94mLoss[0m : 2.34755
[1mStep[0m  [80/169], [94mLoss[0m : 2.35153
[1mStep[0m  [96/169], [94mLoss[0m : 2.60708
[1mStep[0m  [112/169], [94mLoss[0m : 2.21587
[1mStep[0m  [128/169], [94mLoss[0m : 2.30871
[1mStep[0m  [144/169], [94mLoss[0m : 2.42305
[1mStep[0m  [160/169], [94mLoss[0m : 2.32192

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.375, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08607
[1mStep[0m  [16/169], [94mLoss[0m : 2.38336
[1mStep[0m  [32/169], [94mLoss[0m : 2.66749
[1mStep[0m  [48/169], [94mLoss[0m : 2.40375
[1mStep[0m  [64/169], [94mLoss[0m : 2.32754
[1mStep[0m  [80/169], [94mLoss[0m : 2.69577
[1mStep[0m  [96/169], [94mLoss[0m : 2.21705
[1mStep[0m  [112/169], [94mLoss[0m : 2.48030
[1mStep[0m  [128/169], [94mLoss[0m : 2.47489
[1mStep[0m  [144/169], [94mLoss[0m : 2.18291
[1mStep[0m  [160/169], [94mLoss[0m : 2.71803

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.384, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.96177
[1mStep[0m  [16/169], [94mLoss[0m : 2.29303
[1mStep[0m  [32/169], [94mLoss[0m : 2.18944
[1mStep[0m  [48/169], [94mLoss[0m : 2.52115
[1mStep[0m  [64/169], [94mLoss[0m : 2.55176
[1mStep[0m  [80/169], [94mLoss[0m : 2.54386
[1mStep[0m  [96/169], [94mLoss[0m : 2.66837
[1mStep[0m  [112/169], [94mLoss[0m : 2.16440
[1mStep[0m  [128/169], [94mLoss[0m : 2.15570
[1mStep[0m  [144/169], [94mLoss[0m : 2.44067
[1mStep[0m  [160/169], [94mLoss[0m : 1.95230

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.354, [92mTest[0m: 2.310, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65188
[1mStep[0m  [16/169], [94mLoss[0m : 2.50488
[1mStep[0m  [32/169], [94mLoss[0m : 2.29863
[1mStep[0m  [48/169], [94mLoss[0m : 2.36104
[1mStep[0m  [64/169], [94mLoss[0m : 2.56231
[1mStep[0m  [80/169], [94mLoss[0m : 2.19262
[1mStep[0m  [96/169], [94mLoss[0m : 2.38585
[1mStep[0m  [112/169], [94mLoss[0m : 2.15635
[1mStep[0m  [128/169], [94mLoss[0m : 2.46153
[1mStep[0m  [144/169], [94mLoss[0m : 2.08789
[1mStep[0m  [160/169], [94mLoss[0m : 2.02385

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.359, [92mTest[0m: 2.304, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.10151
[1mStep[0m  [16/169], [94mLoss[0m : 2.25467
[1mStep[0m  [32/169], [94mLoss[0m : 2.51288
[1mStep[0m  [48/169], [94mLoss[0m : 2.51056
[1mStep[0m  [64/169], [94mLoss[0m : 2.49477
[1mStep[0m  [80/169], [94mLoss[0m : 2.23546
[1mStep[0m  [96/169], [94mLoss[0m : 2.45431
[1mStep[0m  [112/169], [94mLoss[0m : 2.25291
[1mStep[0m  [128/169], [94mLoss[0m : 2.27953
[1mStep[0m  [144/169], [94mLoss[0m : 2.35873
[1mStep[0m  [160/169], [94mLoss[0m : 2.51937

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.29907
[1mStep[0m  [16/169], [94mLoss[0m : 2.20109
[1mStep[0m  [32/169], [94mLoss[0m : 2.67869
[1mStep[0m  [48/169], [94mLoss[0m : 2.49444
[1mStep[0m  [64/169], [94mLoss[0m : 2.12501
[1mStep[0m  [80/169], [94mLoss[0m : 2.51608
[1mStep[0m  [96/169], [94mLoss[0m : 2.70444
[1mStep[0m  [112/169], [94mLoss[0m : 2.05967
[1mStep[0m  [128/169], [94mLoss[0m : 2.36536
[1mStep[0m  [144/169], [94mLoss[0m : 1.98654
[1mStep[0m  [160/169], [94mLoss[0m : 2.40054

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.329, [92mTest[0m: 2.309, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51834
[1mStep[0m  [16/169], [94mLoss[0m : 2.43650
[1mStep[0m  [32/169], [94mLoss[0m : 2.04109
[1mStep[0m  [48/169], [94mLoss[0m : 1.87405
[1mStep[0m  [64/169], [94mLoss[0m : 2.25250
[1mStep[0m  [80/169], [94mLoss[0m : 2.08523
[1mStep[0m  [96/169], [94mLoss[0m : 2.50337
[1mStep[0m  [112/169], [94mLoss[0m : 1.89671
[1mStep[0m  [128/169], [94mLoss[0m : 2.40962
[1mStep[0m  [144/169], [94mLoss[0m : 2.60013
[1mStep[0m  [160/169], [94mLoss[0m : 2.35717

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.343, [92mTest[0m: 2.314, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33474
[1mStep[0m  [16/169], [94mLoss[0m : 2.70741
[1mStep[0m  [32/169], [94mLoss[0m : 2.30404
[1mStep[0m  [48/169], [94mLoss[0m : 2.29326
[1mStep[0m  [64/169], [94mLoss[0m : 2.36557
[1mStep[0m  [80/169], [94mLoss[0m : 2.84674
[1mStep[0m  [96/169], [94mLoss[0m : 2.37685
[1mStep[0m  [112/169], [94mLoss[0m : 2.44275
[1mStep[0m  [128/169], [94mLoss[0m : 1.93264
[1mStep[0m  [144/169], [94mLoss[0m : 2.47535
[1mStep[0m  [160/169], [94mLoss[0m : 2.16328

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.312, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.95719
[1mStep[0m  [16/169], [94mLoss[0m : 2.45438
[1mStep[0m  [32/169], [94mLoss[0m : 2.31640
[1mStep[0m  [48/169], [94mLoss[0m : 2.12427
[1mStep[0m  [64/169], [94mLoss[0m : 2.19826
[1mStep[0m  [80/169], [94mLoss[0m : 2.28767
[1mStep[0m  [96/169], [94mLoss[0m : 2.33571
[1mStep[0m  [112/169], [94mLoss[0m : 1.95836
[1mStep[0m  [128/169], [94mLoss[0m : 2.31805
[1mStep[0m  [144/169], [94mLoss[0m : 2.25526
[1mStep[0m  [160/169], [94mLoss[0m : 2.26746

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.16434
[1mStep[0m  [16/169], [94mLoss[0m : 2.80241
[1mStep[0m  [32/169], [94mLoss[0m : 2.54357
[1mStep[0m  [48/169], [94mLoss[0m : 2.09111
[1mStep[0m  [64/169], [94mLoss[0m : 2.31567
[1mStep[0m  [80/169], [94mLoss[0m : 2.26778
[1mStep[0m  [96/169], [94mLoss[0m : 2.50141
[1mStep[0m  [112/169], [94mLoss[0m : 2.76676
[1mStep[0m  [128/169], [94mLoss[0m : 2.37760
[1mStep[0m  [144/169], [94mLoss[0m : 2.52090
[1mStep[0m  [160/169], [94mLoss[0m : 2.10328

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.304, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01647
[1mStep[0m  [16/169], [94mLoss[0m : 2.74199
[1mStep[0m  [32/169], [94mLoss[0m : 2.01613
[1mStep[0m  [48/169], [94mLoss[0m : 2.41842
[1mStep[0m  [64/169], [94mLoss[0m : 2.63636
[1mStep[0m  [80/169], [94mLoss[0m : 2.34855
[1mStep[0m  [96/169], [94mLoss[0m : 2.20280
[1mStep[0m  [112/169], [94mLoss[0m : 2.34328
[1mStep[0m  [128/169], [94mLoss[0m : 2.27795
[1mStep[0m  [144/169], [94mLoss[0m : 2.42141
[1mStep[0m  [160/169], [94mLoss[0m : 2.45047

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.290, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.73343
[1mStep[0m  [16/169], [94mLoss[0m : 2.28809
[1mStep[0m  [32/169], [94mLoss[0m : 1.92266
[1mStep[0m  [48/169], [94mLoss[0m : 2.50503
[1mStep[0m  [64/169], [94mLoss[0m : 2.04255
[1mStep[0m  [80/169], [94mLoss[0m : 2.71370
[1mStep[0m  [96/169], [94mLoss[0m : 2.43246
[1mStep[0m  [112/169], [94mLoss[0m : 2.50855
[1mStep[0m  [128/169], [94mLoss[0m : 1.76505
[1mStep[0m  [144/169], [94mLoss[0m : 2.31220
[1mStep[0m  [160/169], [94mLoss[0m : 2.09035

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.295, [92mTest[0m: 2.293, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33528
[1mStep[0m  [16/169], [94mLoss[0m : 2.92640
[1mStep[0m  [32/169], [94mLoss[0m : 1.97112
[1mStep[0m  [48/169], [94mLoss[0m : 2.45746
[1mStep[0m  [64/169], [94mLoss[0m : 2.13537
[1mStep[0m  [80/169], [94mLoss[0m : 2.29894
[1mStep[0m  [96/169], [94mLoss[0m : 1.91729
[1mStep[0m  [112/169], [94mLoss[0m : 2.37712
[1mStep[0m  [128/169], [94mLoss[0m : 2.58437
[1mStep[0m  [144/169], [94mLoss[0m : 2.33532
[1mStep[0m  [160/169], [94mLoss[0m : 2.40166

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.311, [92mTest[0m: 2.304, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38937
[1mStep[0m  [16/169], [94mLoss[0m : 2.17668
[1mStep[0m  [32/169], [94mLoss[0m : 2.45830
[1mStep[0m  [48/169], [94mLoss[0m : 2.44934
[1mStep[0m  [64/169], [94mLoss[0m : 2.93897
[1mStep[0m  [80/169], [94mLoss[0m : 2.54844
[1mStep[0m  [96/169], [94mLoss[0m : 2.43181
[1mStep[0m  [112/169], [94mLoss[0m : 1.84007
[1mStep[0m  [128/169], [94mLoss[0m : 2.41698
[1mStep[0m  [144/169], [94mLoss[0m : 2.39197
[1mStep[0m  [160/169], [94mLoss[0m : 2.26882

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.313, [92mTest[0m: 2.299, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.54442
[1mStep[0m  [16/169], [94mLoss[0m : 2.34626
[1mStep[0m  [32/169], [94mLoss[0m : 2.08508
[1mStep[0m  [48/169], [94mLoss[0m : 2.42287
[1mStep[0m  [64/169], [94mLoss[0m : 2.28126
[1mStep[0m  [80/169], [94mLoss[0m : 2.31063
[1mStep[0m  [96/169], [94mLoss[0m : 2.24001
[1mStep[0m  [112/169], [94mLoss[0m : 2.37353
[1mStep[0m  [128/169], [94mLoss[0m : 2.12897
[1mStep[0m  [144/169], [94mLoss[0m : 2.10571
[1mStep[0m  [160/169], [94mLoss[0m : 2.40419

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.288, [92mTest[0m: 2.291, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.36303
[1mStep[0m  [16/169], [94mLoss[0m : 2.05480
[1mStep[0m  [32/169], [94mLoss[0m : 1.91052
[1mStep[0m  [48/169], [94mLoss[0m : 2.00021
[1mStep[0m  [64/169], [94mLoss[0m : 2.28978
[1mStep[0m  [80/169], [94mLoss[0m : 2.99260
[1mStep[0m  [96/169], [94mLoss[0m : 2.40684
[1mStep[0m  [112/169], [94mLoss[0m : 2.28938
[1mStep[0m  [128/169], [94mLoss[0m : 2.20695
[1mStep[0m  [144/169], [94mLoss[0m : 2.05506
[1mStep[0m  [160/169], [94mLoss[0m : 2.41443

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.290, [92mTest[0m: 2.295, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.25494
[1mStep[0m  [16/169], [94mLoss[0m : 2.14744
[1mStep[0m  [32/169], [94mLoss[0m : 2.11430
[1mStep[0m  [48/169], [94mLoss[0m : 2.26323
[1mStep[0m  [64/169], [94mLoss[0m : 2.01237
[1mStep[0m  [80/169], [94mLoss[0m : 2.19024
[1mStep[0m  [96/169], [94mLoss[0m : 2.53091
[1mStep[0m  [112/169], [94mLoss[0m : 2.40135
[1mStep[0m  [128/169], [94mLoss[0m : 2.31802
[1mStep[0m  [144/169], [94mLoss[0m : 2.49024
[1mStep[0m  [160/169], [94mLoss[0m : 2.20822

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.275, [92mTest[0m: 2.317, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19676
[1mStep[0m  [16/169], [94mLoss[0m : 2.52605
[1mStep[0m  [32/169], [94mLoss[0m : 2.23692
[1mStep[0m  [48/169], [94mLoss[0m : 2.36182
[1mStep[0m  [64/169], [94mLoss[0m : 2.33111
[1mStep[0m  [80/169], [94mLoss[0m : 1.92562
[1mStep[0m  [96/169], [94mLoss[0m : 2.32761
[1mStep[0m  [112/169], [94mLoss[0m : 2.44097
[1mStep[0m  [128/169], [94mLoss[0m : 2.28414
[1mStep[0m  [144/169], [94mLoss[0m : 2.35546
[1mStep[0m  [160/169], [94mLoss[0m : 2.51168

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.287, [92mTest[0m: 2.303, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.00415
[1mStep[0m  [16/169], [94mLoss[0m : 2.01408
[1mStep[0m  [32/169], [94mLoss[0m : 2.02912
[1mStep[0m  [48/169], [94mLoss[0m : 2.21148
[1mStep[0m  [64/169], [94mLoss[0m : 2.09225
[1mStep[0m  [80/169], [94mLoss[0m : 2.49656
[1mStep[0m  [96/169], [94mLoss[0m : 2.48500
[1mStep[0m  [112/169], [94mLoss[0m : 2.45697
[1mStep[0m  [128/169], [94mLoss[0m : 2.64692
[1mStep[0m  [144/169], [94mLoss[0m : 2.04062
[1mStep[0m  [160/169], [94mLoss[0m : 2.47452

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.296, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.336
====================================

Phase 1 - Evaluation MAE:  2.336138427257538
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.3, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.3, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.9
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/169], [94mLoss[0m : 2.56963
[1mStep[0m  [16/169], [94mLoss[0m : 3.24124
[1mStep[0m  [32/169], [94mLoss[0m : 2.70156
[1mStep[0m  [48/169], [94mLoss[0m : 2.39160
[1mStep[0m  [64/169], [94mLoss[0m : 2.64006
[1mStep[0m  [80/169], [94mLoss[0m : 2.63307
[1mStep[0m  [96/169], [94mLoss[0m : 2.91156
[1mStep[0m  [112/169], [94mLoss[0m : 2.15825
[1mStep[0m  [128/169], [94mLoss[0m : 2.64583
[1mStep[0m  [144/169], [94mLoss[0m : 2.33387
[1mStep[0m  [160/169], [94mLoss[0m : 2.77961

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.339, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.17951
[1mStep[0m  [16/169], [94mLoss[0m : 2.61161
[1mStep[0m  [32/169], [94mLoss[0m : 2.55379
[1mStep[0m  [48/169], [94mLoss[0m : 2.13108
[1mStep[0m  [64/169], [94mLoss[0m : 2.17586
[1mStep[0m  [80/169], [94mLoss[0m : 1.68640
[1mStep[0m  [96/169], [94mLoss[0m : 2.66971
[1mStep[0m  [112/169], [94mLoss[0m : 2.38048
[1mStep[0m  [128/169], [94mLoss[0m : 2.24977
[1mStep[0m  [144/169], [94mLoss[0m : 2.31648
[1mStep[0m  [160/169], [94mLoss[0m : 1.87774

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.330, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28448
[1mStep[0m  [16/169], [94mLoss[0m : 2.33902
[1mStep[0m  [32/169], [94mLoss[0m : 2.42135
[1mStep[0m  [48/169], [94mLoss[0m : 1.93158
[1mStep[0m  [64/169], [94mLoss[0m : 2.12550
[1mStep[0m  [80/169], [94mLoss[0m : 2.64112
[1mStep[0m  [96/169], [94mLoss[0m : 2.18483
[1mStep[0m  [112/169], [94mLoss[0m : 2.09592
[1mStep[0m  [128/169], [94mLoss[0m : 2.15977
[1mStep[0m  [144/169], [94mLoss[0m : 2.08431
[1mStep[0m  [160/169], [94mLoss[0m : 2.12034

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.259, [92mTest[0m: 2.348, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99041
[1mStep[0m  [16/169], [94mLoss[0m : 1.89293
[1mStep[0m  [32/169], [94mLoss[0m : 2.51016
[1mStep[0m  [48/169], [94mLoss[0m : 2.18610
[1mStep[0m  [64/169], [94mLoss[0m : 2.27815
[1mStep[0m  [80/169], [94mLoss[0m : 1.92154
[1mStep[0m  [96/169], [94mLoss[0m : 2.20683
[1mStep[0m  [112/169], [94mLoss[0m : 1.76466
[1mStep[0m  [128/169], [94mLoss[0m : 2.37097
[1mStep[0m  [144/169], [94mLoss[0m : 2.15336
[1mStep[0m  [160/169], [94mLoss[0m : 2.38359

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.185, [92mTest[0m: 2.366, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22408
[1mStep[0m  [16/169], [94mLoss[0m : 2.25038
[1mStep[0m  [32/169], [94mLoss[0m : 1.76174
[1mStep[0m  [48/169], [94mLoss[0m : 1.97610
[1mStep[0m  [64/169], [94mLoss[0m : 2.70663
[1mStep[0m  [80/169], [94mLoss[0m : 1.72697
[1mStep[0m  [96/169], [94mLoss[0m : 1.95263
[1mStep[0m  [112/169], [94mLoss[0m : 2.48585
[1mStep[0m  [128/169], [94mLoss[0m : 2.22796
[1mStep[0m  [144/169], [94mLoss[0m : 2.27553
[1mStep[0m  [160/169], [94mLoss[0m : 2.06816

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.139, [92mTest[0m: 2.409, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15524
[1mStep[0m  [16/169], [94mLoss[0m : 2.03162
[1mStep[0m  [32/169], [94mLoss[0m : 2.05948
[1mStep[0m  [48/169], [94mLoss[0m : 2.01477
[1mStep[0m  [64/169], [94mLoss[0m : 2.52276
[1mStep[0m  [80/169], [94mLoss[0m : 1.76181
[1mStep[0m  [96/169], [94mLoss[0m : 1.81936
[1mStep[0m  [112/169], [94mLoss[0m : 2.01540
[1mStep[0m  [128/169], [94mLoss[0m : 1.74193
[1mStep[0m  [144/169], [94mLoss[0m : 2.18093
[1mStep[0m  [160/169], [94mLoss[0m : 2.34043

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.094, [92mTest[0m: 2.426, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28352
[1mStep[0m  [16/169], [94mLoss[0m : 2.12696
[1mStep[0m  [32/169], [94mLoss[0m : 1.66969
[1mStep[0m  [48/169], [94mLoss[0m : 1.96592
[1mStep[0m  [64/169], [94mLoss[0m : 2.26719
[1mStep[0m  [80/169], [94mLoss[0m : 2.03023
[1mStep[0m  [96/169], [94mLoss[0m : 2.11800
[1mStep[0m  [112/169], [94mLoss[0m : 2.34692
[1mStep[0m  [128/169], [94mLoss[0m : 2.23319
[1mStep[0m  [144/169], [94mLoss[0m : 2.09427
[1mStep[0m  [160/169], [94mLoss[0m : 1.70906

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.033, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.77600
[1mStep[0m  [16/169], [94mLoss[0m : 1.75732
[1mStep[0m  [32/169], [94mLoss[0m : 1.95224
[1mStep[0m  [48/169], [94mLoss[0m : 1.45198
[1mStep[0m  [64/169], [94mLoss[0m : 1.77672
[1mStep[0m  [80/169], [94mLoss[0m : 1.76554
[1mStep[0m  [96/169], [94mLoss[0m : 2.14268
[1mStep[0m  [112/169], [94mLoss[0m : 1.63183
[1mStep[0m  [128/169], [94mLoss[0m : 2.24514
[1mStep[0m  [144/169], [94mLoss[0m : 1.87292
[1mStep[0m  [160/169], [94mLoss[0m : 2.11993

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.000, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41990
[1mStep[0m  [16/169], [94mLoss[0m : 1.63287
[1mStep[0m  [32/169], [94mLoss[0m : 1.89887
[1mStep[0m  [48/169], [94mLoss[0m : 2.23908
[1mStep[0m  [64/169], [94mLoss[0m : 1.84302
[1mStep[0m  [80/169], [94mLoss[0m : 2.06763
[1mStep[0m  [96/169], [94mLoss[0m : 1.88422
[1mStep[0m  [112/169], [94mLoss[0m : 1.97286
[1mStep[0m  [128/169], [94mLoss[0m : 2.05681
[1mStep[0m  [144/169], [94mLoss[0m : 2.19710
[1mStep[0m  [160/169], [94mLoss[0m : 2.09181

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.401, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.58505
[1mStep[0m  [16/169], [94mLoss[0m : 1.76483
[1mStep[0m  [32/169], [94mLoss[0m : 1.68351
[1mStep[0m  [48/169], [94mLoss[0m : 1.86819
[1mStep[0m  [64/169], [94mLoss[0m : 1.83744
[1mStep[0m  [80/169], [94mLoss[0m : 2.16499
[1mStep[0m  [96/169], [94mLoss[0m : 2.15560
[1mStep[0m  [112/169], [94mLoss[0m : 1.35591
[1mStep[0m  [128/169], [94mLoss[0m : 1.72667
[1mStep[0m  [144/169], [94mLoss[0m : 1.66848
[1mStep[0m  [160/169], [94mLoss[0m : 1.91680

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.886, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.94120
[1mStep[0m  [16/169], [94mLoss[0m : 1.49809
[1mStep[0m  [32/169], [94mLoss[0m : 1.51298
[1mStep[0m  [48/169], [94mLoss[0m : 1.63756
[1mStep[0m  [64/169], [94mLoss[0m : 1.97327
[1mStep[0m  [80/169], [94mLoss[0m : 1.77985
[1mStep[0m  [96/169], [94mLoss[0m : 1.95741
[1mStep[0m  [112/169], [94mLoss[0m : 1.64415
[1mStep[0m  [128/169], [94mLoss[0m : 1.87260
[1mStep[0m  [144/169], [94mLoss[0m : 1.80782
[1mStep[0m  [160/169], [94mLoss[0m : 2.06693

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.861, [92mTest[0m: 2.445, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.66475
[1mStep[0m  [16/169], [94mLoss[0m : 1.67596
[1mStep[0m  [32/169], [94mLoss[0m : 1.96406
[1mStep[0m  [48/169], [94mLoss[0m : 2.06253
[1mStep[0m  [64/169], [94mLoss[0m : 1.87930
[1mStep[0m  [80/169], [94mLoss[0m : 2.17928
[1mStep[0m  [96/169], [94mLoss[0m : 1.93266
[1mStep[0m  [112/169], [94mLoss[0m : 2.00848
[1mStep[0m  [128/169], [94mLoss[0m : 1.99015
[1mStep[0m  [144/169], [94mLoss[0m : 1.68586
[1mStep[0m  [160/169], [94mLoss[0m : 1.89122

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.830, [92mTest[0m: 2.481, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.59293
[1mStep[0m  [16/169], [94mLoss[0m : 1.91228
[1mStep[0m  [32/169], [94mLoss[0m : 1.70622
[1mStep[0m  [48/169], [94mLoss[0m : 1.60937
[1mStep[0m  [64/169], [94mLoss[0m : 1.63267
[1mStep[0m  [80/169], [94mLoss[0m : 1.85408
[1mStep[0m  [96/169], [94mLoss[0m : 1.69248
[1mStep[0m  [112/169], [94mLoss[0m : 1.90505
[1mStep[0m  [128/169], [94mLoss[0m : 1.66820
[1mStep[0m  [144/169], [94mLoss[0m : 1.79784
[1mStep[0m  [160/169], [94mLoss[0m : 1.90149

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.800, [92mTest[0m: 2.530, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84178
[1mStep[0m  [16/169], [94mLoss[0m : 1.62384
[1mStep[0m  [32/169], [94mLoss[0m : 1.85926
[1mStep[0m  [48/169], [94mLoss[0m : 1.50369
[1mStep[0m  [64/169], [94mLoss[0m : 2.28829
[1mStep[0m  [80/169], [94mLoss[0m : 1.91003
[1mStep[0m  [96/169], [94mLoss[0m : 1.88669
[1mStep[0m  [112/169], [94mLoss[0m : 2.00060
[1mStep[0m  [128/169], [94mLoss[0m : 1.62158
[1mStep[0m  [144/169], [94mLoss[0m : 1.93995
[1mStep[0m  [160/169], [94mLoss[0m : 1.78838

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.778, [92mTest[0m: 2.497, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.26293
[1mStep[0m  [16/169], [94mLoss[0m : 2.21120
[1mStep[0m  [32/169], [94mLoss[0m : 1.73748
[1mStep[0m  [48/169], [94mLoss[0m : 1.40308
[1mStep[0m  [64/169], [94mLoss[0m : 1.80951
[1mStep[0m  [80/169], [94mLoss[0m : 1.72620
[1mStep[0m  [96/169], [94mLoss[0m : 1.93019
[1mStep[0m  [112/169], [94mLoss[0m : 1.79175
[1mStep[0m  [128/169], [94mLoss[0m : 1.78698
[1mStep[0m  [144/169], [94mLoss[0m : 2.17399
[1mStep[0m  [160/169], [94mLoss[0m : 1.79124

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.734, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.63045
[1mStep[0m  [16/169], [94mLoss[0m : 1.89445
[1mStep[0m  [32/169], [94mLoss[0m : 1.48165
[1mStep[0m  [48/169], [94mLoss[0m : 1.63668
[1mStep[0m  [64/169], [94mLoss[0m : 1.87377
[1mStep[0m  [80/169], [94mLoss[0m : 1.77288
[1mStep[0m  [96/169], [94mLoss[0m : 1.75190
[1mStep[0m  [112/169], [94mLoss[0m : 1.58037
[1mStep[0m  [128/169], [94mLoss[0m : 1.82707
[1mStep[0m  [144/169], [94mLoss[0m : 2.14756
[1mStep[0m  [160/169], [94mLoss[0m : 1.64761

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.542, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.41724
[1mStep[0m  [16/169], [94mLoss[0m : 1.45825
[1mStep[0m  [32/169], [94mLoss[0m : 1.94098
[1mStep[0m  [48/169], [94mLoss[0m : 1.72671
[1mStep[0m  [64/169], [94mLoss[0m : 1.63825
[1mStep[0m  [80/169], [94mLoss[0m : 1.54359
[1mStep[0m  [96/169], [94mLoss[0m : 1.56203
[1mStep[0m  [112/169], [94mLoss[0m : 1.53082
[1mStep[0m  [128/169], [94mLoss[0m : 1.84477
[1mStep[0m  [144/169], [94mLoss[0m : 1.52470
[1mStep[0m  [160/169], [94mLoss[0m : 1.37179

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.664, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.67547
[1mStep[0m  [16/169], [94mLoss[0m : 1.74494
[1mStep[0m  [32/169], [94mLoss[0m : 1.68998
[1mStep[0m  [48/169], [94mLoss[0m : 1.61553
[1mStep[0m  [64/169], [94mLoss[0m : 1.57855
[1mStep[0m  [80/169], [94mLoss[0m : 1.59540
[1mStep[0m  [96/169], [94mLoss[0m : 1.67897
[1mStep[0m  [112/169], [94mLoss[0m : 1.56167
[1mStep[0m  [128/169], [94mLoss[0m : 1.55709
[1mStep[0m  [144/169], [94mLoss[0m : 1.66179
[1mStep[0m  [160/169], [94mLoss[0m : 1.62726

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.644, [92mTest[0m: 2.511, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50120
[1mStep[0m  [16/169], [94mLoss[0m : 1.68505
[1mStep[0m  [32/169], [94mLoss[0m : 1.23420
[1mStep[0m  [48/169], [94mLoss[0m : 1.56678
[1mStep[0m  [64/169], [94mLoss[0m : 1.69827
[1mStep[0m  [80/169], [94mLoss[0m : 1.58712
[1mStep[0m  [96/169], [94mLoss[0m : 1.58296
[1mStep[0m  [112/169], [94mLoss[0m : 1.64775
[1mStep[0m  [128/169], [94mLoss[0m : 1.44393
[1mStep[0m  [144/169], [94mLoss[0m : 1.75328
[1mStep[0m  [160/169], [94mLoss[0m : 1.72835

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.612, [92mTest[0m: 2.570, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.68906
[1mStep[0m  [16/169], [94mLoss[0m : 1.38896
[1mStep[0m  [32/169], [94mLoss[0m : 1.29481
[1mStep[0m  [48/169], [94mLoss[0m : 1.57821
[1mStep[0m  [64/169], [94mLoss[0m : 1.31780
[1mStep[0m  [80/169], [94mLoss[0m : 1.36881
[1mStep[0m  [96/169], [94mLoss[0m : 1.49137
[1mStep[0m  [112/169], [94mLoss[0m : 1.60059
[1mStep[0m  [128/169], [94mLoss[0m : 1.58696
[1mStep[0m  [144/169], [94mLoss[0m : 1.40635
[1mStep[0m  [160/169], [94mLoss[0m : 1.72264

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.587, [92mTest[0m: 2.515, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.21585
[1mStep[0m  [16/169], [94mLoss[0m : 1.70403
[1mStep[0m  [32/169], [94mLoss[0m : 1.56655
[1mStep[0m  [48/169], [94mLoss[0m : 1.30914
[1mStep[0m  [64/169], [94mLoss[0m : 1.20641
[1mStep[0m  [80/169], [94mLoss[0m : 1.81029
[1mStep[0m  [96/169], [94mLoss[0m : 1.19251
[1mStep[0m  [112/169], [94mLoss[0m : 1.59996
[1mStep[0m  [128/169], [94mLoss[0m : 1.48041
[1mStep[0m  [144/169], [94mLoss[0m : 1.44575
[1mStep[0m  [160/169], [94mLoss[0m : 1.45488

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.515, [92mTest[0m: 2.552, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.49434
[1mStep[0m  [16/169], [94mLoss[0m : 1.63107
[1mStep[0m  [32/169], [94mLoss[0m : 1.36387
[1mStep[0m  [48/169], [94mLoss[0m : 1.54449
[1mStep[0m  [64/169], [94mLoss[0m : 1.63619
[1mStep[0m  [80/169], [94mLoss[0m : 1.54839
[1mStep[0m  [96/169], [94mLoss[0m : 1.48082
[1mStep[0m  [112/169], [94mLoss[0m : 1.73715
[1mStep[0m  [128/169], [94mLoss[0m : 1.53975
[1mStep[0m  [144/169], [94mLoss[0m : 1.19022
[1mStep[0m  [160/169], [94mLoss[0m : 1.43964

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.491, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.23082
[1mStep[0m  [16/169], [94mLoss[0m : 1.44342
[1mStep[0m  [32/169], [94mLoss[0m : 1.64573
[1mStep[0m  [48/169], [94mLoss[0m : 1.63338
[1mStep[0m  [64/169], [94mLoss[0m : 1.57086
[1mStep[0m  [80/169], [94mLoss[0m : 1.56403
[1mStep[0m  [96/169], [94mLoss[0m : 1.53830
[1mStep[0m  [112/169], [94mLoss[0m : 1.78895
[1mStep[0m  [128/169], [94mLoss[0m : 1.50023
[1mStep[0m  [144/169], [94mLoss[0m : 1.66632
[1mStep[0m  [160/169], [94mLoss[0m : 1.39660

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.488, [92mTest[0m: 2.576, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.83521
[1mStep[0m  [16/169], [94mLoss[0m : 1.68162
[1mStep[0m  [32/169], [94mLoss[0m : 1.24222
[1mStep[0m  [48/169], [94mLoss[0m : 1.66661
[1mStep[0m  [64/169], [94mLoss[0m : 1.40297
[1mStep[0m  [80/169], [94mLoss[0m : 1.60312
[1mStep[0m  [96/169], [94mLoss[0m : 1.23585
[1mStep[0m  [112/169], [94mLoss[0m : 1.50166
[1mStep[0m  [128/169], [94mLoss[0m : 1.33317
[1mStep[0m  [144/169], [94mLoss[0m : 1.88107
[1mStep[0m  [160/169], [94mLoss[0m : 1.53012

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.473, [92mTest[0m: 2.564, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.50051
[1mStep[0m  [16/169], [94mLoss[0m : 1.31110
[1mStep[0m  [32/169], [94mLoss[0m : 1.48357
[1mStep[0m  [48/169], [94mLoss[0m : 1.09680
[1mStep[0m  [64/169], [94mLoss[0m : 1.19941
[1mStep[0m  [80/169], [94mLoss[0m : 1.24487
[1mStep[0m  [96/169], [94mLoss[0m : 1.63878
[1mStep[0m  [112/169], [94mLoss[0m : 1.40021
[1mStep[0m  [128/169], [94mLoss[0m : 1.49747
[1mStep[0m  [144/169], [94mLoss[0m : 1.59543
[1mStep[0m  [160/169], [94mLoss[0m : 1.55658

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.441, [92mTest[0m: 2.545, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.07216
[1mStep[0m  [16/169], [94mLoss[0m : 1.55052
[1mStep[0m  [32/169], [94mLoss[0m : 1.47327
[1mStep[0m  [48/169], [94mLoss[0m : 1.30847
[1mStep[0m  [64/169], [94mLoss[0m : 1.33981
[1mStep[0m  [80/169], [94mLoss[0m : 1.43772
[1mStep[0m  [96/169], [94mLoss[0m : 1.31738
[1mStep[0m  [112/169], [94mLoss[0m : 1.20993
[1mStep[0m  [128/169], [94mLoss[0m : 1.24646
[1mStep[0m  [144/169], [94mLoss[0m : 1.44635
[1mStep[0m  [160/169], [94mLoss[0m : 1.28963

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.424, [92mTest[0m: 2.559, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 25 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.516
====================================

Phase 2 - Evaluation MAE:  2.5159667219434465
MAE score P1       2.336138
MAE score P2       2.515967
loss               1.424247
learning_rate      0.007525
batch_size               64
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.3
momentum                0.9
weight_decay         0.0001
Name: 12, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 10.64796
[1mStep[0m  [33/339], [94mLoss[0m : 9.74952
[1mStep[0m  [66/339], [94mLoss[0m : 9.18173
[1mStep[0m  [99/339], [94mLoss[0m : 10.01849
[1mStep[0m  [132/339], [94mLoss[0m : 7.85232
[1mStep[0m  [165/339], [94mLoss[0m : 5.73974
[1mStep[0m  [198/339], [94mLoss[0m : 5.12537
[1mStep[0m  [231/339], [94mLoss[0m : 3.99777
[1mStep[0m  [264/339], [94mLoss[0m : 4.38722
[1mStep[0m  [297/339], [94mLoss[0m : 3.24552
[1mStep[0m  [330/339], [94mLoss[0m : 1.76846

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 6.504, [92mTest[0m: 10.934, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.04612
[1mStep[0m  [33/339], [94mLoss[0m : 2.92610
[1mStep[0m  [66/339], [94mLoss[0m : 2.24817
[1mStep[0m  [99/339], [94mLoss[0m : 2.76885
[1mStep[0m  [132/339], [94mLoss[0m : 2.78281
[1mStep[0m  [165/339], [94mLoss[0m : 2.29919
[1mStep[0m  [198/339], [94mLoss[0m : 2.22500
[1mStep[0m  [231/339], [94mLoss[0m : 2.82208
[1mStep[0m  [264/339], [94mLoss[0m : 2.33935
[1mStep[0m  [297/339], [94mLoss[0m : 2.42989
[1mStep[0m  [330/339], [94mLoss[0m : 2.81439

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.692, [92mTest[0m: 2.422, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84549
[1mStep[0m  [33/339], [94mLoss[0m : 2.86912
[1mStep[0m  [66/339], [94mLoss[0m : 3.16172
[1mStep[0m  [99/339], [94mLoss[0m : 2.81155
[1mStep[0m  [132/339], [94mLoss[0m : 2.66757
[1mStep[0m  [165/339], [94mLoss[0m : 2.68247
[1mStep[0m  [198/339], [94mLoss[0m : 2.36934
[1mStep[0m  [231/339], [94mLoss[0m : 2.04360
[1mStep[0m  [264/339], [94mLoss[0m : 2.16630
[1mStep[0m  [297/339], [94mLoss[0m : 2.63635
[1mStep[0m  [330/339], [94mLoss[0m : 2.88247

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.595, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.10130
[1mStep[0m  [33/339], [94mLoss[0m : 2.47033
[1mStep[0m  [66/339], [94mLoss[0m : 2.18151
[1mStep[0m  [99/339], [94mLoss[0m : 2.51556
[1mStep[0m  [132/339], [94mLoss[0m : 1.80032
[1mStep[0m  [165/339], [94mLoss[0m : 2.56793
[1mStep[0m  [198/339], [94mLoss[0m : 2.60120
[1mStep[0m  [231/339], [94mLoss[0m : 2.42503
[1mStep[0m  [264/339], [94mLoss[0m : 3.08563
[1mStep[0m  [297/339], [94mLoss[0m : 2.62841
[1mStep[0m  [330/339], [94mLoss[0m : 2.85474

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.549, [92mTest[0m: 2.364, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.47573
[1mStep[0m  [33/339], [94mLoss[0m : 2.51709
[1mStep[0m  [66/339], [94mLoss[0m : 1.85026
[1mStep[0m  [99/339], [94mLoss[0m : 2.21119
[1mStep[0m  [132/339], [94mLoss[0m : 2.43851
[1mStep[0m  [165/339], [94mLoss[0m : 2.61397
[1mStep[0m  [198/339], [94mLoss[0m : 2.18863
[1mStep[0m  [231/339], [94mLoss[0m : 2.72567
[1mStep[0m  [264/339], [94mLoss[0m : 1.76303
[1mStep[0m  [297/339], [94mLoss[0m : 2.89087
[1mStep[0m  [330/339], [94mLoss[0m : 2.33169

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.522, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.20959
[1mStep[0m  [33/339], [94mLoss[0m : 2.72808
[1mStep[0m  [66/339], [94mLoss[0m : 2.09174
[1mStep[0m  [99/339], [94mLoss[0m : 2.15014
[1mStep[0m  [132/339], [94mLoss[0m : 2.14755
[1mStep[0m  [165/339], [94mLoss[0m : 2.88154
[1mStep[0m  [198/339], [94mLoss[0m : 2.55381
[1mStep[0m  [231/339], [94mLoss[0m : 2.60955
[1mStep[0m  [264/339], [94mLoss[0m : 2.16964
[1mStep[0m  [297/339], [94mLoss[0m : 2.86062
[1mStep[0m  [330/339], [94mLoss[0m : 2.47802

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.520, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.47158
[1mStep[0m  [33/339], [94mLoss[0m : 2.04301
[1mStep[0m  [66/339], [94mLoss[0m : 2.74472
[1mStep[0m  [99/339], [94mLoss[0m : 2.54338
[1mStep[0m  [132/339], [94mLoss[0m : 2.94164
[1mStep[0m  [165/339], [94mLoss[0m : 2.09977
[1mStep[0m  [198/339], [94mLoss[0m : 2.57530
[1mStep[0m  [231/339], [94mLoss[0m : 2.49678
[1mStep[0m  [264/339], [94mLoss[0m : 1.85764
[1mStep[0m  [297/339], [94mLoss[0m : 2.09105
[1mStep[0m  [330/339], [94mLoss[0m : 2.47379

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.363, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.18565
[1mStep[0m  [33/339], [94mLoss[0m : 3.15396
[1mStep[0m  [66/339], [94mLoss[0m : 1.96211
[1mStep[0m  [99/339], [94mLoss[0m : 2.38523
[1mStep[0m  [132/339], [94mLoss[0m : 2.39255
[1mStep[0m  [165/339], [94mLoss[0m : 2.59379
[1mStep[0m  [198/339], [94mLoss[0m : 2.86209
[1mStep[0m  [231/339], [94mLoss[0m : 2.17150
[1mStep[0m  [264/339], [94mLoss[0m : 2.12161
[1mStep[0m  [297/339], [94mLoss[0m : 3.05652
[1mStep[0m  [330/339], [94mLoss[0m : 2.61310

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50155
[1mStep[0m  [33/339], [94mLoss[0m : 2.74686
[1mStep[0m  [66/339], [94mLoss[0m : 2.50326
[1mStep[0m  [99/339], [94mLoss[0m : 2.61033
[1mStep[0m  [132/339], [94mLoss[0m : 1.78034
[1mStep[0m  [165/339], [94mLoss[0m : 2.20206
[1mStep[0m  [198/339], [94mLoss[0m : 2.52243
[1mStep[0m  [231/339], [94mLoss[0m : 2.29900
[1mStep[0m  [264/339], [94mLoss[0m : 2.58565
[1mStep[0m  [297/339], [94mLoss[0m : 2.62612
[1mStep[0m  [330/339], [94mLoss[0m : 2.98688

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.346, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.58676
[1mStep[0m  [33/339], [94mLoss[0m : 2.36870
[1mStep[0m  [66/339], [94mLoss[0m : 2.69325
[1mStep[0m  [99/339], [94mLoss[0m : 2.08895
[1mStep[0m  [132/339], [94mLoss[0m : 2.27581
[1mStep[0m  [165/339], [94mLoss[0m : 2.42654
[1mStep[0m  [198/339], [94mLoss[0m : 2.30026
[1mStep[0m  [231/339], [94mLoss[0m : 3.27248
[1mStep[0m  [264/339], [94mLoss[0m : 2.76546
[1mStep[0m  [297/339], [94mLoss[0m : 2.33587
[1mStep[0m  [330/339], [94mLoss[0m : 2.79274

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.452, [92mTest[0m: 2.326, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.45492
[1mStep[0m  [33/339], [94mLoss[0m : 2.34799
[1mStep[0m  [66/339], [94mLoss[0m : 1.79032
[1mStep[0m  [99/339], [94mLoss[0m : 2.71228
[1mStep[0m  [132/339], [94mLoss[0m : 2.93271
[1mStep[0m  [165/339], [94mLoss[0m : 2.39020
[1mStep[0m  [198/339], [94mLoss[0m : 2.58728
[1mStep[0m  [231/339], [94mLoss[0m : 2.30547
[1mStep[0m  [264/339], [94mLoss[0m : 2.39773
[1mStep[0m  [297/339], [94mLoss[0m : 2.67728
[1mStep[0m  [330/339], [94mLoss[0m : 3.05761

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.447, [92mTest[0m: 2.360, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.13686
[1mStep[0m  [33/339], [94mLoss[0m : 1.63926
[1mStep[0m  [66/339], [94mLoss[0m : 2.14848
[1mStep[0m  [99/339], [94mLoss[0m : 2.60218
[1mStep[0m  [132/339], [94mLoss[0m : 2.40941
[1mStep[0m  [165/339], [94mLoss[0m : 2.28755
[1mStep[0m  [198/339], [94mLoss[0m : 2.35248
[1mStep[0m  [231/339], [94mLoss[0m : 2.02124
[1mStep[0m  [264/339], [94mLoss[0m : 2.23276
[1mStep[0m  [297/339], [94mLoss[0m : 2.21877
[1mStep[0m  [330/339], [94mLoss[0m : 2.45072

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.436, [92mTest[0m: 2.328, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.31691
[1mStep[0m  [33/339], [94mLoss[0m : 2.07650
[1mStep[0m  [66/339], [94mLoss[0m : 2.68449
[1mStep[0m  [99/339], [94mLoss[0m : 2.73869
[1mStep[0m  [132/339], [94mLoss[0m : 2.46573
[1mStep[0m  [165/339], [94mLoss[0m : 1.92099
[1mStep[0m  [198/339], [94mLoss[0m : 2.49356
[1mStep[0m  [231/339], [94mLoss[0m : 2.02424
[1mStep[0m  [264/339], [94mLoss[0m : 2.44180
[1mStep[0m  [297/339], [94mLoss[0m : 2.67235
[1mStep[0m  [330/339], [94mLoss[0m : 2.03032

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.424, [92mTest[0m: 2.365, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.35319
[1mStep[0m  [33/339], [94mLoss[0m : 1.77273
[1mStep[0m  [66/339], [94mLoss[0m : 3.06423
[1mStep[0m  [99/339], [94mLoss[0m : 2.17051
[1mStep[0m  [132/339], [94mLoss[0m : 2.86295
[1mStep[0m  [165/339], [94mLoss[0m : 2.67905
[1mStep[0m  [198/339], [94mLoss[0m : 2.40741
[1mStep[0m  [231/339], [94mLoss[0m : 2.82035
[1mStep[0m  [264/339], [94mLoss[0m : 2.35959
[1mStep[0m  [297/339], [94mLoss[0m : 2.36416
[1mStep[0m  [330/339], [94mLoss[0m : 2.38724

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.446, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.52906
[1mStep[0m  [33/339], [94mLoss[0m : 2.44964
[1mStep[0m  [66/339], [94mLoss[0m : 2.27697
[1mStep[0m  [99/339], [94mLoss[0m : 2.72600
[1mStep[0m  [132/339], [94mLoss[0m : 2.79634
[1mStep[0m  [165/339], [94mLoss[0m : 2.20501
[1mStep[0m  [198/339], [94mLoss[0m : 1.97384
[1mStep[0m  [231/339], [94mLoss[0m : 2.23782
[1mStep[0m  [264/339], [94mLoss[0m : 2.01880
[1mStep[0m  [297/339], [94mLoss[0m : 2.34057
[1mStep[0m  [330/339], [94mLoss[0m : 2.47255

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.425, [92mTest[0m: 2.353, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30979
[1mStep[0m  [33/339], [94mLoss[0m : 2.87063
[1mStep[0m  [66/339], [94mLoss[0m : 2.37410
[1mStep[0m  [99/339], [94mLoss[0m : 2.77361
[1mStep[0m  [132/339], [94mLoss[0m : 2.81282
[1mStep[0m  [165/339], [94mLoss[0m : 2.80416
[1mStep[0m  [198/339], [94mLoss[0m : 2.26309
[1mStep[0m  [231/339], [94mLoss[0m : 2.22328
[1mStep[0m  [264/339], [94mLoss[0m : 2.70536
[1mStep[0m  [297/339], [94mLoss[0m : 2.14498
[1mStep[0m  [330/339], [94mLoss[0m : 1.82266

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.439, [92mTest[0m: 2.329, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.29511
[1mStep[0m  [33/339], [94mLoss[0m : 2.17251
[1mStep[0m  [66/339], [94mLoss[0m : 2.48697
[1mStep[0m  [99/339], [94mLoss[0m : 2.58805
[1mStep[0m  [132/339], [94mLoss[0m : 2.58231
[1mStep[0m  [165/339], [94mLoss[0m : 2.44703
[1mStep[0m  [198/339], [94mLoss[0m : 2.85984
[1mStep[0m  [231/339], [94mLoss[0m : 2.21654
[1mStep[0m  [264/339], [94mLoss[0m : 1.95454
[1mStep[0m  [297/339], [94mLoss[0m : 2.51611
[1mStep[0m  [330/339], [94mLoss[0m : 2.27143

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.407, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38800
[1mStep[0m  [33/339], [94mLoss[0m : 2.64586
[1mStep[0m  [66/339], [94mLoss[0m : 2.05290
[1mStep[0m  [99/339], [94mLoss[0m : 2.60502
[1mStep[0m  [132/339], [94mLoss[0m : 1.92306
[1mStep[0m  [165/339], [94mLoss[0m : 1.92218
[1mStep[0m  [198/339], [94mLoss[0m : 2.13178
[1mStep[0m  [231/339], [94mLoss[0m : 1.99129
[1mStep[0m  [264/339], [94mLoss[0m : 2.19682
[1mStep[0m  [297/339], [94mLoss[0m : 2.33885
[1mStep[0m  [330/339], [94mLoss[0m : 1.94858

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.29724
[1mStep[0m  [33/339], [94mLoss[0m : 2.24006
[1mStep[0m  [66/339], [94mLoss[0m : 2.51597
[1mStep[0m  [99/339], [94mLoss[0m : 2.04466
[1mStep[0m  [132/339], [94mLoss[0m : 2.10185
[1mStep[0m  [165/339], [94mLoss[0m : 2.76311
[1mStep[0m  [198/339], [94mLoss[0m : 2.55439
[1mStep[0m  [231/339], [94mLoss[0m : 1.64988
[1mStep[0m  [264/339], [94mLoss[0m : 2.32245
[1mStep[0m  [297/339], [94mLoss[0m : 2.39751
[1mStep[0m  [330/339], [94mLoss[0m : 2.26873

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.423, [92mTest[0m: 2.302, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96367
[1mStep[0m  [33/339], [94mLoss[0m : 2.35845
[1mStep[0m  [66/339], [94mLoss[0m : 2.70859
[1mStep[0m  [99/339], [94mLoss[0m : 2.72934
[1mStep[0m  [132/339], [94mLoss[0m : 2.36723
[1mStep[0m  [165/339], [94mLoss[0m : 3.06318
[1mStep[0m  [198/339], [94mLoss[0m : 2.33067
[1mStep[0m  [231/339], [94mLoss[0m : 2.37303
[1mStep[0m  [264/339], [94mLoss[0m : 1.78150
[1mStep[0m  [297/339], [94mLoss[0m : 2.65052
[1mStep[0m  [330/339], [94mLoss[0m : 2.30710

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.410, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.21494
[1mStep[0m  [33/339], [94mLoss[0m : 2.28940
[1mStep[0m  [66/339], [94mLoss[0m : 2.28500
[1mStep[0m  [99/339], [94mLoss[0m : 2.96078
[1mStep[0m  [132/339], [94mLoss[0m : 2.41431
[1mStep[0m  [165/339], [94mLoss[0m : 2.70042
[1mStep[0m  [198/339], [94mLoss[0m : 1.96949
[1mStep[0m  [231/339], [94mLoss[0m : 2.17213
[1mStep[0m  [264/339], [94mLoss[0m : 2.67370
[1mStep[0m  [297/339], [94mLoss[0m : 2.28856
[1mStep[0m  [330/339], [94mLoss[0m : 2.08925

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.392, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03308
[1mStep[0m  [33/339], [94mLoss[0m : 2.66736
[1mStep[0m  [66/339], [94mLoss[0m : 2.41635
[1mStep[0m  [99/339], [94mLoss[0m : 2.18617
[1mStep[0m  [132/339], [94mLoss[0m : 2.68299
[1mStep[0m  [165/339], [94mLoss[0m : 2.10548
[1mStep[0m  [198/339], [94mLoss[0m : 2.15393
[1mStep[0m  [231/339], [94mLoss[0m : 3.10264
[1mStep[0m  [264/339], [94mLoss[0m : 2.35286
[1mStep[0m  [297/339], [94mLoss[0m : 2.00043
[1mStep[0m  [330/339], [94mLoss[0m : 1.66569

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.394, [92mTest[0m: 2.356, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.18412
[1mStep[0m  [33/339], [94mLoss[0m : 2.77499
[1mStep[0m  [66/339], [94mLoss[0m : 2.44964
[1mStep[0m  [99/339], [94mLoss[0m : 2.28149
[1mStep[0m  [132/339], [94mLoss[0m : 2.55824
[1mStep[0m  [165/339], [94mLoss[0m : 1.71608
[1mStep[0m  [198/339], [94mLoss[0m : 2.15187
[1mStep[0m  [231/339], [94mLoss[0m : 2.43756
[1mStep[0m  [264/339], [94mLoss[0m : 2.18516
[1mStep[0m  [297/339], [94mLoss[0m : 2.03593
[1mStep[0m  [330/339], [94mLoss[0m : 2.50203

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.397, [92mTest[0m: 2.325, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.44984
[1mStep[0m  [33/339], [94mLoss[0m : 2.27243
[1mStep[0m  [66/339], [94mLoss[0m : 2.56748
[1mStep[0m  [99/339], [94mLoss[0m : 2.09183
[1mStep[0m  [132/339], [94mLoss[0m : 1.87361
[1mStep[0m  [165/339], [94mLoss[0m : 2.76462
[1mStep[0m  [198/339], [94mLoss[0m : 2.08380
[1mStep[0m  [231/339], [94mLoss[0m : 2.38216
[1mStep[0m  [264/339], [94mLoss[0m : 2.51333
[1mStep[0m  [297/339], [94mLoss[0m : 3.03649
[1mStep[0m  [330/339], [94mLoss[0m : 2.13616

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.398, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 3.11156
[1mStep[0m  [33/339], [94mLoss[0m : 2.09064
[1mStep[0m  [66/339], [94mLoss[0m : 2.31617
[1mStep[0m  [99/339], [94mLoss[0m : 2.62643
[1mStep[0m  [132/339], [94mLoss[0m : 3.22040
[1mStep[0m  [165/339], [94mLoss[0m : 2.30202
[1mStep[0m  [198/339], [94mLoss[0m : 1.98554
[1mStep[0m  [231/339], [94mLoss[0m : 2.41818
[1mStep[0m  [264/339], [94mLoss[0m : 2.61157
[1mStep[0m  [297/339], [94mLoss[0m : 2.23538
[1mStep[0m  [330/339], [94mLoss[0m : 1.73435

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.401, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.30702
[1mStep[0m  [33/339], [94mLoss[0m : 2.08450
[1mStep[0m  [66/339], [94mLoss[0m : 2.29407
[1mStep[0m  [99/339], [94mLoss[0m : 1.92296
[1mStep[0m  [132/339], [94mLoss[0m : 2.69991
[1mStep[0m  [165/339], [94mLoss[0m : 2.11854
[1mStep[0m  [198/339], [94mLoss[0m : 2.16974
[1mStep[0m  [231/339], [94mLoss[0m : 2.28721
[1mStep[0m  [264/339], [94mLoss[0m : 2.49687
[1mStep[0m  [297/339], [94mLoss[0m : 1.92278
[1mStep[0m  [330/339], [94mLoss[0m : 2.03372

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.406, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04303
[1mStep[0m  [33/339], [94mLoss[0m : 2.25250
[1mStep[0m  [66/339], [94mLoss[0m : 2.09868
[1mStep[0m  [99/339], [94mLoss[0m : 2.12874
[1mStep[0m  [132/339], [94mLoss[0m : 2.31343
[1mStep[0m  [165/339], [94mLoss[0m : 1.98207
[1mStep[0m  [198/339], [94mLoss[0m : 2.00629
[1mStep[0m  [231/339], [94mLoss[0m : 1.83242
[1mStep[0m  [264/339], [94mLoss[0m : 2.37288
[1mStep[0m  [297/339], [94mLoss[0m : 2.80698
[1mStep[0m  [330/339], [94mLoss[0m : 1.63401

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.393, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.50936
[1mStep[0m  [33/339], [94mLoss[0m : 2.36108
[1mStep[0m  [66/339], [94mLoss[0m : 2.41965
[1mStep[0m  [99/339], [94mLoss[0m : 1.94549
[1mStep[0m  [132/339], [94mLoss[0m : 1.90671
[1mStep[0m  [165/339], [94mLoss[0m : 2.68169
[1mStep[0m  [198/339], [94mLoss[0m : 2.35370
[1mStep[0m  [231/339], [94mLoss[0m : 2.32926
[1mStep[0m  [264/339], [94mLoss[0m : 2.10194
[1mStep[0m  [297/339], [94mLoss[0m : 2.47452
[1mStep[0m  [330/339], [94mLoss[0m : 2.28936

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.322, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80710
[1mStep[0m  [33/339], [94mLoss[0m : 2.22650
[1mStep[0m  [66/339], [94mLoss[0m : 2.22498
[1mStep[0m  [99/339], [94mLoss[0m : 2.15507
[1mStep[0m  [132/339], [94mLoss[0m : 2.05011
[1mStep[0m  [165/339], [94mLoss[0m : 2.65831
[1mStep[0m  [198/339], [94mLoss[0m : 2.81804
[1mStep[0m  [231/339], [94mLoss[0m : 2.08877
[1mStep[0m  [264/339], [94mLoss[0m : 2.35369
[1mStep[0m  [297/339], [94mLoss[0m : 1.87596
[1mStep[0m  [330/339], [94mLoss[0m : 2.78258

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.321, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.16018
[1mStep[0m  [33/339], [94mLoss[0m : 2.29223
[1mStep[0m  [66/339], [94mLoss[0m : 2.66041
[1mStep[0m  [99/339], [94mLoss[0m : 2.26439
[1mStep[0m  [132/339], [94mLoss[0m : 2.59294
[1mStep[0m  [165/339], [94mLoss[0m : 2.20163
[1mStep[0m  [198/339], [94mLoss[0m : 2.17368
[1mStep[0m  [231/339], [94mLoss[0m : 1.85487
[1mStep[0m  [264/339], [94mLoss[0m : 1.92593
[1mStep[0m  [297/339], [94mLoss[0m : 2.55665
[1mStep[0m  [330/339], [94mLoss[0m : 2.47042

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.389, [92mTest[0m: 2.340, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.340
====================================

Phase 1 - Evaluation MAE:  2.339655472114023
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Tanh()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Tanh()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/339], [94mLoss[0m : 2.36177
[1mStep[0m  [33/339], [94mLoss[0m : 1.94999
[1mStep[0m  [66/339], [94mLoss[0m : 2.47690
[1mStep[0m  [99/339], [94mLoss[0m : 2.13265
[1mStep[0m  [132/339], [94mLoss[0m : 2.58259
[1mStep[0m  [165/339], [94mLoss[0m : 2.05445
[1mStep[0m  [198/339], [94mLoss[0m : 2.77663
[1mStep[0m  [231/339], [94mLoss[0m : 2.43091
[1mStep[0m  [264/339], [94mLoss[0m : 1.93934
[1mStep[0m  [297/339], [94mLoss[0m : 2.87286
[1mStep[0m  [330/339], [94mLoss[0m : 2.51739

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.463, [92mTest[0m: 2.340, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.73588
[1mStep[0m  [33/339], [94mLoss[0m : 2.29435
[1mStep[0m  [66/339], [94mLoss[0m : 1.93863
[1mStep[0m  [99/339], [94mLoss[0m : 3.00066
[1mStep[0m  [132/339], [94mLoss[0m : 2.50177
[1mStep[0m  [165/339], [94mLoss[0m : 2.41979
[1mStep[0m  [198/339], [94mLoss[0m : 2.96981
[1mStep[0m  [231/339], [94mLoss[0m : 2.10598
[1mStep[0m  [264/339], [94mLoss[0m : 2.30012
[1mStep[0m  [297/339], [94mLoss[0m : 2.25985
[1mStep[0m  [330/339], [94mLoss[0m : 2.63370

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.351, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.34731
[1mStep[0m  [33/339], [94mLoss[0m : 1.68195
[1mStep[0m  [66/339], [94mLoss[0m : 2.19411
[1mStep[0m  [99/339], [94mLoss[0m : 1.90900
[1mStep[0m  [132/339], [94mLoss[0m : 2.57839
[1mStep[0m  [165/339], [94mLoss[0m : 2.41317
[1mStep[0m  [198/339], [94mLoss[0m : 2.52810
[1mStep[0m  [231/339], [94mLoss[0m : 2.11136
[1mStep[0m  [264/339], [94mLoss[0m : 1.70256
[1mStep[0m  [297/339], [94mLoss[0m : 1.74630
[1mStep[0m  [330/339], [94mLoss[0m : 1.92650

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.270, [92mTest[0m: 2.370, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.65813
[1mStep[0m  [33/339], [94mLoss[0m : 1.88087
[1mStep[0m  [66/339], [94mLoss[0m : 2.07329
[1mStep[0m  [99/339], [94mLoss[0m : 1.84134
[1mStep[0m  [132/339], [94mLoss[0m : 1.98570
[1mStep[0m  [165/339], [94mLoss[0m : 2.33148
[1mStep[0m  [198/339], [94mLoss[0m : 1.82498
[1mStep[0m  [231/339], [94mLoss[0m : 2.01490
[1mStep[0m  [264/339], [94mLoss[0m : 2.97592
[1mStep[0m  [297/339], [94mLoss[0m : 1.86409
[1mStep[0m  [330/339], [94mLoss[0m : 2.51845

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.235, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.56677
[1mStep[0m  [33/339], [94mLoss[0m : 2.47446
[1mStep[0m  [66/339], [94mLoss[0m : 1.71917
[1mStep[0m  [99/339], [94mLoss[0m : 2.50426
[1mStep[0m  [132/339], [94mLoss[0m : 1.81251
[1mStep[0m  [165/339], [94mLoss[0m : 1.74173
[1mStep[0m  [198/339], [94mLoss[0m : 1.89657
[1mStep[0m  [231/339], [94mLoss[0m : 2.52845
[1mStep[0m  [264/339], [94mLoss[0m : 2.72589
[1mStep[0m  [297/339], [94mLoss[0m : 2.41997
[1mStep[0m  [330/339], [94mLoss[0m : 2.50291

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.178, [92mTest[0m: 2.358, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.15720
[1mStep[0m  [33/339], [94mLoss[0m : 1.80049
[1mStep[0m  [66/339], [94mLoss[0m : 2.04022
[1mStep[0m  [99/339], [94mLoss[0m : 1.66092
[1mStep[0m  [132/339], [94mLoss[0m : 2.63964
[1mStep[0m  [165/339], [94mLoss[0m : 2.39790
[1mStep[0m  [198/339], [94mLoss[0m : 2.44198
[1mStep[0m  [231/339], [94mLoss[0m : 1.96760
[1mStep[0m  [264/339], [94mLoss[0m : 2.06602
[1mStep[0m  [297/339], [94mLoss[0m : 1.72956
[1mStep[0m  [330/339], [94mLoss[0m : 2.19081

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.127, [92mTest[0m: 2.383, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03761
[1mStep[0m  [33/339], [94mLoss[0m : 1.98420
[1mStep[0m  [66/339], [94mLoss[0m : 1.75750
[1mStep[0m  [99/339], [94mLoss[0m : 1.56924
[1mStep[0m  [132/339], [94mLoss[0m : 1.71251
[1mStep[0m  [165/339], [94mLoss[0m : 2.67897
[1mStep[0m  [198/339], [94mLoss[0m : 1.99029
[1mStep[0m  [231/339], [94mLoss[0m : 2.02845
[1mStep[0m  [264/339], [94mLoss[0m : 2.06447
[1mStep[0m  [297/339], [94mLoss[0m : 2.30833
[1mStep[0m  [330/339], [94mLoss[0m : 1.97706

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.077, [92mTest[0m: 2.372, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88780
[1mStep[0m  [33/339], [94mLoss[0m : 2.16850
[1mStep[0m  [66/339], [94mLoss[0m : 2.57562
[1mStep[0m  [99/339], [94mLoss[0m : 1.94684
[1mStep[0m  [132/339], [94mLoss[0m : 2.45841
[1mStep[0m  [165/339], [94mLoss[0m : 3.13970
[1mStep[0m  [198/339], [94mLoss[0m : 1.97410
[1mStep[0m  [231/339], [94mLoss[0m : 1.95607
[1mStep[0m  [264/339], [94mLoss[0m : 1.96905
[1mStep[0m  [297/339], [94mLoss[0m : 2.26579
[1mStep[0m  [330/339], [94mLoss[0m : 2.09862

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.025, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.14064
[1mStep[0m  [33/339], [94mLoss[0m : 1.37987
[1mStep[0m  [66/339], [94mLoss[0m : 2.21221
[1mStep[0m  [99/339], [94mLoss[0m : 2.16372
[1mStep[0m  [132/339], [94mLoss[0m : 1.65054
[1mStep[0m  [165/339], [94mLoss[0m : 1.84376
[1mStep[0m  [198/339], [94mLoss[0m : 1.42368
[1mStep[0m  [231/339], [94mLoss[0m : 1.61384
[1mStep[0m  [264/339], [94mLoss[0m : 2.74241
[1mStep[0m  [297/339], [94mLoss[0m : 2.25130
[1mStep[0m  [330/339], [94mLoss[0m : 1.78404

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.003, [92mTest[0m: 2.381, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81824
[1mStep[0m  [33/339], [94mLoss[0m : 2.39285
[1mStep[0m  [66/339], [94mLoss[0m : 2.52697
[1mStep[0m  [99/339], [94mLoss[0m : 1.80537
[1mStep[0m  [132/339], [94mLoss[0m : 2.25695
[1mStep[0m  [165/339], [94mLoss[0m : 2.57941
[1mStep[0m  [198/339], [94mLoss[0m : 1.73067
[1mStep[0m  [231/339], [94mLoss[0m : 2.23286
[1mStep[0m  [264/339], [94mLoss[0m : 1.75533
[1mStep[0m  [297/339], [94mLoss[0m : 1.62827
[1mStep[0m  [330/339], [94mLoss[0m : 1.55962

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.957, [92mTest[0m: 2.410, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75253
[1mStep[0m  [33/339], [94mLoss[0m : 1.87169
[1mStep[0m  [66/339], [94mLoss[0m : 1.61147
[1mStep[0m  [99/339], [94mLoss[0m : 1.82888
[1mStep[0m  [132/339], [94mLoss[0m : 1.57655
[1mStep[0m  [165/339], [94mLoss[0m : 1.31661
[1mStep[0m  [198/339], [94mLoss[0m : 2.03188
[1mStep[0m  [231/339], [94mLoss[0m : 2.61301
[1mStep[0m  [264/339], [94mLoss[0m : 2.51953
[1mStep[0m  [297/339], [94mLoss[0m : 1.92549
[1mStep[0m  [330/339], [94mLoss[0m : 1.60615

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.919, [92mTest[0m: 2.403, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75456
[1mStep[0m  [33/339], [94mLoss[0m : 1.62698
[1mStep[0m  [66/339], [94mLoss[0m : 1.69910
[1mStep[0m  [99/339], [94mLoss[0m : 1.75661
[1mStep[0m  [132/339], [94mLoss[0m : 1.46394
[1mStep[0m  [165/339], [94mLoss[0m : 1.90308
[1mStep[0m  [198/339], [94mLoss[0m : 1.78659
[1mStep[0m  [231/339], [94mLoss[0m : 2.09610
[1mStep[0m  [264/339], [94mLoss[0m : 2.18560
[1mStep[0m  [297/339], [94mLoss[0m : 2.29687
[1mStep[0m  [330/339], [94mLoss[0m : 1.39515

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.899, [92mTest[0m: 2.449, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.95614
[1mStep[0m  [33/339], [94mLoss[0m : 2.41802
[1mStep[0m  [66/339], [94mLoss[0m : 1.33196
[1mStep[0m  [99/339], [94mLoss[0m : 2.07871
[1mStep[0m  [132/339], [94mLoss[0m : 2.34879
[1mStep[0m  [165/339], [94mLoss[0m : 1.73904
[1mStep[0m  [198/339], [94mLoss[0m : 1.73574
[1mStep[0m  [231/339], [94mLoss[0m : 2.16906
[1mStep[0m  [264/339], [94mLoss[0m : 2.02422
[1mStep[0m  [297/339], [94mLoss[0m : 1.88304
[1mStep[0m  [330/339], [94mLoss[0m : 1.57372

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.857, [92mTest[0m: 2.436, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.49369
[1mStep[0m  [33/339], [94mLoss[0m : 1.58472
[1mStep[0m  [66/339], [94mLoss[0m : 2.18706
[1mStep[0m  [99/339], [94mLoss[0m : 1.46273
[1mStep[0m  [132/339], [94mLoss[0m : 1.70890
[1mStep[0m  [165/339], [94mLoss[0m : 1.48540
[1mStep[0m  [198/339], [94mLoss[0m : 1.60387
[1mStep[0m  [231/339], [94mLoss[0m : 1.79700
[1mStep[0m  [264/339], [94mLoss[0m : 2.29724
[1mStep[0m  [297/339], [94mLoss[0m : 1.70143
[1mStep[0m  [330/339], [94mLoss[0m : 2.66955

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.831, [92mTest[0m: 2.460, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.79931
[1mStep[0m  [33/339], [94mLoss[0m : 1.38697
[1mStep[0m  [66/339], [94mLoss[0m : 1.15963
[1mStep[0m  [99/339], [94mLoss[0m : 1.80727
[1mStep[0m  [132/339], [94mLoss[0m : 2.23284
[1mStep[0m  [165/339], [94mLoss[0m : 2.22992
[1mStep[0m  [198/339], [94mLoss[0m : 2.20415
[1mStep[0m  [231/339], [94mLoss[0m : 2.26338
[1mStep[0m  [264/339], [94mLoss[0m : 1.73774
[1mStep[0m  [297/339], [94mLoss[0m : 2.24597
[1mStep[0m  [330/339], [94mLoss[0m : 1.59054

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.803, [92mTest[0m: 2.488, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.57516
[1mStep[0m  [33/339], [94mLoss[0m : 1.95908
[1mStep[0m  [66/339], [94mLoss[0m : 1.85393
[1mStep[0m  [99/339], [94mLoss[0m : 1.57381
[1mStep[0m  [132/339], [94mLoss[0m : 1.30418
[1mStep[0m  [165/339], [94mLoss[0m : 1.69764
[1mStep[0m  [198/339], [94mLoss[0m : 2.12392
[1mStep[0m  [231/339], [94mLoss[0m : 1.93103
[1mStep[0m  [264/339], [94mLoss[0m : 1.82287
[1mStep[0m  [297/339], [94mLoss[0m : 1.72973
[1mStep[0m  [330/339], [94mLoss[0m : 1.77700

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.790, [92mTest[0m: 2.447, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.75770
[1mStep[0m  [33/339], [94mLoss[0m : 1.81167
[1mStep[0m  [66/339], [94mLoss[0m : 1.47820
[1mStep[0m  [99/339], [94mLoss[0m : 1.73078
[1mStep[0m  [132/339], [94mLoss[0m : 1.70856
[1mStep[0m  [165/339], [94mLoss[0m : 1.99860
[1mStep[0m  [198/339], [94mLoss[0m : 1.81791
[1mStep[0m  [231/339], [94mLoss[0m : 1.73047
[1mStep[0m  [264/339], [94mLoss[0m : 1.92199
[1mStep[0m  [297/339], [94mLoss[0m : 2.11654
[1mStep[0m  [330/339], [94mLoss[0m : 1.84213

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.750, [92mTest[0m: 2.534, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.64037
[1mStep[0m  [33/339], [94mLoss[0m : 1.65464
[1mStep[0m  [66/339], [94mLoss[0m : 1.67540
[1mStep[0m  [99/339], [94mLoss[0m : 1.50774
[1mStep[0m  [132/339], [94mLoss[0m : 1.44829
[1mStep[0m  [165/339], [94mLoss[0m : 1.76546
[1mStep[0m  [198/339], [94mLoss[0m : 2.00700
[1mStep[0m  [231/339], [94mLoss[0m : 1.98614
[1mStep[0m  [264/339], [94mLoss[0m : 1.76481
[1mStep[0m  [297/339], [94mLoss[0m : 1.96232
[1mStep[0m  [330/339], [94mLoss[0m : 1.24982

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.722, [92mTest[0m: 2.487, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.78925
[1mStep[0m  [33/339], [94mLoss[0m : 1.69209
[1mStep[0m  [66/339], [94mLoss[0m : 1.77376
[1mStep[0m  [99/339], [94mLoss[0m : 1.89728
[1mStep[0m  [132/339], [94mLoss[0m : 1.82809
[1mStep[0m  [165/339], [94mLoss[0m : 1.51131
[1mStep[0m  [198/339], [94mLoss[0m : 1.86790
[1mStep[0m  [231/339], [94mLoss[0m : 1.77615
[1mStep[0m  [264/339], [94mLoss[0m : 1.83519
[1mStep[0m  [297/339], [94mLoss[0m : 1.21203
[1mStep[0m  [330/339], [94mLoss[0m : 1.59237

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.683, [92mTest[0m: 2.454, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.77260
[1mStep[0m  [33/339], [94mLoss[0m : 2.02451
[1mStep[0m  [66/339], [94mLoss[0m : 1.11535
[1mStep[0m  [99/339], [94mLoss[0m : 1.63581
[1mStep[0m  [132/339], [94mLoss[0m : 1.86184
[1mStep[0m  [165/339], [94mLoss[0m : 1.67537
[1mStep[0m  [198/339], [94mLoss[0m : 1.70074
[1mStep[0m  [231/339], [94mLoss[0m : 1.84328
[1mStep[0m  [264/339], [94mLoss[0m : 2.09799
[1mStep[0m  [297/339], [94mLoss[0m : 1.92206
[1mStep[0m  [330/339], [94mLoss[0m : 1.52235

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.680, [92mTest[0m: 2.468, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85375
[1mStep[0m  [33/339], [94mLoss[0m : 1.81200
[1mStep[0m  [66/339], [94mLoss[0m : 1.69561
[1mStep[0m  [99/339], [94mLoss[0m : 1.68195
[1mStep[0m  [132/339], [94mLoss[0m : 1.64807
[1mStep[0m  [165/339], [94mLoss[0m : 1.72215
[1mStep[0m  [198/339], [94mLoss[0m : 0.90187
[1mStep[0m  [231/339], [94mLoss[0m : 2.11391
[1mStep[0m  [264/339], [94mLoss[0m : 1.42192
[1mStep[0m  [297/339], [94mLoss[0m : 1.54226
[1mStep[0m  [330/339], [94mLoss[0m : 1.79265

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.624, [92mTest[0m: 2.453, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85831
[1mStep[0m  [33/339], [94mLoss[0m : 1.39941
[1mStep[0m  [66/339], [94mLoss[0m : 2.13997
[1mStep[0m  [99/339], [94mLoss[0m : 1.74154
[1mStep[0m  [132/339], [94mLoss[0m : 1.24539
[1mStep[0m  [165/339], [94mLoss[0m : 0.88445
[1mStep[0m  [198/339], [94mLoss[0m : 1.20239
[1mStep[0m  [231/339], [94mLoss[0m : 1.40739
[1mStep[0m  [264/339], [94mLoss[0m : 1.72645
[1mStep[0m  [297/339], [94mLoss[0m : 2.01289
[1mStep[0m  [330/339], [94mLoss[0m : 1.92185

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.594, [92mTest[0m: 2.444, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83174
[1mStep[0m  [33/339], [94mLoss[0m : 1.29138
[1mStep[0m  [66/339], [94mLoss[0m : 1.56637
[1mStep[0m  [99/339], [94mLoss[0m : 1.41454
[1mStep[0m  [132/339], [94mLoss[0m : 1.77237
[1mStep[0m  [165/339], [94mLoss[0m : 1.51926
[1mStep[0m  [198/339], [94mLoss[0m : 1.32745
[1mStep[0m  [231/339], [94mLoss[0m : 1.42637
[1mStep[0m  [264/339], [94mLoss[0m : 1.65580
[1mStep[0m  [297/339], [94mLoss[0m : 1.72919
[1mStep[0m  [330/339], [94mLoss[0m : 1.61436

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.578, [92mTest[0m: 2.488, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.34598
[1mStep[0m  [33/339], [94mLoss[0m : 2.05353
[1mStep[0m  [66/339], [94mLoss[0m : 1.67503
[1mStep[0m  [99/339], [94mLoss[0m : 1.46858
[1mStep[0m  [132/339], [94mLoss[0m : 1.36241
[1mStep[0m  [165/339], [94mLoss[0m : 1.62488
[1mStep[0m  [198/339], [94mLoss[0m : 1.73727
[1mStep[0m  [231/339], [94mLoss[0m : 1.64413
[1mStep[0m  [264/339], [94mLoss[0m : 1.72577
[1mStep[0m  [297/339], [94mLoss[0m : 1.55504
[1mStep[0m  [330/339], [94mLoss[0m : 2.16385

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.601, [92mTest[0m: 2.478, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.51494
[1mStep[0m  [33/339], [94mLoss[0m : 1.59811
[1mStep[0m  [66/339], [94mLoss[0m : 1.34954
[1mStep[0m  [99/339], [94mLoss[0m : 1.32849
[1mStep[0m  [132/339], [94mLoss[0m : 2.37430
[1mStep[0m  [165/339], [94mLoss[0m : 1.70967
[1mStep[0m  [198/339], [94mLoss[0m : 1.67156
[1mStep[0m  [231/339], [94mLoss[0m : 1.50013
[1mStep[0m  [264/339], [94mLoss[0m : 1.97160
[1mStep[0m  [297/339], [94mLoss[0m : 1.12251
[1mStep[0m  [330/339], [94mLoss[0m : 1.55740

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.564, [92mTest[0m: 2.461, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83593
[1mStep[0m  [33/339], [94mLoss[0m : 1.29312
[1mStep[0m  [66/339], [94mLoss[0m : 1.92682
[1mStep[0m  [99/339], [94mLoss[0m : 1.39525
[1mStep[0m  [132/339], [94mLoss[0m : 1.38825
[1mStep[0m  [165/339], [94mLoss[0m : 1.63108
[1mStep[0m  [198/339], [94mLoss[0m : 1.77628
[1mStep[0m  [231/339], [94mLoss[0m : 1.18091
[1mStep[0m  [264/339], [94mLoss[0m : 1.81560
[1mStep[0m  [297/339], [94mLoss[0m : 1.39223
[1mStep[0m  [330/339], [94mLoss[0m : 1.17987

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.548, [92mTest[0m: 2.506, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.52963
[1mStep[0m  [33/339], [94mLoss[0m : 1.45951
[1mStep[0m  [66/339], [94mLoss[0m : 1.33915
[1mStep[0m  [99/339], [94mLoss[0m : 1.21605
[1mStep[0m  [132/339], [94mLoss[0m : 1.81658
[1mStep[0m  [165/339], [94mLoss[0m : 1.75530
[1mStep[0m  [198/339], [94mLoss[0m : 1.71896
[1mStep[0m  [231/339], [94mLoss[0m : 1.63819
[1mStep[0m  [264/339], [94mLoss[0m : 1.49962
[1mStep[0m  [297/339], [94mLoss[0m : 1.73019
[1mStep[0m  [330/339], [94mLoss[0m : 1.70154

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.537, [92mTest[0m: 2.497, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.35817
[1mStep[0m  [33/339], [94mLoss[0m : 1.04531
[1mStep[0m  [66/339], [94mLoss[0m : 1.16321
[1mStep[0m  [99/339], [94mLoss[0m : 1.43016
[1mStep[0m  [132/339], [94mLoss[0m : 1.52885
[1mStep[0m  [165/339], [94mLoss[0m : 1.78184
[1mStep[0m  [198/339], [94mLoss[0m : 1.54179
[1mStep[0m  [231/339], [94mLoss[0m : 1.26331
[1mStep[0m  [264/339], [94mLoss[0m : 1.57266
[1mStep[0m  [297/339], [94mLoss[0m : 1.55525
[1mStep[0m  [330/339], [94mLoss[0m : 1.13678

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.500, [92mTest[0m: 2.544, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.36062
[1mStep[0m  [33/339], [94mLoss[0m : 1.81317
[1mStep[0m  [66/339], [94mLoss[0m : 1.30020
[1mStep[0m  [99/339], [94mLoss[0m : 1.57528
[1mStep[0m  [132/339], [94mLoss[0m : 1.57228
[1mStep[0m  [165/339], [94mLoss[0m : 1.32003
[1mStep[0m  [198/339], [94mLoss[0m : 1.41228
[1mStep[0m  [231/339], [94mLoss[0m : 1.51534
[1mStep[0m  [264/339], [94mLoss[0m : 1.83918
[1mStep[0m  [297/339], [94mLoss[0m : 1.96111
[1mStep[0m  [330/339], [94mLoss[0m : 1.36575

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.505, [92mTest[0m: 2.537, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.37427
[1mStep[0m  [33/339], [94mLoss[0m : 1.20044
[1mStep[0m  [66/339], [94mLoss[0m : 1.47331
[1mStep[0m  [99/339], [94mLoss[0m : 1.27989
[1mStep[0m  [132/339], [94mLoss[0m : 1.92862
[1mStep[0m  [165/339], [94mLoss[0m : 1.60283
[1mStep[0m  [198/339], [94mLoss[0m : 1.55997
[1mStep[0m  [231/339], [94mLoss[0m : 1.56853
[1mStep[0m  [264/339], [94mLoss[0m : 1.21970
[1mStep[0m  [297/339], [94mLoss[0m : 1.41350
[1mStep[0m  [330/339], [94mLoss[0m : 2.10426

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.496, [92mTest[0m: 2.528, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.473
====================================

Phase 2 - Evaluation MAE:  2.4731311302269456
MAE score P1       2.339655
MAE score P2       2.473131
loss               1.495505
learning_rate      0.007525
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             tanh
optimizer               sgd
early stopping        False
dropout                 0.2
momentum                0.5
weight_decay          0.001
Name: 13, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 10.56103
[1mStep[0m  [33/339], [94mLoss[0m : 6.18371
[1mStep[0m  [66/339], [94mLoss[0m : 3.27612
[1mStep[0m  [99/339], [94mLoss[0m : 2.00249
[1mStep[0m  [132/339], [94mLoss[0m : 3.38383
[1mStep[0m  [165/339], [94mLoss[0m : 2.71395
[1mStep[0m  [198/339], [94mLoss[0m : 2.70285
[1mStep[0m  [231/339], [94mLoss[0m : 2.95888
[1mStep[0m  [264/339], [94mLoss[0m : 2.40632
[1mStep[0m  [297/339], [94mLoss[0m : 2.65375
[1mStep[0m  [330/339], [94mLoss[0m : 2.59800

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 3.636, [92mTest[0m: 10.937, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.98352
[1mStep[0m  [33/339], [94mLoss[0m : 2.40114
[1mStep[0m  [66/339], [94mLoss[0m : 2.52608
[1mStep[0m  [99/339], [94mLoss[0m : 2.58261
[1mStep[0m  [132/339], [94mLoss[0m : 2.75740
[1mStep[0m  [165/339], [94mLoss[0m : 2.50043
[1mStep[0m  [198/339], [94mLoss[0m : 2.57300
[1mStep[0m  [231/339], [94mLoss[0m : 2.90921
[1mStep[0m  [264/339], [94mLoss[0m : 2.42962
[1mStep[0m  [297/339], [94mLoss[0m : 2.72074
[1mStep[0m  [330/339], [94mLoss[0m : 2.24203

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.669, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49778
[1mStep[0m  [33/339], [94mLoss[0m : 2.26845
[1mStep[0m  [66/339], [94mLoss[0m : 3.03253
[1mStep[0m  [99/339], [94mLoss[0m : 2.45029
[1mStep[0m  [132/339], [94mLoss[0m : 2.69001
[1mStep[0m  [165/339], [94mLoss[0m : 2.05250
[1mStep[0m  [198/339], [94mLoss[0m : 2.74150
[1mStep[0m  [231/339], [94mLoss[0m : 2.54951
[1mStep[0m  [264/339], [94mLoss[0m : 2.34586
[1mStep[0m  [297/339], [94mLoss[0m : 2.47547
[1mStep[0m  [330/339], [94mLoss[0m : 2.73285

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.587, [92mTest[0m: 2.394, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39358
[1mStep[0m  [33/339], [94mLoss[0m : 2.51281
[1mStep[0m  [66/339], [94mLoss[0m : 2.36192
[1mStep[0m  [99/339], [94mLoss[0m : 3.37439
[1mStep[0m  [132/339], [94mLoss[0m : 2.94430
[1mStep[0m  [165/339], [94mLoss[0m : 2.35634
[1mStep[0m  [198/339], [94mLoss[0m : 2.76668
[1mStep[0m  [231/339], [94mLoss[0m : 2.86196
[1mStep[0m  [264/339], [94mLoss[0m : 3.30704
[1mStep[0m  [297/339], [94mLoss[0m : 2.71675
[1mStep[0m  [330/339], [94mLoss[0m : 2.51335

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.576, [92mTest[0m: 2.390, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.33341
[1mStep[0m  [33/339], [94mLoss[0m : 2.24075
[1mStep[0m  [66/339], [94mLoss[0m : 2.09315
[1mStep[0m  [99/339], [94mLoss[0m : 2.57490
[1mStep[0m  [132/339], [94mLoss[0m : 2.25909
[1mStep[0m  [165/339], [94mLoss[0m : 2.18960
[1mStep[0m  [198/339], [94mLoss[0m : 2.91316
[1mStep[0m  [231/339], [94mLoss[0m : 2.55265
[1mStep[0m  [264/339], [94mLoss[0m : 2.80950
[1mStep[0m  [297/339], [94mLoss[0m : 2.15049
[1mStep[0m  [330/339], [94mLoss[0m : 2.51634

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.388, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17597
[1mStep[0m  [33/339], [94mLoss[0m : 2.67844
[1mStep[0m  [66/339], [94mLoss[0m : 2.36927
[1mStep[0m  [99/339], [94mLoss[0m : 2.28249
[1mStep[0m  [132/339], [94mLoss[0m : 2.55607
[1mStep[0m  [165/339], [94mLoss[0m : 2.45987
[1mStep[0m  [198/339], [94mLoss[0m : 2.45047
[1mStep[0m  [231/339], [94mLoss[0m : 2.98999
[1mStep[0m  [264/339], [94mLoss[0m : 2.38299
[1mStep[0m  [297/339], [94mLoss[0m : 2.33149
[1mStep[0m  [330/339], [94mLoss[0m : 2.16431

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.518, [92mTest[0m: 2.389, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08976
[1mStep[0m  [33/339], [94mLoss[0m : 2.23597
[1mStep[0m  [66/339], [94mLoss[0m : 2.17361
[1mStep[0m  [99/339], [94mLoss[0m : 2.55140
[1mStep[0m  [132/339], [94mLoss[0m : 2.89152
[1mStep[0m  [165/339], [94mLoss[0m : 2.14018
[1mStep[0m  [198/339], [94mLoss[0m : 2.51782
[1mStep[0m  [231/339], [94mLoss[0m : 2.16582
[1mStep[0m  [264/339], [94mLoss[0m : 2.35334
[1mStep[0m  [297/339], [94mLoss[0m : 2.78197
[1mStep[0m  [330/339], [94mLoss[0m : 2.23197

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.28677
[1mStep[0m  [33/339], [94mLoss[0m : 2.63948
[1mStep[0m  [66/339], [94mLoss[0m : 1.95632
[1mStep[0m  [99/339], [94mLoss[0m : 2.52070
[1mStep[0m  [132/339], [94mLoss[0m : 2.59104
[1mStep[0m  [165/339], [94mLoss[0m : 2.05144
[1mStep[0m  [198/339], [94mLoss[0m : 2.36887
[1mStep[0m  [231/339], [94mLoss[0m : 2.38094
[1mStep[0m  [264/339], [94mLoss[0m : 2.48737
[1mStep[0m  [297/339], [94mLoss[0m : 1.73538
[1mStep[0m  [330/339], [94mLoss[0m : 2.30021

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.386, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02211
[1mStep[0m  [33/339], [94mLoss[0m : 2.42119
[1mStep[0m  [66/339], [94mLoss[0m : 2.09967
[1mStep[0m  [99/339], [94mLoss[0m : 2.31662
[1mStep[0m  [132/339], [94mLoss[0m : 1.77950
[1mStep[0m  [165/339], [94mLoss[0m : 2.59231
[1mStep[0m  [198/339], [94mLoss[0m : 2.66768
[1mStep[0m  [231/339], [94mLoss[0m : 2.67606
[1mStep[0m  [264/339], [94mLoss[0m : 2.27331
[1mStep[0m  [297/339], [94mLoss[0m : 2.97596
[1mStep[0m  [330/339], [94mLoss[0m : 2.36465

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.96233
[1mStep[0m  [33/339], [94mLoss[0m : 2.36765
[1mStep[0m  [66/339], [94mLoss[0m : 1.82712
[1mStep[0m  [99/339], [94mLoss[0m : 3.28261
[1mStep[0m  [132/339], [94mLoss[0m : 2.61090
[1mStep[0m  [165/339], [94mLoss[0m : 3.03561
[1mStep[0m  [198/339], [94mLoss[0m : 2.76657
[1mStep[0m  [231/339], [94mLoss[0m : 2.56221
[1mStep[0m  [264/339], [94mLoss[0m : 2.50498
[1mStep[0m  [297/339], [94mLoss[0m : 2.18766
[1mStep[0m  [330/339], [94mLoss[0m : 2.92846

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.462, [92mTest[0m: 2.385, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.08020
[1mStep[0m  [33/339], [94mLoss[0m : 2.56919
[1mStep[0m  [66/339], [94mLoss[0m : 2.41924
[1mStep[0m  [99/339], [94mLoss[0m : 2.52513
[1mStep[0m  [132/339], [94mLoss[0m : 2.27655
[1mStep[0m  [165/339], [94mLoss[0m : 2.41336
[1mStep[0m  [198/339], [94mLoss[0m : 2.65695
[1mStep[0m  [231/339], [94mLoss[0m : 2.61909
[1mStep[0m  [264/339], [94mLoss[0m : 2.35027
[1mStep[0m  [297/339], [94mLoss[0m : 1.92194
[1mStep[0m  [330/339], [94mLoss[0m : 2.08925

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.316, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49499
[1mStep[0m  [33/339], [94mLoss[0m : 2.94646
[1mStep[0m  [66/339], [94mLoss[0m : 3.39456
[1mStep[0m  [99/339], [94mLoss[0m : 2.38538
[1mStep[0m  [132/339], [94mLoss[0m : 2.51625
[1mStep[0m  [165/339], [94mLoss[0m : 2.63602
[1mStep[0m  [198/339], [94mLoss[0m : 1.76655
[1mStep[0m  [231/339], [94mLoss[0m : 2.05046
[1mStep[0m  [264/339], [94mLoss[0m : 2.63500
[1mStep[0m  [297/339], [94mLoss[0m : 2.51292
[1mStep[0m  [330/339], [94mLoss[0m : 2.37087

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.445, [92mTest[0m: 2.369, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.76750
[1mStep[0m  [33/339], [94mLoss[0m : 2.25377
[1mStep[0m  [66/339], [94mLoss[0m : 2.28119
[1mStep[0m  [99/339], [94mLoss[0m : 2.41089
[1mStep[0m  [132/339], [94mLoss[0m : 2.17189
[1mStep[0m  [165/339], [94mLoss[0m : 2.54081
[1mStep[0m  [198/339], [94mLoss[0m : 2.97867
[1mStep[0m  [231/339], [94mLoss[0m : 2.54866
[1mStep[0m  [264/339], [94mLoss[0m : 2.19189
[1mStep[0m  [297/339], [94mLoss[0m : 3.07967
[1mStep[0m  [330/339], [94mLoss[0m : 2.35169

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.421, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.84516
[1mStep[0m  [33/339], [94mLoss[0m : 2.22434
[1mStep[0m  [66/339], [94mLoss[0m : 1.78080
[1mStep[0m  [99/339], [94mLoss[0m : 2.54716
[1mStep[0m  [132/339], [94mLoss[0m : 2.33510
[1mStep[0m  [165/339], [94mLoss[0m : 3.60468
[1mStep[0m  [198/339], [94mLoss[0m : 2.48698
[1mStep[0m  [231/339], [94mLoss[0m : 2.42401
[1mStep[0m  [264/339], [94mLoss[0m : 1.76094
[1mStep[0m  [297/339], [94mLoss[0m : 2.53787
[1mStep[0m  [330/339], [94mLoss[0m : 1.72844

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.38601
[1mStep[0m  [33/339], [94mLoss[0m : 2.89913
[1mStep[0m  [66/339], [94mLoss[0m : 2.77538
[1mStep[0m  [99/339], [94mLoss[0m : 2.18036
[1mStep[0m  [132/339], [94mLoss[0m : 2.23277
[1mStep[0m  [165/339], [94mLoss[0m : 1.88390
[1mStep[0m  [198/339], [94mLoss[0m : 2.39433
[1mStep[0m  [231/339], [94mLoss[0m : 2.57470
[1mStep[0m  [264/339], [94mLoss[0m : 2.12068
[1mStep[0m  [297/339], [94mLoss[0m : 2.55120
[1mStep[0m  [330/339], [94mLoss[0m : 2.05915

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.422, [92mTest[0m: 2.351, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09997
[1mStep[0m  [33/339], [94mLoss[0m : 2.06273
[1mStep[0m  [66/339], [94mLoss[0m : 2.52550
[1mStep[0m  [99/339], [94mLoss[0m : 1.88904
[1mStep[0m  [132/339], [94mLoss[0m : 1.97435
[1mStep[0m  [165/339], [94mLoss[0m : 2.35634
[1mStep[0m  [198/339], [94mLoss[0m : 2.58125
[1mStep[0m  [231/339], [94mLoss[0m : 2.46290
[1mStep[0m  [264/339], [94mLoss[0m : 1.97314
[1mStep[0m  [297/339], [94mLoss[0m : 2.05291
[1mStep[0m  [330/339], [94mLoss[0m : 2.13931

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.381, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.49378
[1mStep[0m  [33/339], [94mLoss[0m : 2.33751
[1mStep[0m  [66/339], [94mLoss[0m : 2.54110
[1mStep[0m  [99/339], [94mLoss[0m : 2.61000
[1mStep[0m  [132/339], [94mLoss[0m : 2.42124
[1mStep[0m  [165/339], [94mLoss[0m : 2.42344
[1mStep[0m  [198/339], [94mLoss[0m : 2.70268
[1mStep[0m  [231/339], [94mLoss[0m : 2.07647
[1mStep[0m  [264/339], [94mLoss[0m : 3.79920
[1mStep[0m  [297/339], [94mLoss[0m : 2.12709
[1mStep[0m  [330/339], [94mLoss[0m : 1.97272

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.382, [92mTest[0m: 2.345, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.26619
[1mStep[0m  [33/339], [94mLoss[0m : 2.10268
[1mStep[0m  [66/339], [94mLoss[0m : 2.32299
[1mStep[0m  [99/339], [94mLoss[0m : 2.56855
[1mStep[0m  [132/339], [94mLoss[0m : 2.15145
[1mStep[0m  [165/339], [94mLoss[0m : 2.77660
[1mStep[0m  [198/339], [94mLoss[0m : 2.62442
[1mStep[0m  [231/339], [94mLoss[0m : 1.93744
[1mStep[0m  [264/339], [94mLoss[0m : 2.58568
[1mStep[0m  [297/339], [94mLoss[0m : 2.56609
[1mStep[0m  [330/339], [94mLoss[0m : 1.89274

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.387, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.54226
[1mStep[0m  [33/339], [94mLoss[0m : 2.28521
[1mStep[0m  [66/339], [94mLoss[0m : 1.98324
[1mStep[0m  [99/339], [94mLoss[0m : 2.42688
[1mStep[0m  [132/339], [94mLoss[0m : 2.10805
[1mStep[0m  [165/339], [94mLoss[0m : 2.77160
[1mStep[0m  [198/339], [94mLoss[0m : 2.47230
[1mStep[0m  [231/339], [94mLoss[0m : 1.88759
[1mStep[0m  [264/339], [94mLoss[0m : 2.31059
[1mStep[0m  [297/339], [94mLoss[0m : 1.80423
[1mStep[0m  [330/339], [94mLoss[0m : 2.25384

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.362, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.37696
[1mStep[0m  [33/339], [94mLoss[0m : 2.34324
[1mStep[0m  [66/339], [94mLoss[0m : 2.38729
[1mStep[0m  [99/339], [94mLoss[0m : 2.29264
[1mStep[0m  [132/339], [94mLoss[0m : 2.63255
[1mStep[0m  [165/339], [94mLoss[0m : 2.49120
[1mStep[0m  [198/339], [94mLoss[0m : 2.29715
[1mStep[0m  [231/339], [94mLoss[0m : 2.22094
[1mStep[0m  [264/339], [94mLoss[0m : 1.92539
[1mStep[0m  [297/339], [94mLoss[0m : 2.50835
[1mStep[0m  [330/339], [94mLoss[0m : 2.75877

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.372, [92mTest[0m: 2.348, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.11060
[1mStep[0m  [33/339], [94mLoss[0m : 2.23978
[1mStep[0m  [66/339], [94mLoss[0m : 2.91431
[1mStep[0m  [99/339], [94mLoss[0m : 2.36862
[1mStep[0m  [132/339], [94mLoss[0m : 2.16329
[1mStep[0m  [165/339], [94mLoss[0m : 2.23527
[1mStep[0m  [198/339], [94mLoss[0m : 2.19693
[1mStep[0m  [231/339], [94mLoss[0m : 1.83504
[1mStep[0m  [264/339], [94mLoss[0m : 2.12615
[1mStep[0m  [297/339], [94mLoss[0m : 2.53043
[1mStep[0m  [330/339], [94mLoss[0m : 2.04549

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.00799
[1mStep[0m  [33/339], [94mLoss[0m : 2.59404
[1mStep[0m  [66/339], [94mLoss[0m : 1.97428
[1mStep[0m  [99/339], [94mLoss[0m : 2.98047
[1mStep[0m  [132/339], [94mLoss[0m : 2.24696
[1mStep[0m  [165/339], [94mLoss[0m : 2.31059
[1mStep[0m  [198/339], [94mLoss[0m : 1.94714
[1mStep[0m  [231/339], [94mLoss[0m : 2.33033
[1mStep[0m  [264/339], [94mLoss[0m : 2.69464
[1mStep[0m  [297/339], [94mLoss[0m : 2.45508
[1mStep[0m  [330/339], [94mLoss[0m : 2.57341

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.318, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.93115
[1mStep[0m  [33/339], [94mLoss[0m : 2.15653
[1mStep[0m  [66/339], [94mLoss[0m : 2.10558
[1mStep[0m  [99/339], [94mLoss[0m : 2.14168
[1mStep[0m  [132/339], [94mLoss[0m : 1.86474
[1mStep[0m  [165/339], [94mLoss[0m : 1.84581
[1mStep[0m  [198/339], [94mLoss[0m : 2.05670
[1mStep[0m  [231/339], [94mLoss[0m : 2.18729
[1mStep[0m  [264/339], [94mLoss[0m : 2.29550
[1mStep[0m  [297/339], [94mLoss[0m : 2.37723
[1mStep[0m  [330/339], [94mLoss[0m : 2.23097

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.364, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.05145
[1mStep[0m  [33/339], [94mLoss[0m : 1.67191
[1mStep[0m  [66/339], [94mLoss[0m : 2.62428
[1mStep[0m  [99/339], [94mLoss[0m : 2.11717
[1mStep[0m  [132/339], [94mLoss[0m : 2.29143
[1mStep[0m  [165/339], [94mLoss[0m : 1.92639
[1mStep[0m  [198/339], [94mLoss[0m : 2.20423
[1mStep[0m  [231/339], [94mLoss[0m : 1.97749
[1mStep[0m  [264/339], [94mLoss[0m : 2.69303
[1mStep[0m  [297/339], [94mLoss[0m : 2.32576
[1mStep[0m  [330/339], [94mLoss[0m : 1.68227

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.349, [92mTest[0m: 2.363, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09313
[1mStep[0m  [33/339], [94mLoss[0m : 2.00601
[1mStep[0m  [66/339], [94mLoss[0m : 2.31726
[1mStep[0m  [99/339], [94mLoss[0m : 2.25668
[1mStep[0m  [132/339], [94mLoss[0m : 2.70857
[1mStep[0m  [165/339], [94mLoss[0m : 2.84527
[1mStep[0m  [198/339], [94mLoss[0m : 2.16012
[1mStep[0m  [231/339], [94mLoss[0m : 2.38299
[1mStep[0m  [264/339], [94mLoss[0m : 2.12573
[1mStep[0m  [297/339], [94mLoss[0m : 2.51148
[1mStep[0m  [330/339], [94mLoss[0m : 2.64452

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.337, [92mTest[0m: 2.339, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.57083
[1mStep[0m  [33/339], [94mLoss[0m : 1.90248
[1mStep[0m  [66/339], [94mLoss[0m : 1.96571
[1mStep[0m  [99/339], [94mLoss[0m : 2.34009
[1mStep[0m  [132/339], [94mLoss[0m : 2.26011
[1mStep[0m  [165/339], [94mLoss[0m : 2.43350
[1mStep[0m  [198/339], [94mLoss[0m : 2.02306
[1mStep[0m  [231/339], [94mLoss[0m : 2.55628
[1mStep[0m  [264/339], [94mLoss[0m : 2.48452
[1mStep[0m  [297/339], [94mLoss[0m : 1.97192
[1mStep[0m  [330/339], [94mLoss[0m : 2.52849

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.339, [92mTest[0m: 2.366, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.27952
[1mStep[0m  [33/339], [94mLoss[0m : 1.68991
[1mStep[0m  [66/339], [94mLoss[0m : 2.21006
[1mStep[0m  [99/339], [94mLoss[0m : 2.78943
[1mStep[0m  [132/339], [94mLoss[0m : 1.82462
[1mStep[0m  [165/339], [94mLoss[0m : 2.69109
[1mStep[0m  [198/339], [94mLoss[0m : 2.39982
[1mStep[0m  [231/339], [94mLoss[0m : 2.34171
[1mStep[0m  [264/339], [94mLoss[0m : 2.82235
[1mStep[0m  [297/339], [94mLoss[0m : 2.25281
[1mStep[0m  [330/339], [94mLoss[0m : 2.41535

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.340, [92mTest[0m: 2.320, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.09246
[1mStep[0m  [33/339], [94mLoss[0m : 2.57303
[1mStep[0m  [66/339], [94mLoss[0m : 1.93492
[1mStep[0m  [99/339], [94mLoss[0m : 3.06824
[1mStep[0m  [132/339], [94mLoss[0m : 2.03571
[1mStep[0m  [165/339], [94mLoss[0m : 2.20542
[1mStep[0m  [198/339], [94mLoss[0m : 2.28331
[1mStep[0m  [231/339], [94mLoss[0m : 2.60275
[1mStep[0m  [264/339], [94mLoss[0m : 2.24141
[1mStep[0m  [297/339], [94mLoss[0m : 1.58002
[1mStep[0m  [330/339], [94mLoss[0m : 2.01689

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.322, [92mTest[0m: 2.354, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.80171
[1mStep[0m  [33/339], [94mLoss[0m : 1.89744
[1mStep[0m  [66/339], [94mLoss[0m : 2.13940
[1mStep[0m  [99/339], [94mLoss[0m : 2.19399
[1mStep[0m  [132/339], [94mLoss[0m : 2.59509
[1mStep[0m  [165/339], [94mLoss[0m : 3.22077
[1mStep[0m  [198/339], [94mLoss[0m : 2.10195
[1mStep[0m  [231/339], [94mLoss[0m : 1.99244
[1mStep[0m  [264/339], [94mLoss[0m : 2.13347
[1mStep[0m  [297/339], [94mLoss[0m : 2.72594
[1mStep[0m  [330/339], [94mLoss[0m : 2.65203

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.331, [92mTest[0m: 2.357, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.68042
[1mStep[0m  [33/339], [94mLoss[0m : 2.11490
[1mStep[0m  [66/339], [94mLoss[0m : 2.38124
[1mStep[0m  [99/339], [94mLoss[0m : 2.50892
[1mStep[0m  [132/339], [94mLoss[0m : 2.83796
[1mStep[0m  [165/339], [94mLoss[0m : 1.94459
[1mStep[0m  [198/339], [94mLoss[0m : 2.68076
[1mStep[0m  [231/339], [94mLoss[0m : 2.21588
[1mStep[0m  [264/339], [94mLoss[0m : 2.44120
[1mStep[0m  [297/339], [94mLoss[0m : 2.10144
[1mStep[0m  [330/339], [94mLoss[0m : 2.68709

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.325, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.356
====================================

Phase 1 - Evaluation MAE:  2.355629572826149
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): ReLU()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): ReLU()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.5
    [96mweight decay  [0m: 0.0001
[1mStep[0m  [0/339], [94mLoss[0m : 2.27893
[1mStep[0m  [33/339], [94mLoss[0m : 1.81403
[1mStep[0m  [66/339], [94mLoss[0m : 2.79388
[1mStep[0m  [99/339], [94mLoss[0m : 2.21930
[1mStep[0m  [132/339], [94mLoss[0m : 1.79313
[1mStep[0m  [165/339], [94mLoss[0m : 2.51638
[1mStep[0m  [198/339], [94mLoss[0m : 2.33438
[1mStep[0m  [231/339], [94mLoss[0m : 2.20280
[1mStep[0m  [264/339], [94mLoss[0m : 2.15216
[1mStep[0m  [297/339], [94mLoss[0m : 2.06813
[1mStep[0m  [330/339], [94mLoss[0m : 2.58498

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.496, [92mTest[0m: 2.355, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.04031
[1mStep[0m  [33/339], [94mLoss[0m : 2.60337
[1mStep[0m  [66/339], [94mLoss[0m : 2.17646
[1mStep[0m  [99/339], [94mLoss[0m : 2.40503
[1mStep[0m  [132/339], [94mLoss[0m : 2.40862
[1mStep[0m  [165/339], [94mLoss[0m : 2.50507
[1mStep[0m  [198/339], [94mLoss[0m : 2.24422
[1mStep[0m  [231/339], [94mLoss[0m : 2.10788
[1mStep[0m  [264/339], [94mLoss[0m : 2.72294
[1mStep[0m  [297/339], [94mLoss[0m : 1.71193
[1mStep[0m  [330/339], [94mLoss[0m : 2.73960

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.370, [92mTest[0m: 2.352, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17988
[1mStep[0m  [33/339], [94mLoss[0m : 2.32332
[1mStep[0m  [66/339], [94mLoss[0m : 2.73048
[1mStep[0m  [99/339], [94mLoss[0m : 2.49212
[1mStep[0m  [132/339], [94mLoss[0m : 2.54321
[1mStep[0m  [165/339], [94mLoss[0m : 1.87727
[1mStep[0m  [198/339], [94mLoss[0m : 2.13383
[1mStep[0m  [231/339], [94mLoss[0m : 3.11275
[1mStep[0m  [264/339], [94mLoss[0m : 1.71099
[1mStep[0m  [297/339], [94mLoss[0m : 2.77546
[1mStep[0m  [330/339], [94mLoss[0m : 2.09732

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.314, [92mTest[0m: 2.458, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.96652
[1mStep[0m  [33/339], [94mLoss[0m : 2.21844
[1mStep[0m  [66/339], [94mLoss[0m : 1.90529
[1mStep[0m  [99/339], [94mLoss[0m : 1.82959
[1mStep[0m  [132/339], [94mLoss[0m : 2.13924
[1mStep[0m  [165/339], [94mLoss[0m : 2.39260
[1mStep[0m  [198/339], [94mLoss[0m : 2.93696
[1mStep[0m  [231/339], [94mLoss[0m : 2.77749
[1mStep[0m  [264/339], [94mLoss[0m : 1.95582
[1mStep[0m  [297/339], [94mLoss[0m : 2.50659
[1mStep[0m  [330/339], [94mLoss[0m : 2.35465

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.233, [92mTest[0m: 2.407, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74534
[1mStep[0m  [33/339], [94mLoss[0m : 2.54765
[1mStep[0m  [66/339], [94mLoss[0m : 2.26395
[1mStep[0m  [99/339], [94mLoss[0m : 2.11878
[1mStep[0m  [132/339], [94mLoss[0m : 1.80982
[1mStep[0m  [165/339], [94mLoss[0m : 3.34165
[1mStep[0m  [198/339], [94mLoss[0m : 2.60077
[1mStep[0m  [231/339], [94mLoss[0m : 2.25006
[1mStep[0m  [264/339], [94mLoss[0m : 2.04476
[1mStep[0m  [297/339], [94mLoss[0m : 2.11554
[1mStep[0m  [330/339], [94mLoss[0m : 1.71557

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.176, [92mTest[0m: 2.354, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.91684
[1mStep[0m  [33/339], [94mLoss[0m : 2.09217
[1mStep[0m  [66/339], [94mLoss[0m : 2.19132
[1mStep[0m  [99/339], [94mLoss[0m : 1.92023
[1mStep[0m  [132/339], [94mLoss[0m : 2.25953
[1mStep[0m  [165/339], [94mLoss[0m : 2.21646
[1mStep[0m  [198/339], [94mLoss[0m : 2.13041
[1mStep[0m  [231/339], [94mLoss[0m : 2.84422
[1mStep[0m  [264/339], [94mLoss[0m : 3.11439
[1mStep[0m  [297/339], [94mLoss[0m : 2.04115
[1mStep[0m  [330/339], [94mLoss[0m : 2.63722

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.163, [92mTest[0m: 2.392, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.17111
[1mStep[0m  [33/339], [94mLoss[0m : 2.42689
[1mStep[0m  [66/339], [94mLoss[0m : 2.97031
[1mStep[0m  [99/339], [94mLoss[0m : 1.95280
[1mStep[0m  [132/339], [94mLoss[0m : 2.21486
[1mStep[0m  [165/339], [94mLoss[0m : 2.77640
[1mStep[0m  [198/339], [94mLoss[0m : 2.52449
[1mStep[0m  [231/339], [94mLoss[0m : 2.16140
[1mStep[0m  [264/339], [94mLoss[0m : 1.98454
[1mStep[0m  [297/339], [94mLoss[0m : 2.16809
[1mStep[0m  [330/339], [94mLoss[0m : 2.33127

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.079, [92mTest[0m: 2.387, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.39311
[1mStep[0m  [33/339], [94mLoss[0m : 2.35067
[1mStep[0m  [66/339], [94mLoss[0m : 2.23057
[1mStep[0m  [99/339], [94mLoss[0m : 2.04292
[1mStep[0m  [132/339], [94mLoss[0m : 1.78310
[1mStep[0m  [165/339], [94mLoss[0m : 1.95026
[1mStep[0m  [198/339], [94mLoss[0m : 1.65848
[1mStep[0m  [231/339], [94mLoss[0m : 1.67636
[1mStep[0m  [264/339], [94mLoss[0m : 1.95831
[1mStep[0m  [297/339], [94mLoss[0m : 1.51756
[1mStep[0m  [330/339], [94mLoss[0m : 2.14545

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.032, [92mTest[0m: 2.379, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.41687
[1mStep[0m  [33/339], [94mLoss[0m : 2.16862
[1mStep[0m  [66/339], [94mLoss[0m : 1.90562
[1mStep[0m  [99/339], [94mLoss[0m : 1.90689
[1mStep[0m  [132/339], [94mLoss[0m : 2.12183
[1mStep[0m  [165/339], [94mLoss[0m : 2.42206
[1mStep[0m  [198/339], [94mLoss[0m : 1.79735
[1mStep[0m  [231/339], [94mLoss[0m : 2.03859
[1mStep[0m  [264/339], [94mLoss[0m : 2.24927
[1mStep[0m  [297/339], [94mLoss[0m : 2.10752
[1mStep[0m  [330/339], [94mLoss[0m : 1.61063

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.009, [92mTest[0m: 2.438, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.43714
[1mStep[0m  [33/339], [94mLoss[0m : 2.40445
[1mStep[0m  [66/339], [94mLoss[0m : 1.59909
[1mStep[0m  [99/339], [94mLoss[0m : 1.77337
[1mStep[0m  [132/339], [94mLoss[0m : 1.53420
[1mStep[0m  [165/339], [94mLoss[0m : 1.82032
[1mStep[0m  [198/339], [94mLoss[0m : 2.08079
[1mStep[0m  [231/339], [94mLoss[0m : 1.88716
[1mStep[0m  [264/339], [94mLoss[0m : 1.56810
[1mStep[0m  [297/339], [94mLoss[0m : 1.82655
[1mStep[0m  [330/339], [94mLoss[0m : 1.71902

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 1.945, [92mTest[0m: 2.461, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.68594
[1mStep[0m  [33/339], [94mLoss[0m : 1.76098
[1mStep[0m  [66/339], [94mLoss[0m : 2.09694
[1mStep[0m  [99/339], [94mLoss[0m : 1.81578
[1mStep[0m  [132/339], [94mLoss[0m : 1.78490
[1mStep[0m  [165/339], [94mLoss[0m : 1.28436
[1mStep[0m  [198/339], [94mLoss[0m : 2.07506
[1mStep[0m  [231/339], [94mLoss[0m : 1.45776
[1mStep[0m  [264/339], [94mLoss[0m : 1.94447
[1mStep[0m  [297/339], [94mLoss[0m : 2.00780
[1mStep[0m  [330/339], [94mLoss[0m : 2.12013

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.911, [92mTest[0m: 2.442, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02474
[1mStep[0m  [33/339], [94mLoss[0m : 1.84267
[1mStep[0m  [66/339], [94mLoss[0m : 2.15945
[1mStep[0m  [99/339], [94mLoss[0m : 2.14361
[1mStep[0m  [132/339], [94mLoss[0m : 2.11186
[1mStep[0m  [165/339], [94mLoss[0m : 1.99710
[1mStep[0m  [198/339], [94mLoss[0m : 1.61457
[1mStep[0m  [231/339], [94mLoss[0m : 1.89848
[1mStep[0m  [264/339], [94mLoss[0m : 1.99952
[1mStep[0m  [297/339], [94mLoss[0m : 2.41234
[1mStep[0m  [330/339], [94mLoss[0m : 1.90474

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.883, [92mTest[0m: 2.419, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39289
[1mStep[0m  [33/339], [94mLoss[0m : 1.58166
[1mStep[0m  [66/339], [94mLoss[0m : 1.81879
[1mStep[0m  [99/339], [94mLoss[0m : 1.10921
[1mStep[0m  [132/339], [94mLoss[0m : 1.86940
[1mStep[0m  [165/339], [94mLoss[0m : 1.65556
[1mStep[0m  [198/339], [94mLoss[0m : 1.57141
[1mStep[0m  [231/339], [94mLoss[0m : 1.94176
[1mStep[0m  [264/339], [94mLoss[0m : 1.79498
[1mStep[0m  [297/339], [94mLoss[0m : 2.14831
[1mStep[0m  [330/339], [94mLoss[0m : 2.11732

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.839, [92mTest[0m: 2.483, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83116
[1mStep[0m  [33/339], [94mLoss[0m : 1.77682
[1mStep[0m  [66/339], [94mLoss[0m : 1.89749
[1mStep[0m  [99/339], [94mLoss[0m : 1.97271
[1mStep[0m  [132/339], [94mLoss[0m : 1.57692
[1mStep[0m  [165/339], [94mLoss[0m : 1.56224
[1mStep[0m  [198/339], [94mLoss[0m : 1.25854
[1mStep[0m  [231/339], [94mLoss[0m : 1.33963
[1mStep[0m  [264/339], [94mLoss[0m : 1.61712
[1mStep[0m  [297/339], [94mLoss[0m : 1.86623
[1mStep[0m  [330/339], [94mLoss[0m : 2.15402

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.836, [92mTest[0m: 2.528, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.02227
[1mStep[0m  [33/339], [94mLoss[0m : 1.47030
[1mStep[0m  [66/339], [94mLoss[0m : 1.79023
[1mStep[0m  [99/339], [94mLoss[0m : 1.98599
[1mStep[0m  [132/339], [94mLoss[0m : 1.79077
[1mStep[0m  [165/339], [94mLoss[0m : 1.44139
[1mStep[0m  [198/339], [94mLoss[0m : 2.03881
[1mStep[0m  [231/339], [94mLoss[0m : 1.69970
[1mStep[0m  [264/339], [94mLoss[0m : 1.60091
[1mStep[0m  [297/339], [94mLoss[0m : 2.01782
[1mStep[0m  [330/339], [94mLoss[0m : 1.84290

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 1.776, [92mTest[0m: 2.444, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.88708
[1mStep[0m  [33/339], [94mLoss[0m : 1.80839
[1mStep[0m  [66/339], [94mLoss[0m : 1.73171
[1mStep[0m  [99/339], [94mLoss[0m : 1.49964
[1mStep[0m  [132/339], [94mLoss[0m : 1.59176
[1mStep[0m  [165/339], [94mLoss[0m : 2.34601
[1mStep[0m  [198/339], [94mLoss[0m : 1.90490
[1mStep[0m  [231/339], [94mLoss[0m : 2.29703
[1mStep[0m  [264/339], [94mLoss[0m : 1.32786
[1mStep[0m  [297/339], [94mLoss[0m : 1.76186
[1mStep[0m  [330/339], [94mLoss[0m : 1.62120

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 1.774, [92mTest[0m: 2.548, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.83542
[1mStep[0m  [33/339], [94mLoss[0m : 2.74104
[1mStep[0m  [66/339], [94mLoss[0m : 1.66371
[1mStep[0m  [99/339], [94mLoss[0m : 2.06069
[1mStep[0m  [132/339], [94mLoss[0m : 1.73473
[1mStep[0m  [165/339], [94mLoss[0m : 1.84173
[1mStep[0m  [198/339], [94mLoss[0m : 1.61552
[1mStep[0m  [231/339], [94mLoss[0m : 2.02446
[1mStep[0m  [264/339], [94mLoss[0m : 1.34860
[1mStep[0m  [297/339], [94mLoss[0m : 1.68575
[1mStep[0m  [330/339], [94mLoss[0m : 1.73450

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 1.714, [92mTest[0m: 2.507, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.39805
[1mStep[0m  [33/339], [94mLoss[0m : 1.66724
[1mStep[0m  [66/339], [94mLoss[0m : 1.67665
[1mStep[0m  [99/339], [94mLoss[0m : 1.28604
[1mStep[0m  [132/339], [94mLoss[0m : 1.67476
[1mStep[0m  [165/339], [94mLoss[0m : 1.81507
[1mStep[0m  [198/339], [94mLoss[0m : 1.54251
[1mStep[0m  [231/339], [94mLoss[0m : 2.33089
[1mStep[0m  [264/339], [94mLoss[0m : 2.03507
[1mStep[0m  [297/339], [94mLoss[0m : 1.94723
[1mStep[0m  [330/339], [94mLoss[0m : 1.77286

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 1.695, [92mTest[0m: 2.466, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.21719
[1mStep[0m  [33/339], [94mLoss[0m : 1.92129
[1mStep[0m  [66/339], [94mLoss[0m : 1.44137
[1mStep[0m  [99/339], [94mLoss[0m : 1.96674
[1mStep[0m  [132/339], [94mLoss[0m : 1.09301
[1mStep[0m  [165/339], [94mLoss[0m : 1.55897
[1mStep[0m  [198/339], [94mLoss[0m : 2.11954
[1mStep[0m  [231/339], [94mLoss[0m : 1.89113
[1mStep[0m  [264/339], [94mLoss[0m : 1.54498
[1mStep[0m  [297/339], [94mLoss[0m : 1.76468
[1mStep[0m  [330/339], [94mLoss[0m : 1.73290

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 1.677, [92mTest[0m: 2.453, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.33607
[1mStep[0m  [33/339], [94mLoss[0m : 1.48263
[1mStep[0m  [66/339], [94mLoss[0m : 1.34453
[1mStep[0m  [99/339], [94mLoss[0m : 1.71637
[1mStep[0m  [132/339], [94mLoss[0m : 1.86130
[1mStep[0m  [165/339], [94mLoss[0m : 1.49140
[1mStep[0m  [198/339], [94mLoss[0m : 1.63425
[1mStep[0m  [231/339], [94mLoss[0m : 1.85110
[1mStep[0m  [264/339], [94mLoss[0m : 1.98738
[1mStep[0m  [297/339], [94mLoss[0m : 1.37639
[1mStep[0m  [330/339], [94mLoss[0m : 1.59800

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 1.657, [92mTest[0m: 2.522, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.85868
[1mStep[0m  [33/339], [94mLoss[0m : 1.35006
[1mStep[0m  [66/339], [94mLoss[0m : 1.69972
[1mStep[0m  [99/339], [94mLoss[0m : 2.36132
[1mStep[0m  [132/339], [94mLoss[0m : 1.60345
[1mStep[0m  [165/339], [94mLoss[0m : 1.43659
[1mStep[0m  [198/339], [94mLoss[0m : 1.15032
[1mStep[0m  [231/339], [94mLoss[0m : 1.53622
[1mStep[0m  [264/339], [94mLoss[0m : 1.55840
[1mStep[0m  [297/339], [94mLoss[0m : 1.43206
[1mStep[0m  [330/339], [94mLoss[0m : 1.72252

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 1.620, [92mTest[0m: 2.495, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.49113
[1mStep[0m  [33/339], [94mLoss[0m : 1.76554
[1mStep[0m  [66/339], [94mLoss[0m : 1.23870
[1mStep[0m  [99/339], [94mLoss[0m : 1.80877
[1mStep[0m  [132/339], [94mLoss[0m : 1.49087
[1mStep[0m  [165/339], [94mLoss[0m : 1.47183
[1mStep[0m  [198/339], [94mLoss[0m : 1.54519
[1mStep[0m  [231/339], [94mLoss[0m : 1.05778
[1mStep[0m  [264/339], [94mLoss[0m : 1.76867
[1mStep[0m  [297/339], [94mLoss[0m : 1.21815
[1mStep[0m  [330/339], [94mLoss[0m : 1.58044

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 1.579, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 0.81168
[1mStep[0m  [33/339], [94mLoss[0m : 1.16218
[1mStep[0m  [66/339], [94mLoss[0m : 1.21877
[1mStep[0m  [99/339], [94mLoss[0m : 1.32994
[1mStep[0m  [132/339], [94mLoss[0m : 2.03769
[1mStep[0m  [165/339], [94mLoss[0m : 1.47811
[1mStep[0m  [198/339], [94mLoss[0m : 1.77214
[1mStep[0m  [231/339], [94mLoss[0m : 2.01429
[1mStep[0m  [264/339], [94mLoss[0m : 1.59243
[1mStep[0m  [297/339], [94mLoss[0m : 1.57459
[1mStep[0m  [330/339], [94mLoss[0m : 2.12363

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 1.547, [92mTest[0m: 2.523, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.81659
[1mStep[0m  [33/339], [94mLoss[0m : 1.32372
[1mStep[0m  [66/339], [94mLoss[0m : 1.39833
[1mStep[0m  [99/339], [94mLoss[0m : 2.02976
[1mStep[0m  [132/339], [94mLoss[0m : 1.53213
[1mStep[0m  [165/339], [94mLoss[0m : 2.03363
[1mStep[0m  [198/339], [94mLoss[0m : 1.65012
[1mStep[0m  [231/339], [94mLoss[0m : 1.96302
[1mStep[0m  [264/339], [94mLoss[0m : 1.47428
[1mStep[0m  [297/339], [94mLoss[0m : 1.16037
[1mStep[0m  [330/339], [94mLoss[0m : 1.51209

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 1.528, [92mTest[0m: 2.533, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.74755
[1mStep[0m  [33/339], [94mLoss[0m : 1.72286
[1mStep[0m  [66/339], [94mLoss[0m : 1.78691
[1mStep[0m  [99/339], [94mLoss[0m : 1.38646
[1mStep[0m  [132/339], [94mLoss[0m : 1.58886
[1mStep[0m  [165/339], [94mLoss[0m : 1.62497
[1mStep[0m  [198/339], [94mLoss[0m : 1.16472
[1mStep[0m  [231/339], [94mLoss[0m : 1.61926
[1mStep[0m  [264/339], [94mLoss[0m : 2.07507
[1mStep[0m  [297/339], [94mLoss[0m : 1.46942
[1mStep[0m  [330/339], [94mLoss[0m : 1.32183

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 1.529, [92mTest[0m: 2.528, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.34445
[1mStep[0m  [33/339], [94mLoss[0m : 1.83585
[1mStep[0m  [66/339], [94mLoss[0m : 1.69139
[1mStep[0m  [99/339], [94mLoss[0m : 1.49799
[1mStep[0m  [132/339], [94mLoss[0m : 1.71383
[1mStep[0m  [165/339], [94mLoss[0m : 1.45715
[1mStep[0m  [198/339], [94mLoss[0m : 1.47329
[1mStep[0m  [231/339], [94mLoss[0m : 1.43364
[1mStep[0m  [264/339], [94mLoss[0m : 1.51538
[1mStep[0m  [297/339], [94mLoss[0m : 1.93064
[1mStep[0m  [330/339], [94mLoss[0m : 1.86687

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 1.500, [92mTest[0m: 2.480, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.34613
[1mStep[0m  [33/339], [94mLoss[0m : 1.54000
[1mStep[0m  [66/339], [94mLoss[0m : 1.76065
[1mStep[0m  [99/339], [94mLoss[0m : 1.41180
[1mStep[0m  [132/339], [94mLoss[0m : 1.48298
[1mStep[0m  [165/339], [94mLoss[0m : 1.79136
[1mStep[0m  [198/339], [94mLoss[0m : 1.82349
[1mStep[0m  [231/339], [94mLoss[0m : 1.34333
[1mStep[0m  [264/339], [94mLoss[0m : 1.47925
[1mStep[0m  [297/339], [94mLoss[0m : 1.79410
[1mStep[0m  [330/339], [94mLoss[0m : 1.26544

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 1.485, [92mTest[0m: 2.539, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.09394
[1mStep[0m  [33/339], [94mLoss[0m : 1.24322
[1mStep[0m  [66/339], [94mLoss[0m : 1.37546
[1mStep[0m  [99/339], [94mLoss[0m : 2.24288
[1mStep[0m  [132/339], [94mLoss[0m : 1.50092
[1mStep[0m  [165/339], [94mLoss[0m : 1.54750
[1mStep[0m  [198/339], [94mLoss[0m : 1.52834
[1mStep[0m  [231/339], [94mLoss[0m : 1.36488
[1mStep[0m  [264/339], [94mLoss[0m : 1.70390
[1mStep[0m  [297/339], [94mLoss[0m : 1.45819
[1mStep[0m  [330/339], [94mLoss[0m : 1.21225

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 1.462, [92mTest[0m: 2.481, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 2.03916
[1mStep[0m  [33/339], [94mLoss[0m : 1.14437
[1mStep[0m  [66/339], [94mLoss[0m : 1.71727
[1mStep[0m  [99/339], [94mLoss[0m : 1.27646
[1mStep[0m  [132/339], [94mLoss[0m : 1.40240
[1mStep[0m  [165/339], [94mLoss[0m : 1.57449
[1mStep[0m  [198/339], [94mLoss[0m : 1.14887
[1mStep[0m  [231/339], [94mLoss[0m : 1.19376
[1mStep[0m  [264/339], [94mLoss[0m : 1.79751
[1mStep[0m  [297/339], [94mLoss[0m : 1.65506
[1mStep[0m  [330/339], [94mLoss[0m : 1.64390

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 1.446, [92mTest[0m: 2.508, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/339], [94mLoss[0m : 1.59325
[1mStep[0m  [33/339], [94mLoss[0m : 1.26560
[1mStep[0m  [66/339], [94mLoss[0m : 1.08357
[1mStep[0m  [99/339], [94mLoss[0m : 1.32354
[1mStep[0m  [132/339], [94mLoss[0m : 1.17996
[1mStep[0m  [165/339], [94mLoss[0m : 1.25662
[1mStep[0m  [198/339], [94mLoss[0m : 1.18388
[1mStep[0m  [231/339], [94mLoss[0m : 1.61857
[1mStep[0m  [264/339], [94mLoss[0m : 1.87951
[1mStep[0m  [297/339], [94mLoss[0m : 0.98634
[1mStep[0m  [330/339], [94mLoss[0m : 1.53770

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 1.425, [92mTest[0m: 2.484, [96mlr[0m: 0.006772500000000002
====================================

====================================
[93mEarly stopping was triggerd[0m: epoch 29 from 30
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.500
====================================

Phase 2 - Evaluation MAE:  2.499894155865222
MAE score P1        2.35563
MAE score P2       2.499894
loss               1.424672
learning_rate      0.007525
batch_size               32
hidden_sizes      [250, 50]
epochs                   30
activation             relu
optimizer               sgd
early stopping         True
dropout                 0.2
momentum                0.5
weight_decay         0.0001
Name: 14, dtype: object
Following models where found:

dict_keys(['whole_spec_quantile', 'delta_quantile', 'beta_quantile', 'theta_quantile', 'alpha_quantile'])

Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 10.39181
[1mStep[0m  [16/169], [94mLoss[0m : 9.21368
[1mStep[0m  [32/169], [94mLoss[0m : 7.58655
[1mStep[0m  [48/169], [94mLoss[0m : 5.33149
[1mStep[0m  [64/169], [94mLoss[0m : 3.97808
[1mStep[0m  [80/169], [94mLoss[0m : 3.78488
[1mStep[0m  [96/169], [94mLoss[0m : 2.02815
[1mStep[0m  [112/169], [94mLoss[0m : 2.42707
[1mStep[0m  [128/169], [94mLoss[0m : 2.90456
[1mStep[0m  [144/169], [94mLoss[0m : 2.86594
[1mStep[0m  [160/169], [94mLoss[0m : 2.18778

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 4.575, [92mTest[0m: 10.830, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55307
[1mStep[0m  [16/169], [94mLoss[0m : 2.70179
[1mStep[0m  [32/169], [94mLoss[0m : 2.42887
[1mStep[0m  [48/169], [94mLoss[0m : 2.37085
[1mStep[0m  [64/169], [94mLoss[0m : 2.33494
[1mStep[0m  [80/169], [94mLoss[0m : 2.55233
[1mStep[0m  [96/169], [94mLoss[0m : 2.64958
[1mStep[0m  [112/169], [94mLoss[0m : 2.56910
[1mStep[0m  [128/169], [94mLoss[0m : 2.67213
[1mStep[0m  [144/169], [94mLoss[0m : 2.11014
[1mStep[0m  [160/169], [94mLoss[0m : 2.32137

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.545, [92mTest[0m: 2.416, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51621
[1mStep[0m  [16/169], [94mLoss[0m : 2.02119
[1mStep[0m  [32/169], [94mLoss[0m : 2.71943
[1mStep[0m  [48/169], [94mLoss[0m : 2.38718
[1mStep[0m  [64/169], [94mLoss[0m : 2.42330
[1mStep[0m  [80/169], [94mLoss[0m : 2.74182
[1mStep[0m  [96/169], [94mLoss[0m : 2.72268
[1mStep[0m  [112/169], [94mLoss[0m : 2.53453
[1mStep[0m  [128/169], [94mLoss[0m : 2.34040
[1mStep[0m  [144/169], [94mLoss[0m : 2.92866
[1mStep[0m  [160/169], [94mLoss[0m : 2.28244

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.523, [92mTest[0m: 2.361, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18080
[1mStep[0m  [16/169], [94mLoss[0m : 2.89614
[1mStep[0m  [32/169], [94mLoss[0m : 2.90811
[1mStep[0m  [48/169], [94mLoss[0m : 2.68226
[1mStep[0m  [64/169], [94mLoss[0m : 2.36891
[1mStep[0m  [80/169], [94mLoss[0m : 2.28964
[1mStep[0m  [96/169], [94mLoss[0m : 2.11397
[1mStep[0m  [112/169], [94mLoss[0m : 2.35633
[1mStep[0m  [128/169], [94mLoss[0m : 2.38387
[1mStep[0m  [144/169], [94mLoss[0m : 2.68358
[1mStep[0m  [160/169], [94mLoss[0m : 2.72569

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.527, [92mTest[0m: 2.343, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.61037
[1mStep[0m  [16/169], [94mLoss[0m : 2.73420
[1mStep[0m  [32/169], [94mLoss[0m : 2.57683
[1mStep[0m  [48/169], [94mLoss[0m : 2.55204
[1mStep[0m  [64/169], [94mLoss[0m : 2.12868
[1mStep[0m  [80/169], [94mLoss[0m : 2.82663
[1mStep[0m  [96/169], [94mLoss[0m : 2.75274
[1mStep[0m  [112/169], [94mLoss[0m : 2.61382
[1mStep[0m  [128/169], [94mLoss[0m : 2.87507
[1mStep[0m  [144/169], [94mLoss[0m : 2.01983
[1mStep[0m  [160/169], [94mLoss[0m : 2.58383

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.43641
[1mStep[0m  [16/169], [94mLoss[0m : 2.11021
[1mStep[0m  [32/169], [94mLoss[0m : 2.81154
[1mStep[0m  [48/169], [94mLoss[0m : 2.52722
[1mStep[0m  [64/169], [94mLoss[0m : 2.38735
[1mStep[0m  [80/169], [94mLoss[0m : 2.21790
[1mStep[0m  [96/169], [94mLoss[0m : 2.79208
[1mStep[0m  [112/169], [94mLoss[0m : 2.55930
[1mStep[0m  [128/169], [94mLoss[0m : 2.02075
[1mStep[0m  [144/169], [94mLoss[0m : 2.13102
[1mStep[0m  [160/169], [94mLoss[0m : 2.71208

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.501, [92mTest[0m: 2.334, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34077
[1mStep[0m  [16/169], [94mLoss[0m : 2.20157
[1mStep[0m  [32/169], [94mLoss[0m : 2.57391
[1mStep[0m  [48/169], [94mLoss[0m : 1.93202
[1mStep[0m  [64/169], [94mLoss[0m : 2.61438
[1mStep[0m  [80/169], [94mLoss[0m : 2.50447
[1mStep[0m  [96/169], [94mLoss[0m : 2.61356
[1mStep[0m  [112/169], [94mLoss[0m : 2.47951
[1mStep[0m  [128/169], [94mLoss[0m : 2.92899
[1mStep[0m  [144/169], [94mLoss[0m : 2.54063
[1mStep[0m  [160/169], [94mLoss[0m : 2.72892

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.513, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.64373
[1mStep[0m  [16/169], [94mLoss[0m : 2.31231
[1mStep[0m  [32/169], [94mLoss[0m : 2.22156
[1mStep[0m  [48/169], [94mLoss[0m : 2.49115
[1mStep[0m  [64/169], [94mLoss[0m : 2.76217
[1mStep[0m  [80/169], [94mLoss[0m : 2.65655
[1mStep[0m  [96/169], [94mLoss[0m : 2.85970
[1mStep[0m  [112/169], [94mLoss[0m : 2.68975
[1mStep[0m  [128/169], [94mLoss[0m : 2.61692
[1mStep[0m  [144/169], [94mLoss[0m : 2.68644
[1mStep[0m  [160/169], [94mLoss[0m : 2.38459

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.506, [92mTest[0m: 2.323, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.55195
[1mStep[0m  [16/169], [94mLoss[0m : 3.01634
[1mStep[0m  [32/169], [94mLoss[0m : 2.67659
[1mStep[0m  [48/169], [94mLoss[0m : 2.22833
[1mStep[0m  [64/169], [94mLoss[0m : 2.09462
[1mStep[0m  [80/169], [94mLoss[0m : 2.83347
[1mStep[0m  [96/169], [94mLoss[0m : 2.29362
[1mStep[0m  [112/169], [94mLoss[0m : 2.77304
[1mStep[0m  [128/169], [94mLoss[0m : 2.83668
[1mStep[0m  [144/169], [94mLoss[0m : 2.39274
[1mStep[0m  [160/169], [94mLoss[0m : 2.38268

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.333, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.23025
[1mStep[0m  [16/169], [94mLoss[0m : 2.31238
[1mStep[0m  [32/169], [94mLoss[0m : 2.38249
[1mStep[0m  [48/169], [94mLoss[0m : 2.47363
[1mStep[0m  [64/169], [94mLoss[0m : 2.72382
[1mStep[0m  [80/169], [94mLoss[0m : 3.17439
[1mStep[0m  [96/169], [94mLoss[0m : 2.49601
[1mStep[0m  [112/169], [94mLoss[0m : 2.75341
[1mStep[0m  [128/169], [94mLoss[0m : 2.72684
[1mStep[0m  [144/169], [94mLoss[0m : 2.41597
[1mStep[0m  [160/169], [94mLoss[0m : 2.26494

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.477, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.44027
[1mStep[0m  [16/169], [94mLoss[0m : 2.65609
[1mStep[0m  [32/169], [94mLoss[0m : 2.27461
[1mStep[0m  [48/169], [94mLoss[0m : 2.61755
[1mStep[0m  [64/169], [94mLoss[0m : 2.49921
[1mStep[0m  [80/169], [94mLoss[0m : 3.04174
[1mStep[0m  [96/169], [94mLoss[0m : 2.94631
[1mStep[0m  [112/169], [94mLoss[0m : 2.57069
[1mStep[0m  [128/169], [94mLoss[0m : 2.39800
[1mStep[0m  [144/169], [94mLoss[0m : 2.16599
[1mStep[0m  [160/169], [94mLoss[0m : 2.58483

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 2.497, [92mTest[0m: 2.331, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.21521
[1mStep[0m  [16/169], [94mLoss[0m : 2.37838
[1mStep[0m  [32/169], [94mLoss[0m : 2.49935
[1mStep[0m  [48/169], [94mLoss[0m : 2.86936
[1mStep[0m  [64/169], [94mLoss[0m : 2.51488
[1mStep[0m  [80/169], [94mLoss[0m : 2.07041
[1mStep[0m  [96/169], [94mLoss[0m : 2.71891
[1mStep[0m  [112/169], [94mLoss[0m : 2.20729
[1mStep[0m  [128/169], [94mLoss[0m : 2.26783
[1mStep[0m  [144/169], [94mLoss[0m : 2.49966
[1mStep[0m  [160/169], [94mLoss[0m : 2.97243

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 2.485, [92mTest[0m: 2.318, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.28462
[1mStep[0m  [16/169], [94mLoss[0m : 2.37926
[1mStep[0m  [32/169], [94mLoss[0m : 2.51242
[1mStep[0m  [48/169], [94mLoss[0m : 2.66849
[1mStep[0m  [64/169], [94mLoss[0m : 2.54499
[1mStep[0m  [80/169], [94mLoss[0m : 2.27976
[1mStep[0m  [96/169], [94mLoss[0m : 2.43112
[1mStep[0m  [112/169], [94mLoss[0m : 2.13755
[1mStep[0m  [128/169], [94mLoss[0m : 2.33043
[1mStep[0m  [144/169], [94mLoss[0m : 2.43880
[1mStep[0m  [160/169], [94mLoss[0m : 2.58535

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 2.472, [92mTest[0m: 2.325, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.48116
[1mStep[0m  [16/169], [94mLoss[0m : 2.74639
[1mStep[0m  [32/169], [94mLoss[0m : 2.68330
[1mStep[0m  [48/169], [94mLoss[0m : 2.32442
[1mStep[0m  [64/169], [94mLoss[0m : 2.34086
[1mStep[0m  [80/169], [94mLoss[0m : 2.26442
[1mStep[0m  [96/169], [94mLoss[0m : 2.49080
[1mStep[0m  [112/169], [94mLoss[0m : 2.40223
[1mStep[0m  [128/169], [94mLoss[0m : 2.79577
[1mStep[0m  [144/169], [94mLoss[0m : 2.23215
[1mStep[0m  [160/169], [94mLoss[0m : 2.36493

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 2.487, [92mTest[0m: 2.344, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.53640
[1mStep[0m  [16/169], [94mLoss[0m : 2.32058
[1mStep[0m  [32/169], [94mLoss[0m : 2.41948
[1mStep[0m  [48/169], [94mLoss[0m : 1.95808
[1mStep[0m  [64/169], [94mLoss[0m : 2.84645
[1mStep[0m  [80/169], [94mLoss[0m : 2.43207
[1mStep[0m  [96/169], [94mLoss[0m : 1.92740
[1mStep[0m  [112/169], [94mLoss[0m : 2.62702
[1mStep[0m  [128/169], [94mLoss[0m : 2.53236
[1mStep[0m  [144/169], [94mLoss[0m : 2.21539
[1mStep[0m  [160/169], [94mLoss[0m : 2.41116

====================================
[1mEpoch[0m [14/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.330, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.15850
[1mStep[0m  [16/169], [94mLoss[0m : 2.40675
[1mStep[0m  [32/169], [94mLoss[0m : 2.46915
[1mStep[0m  [48/169], [94mLoss[0m : 2.65994
[1mStep[0m  [64/169], [94mLoss[0m : 2.33399
[1mStep[0m  [80/169], [94mLoss[0m : 2.58972
[1mStep[0m  [96/169], [94mLoss[0m : 2.35330
[1mStep[0m  [112/169], [94mLoss[0m : 2.61652
[1mStep[0m  [128/169], [94mLoss[0m : 2.26808
[1mStep[0m  [144/169], [94mLoss[0m : 2.45659
[1mStep[0m  [160/169], [94mLoss[0m : 1.91482

====================================
[1mEpoch[0m [15/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.321, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.33129
[1mStep[0m  [16/169], [94mLoss[0m : 2.80851
[1mStep[0m  [32/169], [94mLoss[0m : 2.52810
[1mStep[0m  [48/169], [94mLoss[0m : 2.72464
[1mStep[0m  [64/169], [94mLoss[0m : 2.14494
[1mStep[0m  [80/169], [94mLoss[0m : 2.74186
[1mStep[0m  [96/169], [94mLoss[0m : 2.89464
[1mStep[0m  [112/169], [94mLoss[0m : 3.16039
[1mStep[0m  [128/169], [94mLoss[0m : 2.59264
[1mStep[0m  [144/169], [94mLoss[0m : 2.52875
[1mStep[0m  [160/169], [94mLoss[0m : 2.35365

====================================
[1mEpoch[0m [16/30], [94mTrain[0m: 2.479, [92mTest[0m: 2.335, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.38750
[1mStep[0m  [16/169], [94mLoss[0m : 2.65912
[1mStep[0m  [32/169], [94mLoss[0m : 2.22495
[1mStep[0m  [48/169], [94mLoss[0m : 2.67034
[1mStep[0m  [64/169], [94mLoss[0m : 2.37716
[1mStep[0m  [80/169], [94mLoss[0m : 2.81142
[1mStep[0m  [96/169], [94mLoss[0m : 2.06930
[1mStep[0m  [112/169], [94mLoss[0m : 2.44088
[1mStep[0m  [128/169], [94mLoss[0m : 1.94008
[1mStep[0m  [144/169], [94mLoss[0m : 2.48081
[1mStep[0m  [160/169], [94mLoss[0m : 2.64435

====================================
[1mEpoch[0m [17/30], [94mTrain[0m: 2.476, [92mTest[0m: 2.341, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.62531
[1mStep[0m  [16/169], [94mLoss[0m : 2.41635
[1mStep[0m  [32/169], [94mLoss[0m : 2.21863
[1mStep[0m  [48/169], [94mLoss[0m : 2.28013
[1mStep[0m  [64/169], [94mLoss[0m : 2.67641
[1mStep[0m  [80/169], [94mLoss[0m : 2.56307
[1mStep[0m  [96/169], [94mLoss[0m : 2.52746
[1mStep[0m  [112/169], [94mLoss[0m : 2.28995
[1mStep[0m  [128/169], [94mLoss[0m : 2.28762
[1mStep[0m  [144/169], [94mLoss[0m : 2.38234
[1mStep[0m  [160/169], [94mLoss[0m : 2.22850

====================================
[1mEpoch[0m [18/30], [94mTrain[0m: 2.460, [92mTest[0m: 2.338, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.40570
[1mStep[0m  [16/169], [94mLoss[0m : 2.18860
[1mStep[0m  [32/169], [94mLoss[0m : 2.42292
[1mStep[0m  [48/169], [94mLoss[0m : 2.40137
[1mStep[0m  [64/169], [94mLoss[0m : 2.42681
[1mStep[0m  [80/169], [94mLoss[0m : 2.49415
[1mStep[0m  [96/169], [94mLoss[0m : 2.10723
[1mStep[0m  [112/169], [94mLoss[0m : 2.32370
[1mStep[0m  [128/169], [94mLoss[0m : 2.10571
[1mStep[0m  [144/169], [94mLoss[0m : 2.61015
[1mStep[0m  [160/169], [94mLoss[0m : 2.76088

====================================
[1mEpoch[0m [19/30], [94mTrain[0m: 2.489, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.65360
[1mStep[0m  [16/169], [94mLoss[0m : 2.86175
[1mStep[0m  [32/169], [94mLoss[0m : 3.16685
[1mStep[0m  [48/169], [94mLoss[0m : 2.61232
[1mStep[0m  [64/169], [94mLoss[0m : 2.29744
[1mStep[0m  [80/169], [94mLoss[0m : 2.34502
[1mStep[0m  [96/169], [94mLoss[0m : 2.26419
[1mStep[0m  [112/169], [94mLoss[0m : 2.40121
[1mStep[0m  [128/169], [94mLoss[0m : 2.44440
[1mStep[0m  [144/169], [94mLoss[0m : 2.74324
[1mStep[0m  [160/169], [94mLoss[0m : 2.47057

====================================
[1mEpoch[0m [20/30], [94mTrain[0m: 2.478, [92mTest[0m: 2.330, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.01658
[1mStep[0m  [16/169], [94mLoss[0m : 2.40276
[1mStep[0m  [32/169], [94mLoss[0m : 2.06809
[1mStep[0m  [48/169], [94mLoss[0m : 2.31725
[1mStep[0m  [64/169], [94mLoss[0m : 2.71523
[1mStep[0m  [80/169], [94mLoss[0m : 2.02938
[1mStep[0m  [96/169], [94mLoss[0m : 2.63158
[1mStep[0m  [112/169], [94mLoss[0m : 2.66665
[1mStep[0m  [128/169], [94mLoss[0m : 2.38514
[1mStep[0m  [144/169], [94mLoss[0m : 2.73546
[1mStep[0m  [160/169], [94mLoss[0m : 2.38637

====================================
[1mEpoch[0m [21/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.323, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.76580
[1mStep[0m  [16/169], [94mLoss[0m : 2.49332
[1mStep[0m  [32/169], [94mLoss[0m : 2.79516
[1mStep[0m  [48/169], [94mLoss[0m : 2.16132
[1mStep[0m  [64/169], [94mLoss[0m : 2.74813
[1mStep[0m  [80/169], [94mLoss[0m : 2.20751
[1mStep[0m  [96/169], [94mLoss[0m : 2.64067
[1mStep[0m  [112/169], [94mLoss[0m : 2.18675
[1mStep[0m  [128/169], [94mLoss[0m : 3.06365
[1mStep[0m  [144/169], [94mLoss[0m : 2.47847
[1mStep[0m  [160/169], [94mLoss[0m : 2.40755

====================================
[1mEpoch[0m [22/30], [94mTrain[0m: 2.471, [92mTest[0m: 2.329, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.41460
[1mStep[0m  [16/169], [94mLoss[0m : 2.37668
[1mStep[0m  [32/169], [94mLoss[0m : 2.57133
[1mStep[0m  [48/169], [94mLoss[0m : 2.50763
[1mStep[0m  [64/169], [94mLoss[0m : 2.64598
[1mStep[0m  [80/169], [94mLoss[0m : 2.48733
[1mStep[0m  [96/169], [94mLoss[0m : 2.29148
[1mStep[0m  [112/169], [94mLoss[0m : 2.39035
[1mStep[0m  [128/169], [94mLoss[0m : 2.40354
[1mStep[0m  [144/169], [94mLoss[0m : 2.70286
[1mStep[0m  [160/169], [94mLoss[0m : 2.30998

====================================
[1mEpoch[0m [23/30], [94mTrain[0m: 2.469, [92mTest[0m: 2.332, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.51571
[1mStep[0m  [16/169], [94mLoss[0m : 2.87944
[1mStep[0m  [32/169], [94mLoss[0m : 2.61250
[1mStep[0m  [48/169], [94mLoss[0m : 2.23842
[1mStep[0m  [64/169], [94mLoss[0m : 2.61088
[1mStep[0m  [80/169], [94mLoss[0m : 2.56246
[1mStep[0m  [96/169], [94mLoss[0m : 2.13244
[1mStep[0m  [112/169], [94mLoss[0m : 2.55082
[1mStep[0m  [128/169], [94mLoss[0m : 2.56740
[1mStep[0m  [144/169], [94mLoss[0m : 2.36303
[1mStep[0m  [160/169], [94mLoss[0m : 2.59131

====================================
[1mEpoch[0m [24/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.34985
[1mStep[0m  [16/169], [94mLoss[0m : 2.01498
[1mStep[0m  [32/169], [94mLoss[0m : 2.33956
[1mStep[0m  [48/169], [94mLoss[0m : 1.91805
[1mStep[0m  [64/169], [94mLoss[0m : 2.20828
[1mStep[0m  [80/169], [94mLoss[0m : 2.55436
[1mStep[0m  [96/169], [94mLoss[0m : 2.12768
[1mStep[0m  [112/169], [94mLoss[0m : 2.65658
[1mStep[0m  [128/169], [94mLoss[0m : 2.41668
[1mStep[0m  [144/169], [94mLoss[0m : 2.27767
[1mStep[0m  [160/169], [94mLoss[0m : 2.60212

====================================
[1mEpoch[0m [25/30], [94mTrain[0m: 2.466, [92mTest[0m: 2.333, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.18388
[1mStep[0m  [16/169], [94mLoss[0m : 2.56539
[1mStep[0m  [32/169], [94mLoss[0m : 2.60995
[1mStep[0m  [48/169], [94mLoss[0m : 2.64161
[1mStep[0m  [64/169], [94mLoss[0m : 2.58006
[1mStep[0m  [80/169], [94mLoss[0m : 2.46815
[1mStep[0m  [96/169], [94mLoss[0m : 2.55085
[1mStep[0m  [112/169], [94mLoss[0m : 2.34024
[1mStep[0m  [128/169], [94mLoss[0m : 2.50650
[1mStep[0m  [144/169], [94mLoss[0m : 2.46403
[1mStep[0m  [160/169], [94mLoss[0m : 2.22782

====================================
[1mEpoch[0m [26/30], [94mTrain[0m: 2.459, [92mTest[0m: 2.359, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19289
[1mStep[0m  [16/169], [94mLoss[0m : 2.37837
[1mStep[0m  [32/169], [94mLoss[0m : 2.69551
[1mStep[0m  [48/169], [94mLoss[0m : 2.08360
[1mStep[0m  [64/169], [94mLoss[0m : 2.69588
[1mStep[0m  [80/169], [94mLoss[0m : 2.49369
[1mStep[0m  [96/169], [94mLoss[0m : 2.27860
[1mStep[0m  [112/169], [94mLoss[0m : 2.65840
[1mStep[0m  [128/169], [94mLoss[0m : 2.72305
[1mStep[0m  [144/169], [94mLoss[0m : 2.35894
[1mStep[0m  [160/169], [94mLoss[0m : 2.78578

====================================
[1mEpoch[0m [27/30], [94mTrain[0m: 2.475, [92mTest[0m: 2.326, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.45462
[1mStep[0m  [16/169], [94mLoss[0m : 2.44816
[1mStep[0m  [32/169], [94mLoss[0m : 2.54699
[1mStep[0m  [48/169], [94mLoss[0m : 2.19487
[1mStep[0m  [64/169], [94mLoss[0m : 1.92935
[1mStep[0m  [80/169], [94mLoss[0m : 2.53410
[1mStep[0m  [96/169], [94mLoss[0m : 2.33758
[1mStep[0m  [112/169], [94mLoss[0m : 2.80998
[1mStep[0m  [128/169], [94mLoss[0m : 2.23481
[1mStep[0m  [144/169], [94mLoss[0m : 2.53720
[1mStep[0m  [160/169], [94mLoss[0m : 2.26348

====================================
[1mEpoch[0m [28/30], [94mTrain[0m: 2.467, [92mTest[0m: 2.342, [96mlr[0m: 0.006772500000000002
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.50576
[1mStep[0m  [16/169], [94mLoss[0m : 2.57305
[1mStep[0m  [32/169], [94mLoss[0m : 2.49443
[1mStep[0m  [48/169], [94mLoss[0m : 2.62039
[1mStep[0m  [64/169], [94mLoss[0m : 2.32114
[1mStep[0m  [80/169], [94mLoss[0m : 2.57757
[1mStep[0m  [96/169], [94mLoss[0m : 2.66119
[1mStep[0m  [112/169], [94mLoss[0m : 2.76656
[1mStep[0m  [128/169], [94mLoss[0m : 2.38534
[1mStep[0m  [144/169], [94mLoss[0m : 2.66835
[1mStep[0m  [160/169], [94mLoss[0m : 2.78482

====================================
[1mEpoch[0m [29/30], [94mTrain[0m: 2.465, [92mTest[0m: 2.335, [96mlr[0m: 0.006772500000000002
====================================

====================================
[92mFinal Evaluation MAE Loss[0m: 2.331
====================================

Phase 1 - Evaluation MAE:  2.3312508357422694
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
Running on device: cuda
Model Architecture:
FeatureImitationNetwork(
  (_FeatureImitationNetwork__stage_1): ModuleDict(
    (whole_spec_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (delta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (beta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (theta_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (alpha_quantile): DynamicNeuralNetwork(
      (model_arch): ModuleList(
        (0): Sequential(
          (in_linear): Linear(in_features=2500, out_features=6000, bias=True)
          (in_batch norm): BatchNorm1d(6000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (in_activation): ReLU()
          (in_dropout): Dropout(p=0.2, inplace=False)
        )
        (1): Sequential(
          (0_linear): Linear(in_features=6000, out_features=2000, bias=True)
          (0_batch norm): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (0_activation): ReLU()
          (0_dropout): Dropout(p=0.2, inplace=False)
        )
        (2): Sequential(
          (1_linear): Linear(in_features=2000, out_features=100, bias=True)
          (1_batch norm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1_activation): ReLU()
          (1_dropout): Dropout(p=0.2, inplace=False)
        )
      )
    )
  )
  (_FeatureImitationNetwork__stage_2): DynamicNeuralNetwork(
    (model_arch): ModuleList(
      (0): Sequential(
        (in_linear): Linear(in_features=500, out_features=250, bias=True)
        (in_batch norm): BatchNorm1d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (in_activation): Sigmoid()
        (in_dropout): Dropout(p=0.2, inplace=False)
      )
      (1): Sequential(
        (0_linear): Linear(in_features=250, out_features=50, bias=True)
        (0_batch norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (0_activation): Sigmoid()
        (0_dropout): Dropout(p=0.2, inplace=False)
      )
      (2): Sequential(
        (out_linear): Linear(in_features=50, out_features=1, bias=True)
      )
    )
  )
)
[94mOptimizer Parameters:[0m
    [96mlearning rate [0m: 0.007525000000000001
    [96mmomentum      [0m: 0.1
    [96mweight decay  [0m: 0.001
[1mStep[0m  [0/169], [94mLoss[0m : 2.75456
[1mStep[0m  [16/169], [94mLoss[0m : 2.37557
[1mStep[0m  [32/169], [94mLoss[0m : 2.80459
[1mStep[0m  [48/169], [94mLoss[0m : 2.73014
[1mStep[0m  [64/169], [94mLoss[0m : 2.48235
[1mStep[0m  [80/169], [94mLoss[0m : 2.68571
[1mStep[0m  [96/169], [94mLoss[0m : 1.97550
[1mStep[0m  [112/169], [94mLoss[0m : 2.25061
[1mStep[0m  [128/169], [94mLoss[0m : 2.62114
[1mStep[0m  [144/169], [94mLoss[0m : 2.53797
[1mStep[0m  [160/169], [94mLoss[0m : 2.13484

====================================
[1mEpoch[0m [0/30], [94mTrain[0m: 2.488, [92mTest[0m: 2.332, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.86583
[1mStep[0m  [16/169], [94mLoss[0m : 2.09764
[1mStep[0m  [32/169], [94mLoss[0m : 2.83407
[1mStep[0m  [48/169], [94mLoss[0m : 2.55205
[1mStep[0m  [64/169], [94mLoss[0m : 2.10275
[1mStep[0m  [80/169], [94mLoss[0m : 2.19115
[1mStep[0m  [96/169], [94mLoss[0m : 2.68187
[1mStep[0m  [112/169], [94mLoss[0m : 2.32952
[1mStep[0m  [128/169], [94mLoss[0m : 2.41633
[1mStep[0m  [144/169], [94mLoss[0m : 2.12716
[1mStep[0m  [160/169], [94mLoss[0m : 2.21886

====================================
[1mEpoch[0m [1/30], [94mTrain[0m: 2.417, [92mTest[0m: 2.561, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.20283
[1mStep[0m  [16/169], [94mLoss[0m : 2.58444
[1mStep[0m  [32/169], [94mLoss[0m : 2.08556
[1mStep[0m  [48/169], [94mLoss[0m : 2.26865
[1mStep[0m  [64/169], [94mLoss[0m : 2.12929
[1mStep[0m  [80/169], [94mLoss[0m : 2.44743
[1mStep[0m  [96/169], [94mLoss[0m : 2.22899
[1mStep[0m  [112/169], [94mLoss[0m : 2.65738
[1mStep[0m  [128/169], [94mLoss[0m : 2.55285
[1mStep[0m  [144/169], [94mLoss[0m : 2.78849
[1mStep[0m  [160/169], [94mLoss[0m : 2.41742

====================================
[1mEpoch[0m [2/30], [94mTrain[0m: 2.352, [92mTest[0m: 2.539, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85697
[1mStep[0m  [16/169], [94mLoss[0m : 2.68674
[1mStep[0m  [32/169], [94mLoss[0m : 2.70918
[1mStep[0m  [48/169], [94mLoss[0m : 2.36118
[1mStep[0m  [64/169], [94mLoss[0m : 2.08820
[1mStep[0m  [80/169], [94mLoss[0m : 2.43559
[1mStep[0m  [96/169], [94mLoss[0m : 2.09145
[1mStep[0m  [112/169], [94mLoss[0m : 2.39294
[1mStep[0m  [128/169], [94mLoss[0m : 2.23143
[1mStep[0m  [144/169], [94mLoss[0m : 2.30810
[1mStep[0m  [160/169], [94mLoss[0m : 2.65080

====================================
[1mEpoch[0m [3/30], [94mTrain[0m: 2.317, [92mTest[0m: 2.462, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.26785
[1mStep[0m  [16/169], [94mLoss[0m : 2.52576
[1mStep[0m  [32/169], [94mLoss[0m : 2.12618
[1mStep[0m  [48/169], [94mLoss[0m : 2.62858
[1mStep[0m  [64/169], [94mLoss[0m : 1.97933
[1mStep[0m  [80/169], [94mLoss[0m : 2.10112
[1mStep[0m  [96/169], [94mLoss[0m : 2.33812
[1mStep[0m  [112/169], [94mLoss[0m : 1.93715
[1mStep[0m  [128/169], [94mLoss[0m : 2.33873
[1mStep[0m  [144/169], [94mLoss[0m : 2.11299
[1mStep[0m  [160/169], [94mLoss[0m : 2.07653

====================================
[1mEpoch[0m [4/30], [94mTrain[0m: 2.263, [92mTest[0m: 2.566, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.99528
[1mStep[0m  [16/169], [94mLoss[0m : 2.24827
[1mStep[0m  [32/169], [94mLoss[0m : 2.08454
[1mStep[0m  [48/169], [94mLoss[0m : 2.37365
[1mStep[0m  [64/169], [94mLoss[0m : 2.18002
[1mStep[0m  [80/169], [94mLoss[0m : 1.60768
[1mStep[0m  [96/169], [94mLoss[0m : 2.61308
[1mStep[0m  [112/169], [94mLoss[0m : 2.21871
[1mStep[0m  [128/169], [94mLoss[0m : 1.71899
[1mStep[0m  [144/169], [94mLoss[0m : 2.27691
[1mStep[0m  [160/169], [94mLoss[0m : 2.18213

====================================
[1mEpoch[0m [5/30], [94mTrain[0m: 2.192, [92mTest[0m: 2.418, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.08285
[1mStep[0m  [16/169], [94mLoss[0m : 2.08055
[1mStep[0m  [32/169], [94mLoss[0m : 2.21546
[1mStep[0m  [48/169], [94mLoss[0m : 1.95769
[1mStep[0m  [64/169], [94mLoss[0m : 2.36388
[1mStep[0m  [80/169], [94mLoss[0m : 2.03634
[1mStep[0m  [96/169], [94mLoss[0m : 2.01431
[1mStep[0m  [112/169], [94mLoss[0m : 2.09252
[1mStep[0m  [128/169], [94mLoss[0m : 2.12289
[1mStep[0m  [144/169], [94mLoss[0m : 2.45915
[1mStep[0m  [160/169], [94mLoss[0m : 2.08414

====================================
[1mEpoch[0m [6/30], [94mTrain[0m: 2.168, [92mTest[0m: 2.408, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.87352
[1mStep[0m  [16/169], [94mLoss[0m : 2.48460
[1mStep[0m  [32/169], [94mLoss[0m : 2.33405
[1mStep[0m  [48/169], [94mLoss[0m : 1.73714
[1mStep[0m  [64/169], [94mLoss[0m : 1.79877
[1mStep[0m  [80/169], [94mLoss[0m : 1.73896
[1mStep[0m  [96/169], [94mLoss[0m : 2.10412
[1mStep[0m  [112/169], [94mLoss[0m : 1.58130
[1mStep[0m  [128/169], [94mLoss[0m : 2.28218
[1mStep[0m  [144/169], [94mLoss[0m : 2.01876
[1mStep[0m  [160/169], [94mLoss[0m : 2.03424

====================================
[1mEpoch[0m [7/30], [94mTrain[0m: 2.100, [92mTest[0m: 2.380, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.34682
[1mStep[0m  [16/169], [94mLoss[0m : 2.03604
[1mStep[0m  [32/169], [94mLoss[0m : 2.21222
[1mStep[0m  [48/169], [94mLoss[0m : 2.11383
[1mStep[0m  [64/169], [94mLoss[0m : 1.84585
[1mStep[0m  [80/169], [94mLoss[0m : 1.59440
[1mStep[0m  [96/169], [94mLoss[0m : 2.09112
[1mStep[0m  [112/169], [94mLoss[0m : 2.43230
[1mStep[0m  [128/169], [94mLoss[0m : 1.93972
[1mStep[0m  [144/169], [94mLoss[0m : 2.09630
[1mStep[0m  [160/169], [94mLoss[0m : 2.03673

====================================
[1mEpoch[0m [8/30], [94mTrain[0m: 2.064, [92mTest[0m: 2.424, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.22239
[1mStep[0m  [16/169], [94mLoss[0m : 2.16916
[1mStep[0m  [32/169], [94mLoss[0m : 1.83367
[1mStep[0m  [48/169], [94mLoss[0m : 1.95374
[1mStep[0m  [64/169], [94mLoss[0m : 1.73445
[1mStep[0m  [80/169], [94mLoss[0m : 1.87004
[1mStep[0m  [96/169], [94mLoss[0m : 2.33923
[1mStep[0m  [112/169], [94mLoss[0m : 1.84046
[1mStep[0m  [128/169], [94mLoss[0m : 2.30940
[1mStep[0m  [144/169], [94mLoss[0m : 2.27031
[1mStep[0m  [160/169], [94mLoss[0m : 2.13155

====================================
[1mEpoch[0m [9/30], [94mTrain[0m: 2.013, [92mTest[0m: 2.451, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.12142
[1mStep[0m  [16/169], [94mLoss[0m : 1.82378
[1mStep[0m  [32/169], [94mLoss[0m : 1.77966
[1mStep[0m  [48/169], [94mLoss[0m : 1.92495
[1mStep[0m  [64/169], [94mLoss[0m : 2.09161
[1mStep[0m  [80/169], [94mLoss[0m : 1.94316
[1mStep[0m  [96/169], [94mLoss[0m : 1.98390
[1mStep[0m  [112/169], [94mLoss[0m : 1.92739
[1mStep[0m  [128/169], [94mLoss[0m : 2.26897
[1mStep[0m  [144/169], [94mLoss[0m : 1.88930
[1mStep[0m  [160/169], [94mLoss[0m : 1.77793

====================================
[1mEpoch[0m [10/30], [94mTrain[0m: 1.948, [92mTest[0m: 2.452, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 2.19584
[1mStep[0m  [16/169], [94mLoss[0m : 2.06220
[1mStep[0m  [32/169], [94mLoss[0m : 2.18321
[1mStep[0m  [48/169], [94mLoss[0m : 2.08942
[1mStep[0m  [64/169], [94mLoss[0m : 2.08920
[1mStep[0m  [80/169], [94mLoss[0m : 1.77003
[1mStep[0m  [96/169], [94mLoss[0m : 1.65985
[1mStep[0m  [112/169], [94mLoss[0m : 2.01253
[1mStep[0m  [128/169], [94mLoss[0m : 1.75986
[1mStep[0m  [144/169], [94mLoss[0m : 1.62133
[1mStep[0m  [160/169], [94mLoss[0m : 1.76146

====================================
[1mEpoch[0m [11/30], [94mTrain[0m: 1.909, [92mTest[0m: 2.443, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.85923
[1mStep[0m  [16/169], [94mLoss[0m : 1.91103
[1mStep[0m  [32/169], [94mLoss[0m : 1.53627
[1mStep[0m  [48/169], [94mLoss[0m : 2.07671
[1mStep[0m  [64/169], [94mLoss[0m : 2.24795
[1mStep[0m  [80/169], [94mLoss[0m : 1.66965
[1mStep[0m  [96/169], [94mLoss[0m : 1.76933
[1mStep[0m  [112/169], [94mLoss[0m : 1.69965
[1mStep[0m  [128/169], [94mLoss[0m : 1.85016
[1mStep[0m  [144/169], [94mLoss[0m : 1.75799
[1mStep[0m  [160/169], [94mLoss[0m : 1.63536

====================================
[1mEpoch[0m [12/30], [94mTrain[0m: 1.880, [92mTest[0m: 2.473, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.84489
[1mStep[0m  [16/169], [94mLoss[0m : 1.92868
[1mStep[0m  [32/169], [94mLoss[0m : 1.57155
[1mStep[0m  [48/169], [94mLoss[0m : 1.52991
[1mStep[0m  [64/169], [94mLoss[0m : 1.71191
[1mStep[0m  [80/169], [94mLoss[0m : 2.08093
[1mStep[0m  [96/169], [94mLoss[0m : 1.78907
[1mStep[0m  [112/169], [94mLoss[0m : 1.99509
[1mStep[0m  [128/169], [94mLoss[0m : 2.63192
[1mStep[0m  [144/169], [94mLoss[0m : 2.04946
[1mStep[0m  [160/169], [94mLoss[0m : 1.66368

====================================
[1mEpoch[0m [13/30], [94mTrain[0m: 1.840, [92mTest[0m: 2.465, [96mlr[0m: 0.007525000000000001
====================================

[1mStep[0m  [0/169], [94mLoss[0m : 1.40053
[1mStep[0m  [16/169], [94mLoss[0m : 1.90966
[1mStep[0m  [32/169], [94mLoss[0m : 1.99122
[1mStep[0m  [48/169], [94mLoss[0m : 1.64027
[1mStep[0m  [64/169], [94mLoss[0m : 1.68606
[1mStep[0m  [80/169], [94mLoss[0m : 2.16128
[1mStep[0m  [96/169], [94mLoss[0m : 2.00901
